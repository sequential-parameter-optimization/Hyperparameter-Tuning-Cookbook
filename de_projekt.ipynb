{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "lang: de\n",
        "eval: true\n",
        "---\n",
        "\n",
        "# Lernmodul: Kriging Projekt\n",
        "\n",
        "\n",
        "Um die gestellte Aufgabe zu lösen, erstellen wir eine Python-Klasse `KrigingRegressor` für das Kriging-Modell, eine Black-Box-Funktion `f(x)`, eine Funktion zur Erstellung des initialen Stichprobenplans und implementieren dann den sequenziellen Optimierungsablauf.\n",
        "\n",
        "**1. Die `KrigingRegressor`-Klasse**\n",
        "Diese Klasse kapselt die Logik für das Kriging-Modell, einschließlich der Berechnung der Korrelationsmatrizen, der Maximum-Likelihood-Schätzung für den globalen Mittelwert $\\hat{\\mu}$ und die Prozessvarianz $\\hat{\\sigma}^2$, der konzentrierten Log-Likelihood-Funktion zur Hyperparameter-Optimierung sowie der Vorhersagefunktion. Die Optimierung des Aktivitätsparameters $\\vec{\\theta}$ erfolgt über die Maximierung der konzentrierten Log-Likelihood-Funktion, wobei intern die negative Log-Likelihood minimiert wird. Der Glattheitsparameter $\\vec{p}$ wird implizit auf $p_j=2$ gesetzt, da die quadrierte euklidische Distanz verwendet wird. Es wird ein kleiner Nugget-Term (`eps`) zur Diagonalen der Korrelationsmatrix hinzugefügt, um die numerische Stabilität zu gewährleisten.\n",
        "\n",
        "**2. Die Black-Box-Funktion `f(x)`**\n",
        "Diese Funktion simuliert ein teures oder undurchsichtiges System, dessen interne Funktionsweise dem Optimierungsalgorithmus nicht bekannt ist. Im weiteren Schritt wird ein Server zur Beantwortung der Auswertungen bereitgestellt, der die Black-Box-Funktion ausführt. In diesem Beispiel verwenden wir eine analytische Funktion, die eine gewisse Komplexität aufweist, um die Black-Box zu simulieren.\n",
        "\n",
        "**3. Initialer Stichprobenplan `X`**\n",
        "Für einen \"optimalen\" Versuchsplan wird Latin Hypercube Sampling (LHS) verwendet, da dies eine raumfüllende Eigenschaft aufweist. Dies stellt sicher, dass der Eingaberaum effizient erkundet wird. Die Eingabedaten werden intern auf den Bereich $$ skaliert, um die Konsistenz der $\\theta$-Werte über verschiedene Probleme hinweg zu gewährleisten.\n",
        "\n",
        "**4. Sequenzieller Optimierungsablauf**\n",
        "Der Prozess beginnt mit einem initialen Stichprobenplan. In jeder Iteration wird das Kriging-Modell mit den verfügbaren Daten gefittet. Anschließend wird eine Optimierung auf dem Surrogatmodell (dem gefitteten Kriging-Modell) durchgeführt, um den nächsten vielversprechenden Punkt im Designraum zu finden. Dieser Punkt wird der Black-Box-Funktion übergeben, und die erhaltene Beobachtung wird zum Trainingsdatensatz hinzugefügt. Dieser iterative Prozess wird bis zu einer maximalen Anzahl von Funktionsauswertungen fortgesetzt.\n",
        "\n",
        "Hier ist der entsprechende Python-Code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform, cdist\n",
        "from numpy.linalg import cholesky, solve, LinAlgError\n",
        "from numpy import spacing, sqrt, exp, multiply, eye, ones\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import qmc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import (exp, multiply, eye, linspace, spacing, sqrt)\n",
        "from numpy.linalg import cholesky, solve\n",
        "from scipy.spatial.distance import squareform, pdist, cdist\n",
        "from scipy.optimize import minimize # Für die Optimierung\n",
        "\n",
        "# 1. Definition der KrigingRegressor-Klasse\n",
        "\n",
        "class KrigingRegressor:\n",
        "    \"\"\"Ein Kriging-Regressionsmodell mit Hyperparameter-Optimierung.\n",
        "    \n",
        "    Attributes:\n",
        "        initial_theta (float): Startwert für den Aktivitätsparameter Theta.\n",
        "        bounds (list): Liste von Tupeln für die Grenzen der Hyperparameter-Optimierung.\n",
        "        opt_theta_ (float): Optimierter Theta-Wert nach dem Fitting.\n",
        "        X_train_ (array): Trainings-Eingabedaten.\n",
        "        y_train_ (array): Trainings-Zielwerte.\n",
        "        U_ (array): Cholesky-Zerlegung der Korrelationsmatrix.\n",
        "        mu_hat_ (float): Geschätzter Mittelwert.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self, initial_theta=1.0, bounds=[(0.001, 100.0)]):\n",
        "        self.initial_theta = initial_theta\n",
        "        self.bounds = bounds\n",
        "    \n",
        "    def _build_Psi(self, X, log_theta, eps=sqrt(spacing(1))):\n",
        "        \"\"\"Berechnet die Korrelationsmatrix Psi.\n",
        "        \n",
        "        Args:\n",
        "            X (np.ndarray): Eingabedaten-Matrix\n",
        "            log_theta (float or np.ndarray): Log10-transformierte Theta-Werte\n",
        "            eps (float, optional): Nugget-Term für numerische Stabilität\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Korrelationsmatrix Psi\n",
        "        \"\"\"\n",
        "        # Konvertiere log_theta zurück zu theta (immer positiv durch exp)\n",
        "        theta = 10.0**log_theta\n",
        "        \n",
        "        if not isinstance(theta, np.ndarray) or theta.ndim == 0:\n",
        "            theta = np.array([theta])\n",
        "        \n",
        "        D = squareform(pdist(X, metric='sqeuclidean', w=theta))\n",
        "        Psi = exp(-D)\n",
        "        Psi += multiply(eye(X.shape[0]), eps)\n",
        "        return Psi\n",
        "\n",
        "    def _build_psi(self, X_train, x_predict, log_theta):\n",
        "        \"\"\"Berechnet den Korrelationsvektor psi.\n",
        "        \n",
        "        Args:\n",
        "            X_train (np.ndarray): Trainings-Eingabedaten\n",
        "            x_predict (np.ndarray): Vorhersage-Eingabepunkte\n",
        "            log_theta (float or np.ndarray): Log10-transformierte Theta-Werte\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Korrelationsvektor psi\n",
        "        \"\"\"\n",
        "        # Konvertiere log_theta zurück zu theta\n",
        "        theta = 10.0**log_theta\n",
        "        \n",
        "        if not isinstance(theta, np.ndarray) or theta.ndim == 0:\n",
        "            theta = np.array([theta])\n",
        "        \n",
        "        D = cdist(x_predict, X_train, metric='sqeuclidean', w=theta)\n",
        "        psi = exp(-D)\n",
        "        return psi.T\n",
        "    \n",
        "    def _neg_log_likelihood(self, params, X_train, y_train):\n",
        "        \"\"\"Berechnet die negative konzentrierte Log-Likelihood.\"\"\"\n",
        "        theta = params\n",
        "        n = X_train.shape[0]\n",
        "        \n",
        "        try:\n",
        "            Psi = self._build_Psi(X_train, theta)\n",
        "            U = cholesky(Psi).T\n",
        "        except np.linalg.LinAlgError:\n",
        "            return 1e15\n",
        "        \n",
        "        one = np.ones(n).reshape(-1, 1)\n",
        "        \n",
        "        # Berechne mu_hat (MLE des Mittelwerts)\n",
        "        Psi_inv_y = solve(U, solve(U.T, y_train))\n",
        "        Psi_inv_one = solve(U, solve(U.T, one))\n",
        "        mu_hat = (one.T @ Psi_inv_y) / (one.T @ Psi_inv_one)\n",
        "        mu_hat = mu_hat.item()\n",
        "        \n",
        "        # Berechne sigma_hat_sq (MLE der Prozessvarianz)\n",
        "        y_minus_mu_one = y_train - one * mu_hat\n",
        "        sigma_hat_sq = (y_minus_mu_one.T @ Psi_inv_y) / n\n",
        "        sigma_hat_sq = sigma_hat_sq.item()\n",
        "        \n",
        "        if sigma_hat_sq < 1e-10:\n",
        "            return 1e15\n",
        "        \n",
        "        # Berechne negative konzentrierte Log-Likelihood\n",
        "        log_det_Psi = 2 * np.sum(np.log(np.diag(U.T)))\n",
        "        nll = 0.5 * (n * np.log(sigma_hat_sq) + log_det_Psi)\n",
        "        \n",
        "        return nll\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit das Kriging-Modell an die Trainingsdaten.\n",
        "        Args:\n",
        "            X (array-like): Eingabedaten der Form (n_samples, n_features)\n",
        "            y (array-like): Zielwerte der Form (n_samples,)\n",
        "\n",
        "        Returns:\n",
        "            self: Das gefittete Kriging-Modell\n",
        "\n",
        "        Notes:\n",
        "            Diese Methode optimiert die Hyperparameter Theta, indem sie die negative Log-Likelihood minimiert.        \n",
        "\n",
        "        \"\"\"\n",
        "        self.X_train_ = X\n",
        "        self.y_train_ = y.reshape(-1, 1)\n",
        "        \n",
        "        # Optimierung der Hyperparameter\n",
        "        initial_theta = np.array([self.initial_theta])\n",
        "        result = minimize(self._neg_log_likelihood, \n",
        "                        initial_theta,\n",
        "                        args=(self.X_train_, self.y_train_),\n",
        "                        method='L-BFGS-B', \n",
        "                        bounds=self.bounds)\n",
        "        \n",
        "        self.opt_theta_ = result.x\n",
        "        \n",
        "        # Berechne optimale Parameter für Vorhersagen\n",
        "        Psi_opt = self._build_Psi(self.X_train_, self.opt_theta_)\n",
        "        self.U_ = cholesky(Psi_opt).T\n",
        "        n_train = self.X_train_.shape[0]\n",
        "        one = np.ones(n_train).reshape(-1, 1)\n",
        "        \n",
        "        self.mu_hat_ = (one.T @ solve(self.U_, solve(self.U_.T, self.y_train_))) / \\\n",
        "                      (one.T @ solve(self.U_, solve(self.U_.T, one)))\n",
        "        self.mu_hat_ = self.mu_hat_.item()\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Vorhersage für neue Datenpunkte.\n",
        "        \n",
        "        Args:\n",
        "            X (array-like): Eingabedaten für Vorhersage der Form (n_samples, n_features)\n",
        "            \n",
        "        Returns:\n",
        "            array: Vorhergesagte Werte\n",
        "        \"\"\"\n",
        "        n_train = self.X_train_.shape[0]\n",
        "        one = np.ones(n_train).reshape(-1, 1)\n",
        "        # Fix: Use self._build_psi instead of build_psi\n",
        "        psi = self._build_psi(self.X_train_, X, self.opt_theta_)\n",
        "        \n",
        "        return self.mu_hat_ * np.ones(X.shape[0]).reshape(-1, 1) + \\\n",
        "            psi.T @ solve(self.U_, solve(self.U_.T, \n",
        "            self.y_train_ - one * self.mu_hat_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Die Black-Box-Funktion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f_black_box(x):\n",
        "    \"\"\"Analytische Black-Box-Funktion: f(x)\"\"\"\n",
        "    return -x**2*np.cos(x) + 1 + x**2*np.sin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Funktion zur Erstellung des initialen Stichprobenplans (Latin Hypercube Sampling)\n",
        "def create_initial_design(n_points, dimensionality, x_range):\n",
        "    \"\"\"Erstellt einen raumfüllenden initialen Versuchsplan mittels Latin Hypercube Sampling.\n",
        "    \n",
        "    Args:\n",
        "        n_points (int): Anzahl der Designpunkte\n",
        "        dimensionality (int): Dimension des Eingaberaums\n",
        "        x_range (tuple): Tuple mit (unterer_grenze, oberer_grenze) für den Wertebereich\n",
        "        \n",
        "    Returns:\n",
        "        np.ndarray: Matrix der Designpunkte der Form (n_points, dimensionality)\n",
        "    \"\"\"\n",
        "    # Verwende einen Integer als Seed statt SeedSequence\n",
        "    sampler = qmc.LatinHypercube(d=dimensionality, seed=1234)\n",
        "    \n",
        "    # Generiere Samples im Einheitswürfel [0,1]^d\n",
        "    lhs_samples_unit_cube = sampler.random(n=n_points)\n",
        "    \n",
        "    # Extrahiere die Grenzen\n",
        "    x_lower, x_upper = x_range\n",
        "    \n",
        "    # Skaliere die Samples vom Einheitswürfel auf den gewünschten Bereich\n",
        "    X_initial = x_lower + (x_upper - x_lower) * lhs_samples_unit_cube\n",
        "    \n",
        "    # Stelle sicher, dass das Output ein 2D Array ist\n",
        "    return X_initial.reshape(-1, dimensionality)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hauptskript für die sequentielle Optimierung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(42) # Für Reproduzierbarkeit der Zufallszahlen\n",
        "\n",
        "# Definition des Suchraums für die Black-Box-Funktion\n",
        "x_lower_bound = -5.0\n",
        "x_upper_bound = 5.0\n",
        "search_range = (x_lower_bound, x_upper_bound)\n",
        "dimensionality = 1 # f(x) = x^2 + 1 ist 1-dimensional\n",
        "\n",
        "# Parameter für den initialen Stichprobenplan\n",
        "n_initial_points = 7\n",
        "\n",
        "# Maximale Gesamtzahl an Funktionsauswertungen\n",
        "N_max_evaluations = 20 \n",
        "\n",
        "# --- Schritt 1: Initialen Stichprobenplan erstellen und Black-Box-Funktion auswerten ---\n",
        "X_train_current = create_initial_design(n_initial_points, dimensionality, search_range)\n",
        "y_train_current = f_black_box(X_train_current)\n",
        "\n",
        "print(f\"Initialer X_train (n={n_initial_points}):\\n{np.round(X_train_current.flatten(), 3)}\")\n",
        "print(f\"Zugehöriger y_train:\\n{np.round(y_train_current.flatten(), 3)}\")\n",
        "print(\"-\" * 70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    # --- Schritt 2: KrigingRegressor-Modell initialisieren ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bounds für log10(theta), z.B. 10^-3 bis 10^2\n",
        "theta_bounds_log10 = [(-3.0, 2.0)] \n",
        "M_1 = KrigingRegressor(initial_theta=1.0, bounds=theta_bounds_log10)\n",
        "\n",
        "# --- Schritt 3: Sequenzieller Optimierungs-Loop ---\n",
        "# Anzahl der Punkte, die nach dem initialen Plan hinzugefügt werden\n",
        "num_infill_steps = N_max_evaluations - n_initial_points\n",
        "\n",
        "for i in range(num_infill_steps + 1): # +1, um auch nach dem letzten Infill-Punkt zu fitten\n",
        "    current_evals = len(X_train_current)\n",
        "    print(f\"\\n--- Iteration {i+1} (Gesamtauswertungen: {current_evals}) ---\")\n",
        "    \n",
        "    # Modell mit den aktuellen Daten fitten\n",
        "    print(\"Kriging-Modell wird gefittet...\")\n",
        "    M_1.fit(X_train_current, y_train_current)\n",
        "    print(f\"Optimiertes Theta: {np.round(M_1.opt_theta_.item(), 4)}\")\n",
        "    print(f\"Geschätzter globaler Mittelwert (mu_hat): {np.round(M_1.mu_hat_, 4)}\")\n",
        "\n",
        "    # Überprüfen, ob die maximale Anzahl an Auswertungen erreicht wurde\n",
        "    if current_evals >= N_max_evaluations:\n",
        "        print(\"Maximale Anzahl an Auswertungen erreicht. Optimierung wird beendet.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- Schritt 4: Surrogat-basierte Optimierung zur Suche des nächsten Infill-Punktes ---\n",
        "\n",
        "Definiere die Zielfunktion für die innere Optimierung (Minimierung des Surrogats)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def surrogate_objective(x_val):\n",
        "    # Stelle sicher, dass x_val im richtigen Format (2D-Array) für predict ist\n",
        "    x_val_reshaped = np.atleast_2d(x_val) \n",
        "    return M_1.predict(x_val_reshaped).item() # Rückgabe als Skalar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Suchgrenzen für die Optimierung auf dem Surrogatmodell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "surrogate_search_bounds = [(x_lower_bound, x_upper_bound)] * dimensionality\n",
        "\n",
        "print(\"Optimierung auf dem Surrogatmodell, um den nächsten Infill-Punkt zu finden...\")\n",
        "# Anfangsschätzung für die Surrogat-Optimierung (zufälliger Punkt im Suchraum)\n",
        "x0_surrogate_opt = np.random.uniform(low=x_lower_bound, high=x_upper_bound, size=dimensionality)\n",
        "\n",
        "# Verwende L-BFGS-B für die Surrogat-Optimierung\n",
        "surrogate_opt_result = minimize(surrogate_objective, \n",
        "                            x0=x0_surrogate_opt, \n",
        "                            method='L-BFGS-B', \n",
        "                            bounds=surrogate_search_bounds)\n",
        "\n",
        "x_new_infill = np.atleast_2d(surrogate_opt_result.x) # Sicherstellen, dass 2D-Array\n",
        "y_new_infill = f_black_box(x_new_infill) # Auswertung des neuen Punktes durch Black-Box\n",
        "\n",
        "print(f\"Neuer Infill-Punkt x_1: {np.round(x_new_infill.flatten(), 3)}, \"\n",
        "        f\"Zugehöriger y_1 (Black-Box): {np.round(y_new_infill.item(), 3)}\")\n",
        "\n",
        "# Neuen Punkt zum Trainingsdatensatz hinzufügen\n",
        "X_train_current = np.vstack((X_train_current, x_new_infill))\n",
        "y_train_current = np.vstack((y_train_current, y_new_infill))\n",
        "\n",
        "print(f\"Aktueller X_train (total {len(X_train_current)} Punkte):\\n{np.round(X_train_current.flatten(), 3)}\")\n",
        "print(f\"Aktueller y_train:\\n{np.round(y_train_current.flatten(), 3)}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(\"\\n--- Optimierungsprozess abgeschlossen ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fitte das finale Modell mit allen verfügbaren N_max_evaluations Punkten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "M_1.fit(X_train_current, y_train_current)\n",
        "\n",
        "# Visualisierung der Ergebnisse\n",
        "x_plot = np.linspace(x_lower_bound, x_upper_bound, 200).reshape(-1, 1)\n",
        "y_predict_final = M_1.predict(x_plot)\n",
        "y_true = f_black_box(x_plot)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_plot, y_true, 'k--', linewidth=2, label='Wahre Funktion: $f(x)$')\n",
        "plt.plot(X_train_current, y_train_current, 'bo', markersize=8, label=f'Ausgewertete Punkte (N={len(X_train_current)})')\n",
        "plt.plot(x_plot, y_predict_final, 'r-', linewidth=2, label='Kriging-Vorhersage')\n",
        "plt.title('Kriging-Modell mit sequenzieller Optimierung der Hyperparameter')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal optimiertes Theta: {np.round(M_1.opt_theta_.item(), 4)}\")\n",
        "print(f\"Final geschätzter globaler Mittelwert: {np.round(M_1.mu_hat_, 4)}\")\n",
        "# Der letzte optimierte Punkt auf dem Surrogat (x_new_infill) ist eine gute Schätzung des Optimums der Funktion.\n",
        "print(f\"Bester gefundener x-Wert (aus Surrogat-Optimierung): {np.round(surrogate_opt_result.x.item(), 3)}\")\n",
        "print(f\"Zugehöriger y-Wert der Black-Box an diesem x: {np.round(f_black_box(surrogate_opt_result.x).item(), 3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot313/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}