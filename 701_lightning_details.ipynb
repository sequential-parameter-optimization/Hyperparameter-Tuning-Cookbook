{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Details of the Lightning Module Integration in spotpython\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Based on the Diabetes Data set and the `NNLinearRegressor` model, we will provide details on the integration of the Lightning module in spotpython. \n",
        "\n",
        "* @sec-hyperlight-fun: The `Hyperlight` class provides the `fun` method, which takes `X` and `fun_control` as arguments. It calls the `train_model` method.\n",
        "* @sec-trainmodel: The `train_model` method trains the model and returns the loss.\n",
        "* @sec-trainer: The `Trainer` class is used to train the model and validate it. It also uses the `LightDataModule` class to load the data.\n",
        "\n",
        "\n",
        "## 1. spotpython.fun.hyperlight.HyperLight.fun() {#sec-hyperlight-fun}\n",
        "\n",
        "The class `Hyperlight` provides the method `fun`, which takes `X` (`np.ndarray`) and `fun_control` (`dict`) as arguments.\n",
        "It calls the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from math import inf\n",
        "import numpy as np\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.spot import spot\n",
        "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
        "\n",
        "PREFIX=\"000\"\n",
        "\n",
        "data_set = Diabetes()\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    save_experiment=True,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    tensorboard_log=True,\n",
        "    seed=42,)\n",
        "\n",
        "print(gen_design_table(fun_control))\n",
        "\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# set epochs to 2^8:\n",
        "# X[0, 1] = 8\n",
        "# set patience to 2^10:\n",
        "# X[0, 7] = 10\n",
        "\n",
        "print(f\"X: {X}\")\n",
        "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
        "# so that two values are returned\n",
        "X = np.vstack((X, X, X))\n",
        "print(f\"X: {X}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_light = HyperLight(seed=125, log_level=50)\n",
        "hyper_light.fun(X, fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Using the same seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_light = HyperLight(seed=125, log_level=50)\n",
        "hyper_light.fun(X, fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Using a different seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_light = HyperLight(seed=123, log_level=50)\n",
        "hyper_light.fun(X, fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. spotpython.light.trainmodel.train_model() {#sec-trainmodel}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from math import inf\n",
        "import numpy as np\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
        "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
        "from spotpython.light.trainmodel import train_model\n",
        "import pprint\n",
        "\n",
        "PREFIX=\"000\"\n",
        "\n",
        "data_set = Diabetes()\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    save_experiment=True,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    tensorboard_log=True,\n",
        "    seed=42,)\n",
        "\n",
        "print(gen_design_table(fun_control))\n",
        "\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# set epochs to 2^8:\n",
        "# X[0, 1] = 8\n",
        "# set patience to 2^10:\n",
        "# X[0, 7] = 10\n",
        "\n",
        "print(f\"X: {X}\")\n",
        "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
        "# so that two values are returned\n",
        "X = np.vstack((X, X))\n",
        "var_dict = assign_values(X, get_var_name(fun_control))\n",
        "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
        "    pprint.pprint(config)\n",
        "    y = train_model(config, fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Trainer: fit and validate {#sec-trainer}\n",
        "\n",
        "* Generate the `config` dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from math import inf\n",
        "import numpy as np\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
        "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
        "from spotpython.light.trainmodel import train_model\n",
        "\n",
        "PREFIX=\"000\"\n",
        "\n",
        "data_set = Diabetes()\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    save_experiment=True,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    tensorboard_log=True,\n",
        "    seed=42,)\n",
        "print(gen_design_table(fun_control))\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# set epochs to 2^8:\n",
        "X[0, 1] = 10\n",
        "# set patience to 2^10:\n",
        "X[0, 7] = 10\n",
        "print(f\"X: {X}\")\n",
        "var_dict = assign_values(X, get_var_name(fun_control))\n",
        "config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n",
        "config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "_L_in = 10\n",
        "_L_out = 1\n",
        "_L_cond = 0\n",
        "_torchmetric = \"mean_squared_error\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Commented: Using the fun_control dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n",
        "# model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the source code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import lightning as L\n",
        "import torch\n",
        "from torch import nn\n",
        "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
        "import torchmetrics.functional.regression\n",
        "import torch.optim as optim\n",
        "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
        "\n",
        "\n",
        "class NNLinearRegressor(L.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        l1: int,\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        initialization: str,\n",
        "        act_fn: nn.Module,\n",
        "        optimizer: str,\n",
        "        dropout_prob: float,\n",
        "        lr_mult: float,\n",
        "        patience: int,\n",
        "        batch_norm: bool,\n",
        "        _L_in: int,\n",
        "        _L_out: int,\n",
        "        _torchmetric: str,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n",
        "        # checkpointing. It is recommended to ignore them\n",
        "        # using `self.save_hyperparameters(ignore=['act_fn'])`\n",
        "        # self.save_hyperparameters(ignore=[\"act_fn\"])\n",
        "        #\n",
        "        self._L_in = _L_in\n",
        "        self._L_out = _L_out\n",
        "        if _torchmetric is None:\n",
        "            _torchmetric = \"mean_squared_error\"\n",
        "        self._torchmetric = _torchmetric\n",
        "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
        "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
        "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
        "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
        "        # set dummy input array for Tensorboard Graphs\n",
        "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
        "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
        "        if self.hparams.l1 < 4:\n",
        "            raise ValueError(\"l1 must be at least 4\")\n",
        "        hidden_sizes = get_hidden_sizes(_L_in=self._L_in, l1=l1, n=10)\n",
        "\n",
        "        if batch_norm:\n",
        "            # Add batch normalization layers\n",
        "            layers = []\n",
        "            layer_sizes = [self._L_in] + hidden_sizes\n",
        "            for i in range(len(layer_sizes) - 1):\n",
        "                current_layer_size = layer_sizes[i]\n",
        "                next_layer_size = layer_sizes[i + 1]\n",
        "                layers += [\n",
        "                    nn.Linear(current_layer_size, next_layer_size),\n",
        "                    nn.BatchNorm1d(next_layer_size),\n",
        "                    self.hparams.act_fn,\n",
        "                    nn.Dropout(self.hparams.dropout_prob),\n",
        "                ]\n",
        "            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
        "        else:\n",
        "            layers = []\n",
        "            layer_sizes = [self._L_in] + hidden_sizes\n",
        "            for i in range(len(layer_sizes) - 1):\n",
        "                current_layer_size = layer_sizes[i]\n",
        "                next_layer_size = layer_sizes[i + 1]\n",
        "                layers += [\n",
        "                    nn.Linear(current_layer_size, next_layer_size),\n",
        "                    self.hparams.act_fn,\n",
        "                    nn.Dropout(self.hparams.dropout_prob),\n",
        "                ]\n",
        "            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
        "\n",
        "        # Wrap the layers into a sequential container\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Initialization (Xavier, Kaiming, or Default)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if self.hparams.initialization == \"xavier_uniform\":\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "            elif self.hparams.initialization == \"xavier_normal\":\n",
        "                nn.init.xavier_normal_(module.weight)\n",
        "            elif self.hparams.initialization == \"kaiming_uniform\":\n",
        "                nn.init.kaiming_uniform_(module.weight)\n",
        "            elif self.hparams.initialization == \"kaiming_normal\":\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "            else:  # \"Default\"\n",
        "                nn.init.uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor containing a batch of input data.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the output of the model.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "    def _calculate_loss(self, batch):\n",
        "        \"\"\"\n",
        "        Calculate the loss for the given batch.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            mode (str, optional): The mode of the model. Defaults to \"train\".\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y = y.view(len(y), 1)\n",
        "        y_hat = self(x)\n",
        "        loss = self.metric(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single training step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        loss = self._calculate_loss(batch)\n",
        "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single validation step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        loss = self._calculate_loss(batch)\n",
        "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single test step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "        \"\"\"\n",
        "        loss = self._calculate_loss(batch)\n",
        "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single prediction step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the prediction for this batch.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        yhat = self(x)\n",
        "        y = y.view(len(y), 1)\n",
        "        yhat = yhat.view(len(yhat), 1)\n",
        "        print(f\"Predict step x: {x}\")\n",
        "        print(f\"Predict step y: {y}\")\n",
        "        print(f\"Predict step y_hat: {yhat}\")\n",
        "        # pred_loss = F.mse_loss(y_hat, y)\n",
        "        # pred loss not registered\n",
        "        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n",
        "        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n",
        "        # MisconfigurationException: You are trying to `self.log()`\n",
        "        # but the loop's result collection is not registered yet.\n",
        "        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n",
        "        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n",
        "        return (x, y, yhat)\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        \"\"\"\n",
        "        Configures the optimizer for the model.\n",
        "\n",
        "        Notes:\n",
        "            The default Lightning way is to define an optimizer as\n",
        "            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n",
        "            spotpython uses an optimizer handler to create the optimizer, which\n",
        "            adapts the learning rate according to the lr_mult hyperparameter as\n",
        "            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n",
        "\n",
        "        Returns:\n",
        "            torch.optim.Optimizer: The optimizer to use during training.\n",
        "\n",
        "        \"\"\"\n",
        "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n",
        "\n",
        "        num_milestones = 3  # Number of milestones to divide the epochs\n",
        "        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n",
        "\n",
        "        lr_scheduler_config = {\n",
        "            \"scheduler\": scheduler,\n",
        "            \"interval\": \"epoch\",\n",
        "            \"frequency\": 1,\n",
        "        }\n",
        "\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = NNLinearRegressor(**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "\n",
        "data_set = Diabetes()\n",
        "dm = LightDataModule(\n",
        "    dataset=data_set,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    test_size=fun_control[\"test_size\"],\n",
        "    test_seed=fun_control[\"test_seed\"],\n",
        "    scaler=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Using `callbacks` for early stopping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "timestamp = True\n",
        "\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "if not timestamp:\n",
        "    # add ModelCheckpoint only if timestamp is False\n",
        "    callbacks.append(ModelCheckpoint(dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True))  # Save the last checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.utils.eda import generate_config_id\n",
        "if timestamp:\n",
        "    # config id is unique. Since the model is not loaded from a checkpoint,\n",
        "    # the config id is generated here with a timestamp.\n",
        "    config_id = generate_config_id(config, timestamp=True)\n",
        "else:\n",
        "    # config id is not time-dependent and therefore unique,\n",
        "    # so that the model can be loaded from a checkpoint,\n",
        "    # the config id is generated here without a timestamp.\n",
        "    config_id = generate_config_id(config, timestamp=False) + \"_TRAIN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import lightning as L\n",
        "import os\n",
        "trainer = L.Trainer(\n",
        "    # Where to save models\n",
        "    default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n",
        "    max_epochs=model.hparams.epochs,\n",
        "    accelerator=fun_control[\"accelerator\"],\n",
        "    devices=fun_control[\"devices\"],\n",
        "    strategy=fun_control[\"strategy\"],\n",
        "    num_nodes=fun_control[\"num_nodes\"],\n",
        "    precision=fun_control[\"precision\"],\n",
        "    logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n",
        "    callbacks=callbacks,\n",
        "    enable_progress_bar=False,\n",
        "    num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n",
        "    log_every_n_steps=fun_control[\"log_every_n_steps\"],\n",
        "    gradient_clip_val=None,\n",
        "    gradient_clip_algorithm=\"norm\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.fit(model=model, datamodule=dm, ckpt_path=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.validate(model=model, datamodule=dm, verbose=True, ckpt_path=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataModule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "data_module.setup()\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Generate the `config` dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from math import inf\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "import os\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
        "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
        "from spotpython.light.trainmodel import train_model, generate_config_id_with_timestamp\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "PREFIX=\"000\"\n",
        "data_set = Diabetes()\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    save_experiment=True,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    tensorboard_log=True,\n",
        "    seed=42,)\n",
        "print(gen_design_table(fun_control))\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# set epochs to 2^8:\n",
        "X[0, 1] = 10\n",
        "# set patience to 2^10:\n",
        "X[0, 7] = 10\n",
        "print(f\"X: {X}\")\n",
        "var_dict = assign_values(X, get_var_name(fun_control))\n",
        "config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n",
        "_L_in = fun_control[\"_L_in\"]\n",
        "_L_out = fun_control[\"_L_out\"]\n",
        "_L_cond = fun_control[\"_L_cond\"]\n",
        "_torchmetric = fun_control[\"_torchmetric\"]\n",
        "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n",
        "dm = LightDataModule(\n",
        "    dataset=fun_control[\"data_set\"],\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    num_workers=fun_control[\"num_workers\"],\n",
        "    test_size=fun_control[\"test_size\"],\n",
        "    test_seed=fun_control[\"test_seed\"],\n",
        "    scaler=fun_control[\"scaler\"],\n",
        ")\n",
        "config_id = generate_config_id_with_timestamp(config, timestamp=True)\n",
        "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)]\n",
        "trainer = L.Trainer(\n",
        "    default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n",
        "    max_epochs=model.hparams.epochs,\n",
        "    accelerator=fun_control[\"accelerator\"],\n",
        "    devices=fun_control[\"devices\"],\n",
        "    strategy=fun_control[\"strategy\"],\n",
        "    num_nodes=fun_control[\"num_nodes\"],\n",
        "    precision=fun_control[\"precision\"],\n",
        "    logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n",
        "    callbacks=callbacks,\n",
        "    enable_progress_bar=False,\n",
        "    num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n",
        "    log_every_n_steps=fun_control[\"log_every_n_steps\"],\n",
        "    gradient_clip_val=None,\n",
        "    gradient_clip_algorithm=\"norm\",\n",
        ")\n",
        "trainer.fit(model=model, datamodule=dm, ckpt_path=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from math import inf\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.utils.scaler import TorchStandardScaler\n",
        "PREFIX=\"000\"\n",
        "data_set = Diabetes()\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1)\n",
        "X = np.array([[3.0e+00, 5.0, 4.0e+00, 2.0e+00, 1.1e+01, 1.0e-02, 1.0e+00, 1.0e+01, 0.0e+00,\n",
        "  0.0e+00]])\n",
        "var_dict = assign_values(X, get_var_name(fun_control))\n",
        "config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n",
        "_torchmetric = \"mean_squared_error\"\n",
        "model = fun_control[\"core_model\"](**config, _L_in=10, _L_out=1, _L_cond=None, _torchmetric=_torchmetric)\n",
        "dm = LightDataModule(\n",
        "    dataset=data_set,\n",
        "    batch_size=16,\n",
        "    test_size=0.6,\n",
        "    scaler=TorchStandardScaler())\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=32,\n",
        "    enable_progress_bar=False,\n",
        ")\n",
        "trainer.fit(model=model, datamodule=dm, ckpt_path=None)\n",
        "trainer.validate(model=model, datamodule=dm, ckpt_path=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}