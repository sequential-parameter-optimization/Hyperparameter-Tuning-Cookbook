{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Hyperparameter Tuning with PyTorch Lightning and User Data Sets  {#sec-light-user-data-1001}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| label: 1001_user_data_imports\n",
        "import numpy as np\n",
        "import os\n",
        "from math import inf\n",
        "import numpy as np\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
        "from spotpython.utils.eda import print_res_table\n",
        "from spotpython.hyperparameters.values import set_hyperparameter\n",
        "from spotpython.spot import Spot\n",
        "from math import inf\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating thr Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# epochs = 2000\n",
        "# seeds = [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
        "data = pd.read_csv(\"/Users/bartz/workspace/schu25a_netgen_gecco/data/data_man_tca88.csv\")\n",
        "data[\"source_file\"] = [\"tca88\"] * len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from spotpython.data.manydataset import ManyToManyDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_data(data,\n",
        "              input_features=['V tot V red [m³/s]'], \n",
        "              target= 'PI tot V [-]' ,\n",
        "              drop = ['Bereich u2red','source_file'],\n",
        "              group_by='Bereich u2red',\n",
        "              feature_scaling=None, \n",
        "              target_scaling=None, \n",
        "              create_dataset=True, \n",
        "              dataset_type='many_to_many'):\n",
        "\n",
        "    if feature_scaling is not None:\n",
        "        data[input_features] = feature_scaling.fit_transform(data[input_features])\n",
        "    \n",
        "    if target_scaling is not None:\n",
        "        data[target] = target_scaling.fit_transform(data[target])    \n",
        "    \n",
        "    if create_dataset == False:\n",
        "        return data\n",
        "    else:\n",
        "        groups = []\n",
        "        groups_name = []\n",
        "        data_groups = data.groupby(group_by)\n",
        "\n",
        "    for name, group in data_groups:\n",
        "        groups.append(group)\n",
        "        groups_name.append(name)\n",
        "    \n",
        "    if dataset_type == 'many_to_many':\n",
        "        return ManyToManyDataset(groups, target=target, drop=drop), data\n",
        "    elif dataset_type == \"many_to_one\":\n",
        "        return ManyToOneDataset(groups, target=target, drop=drop), data\n",
        "kennlinienfelder = data.groupby(\"source_file\")\n",
        "pred_dict = {}\n",
        "\n",
        "seeds = [42]\n",
        "for kennlinienfeld in kennlinienfelder:\n",
        "    print(f\"kennlinienfeld: {kennlinienfeld[0]}\")\n",
        "    data_name = kennlinienfeld[0]\n",
        "    print(data_name)\n",
        "    ds, data = ds, _ = load_data(kennlinienfeld[1], \n",
        "                                input_features=['PI tot V [-]' ], \n",
        "                                target='V tot V red [m³/s]',\n",
        "                                drop = ['source_file', \"Bereich u2red\"],\n",
        "                                group_by=\"Bereich u2red\",\n",
        "                                # feature_scaling=MinMaxScaler()\n",
        "                                )\n",
        "    \n",
        "    pred_dict[data_name] = {}\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"seed: {seed}\")\n",
        "        # seed_everything(seed)\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        \n",
        "        pred_dict[data_name][seed] = {}\n",
        "        pred_dict[data_name][seed]['x'] = []\n",
        "        pred_dict[data_name][seed]['y_hat'] = []\n",
        "        pred_dict[data_name][seed]['y'] = []\n",
        "        pred_dict[data_name][seed]['mape'] = []\n",
        "        pred_dict[data_name][seed]['rmse'] = []\n",
        "        \n",
        "        # Create indices for the split\n",
        "        indices = list(range(len(ds)))        \n",
        "        for i in indices:\n",
        "            test_indices = [indices[i]]\n",
        "            train_indices = [index for index in indices if index != test_indices[0]]\n",
        "            \n",
        "            train_dataset = torch.utils.data.Subset(ds, train_indices)\n",
        "            test_dataset = torch.utils.data.Subset(ds, test_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the spotpyhon Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fun_control=fun_control_init(\n",
        "    penalty_NA=200,\n",
        "    ocba_delta=1,\n",
        "    TENSORBOARD_CLEAN=False,\n",
        "    tensorboard_log=False,\n",
        "    accelerator=\"cpu\",\n",
        "    collate_fn_name=\"PadSequenceManyToMany\",\n",
        "    show_config=True,\n",
        "    verbosity=1,\n",
        "    save_experiment=True,\n",
        "    save_result=True,\n",
        "    PREFIX=\"1002\",\n",
        "    fun_evals=inf,\n",
        "    fun_repeats=2,\n",
        "    max_time=1,\n",
        "    data_full_train = train_dataset,\n",
        "    data_val=test_dataset,\n",
        "    data_test=test_dataset,\n",
        "    shuffle_train=False,\n",
        "    shuffle_val=False,    \n",
        "    core_model_name=\"light.regression.ManyToManyRNNRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    log_level=50,\n",
        "    _L_in=1,\n",
        "    _L_out=1)\n",
        "\n",
        "# set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
        "set_hyperparameter(fun_control, \"rnn_units\", [6, 12])\n",
        "set_hyperparameter(fun_control, \"fc_units\", [6, 12])\n",
        "set_hyperparameter(fun_control, \"epochs\", [6 , 8])\n",
        "set_hyperparameter(fun_control, \"batch_size\", [6,12])\n",
        "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
        "set_hyperparameter(fun_control, \"patience\", [2, 5])\n",
        "set_hyperparameter(fun_control, \"lr_mult\", [0.1, 20.0])\n",
        "\n",
        "design_control = design_control_init(init_size=20, repeats=2)\n",
        "\n",
        "surrogate_control = surrogate_control_init(log_level=50, noise=True)\n",
        "\n",
        "fun = HyperLight().fun\n",
        "\n",
        "spot_tuner = Spot(fun=fun,fun_control=fun_control, design_control=design_control, surrogate_control=surrogate_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.file import load_and_run_spot_python_experiment\n",
        "from spotpython.data.manydataset import ManyToManyDataset\n",
        "load_and_run_spot_python_experiment(filename=\"1002_exp.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 1001_user_data_run\n",
        "res = spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_res_table(spot_tuner)\n",
        "spot_tuner.plot_important_hyperparameter_contour(max_imp=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(spot_tuner.y)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spot312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
