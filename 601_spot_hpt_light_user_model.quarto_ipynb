{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Hyperparameter Tuning with PyTorch Lightning and User Models {#sec-light-user-model-601}\n"
      ],
      "id": "b313ba4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: 601_user_model_first_imports\n",
        "import numpy as np\n",
        "import os\n",
        "from math import inf\n",
        "import numpy as np\n",
        "import warnings\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "user_model_first_imports",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we will show how a user defined model can be used for the `PyTorch` Lightning hyperparameter tuning workflow with `spotpython`.\n",
        "\n",
        "## Using a User Specified Model\n",
        "\n",
        "As templates, we provide the following three files that allow the user to specify a model in the `/userModel` directory:\n",
        "\n",
        "* `my_regressor.py`, see @sec-my-regressor\n",
        "* `my_hyperdict.json`, see @sec-my-hyper-dict-json\n",
        "* `my_hyperdict.py`, see @sec-my-hyper-dict.\n",
        "\n",
        "The `my_regressor.py` file contains the model class, which is a subclass of `nn.Module`.\n",
        "The `my_hyperdict.json` file contains the hyperparameter settings as a dictionary, which are loaded via the `my_hyperdict.py` file.\n",
        "\n",
        "Note, that we have to add the path to the `userModel` directory to the `sys.path` list as shown below.\n"
      ],
      "id": "a083e275"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_imports_sys\n",
        "import sys\n",
        "sys.path.insert(0, './userModel')\n",
        "import my_regressor\n",
        "import my_hyper_dict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.init import (fun_control_init, design_control_init)\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.hyperparameters.values import set_hyperparameter\n",
        "from spotpython.spot import spot\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=\"601-user-model\",\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = Diabetes(),\n",
        "    _L_in=10,\n",
        "    _L_out=1)\n",
        "\n",
        "add_core_model_to_fun_control(fun_control=fun_control,\n",
        "                              core_model=my_regressor.MyRegressor,\n",
        "                              hyper_dict=my_hyper_dict.MyHyperDict)\n",
        "\n",
        "design_control = design_control_init(init_size=7)\n",
        "\n",
        "fun = HyperLight().fun\n",
        "\n",
        "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)"
      ],
      "id": "user_model_imports_sys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_run\n",
        "res = spot_tuner.run()\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n",
        "spot_tuner.plot_important_hyperparameter_contour(max_imp=3)"
      ],
      "id": "user_model_run",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Details\n",
        "\n",
        "### Model Setup\n",
        "\n",
        "By using `core_model_name = \"my_regressor.MyRegressor\"`, the user specified model class `MyRegressor` [[SOURCE]](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userModel/my_regressor.py) is selected.\n",
        "For this given `core_model_name`, the local hyper_dict is loaded using the `my_hyper_dict.py` file as shown below.\n",
        "\n",
        "### The `my_hyper_dict.py` File {#sec-my-hyper-dict}\n",
        "\n",
        "The `my_hyper_dict.py` file  must be placed in the `/userModel` directory.  It provides a convenience function to load the hyperparameters from user specified the `my_hyper_dict.json` file, see @sec-my-hyper-dict.\n",
        "The user does not need to modify this file, if the JSON file is stored as `my_hyper_dict.json`. \n",
        "Alternative filenames can be specified via the `filename` argument (which is default set to `\"my_hyper_dict.json\"`).\n",
        "\n",
        "### The `my_hyper_dict.json` File {#sec-my-hyper-dict-json}\n",
        "\n",
        "The `my_hyper_dict.json` file contains the hyperparameter settings as a dictionary, which are loaded via the `my_hyper_dict.py` file.\n",
        "The example below shows the content of the `my_hyper_dict.json` file.\n",
        "```json\n",
        "{\n",
        "    \"MyRegressor\": {\n",
        "        \"l1\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 3,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 3,\n",
        "            \"upper\": 8\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 4,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 4,\n",
        "            \"upper\": 9\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 4,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 1,\n",
        "            \"upper\": 4\n",
        "        },\n",
        "        \"act_fn\": {\n",
        "            \"levels\": [\n",
        "                \"Sigmoid\",\n",
        "                \"Tanh\",\n",
        "                \"ReLU\",\n",
        "                \"LeakyReLU\",\n",
        "                \"ELU\",\n",
        "                \"Swish\"\n",
        "            ],\n",
        "            \"type\": \"factor\",\n",
        "            \"default\": \"ReLU\",\n",
        "            \"transform\": \"None\",\n",
        "            \"class_name\": \"spotpython.torch.activation\",\n",
        "            \"core_model_parameter_type\": \"instance()\",\n",
        "            \"lower\": 0,\n",
        "            \"upper\": 5\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"levels\": [\n",
        "                \"Adadelta\",\n",
        "                \"Adagrad\",\n",
        "                \"Adam\",\n",
        "                \"AdamW\",\n",
        "                \"SparseAdam\",\n",
        "                \"Adamax\",\n",
        "                \"ASGD\",\n",
        "                \"NAdam\",\n",
        "                \"RAdam\",\n",
        "                \"RMSprop\",\n",
        "                \"Rprop\",\n",
        "                \"SGD\"\n",
        "            ],\n",
        "            \"type\": \"factor\",\n",
        "            \"default\": \"SGD\",\n",
        "            \"transform\": \"None\",\n",
        "            \"class_name\": \"torch.optim\",\n",
        "            \"core_model_parameter_type\": \"str\",\n",
        "            \"lower\": 0,\n",
        "            \"upper\": 11\n",
        "        },\n",
        "        \"dropout_prob\": {\n",
        "            \"type\": \"float\",\n",
        "            \"default\": 0.01,\n",
        "            \"transform\": \"None\",\n",
        "            \"lower\": 0.0,\n",
        "            \"upper\": 0.25\n",
        "        },\n",
        "        \"lr_mult\": {\n",
        "            \"type\": \"float\",\n",
        "            \"default\": 1.0,\n",
        "            \"transform\": \"None\",\n",
        "            \"lower\": 0.1,\n",
        "            \"upper\": 10.0\n",
        "        },\n",
        "        \"patience\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 2,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 2,\n",
        "            \"upper\": 6\n",
        "        },\n",
        "        \"initialization\": {\n",
        "            \"levels\": [\n",
        "                \"Default\",\n",
        "                \"Kaiming\",\n",
        "                \"Xavier\"\n",
        "            ],\n",
        "            \"type\": \"factor\",\n",
        "            \"default\": \"Default\",\n",
        "            \"transform\": \"None\",\n",
        "            \"core_model_parameter_type\": \"str\",\n",
        "            \"lower\": 0,\n",
        "            \"upper\": 2\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "### The `my_regressor.py` File {#sec-my-regressor}\n",
        "\n",
        "The `my_regressor.py` file contains [[SOURCE]](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userModel/my_regressor.py)Â the model class, which is a subclass of `nn.Module`. It must implement the following methods:\n",
        "\n",
        "* `__init__(self, **kwargs)`: The constructor of the model class. The hyperparameters are passed as keyword arguments.\n",
        "* `forward(self, x: torch.Tensor) -> torch.Tensor`: The forward pass of the model. The input `x` is passed through the model and the output is returned.\n",
        "* `training_step(self, batch, batch_idx) -> torch.Tensor`: The training step of the model. It takes a batch of data and the batch index as input and returns the loss.\n",
        "* `validation_step(self, batch, batch_idx) -> torch.Tensor`: The validation step of the model. It takes a batch of data and the batch index as input and returns the loss.\n",
        "* `test_step(self, batch, batch_idx) -> torch.Tensor`: The test step of the model. It takes a batch of data and the batch index as input and returns the loss.\n",
        "* `predict(self, x: torch.Tensor) -> torch.Tensor`: The prediction method of the model. It takes an input `x` and returns the prediction.\n",
        "* `configure_optimizers(self) -> torch.optim.Optimizer`: The method to configure the optimizer of the model. It returns the optimizer.\n",
        "\n",
        "The file `my_regressor.py` must be placed in the `/userModel` directory. The user can modify the model class to implement a custom model architecture.\n",
        "\n",
        "We will take a closer look at the methods defined in the `my_regressor.py` file in the next subsections.\n",
        "\n",
        "#### The `__init__` Method\n",
        "\n",
        "`__init__()` initializes the `MyRegressor` object. It takes the following arguments:\n",
        "\n",
        "* `l1` (int): The number of neurons in the first hidden layer.\n",
        "* `epochs` (int): The number of epochs to train the model for.\n",
        "* `batch_size` (int): The batch size to use during training.\n",
        "* `initialization` (str): The initialization method to use for the weights.\n",
        "* `act_fn` (nn.Module): The activation function to use in the hidden layers.\n",
        "* `optimizer` (str): The optimizer to use during training.\n",
        "* `dropout_prob` (float): The probability of dropping out a neuron during training.\n",
        "* `lr_mult` (float): The learning rate multiplier for the optimizer.\n",
        "* `patience` (int): The number of epochs to wait before early stopping.\n",
        "* `_L_in` (int): The number of input features. Not a hyperparameter, but needed to create the network.\n",
        "* `_L_out` (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n",
        "* `_torchmetric` (str): The metric to use for the loss function. If `None`, then \"mean_squared_error\" is used.\n",
        "\n",
        "It is implemented as follows:\n"
      ],
      "id": "981e51dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_init\n",
        "#| eval: false\n",
        "class MyRegressor(L.LightningModule):\n",
        "        def __init__(\n",
        "        self,\n",
        "        l1: int,\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        initialization: str,\n",
        "        act_fn: nn.Module,\n",
        "        optimizer: str,\n",
        "        dropout_prob: float,\n",
        "        lr_mult: float,\n",
        "        patience: int,\n",
        "        _L_in: int,\n",
        "        _L_out: int,\n",
        "        _torchmetric: str,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._L_in = _L_in\n",
        "        self._L_out = _L_out\n",
        "        if _torchmetric is None:\n",
        "            _torchmetric = \"mean_squared_error\"\n",
        "        self._torchmetric = _torchmetric\n",
        "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
        "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
        "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
        "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
        "        # set dummy input array for Tensorboard Graphs\n",
        "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
        "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
        "        if self.hparams.l1 < 4:\n",
        "            raise ValueError(\"l1 must be at least 4\")\n",
        "        hidden_sizes = self._get_hidden_sizes()\n",
        "        # Create the network based on the specified hidden sizes\n",
        "        layers = []\n",
        "        layer_sizes = [self._L_in] + hidden_sizes\n",
        "        layer_size_last = layer_sizes[0]\n",
        "        for layer_size in layer_sizes[1:]:\n",
        "            layers += [\n",
        "                nn.Linear(layer_size_last, layer_size),\n",
        "                self.hparams.act_fn,\n",
        "                nn.Dropout(self.hparams.dropout_prob),\n",
        "            ]\n",
        "            layer_size_last = layer_size\n",
        "        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
        "        # nn.Sequential summarizes a list of modules into a single module,\n",
        "        # applying them in sequence\n",
        "        self.layers = nn.Sequential(*layers)"
      ],
      "id": "user_model_init",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `_get_hidden_sizes` Method\n",
        "\n",
        "`__init__()` uses the helper method `_get_hidden_sizes()` to calculate the hidden layer sizes based on the number of neurons in the first hidden layer `l1`. The hidden layer sizes are calculated as follows:\n"
      ],
      "id": "845f5c22"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_get_hidden_sizes\n",
        "#| eval: false\n",
        "def _get_hidden_sizes(self):\n",
        "    # Calculate the hidden layer sizes based on the number of neurons in the first hidden layer\n",
        "    hidden_sizes = [self.hparams.l1]\n",
        "    while hidden_sizes[-1] > 2:\n",
        "        hidden_sizes.append(hidden_sizes[-1] // 2)\n",
        "    return hidden_sizes"
      ],
      "id": "user_model_get_hidden_sizes",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `forward` Method\n",
        "\n",
        "The `forward()` method defines the forward pass of the model. It takes an input tensor `x` and passes it through the network layers to produce an output tensor. It is implemented as follows:\n"
      ],
      "id": "b4f9e482"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_forward\n",
        "#| eval: false\n",
        "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.layers(x)"
      ],
      "id": "user_model_forward",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `_calculate_loss` Method\n",
        "\n",
        "The `_calculate_loss()` method calculates the loss based on the predicted output and the target values. It uses the specified metric to calculate the loss. \n",
        "It takes the following arguments:\n",
        "\n",
        "* `batch (tuple)`: A tuple containing a batch of input data and labels.\n",
        "\n",
        "It is implemented as follows:\n"
      ],
      "id": "7d7da96a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_calculate_loss\n",
        "#| eval: false\n",
        "def _calculate_loss(self, batch):\n",
        "    x, y = batch\n",
        "    y = y.view(len(y), 1)\n",
        "    y_hat = self(x)\n",
        "    loss = self.metric(y_hat, y)\n",
        "    return loss"
      ],
      "id": "user_model_calculate_loss",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `training_step` Method\n",
        "\n",
        "The `training_step()` method defines the training step of the model. It takes a batch of data and returns the loss. It is implemented as follows:\n"
      ],
      "id": "cb610a1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_training_step\n",
        "#| eval: false\n",
        "def training_step(self, batch: tuple) -> torch.Tensor:\n",
        "    val_loss = self._calculate_loss(batch)\n",
        "    return val_loss"
      ],
      "id": "user_model_training_step",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `validation_step` Method\n",
        "\n",
        "The `validation_step()` method defines the validation step of the model. It takes a batch of data and returns the loss. It is implemented as follows:\n"
      ],
      "id": "e8ae141d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_validation_step\n",
        "#| eval: false\n",
        "def validation_step(self, batch: tuple) -> torch.Tensor:\n",
        "    val_loss = self._calculate_loss(batch)\n",
        "    return val_loss"
      ],
      "id": "user_model_validation_step",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `test_step` Method\n",
        "\n",
        "The `test_step()` method defines the test step of the model. It takes a batch of data and returns the loss. It is implemented as follows:\n"
      ],
      "id": "919d3301"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_test_step\n",
        "#| eval: false\n",
        "def test_step(self, batch: tuple) -> torch.Tensor:\n",
        "    val_loss = self._calculate_loss(batch)\n",
        "    return val_loss"
      ],
      "id": "user_model_test_step",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `predict` Method\n",
        "\n",
        "The `predict()` method defines the prediction method of the model. It takes an input tensor `x` and returns \n",
        "a tuple with the input tensor `x`, the target tensor `y`, and the predicted tensor `y_hat`.\n",
        "\n",
        " It is implemented as follows:\n"
      ],
      "id": "87e93204"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_predict\n",
        "#| eval: false\n",
        "def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x, y = batch\n",
        "    yhat = self(x)\n",
        "    y = y.view(len(y), 1)\n",
        "    yhat = yhat.view(len(yhat), 1)\n",
        "    return (x, y, yhat)"
      ],
      "id": "user_model_predict",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `configure_optimizers` Method\n",
        "\n",
        "The `configure_optimizers()` method defines the optimizer to use during training.\n",
        "It uses the `optimizer_handler` from `spotpython.hyperparameter.optimizer` to create the optimizer based on the specified optimizer name, parameters, and learning rate multiplier.\n",
        "It is implemented as follows:\n"
      ],
      "id": "fae60014"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_model_configure_optimizers\n",
        "#| eval: false\n",
        "def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "    optimizer = optimizer_handler(\n",
        "        optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n",
        "    )\n",
        "    return optimizer"
      ],
      "id": "user_model_configure_optimizers",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note, the default Lightning way is to define an optimizer as\n",
        "`optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n",
        "`spotpython` uses an optimizer handler to create the optimizer, which adapts the learning rate according to the `lr_mult` hyperparameter as\n",
        "well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/hyperparameters/optimizer.py) for details.\n",
        "\n",
        "## Connection with the LightDataModule\n",
        "\n",
        "The steps described in @sec-my-regressor are connected to the `LightDataModule` class [[DOC]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/data/lightdatamodule/).\n",
        "This class is used to create the data loaders for the training, validation, and test sets.\n",
        "The `LightDataModule` class is part of the `spotpython` package and class provides the following methods [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/data/lightdatamodule.py):\n",
        "\n",
        "* `prepare_data()`: This method is used to prepare the data set.\n",
        "* `setup()`: This method is used to create the data loaders for the training, validation, and test sets.\n",
        "* `train_dataloader()`: This method is used to return the data loader for the training set.\n",
        "* `val_dataloader()`: This method is used to return the data loader for the validation set.\n",
        "* `test_dataloader()`: This method is used to return the data loader for the test set.\n",
        "* `predict_dataloader()`: This method is used to return the data loader for the prediction set.\n",
        "\n",
        "### The `prepare_data()` Method\n",
        "\n",
        "The `prepare_data()` method is used to prepare the data set.\n",
        "This method is called only once and on a single process.\n",
        "It can be used to download the data set. In our case, the data set is already available, so this method uses a simple `pass` statement.\n",
        "\n",
        "### The `setup()` Method\n",
        "\n",
        "The `stage` is used to define the data set to be returned. It \n",
        "can be `None`, `fit`, `test`, or `predict`.\n",
        "If `stage` is `None`, the method returns the training (`fit`),\n",
        "testing (`test`), and prediction (`predict`) data sets.\n",
        "\n",
        "The `setup` methods splits the data based on the `stage` setting for use in training, validation, and testing.\n",
        "It uses `torch.utils.data.random_split()` to split the data.\n",
        "\n",
        "Splitting is based on the `test_size` and `test_seed`. \n",
        "The `test_size` can be a float or an int.\n",
        "\n",
        "First, the data set sizes are determined as described in @sec-determine-sizes-601.\n",
        "Then, the data sets are split based on the `stage` setting.\n",
        "`spotpython`'s `LightDataModule` class uses the following sizes:\n",
        "\n",
        "* `full_train_size`: The size of the full training data set. This data set is splitted into the final training data set and a validation data set.\n",
        "* `val_size`: The size of the validation data set. The validation data set is used to validate the model during training.\n",
        "* `train_size`: The size of the training data set. The training data set is used to train the model.\n",
        "* `test_size`: The size of the test data set. The test data set is used to evaluate the model after training. It is not used during training (\"hyperparameter tuning\"). Only after everything is finished, the model is evaluated on the test data set.\n",
        "\n",
        "#### Determine the Sizes of the Data Sets {#sec-determine-sizes-601}\n"
      ],
      "id": "0ff82b7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_data_module_setup\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "data_full = Diabetes()\n",
        "test_size = fun_control[\"test_size\"]\n",
        "test_seed=fun_control[\"test_seed\"]\n",
        "# if test_size is float, then train_size is 1 - test_size\n",
        "if isinstance(test_size, float):\n",
        "    full_train_size = round(1.0 - test_size, 2)\n",
        "    val_size = round(full_train_size * test_size, 2)\n",
        "    train_size = round(full_train_size - val_size, 2)\n",
        "else:\n",
        "    # if test_size is int, then train_size is len(data_full) - test_size\n",
        "    full_train_size = len(data_full) - test_size\n",
        "    val_size = int(full_train_size * test_size / len(data_full))\n",
        "    train_size = full_train_size - val_size\n",
        "\n",
        "print(f\"LightDataModule setup(): full_train_size: {full_train_size}\")\n",
        "print(f\"LightDataModule setup(): val_size: {val_size}\")\n",
        "print(f\"LightDataModule setup(): train_size: {train_size}\")\n",
        "print(f\"LightDataModule setup(): test_size: {test_size}\")"
      ],
      "id": "user_data_module_setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The \"setup\" Method: Stage \"fit\" {#sec-stage-fit-601}\n",
        "\n",
        "Here, `train_size` and `val_size` are used to split the data into training and validation sets.\n"
      ],
      "id": "fa2fec73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_data_module_setup_fit\n",
        "stage = \"fit\"\n",
        "scaler = None\n",
        "# Assign train/val datasets for use in dataloaders\n",
        "if stage == \"fit\" or stage is None:\n",
        "    print(f\"train_size: {train_size}, val_size: {val_size} used for train & val data.\")\n",
        "    generator_fit = torch.Generator().manual_seed(test_seed)\n",
        "    data_train, data_val, _ = random_split(\n",
        "        data_full, [train_size, val_size, test_size], generator=generator_fit\n",
        "    )\n",
        "    if scaler is not None:\n",
        "        # Fit the scaler on training data and transform both train and val data\n",
        "        scaler_train_data = torch.stack([data_train[i][0] for i in range(len(data_train))]).squeeze(1)\n",
        "        # train_val_data = data_train[:,0]\n",
        "        print(scaler_train_data.shape)\n",
        "        scaler.fit(scaler_train_data)\n",
        "        data_train = [(scaler.transform(data), target) for data, target in data_train]\n",
        "        data_tensors_train = [data.clone().detach() for data, target in data_train]\n",
        "        target_tensors_train = [target.clone().detach() for data, target in data_train]\n",
        "        data_train = TensorDataset(\n",
        "            torch.stack(data_tensors_train).squeeze(1), torch.stack(target_tensors_train)\n",
        "        )\n",
        "        # print(data_train)\n",
        "        data_val = [(scaler.transform(data), target) for data, target in data_val]\n",
        "        data_tensors_val = [data.clone().detach() for data, target in data_val]\n",
        "        target_tensors_val = [target.clone().detach() for data, target in data_val]\n",
        "        data_val = TensorDataset(torch.stack(data_tensors_val).squeeze(1), torch.stack(target_tensors_val))"
      ],
      "id": "user_data_module_setup_fit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `data_train` and `data_val` data sets are further used to create the training and validation data loaders as \n",
        "described in @sec-train-dataloader-601 and @sec-val-dataloader-601, respectively.\n",
        "\n",
        "#### The \"setup\" Method: Stage \"test\" {#sec-stage-test-601}\n",
        "\n",
        "Here, the test data set, which is based on the `test_size`, is created.\n"
      ],
      "id": "29a670e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_data_module_setup_test\n",
        "stage = \"test\"\n",
        "# Assign test dataset for use in dataloader(s)\n",
        "if stage == \"test\" or stage is None:\n",
        "    print(f\"test_size: {test_size} used for test dataset.\")\n",
        "    # get test data set as test_abs percent of the full dataset\n",
        "    generator_test = torch.Generator().manual_seed(test_seed)\n",
        "    data_test, _ = random_split(data_full, [test_size, full_train_size], generator=generator_test)\n",
        "    if scaler is not None:\n",
        "        data_test = [(scaler.transform(data), target) for data, target in data_test]\n",
        "        data_tensors_test = [data.clone().detach() for data, target in data_test]\n",
        "        target_tensors_test = [target.clone().detach() for data, target in data_test]\n",
        "        data_test = TensorDataset(\n",
        "            torch.stack(data_tensors_test).squeeze(1), torch.stack(target_tensors_test)\n",
        "        )\n",
        "print(f\"LightDataModule setup(): Test set size: {len(data_test)}\")\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ],
      "id": "user_data_module_setup_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The \"setup\" Method: Stage \"predict\" {#sec-stage-predict-601}\n",
        "\n",
        "Prediction and testing use the same data set.\n",
        "The prediction data set is created based on the `test_size`.\n"
      ],
      "id": "20e9fd9f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_user_data_module_setup_predict\n",
        "stage = \"predict\"\n",
        "if stage == \"predict\" or stage is None:\n",
        "    print(f\"test_size: {test_size} used for predict dataset.\")\n",
        "    # get test data set as test_abs percent of the full dataset\n",
        "    generator_predict = torch.Generator().manual_seed(test_seed)\n",
        "    data_predict, _ = random_split(\n",
        "        data_full, [test_size, full_train_size], generator=generator_predict\n",
        "    )\n",
        "    if scaler is not None:\n",
        "        data_predict = [(scaler.transform(data), target) for data, target in data_predict]\n",
        "        data_tensors_predict = [data.clone().detach() for data, target in data_predict]\n",
        "        target_tensors_predict = [target.clone().detach() for data, target in data_predict]\n",
        "        data_predict = TensorDataset(\n",
        "            torch.stack(data_tensors_predict).squeeze(1), torch.stack(target_tensors_predict)\n",
        "        )\n",
        "print(f\"LightDataModule setup(): Predict set size: {len(data_predict)}\")\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(data_predict, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ],
      "id": "user_data_module_setup_predict",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The `train_dataloader()` Method {#sec-train-dataloader-601}\n",
        "\n",
        "The method ``train_dataloader` returns the training dataloader, i.e., a Pytorch DataLoader instance using the training dataset.\n",
        "It simply returns a DataLoader with the `data_train` set that was created in the `setup()` method as described in @sec-stage-fit-601.\n"
      ],
      "id": "9a460aa1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def train_dataloader(self) -> DataLoader:\n",
        "    return DataLoader(data_train, batch_size=batch_size, num_workers=num_workers)"
      ],
      "id": "6029c853",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "#### Using the `train_dataloader()` Method\n",
        "\n",
        "The `train_dataloader()` method can be used as follows:\n"
      ],
      "id": "031e20f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "dataset = Diabetes(target_type=torch.float)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\n",
        "data_module.setup()\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "dl = data_module.train_dataloader()\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dl:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ],
      "id": "11add04c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### The `val_dataloader()` Method {#sec-val-dataloader-601}\n",
        "\n",
        "Returns the validation dataloader, i.e., a Pytorch DataLoader instance using the validation dataset.\n",
        "It simply returns a DataLoader with the `data_val` set that was created in the `setup()` method as desccribed in @sec-stage-fit-601.\n"
      ],
      "id": "768e2680"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def val_dataloader(self) -> DataLoader:\n",
        "    return DataLoader(data_val, batch_size=batch_size, num_workers=num_workers)"
      ],
      "id": "9af15f7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "#### Using the `val_dataloader()` Method\n",
        "\n",
        "The `val_dataloader()` method can be used as follows:\n"
      ],
      "id": "e8ae08b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "dataset = Diabetes(target_type=torch.float)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\n",
        "data_module.setup()\n",
        "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "dl = data_module.val_dataloader()\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dl:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ],
      "id": "045f63bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: \n",
        "\n",
        "\n",
        "### The `test_dataloader()` Method\n",
        "\n",
        "Returns the test dataloader, i.e., a Pytorch DataLoader instance using the test dataset.\n",
        "It simply returns a DataLoader with the `data_test` set that was created in the `setup()` method as described in @sec-stage-test-30.\n"
      ],
      "id": "aceb7f6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def test_dataloader(self) -> DataLoader:\n",
        "    return DataLoader(data_test, batch_size=batch_size, num_workers=num_workers)"
      ],
      "id": "c5ca26cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "#### Using the `test_dataloader()` Method\n",
        "\n",
        "The `test_dataloader()` method can be used as follows:\n"
      ],
      "id": "5ecc8e77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "dataset = Diabetes(target_type=torch.float)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\n",
        "data_module.setup()\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n",
        "dl = data_module.test_dataloader()\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dl:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ],
      "id": "288e256f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: \n",
        "\n",
        "### The `predict_dataloader()` Method\n",
        "\n",
        "Returns the prediction dataloader, i.e., a Pytorch DataLoader instance using the prediction dataset.\n",
        "It simply returns a DataLoader with the `data_predict` set that was created in the `setup()` method as described in @sec-stage-predict-30.\n",
        "\n",
        "::: {.callout-warning}\n",
        "The `batch_size` is set to the length of the `data_predict` set.\n",
        ":::"
      ],
      "id": "27868248"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def predict_dataloader(self) -> DataLoader:\n",
        "    return DataLoader(data_predict, batch_size=len(data_predict), num_workers=num_workers)"
      ],
      "id": "669a2332",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "#### Using the `predict_dataloader()` Method\n",
        "\n",
        "The `predict_dataloader()` method can be used as follows:\n"
      ],
      "id": "09edbb9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "dataset = Diabetes(target_type=torch.float)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\n",
        "data_module.setup()\n",
        "print(f\"Test set size: {len(data_module.data_predict)}\")\n",
        "dl = data_module.predict_dataloader()\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dl:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ],
      "id": "5f06dd7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Using the `LightDataModule` in the `train_model()` Method\n",
        "\n",
        "The methods discussed so far are used in `spotpython`'s  `train_model()` method [[DOC]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/light/trainmodel/) to train the model.\n",
        "It is implemented as follows [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/light/trainmodel.py).\n",
        "\n",
        "First, a `LightDataModule` object is created and the `setup()` method is called."
      ],
      "id": "f3dc52b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "dm = LightDataModule(\n",
        "    dataset=fun_control[\"data_set\"],\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    num_workers=fun_control[\"num_workers\"],\n",
        "    test_size=fun_control[\"test_size\"],\n",
        "    test_seed=fun_control[\"test_seed\"],\n",
        ")\n",
        "dm.setup()"
      ],
      "id": "b1a55558",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, the `Trainer` is initialized."
      ],
      "id": "cf8da3c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# Init trainer\n",
        "trainer = L.Trainer(\n",
        "    default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n",
        "    max_epochs=model.hparams.epochs,\n",
        "    accelerator=fun_control[\"accelerator\"],\n",
        "    devices=fun_control[\"devices\"],\n",
        "    logger=TensorBoardLogger(\n",
        "        save_dir=fun_control[\"TENSORBOARD_PATH\"],\n",
        "        version=config_id,\n",
        "        default_hp_metric=True,\n",
        "        log_graph=fun_control[\"log_graph\"],\n",
        "    ),\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n",
        "    ],\n",
        "    enable_progress_bar=enable_progress_bar,\n",
        ")"
      ],
      "id": "510d770c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, the `fit()` method is called to train the model.\n"
      ],
      "id": "0fbff08a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# Pass the datamodule as arg to trainer.fit to override model hooks :)\n",
        "trainer.fit(model=model, datamodule=dm)"
      ],
      "id": "3740d7b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the `validate()` method is called to validate the model.\n",
        "The `validate()` method returns the validation loss.\n"
      ],
      "id": "17530e33"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# Test best model on validation and test set\n",
        "result = trainer.validate(model=model, datamodule=dm)\n",
        "# unlist the result (from a list of one dict)\n",
        "result = result[0]\n",
        "return result[\"val_loss\"]"
      ],
      "id": "e6b89f34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Last Connection: The `HyperLight` Class\n",
        "\n",
        "The method `train_model()` is part of the `HyperLight` class [[DOC]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/light/trainmodel/). It is called from `spotpython` as an objective function to train the model and return the validation loss.\n",
        "\n",
        "The `HyperLight` class is implemented as follows [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/fun/hyperlight.py).\n"
      ],
      "id": "a5dff9f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| label: 601_user_hyperlight\n",
        "\n",
        "class HyperLight:\n",
        "    def fun(self, X: np.ndarray, fun_control: dict = None) -> np.ndarray:\n",
        "        z_res = np.array([], dtype=float)\n",
        "        self.check_X_shape(X=X, fun_control=fun_control)\n",
        "        var_dict = assign_values(X, get_var_name(fun_control))\n",
        "        for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
        "            df_eval = train_model(config, fun_control)\n",
        "            z_val = fun_control[\"weights\"] * df_eval\n",
        "            z_res = np.append(z_res, z_val)\n",
        "        return z_res"
      ],
      "id": "user_hyperlight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Information \n",
        "\n",
        "### Preprocessing {#sec-preprocessing-601}\n",
        "\n",
        "Preprocessing is handled by `Lightning` and `PyTorch`. It is described in the [LIGHTNINGDATAMODULE](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) documentation. Here you can find information about the `transforms` methods."
      ],
      "id": "76dd9f66"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}