{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Neural ODE Example\n",
        "\n",
        "\n",
        "## Implementation of a Neural ODE\n",
        "\n",
        "The following example is based on the \"UvA Deep Learning Tutorials\" [@lipp22a].\n",
        "\n",
        "::: {#exm-uva-neural-ode}\n",
        "\n",
        "### Example: Neural ODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "%matplotlib inline\n",
        "import time\n",
        "import logging\n",
        "import statistics\n",
        "from typing import Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "try:\n",
        "    import torchdiffeq\n",
        "except ModuleNotFoundError:\n",
        "    !pip install --quiet torchdiffeq\n",
        "    import torchdiffeq\n",
        "\n",
        "try:\n",
        "    import rich\n",
        "except ModuleNotFoundError:\n",
        "    !pip install --quiet rich\n",
        "    import rich\n",
        "\n",
        "try:\n",
        "    # import pytorch_lightning as pl\n",
        "    import lightning as pl\n",
        "except ModuleNotFoundError:\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    # import pytorch_lightning as pl\n",
        "    import lightning as pl\n",
        "from torchmetrics.classification import Accuracy\n",
        "\n",
        "pl.seed_everything(42)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "from torchmetrics.functional import accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we define the core of our Neural ODE model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "class _ODEFunc(nn.Module):\n",
        "    def __init__(self, module, autonomous=True):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "        self.autonomous = autonomous\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        if not self.autonomous:\n",
        "            x = torch.cat([torch.ones_like(x[:, [0]]) * t, x], 1)\n",
        "        return self.module(x)\n",
        "\n",
        "\n",
        "class ODEBlock(nn.Module):\n",
        "    def __init__(self, odefunc: nn.Module, solver: str = 'dopri5',\n",
        "                 rtol: float = 1e-4, atol: float = 1e-4, adjoint: bool = True,\n",
        "                 autonomous: bool = True):\n",
        "        super().__init__()\n",
        "        self.odefunc = _ODEFunc(odefunc, autonomous=autonomous)\n",
        "        self.rtol = rtol\n",
        "        self.atol = atol\n",
        "        self.solver = solver\n",
        "        self.use_adjoint = adjoint\n",
        "        self.integration_time = torch.tensor([0, 1], dtype=torch.float32)  \n",
        "    \n",
        "    @property\n",
        "    def ode_method(self):\n",
        "        return torchdiffeq.odeint_adjoint if self.use_adjoint else torchdiffeq.odeint\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adjoint: bool = True, integration_time=None):\n",
        "        integration_time = self.integration_time if integration_time is None else integration_time\n",
        "        integration_time = integration_time.to(x.device)\n",
        "        ode_method =  torchdiffeq.odeint_adjoint if adjoint else torchdiffeq.odeint\n",
        "        out = ode_method(\n",
        "            self.odefunc, x, integration_time, rtol=self.rtol,\n",
        "            atol=self.atol, method=self.solver)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will wrap everything together in a LightningModule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "class Learner(pl.LightningModule):\n",
        "    def __init__(self, model:nn.Module, t_span:torch.Tensor, learning_rate:float=5e-3):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.t_span = t_span\n",
        "        self.learning_rate = learning_rate\n",
        "        # self.accuracy = Accuracy(num_classes=2)\n",
        "        self.accuracy = accuracy\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def inference(self, x, time_span):\n",
        "        return self.model(x, adjoint=False, integration_time=time_span)\n",
        "\n",
        "    def inference_no_projection(self, x, time_span):\n",
        "        return self.model.forward_no_projection(x, adjoint=False, integration_time=time_span)\n",
        "   \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch      \n",
        "        y_pred = self(x)\n",
        "        y_pred = y_pred[-1]  # select last point of solution trajectory\n",
        "        loss = nn.CrossEntropyLoss()(y_pred, y)\n",
        "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch      \n",
        "        y_pred = self(x)\n",
        "        y_pred = y_pred[-1]  # select last point of solution trajectory\n",
        "        loss = nn.CrossEntropyLoss()(y_pred, y)\n",
        "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
        "        acc = self.accuracy(y_pred.softmax(dim=-1), y, num_classes=2, task=\"MULTICLASS\")\n",
        "        self.log('val_accuracy', acc, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch      \n",
        "        y_pred = self(x)\n",
        "        y_pred = y_pred[-1]  # select last point of solution trajectory\n",
        "        loss = nn.CrossEntropyLoss()(y_pred, y)\n",
        "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
        "        acc = self.accuracy(y_pred.softmax(dim=-1), y, num_classes=2, task=\"MULTICLASS\")\n",
        "        self.log('test_accuracy', acc, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be working will Half Moons Dataset, a non-linearly separable, binary classification dataset.\n",
        "The code is based on the excellent TorchDyn tutorials (https://github.com/DiffEqML/torchdyn), as well as the original TorchDiffEq examples (https://github.com/rtqichen/torchdiffeq). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "class MoonsDataset(Dataset):\n",
        "    \"\"\"Half Moons Classification Dataset\n",
        "    \n",
        "    Adapted from https://github.com/DiffEqML/torchdyn\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples=100, noise_std=1e-4):\n",
        "        self.num_samples = num_samples\n",
        "        self.noise_std = noise_std\n",
        "        self.X, self.y = self.generate_moons(num_samples, noise_std)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_moons(num_samples=100, noise_std=1e-4):\n",
        "        \"\"\"Creates a *moons* dataset of `num_samples` data points.\n",
        "        :param num_samples: number of data points in the generated dataset\n",
        "        :type num_samples: int\n",
        "        :param noise_std: standard deviation of noise magnitude added to each data point\n",
        "        :type noise_std: float\n",
        "        \"\"\"\n",
        "        num_samples_out = num_samples // 2\n",
        "        num_samples_in = num_samples - num_samples_out\n",
        "        theta_out = np.linspace(0, np.pi, num_samples_out)\n",
        "        theta_in = np.linspace(0, np.pi, num_samples_in)\n",
        "        outer_circ_x = np.cos(theta_out)\n",
        "        outer_circ_y = np.sin(theta_out)\n",
        "        inner_circ_x = 1 - np.cos(theta_in)\n",
        "        inner_circ_y = 1 - np.sin(theta_in) - 0.5\n",
        "\n",
        "        X = np.vstack([np.append(outer_circ_x, inner_circ_x),\n",
        "                       np.append(outer_circ_y, inner_circ_y)]).T\n",
        "        y = np.hstack([np.zeros(num_samples_out), np.ones(num_samples_in)])\n",
        "\n",
        "        if noise_std is not None:\n",
        "            X += noise_std * np.random.rand(num_samples, 2)\n",
        "\n",
        "        X = torch.Tensor(X)\n",
        "        y = torch.LongTensor(y)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "def plot_binary_classification_dataset(X, y, title=None):\n",
        "    CLASS_COLORS = ['coral', 'darkviolet']\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.scatter(X[:, 0], X[:, 1], color=[CLASS_COLORS[yi.int()] for yi in y], alpha=0.6)\n",
        "    ax.set_aspect('equal')\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    return fig, ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a sample dataset and visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: \"Half-Moons-Dataset\"\n",
        "sample_dataset = MoonsDataset(num_samples=400, noise_std=1e-1)\n",
        "fig, ax = plot_binary_classification_dataset(sample_dataset.X, sample_dataset.y, title='Half Moons Dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create the train, validation, and test sets, with their corresponding data loaders. We will create a single big dataset and randomly split it in train, val, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "def split_dataset(dataset_size:int, split_percentages:List[float]) -> List[int]:\n",
        "    split_sizes = [int(pi * dataset_size) for pi in split_percentages]\n",
        "    split_sizes[0] += dataset_size - sum(split_sizes)\n",
        "    return split_sizes\n",
        "\n",
        "\n",
        "class ToyDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, dataset_size:int, split_percentages:Optional[float]=None):\n",
        "        super().__init__()\n",
        "        self.dataset_size = dataset_size\n",
        "        if split_percentages is None:\n",
        "            split_percentages = [0.8, 0.1, 0.1]\n",
        "        self.split_sizes = split_dataset(self.dataset_size, split_percentages)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=len(self.train_set), shuffle=True)\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = torch.utils.data.DataLoader(self.val_set, batch_size=len(self.val_set), shuffle=False)\n",
        "        return val_loader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=len(self.test_set), shuffle=False)\n",
        "        return test_loader\n",
        "\n",
        "\n",
        "class HalfMoonsDataModule(ToyDataModule):\n",
        "    def __init__(self, dataset_size:int, split_percentages:Optional[float]=None):\n",
        "        super().__init__(dataset_size, split_percentages=split_percentages)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        dataset = MoonsDataset(num_samples=self.dataset_size, noise_std=1e-1)\n",
        "        self.train_set, self.val_set, self.test_set = torch.utils.data.random_split(dataset, self.split_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a Neural ODE and train it. \n",
        "We will use a simple 2-layer MLP with a *tanh* activation and 64 hidden dimensions. We will train the model using the adjoint method for backpropagation.\n",
        "\n",
        "A quick note on the architectural choices for our model. The **Picard-Lindel&ouml;f theorem** (Coddington and Levinson, 1955) states that the solution to an initial value problem **exists and is\n",
        "unique** if the differential equation is _uniformly Lipschitz continuous_ in $\\mathbf{z}$ and _continuous_ in $t$. It turns out that this\n",
        "theorem holds for our model if the neural network has finite weights and uses Lipschitz nonlinearities, such as tanh or relu. However, not all tools are our deep learning arsenal is c. For example, as shown in [**The Lipschitz Constant of Self-Attention**](https://arxiv.org/abs/2006.04710) by Hyunjik Kim et al., standard self-attention is ___not___ Lipschitz. The authors propose alternative forms of self-attention that are Lipschitz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: \"Neural-ODE-Training\"\n",
        "import torch\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint, RichProgressBar\n",
        "\n",
        "\n",
        "adjoint = True\n",
        "data_module = HalfMoonsDataModule(1000)\n",
        "t_span = torch.linspace(0, 1, 2)\n",
        "f = nn.Sequential(\n",
        "    nn.Linear(2, 64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 2))\n",
        "model = ODEBlock(f, adjoint=adjoint)\n",
        "learner = Learner(model, t_span)\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=200,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    devices=1,\n",
        "    callbacks=[\n",
        "        ModelCheckpoint(mode=\"max\", monitor=\"val_accuracy\"),\n",
        "        RichProgressBar(),\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        ")\n",
        "trainer.fit(learner, datamodule=data_module)\n",
        "val_result = trainer.validate(learner, datamodule=data_module, verbose=True)\n",
        "test_result = trainer.test(learner, datamodule=data_module, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " It seems that in less that 200 epochs we have achieved perfect validation accuracy. Let's now use the trained model to run inference and visualize the trajectories using a dense time span of 100 timesteps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "@torch.no_grad()\n",
        "def run_inference(learner, data_loader, time_span):\n",
        "    learner.to(device)\n",
        "    trajectories = []\n",
        "    classes = []\n",
        "    time_span = torch.from_numpy(time_span).to(device)\n",
        "    for data, target in data_loader:\n",
        "        data = data.to(device)\n",
        "        traj = learner.inference(data, time_span).cpu().numpy()\n",
        "        trajectories.append(traj)\n",
        "        classes.extend(target.numpy())\n",
        "    trajectories = np.concatenate(trajectories, 1)\n",
        "    return trajectories, classes\n",
        "\n",
        "time_span = np.linspace(0.0, 1.0, 100)\n",
        "trajectories, classes = run_inference(learner, data_module.train_dataloader(), time_span)\n",
        "\n",
        "colors = ['coral', 'darkviolet']\n",
        "class_colors = [colors[ci] for ci in classes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now define a few functions to visualize the learned trajectories, the state-space, and the learned vector field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "\n",
        "def plot_trajectories(time_span, trajectories, class_colors):\n",
        "    fig = plt.figure(figsize=(12,6))\n",
        "    ax0 = fig.add_subplot(121)\n",
        "    ax1 = fig.add_subplot(122)\n",
        "    for i in range(trajectories.shape[1]):\n",
        "        ax0.plot(time_span, trajectories[:, i, 0], color=class_colors[i], alpha=0.1)\n",
        "        ax1.plot(time_span, trajectories[:, i, 1], color=class_colors[i], alpha=0.1)\n",
        "\n",
        "    ax0.set_xlabel(r\"$t$ [Depth]\")\n",
        "    ax0.set_ylabel(r\"$\\mathbf{z}_0(t)$\")\n",
        "    ax0.set_title(\"Dimension 0\")\n",
        "    ax1.set_xlabel(r\"$t$ [Depth]\")\n",
        "    ax1.set_ylabel(r\"$\\mathbf{z}_1(t)$\")\n",
        "    ax1.set_title(\"Dimension 1\")\n",
        "\n",
        "\n",
        "def plot_trajectories_3d(time_span, trajectories, class_colors):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    for i in range(trajectories.shape[1]):\n",
        "        ax.plot(trajectories[:, i, 0], trajectories[:, i, 1], time_span,\n",
        "                color=class_colors[i], alpha=0.1)\n",
        "\n",
        "    ax.set_title('3D Trajectories')\n",
        "    ax.set_xlabel(r\"$\\mathbf{z}_0(t)$\")\n",
        "    ax.set_ylabel(r\"$\\mathbf{z}_1(t)$\")\n",
        "    ax.set_zlabel(r\"$t$\")\n",
        "\n",
        "\n",
        "def plot_trajectories_animation(time_span, trajectories, colors, classes, lim=10.0):\n",
        "    def animate_frame(t):\n",
        "        ax.cla()\n",
        "        ax.set_xlim(-lim, lim)\n",
        "        ax.set_ylim(-lim, lim)\n",
        "        ax.set_title('Trajectories')\n",
        "        ax.set_xlabel(r\"$\\mathbf{z}_0(t)$\")\n",
        "        ax.set_ylabel(r\"$\\mathbf{z}_1(t)$\")\n",
        "\n",
        "        zero_classes = np.array(classes) == 0\n",
        "        one_classes = np.array(classes) == 1\n",
        "\n",
        "        scatter_zero = ax.plot(\n",
        "            trajectories[t, zero_classes, 0], trajectories[t, zero_classes, 1],\n",
        "            'o', color=colors[0], alpha=0.2+0.8*t/len(time_span))\n",
        "        scatter_one = ax.plot(\n",
        "            trajectories[t, one_classes, 0], trajectories[t, one_classes, 1],\n",
        "            'o', color=colors[1], alpha=0.2+0.8*t/len(time_span))\n",
        "        return scatter_zero, scatter_one\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    anim = FuncAnimation(fig, animate_frame, frames=len(time_span))\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "\n",
        "def plot_augmented_trajectories_animation(time_span, trajectories, colors, classes, lim=10.0):\n",
        "    def animate_frame(t):\n",
        "        ax.cla()\n",
        "        ax.set_xlim(-lim, lim)\n",
        "        ax.set_ylim(-lim, lim)\n",
        "        ax.set_zlim(-lim, lim)\n",
        "        ax.set_title('Trajectories')\n",
        "        ax.set_xlabel(r\"$\\mathbf{z}_0(t)$\")\n",
        "        ax.set_ylabel(r\"$\\mathbf{z}_1(t)$\")\n",
        "        ax.set_zlabel(r\"$\\mathbf{z}_2(t)$\")\n",
        "\n",
        "        zero_classes = np.array(classes) == 0\n",
        "        one_classes = np.array(classes) == 1\n",
        "\n",
        "        scatter_zero = ax.plot(\n",
        "            trajectories[t, zero_classes, 0], trajectories[t, zero_classes, 1], trajectories[t, zero_classes, 2],\n",
        "            'o', color=colors[0], alpha=0.2+0.8*t/len(time_span))\n",
        "        scatter_one = ax.plot(\n",
        "            trajectories[t, one_classes, 0], trajectories[t, one_classes, 1], trajectories[t, one_classes, 2],\n",
        "            'o', color=colors[1], alpha=0.2+0.8*t/len(time_span))\n",
        "        return scatter_zero, scatter_one\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    anim = FuncAnimation(fig, animate_frame, frames=len(time_span))\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "\n",
        "def plot_state_space(trajectories, class_colors, ax=None):\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(111)\n",
        "\n",
        "    for i in range(trajectories.shape[1]):\n",
        "        ax.plot(trajectories[:, i, 0], trajectories[:, i, 1],\n",
        "                color=class_colors[i], alpha=0.1)\n",
        "\n",
        "    ax.set_title('State-Space Diagram')\n",
        "    ax.set_xlabel(r\"$x$\")\n",
        "    ax.set_ylabel(r\"$y$\")\n",
        "\n",
        "\n",
        "def plot_augmented_state_space(trajectories, class_colors, ax=None):\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    for i in range(trajectories.shape[1]):\n",
        "        ax.plot(trajectories[:, i, 0], trajectories[:, i, 1], trajectories[:, i, 2],\n",
        "                color=class_colors[i], alpha=0.1)\n",
        "\n",
        "    ax.set_title('State-Space Diagram')\n",
        "    ax.set_xlabel(r\"$x$\")\n",
        "    ax.set_ylabel(r\"$y$\")\n",
        "    ax.set_zlabel(r\"$z$\")\n",
        "\n",
        "\n",
        "def plot_static_vector_field(model, trajectory, N=50, device='cpu', ax=None):\n",
        "    X, Y = np.mgrid[trajectory[..., 0].min():trajectory[..., 0].max():N*1j,\n",
        "                    trajectory[..., 1].min():trajectory[..., 1].max():N*1j]\n",
        "    X = X.T\n",
        "    Y = Y.T\n",
        "    P = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = torch.Tensor(P).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vector_field = model.odefunc(0.0, P).cpu()\n",
        "    vector_norm = vector_field.norm(dim=1).view(N, N).numpy()\n",
        "\n",
        "    vector_field = vector_field.view(N, N, 2).numpy()\n",
        "\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(111)\n",
        "    ax.contourf(X, Y, vector_norm, cmap='RdYlBu')\n",
        "    ax.streamplot(X, Y, vector_field[:, :, 0], vector_field[:, :, 1], color='k')\n",
        "\n",
        "    ax.set_xlim([X.min(), X.max()])\n",
        "    ax.set_ylim([Y.min(), Y.max()])\n",
        "    ax.set_xlabel(r\"$x$\")\n",
        "    ax.set_ylabel(r\"$y$\")\n",
        "    ax.set_title(\"Learned Vector Field\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we visualize the trajectories, let's plot the (training) data once again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: plot_half-moons\n",
        "fig, ax = plot_binary_classification_dataset(*data_module.train_set[:], title='Half Moons Dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we visualize the evolution for each of the 2 inputs dimensions as a function of time (depth):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: plot-trajectories\n",
        "plot_trajectories(time_span, trajectories, class_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the same evolution combined in a single plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: plot-trajectories-3d\n",
        "plot_trajectories_3d(time_span, trajectories, class_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 3D plot can be somewhat complicated to decipher. Thus, we also plot an animated version of the evolution. Each timestep of the animation is a slice on the temporal axis of the figure above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: animate-trajectories\n",
        "anim = plot_trajectories_animation(time_span, trajectories, colors, classes, lim=8.0)\n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can visualize the state-space diagram and the learned vector field:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| label: visualize-state-space-vector-field\n",
        "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
        "plot_state_space(trajectories, class_colors, ax=ax[0])\n",
        "plot_static_vector_field(model, trajectories, ax=ax[1], device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot313/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}