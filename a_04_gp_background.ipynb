{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "# Gaussian Processes---Some Background Information\n",
        "\n",
        "The concept of GP (Gaussian Process) regression can be understood as a simple extension of linear modeling. It is worth noting that this approach goes by various names and acronyms, including \"kriging,\" a term derived from geostatistics, as introduced by Matheron in 1963. Additionally, it is referred to as Gaussian spatial modeling or a Gaussian stochastic process, and machine learning (ML) researchers often use the term Gaussian process regression (GPR).\n",
        "In all of these instances, the central focus is on regression. This involves training on both inputs and outputs, with the ultimate objective of making predictions and quantifying uncertainty (referred to as uncertainty quantification or UQ).\n",
        "\n",
        "However, it's important to emphasize that GPs are not a universal solution for every problem. Specialized tools may outperform GPs in specific, non-generic contexts, and GPs have their own set of limitations that need to be considered.\n",
        "\n",
        "## Gaussian Process Prior\n",
        "\n",
        "In the context of GP, any finite collection of realizations, which is represented by $n$ observations, is modeled as having a multivariate normal (MVN) distribution. The characteristics of these realizations can be fully described by two key parameters:\n",
        "\n",
        "1. Their mean, denoted as an $n$-vector $\\mu$.\n",
        "2. The covariance matrix, denoted as an $n \\times n$ matrix $\\Sigma$. This covariance matrix encapsulates the relationships and variability between the individual realizations within the collection.\n",
        "\n",
        "\n",
        "## Covariance Function\n",
        "\n",
        "The covariance function is defined by inverse exponentiated squared Euclidean distance:\n",
        "$$\n",
        "\\Sigma(\\vec{x}, \\vec{x}') = \\exp\\{ - || \\vec{x} - \\vec{x}'||^2 \\},\n",
        "$$\n",
        "where $\\vec{x}$ and $\\vec{x}'$ are two points in the $k$-dimensional input space and $\\| \\cdot \\|$ denotes the Euclidean distance, i.e.,\n",
        "$$\n",
        "|| \\vec{x} - \\vec{x}'||^2 = \\sum_{i=1}^k (x_i - x_i')^2.\n",
        "$$\n",
        "\n",
        "An 1-d example is shown in @fig-exp2euclid. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def squared_euclidean_distance(point1, point2, sigma2=1.0):\n",
        "    return (point1 - point2)**2/ (2 * sigma2)\n",
        "\n",
        "def inverse_exp_squared_distance(point1, point2,sigma2):\n",
        "    return np.exp(-squared_euclidean_distance(point1, point2, sigma2))\n",
        "\n",
        "def generate_line(distance, step=0.01):\n",
        "    return np.arange(0, distance+step, step)\n",
        "\n",
        "def visualize_inverse_exp_squared_distance(distance, point, sigma2_values):\n",
        "    line = generate_line(distance)\n",
        "    \n",
        "    for sigma2 in sigma2_values:\n",
        "        distances = [inverse_exp_squared_distance(p, point, sigma2) for p in line]\n",
        "        plt.plot(line, distances, label=f'sigma2={sigma2}')\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-exp2euclid\n",
        "#| fig-cap: \"One-dim inverse exponentiated squared Euclidean distance\"\n",
        "#| echo: false\n",
        "visualize_inverse_exp_squared_distance(5, 0.0, [0.5, 1, 2.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The covariance function is also referred to as the kernel function. The *Gaussian* kernel uses an additional parameter, $\\sigma^2$, to control the rate of decay. This parameter is referred to as the length scale or the characteristic length scale. The covariance function is then defined as \n",
        "\n",
        "$$\n",
        "\\Sigma(\\vec{x}, \\vec{x}') = \\exp\\{ - || \\vec{x} - \\vec{x}'||^2 / (2 \\sigma^2) \\}.\n",
        "$$ {#eq-Sigma}\n",
        "\n",
        "The covariance decays exponentially fast as $\\vec{x}$ and $\\vec{x}'$ become farther apart. Observe that \n",
        "\n",
        "$$\n",
        "\\Sigma(\\vec{x},\\vec{x}) = 1\n",
        "$$ \n",
        "and \n",
        "\n",
        "$$\n",
        "\\Sigma(\\vec{x}, \\vec{x}') < 1\n",
        "$$ \n",
        "for  $\\vec{x} \\neq \\vec{x}'$. The function $\\Sigma(\\vec{x},\\vec{x}')$ must be positive definite.\n",
        "\n",
        "\n",
        "::: {#rem-krigingbase-gauss}\n",
        "### Kriging and Gaussian Basis Functions\n",
        "\n",
        "The Kriging basis function (@eq-krigingbase) is related to the 1-dim Gaussian basis function (@eq-Sigma),\n",
        "which is defined as\n",
        "$$\n",
        "\\Sigma(\\vec{x}^{(i)}, \\vec{x}^{(j)}) = \\exp\\{ - || \\vec{x}^{(i)} - \\vec{x}^{(j)}||^2 / (2\\sigma^2) \\}.\n",
        "$$ {#eq-Sigma2}\n",
        "\n",
        "There are some differences between Gaussian basis functions and Kriging basis functions:\n",
        "\n",
        "  * Where the Gaussian basis function has $1/(2\\sigma^2)$, the Kriging basis has a vector $\\theta = [\\theta_1, \\theta_2, \\ldots, \\theta_k]^T$.\n",
        "  * The $\\theta$ vector allows the width of the basis function to vary from dimension to dimension.\n",
        "  * In the Gaussian basis function, the exponent is fixed at 2, Kriging allows this exponent $p_l$ to vary (typically from 1 to 2).\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "### Positive Definiteness\n",
        "\n",
        "Positive definiteness in the context of the covariance matrix $\\Sigma_n$ is a fundamental requirement. It is determined by evaluating $\\Sigma(x_i, x_j)$ at pairs of $n$ $\\vec{x}$-values, denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_n$. The condition for positive definiteness is that for all $\\vec{x}$ vectors that are not equal to zero, the expression $\\vec{x}^\\top \\Sigma_n \\vec{x}$ must be greater than zero. This property is essential when intending to use $\\Sigma_n$ as a covariance matrix in multivariate normal (MVN) analysis. It is analogous to the requirement in univariate Gaussian distributions where the variance parameter, $\\sigma^2$, must be positive.\n",
        "\n",
        "\n",
        "\n",
        "Gaussian Processes (GPs) can be effectively utilized to generate random data that follows a smooth functional relationship. The process involves the following steps:\n",
        "\n",
        "1. Select a set of $\\vec{x}$-values, denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_n$.\n",
        "2. Define the covariance matrix $\\Sigma_n$ by evaluating $\\Sigma_n^{ij} = \\Sigma(\\vec{x}_i, \\vec{x}_j)$ for $i, j = 1, 2, \\ldots, n$.\n",
        "3. Generate an $n$-variate realization $Y$ that follows a multivariate normal distribution with a mean of zero and a covariance matrix $\\Sigma_n$, expressed as $Y \\sim \\mathcal{N}_n(0, \\Sigma_n)$.\n",
        "4. Visualize the result by plotting it in the $x$-$y$ plane.\n",
        "\n",
        "## Construction of the Covariance Matrix\n",
        "\n",
        "Here is an one-dimensional example. The process begins by creating an input grid using $\\vec{x}$-values. This grid consists of 100 elements, providing the basis for further analysis and visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "n = 100\n",
        "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the context of this discussion, the construction of the covariance matrix, denoted as $\\Sigma_n$, relies on the concept of inverse exponentiated squared Euclidean distances. However, it's important to note that a modification is introduced later in the process. Specifically, the diagonal of the covariance matrix is augmented with a small value, represented as \"eps\" or $\\epsilon$.\n",
        "\n",
        "The reason for this augmentation is that while inverse exponentiated distances theoretically ensure the covariance matrix's positive definiteness, in practical applications, the matrix can sometimes become numerically ill-conditioned. By adding a small value to the diagonal, such as $\\epsilon$, this ill-conditioning issue is mitigated. In this context, $\\epsilon$ is often referred to as \"jitter.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, spacing, sqrt, arange, append, ravel\n",
        "from numpy.linalg import cholesky, solve\n",
        "from numpy.random import multivariate_normal\n",
        "def build_Sigma(X, sigma2):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = 1/(2*sigma2[l])*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sigma2 = np.array([1.0])\n",
        "Sigma = build_Sigma(X, sigma2)\n",
        "np.round(Sigma[:3,:], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(Sigma, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation of Random Samples and Plotting the Realizations of the Random Function {#sec-random-samples-gp}\n",
        "\n",
        "In the context of the multivariate normal distribution, the next step is to utilize the previously constructed covariance matrix denoted as `Sigma`. It is used as an essential component in generating random samples from the multivariate normal distribution.\n",
        "\n",
        "The function `multivariate_normal` is employed for this purpose. It serves as a random number generator specifically designed for the multivariate normal distribution. In this case, the mean of the distribution is set equal to `mean`, and the covariance matrix is provided as `Psi`. The argument `size` specifies the number of realizations, which, in this specific scenario, is set to one.\n",
        "\n",
        "By default, the mean vector is initialized to zero. To match the number of samples, which is equivalent to the number of rows in the `X` and `Sigma` matrices, the argument `zeros(n)` is used, where `n` represents the number of samples (here taken from the size of the matrix, e.g.,: `Sigma.shape[0]`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng = np.random.default_rng(seed=12345)\n",
        "Y = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = 1, check_valid=\"raise\").reshape(-1,1)\n",
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can plot the results, i.e., a finite realization of the random function $Y()$ under a GP prior with a particular covariance structure. We will plot those `X` and `Y` pairs as connected points on an  $x$-$y$ plane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn1-1\n",
        "#| fig-cap: \"Realization of one random function under a GP prior. sigma2: 1.0\"\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, Y)\n",
        "plt.title(\"Realization of Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn1-3\n",
        "#| fig-cap: \"Realization of three random functions under a GP prior. sigma2: 1.0\"\n",
        "rng = np.random.default_rng(seed=12345)\n",
        "Y = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = 3, check_valid=\"raise\")\n",
        "plt.plot(X, Y.T)\n",
        "plt.title(\"Realization of Three Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Properties of the 1d Example\n",
        "\n",
        "### Several Bumps:\n",
        "In this analysis, we observe several bumps in the $x$-range of $[0,10]$.\n",
        "These bumps in the function occur because shorter distances exhibit high correlation, while longer distances tend to be essentially uncorrelated. This leads to variations in the function's behavior:\n",
        "\n",
        "* When $x$ and $x'$ are one $\\sigma$ unit apart, the correlation is $\\exp\\left(-\\sigma^2 / (2\\sigma^2)\\right) = \\exp(-1/2) \\approx 0.61$, i.e., a relative high correlation.\n",
        "* $2\\sigma$ apart means correlation $\\exp(− 2^2 /2) \\approx 0.14$, i.e., only small correlation.\n",
        "* $4\\sigma$ apart means correlation $\\exp(− 4^2 /2) \\approx 0.0003$, i.e., nearly no correlation---variables are considered independent for almost all practical application.\n",
        "\n",
        "\n",
        "### Smoothness:\n",
        "The function plotted in @fig-mvn1-1 represents only a finite realization, which means that we have data for a limited number of pairs, specifically 100 points. These points appear smooth in a tactile sense because they are closely spaced, and the plot function connects the dots with lines to create the appearance of smoothness. The complete surface, which can be conceptually extended to an infinite realization over a compact domain, is exceptionally smooth in a calculus sense due to the covariance function's property of being infinitely differentiable.\n",
        "\n",
        "### Scale of Two:\n",
        "Regarding the scale of the $Y$ values, they have a range of approximately $[-2,2]$, with a 95% probability of falling within this range. In standard statistical terms, 95% of the data points typically fall within two standard deviations of the mean, which is a common measure of the spread or range of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, spacing, sqrt, arange, append, ravel\n",
        "from numpy.random import multivariate_normal\n",
        "\n",
        "def build_Sigma(X, sigma2):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = 1/(2*sigma2[l])*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)\n",
        "\n",
        "def plot_mvn( a=0, b=10, sigma2=1.0, size=1, n=100, show=True):    \n",
        "    X = np.linspace(a, b, n, endpoint=False).reshape(-1,1)\n",
        "    sigma2 = np.array([sigma2])\n",
        "    Sigma = build_Sigma(X, sigma2)\n",
        "    rng = np.random.default_rng(seed=12345)\n",
        "    Y = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = size, check_valid=\"raise\")\n",
        "    plt.plot(X, Y.T)\n",
        "    plt.title(\"Realization of Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\n",
        "    if show:\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn2\n",
        "#| fig-cap: \"Realization of Random Functions under a GP prior. sigma2: 10\"\n",
        "plot_mvn(a=0, b=10, sigma2=10.0, size=3, n=250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn5\n",
        "#| fig-cap: \"Realization of Random Functions under a GP prior. sigma2: 0.1\"\n",
        "plot_mvn(a=0, b=10, sigma2=0.1, size=3, n=250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Jupyter Notebook\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "* The Jupyter-Notebook of this lecture is available on GitHub in the [Hyperparameter-Tuning-Cookbook Repository](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/a_04_gp_background.ipynb)\n",
        "\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot313/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}