{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Hyperparameter Tuning with PyTorch Lightning: Physics Informed Neural Networks {#sec-light-pinn-model-601}\n"
      ],
      "id": "5a833239"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: 601_user_model_first_imports\n",
        "import numpy as np\n",
        "import os\n",
        "from math import inf\n",
        "import pandas as pd\n",
        "import warnings\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "user_model_first_imports",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we will show how to set up PINN hyperparameter tuner from scratch based on the `spotpython` programs from @sec-light-user-model-601.\n",
        "\n",
        "## The Ground Truth Model\n",
        "\n",
        "Definition of the (unknown) differential equation:\n"
      ],
      "id": "cf6571e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as thdat\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# boundaries for the frequency range\n",
        "a = 0\n",
        "b = 500\n",
        "\n",
        "def ode(frequency, loc, sigma, R):\n",
        "    \"\"\"Computes the amplitude. Defining equation, used\n",
        "    to generate data and train models.\n",
        "    The equation itself is not known to the model.\n",
        "\n",
        "    Args:\n",
        "        frequency: (N,) array-like\n",
        "        loc: float\n",
        "        sigma: float\n",
        "        R: float\n",
        "    \n",
        "    Returns:\n",
        "        (N,) array-like\n",
        "    \n",
        "    Examples:\n",
        "        >>> ode(0, 25, 100, 0.005)\n",
        "        100.0\n",
        "    \"\"\"\n",
        "    A = np.exp(-R * (frequency - loc)**2/sigma**2)\n",
        "    return A"
      ],
      "id": "9cfdc0bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting the parameters for the ode\n"
      ],
      "id": "7ed64f7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(10)\n",
        "loc = 250\n",
        "sigma = 100\n",
        "R = 0.5"
      ],
      "id": "c8b444ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Generating the data\n"
      ],
      "id": "49a194c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frequencies = np.linspace(a, b, 1000)\n",
        "eq = functools.partial(ode, loc=loc, sigma=sigma, R=R)\n",
        "amplitudes = eq(frequencies)"
      ],
      "id": "fad8a882",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Now we have the ground truth for the full frequency range and can take a look at the first 10 values:\n"
      ],
      "id": "5caeaae9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.DataFrame({'Frequency': frequencies[:10], 'Amplitude': amplitudes[:10]})\n",
        "print(df)"
      ],
      "id": "f1459b0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We generate the training data as a subset of the full frequency range and add some noise:\n"
      ],
      "id": "42f02f7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Make training data\n",
        "t = np.linspace(a, 2*b/3, 10)\n",
        "A = eq(t) +  0.2 * np.random.randn(10)"
      ],
      "id": "a9dd3793",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Plot of the training data and the ground truth:\n"
      ],
      "id": "c6a6544c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(frequencies, amplitudes)\n",
        "plt.plot(t, A, 'o')\n",
        "plt.legend(['Equation (ground truth)', 'Training data'])\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlabel('Frequency')"
      ],
      "id": "3cf91f76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Required Files\n",
        "\n",
        "We use the files from the `/userModel` directory as templates. They are renamed as follows:\n",
        "\n",
        "* `my_regressor.py` $\\Rightarrow$ `pinn_regressor.py`, see @sec-pinn-regressor\n",
        "* `my_hyperdict.json` $\\Rightarrow$ `pinn_hyperdict.py`, see @sec-pinn-hyper-dict-json\n",
        "* `my_hyperdict.py` $\\Rightarrow$ `pinn_hyperdict.py`, see @sec-pinn-hyperdict.\n",
        "\n",
        "### The New `pinn_hyperdict.py` File {#sec-pinn-hyperdict}\n",
        "\n",
        "Modifying the `pin_hyperdict.py` file is very easy. We simply have to change the classname ` MyHyperDict` to ` PINNHyperDict` and the `filename` from `\"my_hyper_dict.json\"` to `\"pinn_hyper_dict.json\"`. The file is shown below.\n"
      ],
      "id": "1d95c57d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "from spotpython.data import base\n",
        "import pathlib\n",
        "\n",
        "class PINNHyperDict(base.FileConfig):\n",
        "    def __init__(\n",
        "        self,\n",
        "        filename: str = \"pinn_hyper_dict.json\",\n",
        "        directory: None = None,\n",
        "    ) -> None:\n",
        "        super().__init__(filename=filename, directory=directory)\n",
        "        self.filename = filename\n",
        "        self.directory = directory\n",
        "        self.hyper_dict = self.load()\n",
        "\n",
        "    @property\n",
        "    def path(self):\n",
        "        if self.directory:\n",
        "            return pathlib.Path(self.directory).joinpath(self.filename)\n",
        "        return pathlib.Path(__file__).parent.joinpath(self.filename)\n",
        "\n",
        "    def load(self) -> dict:\n",
        "        with open(self.path, \"r\") as f:\n",
        "            d = json.load(f)\n",
        "        return d"
      ],
      "id": "5f7b95e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The New `pinn_regressor.py` File {#sec-pinn-regressor}\n",
        "\n",
        "::: {.callout-warning}\n",
        "### Warning\n",
        "\n",
        "The document is not complete. The code below is a template and needs to be modified to work with the PINN model.\n",
        "\n",
        ":::\n"
      ],
      "id": "e414e49d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_pinn_regressor\n",
        "#| eval: false\n",
        "import lightning as L\n",
        "import torch\n",
        "from torch import nn\n",
        "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
        "import torchmetrics.functional.regression\n",
        "\n",
        "class PINNRegressor(L.LightningModule):\n",
        "    \"\"\"\n",
        "    A LightningModule class for a regression neural network model.\n",
        "\n",
        "    Attributes:\n",
        "        l1 (int):\n",
        "            The number of neurons in the first hidden layer.\n",
        "        epochs (int):\n",
        "            The number of epochs to train the model for.\n",
        "        batch_size (int):\n",
        "            The batch size to use during training.\n",
        "        initialization (str):\n",
        "            The initialization method to use for the weights.\n",
        "        act_fn (nn.Module):\n",
        "            The activation function to use in the hidden layers.\n",
        "        optimizer (str):\n",
        "            The optimizer to use during training.\n",
        "        dropout_prob (float):\n",
        "            The probability of dropping out a neuron during training.\n",
        "        lr_mult (float):\n",
        "            The learning rate multiplier for the optimizer.\n",
        "        patience (int):\n",
        "            The number of epochs to wait before early stopping.\n",
        "        _L_in (int):\n",
        "            The number of input features.\n",
        "        _L_out (int):\n",
        "            The number of output classes.\n",
        "        _torchmetric (str):\n",
        "            The metric to use for the loss function. If `None`,\n",
        "            then \"mean_squared_error\" is used.\n",
        "        layers (nn.Sequential):\n",
        "            The neural network model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        l1: int,\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        initialization: str,\n",
        "        act_fn: nn.Module,\n",
        "        optimizer: str,\n",
        "        dropout_prob: float,\n",
        "        lr_mult: float,\n",
        "        patience: int,\n",
        "        _L_in: int,\n",
        "        _L_out: int,\n",
        "        _torchmetric: str,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the MyRegressor object.\n",
        "\n",
        "        Args:\n",
        "            l1 (int):\n",
        "                The number of neurons in the first hidden layer.\n",
        "            epochs (int):\n",
        "                The number of epochs to train the model for.\n",
        "            batch_size (int):\n",
        "                The batch size to use during training.\n",
        "            initialization (str):\n",
        "                The initialization method to use for the weights.\n",
        "            act_fn (nn.Module):\n",
        "                The activation function to use in the hidden layers.\n",
        "            optimizer (str):\n",
        "                The optimizer to use during training.\n",
        "            dropout_prob (float):\n",
        "                The probability of dropping out a neuron during training.\n",
        "            lr_mult (float):\n",
        "                The learning rate multiplier for the optimizer.\n",
        "            patience (int):\n",
        "                The number of epochs to wait before early stopping.\n",
        "            _L_in (int):\n",
        "                The number of input features. Not a hyperparameter, but needed to create the network.\n",
        "            _L_out (int):\n",
        "                The number of output classes. Not a hyperparameter, but needed to create the network.\n",
        "            _torchmetric (str):\n",
        "                The metric to use for the loss function. If `None`,\n",
        "                then \"mean_squared_error\" is used.\n",
        "\n",
        "        Returns:\n",
        "            (NoneType): None\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If l1 is less than 4.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n",
        "        # checkpointing. It is recommended to ignore them\n",
        "        # using `self.save_hyperparameters(ignore=['act_fn'])`\n",
        "        # self.save_hyperparameters(ignore=[\"act_fn\"])\n",
        "        #\n",
        "        self._L_in = _L_in\n",
        "        self._L_out = _L_out\n",
        "        if _torchmetric is None:\n",
        "            _torchmetric = \"mean_squared_error\"\n",
        "        self._torchmetric = _torchmetric\n",
        "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
        "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
        "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
        "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
        "        # set dummy input array for Tensorboard Graphs\n",
        "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
        "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
        "        if self.hparams.l1 < 4:\n",
        "            raise ValueError(\"l1 must be at least 4\")\n",
        "        hidden_sizes = self._get_hidden_sizes()\n",
        "        # Create the network based on the specified hidden sizes\n",
        "        layers = []\n",
        "        layer_sizes = [self._L_in] + hidden_sizes\n",
        "        layer_size_last = layer_sizes[0]\n",
        "        for layer_size in layer_sizes[1:]:\n",
        "            layers += [\n",
        "                nn.Linear(layer_size_last, layer_size),\n",
        "                self.hparams.act_fn,\n",
        "                nn.Dropout(self.hparams.dropout_prob),\n",
        "            ]\n",
        "            layer_size_last = layer_size\n",
        "        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
        "        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def _generate_div2_list(self, n, n_min) -> list:\n",
        "        \"\"\"\n",
        "        Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n",
        "        until the result is less than n_min.\n",
        "        This function starts with n and keeps dividing it by 2 until n_min is reached.\n",
        "        The number of times each value is added to the list is determined by n // current.\n",
        "        No more than 4 repeats of the same value (`max_repeats` below) are added to the list.\n",
        "\n",
        "        Args:\n",
        "            n (int): The number to start with.\n",
        "            n_min (int): The minimum number to stop at.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of numbers from n to n_min (inclusive).\n",
        "\n",
        "        Examples:\n",
        "            _generate_div2_list(10, 1)\n",
        "            [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "            _ generate_div2_list(10, 2)\n",
        "            [10, 5, 5, 2, 2, 2, 2, 2]\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        current = n\n",
        "        repeats = 1\n",
        "        max_repeats = 4\n",
        "        while current >= n_min:\n",
        "            result.extend([current] * min(repeats, max_repeats))\n",
        "            current = current // 2\n",
        "            repeats = repeats + 1\n",
        "        return result\n",
        "\n",
        "    def _get_hidden_sizes(self):\n",
        "        \"\"\"\n",
        "        Generate the hidden layer sizes for the network.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of hidden layer sizes.\n",
        "\n",
        "        \"\"\"\n",
        "        n_low = self._L_in // 4\n",
        "        n_high = max(self.hparams.l1, 2 * n_low)\n",
        "        hidden_sizes = self._generate_div2_list(n_high, n_low)\n",
        "        return hidden_sizes\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor containing a batch of input data.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the output of the model.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "    def _calculate_loss(self, batch):\n",
        "        \"\"\"\n",
        "        Calculate the loss for the given batch.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y = y.view(len(y), 1)\n",
        "        y_hat = self(x)\n",
        "        loss = self.metric(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single training step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return val_loss\n",
        "\n",
        "    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single validation step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
        "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
        "        return val_loss\n",
        "\n",
        "    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single test step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "        \"\"\"\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
        "        return val_loss\n",
        "\n",
        "    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single prediction step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the input data, the true labels, and the predicted values.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        yhat = self(x)\n",
        "        y = y.view(len(y), 1)\n",
        "        yhat = yhat.view(len(yhat), 1)\n",
        "        print(f\"Predict step x: {x}\")\n",
        "        print(f\"Predict step y: {y}\")\n",
        "        print(f\"Predict step y_hat: {yhat}\")\n",
        "        # pred_loss = F.mse_loss(y_hat, y)\n",
        "        # pred loss not registered\n",
        "        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n",
        "        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n",
        "        # MisconfigurationException: You are trying to `self.log()`\n",
        "        # but the loop's result collection is not registered yet.\n",
        "        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n",
        "        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n",
        "        return (x, y, yhat)\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        \"\"\"\n",
        "        Configures the optimizer for the model.\n",
        "\n",
        "        Notes:\n",
        "            The default Lightning way is to define an optimizer as\n",
        "            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n",
        "            spotpython uses an optimizer handler to create the optimizer, which\n",
        "            adapts the learning rate according to the lr_mult hyperparameter as\n",
        "            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n",
        "\n",
        "        Returns:\n",
        "            torch.optim.Optimizer: The optimizer to use during training.\n",
        "\n",
        "        \"\"\"\n",
        "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        optimizer = optimizer_handler(\n",
        "            optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n",
        "        )\n",
        "        return optimizer"
      ],
      "id": "pinn_regressor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The New `pinn_hyperdict.json` File {#sec-pinn-hyper-dict-json}"
      ],
      "id": "66679e4e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}