{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43dbc2a",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "jupyter: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Hyperparameter Tuning with PyTorch Lightning: Physics Informed Neural Networks {#sec-light-pinn-model-601}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "user_model_first_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: 601_user_model_first_imports\n",
    "import numpy as np\n",
    "import os\n",
    "from math import inf\n",
    "import pandas as pd\n",
    "import warnings\n",
    "if not os.path.exists('./figures'):\n",
    "    os.makedirs('./figures')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2a034",
   "metadata": {},
   "source": [
    "In this section, we will show how to set up PINN hyperparameter tuner from scratch based on the `spotpython` programs from @sec-light-user-model-601.\n",
    "\n",
    "## The Ground Truth Model\n",
    "\n",
    "Definition of the (unknown) differential equation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedd3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as thdat\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# boundaries for the frequency range\n",
    "a = 0\n",
    "b = 500\n",
    "\n",
    "def ode(frequency, loc, sigma, R):\n",
    "    \"\"\"Computes the amplitude. Defining equation, used\n",
    "    to generate data and train models.\n",
    "    The equation itself is not known to the model.\n",
    "\n",
    "    Args:\n",
    "        frequency: (N,) array-like\n",
    "        loc: float\n",
    "        sigma: float\n",
    "        R: float\n",
    "    \n",
    "    Returns:\n",
    "        (N,) array-like\n",
    "    \n",
    "    Examples:\n",
    "        >>> ode(0, 25, 100, 0.005)\n",
    "        100.0\n",
    "    \"\"\"\n",
    "    A = np.exp(-R * (frequency - loc)**2/sigma**2)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74035ec9",
   "metadata": {},
   "source": [
    "Setting the parameters for the ode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7f4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "loc = 250\n",
    "sigma = 100\n",
    "R = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63977a1e",
   "metadata": {},
   "source": [
    "* Generating the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb810ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.linspace(a, b, 1000)\n",
    "eq = functools.partial(ode, loc=loc, sigma=sigma, R=R)\n",
    "amplitudes = eq(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539a2f7",
   "metadata": {},
   "source": [
    "* Now we have the ground truth for the full frequency range and can take a look at the first 10 values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd518289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Frequency  Amplitude\n",
      "0   0.000000   0.043937\n",
      "1   0.500501   0.044490\n",
      "2   1.001001   0.045048\n",
      "3   1.501502   0.045612\n",
      "4   2.002002   0.046183\n",
      "5   2.502503   0.046759\n",
      "6   3.003003   0.047341\n",
      "7   3.503504   0.047929\n",
      "8   4.004004   0.048524\n",
      "9   4.504505   0.049124\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Frequency': frequencies[:10], 'Amplitude': amplitudes[:10]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627275b",
   "metadata": {},
   "source": [
    "* We generate the training data as a subset of the full frequency range and add some noise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1576d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training data\n",
    "t = np.linspace(a, 2*b/3, 10)\n",
    "A = eq(t) +  0.2 * np.random.randn(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117b4c1",
   "metadata": {},
   "source": [
    "* Plot of the training data and the ground truth:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac024ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Frequency')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/pdf": "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",
      "text/plain": [
       "<Figure size 1650x1050 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(frequencies, amplitudes)\n",
    "plt.plot(t, A, 'o')\n",
    "plt.legend(['Equation (ground truth)', 'Training data'])\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46c723",
   "metadata": {},
   "source": [
    "## Required Files\n",
    "\n",
    "We use the files from the `/userModel` directory as templates. They are renamed as follows:\n",
    "\n",
    "* `my_regressor.py` $\\Rightarrow$ `pinn_regressor.py`, see @sec-pinn-regressor\n",
    "* `my_hyperdict.json` $\\Rightarrow$ `pinn_hyperdict.py`, see @sec-pinn-hyper-dict-json\n",
    "* `my_hyperdict.py` $\\Rightarrow$ `pinn_hyperdict.py`, see @sec-pinn-hyperdict.\n",
    "\n",
    "### The New `pinn_hyperdict.py` File {#sec-pinn-hyperdict}\n",
    "\n",
    "Modifying the `pin_hyperdict.py` file is very easy. We simply have to change the classname ` MyHyperDict` to ` PINNHyperDict` and the `filename` from `\"my_hyper_dict.json\"` to `\"pinn_hyper_dict.json\"`. The file is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a7099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spotpython.data import base\n",
    "import pathlib\n",
    "\n",
    "class PINNHyperDict(base.FileConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str = \"pinn_hyper_dict.json\",\n",
    "        directory: None = None,\n",
    "    ) -> None:\n",
    "        super().__init__(filename=filename, directory=directory)\n",
    "        self.filename = filename\n",
    "        self.directory = directory\n",
    "        self.hyper_dict = self.load()\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        if self.directory:\n",
    "            return pathlib.Path(self.directory).joinpath(self.filename)\n",
    "        return pathlib.Path(__file__).parent.joinpath(self.filename)\n",
    "\n",
    "    def load(self) -> dict:\n",
    "        with open(self.path, \"r\") as f:\n",
    "            d = json.load(f)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39630051",
   "metadata": {},
   "source": [
    "### The New `pinn_regressor.py` File {#sec-pinn-regressor}\n",
    "\n",
    "::: {.callout-warning}\n",
    "### Warning\n",
    "\n",
    "The document is not complete. The code below is a template and needs to be modified to work with the PINN model.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pinn_regressor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: 601_pinn_regressor\n",
    "#| eval: false\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
    "import torchmetrics.functional.regression\n",
    "\n",
    "class PINNRegressor(L.LightningModule):\n",
    "    \"\"\"\n",
    "    A LightningModule class for a regression neural network model.\n",
    "\n",
    "    Attributes:\n",
    "        l1 (int):\n",
    "            The number of neurons in the first hidden layer.\n",
    "        epochs (int):\n",
    "            The number of epochs to train the model for.\n",
    "        batch_size (int):\n",
    "            The batch size to use during training.\n",
    "        initialization (str):\n",
    "            The initialization method to use for the weights.\n",
    "        act_fn (nn.Module):\n",
    "            The activation function to use in the hidden layers.\n",
    "        optimizer (str):\n",
    "            The optimizer to use during training.\n",
    "        dropout_prob (float):\n",
    "            The probability of dropping out a neuron during training.\n",
    "        lr_mult (float):\n",
    "            The learning rate multiplier for the optimizer.\n",
    "        patience (int):\n",
    "            The number of epochs to wait before early stopping.\n",
    "        _L_in (int):\n",
    "            The number of input features.\n",
    "        _L_out (int):\n",
    "            The number of output classes.\n",
    "        _torchmetric (str):\n",
    "            The metric to use for the loss function. If `None`,\n",
    "            then \"mean_squared_error\" is used.\n",
    "        layers (nn.Sequential):\n",
    "            The neural network model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l1: int,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        initialization: str,\n",
    "        act_fn: nn.Module,\n",
    "        optimizer: str,\n",
    "        dropout_prob: float,\n",
    "        lr_mult: float,\n",
    "        patience: int,\n",
    "        _L_in: int,\n",
    "        _L_out: int,\n",
    "        _torchmetric: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the MyRegressor object.\n",
    "\n",
    "        Args:\n",
    "            l1 (int):\n",
    "                The number of neurons in the first hidden layer.\n",
    "            epochs (int):\n",
    "                The number of epochs to train the model for.\n",
    "            batch_size (int):\n",
    "                The batch size to use during training.\n",
    "            initialization (str):\n",
    "                The initialization method to use for the weights.\n",
    "            act_fn (nn.Module):\n",
    "                The activation function to use in the hidden layers.\n",
    "            optimizer (str):\n",
    "                The optimizer to use during training.\n",
    "            dropout_prob (float):\n",
    "                The probability of dropping out a neuron during training.\n",
    "            lr_mult (float):\n",
    "                The learning rate multiplier for the optimizer.\n",
    "            patience (int):\n",
    "                The number of epochs to wait before early stopping.\n",
    "            _L_in (int):\n",
    "                The number of input features. Not a hyperparameter, but needed to create the network.\n",
    "            _L_out (int):\n",
    "                The number of output classes. Not a hyperparameter, but needed to create the network.\n",
    "            _torchmetric (str):\n",
    "                The metric to use for the loss function. If `None`,\n",
    "                then \"mean_squared_error\" is used.\n",
    "\n",
    "        Returns:\n",
    "            (NoneType): None\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If l1 is less than 4.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n",
    "        # checkpointing. It is recommended to ignore them\n",
    "        # using `self.save_hyperparameters(ignore=['act_fn'])`\n",
    "        # self.save_hyperparameters(ignore=[\"act_fn\"])\n",
    "        #\n",
    "        self._L_in = _L_in\n",
    "        self._L_out = _L_out\n",
    "        if _torchmetric is None:\n",
    "            _torchmetric = \"mean_squared_error\"\n",
    "        self._torchmetric = _torchmetric\n",
    "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
    "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
    "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
    "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
    "        # set dummy input array for Tensorboard Graphs\n",
    "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
    "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
    "        if self.hparams.l1 < 4:\n",
    "            raise ValueError(\"l1 must be at least 4\")\n",
    "        hidden_sizes = self._get_hidden_sizes()\n",
    "        # Create the network based on the specified hidden sizes\n",
    "        layers = []\n",
    "        layer_sizes = [self._L_in] + hidden_sizes\n",
    "        layer_size_last = layer_sizes[0]\n",
    "        for layer_size in layer_sizes[1:]:\n",
    "            layers += [\n",
    "                nn.Linear(layer_size_last, layer_size),\n",
    "                self.hparams.act_fn,\n",
    "                nn.Dropout(self.hparams.dropout_prob),\n",
    "            ]\n",
    "            layer_size_last = layer_size\n",
    "        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
    "        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def _generate_div2_list(self, n, n_min) -> list:\n",
    "        \"\"\"\n",
    "        Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n",
    "        until the result is less than n_min.\n",
    "        This function starts with n and keeps dividing it by 2 until n_min is reached.\n",
    "        The number of times each value is added to the list is determined by n // current.\n",
    "        No more than 4 repeats of the same value (`max_repeats` below) are added to the list.\n",
    "\n",
    "        Args:\n",
    "            n (int): The number to start with.\n",
    "            n_min (int): The minimum number to stop at.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of numbers from n to n_min (inclusive).\n",
    "\n",
    "        Examples:\n",
    "            _generate_div2_list(10, 1)\n",
    "            [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "            _ generate_div2_list(10, 2)\n",
    "            [10, 5, 5, 2, 2, 2, 2, 2]\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        current = n\n",
    "        repeats = 1\n",
    "        max_repeats = 4\n",
    "        while current >= n_min:\n",
    "            result.extend([current] * min(repeats, max_repeats))\n",
    "            current = current // 2\n",
    "            repeats = repeats + 1\n",
    "        return result\n",
    "\n",
    "    def _get_hidden_sizes(self):\n",
    "        \"\"\"\n",
    "        Generate the hidden layer sizes for the network.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of hidden layer sizes.\n",
    "\n",
    "        \"\"\"\n",
    "        n_low = self._L_in // 4\n",
    "        n_high = max(self.hparams.l1, 2 * n_low)\n",
    "        hidden_sizes = self._generate_div2_list(n_high, n_low)\n",
    "        return hidden_sizes\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A tensor containing a batch of input data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the output of the model.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def _calculate_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Calculate the loss for the given batch.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y = y.view(len(y), 1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.metric(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single training step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "\n",
    "        \"\"\"\n",
    "        val_loss = self._calculate_loss(batch)\n",
    "        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "\n",
    "        \"\"\"\n",
    "        val_loss = self._calculate_loss(batch)\n",
    "        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
    "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single test step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "        \"\"\"\n",
    "        val_loss = self._calculate_loss(batch)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
    "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
    "        return val_loss\n",
    "\n",
    "    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single prediction step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the input data, the true labels, and the predicted values.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        yhat = self(x)\n",
    "        y = y.view(len(y), 1)\n",
    "        yhat = yhat.view(len(yhat), 1)\n",
    "        print(f\"Predict step x: {x}\")\n",
    "        print(f\"Predict step y: {y}\")\n",
    "        print(f\"Predict step y_hat: {yhat}\")\n",
    "        # pred_loss = F.mse_loss(y_hat, y)\n",
    "        # pred loss not registered\n",
    "        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n",
    "        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n",
    "        # MisconfigurationException: You are trying to `self.log()`\n",
    "        # but the loop's result collection is not registered yet.\n",
    "        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n",
    "        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n",
    "        return (x, y, yhat)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Configures the optimizer for the model.\n",
    "\n",
    "        Notes:\n",
    "            The default Lightning way is to define an optimizer as\n",
    "            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n",
    "            spotpython uses an optimizer handler to create the optimizer, which\n",
    "            adapts the learning rate according to the lr_mult hyperparameter as\n",
    "            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: The optimizer to use during training.\n",
    "\n",
    "        \"\"\"\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer = optimizer_handler(\n",
    "            optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df63fcd",
   "metadata": {},
   "source": [
    "### The New `pinn_hyperdict.json` File {#sec-pinn-hyper-dict-json}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
