---
execute:
  cache: false
  eval: true
  echo: true
  warning: false
---

# Sequential Parameter Optimization: Using `scipy` Optimizers {#sec-scipy-optimizers}


This notebook describes how different optimizers from the `scipy optimize` package can be used on the surrogate.
The optimization algorithms are available from [https://docs.scipy.org/doc/scipy/reference/optimize.html](https://docs.scipy.org/doc/scipy/reference/optimize.html)

```{python}
import numpy as np
from math import inf
from spotPython.fun.objectivefunctions import analytical
from spotPython.spot import spot
from scipy.optimize import shgo
from scipy.optimize import direct
from scipy.optimize import differential_evolution
from scipy.optimize import dual_annealing
from scipy.optimize import basinhopping
```

## The Objective Function Branin

* The `spotPython` package provides several classes of objective functions.
* We will use an analytical objective function, i.e., a function that can be described by a (closed) formula.
* Here we will use the Branin function. The 2-dim Branin function is

    $$y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * cos(x1) + s,$$ 
    where values of a, b, c, r, s and t are: 
    $a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10$ and $t = 1 / (8*pi)$.

* It has three global minima:
    
    $f(x) = 0.397887$ at $(-\pi, 12.275)$, $(\pi, 2.275)$, and $(9.42478, 2.475)$.

* Input Domain: This function is usually evaluated on the square  x1 in  [-5, 10] x x2 in [0, 15].

```{python}
from spotPython.fun.objectivefunctions import analytical
lower = np.array([-5,-0])
upper = np.array([10,15])
```

```{python}
fun = analytical(seed=123).fun_branin
```

## The Optimizer

* Differential Evalution from the `scikit.optimize` package, see [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution) is the default optimizer for the search on the surrogate.
* Other optimiers that are available in `spotPython`:
    * `dual_annealing`
    *  `direct`
    * `shgo`
    * `basinhopping`, see [https://docs.scipy.org/doc/scipy/reference/optimize.html#global-optimization](https://docs.scipy.org/doc/scipy/reference/optimize.html#global-optimization).

* These can be selected as follows:

    ` surrogate_control = "model_optimizer": differential_evolution`

* We will use `differential_evolution`.
* The optimizer can use `1000` evaluations. This value will be passed to the `differential_evolution` method, which has the argument `maxiter` (int). It defines the maximum number of generations over which the entire differential evolution population is evolved, see [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution)


:::{.callout-note}
#### TensorBoard

Similar to the one-dimensional case, which was introduced in Section @sec-visualizing-tensorboard-01, we can use TensorBoard to monitor the progress of the optimization. We will use the same code, only the prefix is different:

```{python}
from spotPython.utils.file import get_experiment_name
from spotPython.utils.init import fun_control_init
from spotPython.utils.file import get_spot_tensorboard_path

PREFIX = "05_DE_"
experiment_name = get_experiment_name(prefix=PREFIX)
print(experiment_name)

fun_control = fun_control_init(
    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name))
```

:::

```{python}
spot_de = spot.Spot(fun=fun,
                   lower = lower,
                   upper = upper,
                   fun_evals = 20,
                   max_time = inf,
                   seed=125,
                   noise=False,
                   show_models= False,
                   design_control={"init_size": 10},
                   surrogate_control={"n_theta": len(lower),
                                      "model_optimizer": differential_evolution,
                                      "model_fun_evals": 1000,
                                      },
                  fun_control=fun_control)
spot_de.run()
```

### TensorBoard

Now we can start TensorBoard in the background with the following command:

```{raw}
tensorboard --logdir="./runs"
```

We can access the TensorBoard web server with the following URL:

```{raw}
http://localhost:6006/
```

The TensorBoard plot illustrates how `spotPython` can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate $\theta$ of the Kriging surrogate is plotted against the number of optimization steps.

![TensorBoard visualization of the spotPython optimization process and the surrogate model.](figures_static/05_tensorboard_01.png){width="100%"}


## Print the Results

```{python}
spot_de.print_results()
```

## Show the Progress

```{python}
spot_de.plot_progress(log_y=True)
```

```{python}
spot_de.surrogate.plot()
```

## Exercises


### `dual_annealing`

* Describe the optimization algorithm
* Use the algorithm as an optimizer on the surrogate

### `direct`

* Describe the optimization algorithm
* Use the algorithm as an optimizer on the surrogate

### `shgo`

* Describe the optimization algorithm
* Use the algorithm as an optimizer on the surrogate

### `basinhopping`

* Describe the optimization algorithm
* Use the algorithm as an optimizer on the surrogate

### Performance Comparison

Compare the performance and run time of the 5 different optimizers:

    * `differential_evolution`
    * `dual_annealing`
    *  `direct`
    * `shgo`
    * `basinhopping`.

The Branin function has three global minima:

  * $f(x) = 0.397887$  at 
    * $(-\pi, 12.275)$, 
    * $(\pi, 2.275)$, and 
    * $(9.42478, 2.475)$.    
  * Which optima are found by the optimizers? Does the `seed` change this behavior?

