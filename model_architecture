digraph {
	graph [size="40.949999999999996,40.949999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	14612400880 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	14612287584 -> 14941246624 [dir=none]
	14941246624 [label="input
 (1, 2)" fillcolor=orange]
	14612287584 -> 14941244544 [dir=none]
	14941244544 [label="weight
 (1, 2)" fillcolor=orange]
	14612287584 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612466848 -> 14612287584
	14612466848 -> 14941246704 [dir=none]
	14941246704 [label="other
 (1, 2)" fillcolor=orange]
	14612466848 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14563469840 -> 14612466848
	14563469840 -> 14941246464 [dir=none]
	14941246464 [label="input
 (1, 2)" fillcolor=orange]
	14563469840 -> 14941244384 [dir=none]
	14941244384 [label="weight
 (2, 2)" fillcolor=orange]
	14563469840 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612471264 -> 14563469840
	14612471264 -> 14941246544 [dir=none]
	14941246544 [label="other
 (1, 2)" fillcolor=orange]
	14612471264 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612488080 -> 14612471264
	14612488080 -> 14941246304 [dir=none]
	14941246304 [label="input
 (1, 2)" fillcolor=orange]
	14612488080 -> 14941244224 [dir=none]
	14941244224 [label="weight
 (2, 2)" fillcolor=orange]
	14612488080 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612481696 -> 14612488080
	14612481696 -> 14941246384 [dir=none]
	14941246384 [label="other
 (1, 2)" fillcolor=orange]
	14612481696 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612506576 -> 14612481696
	14612506576 -> 14941246144 [dir=none]
	14941246144 [label="input
 (1, 2)" fillcolor=orange]
	14612506576 -> 14941244064 [dir=none]
	14941244064 [label="weight
 (2, 2)" fillcolor=orange]
	14612506576 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612506768 -> 14612506576
	14612506768 -> 14941246224 [dir=none]
	14941246224 [label="other
 (1, 2)" fillcolor=orange]
	14612506768 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612506720 -> 14612506768
	14612506720 -> 14941244784 [dir=none]
	14941244784 [label="input
 (1, 4)" fillcolor=orange]
	14612506720 -> 14941243904 [dir=none]
	14941243904 [label="weight
 (2, 4)" fillcolor=orange]
	14612506720 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612507104 -> 14612506720
	14612507104 -> 14941244944 [dir=none]
	14941244944 [label="other
 (1, 4)" fillcolor=orange]
	14612507104 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612507056 -> 14612507104
	14612507056 -> 14941246064 [dir=none]
	14941246064 [label="input
 (1, 4)" fillcolor=orange]
	14612507056 -> 14941243744 [dir=none]
	14941243744 [label="weight
 (4, 4)" fillcolor=orange]
	14612507056 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612507392 -> 14612507056
	14612507392 -> 14941245984 [dir=none]
	14941245984 [label="other
 (1, 4)" fillcolor=orange]
	14612507392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612507344 -> 14612507392
	14612507344 -> 14941241904 [dir=none]
	14941241904 [label="input
 (1, 4)" fillcolor=orange]
	14612507344 -> 14941243584 [dir=none]
	14941243584 [label="weight
 (4, 4)" fillcolor=orange]
	14612507344 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612507680 -> 14612507344
	14612507680 -> 14941241744 [dir=none]
	14941241744 [label="other
 (1, 4)" fillcolor=orange]
	14612507680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612507632 -> 14612507680
	14612507632 -> 14941245584 [dir=none]
	14941245584 [label="input
 (1, 4)" fillcolor=orange]
	14612507632 -> 14941243424 [dir=none]
	14941243424 [label="weight
 (4, 4)" fillcolor=orange]
	14612507632 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612507968 -> 14612507632
	14612507968 -> 14941245744 [dir=none]
	14941245744 [label="other
 (1, 4)" fillcolor=orange]
	14612507968 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612507920 -> 14612507968
	14612507920 -> 14941241504 [dir=none]
	14941241504 [label="input
 (1, 8)" fillcolor=orange]
	14612507920 -> 14941243264 [dir=none]
	14941243264 [label="weight
 (4, 8)" fillcolor=orange]
	14612507920 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612508256 -> 14612507920
	14612508256 -> 14941241424 [dir=none]
	14941241424 [label="other
 (1, 8)" fillcolor=orange]
	14612508256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612508208 -> 14612508256
	14612508208 -> 14941245904 [dir=none]
	14941245904 [label="input
 (1, 8)" fillcolor=orange]
	14612508208 -> 14941243104 [dir=none]
	14941243104 [label="weight
 (8, 8)" fillcolor=orange]
	14612508208 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612508544 -> 14612508208
	14612508544 -> 14941241584 [dir=none]
	14941241584 [label="other
 (1, 8)" fillcolor=orange]
	14612508544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612508496 -> 14612508544
	14612508496 -> 14941245104 [dir=none]
	14941245104 [label="input
 (1, 8)" fillcolor=orange]
	14612508496 -> 14941242944 [dir=none]
	14941242944 [label="weight
 (8, 8)" fillcolor=orange]
	14612508496 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612508832 -> 14612508496
	14612508832 -> 14941245664 [dir=none]
	14941245664 [label="other
 (1, 8)" fillcolor=orange]
	14612508832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612508784 -> 14612508832
	14612508784 -> 14612258304 [dir=none]
	14612258304 [label="input
 (1, 16)" fillcolor=orange]
	14612508784 -> 14941242784 [dir=none]
	14941242784 [label="weight
 (8, 16)" fillcolor=orange]
	14612508784 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612509120 -> 14612508784
	14612509120 -> 14612265984 [dir=none]
	14612265984 [label="other
 (1, 16)" fillcolor=orange]
	14612509120 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612509072 -> 14612509120
	14612509072 -> 14612264864 [dir=none]
	14612264864 [label="input
 (1, 16)" fillcolor=orange]
	14612509072 -> 14941241984 [dir=none]
	14941241984 [label="weight
 (16, 16)" fillcolor=orange]
	14612509072 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612509360 -> 14612509072
	14612509360 -> 14612264544 [dir=none]
	14612264544 [label="other
 (1, 16)" fillcolor=orange]
	14612509360 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612509408 -> 14612509360
	14612509408 -> 14612251024 [dir=none]
	14612251024 [label="input
 (1, 32)" fillcolor=orange]
	14612509408 -> 14941241664 [dir=none]
	14941241664 [label="weight
 (16, 32)" fillcolor=orange]
	14612509408 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612509648 -> 14612509408
	14612509648 -> 14612250864 [dir=none]
	14612250864 [label="other
 (1, 32)" fillcolor=orange]
	14612509648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14612509696 -> 14612509648
	14612509696 -> 14423404976 [dir=none]
	14423404976 [label="input
 (1, 10)" fillcolor=orange]
	14612509696 -> 14941242464 [dir=none]
	14941242464 [label="weight
 (32, 10)" fillcolor=orange]
	14612509696 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14612509936 -> 14612509696
	14612509936 [label=ToCopyBackward0]
	14612363440 -> 14612509936
	14606259600 [label="
 (1, 10)" fillcolor=lightblue]
	14606259600 -> 14612363440
	14612363440 [label=AccumulateGrad]
	14612510032 -> 14612509696
	14941242464 [label="layers.0.weight
 (32, 10)" fillcolor=lightblue]
	14941242464 -> 14612510032
	14612510032 [label=AccumulateGrad]
	14612509792 -> 14612509696
	14941242544 [label="layers.0.bias
 (32)" fillcolor=lightblue]
	14941242544 -> 14612509792
	14612509792 [label=AccumulateGrad]
	14612509744 -> 14612509408
	14941241664 [label="layers.3.weight
 (16, 32)" fillcolor=lightblue]
	14941241664 -> 14612509744
	14612509744 [label=AccumulateGrad]
	14612509504 -> 14612509408
	14941242624 [label="layers.3.bias
 (16)" fillcolor=lightblue]
	14941242624 -> 14612509504
	14612509504 [label=AccumulateGrad]
	14612509456 -> 14612509072
	14941241984 [label="layers.6.weight
 (16, 16)" fillcolor=lightblue]
	14941241984 -> 14612509456
	14612509456 [label=AccumulateGrad]
	14612509216 -> 14612509072
	14941242704 [label="layers.6.bias
 (16)" fillcolor=lightblue]
	14941242704 -> 14612509216
	14612509216 [label=AccumulateGrad]
	14612509168 -> 14612508784
	14941242784 [label="layers.9.weight
 (8, 16)" fillcolor=lightblue]
	14941242784 -> 14612509168
	14612509168 [label=AccumulateGrad]
	14612508928 -> 14612508784
	14941242864 [label="layers.9.bias
 (8)" fillcolor=lightblue]
	14941242864 -> 14612508928
	14612508928 [label=AccumulateGrad]
	14612508880 -> 14612508496
	14941242944 [label="layers.12.weight
 (8, 8)" fillcolor=lightblue]
	14941242944 -> 14612508880
	14612508880 [label=AccumulateGrad]
	14612508640 -> 14612508496
	14941243024 [label="layers.12.bias
 (8)" fillcolor=lightblue]
	14941243024 -> 14612508640
	14612508640 [label=AccumulateGrad]
	14612508592 -> 14612508208
	14941243104 [label="layers.15.weight
 (8, 8)" fillcolor=lightblue]
	14941243104 -> 14612508592
	14612508592 [label=AccumulateGrad]
	14612508352 -> 14612508208
	14941243184 [label="layers.15.bias
 (8)" fillcolor=lightblue]
	14941243184 -> 14612508352
	14612508352 [label=AccumulateGrad]
	14612508304 -> 14612507920
	14941243264 [label="layers.18.weight
 (4, 8)" fillcolor=lightblue]
	14941243264 -> 14612508304
	14612508304 [label=AccumulateGrad]
	14612508064 -> 14612507920
	14941243344 [label="layers.18.bias
 (4)" fillcolor=lightblue]
	14941243344 -> 14612508064
	14612508064 [label=AccumulateGrad]
	14612508016 -> 14612507632
	14941243424 [label="layers.21.weight
 (4, 4)" fillcolor=lightblue]
	14941243424 -> 14612508016
	14612508016 [label=AccumulateGrad]
	14612507776 -> 14612507632
	14941243504 [label="layers.21.bias
 (4)" fillcolor=lightblue]
	14941243504 -> 14612507776
	14612507776 [label=AccumulateGrad]
	14612507728 -> 14612507344
	14941243584 [label="layers.24.weight
 (4, 4)" fillcolor=lightblue]
	14941243584 -> 14612507728
	14612507728 [label=AccumulateGrad]
	14612507488 -> 14612507344
	14941243664 [label="layers.24.bias
 (4)" fillcolor=lightblue]
	14941243664 -> 14612507488
	14612507488 [label=AccumulateGrad]
	14612507440 -> 14612507056
	14941243744 [label="layers.27.weight
 (4, 4)" fillcolor=lightblue]
	14941243744 -> 14612507440
	14612507440 [label=AccumulateGrad]
	14612507200 -> 14612507056
	14941243824 [label="layers.27.bias
 (4)" fillcolor=lightblue]
	14941243824 -> 14612507200
	14612507200 [label=AccumulateGrad]
	14612507152 -> 14612506720
	14941243904 [label="layers.30.weight
 (2, 4)" fillcolor=lightblue]
	14941243904 -> 14612507152
	14612507152 [label=AccumulateGrad]
	14612506912 -> 14612506720
	14941243984 [label="layers.30.bias
 (2)" fillcolor=lightblue]
	14941243984 -> 14612506912
	14612506912 [label=AccumulateGrad]
	14612506816 -> 14612506576
	14941244064 [label="layers.33.weight
 (2, 2)" fillcolor=lightblue]
	14941244064 -> 14612506816
	14612506816 [label=AccumulateGrad]
	14612504512 -> 14612506576
	14941244144 [label="layers.33.bias
 (2)" fillcolor=lightblue]
	14941244144 -> 14612504512
	14612504512 [label=AccumulateGrad]
	14612480352 -> 14612488080
	14941244224 [label="layers.36.weight
 (2, 2)" fillcolor=lightblue]
	14941244224 -> 14612480352
	14612480352 [label=AccumulateGrad]
	14612499040 -> 14612488080
	14941244304 [label="layers.36.bias
 (2)" fillcolor=lightblue]
	14941244304 -> 14612499040
	14612499040 [label=AccumulateGrad]
	14612470880 -> 14563469840
	14941244384 [label="layers.39.weight
 (2, 2)" fillcolor=lightblue]
	14941244384 -> 14612470880
	14612470880 [label=AccumulateGrad]
	14612491776 -> 14563469840
	14941244464 [label="layers.39.bias
 (2)" fillcolor=lightblue]
	14941244464 -> 14612491776
	14612491776 [label=AccumulateGrad]
	14606033648 -> 14612287584
	14941244544 [label="layers.42.weight
 (1, 2)" fillcolor=lightblue]
	14941244544 -> 14606033648
	14606033648 [label=AccumulateGrad]
	14594691504 -> 14612287584
	14941244624 [label="layers.42.bias
 (1)" fillcolor=lightblue]
	14941244624 -> 14594691504
	14594691504 [label=AccumulateGrad]
	14612287584 -> 14612400880
}
