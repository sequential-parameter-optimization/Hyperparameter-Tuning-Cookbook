digraph {
	graph [size="23.849999999999998,23.849999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	14985498768 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	14585087360 -> 14660299552 [dir=none]
	14660299552 [label="input
 (1, 2)" fillcolor=orange]
	14585087360 -> 14985510128 [dir=none]
	14985510128 [label="weight
 (1, 2)" fillcolor=orange]
	14585087360 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14899915904 -> 14585087360
	14899915904 -> 14660299312 [dir=none]
	14660299312 [label="condition
 (1, 2)" fillcolor=orange]
	14899915904 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14660425072 -> 14899915904
	14660425072 -> 14660299392 [dir=none]
	14660299392 [label="input
 (1, 2)" fillcolor=orange]
	14660425072 -> 14985498448 [dir=none]
	14985498448 [label="weight
 (2, 2)" fillcolor=orange]
	14660425072 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14660425216 -> 14660425072
	14660425216 -> 14660299152 [dir=none]
	14660299152 [label="condition
 (1, 2)" fillcolor=orange]
	14660425216 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14660425408 -> 14660425216
	14660425408 -> 14660299232 [dir=none]
	14660299232 [label="input
 (1, 2)" fillcolor=orange]
	14660425408 -> 14985498208 [dir=none]
	14985498208 [label="weight
 (2, 2)" fillcolor=orange]
	14660425408 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14660425696 -> 14660425408
	14660425696 -> 14660298192 [dir=none]
	14660298192 [label="condition
 (1, 2)" fillcolor=orange]
	14660425696 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14660425744 -> 14660425696
	14660425744 -> 14660299072 [dir=none]
	14660299072 [label="input
 (1, 4)" fillcolor=orange]
	14660425744 -> 14985509808 [dir=none]
	14985509808 [label="weight
 (2, 4)" fillcolor=orange]
	14660425744 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14660426032 -> 14660425744
	14660426032 -> 14985507568 [dir=none]
	14985507568 [label="condition
 (1, 4)" fillcolor=orange]
	14660426032 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14660426080 -> 14660426032
	14660426080 -> 14660297792 [dir=none]
	14660297792 [label="input
 (1, 4)" fillcolor=orange]
	14660426080 -> 14985509488 [dir=none]
	14985509488 [label="weight
 (4, 4)" fillcolor=orange]
	14660426080 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14660426368 -> 14660426080
	14660426368 -> 14985499168 [dir=none]
	14985499168 [label="condition
 (1, 4)" fillcolor=orange]
	14660426368 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14660426416 -> 14660426368
	14660426416 -> 14985508848 [dir=none]
	14985508848 [label="input
 (1, 8)" fillcolor=orange]
	14660426416 -> 14985509568 [dir=none]
	14985509568 [label="weight
 (4, 8)" fillcolor=orange]
	14660426416 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14660426704 -> 14660426416
	14660426704 -> 14985458496 [dir=none]
	14985458496 [label="condition
 (1, 8)" fillcolor=orange]
	14660426704 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14660426752 -> 14660426704
	14660426752 -> 14660134032 [dir=none]
	14660134032 [label="input
 (1, 10)" fillcolor=orange]
	14660426752 -> 14985508688 [dir=none]
	14985508688 [label="weight
 (8, 10)" fillcolor=orange]
	14660426752 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14660427040 -> 14660426752
	14660427040 [label=ToCopyBackward0]
	14660427088 -> 14660427040
	14584700896 [label="
 (1, 10)" fillcolor=lightblue]
	14584700896 -> 14660427088
	14660427088 [label=AccumulateGrad]
	14660426944 -> 14660426752
	14985508688 [label="layers.0.weight
 (8, 10)" fillcolor=lightblue]
	14985508688 -> 14660426944
	14660426944 [label=AccumulateGrad]
	14660426992 -> 14660426752
	14985508528 [label="layers.0.bias
 (8)" fillcolor=lightblue]
	14985508528 -> 14660426992
	14660426992 [label=AccumulateGrad]
	14660426896 -> 14660426704
	14660426896 -> 14660502704 [dir=none]
	14660502704 [label="other
 ()" fillcolor=orange]
	14660426896 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14660426752 -> 14660426896
	14660426608 -> 14660426416
	14985509568 [label="layers.3.weight
 (4, 8)" fillcolor=lightblue]
	14985509568 -> 14660426608
	14660426608 [label=AccumulateGrad]
	14660426656 -> 14660426416
	14985509648 [label="layers.3.bias
 (4)" fillcolor=lightblue]
	14985509648 -> 14660426656
	14660426656 [label=AccumulateGrad]
	14660426560 -> 14660426368
	14660426560 -> 14660503504 [dir=none]
	14660503504 [label="other
 ()" fillcolor=orange]
	14660426560 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14660426416 -> 14660426560
	14660426272 -> 14660426080
	14985509488 [label="layers.6.weight
 (4, 4)" fillcolor=lightblue]
	14985509488 -> 14660426272
	14660426272 [label=AccumulateGrad]
	14660426320 -> 14660426080
	14985509728 [label="layers.6.bias
 (4)" fillcolor=lightblue]
	14985509728 -> 14660426320
	14660426320 [label=AccumulateGrad]
	14660426224 -> 14660426032
	14660426224 -> 14660504304 [dir=none]
	14660504304 [label="other
 ()" fillcolor=orange]
	14660426224 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14660426080 -> 14660426224
	14660425936 -> 14660425744
	14985509808 [label="layers.9.weight
 (2, 4)" fillcolor=lightblue]
	14985509808 -> 14660425936
	14660425936 [label=AccumulateGrad]
	14660425984 -> 14660425744
	14985509888 [label="layers.9.bias
 (2)" fillcolor=lightblue]
	14985509888 -> 14660425984
	14660425984 [label=AccumulateGrad]
	14660425888 -> 14660425696
	14660425888 -> 14660505104 [dir=none]
	14660505104 [label="other
 ()" fillcolor=orange]
	14660425888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14660425744 -> 14660425888
	14660425600 -> 14660425408
	14985498208 [label="layers.12.weight
 (2, 2)" fillcolor=lightblue]
	14985498208 -> 14660425600
	14660425600 [label=AccumulateGrad]
	14660425648 -> 14660425408
	14985509968 [label="layers.12.bias
 (2)" fillcolor=lightblue]
	14985509968 -> 14660425648
	14660425648 [label=AccumulateGrad]
	14660425552 -> 14660425216
	14660425552 -> 14660505904 [dir=none]
	14660505904 [label="other
 ()" fillcolor=orange]
	14660425552 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14660425408 -> 14660425552
	14660425264 -> 14660425072
	14985498448 [label="layers.15.weight
 (2, 2)" fillcolor=lightblue]
	14985498448 -> 14660425264
	14660425264 [label=AccumulateGrad]
	14660425312 -> 14660425072
	14985510048 [label="layers.15.bias
 (2)" fillcolor=lightblue]
	14985510048 -> 14660425312
	14660425312 [label=AccumulateGrad]
	14660425120 -> 14899915904
	14660425120 -> 14660506704 [dir=none]
	14660506704 [label="other
 ()" fillcolor=orange]
	14660425120 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14660425072 -> 14660425120
	14659570848 -> 14585087360
	14985510128 [label="layers.18.weight
 (1, 2)" fillcolor=lightblue]
	14985510128 -> 14659570848
	14659570848 [label=AccumulateGrad]
	14659571184 -> 14585087360
	14985510208 [label="layers.18.bias
 (1)" fillcolor=lightblue]
	14985510208 -> 14659571184
	14659571184 [label=AccumulateGrad]
	14585087360 -> 14985498768
}
