digraph {
	graph [size="18.3,18.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	6282047440 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	13182410928 [label=LinearBackward0]
	15358346864 -> 13182410928
	15358346864 [label=WhereBackward0]
	13192118672 -> 15358346864
	13192118672 [label=LinearBackward0]
	13192121360 -> 13192118672
	13192121360 [label=WhereBackward0]
	13192121504 -> 13192121360
	13192121504 [label=LinearBackward0]
	13192121648 -> 13192121504
	13192121648 [label=WhereBackward0]
	13192121840 -> 13192121648
	13192121840 [label=LinearBackward0]
	13192121984 -> 13192121840
	13192121984 [label=WhereBackward0]
	13192122176 -> 13192121984
	13192122176 [label=LinearBackward0]
	13192122320 -> 13192122176
	13192122320 [label=WhereBackward0]
	13192122512 -> 13192122320
	13192122512 [label=LinearBackward0]
	13192122656 -> 13192122512
	13192122656 [label=WhereBackward0]
	13192122848 -> 13192122656
	13192122848 [label=LinearBackward0]
	13192122992 -> 13192122848
	13192122992 [label=WhereBackward0]
	13192123184 -> 13192122992
	13192123184 [label=LinearBackward0]
	13192123328 -> 13192123184
	13192123328 [label=ToCopyBackward0]
	13192123520 -> 13192123328
	13182090800 [label="
 (1, 10)" fillcolor=lightblue]
	13182090800 -> 13192123520
	13192123520 [label=AccumulateGrad]
	13192123280 -> 13192123184
	13182091120 [label="layers.0.weight
 (320, 10)" fillcolor=lightblue]
	13182091120 -> 13192123280
	13192123280 [label=AccumulateGrad]
	13192123232 -> 13192123184
	13182542272 [label="layers.0.bias
 (320)" fillcolor=lightblue]
	13182542272 -> 13192123232
	13192123232 [label=AccumulateGrad]
	13192123136 -> 13192122992
	13192123136 [label=MulBackward0]
	13192123184 -> 13192123136
	13192122944 -> 13192122848
	13182542352 [label="layers.3.weight
 (160, 320)" fillcolor=lightblue]
	13182542352 -> 13192122944
	13192122944 [label=AccumulateGrad]
	13192122896 -> 13192122848
	13182542432 [label="layers.3.bias
 (160)" fillcolor=lightblue]
	13182542432 -> 13192122896
	13192122896 [label=AccumulateGrad]
	13192122800 -> 13192122656
	13192122800 [label=MulBackward0]
	13192122848 -> 13192122800
	13192122608 -> 13192122512
	13182541792 [label="layers.6.weight
 (320, 160)" fillcolor=lightblue]
	13182541792 -> 13192122608
	13192122608 [label=AccumulateGrad]
	13192122560 -> 13192122512
	13182542512 [label="layers.6.bias
 (320)" fillcolor=lightblue]
	13182542512 -> 13192122560
	13192122560 [label=AccumulateGrad]
	13192122464 -> 13192122320
	13192122464 [label=MulBackward0]
	13192122512 -> 13192122464
	13192122272 -> 13192122176
	13182542592 [label="layers.9.weight
 (160, 320)" fillcolor=lightblue]
	13182542592 -> 13192122272
	13192122272 [label=AccumulateGrad]
	13192122224 -> 13192122176
	13182542672 [label="layers.9.bias
 (160)" fillcolor=lightblue]
	13182542672 -> 13192122224
	13192122224 [label=AccumulateGrad]
	13192122128 -> 13192121984
	13192122128 [label=MulBackward0]
	13192122176 -> 13192122128
	13192121936 -> 13192121840
	13182542752 [label="layers.12.weight
 (160, 160)" fillcolor=lightblue]
	13182542752 -> 13192121936
	13192121936 [label=AccumulateGrad]
	13192121888 -> 13192121840
	13182542832 [label="layers.12.bias
 (160)" fillcolor=lightblue]
	13182542832 -> 13192121888
	13192121888 [label=AccumulateGrad]
	13192121792 -> 13192121648
	13192121792 [label=MulBackward0]
	13192121840 -> 13192121792
	13192121600 -> 13192121504
	13182542912 [label="layers.15.weight
 (80, 160)" fillcolor=lightblue]
	13182542912 -> 13192121600
	13192121600 [label=AccumulateGrad]
	13192121552 -> 13192121504
	13182542992 [label="layers.15.bias
 (80)" fillcolor=lightblue]
	13182542992 -> 13192121552
	13192121552 [label=AccumulateGrad]
	13192121456 -> 13192121360
	13192121456 [label=MulBackward0]
	13192121504 -> 13192121456
	13192121168 -> 13192118672
	13182283808 [label="layers.18.weight
 (80, 80)" fillcolor=lightblue]
	13182283808 -> 13192121168
	13192121168 [label=AccumulateGrad]
	13192121264 -> 13192118672
	15357701136 [label="layers.18.bias
 (80)" fillcolor=lightblue]
	15357701136 -> 13192121264
	13192121264 [label=AccumulateGrad]
	13192120928 -> 15358346864
	13192120928 [label=MulBackward0]
	13192118672 -> 13192120928
	13182413712 -> 13182410928
	13182543072 [label="layers.21.weight
 (1, 80)" fillcolor=lightblue]
	13182543072 -> 13182413712
	13182413712 [label=AccumulateGrad]
	13192121072 -> 13182410928
	13182543152 [label="layers.21.bias
 (1)" fillcolor=lightblue]
	13182543152 -> 13192121072
	13192121072 [label=AccumulateGrad]
	13182410928 -> 6282047440
}
