digraph {
	graph [size="18.3,18.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	16063686416 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	16258518464 [label=LinearBackward0]
	16063603488 -> 16258518464
	16063603488 [label=WhereBackward0]
	16063600752 -> 16063603488
	16063600752 [label=LinearBackward0]
	16063157232 -> 16063600752
	16063157232 [label=WhereBackward0]
	16259259872 -> 16063157232
	16259259872 [label=LinearBackward0]
	16259259968 -> 16259259872
	16259259968 [label=WhereBackward0]
	16259260160 -> 16259259968
	16259260160 [label=LinearBackward0]
	16259260304 -> 16259260160
	16259260304 [label=WhereBackward0]
	16259260496 -> 16259260304
	16259260496 [label=LinearBackward0]
	16259260640 -> 16259260496
	16259260640 [label=WhereBackward0]
	16259260832 -> 16259260640
	16259260832 [label=LinearBackward0]
	16259260976 -> 16259260832
	16259260976 [label=WhereBackward0]
	16259261168 -> 16259260976
	16259261168 [label=LinearBackward0]
	16259261312 -> 16259261168
	16259261312 [label=WhereBackward0]
	16259261504 -> 16259261312
	16259261504 [label=LinearBackward0]
	16259261648 -> 16259261504
	16259261648 [label=ToCopyBackward0]
	16259261840 -> 16259261648
	16063686656 [label="
 (1, 10)" fillcolor=lightblue]
	16063686656 -> 16259261840
	16259261840 [label=AccumulateGrad]
	16259261600 -> 16259261504
	16063688096 [label="layers.0.weight
 (320, 10)" fillcolor=lightblue]
	16063688096 -> 16259261600
	16259261600 [label=AccumulateGrad]
	16259261552 -> 16259261504
	16063688176 [label="layers.0.bias
 (320)" fillcolor=lightblue]
	16063688176 -> 16259261552
	16259261552 [label=AccumulateGrad]
	16259261456 -> 16259261312
	16259261456 [label=MulBackward0]
	16259261504 -> 16259261456
	16259261264 -> 16259261168
	16063688336 [label="layers.3.weight
 (160, 320)" fillcolor=lightblue]
	16063688336 -> 16259261264
	16259261264 [label=AccumulateGrad]
	16259261216 -> 16259261168
	16063688416 [label="layers.3.bias
 (160)" fillcolor=lightblue]
	16063688416 -> 16259261216
	16259261216 [label=AccumulateGrad]
	16259261120 -> 16259260976
	16259261120 [label=MulBackward0]
	16259261168 -> 16259261120
	16259260928 -> 16259260832
	16063688256 [label="layers.6.weight
 (320, 160)" fillcolor=lightblue]
	16063688256 -> 16259260928
	16259260928 [label=AccumulateGrad]
	16259260880 -> 16259260832
	16063688496 [label="layers.6.bias
 (320)" fillcolor=lightblue]
	16063688496 -> 16259260880
	16259260880 [label=AccumulateGrad]
	16259260784 -> 16259260640
	16259260784 [label=MulBackward0]
	16259260832 -> 16259260784
	16259260592 -> 16259260496
	16063688576 [label="layers.9.weight
 (160, 320)" fillcolor=lightblue]
	16063688576 -> 16259260592
	16259260592 [label=AccumulateGrad]
	16259260544 -> 16259260496
	16063688656 [label="layers.9.bias
 (160)" fillcolor=lightblue]
	16063688656 -> 16259260544
	16259260544 [label=AccumulateGrad]
	16259260448 -> 16259260304
	16259260448 [label=MulBackward0]
	16259260496 -> 16259260448
	16259260256 -> 16259260160
	16063688736 [label="layers.12.weight
 (160, 160)" fillcolor=lightblue]
	16063688736 -> 16259260256
	16259260256 [label=AccumulateGrad]
	16259260208 -> 16259260160
	16063688816 [label="layers.12.bias
 (160)" fillcolor=lightblue]
	16063688816 -> 16259260208
	16259260208 [label=AccumulateGrad]
	16259260112 -> 16259259968
	16259260112 [label=MulBackward0]
	16259260160 -> 16259260112
	16259259920 -> 16259259872
	16063688896 [label="layers.15.weight
 (80, 160)" fillcolor=lightblue]
	16063688896 -> 16259259920
	16259259920 [label=AccumulateGrad]
	16259259728 -> 16259259872
	16063688976 [label="layers.15.bias
 (80)" fillcolor=lightblue]
	16063688976 -> 16259259728
	16259259728 [label=AccumulateGrad]
	16259259776 -> 16063157232
	16259259776 [label=MulBackward0]
	16259259872 -> 16259259776
	16259259632 -> 16063600752
	16063689056 [label="layers.18.weight
 (80, 80)" fillcolor=lightblue]
	16063689056 -> 16259259632
	16259259632 [label=AccumulateGrad]
	16259259488 -> 16063600752
	16063689136 [label="layers.18.bias
 (80)" fillcolor=lightblue]
	16063689136 -> 16259259488
	16259259488 [label=AccumulateGrad]
	16063594896 -> 16063603488
	16063594896 [label=MulBackward0]
	16063600752 -> 16063594896
	16063603680 -> 16258518464
	16063617520 [label="layers.21.weight
 (1, 80)" fillcolor=lightblue]
	16063617520 -> 16063603680
	16063603680 [label=AccumulateGrad]
	16063603296 -> 16258518464
	16063689216 [label="layers.21.bias
 (1)" fillcolor=lightblue]
	16063689216 -> 16063603296
	16063603296 [label=AccumulateGrad]
	16258518464 -> 16063686416
}
