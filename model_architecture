digraph {
	graph [size="18.3,18.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	16105101328 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	13418921264 [label=LinearBackward0]
	16099991968 -> 13418921264
	16099991968 [label=WhereBackward0]
	16104715168 -> 16099991968
	16104715168 [label=LinearBackward0]
	16104944736 -> 16104715168
	16104944736 [label=WhereBackward0]
	16104942048 -> 16104944736
	16104942048 [label=LinearBackward0]
	16453407728 -> 16104942048
	16453407728 [label=WhereBackward0]
	16453407824 -> 16453407728
	16453407824 [label=LinearBackward0]
	16453408112 -> 16453407824
	16453408112 [label=WhereBackward0]
	16453408304 -> 16453408112
	16453408304 [label=LinearBackward0]
	16453408448 -> 16453408304
	16453408448 [label=WhereBackward0]
	16453408640 -> 16453408448
	16453408640 [label=LinearBackward0]
	16453408784 -> 16453408640
	16453408784 [label=WhereBackward0]
	16453408976 -> 16453408784
	16453408976 [label=LinearBackward0]
	16453409120 -> 16453408976
	16453409120 [label=WhereBackward0]
	16453409312 -> 16453409120
	16453409312 [label=LinearBackward0]
	16453409456 -> 16453409312
	16453409456 [label=ToCopyBackward0]
	16453409648 -> 16453409456
	16105098848 [label="
 (1, 10)" fillcolor=lightblue]
	16105098848 -> 16453409648
	16453409648 [label=AccumulateGrad]
	16453409408 -> 16453409312
	16105106368 [label="layers.0.weight
 (320, 10)" fillcolor=lightblue]
	16105106368 -> 16453409408
	16453409408 [label=AccumulateGrad]
	16453409360 -> 16453409312
	16105105888 [label="layers.0.bias
 (320)" fillcolor=lightblue]
	16105105888 -> 16453409360
	16453409360 [label=AccumulateGrad]
	16453409264 -> 16453409120
	16453409264 [label=MulBackward0]
	16453409312 -> 16453409264
	16453409072 -> 16453408976
	16105106528 [label="layers.3.weight
 (160, 320)" fillcolor=lightblue]
	16105106528 -> 16453409072
	16453409072 [label=AccumulateGrad]
	16453409024 -> 16453408976
	16105106608 [label="layers.3.bias
 (160)" fillcolor=lightblue]
	16105106608 -> 16453409024
	16453409024 [label=AccumulateGrad]
	16453408928 -> 16453408784
	16453408928 [label=MulBackward0]
	16453408976 -> 16453408928
	16453408736 -> 16453408640
	16105106448 [label="layers.6.weight
 (320, 160)" fillcolor=lightblue]
	16105106448 -> 16453408736
	16453408736 [label=AccumulateGrad]
	16453408688 -> 16453408640
	16105106688 [label="layers.6.bias
 (320)" fillcolor=lightblue]
	16105106688 -> 16453408688
	16453408688 [label=AccumulateGrad]
	16453408592 -> 16453408448
	16453408592 [label=MulBackward0]
	16453408640 -> 16453408592
	16453408400 -> 16453408304
	16105106768 [label="layers.9.weight
 (160, 320)" fillcolor=lightblue]
	16105106768 -> 16453408400
	16453408400 [label=AccumulateGrad]
	16453408352 -> 16453408304
	16105106848 [label="layers.9.bias
 (160)" fillcolor=lightblue]
	16105106848 -> 16453408352
	16453408352 [label=AccumulateGrad]
	16453408256 -> 16453408112
	16453408256 [label=MulBackward0]
	16453408304 -> 16453408256
	16453408064 -> 16453407824
	16105106928 [label="layers.12.weight
 (160, 160)" fillcolor=lightblue]
	16105106928 -> 16453408064
	16453408064 [label=AccumulateGrad]
	16453408016 -> 16453407824
	16105107008 [label="layers.12.bias
 (160)" fillcolor=lightblue]
	16105107008 -> 16453408016
	16453408016 [label=AccumulateGrad]
	16453407968 -> 16453407728
	16453407968 [label=MulBackward0]
	16453407824 -> 16453407968
	16453407680 -> 16104942048
	16105107088 [label="layers.15.weight
 (80, 160)" fillcolor=lightblue]
	16105107088 -> 16453407680
	16453407680 [label=AccumulateGrad]
	16453407584 -> 16104942048
	16105107168 [label="layers.15.bias
 (80)" fillcolor=lightblue]
	16105107168 -> 16453407584
	16453407584 [label=AccumulateGrad]
	16453407536 -> 16104944736
	16453407536 [label=MulBackward0]
	16104942048 -> 16453407536
	16104933984 -> 16104715168
	16104987040 [label="layers.18.weight
 (80, 80)" fillcolor=lightblue]
	16104987040 -> 16104933984
	16104933984 [label=AccumulateGrad]
	16104941904 -> 16104715168
	16105076400 [label="layers.18.bias
 (80)" fillcolor=lightblue]
	16105076400 -> 16104941904
	16104941904 [label=AccumulateGrad]
	16104841440 -> 16099991968
	16104841440 [label=MulBackward0]
	16104715168 -> 16104841440
	16104790224 -> 13418921264
	16105107248 [label="layers.21.weight
 (1, 80)" fillcolor=lightblue]
	16105107248 -> 16104790224
	16104790224 [label=AccumulateGrad]
	16104793440 -> 13418921264
	16105107328 [label="layers.21.bias
 (1)" fillcolor=lightblue]
	16105107328 -> 16104793440
	16104793440 [label=AccumulateGrad]
	13418921264 -> 16105101328
}
