digraph {
	graph [size="30.15,30.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5263120880 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	13450437024 -> 13471618608 [dir=none]
	13471618608 [label="input
 (1, 16)" fillcolor=orange]
	13450437024 -> 13471448144 [dir=none]
	13471448144 [label="weight
 (1, 16)" fillcolor=orange]
	13450437024 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13450638432 -> 13450437024
	13450638432 -> 13471618688 [dir=none]
	13471618688 [label="other
 (1, 16)" fillcolor=orange]
	13450638432 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13450158112 -> 13450638432
	13450158112 -> 13471618448 [dir=none]
	13471618448 [label="input
 (1, 16)" fillcolor=orange]
	13450158112 -> 13471447984 [dir=none]
	13471447984 [label="weight
 (16, 16)" fillcolor=orange]
	13450158112 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13419460016 -> 13450158112
	13419460016 -> 13471618528 [dir=none]
	13471618528 [label="other
 (1, 16)" fillcolor=orange]
	13419460016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471393984 -> 13419460016
	13471393984 -> 13471618288 [dir=none]
	13471618288 [label="input
 (1, 16)" fillcolor=orange]
	13471393984 -> 13471447664 [dir=none]
	13471447664 [label="weight
 (16, 16)" fillcolor=orange]
	13471393984 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471388032 -> 13471393984
	13471388032 -> 13471618368 [dir=none]
	13471618368 [label="other
 (1, 16)" fillcolor=orange]
	13471388032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471397776 -> 13471388032
	13471397776 -> 13471618128 [dir=none]
	13471618128 [label="input
 (1, 16)" fillcolor=orange]
	13471397776 -> 13471447344 [dir=none]
	13471447344 [label="weight
 (16, 16)" fillcolor=orange]
	13471397776 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471396720 -> 13471397776
	13471396720 -> 13471618208 [dir=none]
	13471618208 [label="other
 (1, 16)" fillcolor=orange]
	13471396720 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471387168 -> 13471396720
	13471387168 -> 13471617968 [dir=none]
	13471617968 [label="input
 (1, 32)" fillcolor=orange]
	13471387168 -> 13471447024 [dir=none]
	13471447024 [label="weight
 (16, 32)" fillcolor=orange]
	13471387168 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471396816 -> 13471387168
	13471396816 -> 13471618048 [dir=none]
	13471618048 [label="other
 (1, 32)" fillcolor=orange]
	13471396816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471393024 -> 13471396816
	13471393024 -> 13471617808 [dir=none]
	13471617808 [label="input
 (1, 32)" fillcolor=orange]
	13471393024 -> 13471445344 [dir=none]
	13471445344 [label="weight
 (32, 32)" fillcolor=orange]
	13471393024 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471392448 -> 13471393024
	13471392448 -> 13471617888 [dir=none]
	13471617888 [label="other
 (1, 32)" fillcolor=orange]
	13471392448 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471391152 -> 13471392448
	13471391152 -> 13471617648 [dir=none]
	13471617648 [label="input
 (1, 32)" fillcolor=orange]
	13471391152 -> 13471315872 [dir=none]
	13471315872 [label="weight
 (32, 32)" fillcolor=orange]
	13471391152 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471389664 -> 13471391152
	13471389664 -> 13471617728 [dir=none]
	13471617728 [label="other
 (1, 32)" fillcolor=orange]
	13471389664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471394512 -> 13471389664
	13471394512 -> 13471617488 [dir=none]
	13471617488 [label="input
 (1, 64)" fillcolor=orange]
	13471394512 -> 13471446224 [dir=none]
	13471446224 [label="weight
 (32, 64)" fillcolor=orange]
	13471394512 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471396048 -> 13471394512
	13471396048 -> 13471617568 [dir=none]
	13471617568 [label="other
 (1, 64)" fillcolor=orange]
	13471396048 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471396432 -> 13471396048
	13471396432 -> 13471617328 [dir=none]
	13471617328 [label="input
 (1, 64)" fillcolor=orange]
	13471396432 -> 13471445744 [dir=none]
	13471445744 [label="weight
 (64, 64)" fillcolor=orange]
	13471396432 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471391536 -> 13471396432
	13471391536 -> 13471617408 [dir=none]
	13471617408 [label="other
 (1, 64)" fillcolor=orange]
	13471391536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471394416 -> 13471391536
	13471394416 -> 13471617248 [dir=none]
	13471617248 [label="input
 (1, 128)" fillcolor=orange]
	13471394416 -> 13471445664 [dir=none]
	13471445664 [label="weight
 (64, 128)" fillcolor=orange]
	13471394416 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471393312 -> 13471394416
	13471393312 -> 13471617168 [dir=none]
	13471617168 [label="other
 (1, 128)" fillcolor=orange]
	13471393312 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13471399216 -> 13471393312
	13471399216 -> 13265773632 [dir=none]
	13265773632 [label="input
 (1, 10)" fillcolor=orange]
	13471399216 -> 13471445424 [dir=none]
	13471445424 [label="weight
 (128, 10)" fillcolor=orange]
	13471399216 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13471399600 -> 13471399216
	13471399600 [label=ToCopyBackward0]
	13471399552 -> 13471399600
	13471425440 [label="
 (1, 10)" fillcolor=lightblue]
	13471425440 -> 13471399552
	13471399552 [label=AccumulateGrad]
	13471399648 -> 13471399216
	13471445424 [label="layers.0.weight
 (128, 10)" fillcolor=lightblue]
	13471445424 -> 13471399648
	13471399648 [label=AccumulateGrad]
	13471387312 -> 13471399216
	13471444384 [label="layers.0.bias
 (128)" fillcolor=lightblue]
	13471444384 -> 13471387312
	13471387312 [label=AccumulateGrad]
	13471397248 -> 13471394416
	13471445664 [label="layers.3.weight
 (64, 128)" fillcolor=lightblue]
	13471445664 -> 13471397248
	13471397248 [label=AccumulateGrad]
	13471392064 -> 13471394416
	13471444704 [label="layers.3.bias
 (64)" fillcolor=lightblue]
	13471444704 -> 13471392064
	13471392064 [label=AccumulateGrad]
	13471394464 -> 13471396432
	13471445744 [label="layers.6.weight
 (64, 64)" fillcolor=lightblue]
	13471445744 -> 13471394464
	13471394464 [label=AccumulateGrad]
	13471385920 -> 13471396432
	13471445824 [label="layers.6.bias
 (64)" fillcolor=lightblue]
	13471445824 -> 13471385920
	13471385920 [label=AccumulateGrad]
	13471397728 -> 13471394512
	13471446224 [label="layers.9.weight
 (32, 64)" fillcolor=lightblue]
	13471446224 -> 13471397728
	13471397728 [label=AccumulateGrad]
	13471396096 -> 13471394512
	13471446464 [label="layers.9.bias
 (32)" fillcolor=lightblue]
	13471446464 -> 13471396096
	13471396096 [label=AccumulateGrad]
	13471393744 -> 13471391152
	13471315872 [label="layers.12.weight
 (32, 32)" fillcolor=lightblue]
	13471315872 -> 13471393744
	13471393744 [label=AccumulateGrad]
	13471391392 -> 13471391152
	13471446704 [label="layers.12.bias
 (32)" fillcolor=lightblue]
	13471446704 -> 13471391392
	13471391392 [label=AccumulateGrad]
	13471387072 -> 13471393024
	13471445344 [label="layers.15.weight
 (32, 32)" fillcolor=lightblue]
	13471445344 -> 13471387072
	13471387072 [label=AccumulateGrad]
	13471392640 -> 13471393024
	13471446944 [label="layers.15.bias
 (32)" fillcolor=lightblue]
	13471446944 -> 13471392640
	13471392640 [label=AccumulateGrad]
	13471390192 -> 13471387168
	13471447024 [label="layers.18.weight
 (16, 32)" fillcolor=lightblue]
	13471447024 -> 13471390192
	13471390192 [label=AccumulateGrad]
	13471395280 -> 13471387168
	13471447104 [label="layers.18.bias
 (16)" fillcolor=lightblue]
	13471447104 -> 13471395280
	13471395280 [label=AccumulateGrad]
	13471395424 -> 13471397776
	13471447344 [label="layers.21.weight
 (16, 16)" fillcolor=lightblue]
	13471447344 -> 13471395424
	13471395424 [label=AccumulateGrad]
	13471398832 -> 13471397776
	13471447584 [label="layers.21.bias
 (16)" fillcolor=lightblue]
	13471447584 -> 13471398832
	13471398832 [label=AccumulateGrad]
	13471396288 -> 13471393984
	13471447664 [label="layers.24.weight
 (16, 16)" fillcolor=lightblue]
	13471447664 -> 13471396288
	13471396288 [label=AccumulateGrad]
	13471384480 -> 13471393984
	13471447904 [label="layers.24.bias
 (16)" fillcolor=lightblue]
	13471447904 -> 13471384480
	13471384480 [label=AccumulateGrad]
	13419457472 -> 13450158112
	13471447984 [label="layers.27.weight
 (16, 16)" fillcolor=lightblue]
	13471447984 -> 13419457472
	13419457472 [label=AccumulateGrad]
	13419461984 -> 13450158112
	13471448064 [label="layers.27.bias
 (16)" fillcolor=lightblue]
	13471448064 -> 13419461984
	13419461984 [label=AccumulateGrad]
	13450440192 -> 13450437024
	13471448144 [label="layers.30.weight
 (1, 16)" fillcolor=lightblue]
	13471448144 -> 13450440192
	13450440192 [label=AccumulateGrad]
	13450429968 -> 13450437024
	13471448224 [label="layers.30.bias
 (1)" fillcolor=lightblue]
	13471448224 -> 13450429968
	13450429968 [label=AccumulateGrad]
	13450437024 -> 5263120880
}
