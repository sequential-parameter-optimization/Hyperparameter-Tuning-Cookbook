digraph {
	graph [size="30.15,30.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	14531582048 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	14057649008 -> 14278675024 [dir=none]
	14278675024 [label="input
 (1, 16)" fillcolor=orange]
	14057649008 -> 14531630160 [dir=none]
	14531630160 [label="weight
 (1, 16)" fillcolor=orange]
	14057649008 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14058025648 -> 14057649008
	14058025648 -> 14278675104 [dir=none]
	14278675104 [label="other
 (1, 16)" fillcolor=orange]
	14058025648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14057640608 -> 14058025648
	14057640608 -> 14278674864 [dir=none]
	14278674864 [label="input
 (1, 16)" fillcolor=orange]
	14057640608 -> 14531630000 [dir=none]
	14531630000 [label="weight
 (16, 16)" fillcolor=orange]
	14057640608 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278614832 -> 14057640608
	14278614832 -> 14278674944 [dir=none]
	14278674944 [label="other
 (1, 16)" fillcolor=orange]
	14278614832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278614784 -> 14278614832
	14278614784 -> 14278674704 [dir=none]
	14278674704 [label="input
 (1, 16)" fillcolor=orange]
	14278614784 -> 14531629840 [dir=none]
	14531629840 [label="weight
 (16, 16)" fillcolor=orange]
	14278614784 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278615072 -> 14278614784
	14278615072 -> 14278674784 [dir=none]
	14278674784 [label="other
 (1, 16)" fillcolor=orange]
	14278615072 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278615120 -> 14278615072
	14278615120 -> 14278674544 [dir=none]
	14278674544 [label="input
 (1, 16)" fillcolor=orange]
	14278615120 -> 14531629680 [dir=none]
	14531629680 [label="weight
 (16, 16)" fillcolor=orange]
	14278615120 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278615360 -> 14278615120
	14278615360 -> 14278674624 [dir=none]
	14278674624 [label="other
 (1, 16)" fillcolor=orange]
	14278615360 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278615408 -> 14278615360
	14278615408 -> 14278674384 [dir=none]
	14278674384 [label="input
 (1, 32)" fillcolor=orange]
	14278615408 -> 14531629520 [dir=none]
	14531629520 [label="weight
 (16, 32)" fillcolor=orange]
	14278615408 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278615648 -> 14278615408
	14278615648 -> 14278674464 [dir=none]
	14278674464 [label="other
 (1, 32)" fillcolor=orange]
	14278615648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278615696 -> 14278615648
	14278615696 -> 14278674224 [dir=none]
	14278674224 [label="input
 (1, 32)" fillcolor=orange]
	14278615696 -> 14531627760 [dir=none]
	14531627760 [label="weight
 (32, 32)" fillcolor=orange]
	14278615696 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278615936 -> 14278615696
	14278615936 -> 14278674304 [dir=none]
	14278674304 [label="other
 (1, 32)" fillcolor=orange]
	14278615936 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278615984 -> 14278615936
	14278615984 -> 14278674064 [dir=none]
	14278674064 [label="input
 (1, 32)" fillcolor=orange]
	14278615984 -> 14531578848 [dir=none]
	14531578848 [label="weight
 (32, 32)" fillcolor=orange]
	14278615984 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278616224 -> 14278615984
	14278616224 -> 14278674144 [dir=none]
	14278674144 [label="other
 (1, 32)" fillcolor=orange]
	14278616224 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278616272 -> 14278616224
	14278616272 -> 14278673184 [dir=none]
	14278673184 [label="input
 (1, 64)" fillcolor=orange]
	14278616272 -> 14531629200 [dir=none]
	14531629200 [label="weight
 (32, 64)" fillcolor=orange]
	14278616272 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278616512 -> 14278616272
	14278616512 -> 14278673984 [dir=none]
	14278673984 [label="other
 (1, 64)" fillcolor=orange]
	14278616512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278616560 -> 14278616512
	14278616560 -> 14531581008 [dir=none]
	14531581008 [label="input
 (1, 64)" fillcolor=orange]
	14278616560 -> 14531629040 [dir=none]
	14531629040 [label="weight
 (64, 64)" fillcolor=orange]
	14278616560 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278616800 -> 14278616560
	14278616800 -> 14278672624 [dir=none]
	14278672624 [label="other
 (1, 64)" fillcolor=orange]
	14278616800 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278616848 -> 14278616800
	14278616848 -> 14531590048 [dir=none]
	14531590048 [label="input
 (1, 128)" fillcolor=orange]
	14278616848 -> 14531628960 [dir=none]
	14531628960 [label="weight
 (64, 128)" fillcolor=orange]
	14278616848 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278617088 -> 14278616848
	14278617088 -> 14531581968 [dir=none]
	14531581968 [label="other
 (1, 128)" fillcolor=orange]
	14278617088 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14278617328 -> 14278617088
	14278617328 -> 14531608112 [dir=none]
	14531608112 [label="input
 (1, 10)" fillcolor=orange]
	14278617328 -> 14531628720 [dir=none]
	14531628720 [label="weight
 (128, 10)" fillcolor=orange]
	14278617328 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14278617424 -> 14278617328
	14278617424 [label=ToCopyBackward0]
	14278617376 -> 14278617424
	14057047936 [label="
 (1, 10)" fillcolor=lightblue]
	14057047936 -> 14278617376
	14278617376 [label=AccumulateGrad]
	14278617472 -> 14278617328
	14531628720 [label="layers.0.weight
 (128, 10)" fillcolor=lightblue]
	14531628720 -> 14278617472
	14278617472 [label=AccumulateGrad]
	14278617232 -> 14278617328
	14531628880 [label="layers.0.bias
 (128)" fillcolor=lightblue]
	14531628880 -> 14278617232
	14278617232 [label=AccumulateGrad]
	14278617184 -> 14278616848
	14531628960 [label="layers.3.weight
 (64, 128)" fillcolor=lightblue]
	14531628960 -> 14278617184
	14278617184 [label=AccumulateGrad]
	14278616944 -> 14278616848
	14531628800 [label="layers.3.bias
 (64)" fillcolor=lightblue]
	14531628800 -> 14278616944
	14278616944 [label=AccumulateGrad]
	14278616896 -> 14278616560
	14531629040 [label="layers.6.weight
 (64, 64)" fillcolor=lightblue]
	14531629040 -> 14278616896
	14278616896 [label=AccumulateGrad]
	14278616656 -> 14278616560
	14531629120 [label="layers.6.bias
 (64)" fillcolor=lightblue]
	14531629120 -> 14278616656
	14278616656 [label=AccumulateGrad]
	14278616608 -> 14278616272
	14531629200 [label="layers.9.weight
 (32, 64)" fillcolor=lightblue]
	14531629200 -> 14278616608
	14278616608 [label=AccumulateGrad]
	14278616368 -> 14278616272
	14531629280 [label="layers.9.bias
 (32)" fillcolor=lightblue]
	14531629280 -> 14278616368
	14278616368 [label=AccumulateGrad]
	14278616320 -> 14278615984
	14531578848 [label="layers.12.weight
 (32, 32)" fillcolor=lightblue]
	14531578848 -> 14278616320
	14278616320 [label=AccumulateGrad]
	14278616080 -> 14278615984
	14531629360 [label="layers.12.bias
 (32)" fillcolor=lightblue]
	14531629360 -> 14278616080
	14278616080 [label=AccumulateGrad]
	14278616032 -> 14278615696
	14531627760 [label="layers.15.weight
 (32, 32)" fillcolor=lightblue]
	14531627760 -> 14278616032
	14278616032 [label=AccumulateGrad]
	14278615792 -> 14278615696
	14531629440 [label="layers.15.bias
 (32)" fillcolor=lightblue]
	14531629440 -> 14278615792
	14278615792 [label=AccumulateGrad]
	14278615744 -> 14278615408
	14531629520 [label="layers.18.weight
 (16, 32)" fillcolor=lightblue]
	14531629520 -> 14278615744
	14278615744 [label=AccumulateGrad]
	14278615504 -> 14278615408
	14531629600 [label="layers.18.bias
 (16)" fillcolor=lightblue]
	14531629600 -> 14278615504
	14278615504 [label=AccumulateGrad]
	14278615456 -> 14278615120
	14531629680 [label="layers.21.weight
 (16, 16)" fillcolor=lightblue]
	14531629680 -> 14278615456
	14278615456 [label=AccumulateGrad]
	14278615216 -> 14278615120
	14531629760 [label="layers.21.bias
 (16)" fillcolor=lightblue]
	14531629760 -> 14278615216
	14278615216 [label=AccumulateGrad]
	14278615168 -> 14278614784
	14531629840 [label="layers.24.weight
 (16, 16)" fillcolor=lightblue]
	14531629840 -> 14278615168
	14278615168 [label=AccumulateGrad]
	14278614928 -> 14278614784
	14531629920 [label="layers.24.bias
 (16)" fillcolor=lightblue]
	14531629920 -> 14278614928
	14278614928 [label=AccumulateGrad]
	14278614688 -> 14057640608
	14531630000 [label="layers.27.weight
 (16, 16)" fillcolor=lightblue]
	14531630000 -> 14278614688
	14278614688 [label=AccumulateGrad]
	14278614640 -> 14057640608
	14531630080 [label="layers.27.bias
 (16)" fillcolor=lightblue]
	14531630080 -> 14278614640
	14278614640 [label=AccumulateGrad]
	14088687040 -> 14057649008
	14531630160 [label="layers.30.weight
 (1, 16)" fillcolor=lightblue]
	14531630160 -> 14088687040
	14088687040 [label=AccumulateGrad]
	14056451008 -> 14057649008
	14531630240 [label="layers.30.bias
 (1)" fillcolor=lightblue]
	14531630240 -> 14056451008
	14056451008 [label=AccumulateGrad]
	14057649008 -> 14531582048
}
