digraph {
	graph [size="18.3,18.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	16432649856 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	16432217152 [label=LinearBackward0]
	13487690736 -> 16432217152
	13487690736 [label=WhereBackward0]
	16432495200 -> 13487690736
	16432495200 [label=LinearBackward0]
	16432360432 -> 16432495200
	16432360432 [label=WhereBackward0]
	16669338080 -> 16432360432
	16669338080 [label=LinearBackward0]
	16669338320 -> 16669338080
	16669338320 [label=WhereBackward0]
	16669338368 -> 16669338320
	16669338368 [label=LinearBackward0]
	16669338560 -> 16669338368
	16669338560 [label=WhereBackward0]
	16669338752 -> 16669338560
	16669338752 [label=LinearBackward0]
	16669338896 -> 16669338752
	16669338896 [label=WhereBackward0]
	16669339088 -> 16669338896
	16669339088 [label=LinearBackward0]
	16669339232 -> 16669339088
	16669339232 [label=WhereBackward0]
	16669339424 -> 16669339232
	16669339424 [label=LinearBackward0]
	16669339568 -> 16669339424
	16669339568 [label=WhereBackward0]
	16669339760 -> 16669339568
	16669339760 [label=LinearBackward0]
	16669339904 -> 16669339760
	16669339904 [label=ToCopyBackward0]
	16669340096 -> 16669339904
	16432501360 [label="
 (1, 10)" fillcolor=lightblue]
	16432501360 -> 16669340096
	16669340096 [label=AccumulateGrad]
	16669339856 -> 16669339760
	16432653936 [label="layers.0.weight
 (320, 10)" fillcolor=lightblue]
	16432653936 -> 16669339856
	16669339856 [label=AccumulateGrad]
	16669339808 -> 16669339760
	16432653456 [label="layers.0.bias
 (320)" fillcolor=lightblue]
	16432653456 -> 16669339808
	16669339808 [label=AccumulateGrad]
	16669339712 -> 16669339568
	16669339712 [label=MulBackward0]
	16669339760 -> 16669339712
	16669339520 -> 16669339424
	16432654096 [label="layers.3.weight
 (160, 320)" fillcolor=lightblue]
	16432654096 -> 16669339520
	16669339520 [label=AccumulateGrad]
	16669339472 -> 16669339424
	16432654176 [label="layers.3.bias
 (160)" fillcolor=lightblue]
	16432654176 -> 16669339472
	16669339472 [label=AccumulateGrad]
	16669339376 -> 16669339232
	16669339376 [label=MulBackward0]
	16669339424 -> 16669339376
	16669339184 -> 16669339088
	16432654016 [label="layers.6.weight
 (320, 160)" fillcolor=lightblue]
	16432654016 -> 16669339184
	16669339184 [label=AccumulateGrad]
	16669339136 -> 16669339088
	16432654256 [label="layers.6.bias
 (320)" fillcolor=lightblue]
	16432654256 -> 16669339136
	16669339136 [label=AccumulateGrad]
	16669339040 -> 16669338896
	16669339040 [label=MulBackward0]
	16669339088 -> 16669339040
	16669338848 -> 16669338752
	16432654336 [label="layers.9.weight
 (160, 320)" fillcolor=lightblue]
	16432654336 -> 16669338848
	16669338848 [label=AccumulateGrad]
	16669338800 -> 16669338752
	16432654416 [label="layers.9.bias
 (160)" fillcolor=lightblue]
	16432654416 -> 16669338800
	16669338800 [label=AccumulateGrad]
	16669338704 -> 16669338560
	16669338704 [label=MulBackward0]
	16669338752 -> 16669338704
	16669338512 -> 16669338368
	16432654496 [label="layers.12.weight
 (160, 160)" fillcolor=lightblue]
	16432654496 -> 16669338512
	16669338512 [label=AccumulateGrad]
	16669338464 -> 16669338368
	16432654576 [label="layers.12.bias
 (160)" fillcolor=lightblue]
	16432654576 -> 16669338464
	16669338464 [label=AccumulateGrad]
	16669338032 -> 16669338320
	16669338032 [label=MulBackward0]
	16669338368 -> 16669338032
	16669338224 -> 16669338080
	16432654656 [label="layers.15.weight
 (80, 160)" fillcolor=lightblue]
	16432654656 -> 16669338224
	16669338224 [label=AccumulateGrad]
	16669338128 -> 16669338080
	16432654736 [label="layers.15.bias
 (80)" fillcolor=lightblue]
	16432654736 -> 16669338128
	16669338128 [label=AccumulateGrad]
	16669337936 -> 16432360432
	16669337936 [label=MulBackward0]
	16669338080 -> 16669337936
	16432359232 -> 16432495200
	16432654816 [label="layers.18.weight
 (80, 80)" fillcolor=lightblue]
	16432654816 -> 16432359232
	16432359232 [label=AccumulateGrad]
	16432356304 -> 16432495200
	16432654896 [label="layers.18.bias
 (80)" fillcolor=lightblue]
	16432654896 -> 16432356304
	16432356304 [label=AccumulateGrad]
	16432489536 -> 13487690736
	16432489536 [label=MulBackward0]
	16432495200 -> 16432489536
	16432214752 -> 16432217152
	16432497840 [label="layers.21.weight
 (1, 80)" fillcolor=lightblue]
	16432497840 -> 16432214752
	16432214752 [label=AccumulateGrad]
	6215502720 -> 16432217152
	16432151312 [label="layers.21.bias
 (1)" fillcolor=lightblue]
	16432151312 -> 6215502720
	6215502720 [label=AccumulateGrad]
	16432217152 -> 16432649856
}
