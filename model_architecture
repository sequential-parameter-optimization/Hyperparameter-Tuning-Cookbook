digraph {
	graph [size="18.3,18.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	13180093936 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	12901448480 [label=LinearBackward0]
	6258764688 -> 12901448480
	6258764688 [label=WhereBackward0]
	12901222240 -> 6258764688
	12901222240 [label=LinearBackward0]
	12901550880 -> 12901222240
	12901550880 [label=WhereBackward0]
	16173656736 -> 12901550880
	16173656736 [label=LinearBackward0]
	16173656688 -> 16173656736
	16173656688 [label=WhereBackward0]
	16173657024 -> 16173656688
	16173657024 [label=LinearBackward0]
	16173657168 -> 16173657024
	16173657168 [label=WhereBackward0]
	16173657360 -> 16173657168
	16173657360 [label=LinearBackward0]
	16173657504 -> 16173657360
	16173657504 [label=WhereBackward0]
	16173657696 -> 16173657504
	16173657696 [label=LinearBackward0]
	16173657840 -> 16173657696
	16173657840 [label=WhereBackward0]
	16173658032 -> 16173657840
	16173658032 [label=LinearBackward0]
	16173658176 -> 16173658032
	16173658176 [label=WhereBackward0]
	16173658368 -> 16173658176
	16173658368 [label=LinearBackward0]
	16173658512 -> 16173658368
	16173658512 [label=ToCopyBackward0]
	13180494288 -> 16173658512
	12901569136 [label="
 (1, 10)" fillcolor=lightblue]
	12901569136 -> 13180494288
	13180494288 [label=AccumulateGrad]
	16173658464 -> 16173658368
	12901577136 [label="layers.0.weight
 (320, 10)" fillcolor=lightblue]
	12901577136 -> 16173658464
	16173658464 [label=AccumulateGrad]
	16173658416 -> 16173658368
	12901576656 [label="layers.0.bias
 (320)" fillcolor=lightblue]
	12901576656 -> 16173658416
	16173658416 [label=AccumulateGrad]
	16173658320 -> 16173658176
	16173658320 [label=MulBackward0]
	16173658368 -> 16173658320
	16173658128 -> 16173658032
	12901577296 [label="layers.3.weight
 (160, 320)" fillcolor=lightblue]
	12901577296 -> 16173658128
	16173658128 [label=AccumulateGrad]
	16173658080 -> 16173658032
	12901577376 [label="layers.3.bias
 (160)" fillcolor=lightblue]
	12901577376 -> 16173658080
	16173658080 [label=AccumulateGrad]
	16173657984 -> 16173657840
	16173657984 [label=MulBackward0]
	16173658032 -> 16173657984
	16173657792 -> 16173657696
	12901577216 [label="layers.6.weight
 (320, 160)" fillcolor=lightblue]
	12901577216 -> 16173657792
	16173657792 [label=AccumulateGrad]
	16173657744 -> 16173657696
	12901577456 [label="layers.6.bias
 (320)" fillcolor=lightblue]
	12901577456 -> 16173657744
	16173657744 [label=AccumulateGrad]
	16173657648 -> 16173657504
	16173657648 [label=MulBackward0]
	16173657696 -> 16173657648
	16173657456 -> 16173657360
	12901577536 [label="layers.9.weight
 (160, 320)" fillcolor=lightblue]
	12901577536 -> 16173657456
	16173657456 [label=AccumulateGrad]
	16173657408 -> 16173657360
	12901577616 [label="layers.9.bias
 (160)" fillcolor=lightblue]
	12901577616 -> 16173657408
	16173657408 [label=AccumulateGrad]
	16173657312 -> 16173657168
	16173657312 [label=MulBackward0]
	16173657360 -> 16173657312
	16173657120 -> 16173657024
	12901577696 [label="layers.12.weight
 (160, 160)" fillcolor=lightblue]
	12901577696 -> 16173657120
	16173657120 [label=AccumulateGrad]
	16173657072 -> 16173657024
	12901577776 [label="layers.12.bias
 (160)" fillcolor=lightblue]
	12901577776 -> 16173657072
	16173657072 [label=AccumulateGrad]
	16173656976 -> 16173656688
	16173656976 [label=MulBackward0]
	16173657024 -> 16173656976
	16173656448 -> 16173656736
	12901577856 [label="layers.15.weight
 (80, 160)" fillcolor=lightblue]
	12901577856 -> 16173656448
	16173656448 [label=AccumulateGrad]
	16173656832 -> 16173656736
	12901577936 [label="layers.15.bias
 (80)" fillcolor=lightblue]
	12901577936 -> 16173656832
	16173656832 [label=AccumulateGrad]
	16173656640 -> 12901550880
	16173656640 [label=MulBackward0]
	16173656736 -> 16173656640
	12901550832 -> 12901222240
	12901578016 [label="layers.18.weight
 (80, 80)" fillcolor=lightblue]
	12901578016 -> 12901550832
	12901550832 [label=AccumulateGrad]
	16173656400 -> 12901222240
	12901578096 [label="layers.18.bias
 (80)" fillcolor=lightblue]
	12901578096 -> 16173656400
	16173656400 [label=AccumulateGrad]
	12901232848 -> 6258764688
	12901232848 [label=MulBackward0]
	12901222240 -> 12901232848
	12901146512 -> 12901448480
	12901494096 [label="layers.21.weight
 (1, 80)" fillcolor=lightblue]
	12901494096 -> 12901146512
	12901146512 [label=AccumulateGrad]
	12901154768 -> 12901448480
	13180308368 [label="layers.21.bias
 (1)" fillcolor=lightblue]
	13180308368 -> 12901154768
	12901154768 [label=AccumulateGrad]
	12901448480 -> 13180093936
}
