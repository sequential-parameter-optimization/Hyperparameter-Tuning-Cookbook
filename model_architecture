digraph {
	graph [size="51.449999999999996,51.449999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	14330945904 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4348679152 -> 15965058096 [dir=none]
	15965058096 [label="input
 (1, 2)" fillcolor=orange]
	4348679152 -> 14413776288 [dir=none]
	14413776288 [label="weight
 (1, 2)" fillcolor=orange]
	4348679152 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15661759888 -> 4348679152
	15661759888 -> 15965057856 [dir=none]
	15965057856 [label="condition
 (1, 2)" fillcolor=orange]
	15661759888 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14413771344 -> 15661759888
	14413771344 -> 15965057936 [dir=none]
	15965057936 [label="input
 (1, 2)" fillcolor=orange]
	14413771344 -> 14413782448 [dir=none]
	14413782448 [label="weight
 (2, 2)" fillcolor=orange]
	14413771344 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965211184 -> 14413771344
	15965211184 -> 15965057696 [dir=none]
	15965057696 [label="condition
 (1, 2)" fillcolor=orange]
	15965211184 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965211376 -> 15965211184
	15965211376 -> 15965057776 [dir=none]
	15965057776 [label="input
 (1, 2)" fillcolor=orange]
	15965211376 -> 14413789008 [dir=none]
	14413789008 [label="weight
 (2, 2)" fillcolor=orange]
	15965211376 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965211712 -> 15965211376
	15965211712 -> 15965057536 [dir=none]
	15965057536 [label="condition
 (1, 2)" fillcolor=orange]
	15965211712 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965211760 -> 15965211712
	15965211760 -> 15965057616 [dir=none]
	15965057616 [label="input
 (1, 2)" fillcolor=orange]
	15965211760 -> 14413780528 [dir=none]
	14413780528 [label="weight
 (2, 2)" fillcolor=orange]
	15965211760 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965212048 -> 15965211760
	15965212048 -> 15965057376 [dir=none]
	15965057376 [label="condition
 (1, 2)" fillcolor=orange]
	15965212048 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965212096 -> 15965212048
	15965212096 -> 15965057456 [dir=none]
	15965057456 [label="input
 (1, 4)" fillcolor=orange]
	15965212096 -> 14413787168 [dir=none]
	14413787168 [label="weight
 (2, 4)" fillcolor=orange]
	15965212096 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965212384 -> 15965212096
	15965212384 -> 15965057216 [dir=none]
	15965057216 [label="condition
 (1, 4)" fillcolor=orange]
	15965212384 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965212432 -> 15965212384
	15965212432 -> 15965057296 [dir=none]
	15965057296 [label="input
 (1, 4)" fillcolor=orange]
	15965212432 -> 14413789408 [dir=none]
	14413789408 [label="weight
 (4, 4)" fillcolor=orange]
	15965212432 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965212720 -> 15965212432
	15965212720 -> 15965057056 [dir=none]
	15965057056 [label="condition
 (1, 4)" fillcolor=orange]
	15965212720 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965212768 -> 15965212720
	15965212768 -> 15965057136 [dir=none]
	15965057136 [label="input
 (1, 4)" fillcolor=orange]
	15965212768 -> 14413787008 [dir=none]
	14413787008 [label="weight
 (4, 4)" fillcolor=orange]
	15965212768 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965213056 -> 15965212768
	15965213056 -> 15965056896 [dir=none]
	15965056896 [label="condition
 (1, 4)" fillcolor=orange]
	15965213056 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965213104 -> 15965213056
	15965213104 -> 15965056976 [dir=none]
	15965056976 [label="input
 (1, 4)" fillcolor=orange]
	15965213104 -> 14413785168 [dir=none]
	14413785168 [label="weight
 (4, 4)" fillcolor=orange]
	15965213104 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965213392 -> 15965213104
	15965213392 -> 15965056736 [dir=none]
	15965056736 [label="condition
 (1, 4)" fillcolor=orange]
	15965213392 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965213440 -> 15965213392
	15965213440 -> 15965056816 [dir=none]
	15965056816 [label="input
 (1, 8)" fillcolor=orange]
	15965213440 -> 14413780688 [dir=none]
	14413780688 [label="weight
 (4, 8)" fillcolor=orange]
	15965213440 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965213728 -> 15965213440
	15965213728 -> 15965056576 [dir=none]
	15965056576 [label="condition
 (1, 8)" fillcolor=orange]
	15965213728 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965213776 -> 15965213728
	15965213776 -> 15965056656 [dir=none]
	15965056656 [label="input
 (1, 8)" fillcolor=orange]
	15965213776 -> 14413784768 [dir=none]
	14413784768 [label="weight
 (8, 8)" fillcolor=orange]
	15965213776 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965214064 -> 15965213776
	15965214064 -> 15965056416 [dir=none]
	15965056416 [label="condition
 (1, 8)" fillcolor=orange]
	15965214064 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965214112 -> 15965214064
	15965214112 -> 15965056496 [dir=none]
	15965056496 [label="input
 (1, 8)" fillcolor=orange]
	15965214112 -> 14413783088 [dir=none]
	14413783088 [label="weight
 (8, 8)" fillcolor=orange]
	15965214112 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965214400 -> 15965214112
	15965214400 -> 15965056016 [dir=none]
	15965056016 [label="condition
 (1, 8)" fillcolor=orange]
	15965214400 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965214448 -> 15965214400
	15965214448 -> 15965049216 [dir=none]
	15965049216 [label="input
 (1, 16)" fillcolor=orange]
	15965214448 -> 14413790288 [dir=none]
	14413790288 [label="weight
 (8, 16)" fillcolor=orange]
	15965214448 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965214736 -> 15965214448
	15965214736 -> 4348706672 [dir=none]
	4348706672 [label="condition
 (1, 16)" fillcolor=orange]
	15965214736 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965214784 -> 15965214736
	15965214784 -> 14396587328 [dir=none]
	14396587328 [label="input
 (1, 16)" fillcolor=orange]
	15965214784 -> 14413790928 [dir=none]
	14413790928 [label="weight
 (16, 16)" fillcolor=orange]
	15965214784 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965215120 -> 15965214784
	15965215120 -> 4348705472 [dir=none]
	4348705472 [label="condition
 (1, 16)" fillcolor=orange]
	15965215120 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965215072 -> 15965215120
	15965215072 -> 4348705392 [dir=none]
	4348705392 [label="input
 (1, 32)" fillcolor=orange]
	15965215072 -> 14413778288 [dir=none]
	14413778288 [label="weight
 (16, 32)" fillcolor=orange]
	15965215072 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965215456 -> 15965215072
	15965215456 -> 4348706752 [dir=none]
	4348706752 [label="condition
 (1, 32)" fillcolor=orange]
	15965215456 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	15965215408 -> 15965215456
	15965215408 -> 15964940608 [dir=none]
	15964940608 [label="input
 (1, 10)" fillcolor=orange]
	15965215408 -> 14413781488 [dir=none]
	14413781488 [label="weight
 (32, 10)" fillcolor=orange]
	15965215408 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	15965215792 -> 15965215408
	15965215792 [label=ToCopyBackward0]
	15965215744 -> 15965215792
	14413826800 [label="
 (1, 10)" fillcolor=lightblue]
	14413826800 -> 15965215744
	15965215744 [label=AccumulateGrad]
	15965215648 -> 15965215408
	14413781488 [label="layers.0.weight
 (32, 10)" fillcolor=lightblue]
	14413781488 -> 15965215648
	15965215648 [label=AccumulateGrad]
	15965215696 -> 15965215408
	14413785888 [label="layers.0.bias
 (32)" fillcolor=lightblue]
	14413785888 -> 15965215696
	15965215696 [label=AccumulateGrad]
	15965215600 -> 15965215456
	15965215600 -> 15965248144 [dir=none]
	15965248144 [label="other
 ()" fillcolor=orange]
	15965215600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965215408 -> 15965215600
	15965215312 -> 15965215072
	14413778288 [label="layers.3.weight
 (16, 32)" fillcolor=lightblue]
	14413778288 -> 15965215312
	15965215312 [label=AccumulateGrad]
	15965215360 -> 15965215072
	14413787648 [label="layers.3.bias
 (16)" fillcolor=lightblue]
	14413787648 -> 15965215360
	15965215360 [label=AccumulateGrad]
	15965215264 -> 15965215120
	15965215264 -> 15965248944 [dir=none]
	15965248944 [label="other
 ()" fillcolor=orange]
	15965215264 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965215072 -> 15965215264
	15965214976 -> 15965214784
	14413790928 [label="layers.6.weight
 (16, 16)" fillcolor=lightblue]
	14413790928 -> 15965214976
	15965214976 [label=AccumulateGrad]
	15965215024 -> 15965214784
	14413777088 [label="layers.6.bias
 (16)" fillcolor=lightblue]
	14413777088 -> 15965215024
	15965215024 [label=AccumulateGrad]
	15965214928 -> 15965214736
	15965214928 -> 15965249744 [dir=none]
	15965249744 [label="other
 ()" fillcolor=orange]
	15965214928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965214784 -> 15965214928
	15965214640 -> 15965214448
	14413790288 [label="layers.9.weight
 (8, 16)" fillcolor=lightblue]
	14413790288 -> 15965214640
	15965214640 [label=AccumulateGrad]
	15965214688 -> 15965214448
	14413776848 [label="layers.9.bias
 (8)" fillcolor=lightblue]
	14413776848 -> 15965214688
	15965214688 [label=AccumulateGrad]
	15965214592 -> 15965214400
	15965214592 -> 15965250544 [dir=none]
	15965250544 [label="other
 ()" fillcolor=orange]
	15965214592 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965214448 -> 15965214592
	15965214304 -> 15965214112
	14413783088 [label="layers.12.weight
 (8, 8)" fillcolor=lightblue]
	14413783088 -> 15965214304
	15965214304 [label=AccumulateGrad]
	15965214352 -> 15965214112
	14413790048 [label="layers.12.bias
 (8)" fillcolor=lightblue]
	14413790048 -> 15965214352
	15965214352 [label=AccumulateGrad]
	15965214256 -> 15965214064
	15965214256 -> 15965251344 [dir=none]
	15965251344 [label="other
 ()" fillcolor=orange]
	15965214256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965214112 -> 15965214256
	15965213968 -> 15965213776
	14413784768 [label="layers.15.weight
 (8, 8)" fillcolor=lightblue]
	14413784768 -> 15965213968
	15965213968 [label=AccumulateGrad]
	15965214016 -> 15965213776
	14413776208 [label="layers.15.bias
 (8)" fillcolor=lightblue]
	14413776208 -> 15965214016
	15965214016 [label=AccumulateGrad]
	15965213920 -> 15965213728
	15965213920 -> 15965252144 [dir=none]
	15965252144 [label="other
 ()" fillcolor=orange]
	15965213920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965213776 -> 15965213920
	15965213632 -> 15965213440
	14413780688 [label="layers.18.weight
 (4, 8)" fillcolor=lightblue]
	14413780688 -> 15965213632
	15965213632 [label=AccumulateGrad]
	15965213680 -> 15965213440
	14413778608 [label="layers.18.bias
 (4)" fillcolor=lightblue]
	14413778608 -> 15965213680
	15965213680 [label=AccumulateGrad]
	15965213584 -> 15965213392
	15965213584 -> 15965252944 [dir=none]
	15965252944 [label="other
 ()" fillcolor=orange]
	15965213584 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965213440 -> 15965213584
	15965213296 -> 15965213104
	14413785168 [label="layers.21.weight
 (4, 4)" fillcolor=lightblue]
	14413785168 -> 15965213296
	15965213296 [label=AccumulateGrad]
	15965213344 -> 15965213104
	14413783808 [label="layers.21.bias
 (4)" fillcolor=lightblue]
	14413783808 -> 15965213344
	15965213344 [label=AccumulateGrad]
	15965213248 -> 15965213056
	15965213248 -> 15965253744 [dir=none]
	15965253744 [label="other
 ()" fillcolor=orange]
	15965213248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965213104 -> 15965213248
	15965212960 -> 15965212768
	14413787008 [label="layers.24.weight
 (4, 4)" fillcolor=lightblue]
	14413787008 -> 15965212960
	15965212960 [label=AccumulateGrad]
	15965213008 -> 15965212768
	14413787328 [label="layers.24.bias
 (4)" fillcolor=lightblue]
	14413787328 -> 15965213008
	15965213008 [label=AccumulateGrad]
	15965212912 -> 15965212720
	15965212912 -> 15965254544 [dir=none]
	15965254544 [label="other
 ()" fillcolor=orange]
	15965212912 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965212768 -> 15965212912
	15965212624 -> 15965212432
	14413789408 [label="layers.27.weight
 (4, 4)" fillcolor=lightblue]
	14413789408 -> 15965212624
	15965212624 [label=AccumulateGrad]
	15965212672 -> 15965212432
	14413790128 [label="layers.27.bias
 (4)" fillcolor=lightblue]
	14413790128 -> 15965212672
	15965212672 [label=AccumulateGrad]
	15965212576 -> 15965212384
	15965212576 -> 15965255344 [dir=none]
	15965255344 [label="other
 ()" fillcolor=orange]
	15965212576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965212432 -> 15965212576
	15965212288 -> 15965212096
	14413787168 [label="layers.30.weight
 (2, 4)" fillcolor=lightblue]
	14413787168 -> 15965212288
	15965212288 [label=AccumulateGrad]
	15965212336 -> 15965212096
	14413782768 [label="layers.30.bias
 (2)" fillcolor=lightblue]
	14413782768 -> 15965212336
	15965212336 [label=AccumulateGrad]
	15965212240 -> 15965212048
	15965212240 -> 15965256144 [dir=none]
	15965256144 [label="other
 ()" fillcolor=orange]
	15965212240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965212096 -> 15965212240
	15965211952 -> 15965211760
	14413780528 [label="layers.33.weight
 (2, 2)" fillcolor=lightblue]
	14413780528 -> 15965211952
	15965211952 [label=AccumulateGrad]
	15965212000 -> 15965211760
	14413780928 [label="layers.33.bias
 (2)" fillcolor=lightblue]
	14413780928 -> 15965212000
	15965212000 [label=AccumulateGrad]
	15965211904 -> 15965211712
	15965211904 -> 15965256944 [dir=none]
	15965256944 [label="other
 ()" fillcolor=orange]
	15965211904 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965211760 -> 15965211904
	15965211472 -> 15965211376
	14413789008 [label="layers.36.weight
 (2, 2)" fillcolor=lightblue]
	14413789008 -> 15965211472
	15965211472 [label=AccumulateGrad]
	15965211664 -> 15965211376
	14413787408 [label="layers.36.bias
 (2)" fillcolor=lightblue]
	14413787408 -> 15965211664
	15965211664 [label=AccumulateGrad]
	15965211616 -> 15965211184
	15965211616 -> 15965274192 [dir=none]
	15965274192 [label="other
 ()" fillcolor=orange]
	15965211616 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	15965211376 -> 15965211616
	15965211328 -> 14413771344
	14413782448 [label="layers.39.weight
 (2, 2)" fillcolor=lightblue]
	14413782448 -> 15965211328
	15965211328 [label=AccumulateGrad]
	15965211280 -> 14413771344
	14413778208 [label="layers.39.bias
 (2)" fillcolor=lightblue]
	14413778208 -> 15965211280
	15965211280 [label=AccumulateGrad]
	14414507376 -> 15661759888
	14414507376 -> 15965274992 [dir=none]
	15965274992 [label="other
 ()" fillcolor=orange]
	14414507376 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14413771344 -> 14414507376
	14331276896 -> 4348679152
	14413776288 [label="layers.42.weight
 (1, 2)" fillcolor=lightblue]
	14413776288 -> 14331276896
	14331276896 [label=AccumulateGrad]
	4348679440 -> 4348679152
	14413782528 [label="layers.42.bias
 (1)" fillcolor=lightblue]
	14413782528 -> 4348679440
	4348679440 [label=AccumulateGrad]
	4348679152 -> 14330945904
}
