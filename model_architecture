digraph {
	graph [size="18.3,18.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	12981901248 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	12981645808 [label=LinearBackward0]
	12981751376 -> 12981645808
	12981751376 [label=WhereBackward0]
	13089776032 -> 12981751376
	13089776032 [label=LinearBackward0]
	15618588000 -> 13089776032
	15618588000 [label=WhereBackward0]
	15618588192 -> 15618588000
	15618588192 [label=LinearBackward0]
	15618588288 -> 15618588192
	15618588288 [label=WhereBackward0]
	15618588480 -> 15618588288
	15618588480 [label=LinearBackward0]
	15618588624 -> 15618588480
	15618588624 [label=WhereBackward0]
	15618752720 -> 15618588624
	15618752720 [label=LinearBackward0]
	15618752864 -> 15618752720
	15618752864 [label=WhereBackward0]
	15618753056 -> 15618752864
	15618753056 [label=LinearBackward0]
	15618753200 -> 15618753056
	15618753200 [label=WhereBackward0]
	15618753392 -> 15618753200
	15618753392 [label=LinearBackward0]
	15618753536 -> 15618753392
	15618753536 [label=WhereBackward0]
	15618753728 -> 15618753536
	15618753728 [label=LinearBackward0]
	15618753872 -> 15618753728
	15618753872 [label=ToCopyBackward0]
	12981877776 -> 15618753872
	12981710160 [label="
 (1, 10)" fillcolor=lightblue]
	12981710160 -> 12981877776
	12981877776 [label=AccumulateGrad]
	15618753824 -> 15618753728
	12981712640 [label="layers.0.weight
 (320, 10)" fillcolor=lightblue]
	12981712640 -> 15618753824
	15618753824 [label=AccumulateGrad]
	15618753776 -> 15618753728
	12981998032 [label="layers.0.bias
 (320)" fillcolor=lightblue]
	12981998032 -> 15618753776
	15618753776 [label=AccumulateGrad]
	15618753680 -> 15618753536
	15618753680 [label=MulBackward0]
	15618753728 -> 15618753680
	15618753488 -> 15618753392
	12981998112 [label="layers.3.weight
 (160, 320)" fillcolor=lightblue]
	12981998112 -> 15618753488
	15618753488 [label=AccumulateGrad]
	15618753440 -> 15618753392
	12981998192 [label="layers.3.bias
 (160)" fillcolor=lightblue]
	12981998192 -> 15618753440
	15618753440 [label=AccumulateGrad]
	15618753344 -> 15618753200
	15618753344 [label=MulBackward0]
	15618753392 -> 15618753344
	15618753152 -> 15618753056
	12981997552 [label="layers.6.weight
 (320, 160)" fillcolor=lightblue]
	12981997552 -> 15618753152
	15618753152 [label=AccumulateGrad]
	15618753104 -> 15618753056
	12981998272 [label="layers.6.bias
 (320)" fillcolor=lightblue]
	12981998272 -> 15618753104
	15618753104 [label=AccumulateGrad]
	15618753008 -> 15618752864
	15618753008 [label=MulBackward0]
	15618753056 -> 15618753008
	15618752816 -> 15618752720
	12981998352 [label="layers.9.weight
 (160, 320)" fillcolor=lightblue]
	12981998352 -> 15618752816
	15618752816 [label=AccumulateGrad]
	15618752768 -> 15618752720
	12981998432 [label="layers.9.bias
 (160)" fillcolor=lightblue]
	12981998432 -> 15618752768
	15618752768 [label=AccumulateGrad]
	15618752672 -> 15618588624
	15618752672 [label=MulBackward0]
	15618752720 -> 15618752672
	15618588576 -> 15618588480
	12981998512 [label="layers.12.weight
 (160, 160)" fillcolor=lightblue]
	12981998512 -> 15618588576
	15618588576 [label=AccumulateGrad]
	15618588528 -> 15618588480
	12981998592 [label="layers.12.bias
 (160)" fillcolor=lightblue]
	12981998592 -> 15618588528
	15618588528 [label=AccumulateGrad]
	15618588432 -> 15618588288
	15618588432 [label=MulBackward0]
	15618588480 -> 15618588432
	15618588240 -> 15618588192
	12981998672 [label="layers.15.weight
 (80, 160)" fillcolor=lightblue]
	12981998672 -> 15618588240
	15618588240 [label=AccumulateGrad]
	15618588144 -> 15618588192
	12981998752 [label="layers.15.bias
 (80)" fillcolor=lightblue]
	12981998752 -> 15618588144
	15618588144 [label=AccumulateGrad]
	15618588096 -> 15618588000
	15618588096 [label=MulBackward0]
	15618588192 -> 15618588096
	15618583200 -> 13089776032
	12981998832 [label="layers.18.weight
 (80, 80)" fillcolor=lightblue]
	12981998832 -> 15618583200
	15618583200 [label=AccumulateGrad]
	15618587808 -> 13089776032
	12981998912 [label="layers.18.bias
 (80)" fillcolor=lightblue]
	12981998912 -> 15618587808
	15618587808 [label=AccumulateGrad]
	15618587904 -> 12981751376
	15618587904 [label=MulBackward0]
	13089776032 -> 15618587904
	12981633136 -> 12981645808
	12981710880 [label="layers.21.weight
 (1, 80)" fillcolor=lightblue]
	12981710880 -> 12981633136
	12981633136 [label=AccumulateGrad]
	12981646480 -> 12981645808
	12981813728 [label="layers.21.bias
 (1)" fillcolor=lightblue]
	12981813728 -> 12981646480
	12981646480 [label=AccumulateGrad]
	12981645808 -> 12981901248
}
