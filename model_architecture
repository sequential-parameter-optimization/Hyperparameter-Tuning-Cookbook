digraph {
	graph [size="18.3,18.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	16221349008 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	16221024272 [label=LinearBackward0]
	16221326112 -> 16221024272
	16221326112 [label=WhereBackward0]
	16221628944 -> 16221326112
	16221628944 [label=LinearBackward0]
	16221628992 -> 16221628944
	16221628992 [label=WhereBackward0]
	16221629232 -> 16221628992
	16221629232 [label=LinearBackward0]
	16221629616 -> 16221629232
	16221629616 [label=WhereBackward0]
	16221629808 -> 16221629616
	16221629808 [label=LinearBackward0]
	16221629952 -> 16221629808
	16221629952 [label=WhereBackward0]
	16221630144 -> 16221629952
	16221630144 [label=LinearBackward0]
	16221630288 -> 16221630144
	16221630288 [label=WhereBackward0]
	16221630480 -> 16221630288
	16221630480 [label=LinearBackward0]
	16221630624 -> 16221630480
	16221630624 [label=WhereBackward0]
	16221630816 -> 16221630624
	16221630816 [label=LinearBackward0]
	16221630960 -> 16221630816
	16221630960 [label=WhereBackward0]
	16221631152 -> 16221630960
	16221631152 [label=LinearBackward0]
	16221631296 -> 16221631152
	16221631296 [label=ToCopyBackward0]
	16221631488 -> 16221631296
	16221345808 [label="
 (1, 10)" fillcolor=lightblue]
	16221345808 -> 16221631488
	16221631488 [label=AccumulateGrad]
	16221631248 -> 16221631152
	16221353568 [label="layers.0.weight
 (320, 10)" fillcolor=lightblue]
	16221353568 -> 16221631248
	16221631248 [label=AccumulateGrad]
	16221631200 -> 16221631152
	16221353088 [label="layers.0.bias
 (320)" fillcolor=lightblue]
	16221353088 -> 16221631200
	16221631200 [label=AccumulateGrad]
	16221631104 -> 16221630960
	16221631104 [label=MulBackward0]
	16221631152 -> 16221631104
	16221630912 -> 16221630816
	16221353728 [label="layers.3.weight
 (160, 320)" fillcolor=lightblue]
	16221353728 -> 16221630912
	16221630912 [label=AccumulateGrad]
	16221630864 -> 16221630816
	16221353808 [label="layers.3.bias
 (160)" fillcolor=lightblue]
	16221353808 -> 16221630864
	16221630864 [label=AccumulateGrad]
	16221630768 -> 16221630624
	16221630768 [label=MulBackward0]
	16221630816 -> 16221630768
	16221630576 -> 16221630480
	16221353648 [label="layers.6.weight
 (320, 160)" fillcolor=lightblue]
	16221353648 -> 16221630576
	16221630576 [label=AccumulateGrad]
	16221630528 -> 16221630480
	16221353888 [label="layers.6.bias
 (320)" fillcolor=lightblue]
	16221353888 -> 16221630528
	16221630528 [label=AccumulateGrad]
	16221630432 -> 16221630288
	16221630432 [label=MulBackward0]
	16221630480 -> 16221630432
	16221630240 -> 16221630144
	16221353968 [label="layers.9.weight
 (160, 320)" fillcolor=lightblue]
	16221353968 -> 16221630240
	16221630240 [label=AccumulateGrad]
	16221630192 -> 16221630144
	16221354048 [label="layers.9.bias
 (160)" fillcolor=lightblue]
	16221354048 -> 16221630192
	16221630192 [label=AccumulateGrad]
	16221630096 -> 16221629952
	16221630096 [label=MulBackward0]
	16221630144 -> 16221630096
	16221629904 -> 16221629808
	16221354128 [label="layers.12.weight
 (160, 160)" fillcolor=lightblue]
	16221354128 -> 16221629904
	16221629904 [label=AccumulateGrad]
	16221629856 -> 16221629808
	16221354208 [label="layers.12.bias
 (160)" fillcolor=lightblue]
	16221354208 -> 16221629856
	16221629856 [label=AccumulateGrad]
	16221629760 -> 16221629616
	16221629760 [label=MulBackward0]
	16221629808 -> 16221629760
	16221629568 -> 16221629232
	16221354288 [label="layers.15.weight
 (80, 160)" fillcolor=lightblue]
	16221354288 -> 16221629568
	16221629568 [label=AccumulateGrad]
	16221629520 -> 16221629232
	16221354368 [label="layers.15.bias
 (80)" fillcolor=lightblue]
	16221354368 -> 16221629520
	16221629520 [label=AccumulateGrad]
	16221629472 -> 16221628992
	16221629472 [label=MulBackward0]
	16221629232 -> 16221629472
	16221629328 -> 16221628944
	16221354448 [label="layers.18.weight
 (80, 80)" fillcolor=lightblue]
	16221354448 -> 16221629328
	16221629328 [label=AccumulateGrad]
	16221629184 -> 16221628944
	16221354528 [label="layers.18.bias
 (80)" fillcolor=lightblue]
	16221354528 -> 16221629184
	16221629184 [label=AccumulateGrad]
	16221629088 -> 16221326112
	16221629088 [label=MulBackward0]
	16221628944 -> 16221629088
	16221333312 -> 16221024272
	16221236720 [label="layers.21.weight
 (1, 80)" fillcolor=lightblue]
	16221236720 -> 16221333312
	16221333312 [label=AccumulateGrad]
	16221025088 -> 16221024272
	6254251776 [label="layers.21.bias
 (1)" fillcolor=lightblue]
	6254251776 -> 16221025088
	16221025088 [label=AccumulateGrad]
	16221024272 -> 16221349008
}
