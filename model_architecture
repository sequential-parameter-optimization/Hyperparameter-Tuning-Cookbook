digraph {
	graph [size="23.849999999999998,23.849999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	14697968928 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	14697714816 -> 14614783600 [dir=none]
	14614783600 [label="input
 (1, 2)" fillcolor=orange]
	14697714816 -> 14697986192 [dir=none]
	14697986192 [label="weight
 (1, 2)" fillcolor=orange]
	14697714816 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14697825712 -> 14697714816
	14697825712 -> 14614783360 [dir=none]
	14614783360 [label="condition
 (1, 2)" fillcolor=orange]
	14697825712 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14614892400 -> 14697825712
	14614892400 -> 14614783440 [dir=none]
	14614783440 [label="input
 (1, 2)" fillcolor=orange]
	14614892400 -> 14697986032 [dir=none]
	14697986032 [label="weight
 (2, 2)" fillcolor=orange]
	14614892400 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14614892352 -> 14614892400
	14614892352 -> 14614783200 [dir=none]
	14614783200 [label="condition
 (1, 2)" fillcolor=orange]
	14614892352 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14614892592 -> 14614892352
	14614892592 -> 14614783280 [dir=none]
	14614783280 [label="input
 (1, 2)" fillcolor=orange]
	14614892592 -> 14697862224 [dir=none]
	14697862224 [label="weight
 (2, 2)" fillcolor=orange]
	14614892592 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14614892880 -> 14614892592
	14614892880 -> 14614783040 [dir=none]
	14614783040 [label="condition
 (1, 2)" fillcolor=orange]
	14614892880 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14614892928 -> 14614892880
	14614892928 -> 14614783120 [dir=none]
	14614783120 [label="input
 (1, 4)" fillcolor=orange]
	14614892928 -> 14697985872 [dir=none]
	14697985872 [label="weight
 (2, 4)" fillcolor=orange]
	14614892928 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14614893216 -> 14614892928
	14614893216 -> 14614782640 [dir=none]
	14614782640 [label="condition
 (1, 4)" fillcolor=orange]
	14614893216 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14614893264 -> 14614893216
	14614893264 -> 14614781680 [dir=none]
	14614781680 [label="input
 (1, 4)" fillcolor=orange]
	14614893264 -> 14697985792 [dir=none]
	14697985792 [label="weight
 (4, 4)" fillcolor=orange]
	14614893264 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14614893552 -> 14614893264
	14614893552 -> 14697975632 [dir=none]
	14697975632 [label="condition
 (1, 4)" fillcolor=orange]
	14614893552 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14614893600 -> 14614893552
	14614893600 -> 14697984352 [dir=none]
	14697984352 [label="input
 (1, 8)" fillcolor=orange]
	14614893600 -> 14697984912 [dir=none]
	14697984912 [label="weight
 (4, 8)" fillcolor=orange]
	14614893600 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14614893888 -> 14614893600
	14614893888 -> 14697971248 [dir=none]
	14697971248 [label="condition
 (1, 8)" fillcolor=orange]
	14614893888 [label="WhereBackward0
-------------------------
condition: [saved tensor]"]
	14614893936 -> 14614893888
	14614893936 -> 14614618000 [dir=none]
	14614618000 [label="input
 (1, 10)" fillcolor=orange]
	14614893936 -> 14697974592 [dir=none]
	14697974592 [label="weight
 (8, 10)" fillcolor=orange]
	14614893936 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	14614894224 -> 14614893936
	14614894224 [label=ToCopyBackward0]
	14697833008 -> 14614894224
	14855836192 [label="
 (1, 10)" fillcolor=lightblue]
	14855836192 -> 14697833008
	14697833008 [label=AccumulateGrad]
	14614894128 -> 14614893936
	14697974592 [label="layers.0.weight
 (8, 10)" fillcolor=lightblue]
	14697974592 -> 14614894128
	14614894128 [label=AccumulateGrad]
	14614894176 -> 14614893936
	14697985152 [label="layers.0.bias
 (8)" fillcolor=lightblue]
	14697985152 -> 14614894176
	14614894176 [label=AccumulateGrad]
	14614894080 -> 14614893888
	14614894080 -> 14614970448 [dir=none]
	14614970448 [label="other
 ()" fillcolor=orange]
	14614894080 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14614893936 -> 14614894080
	14614893792 -> 14614893600
	14697984912 [label="layers.3.weight
 (4, 8)" fillcolor=lightblue]
	14697984912 -> 14614893792
	14614893792 [label=AccumulateGrad]
	14614893840 -> 14614893600
	14697985712 [label="layers.3.bias
 (4)" fillcolor=lightblue]
	14697985712 -> 14614893840
	14614893840 [label=AccumulateGrad]
	14614893744 -> 14614893552
	14614893744 -> 14614971248 [dir=none]
	14614971248 [label="other
 ()" fillcolor=orange]
	14614893744 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14614893600 -> 14614893744
	14614893456 -> 14614893264
	14697985792 [label="layers.6.weight
 (4, 4)" fillcolor=lightblue]
	14697985792 -> 14614893456
	14614893456 [label=AccumulateGrad]
	14614893504 -> 14614893264
	14697984752 [label="layers.6.bias
 (4)" fillcolor=lightblue]
	14697984752 -> 14614893504
	14614893504 [label=AccumulateGrad]
	14614893408 -> 14614893216
	14614893408 -> 14614972048 [dir=none]
	14614972048 [label="other
 ()" fillcolor=orange]
	14614893408 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14614893264 -> 14614893408
	14614893120 -> 14614892928
	14697985872 [label="layers.9.weight
 (2, 4)" fillcolor=lightblue]
	14697985872 -> 14614893120
	14614893120 [label=AccumulateGrad]
	14614893168 -> 14614892928
	14697985952 [label="layers.9.bias
 (2)" fillcolor=lightblue]
	14697985952 -> 14614893168
	14614893168 [label=AccumulateGrad]
	14614893072 -> 14614892880
	14614893072 -> 14614972848 [dir=none]
	14614972848 [label="other
 ()" fillcolor=orange]
	14614893072 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14614892928 -> 14614893072
	14614892784 -> 14614892592
	14697862224 [label="layers.12.weight
 (2, 2)" fillcolor=lightblue]
	14697862224 -> 14614892784
	14614892784 [label=AccumulateGrad]
	14614892832 -> 14614892592
	14697974512 [label="layers.12.bias
 (2)" fillcolor=lightblue]
	14697974512 -> 14614892832
	14614892832 [label=AccumulateGrad]
	14614892736 -> 14614892352
	14614892736 -> 14614973648 [dir=none]
	14614973648 [label="other
 ()" fillcolor=orange]
	14614892736 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14614892592 -> 14614892736
	14614892496 -> 14614892400
	14697986032 [label="layers.15.weight
 (2, 2)" fillcolor=lightblue]
	14697986032 -> 14614892496
	14614892496 [label=AccumulateGrad]
	14614892448 -> 14614892400
	14697986112 [label="layers.15.bias
 (2)" fillcolor=lightblue]
	14697986112 -> 14614892448
	14614892448 [label=AccumulateGrad]
	14614892256 -> 14697825712
	14614892256 -> 14614974448 [dir=none]
	14614974448 [label="other
 ()" fillcolor=orange]
	14614892256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	14614892400 -> 14614892256
	14697838432 -> 14697714816
	14697986192 [label="layers.18.weight
 (1, 2)" fillcolor=lightblue]
	14697986192 -> 14697838432
	14697838432 [label=AccumulateGrad]
	14547887936 -> 14697714816
	14697986272 [label="layers.18.bias
 (1)" fillcolor=lightblue]
	14697986272 -> 14547887936
	14547887936 [label=AccumulateGrad]
	14697714816 -> 14697968928
}
