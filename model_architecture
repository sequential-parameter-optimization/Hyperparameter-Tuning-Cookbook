digraph {
	graph [size="40.949999999999996,40.949999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	13769961584 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	13588103568 -> 13824517904 [dir=none]
	13824517904 [label="input
 (1, 2)" fillcolor=orange]
	13588103568 -> 13824515504 [dir=none]
	13824515504 [label="weight
 (1, 2)" fillcolor=orange]
	13588103568 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13625407808 -> 13588103568
	13625407808 -> 13824517984 [dir=none]
	13824517984 [label="other
 (1, 2)" fillcolor=orange]
	13625407808 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13769937568 -> 13625407808
	13769937568 -> 13824517744 [dir=none]
	13824517744 [label="input
 (1, 2)" fillcolor=orange]
	13769937568 -> 13824515344 [dir=none]
	13824515344 [label="weight
 (2, 2)" fillcolor=orange]
	13769937568 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13769995744 -> 13769937568
	13769995744 -> 13824517824 [dir=none]
	13824517824 [label="other
 (1, 2)" fillcolor=orange]
	13769995744 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824464048 -> 13769995744
	13824464048 -> 13824517584 [dir=none]
	13824517584 [label="input
 (1, 2)" fillcolor=orange]
	13824464048 -> 13824515184 [dir=none]
	13824515184 [label="weight
 (2, 2)" fillcolor=orange]
	13824464048 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824464240 -> 13824464048
	13824464240 -> 13824517664 [dir=none]
	13824517664 [label="other
 (1, 2)" fillcolor=orange]
	13824464240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824464384 -> 13824464240
	13824464384 -> 13824517424 [dir=none]
	13824517424 [label="input
 (1, 2)" fillcolor=orange]
	13824464384 -> 13824515024 [dir=none]
	13824515024 [label="weight
 (2, 2)" fillcolor=orange]
	13824464384 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824464672 -> 13824464384
	13824464672 -> 13824517504 [dir=none]
	13824517504 [label="other
 (1, 2)" fillcolor=orange]
	13824464672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824464624 -> 13824464672
	13824464624 -> 13824516144 [dir=none]
	13824516144 [label="input
 (1, 4)" fillcolor=orange]
	13824464624 -> 13824514864 [dir=none]
	13824514864 [label="weight
 (2, 4)" fillcolor=orange]
	13824464624 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824464960 -> 13824464624
	13824464960 -> 13824517344 [dir=none]
	13824517344 [label="other
 (1, 4)" fillcolor=orange]
	13824464960 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824464912 -> 13824464960
	13824464912 -> 13824515984 [dir=none]
	13824515984 [label="input
 (1, 4)" fillcolor=orange]
	13824464912 -> 13824514704 [dir=none]
	13824514704 [label="weight
 (4, 4)" fillcolor=orange]
	13824464912 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824465248 -> 13824464912
	13824465248 -> 13824517264 [dir=none]
	13824517264 [label="other
 (1, 4)" fillcolor=orange]
	13824465248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824465200 -> 13824465248
	13824465200 -> 13824513024 [dir=none]
	13824513024 [label="input
 (1, 4)" fillcolor=orange]
	13824465200 -> 13824514544 [dir=none]
	13824514544 [label="weight
 (4, 4)" fillcolor=orange]
	13824465200 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824465536 -> 13824465200
	13824465536 -> 13824512864 [dir=none]
	13824512864 [label="other
 (1, 4)" fillcolor=orange]
	13824465536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824465488 -> 13824465536
	13824465488 -> 13824516944 [dir=none]
	13824516944 [label="input
 (1, 4)" fillcolor=orange]
	13824465488 -> 13824514384 [dir=none]
	13824514384 [label="weight
 (4, 4)" fillcolor=orange]
	13824465488 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824465824 -> 13824465488
	13824465824 -> 13824517024 [dir=none]
	13824517024 [label="other
 (1, 4)" fillcolor=orange]
	13824465824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824465776 -> 13824465824
	13824465776 -> 13824507984 [dir=none]
	13824507984 [label="input
 (1, 8)" fillcolor=orange]
	13824465776 -> 13824514224 [dir=none]
	13824514224 [label="weight
 (4, 8)" fillcolor=orange]
	13824465776 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824466112 -> 13824465776
	13824466112 -> 13824508064 [dir=none]
	13824508064 [label="other
 (1, 8)" fillcolor=orange]
	13824466112 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824466064 -> 13824466112
	13824466064 -> 13824515904 [dir=none]
	13824515904 [label="input
 (1, 8)" fillcolor=orange]
	13824466064 -> 13824514064 [dir=none]
	13824514064 [label="weight
 (8, 8)" fillcolor=orange]
	13824466064 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824466400 -> 13824466064
	13824466400 -> 13824508144 [dir=none]
	13824508144 [label="other
 (1, 8)" fillcolor=orange]
	13824466400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824466352 -> 13824466400
	13824466352 -> 13824508304 [dir=none]
	13824508304 [label="input
 (1, 8)" fillcolor=orange]
	13824466352 -> 13824513904 [dir=none]
	13824513904 [label="weight
 (8, 8)" fillcolor=orange]
	13824466352 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824466688 -> 13824466352
	13824466688 -> 13824515824 [dir=none]
	13824515824 [label="other
 (1, 8)" fillcolor=orange]
	13824466688 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824466640 -> 13824466688
	13824466640 -> 13824509264 [dir=none]
	13824509264 [label="input
 (1, 16)" fillcolor=orange]
	13824466640 -> 13824513744 [dir=none]
	13824513744 [label="weight
 (8, 16)" fillcolor=orange]
	13824466640 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824466976 -> 13824466640
	13824466976 -> 13824509344 [dir=none]
	13824509344 [label="other
 (1, 16)" fillcolor=orange]
	13824466976 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824466928 -> 13824466976
	13824466928 -> 13824508464 [dir=none]
	13824508464 [label="input
 (1, 16)" fillcolor=orange]
	13824466928 -> 13824512784 [dir=none]
	13824512784 [label="weight
 (16, 16)" fillcolor=orange]
	13824466928 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824467216 -> 13824466928
	13824467216 -> 13824509424 [dir=none]
	13824509424 [label="other
 (1, 16)" fillcolor=orange]
	13824467216 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824467264 -> 13824467216
	13824467264 -> 13824509584 [dir=none]
	13824509584 [label="input
 (1, 32)" fillcolor=orange]
	13824467264 -> 13824513584 [dir=none]
	13824513584 [label="weight
 (16, 32)" fillcolor=orange]
	13824467264 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824467504 -> 13824467264
	13824467504 -> 13824508224 [dir=none]
	13824508224 [label="other
 (1, 32)" fillcolor=orange]
	13824467504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	13824467552 -> 13824467504
	13824467552 -> 13588168640 [dir=none]
	13588168640 [label="input
 (1, 10)" fillcolor=orange]
	13824467552 -> 13824439504 [dir=none]
	13824439504 [label="weight
 (32, 10)" fillcolor=orange]
	13824467552 [label="LinearBackward0
----------------------
input : [saved tensor]
weight: [saved tensor]"]
	13824467792 -> 13824467552
	13824467792 [label=ToCopyBackward0]
	13824467840 -> 13824467792
	13770074992 [label="
 (1, 10)" fillcolor=lightblue]
	13770074992 -> 13824467840
	13824467840 [label=AccumulateGrad]
	13824467888 -> 13824467552
	13824439504 [label="layers.0.weight
 (32, 10)" fillcolor=lightblue]
	13824439504 -> 13824467888
	13824467888 [label=AccumulateGrad]
	13824467648 -> 13824467552
	13824433904 [label="layers.0.bias
 (32)" fillcolor=lightblue]
	13824433904 -> 13824467648
	13824467648 [label=AccumulateGrad]
	13824467600 -> 13824467264
	13824513584 [label="layers.3.weight
 (16, 32)" fillcolor=lightblue]
	13824513584 -> 13824467600
	13824467600 [label=AccumulateGrad]
	13824467360 -> 13824467264
	13824513104 [label="layers.3.bias
 (16)" fillcolor=lightblue]
	13824513104 -> 13824467360
	13824467360 [label=AccumulateGrad]
	13824467312 -> 13824466928
	13824512784 [label="layers.6.weight
 (16, 16)" fillcolor=lightblue]
	13824512784 -> 13824467312
	13824467312 [label=AccumulateGrad]
	13824467072 -> 13824466928
	13824513664 [label="layers.6.bias
 (16)" fillcolor=lightblue]
	13824513664 -> 13824467072
	13824467072 [label=AccumulateGrad]
	13824467024 -> 13824466640
	13824513744 [label="layers.9.weight
 (8, 16)" fillcolor=lightblue]
	13824513744 -> 13824467024
	13824467024 [label=AccumulateGrad]
	13824466784 -> 13824466640
	13824513824 [label="layers.9.bias
 (8)" fillcolor=lightblue]
	13824513824 -> 13824466784
	13824466784 [label=AccumulateGrad]
	13824466736 -> 13824466352
	13824513904 [label="layers.12.weight
 (8, 8)" fillcolor=lightblue]
	13824513904 -> 13824466736
	13824466736 [label=AccumulateGrad]
	13824466496 -> 13824466352
	13824513984 [label="layers.12.bias
 (8)" fillcolor=lightblue]
	13824513984 -> 13824466496
	13824466496 [label=AccumulateGrad]
	13824466448 -> 13824466064
	13824514064 [label="layers.15.weight
 (8, 8)" fillcolor=lightblue]
	13824514064 -> 13824466448
	13824466448 [label=AccumulateGrad]
	13824466208 -> 13824466064
	13824514144 [label="layers.15.bias
 (8)" fillcolor=lightblue]
	13824514144 -> 13824466208
	13824466208 [label=AccumulateGrad]
	13824466160 -> 13824465776
	13824514224 [label="layers.18.weight
 (4, 8)" fillcolor=lightblue]
	13824514224 -> 13824466160
	13824466160 [label=AccumulateGrad]
	13824465920 -> 13824465776
	13824514304 [label="layers.18.bias
 (4)" fillcolor=lightblue]
	13824514304 -> 13824465920
	13824465920 [label=AccumulateGrad]
	13824465872 -> 13824465488
	13824514384 [label="layers.21.weight
 (4, 4)" fillcolor=lightblue]
	13824514384 -> 13824465872
	13824465872 [label=AccumulateGrad]
	13824465632 -> 13824465488
	13824514464 [label="layers.21.bias
 (4)" fillcolor=lightblue]
	13824514464 -> 13824465632
	13824465632 [label=AccumulateGrad]
	13824465584 -> 13824465200
	13824514544 [label="layers.24.weight
 (4, 4)" fillcolor=lightblue]
	13824514544 -> 13824465584
	13824465584 [label=AccumulateGrad]
	13824465344 -> 13824465200
	13824514624 [label="layers.24.bias
 (4)" fillcolor=lightblue]
	13824514624 -> 13824465344
	13824465344 [label=AccumulateGrad]
	13824465296 -> 13824464912
	13824514704 [label="layers.27.weight
 (4, 4)" fillcolor=lightblue]
	13824514704 -> 13824465296
	13824465296 [label=AccumulateGrad]
	13824465056 -> 13824464912
	13824514784 [label="layers.27.bias
 (4)" fillcolor=lightblue]
	13824514784 -> 13824465056
	13824465056 [label=AccumulateGrad]
	13824465008 -> 13824464624
	13824514864 [label="layers.30.weight
 (2, 4)" fillcolor=lightblue]
	13824514864 -> 13824465008
	13824465008 [label=AccumulateGrad]
	13824464768 -> 13824464624
	13824514944 [label="layers.30.bias
 (2)" fillcolor=lightblue]
	13824514944 -> 13824464768
	13824464768 [label=AccumulateGrad]
	13824464720 -> 13824464384
	13824515024 [label="layers.33.weight
 (2, 2)" fillcolor=lightblue]
	13824515024 -> 13824464720
	13824464720 [label=AccumulateGrad]
	13824464480 -> 13824464384
	13824515104 [label="layers.33.bias
 (2)" fillcolor=lightblue]
	13824515104 -> 13824464480
	13824464480 [label=AccumulateGrad]
	13824464432 -> 13824464048
	13824515184 [label="layers.36.weight
 (2, 2)" fillcolor=lightblue]
	13824515184 -> 13824464432
	13824464432 [label=AccumulateGrad]
	13824463952 -> 13824464048
	13824515264 [label="layers.36.bias
 (2)" fillcolor=lightblue]
	13824515264 -> 13824463952
	13824463952 [label=AccumulateGrad]
	13581953744 -> 13769937568
	13824515344 [label="layers.39.weight
 (2, 2)" fillcolor=lightblue]
	13824515344 -> 13581953744
	13581953744 [label=AccumulateGrad]
	13824464144 -> 13769937568
	13824515424 [label="layers.39.bias
 (2)" fillcolor=lightblue]
	13824515424 -> 13824464144
	13824464144 [label=AccumulateGrad]
	13588111248 -> 13588103568
	13824515504 [label="layers.42.weight
 (1, 2)" fillcolor=lightblue]
	13824515504 -> 13588111248
	13588111248 [label=AccumulateGrad]
	13588107696 -> 13588103568
	13824515584 [label="layers.42.bias
 (1)" fillcolor=lightblue]
	13824515584 -> 13588107696
	13588107696 [label=AccumulateGrad]
	13588103568 -> 13769961584
}
