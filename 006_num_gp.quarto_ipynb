{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "\n",
        "# Kriging (Gaussian Process Regression)\n",
        "\n",
        "<!-- bart21mSlides2022Lec-05 -->\n",
        "\n",
        "## DACE and RSM\n",
        "\n",
        "Mathematical models implemented in computer codes are used to circumvent the need for expensive field data collection. These models are particularly useful when dealing with highly nonlinear response surfaces, high signal-to-noise ratios (which often involve deterministic evaluations), and a global scope. As a result, a new approach is required in comparison to Response Surface Methodology (RSM).\n",
        "\n",
        "With the improvement in computing power and simulation fidelity, researchers gain higher confidence and a better understanding of the dynamics in physical, biological, and social systems. However, the expansion of configuration spaces and increasing input dimensions necessitates more extensive designs. High-performance computing (HPC) allows for thousands of runs, whereas previously only tens were possible. This shift towards larger models and training data presents new computational challenges.\n",
        "\n",
        "Research questions for DACE (Design and Analysis of Computer Experiments) include how to design computer experiments that make efficient use of computation and how to meta-model computer codes to save on simulation effort. The choice of surrogate model for computer codes significantly impacts the optimal experiment design, and the preferred model-design pairs can vary depending on the specific goal.\n",
        "\n",
        "The combination of computer simulation, design, and modeling with field data from similar real-world experiments introduces a new category of computer model tuning problems. The ultimate goal is to automate these processes to the greatest extent possible, allowing for the deployment of HPC with minimal human intervention.\n",
        "\n",
        "One of the remaining differences between RSM and DACE lies in how they handle noise. DACE employs replication, a technique that would not be used in a deterministic setting, to separate signal from noise. Traditional RSM is best suited for situations where a substantial proportion of the variability in the data is due to noise, and where the acquisition of data values can be severely limited. Consequently, RSM is better suited for a different class of problems, aligning with its intended purposes.\n",
        "\n",
        "Two very good texts on computer experiments and surrogate modeling are @Sant03a and @Forr08a. The former is the canonical reference in the statistics literature and the latter is perhaps more popular in engineering.\n",
        "\n",
        "\n",
        "## Background: Expectation, Mean, Standard Deviation\n",
        "\n",
        "The distribution of a random vector is characterized by some indexes. One of them is the expected value, which is defined as\n",
        "$$\n",
        "E[X] = \\sum_{x \\in D_X} xp_X(x)  \\qquad \\text{if $X$ is discrete}\n",
        "$$\n",
        "$$\n",
        "E[X] = \\int\\limits_{x \\in D_X} xf_X(x)\\mathrm{d}x  \\quad  \\text{if $X$ is continuous.}\n",
        "$$\n",
        "\n",
        "The mean, $\\mu$, of a probability distribution is a measure of its central tendency or location. That is, $E(X)$ is defined as the average of all possible values of $X$, weighted by their probabilities.\n",
        "\n",
        "\n",
        ":::{.callout-note}\n",
        "### Example: Expectation\n",
        "\n",
        "Let $X$ denote the number produced by rolling a fair die.\n",
        "Then\n",
        "$$\n",
        "E(X) = 1 \\times 1/6 + 2 \\times 1/6 + 3 \\times 1/6 + 4 \\times 1/6 + 5 \\times 1/6 + 6\\times 1/6 = 3.5.\n",
        "$$\n",
        ":::\n",
        "\n",
        "### Sample Mean\n",
        "\n",
        "The sample mean is an important estimate of the population mean. The sample mean of a sample $\\{x_i\\}$ ($i=1,2,\\ldots,n$) is defined as \n",
        "$$\\overline{x}  = \\frac{1}{n} \\sum_i x_i.$$\n",
        "\n",
        "### Variance and Standard Deviation\n",
        "\n",
        "If we are trying to predict the value of a random variable $X$ by its mean $\\mu = E(X)$, the error will be $X-\\mu$. In many situations it is useful to have an idea how large this deviation or error is.  Since $E(X-\\mu) = E(X) -\\mu = 0$, it is necessary to use the absolute value or the square of ($X-\\mu$).  The squared error is the first choice, because the derivatives are easier to calculate.  These considerations motivate the definition of the variance:\n",
        "\n",
        "The variance of a random variable $X$ is the mean squared deviation of $X$ from its expected value $\\mu = E(X)$.\n",
        "\\begin{equation}\n",
        "Var(X) = E[ (X-\\mu)^2].\n",
        "\\end{equation}\n",
        "\n",
        "### Standard Deviation\n",
        "\n",
        "Taking the square root of the variance to get back to the same scale of units as $X$ gives the standard deviation. The standard deviation of $X$ is the square root of the variance of $X$.\n",
        "\\begin{equation}\n",
        "sd(X) = \\sqrt{Var(X)}.\n",
        "\\end{equation}\n",
        "\n",
        "### Calculation of the Standard Deviation with Python\n",
        "\n",
        "The function `numpy.std` returns the standard deviation, a measure of the spread of a distribution, of the array elements.  The argument `ddof` specifies the Delta Degrees of Freedom. The divisor used in calculations is `N - ddof`, where `N` represents the number of elements.  By default `ddof` is zero, i.e., `std` uses the formula \n",
        "\\begin{equation}  \\sqrt{  \\frac{1}{N} \\sum_i \\left( x_i - \\bar{x} \\right)^2  } \\qquad \\text{with } \\quad \\bar{x} = \\sum_{i=1}^N x_i /N. \\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Example: Standard Deviation with Python\n",
        "\n",
        "Consider the array $[1,2,3]$:\n",
        "Since $\\bar{x} = 2$, the following value is computed: $$ \\sqrt{1/3 \\times \\left( (1-2)^2 + (2-2)^2 + (3-2)^2  \\right)} = \\sqrt{2/3}.$$\n"
      ],
      "id": "0eaf11a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "a = np.array([[1, 2, 3]])\n",
        "np.std(a)"
      ],
      "id": "0e08c01f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### The Empirical Standard Deviation\n",
        "\n",
        "The empirical standard deviation (which uses $N-1$),  $\\sqrt{1/2 \\times \\left( (1-2)^2 + (2-2)^2 + (3-2)^2  \\right)} = \\sqrt{2/2}$,  can be calculated as follows:\n"
      ],
      "id": "7a5875ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.std(a, ddof=1)"
      ],
      "id": "76fca592",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Argument \"axis\" \n",
        "\n",
        "::: {.callout-note}\n",
        "### Axes along which the standard deviation is computed\n",
        "\n",
        "* When you compute np.std with axis=0, it calculates the standard deviation along the vertical axis, meaning it computes the standard deviation for each column of the array.\n",
        "* On the other hand, when you compute np.std with axis=1, it calculates the standard deviation along the horizontal axis, meaning it computes the standard deviation for each row of the array.\n",
        "* If the axis parameter is not specified, np.std computes the standard deviation of the flattened array.\n",
        "\n",
        ":::\n"
      ],
      "id": "0a04d726"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A = np.array([[1, 2], [3, 4]])\n",
        "A"
      ],
      "id": "3c446ae3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.std(A)"
      ],
      "id": "de840c8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.std(A, axis=0)"
      ],
      "id": "e2ea4802",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.std(A, axis=1)"
      ],
      "id": "6c78a33c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Types and Precision in Python\n",
        "\n",
        "We consider single versus double precision in Python.\n",
        "In single precision, `std()` can be inaccurate:\n"
      ],
      "id": "af78e219"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = np.zeros((2, 4*4), dtype=np.float32)\n",
        "a[0, :] = 1.0\n",
        "a[1, :] = 0.1\n",
        "a "
      ],
      "id": "1e63291e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.std(a, axis=0)"
      ],
      "id": "21cd31b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.std(a, axis=1)"
      ],
      "id": "20c4fa15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "abs(0.45 - np.std(a))"
      ],
      "id": "77a592ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "### Float data types\n",
        "\n",
        "* float32 and float64 are data types in numpy that specify the precision of floating point numbers.\n",
        "* float32 is a single-precision floating point number that occupies 32 bits of memory. It has a precision of about 7 decimal digits.\n",
        "* float64 is a double-precision floating point number that occupies 64 bits of memory. It has a precision of about 15 decimal digits.\n",
        "* The main difference between float32 and float64 is the precision and memory usage. float64 provides a higher precision but uses more memory, while float32 uses less memory but has a lower precision. \n",
        ":::\n",
        "\n",
        "Computing the standard deviation in float64 is more accurate (result may vary), see [https://numpy.org/devdocs/reference/generated/numpy.std.html](https://numpy.org/devdocs/reference/generated/numpy.std.html).\n"
      ],
      "id": "576b7523"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "abs(0.45 - np.std(a, dtype=np.float64))"
      ],
      "id": "1a2ae04d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "#### Example: 32 versus 64 bit\n"
      ],
      "id": "323ce7b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a number\n",
        "num = 0.123456789123456789\n",
        "\n",
        "# Convert to float32 and float64\n",
        "num_float32 = np.float32(num)\n",
        "num_float64 = np.float64(num)\n",
        "\n",
        "# Print the number in both formats\n",
        "print(\"float32: \", num_float32)\n",
        "print(\"float64: \", num_float64)"
      ],
      "id": "f91ac8ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "The float32 data type in numpy represents a single-precision floating point number. It uses 32 bits of memory, which gives it a precision of about 7 decimal digits. On the other hand, float64 represents a double-precision floating point number. It uses 64 bits of memory, which gives it a precision of about 15 decimal digits.\n",
        "\n",
        "The reason float32 shows fewer digits is because it has less precision due to using less memory. The bits of memory are used to store the sign, exponent, and fraction parts of the floating point number, and with fewer bits, you can represent fewer digits accurately.\n",
        "\n",
        "## Distributions and Random Numbers in Python\n",
        "\n",
        "Results from computers are deterministic, so it sounds like a contradiction in terms to generate random numbers on a computer.\n",
        "Standard computers generate pseudo-randomnumbers, i.e., numbers that behave as if they were drawn randomly.\n",
        "\n",
        "::: {.callout-note}\n",
        "### Deterministic Random Numbers\n",
        "\n",
        "* Idea: Generate deterministically numbers that **look** (behave) as if they were drawn randomly.\n",
        "\n",
        ":::\n",
        "\n",
        "### The Uniform Distribution\n",
        "\n",
        "The probability density function of the uniform distribution is defined as:\n",
        "$$\n",
        "f_X(x) = \\frac{1}{b-a} \\qquad \\text{for $x \\in [a,b]$}.\n",
        "$$\n",
        "\n",
        "\n",
        "Generate 10 random numbers from a uniform distribution between $a=0$ and $b=1$:\n"
      ],
      "id": "5758bbd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "# Initialize the random number generator\n",
        "rng = np.random.default_rng(seed=123456789)\n",
        "n = 10\n",
        "x = rng.uniform(low=0.0, high=1.0, size=n)\n",
        "x"
      ],
      "id": "4c75f4f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate 10,000 random numbers from a uniform distribution between 0 and 10 and plot a histogram of the numbers:\n"
      ],
      "id": "5f4b78df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the random number generator\n",
        "rng = np.random.default_rng(seed=123456789)\n",
        "\n",
        "# Generate random numbers from a uniform distribution\n",
        "x = rng.uniform(low=0, high=10, size=10000)\n",
        "\n",
        "# Plot a histogram of the numbers\n",
        "plt.hist(x, bins=50, density=True, edgecolor='black')\n",
        "plt.title('Uniform Distribution [0,10]')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "id": "9fa2983f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Normal Distribution\n",
        "\n",
        "The probability density function of the normal distribution is defined as:\n",
        "$$\n",
        "f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right),\n",
        "$$  {#eq-normal-one}\n",
        "where:\n",
        "$\\mu$ is the mean;\n",
        "$\\sigma$ is the standard deviation.\n",
        "\n",
        "\n",
        "To generate ten random numbers from a normal distribution, the following command can be used.\n"
      ],
      "id": "bcce9173"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# generate 10 random numbers between from a normal distribution\n",
        "import numpy as np\n",
        "rng = np.random.default_rng()\n",
        "n = 10\n",
        "mu, sigma = 2, 0.1\n",
        "x = rng.normal(mu, sigma, n)\n",
        "x"
      ],
      "id": "46339a9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the mean:\n"
      ],
      "id": "9feb3620"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "abs(mu - np.mean(x))"
      ],
      "id": "1259cd2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: To verify the standard deviation, we use `ddof = 1` (empirical standard deviation):\n"
      ],
      "id": "3d371cd5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "abs(sigma - np.std(x, ddof=1))"
      ],
      "id": "e73f03e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A normally distributed random variable is a random variable whose associated probability distribution is the normal (or Gaussian) distribution. The normal distribution is a continuous probability distribution characterized by a symmetric bell-shaped curve.\n",
        "\n",
        "The distribution is defined by two parameters: the mean $\\mu$  and the standard deviation $\\sigma$. The mean indicates the center of the distribution, while the standard deviation measures the spread or dispersion of the distribution.\n",
        "\n",
        "This distribution is widely used in statistics and the natural and social sciences as a simple model for random variables with unknown distributions.\n"
      ],
      "id": "33625aee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def generate_normal_data(mu, sigma, num_samples):\n",
        "    return mu + sigma * np.random.randn(num_samples)\n",
        "\n",
        "def plot_normal_distribution(mu, sigma, num_samples):\n",
        "    # Generate normally distributed data\n",
        "    data = generate_normal_data(mu, sigma, num_samples)\n",
        "    \n",
        "    # Plot histogram\n",
        "    count, bins, ignored = plt.hist(data, 30, density=True)\n",
        "    \n",
        "    # Plot normal distribution curve\n",
        "    plt.plot(bins, norm.pdf(bins, mu, sigma), linewidth=2, color='r')\n",
        "    plt.show()"
      ],
      "id": "76d17586",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_normal_distribution(mu=0, sigma=1, num_samples=10000)"
      ],
      "id": "476942d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization of the Standard Deviation\n",
        "\n",
        "The standard deviation of normal distributed can be visualized in terms of the histogram of $X$:\n",
        "\n",
        "* about 68\\% of the values will lie in the interval within one standard deviation of the mean\n",
        "* 95% lie within two standard deviation of the mean\n",
        "* and 99.9% lie within 3 standard deviations of the mean. \n"
      ],
      "id": "84dbe65b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# example data\n",
        "mu = 0.0  # mean of distribution\n",
        "sigma = 1  # standard deviation of distribution\n",
        "x = mu + sigma * np.random.randn(2000)\n",
        "num_bins = 33\n",
        "fig, ax = plt.subplots()\n",
        "# the histogram of the data\n",
        "n, bins, patches = ax.hist(x, num_bins, density=1)\n",
        "# add a 'best fit' line\n",
        "y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
        "     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
        "ax.plot(bins, y, '--')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('Probability density')\n",
        "ax.set_title(r'Histogram: $\\mu=0$, $\\sigma=1$')\n",
        "ax.vlines(-1, ymin=0, ymax = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
        "     np.exp(-0.5 * (1 / sigma * (-1.0 - mu))**2)), colors=\"orange\", linestyles=\"-.\")\n",
        "ax.vlines(1, ymin=0, ymax = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
        "     np.exp(-0.5 * (1 / sigma * (1.0 - mu))**2)), colors=\"orange\", linestyles=\"-.\")\n",
        "# Tweak spacing to prevent clipping of ylabel\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "id": "4ec882f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standardization of Random Variables\n",
        "\n",
        "To compare statistical properties of random variables which use different units, it is a common practice to transform these random variables into standardized variables. \n",
        "If a random variable $X$ has expectation $E(X) = \\mu$ and standard deviation $sd(X) = \\sigma >0$, the random variable \n",
        "$$\n",
        "X^{\\ast} = (X-\\mu)/\\sigma\n",
        "$$ \n",
        "is called $X$ in standard units. \n",
        "It has $E(X^{\\ast}) = 0$ and $sd(X^{\\ast}) =1$.\n",
        "\n",
        "\n",
        "### Realizations of a Normal Distribution\n",
        "\n",
        "Realizations of a normal distribution refers to the actual values that you get when you draw samples from a normal distribution.\n",
        "Each sample drawn from the distribution is a realization of that distribution.\n",
        "\n",
        "For example, if you have a normal distribution with a mean of 0 and a standard deviation of 1, each number you draw from that distribution is a realization.\n",
        "\n",
        "Here's a Python example:\n"
      ],
      "id": "791982d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the parameters of the normal distribution\n",
        "mu = 0\n",
        "sigma = 1\n",
        "\n",
        "# Draw 10 samples (realizations) from the normal distribution\n",
        "realizations = np.random.normal(mu, sigma, 10)\n",
        "\n",
        "print(realizations)"
      ],
      "id": "726edd3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this code, np.random.normal generates 10 realizations of a normal distribution with a mean of 0 and a standard deviation of 1.\n",
        "The realizations array contains the actual values drawn from the distribution.\n",
        "\n",
        "### The Multivariate Normal Distribution\n",
        "\n",
        "The multivariate normal, multinormal, or Gaussian distribution serves as a generalization of the one-dimensional normal distribution to higher dimensions.\n",
        "We will consider $k$-dimensional random vectors $X = (X_1, X_2, \\ldots, X_k)$. When drawing samples from this distribution, it results in a set of values represented as $\\{x_1, x_2, \\ldots, x_k\\}$.\n",
        "To fully define this distribution, it is necessary to specify its mean $\\mu$ and covariance matrix $\\Sigma$. These parameters are analogous to the mean, which represents the central location, and the variance (squared standard deviation) of the one-dimensional normal distribution introduced in @eq-normal-one.\n",
        "\n",
        "In the context of the multivariate normal distribution, the mean takes the form of a coordinate within an $k$-dimensional space. This coordinate represents the location where samples are most likely to be generated, akin to the peak of the bell curve in a one-dimensional or univariate normal distribution.\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Covariance of two random variables\n",
        "\n",
        "For two random variables\n",
        "$X$\n",
        "and \n",
        "$Y$,\n",
        "the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values:\n",
        "$$\n",
        "\\operatorname{cov}(X, Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}\n",
        "$$\n",
        "\n",
        "\n",
        "The covariance within the multivariate normal distribution denotes the extent to which two variables vary together. \n",
        "The elements of the covariance matrix, such as $\\Sigma_{ij}$, represent the covariances between the variables $x_i$ and $x_j$. These covariances describe how the different variables in the distribution are related to each other in terms of their variability.\n",
        "\n",
        "The probability density function (PDF) of the multivariate normal distribution is defined as:\n",
        "$$\n",
        "f_X(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (x-\\mu)^T\\Sigma^{-1} (x-\\mu)\\right),\n",
        "$$\n",
        "where:\n",
        "$\\mu$ is the $k \\times 1$  mean vector;\n",
        "$\\Sigma$ is the  $k \\times k$ covariance matrix.\n",
        "The covariance matrix  $\\Sigma$ is assumed to be positive definite, so that its determinant is strictly positive.\n",
        "\n",
        "For discrete random variables, covariance can be written as:\n",
        "$$\n",
        "\\operatorname{cov} (X,Y) = \\frac{1}{n}\\sum_{i=1}^n (x_i-E(X)) (y_i-E(Y)).\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "@fig-bi9040 shows draws from a bivariate normal distribution with $\\mu = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ and \n",
        "$\\Sigma=\\begin{pmatrix} 9 & 4 \\\\ 4 & 9 \\end{pmatrix}$.\n"
      ],
      "id": "598bf966"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-bi9040\n",
        "#| fig-cap: 'Bivariate Normal. Mean zero and covariance $\\Sigma=\\begin{pmatrix} 9 & 4 \\\\ 4 & 9\\end{pmatrix}$'\n",
        "import numpy as np\n",
        "rng = np.random.default_rng()\n",
        "import matplotlib.pyplot as plt\n",
        "mean = [0, 0]\n",
        "cov = [[9, 4], [4, 9]]  # diagonal covariance\n",
        "x, y = rng.multivariate_normal(mean, cov, 1000).T\n",
        "# Create a scatter plot of the numbers\n",
        "plt.scatter(x, y, s=2)\n",
        "plt.axis('equal')\n",
        "plt.grid()\n",
        "plt.title(f\"Bivariate Normal. Mean zero and positive covariance: {cov}\")\n",
        "plt.show()"
      ],
      "id": "fig-bi9040",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The covariance matrix of a bivariate normal distribution determines the shape, orientation, and spread of the distribution in the two-dimensional space.\n",
        "\n",
        "The diagonal elements of the covariance matrix ($\\sigma_1^2$, $\\sigma_2^2$) are the variances of the individual variables. They determine the spread of the distribution along each axis. A larger variance corresponds to a greater spread along that axis.\n",
        "\n",
        "The off-diagonal elements of the covariance matrix ($\\sigma_{12}, \\sigma_{21}$) are the covariances between the variables.\n",
        "They determine the orientation and shape of the distribution.\n",
        "If the covariance is positive, the distribution is stretched along the line $y=x$,\n",
        "indicating that the variables tend to increase together.\n",
        "If the covariance is negative, the distribution is stretched along the line $y=-x$,\n",
        "indicating that one variable tends to decrease as the other increases.\n",
        "If the covariance is zero, the variables are uncorrelated and the distribution is axis-aligned.\n",
        "\n",
        "In @fig-bi9040, the variances are identical and the variables are correlated (covariance is 4), so the distribution is stretched along the line $y=x$.\n"
      ],
      "id": "a4f0672d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-bi90403d\n",
        "#| fig-cap: 'Bivariate Normal. Mean zero and covariance $\\Sigma=\\begin{pmatrix} 9 & 4 \\\\ 4 & 9\\end{pmatrix}$'\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# Parameters\n",
        "mu = np.array([0, 0])\n",
        "cov = np.array([[9, 4], [4, 9]])\n",
        "\n",
        "# Create grid and multivariate normal\n",
        "x = np.linspace(-10,10,100)\n",
        "y = np.linspace(-10,10,100)\n",
        "X, Y = np.meshgrid(x,y)\n",
        "pos = np.empty(X.shape + (2,))\n",
        "pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
        "rv = multivariate_normal(mu, cov)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')  \n",
        "surf=ax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\n",
        "ax.set_xlabel('X axis')\n",
        "ax.set_ylabel('Y axis')\n",
        "ax.set_zlabel('Z axis')\n",
        "ax.set_title('Bivariate Normal Distribution')\n",
        "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
        "plt.show()"
      ],
      "id": "fig-bi90403d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Bivariate Normal Distribution with Mean Zero and Zero Covariances $\\sigma_{12} = \\sigma_{21} = 0$\n",
        "\n",
        "\n",
        "$\\Sigma=\\begin{pmatrix} 9 & 0 \\\\ 0 & 9\\end{pmatrix}$\n"
      ],
      "id": "6081abb4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: fig-bi9000\n",
        "#| fig-cap: 'Bivariate Normal. Mean zero and covariance $\\Sigma=\\begin{pmatrix} 9 & 0 \\\\ 0 & 9\\end{pmatrix}$'\n",
        "import numpy as np\n",
        "rng = np.random.default_rng()\n",
        "import matplotlib.pyplot as plt\n",
        "mean = [0, 0]\n",
        "cov = [[9, 0], [0, 9]]  # diagonal covariance\n",
        "x, y = rng.multivariate_normal(mean, cov, 1000).T\n",
        "plt.scatter(x, y, s=2)\n",
        "plt.axis('equal')\n",
        "plt.grid()\n",
        "plt.title(f\"Bivariate Normal. Mean zero and covariance: {cov}\")\n",
        "plt.show()"
      ],
      "id": "fig-bi9000",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Bivariate Normal Distribution with Mean Zero and Negative Covariances $\\sigma_{12} = \\sigma_{21} = -4$\n",
        "\n",
        "$\\Sigma=\\begin{pmatrix} 9 & -4 \\\\ -4 & 9\\end{pmatrix}$\n"
      ],
      "id": "1f4d854f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: fig-bi9449\n",
        "#| fig-cap: 'Bivariate Normal. Mean zero and covariance $\\Sigma=\\begin{pmatrix} 9 & -4 \\\\ -4 & 9\\end{pmatrix}$'\n",
        "import numpy as np\n",
        "rng = np.random.default_rng()\n",
        "import matplotlib.pyplot as plt\n",
        "mean = [0, 0]\n",
        "cov = [[9, -4], [-4, 9]]  # diagonal covariance\n",
        "x, y = rng.multivariate_normal(mean, cov, 1000).T\n",
        "plt.scatter(x, y, s=2)\n",
        "plt.axis('equal')\n",
        "plt.grid()\n",
        "plt.title(f\"Bivariate Normal. Mean zero and covariance: {cov}\")\n",
        "plt.show()"
      ],
      "id": "fig-bi9449",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky Decomposition and Positive Definite Matrices\n",
        "\n",
        "The covariance matrix must be positive definite for a multivariate normal distribution for a couple of reasons:\n",
        "\n",
        "* Semidefinite vs Definite: A covariance matrix is always symmetric and positive semidefinite. However, for a multivariate normal distribution, it must be positive definite, not just semidefinite. This is because a positive semidefinite matrix can have zero eigenvalues, which would imply that some dimensions in the distribution have zero variance, collapsing the distribution in those dimensions. A positive definite matrix has all positive eigenvalues, ensuring that the distribution has positive variance in all dimensions.\n",
        "* Invertibility: The multivariate normal distribution's probability density function involves the inverse of the covariance matrix. If the covariance matrix is not positive definite, it may not be invertible, and the density function would be undefined.\n",
        "\n",
        "In summary, the covariance matrix being positive definite ensures that the multivariate normal distribution is well-defined and has positive variance in all dimensions.\n"
      ],
      "id": "ece74d53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def is_positive_definite(matrix):\n",
        "    return np.all(np.linalg.eigvals(matrix) > 0)\n",
        "\n",
        "matrix = np.array([[9, 4], [4, 9]])\n",
        "print(is_positive_definite(matrix))  # Outputs: True"
      ],
      "id": "57ae394a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More effficent (and check if symmetric) is based on Cholesky decomposition.\n"
      ],
      "id": "211597cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def is_pd(K):\n",
        "    try:\n",
        "        np.linalg.cholesky(K)\n",
        "        return True\n",
        "    except np.linalg.linalg.LinAlgError as err:\n",
        "        if 'Matrix is not positive definite' in err.message:\n",
        "            return False\n",
        "        else:\n",
        "            raise\n",
        "matrix = np.array([[9, 4], [4, 9]])\n",
        "print(is_pd(matrix))  # Outputs: True"
      ],
      "id": "5384e982",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "### Example: Cholesky decomposition.\n",
        "\n",
        "`linalg.cholesky` computes the Cholesky decomposition of a matrix, i.e., it computes a lower triangular matrix $L$ such that $LL^T = A$. If the matrix is not positive definite, an error (`LinAlgError`) is raised.\n"
      ],
      "id": "33d7d9bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a Hermitian, positive-definite matrix\n",
        "A = np.array([[9, 4], [4, 9]]) \n",
        "\n",
        "# Compute the Cholesky decomposition\n",
        "L = np.linalg.cholesky(A)\n",
        "\n",
        "print(\"L = \\n\", L)\n",
        "print(\"L*LT = \\n\", np.dot(L, L.T))"
      ],
      "id": "62634d0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: \n",
        "\n",
        "\n",
        "## Maximum Likelihood Estimation: Multivariate Normal Distribution\n",
        "\n",
        "Consider the first $n$ terms of an identically and independently distributed (i.i..d.) sequence ${X^{(j)}}$ of $k$-dimensional multivariate normal random vectors, i.e., $X^{(j)} \\sim N(\\mu, \\Sigma)$, $j=1,2,\\ldots$.\n",
        "The joint probability density function of the  $j$-th term of the sequence is\n",
        "$$\n",
        "f_X(x_j) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu)\\right),\n",
        "$$\n",
        "\n",
        "where:\n",
        "$\\mu$ is the $k \\times 1$  mean vector;\n",
        "$\\Sigma$ is the  $k \\times k$ covariance matrix.\n",
        "The covariance matrix $\\Sigma$ is assumed to be positive definite, so that its determinant is strictly positive.\n",
        "We use $x_1, \\ldots x_n$, i.e., the realizations of the first $n$ random vectors in the sequence, to estimate the two unknown parameters  $\\mu$ and  $\\Sigma$.\n",
        "\n",
        "The likelihood function is defined as the joint probability density function of the observed data, viewed as a function of the unknown parameters. Since the terms in the sequence are independent, their joint density is equal to the product of their marginal densities. As a consequence, the likelihood function can be written as the product of the individual densities:\n",
        "\n",
        "$$\n",
        "L(\\mu, \\Sigma) = \\prod_{j=1}^n f_X(x_j) = \\prod_{j=1}^n \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu)\\right)\n",
        "$$\n",
        "$$\n",
        "= \\frac{1}{(2\\pi)^{nk/2} \\det(\\Sigma)^{n/2}} \\exp\\left(-\\frac{1}{2} \\sum_{j=1}^n (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu)\\right).\n",
        "$$\n",
        "The log-likelihood function is\n",
        "$$\n",
        "\\ell(\\mu, \\Sigma) = -\\frac{nk}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\det(\\Sigma)) - \\frac{1}{2} \\sum_{j=1}^n (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu).\n",
        "$$\n",
        "The likelihood function is well-defined only if $\\det(\\Sigma)>0$.\n",
        "\n",
        "\n",
        "## Introduction to Gaussian Processes\n",
        "\n",
        "The concept of GP (Gaussian Process) regression can be understood as a simple extension of linear modeling. It is worth noting that this approach goes by various names and acronyms, including \"kriging,\" a term derived from geostatistics, as introduced by Matheron in 1963. Additionally, it is referred to as Gaussian spatial modeling or a Gaussian stochastic process, and machine learning (ML) researchers often use the term Gaussian process regression (GPR).\n",
        "In all of these instances, the central focus is on regression. This involves training on both inputs and outputs, with the ultimate objective of making predictions and quantifying uncertainty (referred to as uncertainty quantification or UQ).\n",
        "\n",
        "However, it's important to emphasize that GPs are not a universal solution for every problem. Specialized tools may outperform GPs in specific, non-generic contexts, and GPs have their own set of limitations that need to be considered.\n",
        "\n",
        "### Gaussian Process Prior\n",
        "\n",
        "In the context of GP, any finite collection of realizations, which is represented by $n$ observations, is modeled as having a multivariate normal (MVN) distribution. The characteristics of these realizations can be fully described by two key parameters:\n",
        "\n",
        "1. Their mean, denoted as an $n$-vector $\\mu$.\n",
        "2. The covariance matrix, denoted as an $n \\times n$ matrix $\\Sigma$. This covariance matrix encapsulates the relationships and variability between the individual realizations within the collection.\n",
        "\n",
        "\n",
        "### Covariance Function\n",
        "\n",
        "The covariance function is defined by inverse exponentiated squared Euclidean distance:\n",
        "$$\n",
        "\\Sigma(\\vec{x}, \\vec{x}') = \\exp\\{ - || \\vec{x} - \\vec{x}'||^2 \\},\n",
        "$$\n",
        "where $\\vec{x}$ and $\\vec{x}'$ are two points in the $k$-dimensional input space and $\\| \\cdot \\|$ denotes the Euclidean distance, i.e.,\n",
        "$$\n",
        "|| \\vec{x} - \\vec{x}'||^2 = \\sum_{i=1}^k (x_i - x_i')^2.\n",
        "$$\n",
        "\n",
        "An 1-d example is shown in @fig-exp2euclid. \n"
      ],
      "id": "c59efec1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def squared_euclidean_distance(point1, point2, sigma2=1.0):\n",
        "    return (point1 - point2)**2/ (2 * sigma2)\n",
        "\n",
        "def inverse_exp_squared_distance(point1, point2,sigma2):\n",
        "    return np.exp(-squared_euclidean_distance(point1, point2, sigma2))\n",
        "\n",
        "def generate_line(distance, step=0.01):\n",
        "    return np.arange(0, distance+step, step)\n",
        "\n",
        "def visualize_inverse_exp_squared_distance(distance, point, sigma2_values):\n",
        "    line = generate_line(distance)\n",
        "    \n",
        "    for sigma2 in sigma2_values:\n",
        "        distances = [inverse_exp_squared_distance(p, point, sigma2) for p in line]\n",
        "        plt.plot(line, distances, label=f'sigma2={sigma2}')\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "id": "8231f388",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-exp2euclid\n",
        "#| fig-cap: One-dim inverse exponentiated squared Euclidean distance\n",
        "visualize_inverse_exp_squared_distance(5, 0.0, [0.5, 1, 2.0])"
      ],
      "id": "fig-exp2euclid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The covariance function is also referred to as the kernel function. The *Gaussian* kernel uses an additional parameter, $\\sigma^2$, to control the rate of decay. This parameter is referred to as the length scale or the characteristic length scale. The covariance function is then defined as \n",
        "\n",
        "$$\n",
        "\\Sigma(\\vec{x}, \\vec{x}') = \\exp\\{ - || \\vec{x} - \\vec{x}'||^2 / (2 \\sigma^2) \\}.\n",
        "$$ {#eq-Sigma}\n",
        "\n",
        "The covariance decays exponentially fast as $\\vec{x}$ and $\\vec{x}'$ become farther apart. Observe that \n",
        "\n",
        "$$\n",
        "\\Sigma(\\vec{x},\\vec{x}) = 1\n",
        "$$ \n",
        "and \n",
        "\n",
        "$$\n",
        "\\Sigma(\\vec{x}, \\vec{x}') < 1\n",
        "$$ \n",
        "for  $\\vec{x} \\neq \\vec{x}'$. The function $\\Sigma(\\vec{x},\\vec{x}')$ must be positive definite.\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "#### Positive Definiteness\n",
        "\n",
        "Positive definiteness in the context of the covariance matrix $\\Sigma_n$ is a fundamental requirement. It is determined by evaluating $\\Sigma(x_i, x_j)$ at pairs of $n$ $\\vec{x}$-values, denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_n$. The condition for positive definiteness is that for all $\\vec{x}$ vectors that are not equal to zero, the expression $\\vec{x}^\\top \\Sigma_n \\vec{x}$ must be greater than zero. This property is essential when intending to use $\\Sigma_n$ as a covariance matrix in multivariate normal (MVN) analysis. It is analogous to the requirement in univariate Gaussian distributions where the variance parameter, $\\sigma^2$, must be positive.\n",
        "\n",
        "::: \n",
        "\n",
        "Gaussian Processes (GPs) can be effectively utilized to generate random data that follows a smooth functional relationship. The process involves the following steps:\n",
        "\n",
        "1. Select a set of $\\vec{x}$-values, denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_n$.\n",
        "2. Define the covariance matrix $\\Sigma_n$ by evaluating $\\Sigma_n^{ij} = \\Sigma(\\vec{x}_i, \\vec{x}_j)$ for $i, j = 1, 2, \\ldots, n$.\n",
        "3. Generate an $n$-variate realization $Y$ that follows a multivariate normal distribution with a mean of zero and a covariance matrix $\\Sigma_n$, expressed as $Y \\sim \\mathcal{N}_n(0, \\Sigma_n)$.\n",
        "4. Visualize the result by plotting it in the $x$-$y$ plane.\n",
        "\n",
        "### Construction of the Covariance Matrix\n",
        "\n",
        "Here is an one-dimensional example. The process begins by creating an input grid using $\\vec{x}$-values. This grid consists of 100 elements, providing the basis for further analysis and visualization.\n"
      ],
      "id": "06766c7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "n = 100\n",
        "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)"
      ],
      "id": "0e2ecdad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the context of this discussion, the construction of the covariance matrix, denoted as $\\Sigma_n$, relies on the concept of inverse exponentiated squared Euclidean distances. However, it's important to note that a modification is introduced later in the process. Specifically, the diagonal of the covariance matrix is augmented with a small value, represented as \"eps\" or $\\epsilon$.\n",
        "\n",
        "The reason for this augmentation is that while inverse exponentiated distances theoretically ensure the covariance matrix's positive definiteness, in practical applications, the matrix can sometimes become numerically ill-conditioned. By adding a small value to the diagonal, such as $\\epsilon$, this ill-conditioning issue is mitigated. In this context, $\\epsilon$ is often referred to as \"jitter.\"\n"
      ],
      "id": "e0527d21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
        "from numpy.linalg import cholesky, solve\n",
        "from numpy.random import multivariate_normal\n",
        "def build_Sigma(X, sigma2):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = 1/(2*sigma2[l])*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)  "
      ],
      "id": "25fba1b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sigma2 = np.array([1.0])\n",
        "Sigma = build_Sigma(X, sigma2)\n",
        "np.round(Sigma[:3,:], 3)"
      ],
      "id": "bcbc16e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(Sigma, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "id": "676b65e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generation of Random Samples and Plotting the Realizations of the Random Function\n",
        "\n",
        "In the context of the multivariate normal distribution, the next step is to utilize the previously constructed covariance matrix denoted as `Sigma`. It is used as an essential component in generating random samples from the multivariate normal distribution.\n",
        "\n",
        "The function `multivariate_normal` is employed for this purpose. It serves as a random number generator specifically designed for the multivariate normal distribution. In this case, the mean of the distribution is set equal to `mean`, and the covariance matrix is provided as `Psi`. The argument `size` specifies the number of realizations, which, in this specific scenario, is set to one.\n",
        "\n",
        "By default, the mean vector is initialized to zero. To match the number of samples, which is equivalent to the number of rows in the `X` and `Sigma` matrices, the argument `zeros(n)` is used, where `n` represents the number of samples (here taken from the size of the matrix, e.g.,: `Sigma.shape[0]`).\n"
      ],
      "id": "e32047eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng = np.random.default_rng(seed=12345)\n",
        "Y = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = 1, check_valid=\"raise\").reshape(-1,1)\n",
        "Y.shape"
      ],
      "id": "fd1990f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can plot the results, i.e., a finite realization of the random function $Y()$ under a GP prior with a particular covariance structure. We will plot those `X` and `Y` pairs as connected points on an  $x$-$y$ plane.\n"
      ],
      "id": "a2638423"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn1-1\n",
        "#| fig-cap: 'Realization of one random function under a GP prior. sigma2: 1.0'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, Y)\n",
        "plt.title(\"Realization of Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\n",
        "plt.show()"
      ],
      "id": "fig-mvn1-1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn1-3\n",
        "#| fig-cap: 'Realization of three random functions under a GP prior. sigma2: 1.0'\n",
        "rng = np.random.default_rng(seed=12345)\n",
        "Y = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = 3, check_valid=\"raise\")\n",
        "plt.plot(X, Y.T)\n",
        "plt.title(\"Realization of Three Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\n",
        "plt.show()"
      ],
      "id": "fig-mvn1-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Properties of the 1d Example\n",
        "\n",
        "#### Several Bumps:\n",
        "In this analysis, we observe several bumps in the $x$-range of $[0,10]$.\n",
        "These bumps in the function occur because shorter distances exhibit high correlation, while longer distances tend to be essentially uncorrelated. This leads to variations in the function's behavior:\n",
        "\n",
        "* When $x$ and $x'$ are one $\\sigma$ unit apart, the correlation is $\\exp\\left(-\\sigma^2 / (2\\sigma^2)\\right) = \\exp(-1/2) \\approx 0.61$, i.e., a relative high correlation.\n",
        "* $2\\sigma$ apart means correlation $\\exp( 2^2 /2) \\approx 0.14$, i.e., only small correlation.\n",
        "* $4\\sigma$ apart means correlation $\\exp( 4^2 /2) \\approx 0.0003$, i.e., nearly no correlation---variables are considered independent for almost all practical application.\n",
        "\n",
        "\n",
        "#### Smoothness:\n",
        "The function plotted in @fig-mvn1-1 represents only a finite realization, which means that we have data for a limited number of pairs, specifically 100 points. These points appear smooth in a tactile sense because they are closely spaced, and the plot function connects the dots with lines to create the appearance of smoothness. The complete surface, which can be conceptually extended to an infinite realization over a compact domain, is exceptionally smooth in a calculus sense due to the covariance function's property of being infinitely differentiable.\n",
        "\n",
        "#### Scale of Two:\n",
        "Regarding the scale of the $Y$ values, they have a range of approximately $[-2,2]$, with a 95% probability of falling within this range. In standard statistical terms, 95% of the data points typically fall within two standard deviations of the mean, which is a common measure of the spread or range of data.\n"
      ],
      "id": "13ba85a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
        "from numpy.random import multivariate_normal\n",
        "\n",
        "def build_Sigma(X, sigma2):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = 1/(2*sigma2[l])*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)\n",
        "\n",
        "def plot_mvn( a=0, b=10, sigma2=1.0, size=1, n=100, show=True):    \n",
        "    X = np.linspace(a, b, n, endpoint=False).reshape(-1,1)\n",
        "    sigma2 = np.array([sigma2])\n",
        "    Sigma = build_Sigma(X, sigma2)\n",
        "    rng = np.random.default_rng(seed=12345)\n",
        "    Y = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = size, check_valid=\"raise\")\n",
        "    plt.plot(X, Y.T)\n",
        "    plt.title(\"Realization of Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\n",
        "    if show:\n",
        "        plt.show()"
      ],
      "id": "d88c0ba1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn2\n",
        "#| fig-cap: 'Realization of Random Functions under a GP prior. sigma2: 10'\n",
        "plot_mvn(a=0, b=10, sigma2=10.0, size=3, n=250)"
      ],
      "id": "fig-mvn2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mvn5\n",
        "#| fig-cap: 'Realization of Random Functions under a GP prior. sigma2: 0.1'\n",
        "plot_mvn(a=0, b=10, sigma2=0.1, size=3, n=250)"
      ],
      "id": "fig-mvn5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kriging: Modeling Basics\n",
        "\n",
        "### The Kriging Idea in a Nutshell\n",
        "\n",
        "We consider observed data of an unknown function $f$ at $n$ points $x_1, \\ldots, x_n$, see @fig-unknownf.\n",
        "These measurements a considered as realizations of MVN random variables $Y_1, \\ldots, Y_n$ with mean $\\mu$ and covariance matrix $\\Sigma_n$ as shown in @fig-mvn1-3, @fig-mvn2 or @fig-mvn5. In Kriging, a more general covariance matrix (or equivalently, a correlation matrix $\\Psi$) is used, see @eq-krigingbase.\n",
        "Using a maximum likelihood approach, we can estimate the unknown parameters $\\mu$ and $\\Sigma_n$ from the data so that the likelihood function is maximized.\n"
      ],
      "id": "ceb6f681"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-unknownf\n",
        "#| fig-cap: Eight measurements of an unknown function\n",
        "#| echo: false\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "n = 8\n",
        "X = np.linspace(0, 2*np.pi, n, endpoint=False).reshape(-1,1)\n",
        "y = 3.5 * np.sin(X)*np.cos(X)\n",
        "plt.plot(X, y, \"bo\", label=\"Measurements\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "id": "fig-unknownf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Kriging Basis Function\n",
        "\n",
        "$k$-dimensional basis functions of the form \n",
        "$$\n",
        "\\psi(\\vec{x}^{(i)}, \\vec{x}^{(j)}) = \\exp \\left( - \\sum_{l=1}^k \\theta_l | x_{l}^{(i)} - x_{l}^{(j)} | ^{p_l} \\right)\n",
        "$$ {#eq-krigingbase}\n",
        "are used in a method known as Kriging. Note, $\\vec{x}^{(i)}$ denotes the $k$-dim vector $\\vec{x}^{(i)}= (x_1^{(i)}, \\ldots, x_k^{(i)})^T$.\n",
        "\n",
        "The Kriging basis function is related to the 1-dim Gaussian basis function (@eq-Sigma),\n",
        "which is defined as\n",
        "$$\n",
        "\\Sigma(\\vec{x}^{(i)}, \\vec{x}^{(j)}) = \\exp\\{ - || \\vec{x}^{(i)} - \\vec{x}^{(j)}||^2 / (2\\sigma^2) \\}.\n",
        "$$ {#eq-Sigma2}\n",
        "\n",
        "There are some differences between Gaussian basis functions and Kriging basis functions:\n",
        "\n",
        "  * Where the Gaussian basis function has $1/(2\\sigma^2)$, the Kriging basis has a vector $\\theta = [\\theta_1, \\theta_2, \\ldots, \\theta_k]^T$.\n",
        "  * The $\\theta$ vector allows the width of the basis function to vary from dimension to dimension.\n",
        "  * In the Gaussian basis function, the exponent is fixed at 2, Kriging allows this exponent $p_l$ to vary (typically from 1 to 2).\n",
        "\n",
        "### The Correlation Coefficient\n",
        "\n",
        "In a bivariate normal distribution, the covariance matrix and the correlation coefficient are closely related.\n",
        "The covariance matrix $\\Sigma$ for a bivariate normal distribution is a $2\\times 2$ matrix that looks like this:\n",
        "\n",
        "$$\n",
        "\\Sigma =\n",
        "\\begin{pmatrix}\n",
        "\\sigma_1^2 & \\sigma_{12}\\\\\n",
        "\\sigma_{21} & \\sigma_2^2\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "where $\\sigma_1^2$ and $\\sigma_2^2$ are the variances of $X_1$ and $X_2$, and $\\sigma_{12} = \\sigma_{21}$ is the covariance between $X_1$ and $X_2$.\n",
        "\n",
        "The correlation coefficient, often denoted as $\\rho$, is a normalized measure of the linear relationship between two variables. It is calculated from the covariance and the standard deviations $\\sigma_1$ and $\\sigma_2$ (or the square roots of the variances) of $X_1$ and $X_2$ as follows:\n",
        "$$\n",
        "\\rho = \\sigma_{12} / (\\sqrt{\\sigma_1^2} \\times \\sqrt{\\sigma_2^2}) = \\sigma_{12} / (\\sigma_1 \\times \\sigma_2).\n",
        "$$\n",
        "\n",
        "So we can express the correlation coefficient $\\rho$ in terms of the elements of the covariance matrix $\\Sigma$. It can be interpreted as follows:\n",
        "The correlation coefficient ranges from -1 to 1. A value of 1 means that $X_1$ and $X_2$ are perfectly positively correlated, a value of -1 means they are perfectly negatively correlated, and a value of 0 means they are uncorrelated. This gives the same information as the covariance, but on a standardized scale that does not depend on the units of $X_1$ and $X_2$.\n",
        "\n",
        "### Covariance Matrix and Correlation Matrix\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "### Covariance and Correlation (taken from @Forr08a)\n",
        "\n",
        "Covariance is a measure of the correlation between two or more sets of random variables.\n",
        "\n",
        "$$\n",
        "\\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\n",
        "$$\n",
        "\n",
        "From the covariance, we can derive the correlation\n",
        "\n",
        "$$\n",
        "\\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X\\sigma_Y}.\n",
        "$$ {#eq-corrxy}\n",
        "\n",
        "For a vector of random variables \n",
        "\n",
        "$$\n",
        "Y = \n",
        "\\begin{pmatrix}\n",
        "(Y^{(l)}, \\ldots, Y^{(n)})\n",
        "\\end{pmatrix}^T\n",
        "$$\n",
        "\n",
        "the covariance matrix is a matrix of covariances between the random variables\n",
        "\n",
        "$$\n",
        "\\Sigma =\n",
        "\\text{Cov}(Y, Y) =\n",
        "\\begin{pmatrix}\n",
        "\\text{Cov}(Y^{(1)}, Y^{(1)}) & \\ldots & \\text{Cov}(Y^{(1)}, Y^{(n)}) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\text{Cov}(Y^{(n)}, Y^{(1)}) & \\ldots & \\text{Cov}(Y^{(n)}, Y^{(n)})\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "and from @eq-corrxy\n",
        "\n",
        "$$\n",
        "\\text{Cov}(Y, Y) = \\sigma_Y^2 \\text{Cor}(Y, Y).\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "You can compute the correlation matrix $\\Psi$ from a covariance matrix $\\Sigma$ in Python using the numpy library. The correlation matrix is computed by dividing each element of the covariance matrix by the product of the standard deviations of the corresponding variables.\n",
        "\n",
        "The function `covariance_to_correlation` first computes the standard deviations of the variables with `np.sqrt(np.diag(cov))`. It then computes the correlation matrix by dividing each element of the covariance matrix by the product of the standard deviations of the corresponding variables with `cov / np.outer(std_devs, std_devs)`.\n"
      ],
      "id": "263bdacd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def covariance_to_correlation(cov):\n",
        "    # Compute standard deviations\n",
        "    std_devs = np.sqrt(np.diag(cov))\n",
        "    \n",
        "    # Compute correlation matrix\n",
        "    corr = cov / np.outer(std_devs, std_devs)\n",
        "    \n",
        "    return corr\n",
        "\n",
        "cov = np.array([[9, -4], [-4, 9]])\n",
        "print(covariance_to_correlation(cov))"
      ],
      "id": "deb49579",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Kriging Model\n",
        "\n",
        "Consider sample data $\\vec{X}$ and $\\vec{y}$ from $n$ locations that are available in matrix form:\n",
        "$\\vec{X}$ is a $(n \\times k)$ matrix, where $k$ denotes the problem dimension and\n",
        "$\\vec{y}$ is a $(n\\times 1)$ vector.\n",
        "\n",
        "The observed responses $\\vec{y}$ are considered as if they are from a stochastic\n",
        "process, which will be denoted as\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\vec{Y}(\\vec{x}^{(1)})\\\\\n",
        "\\vdots\\\\\n",
        "\\vec{Y}(\\vec{x}^{(n)})\\\\\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "The set of random vectors (also referred to as a *random field*) has a mean of $\\vec{1} \\mu$, which is a $(n\\times 1)$ vector.\n",
        "\n",
        "### Correlations\n",
        "\n",
        "The random vectors are correlated with each other using the basis function expression from @eq-krigingbase:\n",
        "$$\n",
        "\\text{cor} \\left(\\vec{Y}(\\vec{x}^{(i)}),\\vec{Y}(\\vec{x}^{(l)}) \\right) = \\exp\\left\\{ - \\sum_{j=1}^k \\theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\\right\\}.\n",
        "$$\n",
        "\n",
        "The $(n \\times n)$ correlation matrix of the observed sample data is \n",
        "\n",
        "$$\n",
        "\\vec{\\Psi} = \\begin{pmatrix}\n",
        "\\text{cor}\\left(\n",
        "\\vec{Y}(\\vec{x}^{(i)}),\n",
        "\\vec{Y}(\\vec{x}^{(l)}) \n",
        "\\right) & \\ldots &\n",
        "\\text{cor}\\left(\n",
        "\\vec{Y}(\\vec{x}^{(i)}),\n",
        "\\vec{Y}(\\vec{x}^{(l)}) \n",
        "\\right)\\\\\n",
        "\\vdots  & \\vdots &  \\vdots\\\\\n",
        " \\text{cor}\\left(\n",
        "\\vec{Y}(\\vec{x}^{(i)}),\n",
        "\\vec{Y}(\\vec{x}^{(l)}) \n",
        "\\right)&\n",
        "\\ldots &\n",
        "\\text{cor}\\left(\n",
        "\\vec{Y}(\\vec{x}^{(i)}),\n",
        "\\vec{Y}(\\vec{x}^{(l)}) \n",
        "\\right)\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Note: correlations depend on the absolute distances between sample points\n",
        "$|x_j^{(n)} - x_j^{(n)}|$ and the parameters $p_j$ and $\\theta_j$.\n",
        "\n",
        "Correlation is intuitive, because when two points move close together, then $|x_l^{(i)} - x_l| \\to 0$ and $\\exp(-|x_l^{(i)} - x_l| \\to 1$, points show very close correlation and $Y(x_l^{(i)}) = Y(x_l)$.\n",
        "\n",
        "$\\theta$ can be seen as a width parameter:\n",
        "\n",
        "* low $\\theta_j$ means that all points will have a high correlation, with $Y(x_j)$ being similar across the sample.\n",
        "* high $\\theta_j$ means that there is a significant difference between the $Y(x_j)$'s.\n",
        "* $\\theta_j$ is a measure of how active the function we are approximating is.\n",
        "* High $\\theta_j$ indicate important parameters, see @fig-theta12.\n"
      ],
      "id": "629f8388"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def squared_euclidean_distance(point1, point2, theta=1.0):\n",
        "    return theta* (point1 - point2)**2\n",
        "\n",
        "def inverse_exp_squared_distance(point1, point2,theta):\n",
        "    return np.exp(-squared_euclidean_distance(point1, point2, theta))\n",
        "\n",
        "def generate_line(distance, step=0.01):\n",
        "    return np.arange(0, distance+step, step)\n",
        "\n",
        "def visualize_inverse_exp_squared_distance(distance, point, theta_values):\n",
        "    line = generate_line(distance)\n",
        "    \n",
        "    for theta in theta_values:\n",
        "        distances = [inverse_exp_squared_distance(p, point, theta) for p in line]\n",
        "        plt.plot(line, distances, label=f'theta={theta}')\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "id": "4c05b095",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-theta12\n",
        "#| fig-cap: 'Theta set to 1/2, 1, and 2'\n",
        "visualize_inverse_exp_squared_distance(5, 0, theta_values=[0.5, 1, 2.0])"
      ],
      "id": "fig-theta12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "#### Example: The Correlation Matrix (Detailed Computation)\n",
        "\n",
        "Let $n=4$ and $k=3$. The sample plan is represented by the following matrix $X$: \n",
        "$$\n",
        "X = \\begin{pmatrix} x_{11} & x_{12} & x_{13}\\\\\n",
        "x_{21} & x_{22} & x_{23}\\\\\n",
        "x_{31} & x_{32} & x_{33}\\\\\n",
        "x_{41} & x_{42} & x_{43}\\\\ \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "To compute the elements of the matrix $\\Psi$, the following $k$ (one for each of the $k$ dimensions) $(n,n)$-matrices have to be computed:\n",
        "$$\n",
        "D_1 = \\begin{pmatrix} x_{11} - x_{11} & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\\\  x_{21} - x_{11} & x_{21} - x_{21} & x_{21} -x_{31} & x_{21} - x_{41} \\\\ x_{31} - x_{11} & x_{31} - x_{21} & x_{31} -x_{31} & x_{31} - x_{41} \\\\ x_{41} - x_{11} & x_{41} - x_{21} & x_{41} -x_{31} & x_{41} - x_{41} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "D_2 = \\begin{pmatrix} x_{12} - x_{12} & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\\\  x_{22} - x_{12} & x_{22} - x_{22} & x_{22} -x_{32} & x_{22} - x_{42} \\\\ x_{32} - x_{12} & x_{32} - x_{22} & x_{32} -x_{32} & x_{32} - x_{42} \\\\ x_{42} - x_{12} & x_{42} - x_{22} & x_{42} -x_{32} & x_{42} - x_{42} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "D_3 = \\begin{pmatrix} x_{13} - x_{13} & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\\\  x_{23} - x_{13} & x_{23} - x_{23} & x_{23} -x_{33} & x_{23} - x_{43} \\\\ x_{33} - x_{13} & x_{33} - x_{23} & x_{33} -x_{33} & x_{33} - x_{43} \\\\ x_{43} - x_{13} & x_{43} - x_{23} & x_{43} -x_{33} & x_{43} - x_{43} \\\\\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Since the matrices are symmetric and the main diagonals are zero, it is sufficient to compute the following matrices:\n",
        "$$\n",
        "D_1 = \\begin{pmatrix} 0 & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\\\  0 &  0 & x_{21} -x_{31} & x_{21} - x_{41} \\\\ 0 & 0 & 0 & x_{31} - x_{41} \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n",
        "$$\n",
        "$$\n",
        "D_2 = \\begin{pmatrix} 0 & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\\\  0 & 0 & x_{22} -x_{32} & x_{22} - x_{42} \\\\ 0 & 0 & 0 & x_{32} - x_{42} \\\\ 0 & 0 & 0 & 0 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "D_3 = \\begin{pmatrix} 0 & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\\\  0 & 0 & x_{23} -x_{33} & x_{23} - x_{43} \\\\ 0 & 0 & 0 & x_{33} - x_{43} \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We will consider $p_l=2$. The differences will be squared and multiplied by $\\theta_i$, i.e.:\n",
        "\n",
        "$$\n",
        "D_1 = \\theta_1 \\begin{pmatrix} 0 & (x_{11} - x_{21})^2 & (x_{11} -x_{31})^2 & (x_{11} - x_{41})^2 \\\\  0 &  0 & (x_{21} -x_{31})^2 & (x_{21} - x_{41})^2 \\\\ 0 & 0 & 0 & (x_{31} - x_{41})^2 \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "D_2 = \\theta_2 \\begin{pmatrix} 0 & (x_{12} - x_{22})^2 & (x_{12} -x_{32})^2 & (x_{12} - x_{42})^2 \\\\  0 & 0 & (x_{22} -x_{32})^2 & (x_{22} - x_{42})^2 \\\\ 0 & 0 & 0 & (x_{32} - x_{42})^2 \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "D_3 = \\theta_3 \\begin{pmatrix} 0 & (x_{13} - x_{23})^2 & (x_{13} -x_{33})^2 & (x_{13} - x_{43})^2 \\\\  0 & 0 & (x_{23} -x_{33})^2 & (x_{23} - x_{43})^2 \\\\ 0 & 0 & 0 & (x_{33} - x_{43})^2 \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "The sum of the three matrices $D=D_1+ D_2 + D_3$ will be calculated next: \n",
        "\n",
        "$$\n",
        "\\begin{pmatrix} 0 & \n",
        "\\theta_1  (x_{11} - x_{21})^2 + \\theta_2 (x_{12} - x_{22})^2 + \\theta_3  (x_{13} - x_{23})^2  &\n",
        "\\theta_1 (x_{11} -x_{31})^2 + \\theta_2  (x_{12} -x_{32})^2 + \\theta_3  (x_{13} -x_{33})^2 &\n",
        "\\theta_1  (x_{11} - x_{41})^2 + \\theta_2  (x_{12} - x_{42})^2 + \\theta_3 (x_{13} - x_{43})^2\n",
        "\\\\  0 &  0 & \n",
        "\\theta_1  (x_{21} -x_{31})^2 + \\theta_2 (x_{22} -x_{32})^2 + \\theta_3  (x_{23} -x_{33})^2 &\n",
        "\\theta_1  x_{21} - x_{41})^2 + \\theta_2  (x_{22} - x_{42})^2 + \\theta_3 (x_{23} - x_{43})^2\n",
        "\\\\ 0 & 0 & 0 & \n",
        "\\theta_1 (x_{31} - x_{41})^2 + \\theta_2 (x_{32} - x_{42})^2 + \\theta_3 (x_{33} - x_{43})^2\n",
        "\\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Finally, $$ \\Psi = \\exp(-D)$$ is computed.\n",
        "\n",
        "Next, we will demonstrate how this computation can be implemented in Python.\n"
      ],
      "id": "ec9d0bfb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from numpy import (array, zeros, power, ones, exp, multiply,\n",
        "                    eye, linspace, mat, spacing, sqrt, arange,\n",
        "                    append, ravel)\n",
        "from numpy.linalg import cholesky, solve\n",
        "theta = np.array([1,2,3])\n",
        "X = np.array([ [1,0,0], [0,1,0], [100, 100, 100], [101, 100, 100]])\n",
        "X"
      ],
      "id": "d38b51ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_Psi(X, theta):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)  "
      ],
      "id": "26ec1964",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Psi = build_Psi(X, theta)\n",
        "Psi"
      ],
      "id": "0ebac03c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.callout-note}\n",
        "### Example: The Correlation Matrix (Using Existing Functions)\n",
        "\n",
        "The same result as computed in the previous example can be obtained with existing python functions, e.g., from the package `scipy`.\n"
      ],
      "id": "15a484d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.spatial.distance import squareform\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "def build_Psi(X, theta, eps=sqrt(spacing(1))):\n",
        "    return exp(- squareform(pdist(X,\n",
        "                            metric='sqeuclidean',\n",
        "                            out=None,\n",
        "                            w=theta))) +  multiply(eye(X.shape[0]),\n",
        "                                                   eps)\n",
        "\n",
        "Psi = build_Psi(X, theta, eps=.0)\n",
        "Psi"
      ],
      "id": "05cbf0e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### The Condition Number\n",
        "\n",
        "A small value, `eps`, can be passed to the function `build_Psi` to improve the condition number. For example, `eps=sqrt(spacing(1))` can be used. The numpy function `spacing()` returns the distance between a number and its nearest adjacent number.\n",
        "\n",
        "The condition number of a matrix is a measure of its sensitivity to small changes in its elements. It is used to estimate how much the output of a function will change if the input is slightly altered.\n",
        "\n",
        "A matrix with a low condition number is well-conditioned, which means its behavior is relatively stable, while a matrix with a high condition number is ill-conditioned, meaning its behavior is unstable with respect to numerical precision.\n"
      ],
      "id": "ad0ac39f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a well-conditioned matrix (low condition number)\n",
        "A = np.array([[1, 0.1], [0.1, 1]])\n",
        "print(\"Condition number of A: \", np.linalg.cond(A))\n",
        "\n",
        "# Define an ill-conditioned matrix (high condition number)\n",
        "B = np.array([[1, 0.99999999], [0.99999999, 1]])\n",
        "print(\"Condition number of B: \", np.linalg.cond(B))"
      ],
      "id": "57eb1d0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.linalg.cond(Psi)"
      ],
      "id": "0a2eb664",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLE to estimate $\\theta$ and $p$\n",
        "\n",
        "We know what the correlations mean, but how do we estimate the values of $\\theta_j$ and where does our observed data $y$ come in?\n",
        "To estimate the values of $\\vec{\\theta}$ and $\\vec{p}$, they are chosen to maximize the likelihood of $\\vec{y}$,\n",
        "which can be expressed in terms of the sample data \n",
        "$$L\\left(\\vec{Y}(\\vec{x}^{(1)}), \\ldots, \\vec{Y}(\\vec{x}^{(n)}) | \\mu, \\sigma \\right) = \\frac{1}{(2\\pi \\sigma)^{n/2} |\\vec{\\Psi}|^{1/2}} \\exp\\left\\{ \\frac{-(\\vec{y} - \\vec{1}\\mu)^T \\vec{\\Psi}^{-1}(\\vec{y} - \\vec{1}\\mu) }{2 \\sigma^2}\\right\\},$$ \n",
        "and formulated as the log-likelihood:\n",
        "$$\\ln(L) = - \\frac{n}{2} \\ln(2\\pi \\sigma) - \\frac{1}{2} \\ln |\\vec{\\Psi}| \\frac{-(\\vec{y} - \\vec{1}\\mu)^T \\vec{\\Psi}^{-1}(\\vec{y} - \\vec{1}\\mu) }{2 \\sigma^2}.$$\n",
        "\n",
        "Optimization of the log-likelihood by taking derivatives with respect to $\\mu$ and $\\sigma$ results in\n",
        "$$\\hat{\\mu} = \\frac{\\vec{1}^T \\vec{\\Psi}^{-1} \\vec{y}^T}{\\vec{1}^T \\vec{\\Psi}^{-1} \\vec{1}^T}$$ \n",
        "and \n",
        "$$\\hat{\\sigma} = \\frac{(\\vec{y} - \\vec{1}\\mu)^T \\vec{\\Psi}^{-1}(\\vec{y} - \\vec{1}\\mu)}{n}.$$\n",
        "\n",
        "Combining the equations leads to the concentrated log-likelihood: \n",
        "$$\\ln(L) = - \\frac{n}{2} \\ln(\\hat{\\sigma}) - \\frac{1}{2} \\ln |\\vec{\\Psi}|.$$ {#eq-concentrated-loglikelihood}\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Note: The Concentrated Log-Likelihood\n",
        "* The first term in @eq-concentrated-loglikelihood requires information about the measured point (observations) $y_i$.\n",
        "* To maximize $\\ln(L)$, optimal values of $\\vec{\\theta}$ and $\\vec{p}$ are determined numerically, because the equation  is not differentiable.\n",
        ":::\n",
        "\n",
        "\n",
        "### Tuning $\\theta$ and $p$\n",
        "\n",
        "Optimizers such as Nelder-Mead, Conjugate Gradient, or Simulated Annealing can be used to determine optimal values for $\\theta$ and $p$.\n",
        "After the optimization, the correlation matrix $\\Psi$ is build with the optimized $\\theta$ and $p$ values. This is best (most likely) Kriging model for the given data $y$. \n",
        "\n",
        "## Kriging Prediction\n",
        "\n",
        "### The Augmented Correlation Matrix\n",
        "\n",
        "We will use the Kriging correlation $\\Psi$ to predict new values based on the observed data. The matrix algebra involved for calculating the likelihood is the most computationally intensive part of the Kriging process. Care must be taken that the computer code is as efficient as possible.\n",
        "\n",
        "Basic elements of the Kriging based surrogate optimization \n",
        "such as interpolation, expected improvement, and regression are presented. The presentation follows the approach described in @Forr08a and @bart21i.\n",
        "\n",
        "Main idea for prediction is that the new $Y(\\vec{x})$ should be consistent with the old sample data $X$.  For a new prediction $\\hat{y}$ at $\\vec{x}$, the value of $\\hat{y}$ is chosen so that it maximizes the likelihood of the sample data $\\vec{X}$ and the prediction, given the (optimized) correlation parameter $\\vec{\\theta}$ and $\\vec{p}$ from above.  The observed data $\\vec{y}$ is augmented with the new prediction $\\hat{y}$ which results in the augmented vector $\\vec{\\tilde{y}} = ( \\vec{y}^T, \\hat{y})^T$. A vector of correlations between the observed data and the new prediction is defined as\n",
        "\n",
        "$$ \\vec{\\psi} = \\begin{pmatrix}\n",
        "\\text{cor}\\left(\n",
        "\\vec{Y}(\\vec{x}^{(1)}),\n",
        "\\vec{Y}(\\vec{x}) \n",
        "\\right) \\\\\n",
        "\\vdots  \\\\\n",
        "\\text{cor}\\left(\n",
        "\\vec{Y}(\\vec{x}^{(n)}),\n",
        "\\vec{Y}(\\vec{x}) \n",
        "\\right)\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\vec{\\psi}^{(1)}\\\\\n",
        "\\vdots\\\\\n",
        "\\vec{\\psi}^{(n)}\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "The augmented correlation matrix is constructed as\n",
        "$$ \\tilde{\\vec{\\Psi}} =\n",
        "\\begin{pmatrix}\n",
        "\\vec{\\Psi} & \\vec{\\psi} \\\\\n",
        "\\vec{\\psi}^T & 1\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "The log-likelihood of the augmented data is\n",
        "$$\n",
        "\\ln(L) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\hat{\\sigma}^2) - \\frac{1}{2} \\ln |\\vec{\\hat{\\Psi}}| -  \\frac{(\\vec{\\tilde{y}} - \\vec{1}\\hat{\\mu})^T \\vec{\\tilde{\\Psi}}^{-1}(\\vec{\\tilde{y}} - \\vec{1}\\hat{\\mu})}{2 \\hat{\\sigma}^2}.\n",
        "$$\n",
        "\n",
        "The MLE for $\\hat{y}$ can be calculated as \n",
        "$$\n",
        "\\hat{y}(\\vec{x}) = \\hat{\\mu} + \\vec{\\psi}^T \\vec{\\tilde{\\Psi}}^{-1} (\\vec{y} - \\vec{1}\\hat{\\mu}).\n",
        "$$ {#eq-mle-yhat}\n",
        "\n",
        "\n",
        "### Properties of the Predictor\n",
        "\n",
        "@eq-mle-yhat reveals two important properties of the Kriging predictor:\n",
        "\n",
        "1. Basis functions: The basis function impacts the vector $\\vec{\\psi}$, which contains the $n$ correlations between the new point $\\vec{x}$ and the observed locations. Values from the $n$ basis functions are added to a mean base term $\\mu$ with weightings $\\vec{w} = \\vec{\\tilde{\\Psi}}^{(-1)} (\\vec{y} - \\vec{1}\\hat{\\mu})$.\n",
        "2. Interpolation: The predictions interpolate the sample data. When calculating the prediction at the $i$th sample point, $\\vec{x}^{(i)}$, the $i$th column of $\\vec{\\Psi}^{-1}$ is $\\vec{\\psi}$, and $\\vec{\\psi}  \\vec{\\Psi}^{-1}$ is the $i$th unit vector. Hence, $\\hat{y}(\\vec{x}^{(i)}) = y^{(i)}$.\n",
        "\n",
        "## Kriging Example: Sinusoid Function \n",
        "\n",
        "Toy example in 1d where the response is a simple sinusoid measured at eight equally spaced  $x$-locations in the span of a single period of oscillation.\n",
        "\n",
        "### Calculating the Correlation Matrix $\\Psi$\n",
        "\n",
        "The correlation matrix $\\Psi$ is based on the pairwise squared distances between the input locations. Here we will use $n=8$ sample locations and $\\theta$ is set to 1.0.\n"
      ],
      "id": "7f4cb378"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 8\n",
        "X = np.linspace(0, 2*np.pi, n, endpoint=False).reshape(-1,1)\n",
        "# theta should be an array (of one value, for the moment, will be changed later)\n",
        "theta = np.array([1.0])\n",
        "Psi = build_Psi(X, theta)"
      ],
      "id": "3a351ea6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate at sample points\n"
      ],
      "id": "9ca378dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = np.sin(X)"
      ],
      "id": "3adbb65a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, y, \"bo\")\n",
        "plt.title(f\"Sin(x) evaluated at {n} points\")\n",
        "plt.show()"
      ],
      "id": "e7a6073c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computing the $\\psi$ Vector\n",
        "\n",
        "Distances between testing locations $x$ and training data locations $X$.\n"
      ],
      "id": "d1419e8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def build_psi(X, x, theta, eps=sqrt(spacing(1))):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    m = x.shape[0]\n",
        "    psi = zeros((n, m))\n",
        "    theta = theta * ones(k)\n",
        "    D = zeros((n, m))\n",
        "    D = cdist(x.reshape(-1, k),\n",
        "              X.reshape(-1, k),\n",
        "              metric='sqeuclidean',\n",
        "              out=None,\n",
        "              w=theta)\n",
        "    print(D.shape)\n",
        "    psi = exp(-D)\n",
        "    # return psi transpose to be consistent with the literature\n",
        "    return(psi.T)"
      ],
      "id": "8f27645e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicting at New Locations\n",
        "\n",
        "We would like to predict at $m = 100$ new locations in the interval $[0, 2\\pi]$. The new locations are stored in the variable `x`.\n"
      ],
      "id": "21e47214"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "m = 100\n",
        "x = np.linspace(0, 2*np.pi, m, endpoint=False).reshape(-1,1)\n",
        "psi = build_psi(X, x, theta)"
      ],
      "id": "b3f43d0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computation of the predictive equations.\n"
      ],
      "id": "366e2609"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "U = cholesky(Psi).T\n",
        "one = np.ones(n).reshape(-1,1)\n",
        "mu = (one.T.dot(solve(U, solve(U.T, y)))) / one.T.dot(solve(U, solve(U.T, one)))\n",
        "f = mu * ones(m).reshape(-1,1) + psi.T.dot(solve(U, solve(U.T, y - one * mu)))"
      ],
      "id": "735eb4be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compute $f$, @eq-mle-yhat is used. \n",
        "\n",
        "### Visualization\n"
      ],
      "id": "1b95d791"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x, f, color = \"orange\", label=\"Fitted\")\n",
        "plt.plot(x, np.sin(x), color = \"grey\", label=\"Original\")\n",
        "plt.plot(X, y, \"bo\", label=\"Measurements\")\n",
        "plt.title(\"Kriging prediction of sin(x) with {} points.\\n theta: {}\".format(n, theta[0]))\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "id": "30568b5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky Example With Two Points\n",
        "\n",
        "### Cholesky Decomposition\n",
        "\n",
        "We consider $k=1$ and $n=2$ sample points. The sample points are located at $x_1=1$ and $x_2=5$. \n",
        "The response values are $y_1=2$ and $y_2=10$. The correlation parameter is $\\theta=1$ and $p$ is set to $1$.\n",
        "Using @eq-krigingbase, we can compute the correlation matrix $\\Psi$:\n",
        "\n",
        "$$\n",
        "\\Psi = \\begin{pmatrix}\n",
        "1 & e^{-1}\\\\\n",
        "e^{-1} & 1\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "To determine MLE as in @eq-mle-yhat, we need to compute $\\Psi^{-1}$:\n",
        "\n",
        "$$\n",
        "\\Psi^{-1} = \\frac{e}{e^2 -1} \\begin{pmatrix}\n",
        "e & -1\\\\\n",
        "-1 & e\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Cholesky-decomposition of $\\Psi$ is recommended to compute $\\Psi^{-1}$. Cholesky decomposition is a decomposition of a positive definite symmetric matrix into the product of a lower triangular matrix $L$, a diagonal matrix $D$ and the transpose of $L$, which is denoted as $L^T$.\n",
        "Consider the following example:\n",
        "\n",
        "$$\n",
        "LDL^T=\n",
        "\\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "l_{21} & 1\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "d_{11} & 0 \\\\\n",
        "0 & d_{22}\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1 & l_{21} \\\\\n",
        "0 & 1\n",
        "\\end{pmatrix}=\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "d_{11} & 0 \\\\\n",
        "d_{11} l_{21} & d_{22}\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1 & l_{21} \\\\\n",
        "0 & 1\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "d_{11} & d_{11} l_{21} \\\\\n",
        "d_{11} l_{21} & d_{11} l_{21}^2 + d_{22}\n",
        "\\end{pmatrix}.\n",
        "$$ {#eq-cholex}\n",
        "\n",
        "\n",
        "\n",
        "Using @eq-cholex, we can compute the Cholesky decomposition of $\\Psi$:\n",
        "\n",
        "1. $d_{11} = 1$,\n",
        "2. $l_{21}d_{11} = e^{-1} \\Rightarrow l_{21} = e^{-1}$, and\n",
        "3. $d_{11} l_{21}^2 + d_{22} = 1 \\Rightarrow d_{22} = 1 - e^{-2}$.\n",
        "\n",
        "The Cholesky decomposition of $\\Psi$ is\n",
        "$$\n",
        "\\Psi = \\begin{pmatrix}\n",
        "1 & 0\\\\\n",
        "e^{-1} & 1\\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1 & 0\\\\\n",
        "0 & 1 - e^{-2}\\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1 & e^{-1}\\\\\n",
        "0 & 1\\\\\n",
        "\\end{pmatrix}\n",
        "= LDL^T$$\n",
        "\n",
        "Some programs use $U$ instead of $L$. The Cholesky decomposition of $\\Psi$ is\n",
        "$$\n",
        "\\Psi = LDL^T = U^TDU.\n",
        "$$\n",
        "\n",
        "Using \n",
        "$$\n",
        "\\sqrt{D} =\\begin{pmatrix}\n",
        "1 & 0\\\\\n",
        "0 & \\sqrt{1 - e^{-2}}\\\\\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "we can write the Cholesky decomposition of $\\Psi$ without a diagonal matrix $D$ as\n",
        "$$\n",
        "\\Psi = \\begin{pmatrix}\n",
        "1 & 0\\\\\n",
        "e^{-1} & \\sqrt{1 - e^{-2}}\\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1 & e^{-1}\\\\\n",
        "0 & \\sqrt{1 - e^{-2}}\\\\\n",
        "\\end{pmatrix}\n",
        "= U^TU.\n",
        "$$\n",
        "\n",
        "\n",
        "### Computation of the Inverse Matrix\n",
        "\n",
        "To compute the inverse of a matrix using the Cholesky decomposition, you can follow these steps:\n",
        "\n",
        "1. Decompose the matrix $A$ into $L$ and $L^T$, where $L$ is a lower triangular matrix and $L^T$ is the transpose of $L$.\n",
        "2. Compute $L^{-1}$, the inverse of $L$.\n",
        "3. The inverse of $A$ is then $(L^{-1})^T  L^-1$.\n",
        "\n",
        "Please note that this method only applies to symmetric, positive-definite matrices.\n",
        "\n",
        "The inverse of the matrix $\\Psi$ from above is:\n",
        "\n",
        "$$\n",
        "\\Psi^{-1} = \\frac{e}{e^2 -1} \\begin{pmatrix}\n",
        "e & -1\\\\\n",
        "-1 & e\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "Heres an example of how to compute the inverse of a matrix using Cholesky decomposition in Python:\n"
      ],
      "id": "47e5c852"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import cholesky, inv\n",
        "E = np.exp(1)\n",
        "\n",
        "# Psi is a symmetric, positive-definite matrix \n",
        "Psi = np.array([[1, 1/E], [1/E, 1]])\n",
        "L = cholesky(Psi, lower=True)\n",
        "L_inv = inv(L)\n",
        "# The inverse of A is (L^-1)^T * L^-1\n",
        "Psi_inv = np.dot(L_inv.T, L_inv)\n",
        "\n",
        "print(\"Psi:\\n\", Psi)\n",
        "print(\"Psi Inverse:\\n\", Psi_inv)"
      ],
      "id": "35c6147b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Jupyter Notebook\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "* The Jupyter-Notebook of this lecture is available on GitHub in the [Hyperparameter-Tuning-Cookbook Repository](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_num_gp.ipynb)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!-- \n"
      ],
      "id": "0e1aa74f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotPython.build.kriging import Kriging\n",
        "import numpy as np\n",
        "nat_X = np.array([[1], [2]])\n",
        "nat_y = np.array([5, 10])\n",
        "n=2\n",
        "p=1\n",
        "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n",
        "S.initialize_variables(nat_X, nat_y)\n",
        "S.set_variable_types()\n",
        "S.set_theta_values()\n",
        "S.initialize_matrices()\n",
        "S.build_Psi()\n",
        "S.build_U()\n",
        "S.likelihood()\n",
        "# assert S.mu is close to 7.5 with a tolerance of 1e-6\n",
        "assert np.allclose(S.mu, 7.5, atol=1e-6)\n",
        "E = np.exp(1)\n",
        "sigma2 = E/(E**2 -1) * (25/4 + 25/4*E)\n",
        "# asssert S.SigmaSqr is close to sigma2 with a tolerance of 1e-6\n",
        "assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)"
      ],
      "id": "7a9f0cfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-->\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<!-- \n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1 Number of Sample Points\n",
        "\n",
        "* The example uses $n=8$ sample points to fit the sin function.\n",
        "  * What happens, if less than 8 samples are available?\n",
        "\n",
        "### 2 Modified $\\theta$ values\n",
        "\n",
        "* The example uses a $\\theta$ value of $1.0$.\n",
        "  * What happens if $\\theta$ is modified?\n",
        "  * Can get better predictions with smaller or larger $\\theta$ values?\n",
        "\n",
        "### 3 Prediction Interval\n",
        "\n",
        "* The prediction interval was identical to the measurement interval, i.e., in the range from $0$ to $2\\pi$. This is referred to as \"interpolation\".\n",
        "  * What happens if this interval is increased (which is referred to as \"extrapolation\")?\n",
        "\n",
        "\n",
        "\n",
        "### Exercise RBF\n",
        "\n",
        "\n",
        "#### Package Loading\n"
      ],
      "id": "38f4cc58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from numpy.matlib import eye\n",
        "import scipy.linalg\n",
        "from numpy import linalg as LA\n",
        "from spotPython.design.spacefilling import spacefilling\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "d5991042",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a small number\n"
      ],
      "id": "4e1158d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eps = np.sqrt(np.spacing(1))"
      ],
      "id": "44ad48e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Sampling Plan (X)\n",
        "\n",
        "* We will use 256 points.\n",
        "* The first 10 points are shown below.\n"
      ],
      "id": "94a704d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gen = spacefilling(2)\n",
        "rng = np.random.RandomState(1)\n",
        "lower = np.array([-1,-1])\n",
        "upper = np.array([2,2])\n",
        "X = gen.scipy_lhd(256, lower=lower, upper = upper)\n",
        "X[1:10]"
      ],
      "id": "15c6a14f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Objective Function\n",
        "\n",
        "* Here we use $\\sum_{i=1}^n (x_i-1)^2$.\n",
        "* `f_map()` is a helper function that maps $f$ to the entries (points) in the matrix $X$.\n"
      ],
      "id": "7f685e0e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f(x):\n",
        "    return np.sum((x-1.0)**2)\n",
        "\n",
        "def f_map(x):\n",
        "    return np.array(list(map(f, x)))"
      ],
      "id": "34dac25c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = f_map(X)\n",
        "y[1:10]"
      ],
      "id": "89a3ff09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Alternatively, we can use pre-defined functions from the `pyspot` package:\n"
      ],
      "id": "0bc40721"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# fun = analytical(sigma=0).fun_branin\n",
        "# fun = analytical(sigma=0).fun_sphere"
      ],
      "id": "2746e1d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))\n",
        "zz = np.array([f_map(np.array([xi, yi]).reshape(-1,2)) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)"
      ],
      "id": "73c03049",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n",
        "co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')"
      ],
      "id": "96204a63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n",
        "co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 10))"
      ],
      "id": "dc539475",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Gram Matrix\n"
      ],
      "id": "97aeb48f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_Gram(X):\n",
        "        \"\"\"\n",
        "        Construction of the Gram matrix.\n",
        "        \"\"\"\n",
        "        n = X.shape[0]\n",
        "        G = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                G[i, j] = np.linalg.norm(X[i] - X[j])\n",
        "        G = G + G.T    \n",
        "        return G"
      ],
      "id": "654dfbfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "G = build_Gram(X)\n",
        "np.round(G,2)"
      ],
      "id": "9e629c3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Radial Basis Functions\n"
      ],
      "id": "055dfa1e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def basis_linear(r):\n",
        "    return r*r*r"
      ],
      "id": "1119a6fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def basis_gauss(r, sigma = 1e-1):\n",
        "    return np.exp(-r**2/sigma)"
      ],
      "id": "42319744",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ We select the Gaussian basis function for the following examples:\n"
      ],
      "id": "1734fb93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "basis = basis_gauss"
      ],
      "id": "9b2f437b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The $\\Psi$ Matrix\n"
      ],
      "id": "be65d8c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_Phi(G, basis, eps=np.sqrt(np.spacing(1))):\n",
        "    n = G.shape[0]\n",
        "    Phi = np.zeros((n,n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            Phi[i,j] = basis(G[i,j])\n",
        "    Phi = Phi +  np.multiply(np.mat(eye(n)), eps)\n",
        "    return Phi"
      ],
      "id": "657e1ee1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Phi = build_Phi(G, basis=basis)\n",
        "Phi[0:3,0:3]"
      ],
      "id": "8298b315",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inverting $\\Psi$ via Cholesky Factorization\n",
        "\n",
        "* There a two different implementations of the Cholesky factorization oin Python:\n",
        "  * `numpy`'s  `linalg.cholesky()` and\n",
        "  * `scipy`'s  `linalg.cholesky()`\n",
        "* We will use `numpy`'s version.\n"
      ],
      "id": "234a3515"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_rbf_weights(Phi, y):\n",
        "    \"\"\" \n",
        "    Calculating the weights of the radial basis function surrogate.\n",
        "    Cholesky factorization used.\n",
        "    LU decomposition otherwise (not implemented yet).\n",
        "    \"\"\"\n",
        "    # U = scipy.linalg.cholesky(Phi, lower=True)\n",
        "    U = np.linalg.cholesky(Phi)\n",
        "    U = U.T\n",
        "    # w = U\\(U'\\ModelInfo.y)\n",
        "    w = np.linalg.solve(U, np.linalg.solve(U.T, y))\n",
        "    return w"
      ],
      "id": "002880a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "w = get_rbf_weights(Phi, y)\n",
        "w[0:3]"
      ],
      "id": "bd8e5847",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Predictions\n",
        "\n",
        "##### The Predictor\n"
      ],
      "id": "67e89f5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pred_rbf(x, X, basis, w):\n",
        "    n = X.shape[0]\n",
        "    d = np.zeros((n))\n",
        "    phi = np.zeros((n))\n",
        "    for i in range(n):\n",
        "        d[i] = np.linalg.norm(x - X[i])\n",
        "    for i in range(n):\n",
        "        phi[i] = basis(d[i])\n",
        "    return w @ phi    "
      ],
      "id": "bf79d788",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Testing some Example Points\n"
      ],
      "id": "53639ca6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = X[0]\n",
        "x"
      ],
      "id": "da1e9aad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The RBF Prediction $\\hat{f}$\n"
      ],
      "id": "25225f99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pred_rbf(x=x, X=X, basis=basis, w=w)"
      ],
      "id": "b4192963",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### The Original (True) Value $f$\n"
      ],
      "id": "b3800b78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f_map(np.array(x).reshape(1,-1))"
      ],
      "id": "557dc728",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Visualizations\n"
      ],
      "id": "7f5a5268"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))\n",
        "zz = np.array([pred_rbf(x=np.array([xi, yi]), X=X, basis=basis,w=w) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)"
      ],
      "id": "d8c8577d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n",
        "co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')"
      ],
      "id": "d43262cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n",
        "co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 5))"
      ],
      "id": "1d09c939",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Note\n",
        "\n",
        "The original function $f$ is cheaper than the surrogate $\\hat{f}$ in this example, because we have chosen a simple analytical function as the ground truth. This is not the case in real-world settings.\n",
        "\n",
        "#### Cholesky Factorization\n",
        "\n",
        "##### $A = U^T U$\n",
        "\n",
        "* $U$ is an upper triangular matrix\n"
      ],
      "id": "3f609a2c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cholesky_U(A):\n",
        "    N = A.shape[0]\n",
        "    U = np.zeros((N,N))\n",
        "    for k in range(0,N):\n",
        "         # compute diagonal entry\n",
        "         U[k,k] = A[k,k]\n",
        "         for j in range(0,k):\n",
        "             U[k,k] = U[k,k] - U[j,k]*U[j,k]\n",
        "         U[k,k] = np.sqrt(U[k,k])\n",
        "         # compute remaining column\n",
        "         for i in range(k+1,N):\n",
        "             U[k,i] = A[k,i]\n",
        "             for j in range(0,k):\n",
        "                 U[k,i] = U[k,i] - U[j,i]*U[j,k]\n",
        "             U[k,i] = U[k,i] / U[k,k]\n",
        "    return U"
      ],
      "id": "9e7edde3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $A = L L^T$\n",
        "\n",
        "$L$ is a lower triangular matrix\n"
      ],
      "id": "40fc347e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cholesky_L(A):\n",
        "    N = A.shape[0]\n",
        "    L = np.zeros((N,N))\n",
        "    for k in range(0,N):\n",
        "         # compute diagonal entry\n",
        "         L[k,k] = A[k,k]\n",
        "         for j in range(0,k):\n",
        "             L[k,k] = L[k,k] - L[k,j]*L[k,j]\n",
        "         L[k,k] = np.sqrt(L[k,k])\n",
        "         # compute remaining column\n",
        "         for i in range(k+1,N):\n",
        "             L[i,k] = A[i,k]\n",
        "             for j in range(0,k):\n",
        "                 L[i,k] = L[i,k] - L[i,j]*L[k,j]\n",
        "             L[i,k] = L[i,k] / L[k,k]\n",
        "    return L"
      ],
      "id": "112d72fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example\n"
      ],
      "id": "756454fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A = np.array([[4, 2, 4, 4], [2, 10, 5, 2], [4, 5, 9, 6], [4, 2, 6, 9]])\n",
        "A"
      ],
      "id": "a4134c54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check: Is $A$ positive definite?\n"
      ],
      "id": "21ea6be6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "assert(np.all(np.linalg.eigvals(A) > 0))"
      ],
      "id": "5ebc17ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  $A = U^T U$\n",
        "\n",
        "Perform Cholesky Factorization\n"
      ],
      "id": "6cb13f1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "U = cholesky_U(A)\n",
        "U"
      ],
      "id": "67d7be45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Result\n"
      ],
      "id": "0d0b3302"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "U.T @ U"
      ],
      "id": "03dde24c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  $A = L L^T$\n"
      ],
      "id": "da3b359f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "L = cholesky_L(A)\n",
        "L"
      ],
      "id": "67246483",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Result\n"
      ],
      "id": "0de11568"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "L @ L.T"
      ],
      "id": "e1ee1594",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "### Gaussian Basis Function\n",
        "\n",
        "* Plot the Gaussian Basis Function `basis_gauss` in the range from -2 to 2 using `matplotlib.pyplot`\n",
        "  * Hint: Check the [matplotlib documentation](https://matplotlib.org/stable/tutorials/introductory/pyplot.html) for examples.\n",
        "  * Generate a plot with several `sigma` values, e.g., 0.1, 1.0, and 10.\n",
        "* What is the meaning of the `sigma` parameter: Can you explain its influence / effect on the model quality?\n",
        "  * Is the `sigma` value important?\n",
        "\n",
        "### Linear Basis Function\n",
        "\n",
        "* Select the linear basis function?\n",
        "* What errors occur?\n",
        "* Do you have any ideas how to fix this error? -->"
      ],
      "id": "34eb1b5d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spotCondaEnv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}