{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Saving and Loading\n",
        "jupyter: python3\n",
        "eval: true\n",
        "---\n",
        "\n",
        "\n",
        "This tutorial shows how to save and load objects in `spotpython`.\n",
        "It is split into the following parts:\n",
        "\n",
        "* @sec-spotpython-saving-and-loading shows how to save and load objects in `spotpython`, if `spotpython` is used as an optimizer.\n",
        "* @sec-spotpython-as-a-hyperparameter-tuner-604 shows how to save and load hyperparameter tuning experiments.\n",
        "* @sec-saving-and-loading-pytorch-lightning-models-604 shows how to save and load `PyTorch Lightning` models.\n",
        "* @sec-converting-a-lightning-model-to-a-plain-torch-model-604 shows how to convert a `PyTorch Lightning` model to a plain `PyTorch` model.\n",
        "\n",
        "## Required Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-required-packages\n",
        "import os\n",
        "import pprint\n",
        "import numpy as np\n",
        "from math import inf\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from spotpython.utils.file import (load_experiment, load_result, get_experiment_filename, load_and_run_spot_python_experiment)\n",
        "from spotpython.spot import Spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init,\n",
        "    design_control_init,\n",
        "    surrogate_control_init,\n",
        "    optimizer_control_init)\n",
        "from spotpython.fun.objectivefunctions import Analytical\n",
        "from spotpython.hyperparameters.values import get_tuned_hyperparameters\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.eda import print_exp_table\n",
        "from spotpython.hyperparameters.values import set_hyperparameter, get_tuned_architecture\n",
        "from spotpython.light.testmodel import test_model\n",
        "from spotpython.light.loadmodel import load_light_from_checkpoint\n",
        "from spotpython.utils.device import getDevice\n",
        "from spotpython.utils.classes import get_removed_attributes_and_base_net\n",
        "from spotpython.hyperparameters.optimizer import optimizer_handler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we will suppress warnings in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: warnings-off-604_12\n",
        "#| echo: false\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Overview of `spotpython` Save and Load Functions\n",
        "\n",
        "Because the most common use case is to save and load hyperparameter tuning experiments, the `save_experiment` and `load_experiment` functions are explained first. In the real- world setting, users specify an experiment on their local machine and then run the experiment on a remote machine. Then, the result file is copied back to the local machine and loaded for further analysis.\n",
        "So, the main steps can be summarized as follows:\n",
        "\n",
        "1. Generate an experiment configuration on the local machine, say `42_exp.pkl`. To generate the configuration, the `fun_control` dictionary has to be initialized with `save_experiment=True`. \n",
        "2. Copy the configuration file to the remote machine.\n",
        "3. Run the experiment on the remote machine. The run on the remote machine is started with the command `load_and_run_spot_python_experiment(filename=\"42_exp.pkl\")`. This generates a result file, say `42_res.pkl`.\n",
        "4. Copy the result file back to the local machine. Analyze the results on the local machine as shown below.\n",
        "\n",
        "As can be seen from these step, we will distinguish between experiment/design/configuration files (with `_exp` in the filename) and result files (with `_res` in the filename).\n",
        "\n",
        "::: {#exm-spotpython-save-load}\n",
        "## Save and Load Hyperparameter Tuning Experiment\n",
        "\n",
        "1. Generate an experiment configuration on the local machine, say `42_exp.pkl` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-save-experiment-604_1\n",
        "PREFIX = \"42\"\n",
        "data_set = Diabetes()\n",
        "fun_control = fun_control_init(\n",
        "    save_experiment=True,\n",
        "    PREFIX=PREFIX,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1)\n",
        "\n",
        "fun = HyperLight().fun\n",
        "\n",
        "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
        "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
        "set_hyperparameter(fun_control, \"epochs\", [3,5])\n",
        "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
        "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
        "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
        "\n",
        "design_control = design_control_init(init_size=10)\n",
        "\n",
        "print_exp_table(fun_control)\n",
        "\n",
        "S = Spot(fun=fun,fun_control=fun_control, design_control=design_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the output shows, the configuration is saved as a pickle-file that contains the full information. In our example, the filename is `42_exp.pkl`.\n",
        "\n",
        "2. Copy the configuration file to the remote machine. This can be done with the `scp` command, see below, but is omitted here for brevity.\n",
        "\n",
        "3. Run the experiment on the remote machine. This step is simulated on the local machine for demonstration purposes. This generates a result file, say `42_res.pkl`. This can be done with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-run-experiment-604_2\n",
        "# S_res = S.run() is NOT used here, because we want to simulate the remote run\n",
        "# Instead, we use the load_and_run_spot_python_experiment function:\n",
        "S_remote = load_and_run_spot_python_experiment(filename=\"42_exp.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A result file with the name `42_res.pkl` is stored in the current directory.\n",
        "\n",
        "4. Copy the result file back to the local machine. This can be done with the `scp` command, see below, but is omitted here for brevity. The result file can be loaded with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-load-result-604_1\n",
        "S_res = load_result(PREFIX=\"42\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-get-tuned-hyperparameters-fun-ctrl604\n",
        "S_res.plot_progress(log_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "S_res.print_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you add `fun_control=S_res.fun_control` as an argument to the `get_tuned_hyperparameters` function, the names of the hyperparameters are used as keys in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_tuned_hyperparameters(S_res, fun_control=S_res.fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the transformed hyperparameters that can be passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "config = get_tuned_architecture(S_res)\n",
        "pprint.pprint(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After getting the tuned architecture, the model can be created and tested with the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_model(config, S_res.fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "S_res.plot_important_hyperparameter_contour(max_imp=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "S_res.plot_importance()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## spotpython: Saving and Loading Optimization Experiments {#sec-spotpython-saving-and-loading}\n",
        "\n",
        "In this section, we will show how results from `spotpython` can be saved and reloaded.\n",
        "Here, `spotpython` can be used as an optimizer. \n",
        "If `spotpython` is used as an optimizer, no dictionary of hyperparameters has be specified.\n",
        "The `fun_control` dictionary is sufficient. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-optimization-experiment-604_1\n",
        "PREFIX=\"branin\"\n",
        "\n",
        "fun = Analytical().fun_branin\n",
        "fun_control = fun_control_init(\n",
        "            PREFIX=PREFIX,\n",
        "            lower = np.array([0, 0]),\n",
        "            upper = np.array([10, 10]),\n",
        "            fun_evals=8,\n",
        "            fun_repeats=1,\n",
        "            max_time=inf,\n",
        "            noise=False,\n",
        "            tolerance_x=0,\n",
        "            ocba_delta=0,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            infill_criterion=\"ei\",\n",
        "            n_points=1,\n",
        "            seed=123,\n",
        "            log_level=20,\n",
        "            show_models=False,\n",
        "            save_experiment=True,\n",
        "            show_progress=True)\n",
        "design_control = design_control_init(\n",
        "            init_size=5,\n",
        "            repeats=1)\n",
        "surrogate_control = surrogate_control_init(\n",
        "            model_fun_evals=10000,\n",
        "            min_theta=-3,\n",
        "            max_theta=3,\n",
        "            theta_init_zero=True,\n",
        "            n_p=1,\n",
        "            optim_p=False,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            seed=124)\n",
        "optimizer_control = optimizer_control_init(\n",
        "            max_iter=1000,\n",
        "            seed=125)\n",
        "S = Spot(fun=fun,\n",
        "            fun_control=fun_control,\n",
        "            design_control=design_control,\n",
        "            surrogate_control=surrogate_control,\n",
        "            optimizer_control=optimizer_control)\n",
        "S.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-reload-optimization-experiment-604_2\n",
        "S_exp = load_experiment(PREFIX=PREFIX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-reload-optimization-experiment-604_3\n",
        "S_res = load_result(PREFIX=PREFIX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The progress of the original experiment is shown in @fig-plot-progress-604a_1 and the reloaded experiment in @fig-plot-progress-604b_2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-plot-progress-604a_1\n",
        "#| fig-cap: Progress of the original experiment\n",
        "S.plot_progress(log_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-plot-progress-604b_2\n",
        "#| fig-cap: Progress of the reloaded experiment\n",
        "S_res.plot_progress(log_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results from the original experiment are shown in @tbl-results-604a_1 and the reloaded experiment in @tbl-results-604b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-results-604a_1\n",
        "S.print_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-results-604b\n",
        "S_res.print_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Getting the Tuned Hyperparameters\n",
        "\n",
        "The tuned hyperparameters can be obtained as a dictionary with the following code.\n",
        "Since `spotpython` is used as an optimizer, the numerical levels of the hyperparameters are identical to the optimized values of the underlying optimization problem, here: the Branin function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-get-tuned-optimization-604_11\n",
        "get_tuned_hyperparameters(spot_tuner=S_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "### Summary: Saving and Loading Optimization Experiments\n",
        "* If `spotpython` is used as an optimizer (without an hyperparameter dictionary), experiments can be saved and reloaded with the `save_experiment` and `load_experiment` functions.\n",
        "* The tuned hyperparameters can be obtained with the `get_tuned_hyperparameters` function.\n",
        ":::\n",
        "\n",
        "## spotpython as a Hyperparameter Tuner {#sec-spotpython-as-a-hyperparameter-tuner-604}\n",
        "\n",
        "If `spotpython` is used as a hyperparameter tuner,\n",
        "in addition to the `fun_control` dictionary a `core_model` dictionary has to be specified.\n",
        "Furthermore, a data set has to be selected and added to the `fun_control` dictionary.\n",
        "Here, we will use the `Diabetes` data set.\n",
        "\n",
        "\n",
        "### The Diabetes Data Set\n",
        "\n",
        "The hyperparameter tuning of a `PyTorch Lightning` network on the `Diabetes` data set is used as an example. The `Diabetes` data set is a PyTorch Dataset for regression, which originates from the `scikit-learn` package, see [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes).\n",
        "\n",
        "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients,  as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
        "The `Diabetes` data set is has the following properties:\n",
        "\n",
        "* Samples total: 442\n",
        "* Dimensionality: 10\n",
        "* Features: real, $-.2 < x < .2$\n",
        "* Targets: integer $25 - 346$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-diabetes-data-set-604_13\n",
        "#| eval: true\n",
        "data_set = Diabetes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-hyperparameter-tuning-604_14\n",
        "PREFIX=\"604\"\n",
        "fun_control = fun_control_init(\n",
        "    save_experiment=True,\n",
        "    PREFIX=PREFIX,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1)\n",
        "\n",
        "fun = HyperLight().fun\n",
        "\n",
        "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
        "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
        "set_hyperparameter(fun_control, \"epochs\", [3,5])\n",
        "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
        "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
        "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
        "\n",
        "design_control = design_control_init(init_size=10)\n",
        "\n",
        "print_exp_table(fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In contrast to the default setting, where `save_experiment` is set to `False`,\n",
        "here the `fun_control` dictionary is initialized `save_experiment=True`.\n",
        "Alternatively, an existing `fun_control` dictionary can be updated with `{\"save_experiment\": True}` as shown in the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 604_save_experiment_15\n",
        "fun_control.update({\"save_experiment\": True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If `save_experiment` is set to `True`, the results of the hyperparameter tuning experiment are stored in a pickle file with the name `PREFIX` after the tuning is finished in the current directory.\n",
        "\n",
        "Alternatively, the spot object and the corresponding dictionaries can be saved with the `save_experiment` method, which is part of the `spot` object.\n",
        "Therefore, the `spot` object has to be created as shown in the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "S_diabetes = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
        "S_diabetes.save_experiment(path=\"userExperiment\", overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we have added a `path` argument to specify the directory where the experiment is saved.\n",
        "The resulting pickle file can be copied to another directory or computer and reloaded with the `load_experiment` function.\n",
        "It can also be used for performing the tuning run.\n",
        "Here, we will execute the tuning run on the local machine, which can be done with the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "S_diabetes_res = S_diabetes.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the tuning run is finished, a pickle file with the name `spot_604_experiment.pickle` is stored in the local directory.\n",
        "This is a result of setting the `save_experiment` argument to `True` in the `fun_control` dictionary.\n",
        "We can load the experiment with the following code. Here, we have specified the `PREFIX` as an argument to the `load_experiment` function.\n",
        "Alternatively, the filename (`filename`) can be used as an argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-reload-hyper-experiment-37\n",
        "S_diabetes_load_exp = load_experiment(PREFIX=PREFIX)\n",
        "S_diabetes_load_res = load_result(PREFIX=PREFIX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For comparison, the tuned hyperparameters of the original experiment are shown first: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-get-tuned-hyperparameters-fun-ctrl604a\n",
        "get_tuned_hyperparameters(S_diabetes, fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second, the tuned hyperparameters of the reloaded experiment are shown:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-get-tuned-hyperparameters-fun-ctrl604b\n",
        "get_tuned_hyperparameters(S_diabetes_load_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: The numerical levels of the hyperparameters are used as keys in the dictionary.\n",
        "If the `fun_control` dictionary is used, the names of the hyperparameters are used as keys in the dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_tuned_hyperparameters(S_diabetes_load_res, fun_control=S_diabetes_load_exp.fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the progress of the original experiment are identical to the reloaded experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-plot-progress-604aa\n",
        "S_diabetes.plot_progress()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-plot-progress-604bb\n",
        "S_diabetes_load_res.plot_progress()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "### Summary: Saving and Loading Hyperparameter-Tuning Experiments\n",
        "* If `spotpython` is used as an hyperparameter tuner (with an hyperparameter dictionary), experiments can be saved and reloaded with the `save_experiment` and `load_experiment` functions.\n",
        "* The tuned hyperparameters can be obtained with the `get_tuned_hyperparameters` function.\n",
        ":::\n",
        "\n",
        "\n",
        "## Saving and Loading PyTorch Lightning Models {#sec-saving-and-loading-pytorch-lightning-models-604}\n",
        "\n",
        "@sec-spotpython-saving-and-loading  and @sec-spotpython-as-a-hyperparameter-tuner-604 explained how to save and load optimization and hyperparameter tuning experiments and how to get the tuned hyperparameters as a dictionary.\n",
        "This section shows how to save and load `PyTorch Lightning` models.\n",
        "\n",
        "\n",
        "### Get the Tuned Architecture {#sec-get-spot-results-604}\n",
        "\n",
        "In contrast to the function `get_tuned_hyperparameters`, the function `get_tuned_architecture` returns the tuned architecture of the model as a dictionary. Here, the transformations are already applied to the numerical levels of the hyperparameters and the encoding (and types) are the original types of the hyperparameters used by the model.\n",
        "Important: The `config` dictionary from `get_tuned_architecture` can be passed to the model without any modifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "config = get_tuned_architecture(S_diabetes)\n",
        "pprint.pprint(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After getting the tuned architecture, the model can be created and tested with the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_model(config, fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load a Model from Checkpoint\n",
        "\n",
        "The method `load_light_from_checkpoint` loads a model from a checkpoint file.\n",
        "Important: The model has to be trained before the checkpoint is loaded. As shown here, loading a model with trained weights is possible, but requires two steps:\n",
        "\n",
        "1. The model weights have  to be learned using `test_model`. The `test_model` method writes a checkpoint file.\n",
        "2. The model has to be loaded from the checkpoint file.\n",
        "\n",
        "#### Details About the `load_light_from_checkpoint` Method\n",
        "\n",
        "* The `test_model` method saves the last checkpoint to a file using the following code:\n",
        "```python\n",
        "ModelCheckpoint(\n",
        "    dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True\n",
        "), \n",
        "```\n",
        "\n",
        "The filename of the last checkpoint has a specific structure:\n",
        "\n",
        "* A `config_id` is generated from the `config` dictionary. It does not use a timestamp. This differs from the config id generated in cvmodel.py and trainmodel.py, which provide time information for the TensorBoard logging.\n",
        "* Furthermore, the postfix `_TEST` is added to the `config_id` to indicate that the model is tested.\n",
        "* For example: `runs/saved_models/16_16_64_LeakyReLU_Adadelta_0.0014_8.5895_8_False_kaiming_uniform_TEST/last.ckpt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_loaded = load_light_from_checkpoint(config, fun_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vars(model_loaded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.save(model_loaded, \"model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the following warning:\n",
        "In PyTorch 2.6, the default value of the `weights_only` argument in `torch.load`  was changed from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mymodel = torch.load(\"model.pt\", weights_only=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show all attributes of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vars(mymodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting a Lightning Model to a Plain Torch Model {#sec-converting-a-lightning-model-to-a-plain-torch-model-604}\n",
        "\n",
        "### The Function `get_removed_attributes_and_base_net`\n",
        "\n",
        "`spotpython` provides a function to covert a `PyTorch Lightning` model to a plain `PyTorch` model. The function `get_removed_attributes_and_base_net` returns a tuple with the removed attributes and the base net. The base net is a plain `PyTorch` model. The removed attributes are the attributes of the `PyTorch Lightning` model that are not part of the base net.\n",
        "\n",
        "This conversion can be reverted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "removed_attributes, torch_net = get_removed_attributes_and_base_net(net=mymodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(removed_attributes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(torch_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  An Example how to use the Plain Torch Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the Diabetes dataset from sklearn\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create a PyTorch dataloader\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "torch_net.to(getDevice(\"cpu\"))\n",
        "\n",
        "# train the net\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(torch_net.parameters(), lr=0.01)\n",
        "n_epochs = 100\n",
        "losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    for inputs, targets in train_dataloader:\n",
        "        targets = targets.view(-1, 1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = torch_net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "# visualize the network training\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot313/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}