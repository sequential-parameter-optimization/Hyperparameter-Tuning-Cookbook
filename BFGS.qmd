## Basics

### Gradient

The gradient $ \nabla f(\mathbf{x}) $ for a scalar function $ f(\mathbf{x}) $ with $n$ different variables is defined by its partial derivatives:

$ \nabla f(\mathbf{x}) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right] $

### Jacobian Matrix

The Jacobian matrix $ J(\mathbf{x})$ for a vector-valued function $\mathbf{F}(\mathbf{x}) = [f_1(\mathbf{x}), f_2(\mathbf{x}), \ldots, f_m(\mathbf{x})] $ is defined as:

$ J(\mathbf{x}) = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \ldots & \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \ldots & \frac{\partial f_2}{\partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \ldots & \frac{\partial f_m}{\partial x_n} \end{bmatrix} $

It consists of the first order partial derivatives and gives therefore an overview about the gradients of a vector valued function.

**Example:** 

Consider a vector-valued function $ \mathbf{f} : \mathbb{R}^2 \rightarrow \mathbb{R}^3 $ defined as follows:

$ \mathbf{f}(\mathbf{x}) = \begin{bmatrix} x_1^2 + 2x_2 \\ 3x_1 - \sin(x_2) \\ e^{x_1 + x_2} \end{bmatrix} $

Let's compute the partial derivatives and construct the Jacobian matrix:

$ \frac{\partial f_1}{\partial x_1} = 2x_1, \quad \frac{\partial f_1}{\partial x_2} = 2 $

$ \frac{\partial f_2}{\partial x_1} = 3, \quad \frac{\partial f_2}{\partial x_2} = -\cos(x_2) $

$ \frac{\partial f_3}{\partial x_1} = e^{x_1 + x_2}, \quad \frac{\partial f_3}{\partial x_2} = e^{x_1 + x_2} $

So, the Jacobian matrix is:

$ \mathbf{J}(\mathbf{x}) = \begin{bmatrix} 2x_1 & 2 \\ 3 & -\cos(x_2) \\ e^{x_1 + x_2} & e^{x_1 + x_2} \end{bmatrix} $

This Jacobian matrix provides information about how small changes in the input variables $x_1$ and $x_2$ affect the corresponding changes in each component of the output vector.

### Hessian Matrix

The Hessian matrix $ H(\mathbf{x}) $ for a scalar function $ f(\mathbf{x}) $ is defined as:

$ H(\mathbf{x}) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \ldots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix} $

So, the Hessian matrix consists of the second order dervatives of the function. It provides information about the local curvature of the function with respect to changes in the input variables.

**Example:**

Consider a scalar-valued function:

$ f(\mathbf{x}) = x_1^2 + 2x_2^2 + \sin(x_1 \cdot x_2) $

The Hessian matrix of this scalar-valued function is the matrix of its second-order partial derivatives with respect to the input variables:

$ \mathbf{H}(\mathbf{x}) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} \end{bmatrix} $

Let's compute the second-order partial derivatives and construct the Hessian matrix:

$ \frac{\partial^2 f}{\partial x_1^2} = 2 + \cos(x_1 \cdot x_2) \cdot x_2^2 $

$ \frac{\partial^2 f}{\partial x_1 \partial x_2} = 2x_1 \cdot x_2 \cdot \cos(x_1 \cdot x_2) - \sin(x_1 \cdot x_2) $

$ \frac{\partial^2 f}{\partial x_2 \partial x_1} = 2x_1 \cdot x_2 \cdot \cos(x_1 \cdot x_2) - \sin(x_1 \cdot x_2) $

$ \frac{\partial^2 f}{\partial x_2^2} = 4x_2^2 + \cos(x_1 \cdot x_2) \cdot x_1^2 $

So, the Hessian matrix is:

$ \mathbf{H}(\mathbf{x}) = \begin{bmatrix} 2 + \cos(x_1 \cdot x_2) \cdot x_2^2 & 2x_1 \cdot x_2 \cdot \cos(x_1 \cdot x_2) - \sin(x_1 \cdot x_2) \\ 2x_1 \cdot x_2 \cdot \cos(x_1 \cdot x_2) - \sin(x_1 \cdot x_2) & 4x_2^2 + \cos(x_1 \cdot x_2) \cdot x_1^2 \end{bmatrix} $


## Gradient for Optimization


In optimization, the goal is to find the minimum or maximum of a function. Gradient-based optimization methods utilize information about the gradient (or derivative) of the function to guide the search for the optimal solution. This is particularly useful when dealing with complex, high-dimensional functions where an exhaustive search is impractical.

### Example: Gradient Descent

Let's consider a simple quadratic function as an example:

$ f(x) = x^2 + 4x + y^2 + 2y +4 $

We'll use gradient descent, a gradient-based optimization algorithm, to find the minimum of this function.

The gradient descent method can be divided in the following steps:

+ **Initilize:** start with an initial guess for the parameters of the function to be optimized.

+ **Compute Gradient:** Calculate the gradient (partial derivatives) of the function with respect to each parameter at the current point. The gradient indicates the direction of the steepest increase in the function.
+ **Update Parameters:** Adjust the parameters in the opposite direction of the gradient, scaled by a learning rate. This step aims to move towards the minimum of the function: 
    * $x_{k+1} = x_k - \alpha \times \nabla f(x_{k})$
    * $x_{x}$ is current parameter vector or point in the parameter space. 
    * $\alpha$ is the learning rate, a positive scalar that determines the step size in each iteration.
    * $\nabla f(x)$ is the gradient of the objective function.

    <br>

+ **Iterate:** Repeat the above steps until convergence or a predefined number of iterations. Convergence is typically determined when the change in the function value or parameters becomes negligible.
Optimal Parameters:

```{python}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define the quadratic function
def quadratic_function(x, y):
    return x**2 + 4*x + y**2 + 2*y + 4

# Define the gradient of the quadratic function
def gradient_quadratic_function(x, y):
    grad_x = 2*x + 4
    grad_y = 2*y + 2
    return np.array([grad_x, grad_y])

# Gradient Descent for optimization in 2D
def gradient_descent(initial_point, learning_rate, num_iterations):
    points = [np.array(initial_point)]
    
    for _ in range(num_iterations):
        current_point = points[-1]
        gradient = gradient_quadratic_function(*current_point)
        new_point = current_point - learning_rate * gradient
        
        points.append(new_point)
        
    return points

# Visualization of optimization process with 3D surface and consistent arrow sizes
def plot_optimization_process_3d_consistent_arrows(points):
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    x_vals = np.linspace(-10, 2, 100)
    y_vals = np.linspace(-10, 2, 100)
    X, Y = np.meshgrid(x_vals, y_vals)
    Z = quadratic_function(X, Y)

    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)
    ax.scatter(*zip(*points), [quadratic_function(*p) for p in points], c='red', label='Optimization Trajectory')

    for i in range(len(points) - 1):  
        x, y = points[i]
        dx, dy = points[i + 1] - points[i]
        dz = quadratic_function(*(points[i + 1])) - quadratic_function(*points[i])
        gradient_length = 0.5

        ax.quiver(x, y, quadratic_function(*points[i]), dx, dy, dz, color='blue', length=gradient_length, normalize=False, arrow_length_ratio=0.1)

    ax.set_title('Gradient-Based Optimization with 2D Quadratic Function')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_zlabel('f(x, y)')
    ax.legend()
    plt.show()

# Initial guess and parameters
initial_guess = [-9.0, -9.0]
learning_rate = 0.2
num_iterations = 10

# Run gradient descent in 2D and visualize the optimization process with 3D surface and consistent arrow sizes
trajectory = gradient_descent(initial_guess, learning_rate, num_iterations)
plot_optimization_process_3d_consistent_arrows(trajectory)
```

## Newton Method

**Initialization:** Start with an initial guess for the optimal solution, $x_0$.

**Iteration:** Repeat the following steps until convergence or a predefined stopping criterion is met:

   1) Calculate the gradient (nabla) and the Hessian matrix of the objective function at the current point:
     $\nabla f(x_k) \quad \text{and} \quad \nabla^2 f(x_k)$

   2) Update the current solution using the Newton-Raphson update formula:
     $x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$

  where: 

  + $\nabla f(x_k)$ is the gradient (first derivative) of the objective function with respect to the variable $x$, evaluated at the current solution $x_k$.

  + $\nabla^2 f(x_k)$: The Hessian matrix (second derivative) of the objective function with respect to $x$, evaluated at the current solution $x_k$.

  + $x_k$: The current solution or point in the optimization process.

  + $\nabla^2 f(x_k)]^{-1}$: The inverse of the Hessian matrix at the current point, representing the approximation of the curvature of the objective function.

  + $x_{k+1}$: The updated solution or point after applying the Newton-Raphson update.


   3) Check for convergence.


   

**Example Newton Method:**

We want to optimize the Rosenbrock function and use the Hessian and the Jacobian (which is equal to the gradient vector for scalar objective function) to the `minimize` function. 

```{python}
def rosenbrock(x):
    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2

def rosenbrock_gradient(x):
    dfdx0 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])
    dfdx1 = 200 * (x[1] - x[0]**2)
    return np.array([dfdx0, dfdx1])

def rosenbrock_hessian(x):
    d2fdx0 = 1200 * x[0]**2 - 400 * x[1] + 2
    d2fdx1 = -400 * x[0]
    return np.array([[d2fdx0, d2fdx1], [d2fdx1, 200]])

def classical_newton_optimization_2d(initial_guess, tol=1e-6, max_iter=100):
    x = initial_guess.copy()

    for i in range(max_iter):
        gradient = rosenbrock_gradient(x)
        hessian = rosenbrock_hessian(x)

        # Solve the linear system H * d = -g for d
        d = np.linalg.solve(hessian, -gradient)

        # Update x
        x += d

        # Check for convergence
        if np.linalg.norm(gradient, ord=np.inf) < tol:
            break

    return x

# Initial guess
initial_guess_2d = np.array([0.0, 0.0])

# Run classical Newton optimization for the 2D Rosenbrock function
result_2d = classical_newton_optimization_2d(initial_guess_2d)

# Print the result
print("Optimal solution:", result_2d)
print("Objective value:", rosenbrock(result_2d))

```

## BFGS - Algorithm

BFGS is an optimization algorithm designed for unconstrained optimization problems. It belongs to the class of quasi-Newton methods and is known for its efficiency in finding the minimum of a smooth, unconstrained objective function.

### Procedure:

1. **Initialization:**
* Start with an initial guess for the parameters of the objective function.
* Initialize an approximation of the Hessian matrix (inverse) denoted by H.  
<br>

2. **Iterative Update:**
* At each iteration, compute the gradient vector at the current point.
* Update the parameters using the BFGS update formula, which involves the inverse Hessian matrix approximation, the gradient, and the difference in parameter vectors between successive iterations.

    $ x_{k+1} = x_k - H_k^{-1} \nabla f(x_k) $

* Update the inverse Hessian approximation using the BFGS update formula for the inverse Hessian.

    $ H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^T}{\Delta x_k^T \Delta g_k} - \frac{H_k g_k g_k^T H_k}{g_k^T H_k g_k} $

    where:
- $ x_k $ and $ x_{k+1} $ are the parameter vectors at the current and updated iterations, respectively.
- $ \nabla f(x_k) $ is the gradient vector at the current iteration.
- $ \Delta x_k = x_{k+1} - x_k $ is the change in parameter vectors.
- $ \Delta g_k = \nabla f(x_{k+1}) - \nabla f(x_k) $ is the change in gradient vectors.

<br>

3. **Convergence:**

* Repeat the iterative update until the optimization converges. Convergence is typically determined by reaching a sufficiently low gradient or parameter change.


### BFGS for Rosenbrock

```{python}
import numpy as np
from scipy.optimize import minimize

# Define the 2D Rosenbrock function
def rosenbrock(x):
    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2

# Initial guess
initial_guess = np.array([0.0, 0.0])

# Minimize the Rosenbrock function using BFGS
result = minimize(rosenbrock, initial_guess, method='BFGS')
```

```{python}
result
```

### Visualization BFGS for Rosenbrock

source: https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif

<img src="Rosenbrock-bfgs-animation.gif" alt="BFGS Search Process" width="400"/>

## Tasks until 05.12.

* In which situations is it possible to use algorithms like BFGS, but not the classical Newton method?
* Investigate the Newton-CG method
* Use an objective function of your choice and apply Newton-CG
* Compare the Newton-CG method with the BFGS. What are the similarities and differences between the two algorithms?


