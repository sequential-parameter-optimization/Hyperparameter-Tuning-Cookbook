{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: false\n",
        "  echo: true\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "# Kriging with spotPython based on the Forrester et al. textbook\n",
        "\n",
        "This chapter xplains how the spotpython Kriging class implements standard formulas from Forrester et al. (Engineering Design via Surrogate Modelling), focusing on Chapters 2, 3, and 6 (Gaussian process/Kriging modelling, likelihood-based hyperparameter estimation, prediction, and expected improvement).\n",
        "\n",
        "## High-level model and notation\n",
        "\n",
        "- Training data: X ∈ R^{n×k}, y ∈ R^n.\n",
        "- Correlation model: R(i,j) = exp(-∑_d θ_d · (x_i,d − x_j,d)^2) for ordered/numeric variables; factor variables use a categorical distance with the same exp(-D) envelope. θ are in log10 in this code, internally exponentiated as 10^θ.\n",
        "- Nugget: λ ≥ 0 (also in log10 here, exponentiated as 10^λ). Interpolation uses eps (tiny) instead of λ; regression uses λ; reinterpolation uses a λ→ε adjustment during prediction variance.\n",
        "- Mean function: constant mean μ (ordinary Kriging).\n",
        "- Concentrated likelihood (Forrester): given R (n×n) and residual variance σ^2 = (r^T R^{-1} r)/n with r = y − 1·μ and μ = (1^T R^{-1} y)/(1^T R^{-1} 1), the concentrated negative log-likelihood is n/2·log(σ^2) + 1/2·log|R| (constants dropped).\n",
        "\n",
        "\n",
        "## Correlation matrix build: build_Psi()\n",
        "\n",
        "- Psi is the correlation matrix R (without the nugget) from pairwise distances:\n",
        "  - Ordered/numeric variables use squared Euclidean dist weighted by 10^θ.\n",
        "  - Factor variables use a categorical metric (metric_factorial).\n",
        "  - Final Psi = exp(−D).\n",
        "- The method returns the upper triangle; the full R is formed later as R = Psi + Psi^T + I (and + λI if applicable).\n",
        "\n",
        "Code mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def build_Psi(self) -> None:\n",
        "    n, k = self.X_.shape\n",
        "    theta10 = self._get_theta10_from_logtheta()  # 10**theta, isotropic expands to k\n",
        "    Psi = np.zeros((n, n), dtype=np.float64)\n",
        "\n",
        "    if self.ordered_mask.any():\n",
        "        X_ordered = self.X_[:, self.ordered_mask]\n",
        "        D_ordered = squareform(\n",
        "            pdist(X_ordered,\n",
        "            metric=\"sqeuclidean\",\n",
        "            w=theta10[self.ordered_mask]))\n",
        "        Psi += D_ordered\n",
        "\n",
        "    if self.factor_mask.any():\n",
        "        X_factor = self.X_[:, self.factor_mask]\n",
        "        D_factor = squareform(\n",
        "            pdist(X_factor, metric=self.metric_factorial,\n",
        "            w=theta10[self.factor_mask]))\n",
        "        Psi += D_factor\n",
        "\n",
        "    Psi = np.exp(-Psi)                 # R = exp(-D)\n",
        "    return np.triu(Psi, k=1)           # upper triangle only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Likelihood and hyperparameters: likelihood()\n",
        "\n",
        "- Input x packs [log10 θ] plus, for regression/reinterpolation, log10 λ, and optionally p exponents (if enabled).\n",
        "- The method:\n",
        "  1) Forms R = (upper + transpose) + I + λI (with λ = 10^logλ; for interpolation λ := eps).\n",
        "  2) Cholesky factorization R = U U^T.\n",
        "  3) μ = (1^T R^{-1} y)/(1^T R^{-1} 1) via triangular solves with U.\n",
        "  4) σ^2 = (r^T R^{-1} r)/n with r = y − 1·μ.\n",
        "  5) −log L = n/2·log(σ^2) + 1/2·log|R|, with log|R| = 2·∑ log diag(U).\n",
        "\n",
        "Code mapping (directly implements the standard concentrated likelihood):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def likelihood(self, x: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray]:\n",
        "    X = self.X_; y = self.y_.flatten()\n",
        "    self.theta = x[: self.n_theta]\n",
        "\n",
        "    if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n",
        "        lambda_ = 10.0**x[self.n_theta : self.n_theta + 1]   # nugget on linear scale\n",
        "        if self.optim_p:\n",
        "            self.p_val = x[self.n_theta + 1 : self.n_theta + 1 + self.n_p]\n",
        "    elif self.method == \"interpolation\":\n",
        "        lambda_ = self.eps\n",
        "        if self.optim_p:\n",
        "            self.p_val = x[self.n_theta : self.n_theta + self.n_p]\n",
        "\n",
        "    n = X.shape[0]; one = np.ones(n)\n",
        "    Psi_up = self.build_Psi()\n",
        "    Psi = Psi_up + Psi_up.T + np.eye(n) + np.eye(n) * lambda_  # R = corr + I + λI\n",
        "\n",
        "    U = np.linalg.cholesky(Psi)\n",
        "    LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(U))))        # log|R|\n",
        "\n",
        "    # R^{-1}y and R^{-1}1 via triangular solves\n",
        "    temp_y  = np.linalg.solve(U, y);   vy   = np.linalg.solve(U.T, temp_y)\n",
        "    temp_1  = np.linalg.solve(U, one); vone = np.linalg.solve(U.T, temp_1)\n",
        "\n",
        "    mu = (one @ vy) / (one @ vone)\n",
        "    resid = y - one * mu\n",
        "    tresid = np.linalg.solve(U, resid); tresid = np.linalg.solve(U.T, tresid)\n",
        "    SigmaSqr = (resid @ tresid) / n\n",
        "\n",
        "    negLnLike = (n / 2.0) * np.log(SigmaSqr) + 0.5 * LnDetPsi\n",
        "    return negLnLike, Psi, U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter optimization: max_likelihood()\n",
        "\n",
        "- Minimizes the concentrated negative log-likelihood over [log10 θ, log10 λ, p] using differential evolution.\n",
        "- Objective = likelihood(x)[0].\n",
        "- Bounds are assembled in fit() depending on method and options.\n",
        "\n",
        "Code mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def max_likelihood(self, bounds: List[Tuple[float, float]]) -> Tuple[np.ndarray, float]:\n",
        "    def objective(logtheta_loglambda_p_: np.ndarray) -> float:\n",
        "        neg_ln_like, _, _ = self.likelihood(logtheta_loglambda_p_)\n",
        "        return neg_ln_like\n",
        "\n",
        "    result = differential_evolution(func=objective, bounds=bounds, seed=self.seed)\n",
        "    return result.x, result.fun"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model fitting workflow: fit()\n",
        "\n",
        "- Sets k, n, variable type masks; selects n_theta (1 if isotropic else k).\n",
        "- Builds bounds:\n",
        "  - interpolation: k θ-bounds only.\n",
        "  - regression/reinterpolation: k θ-bounds + 1 λ-bound.\n",
        "  - If optim_p: adds p-bounds (n_p entries).\n",
        "- Calls max_likelihood to obtain best x = [log10 θ, log10 λ, p].\n",
        "- Stores:\n",
        "  - theta = first n_theta entries of x.\n",
        "  - Lambda = next 1 entry (for regression/reinterpolation) — kept on log10 scale in the model’s state; transformed to linear only inside likelihood/_pred.\n",
        "  - p_val if enabled.\n",
        "- Computes final Psi, U and negLnLike at the found hyperparameters.\n",
        "\n",
        "Code mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def fit(self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        bounds: Optional[List[Tuple[float, float]]] = None) -> \"Kriging\":\n",
        "    X = np.asarray(X); y = np.asarray(y).flatten()\n",
        "    self.X_, self.y_ = X, y\n",
        "    self.n, self.k = self.X_.shape\n",
        "    self._set_variable_types()\n",
        "    if self.n_theta is None: self._set_theta()\n",
        "    self.min_X = np.min(self.X_, axis=0); self.max_X = np.max(self.X_, axis=0)\n",
        "\n",
        "    if bounds is None:\n",
        "        if self.method == \"interpolation\":\n",
        "            bounds = [(self.min_theta, self.max_theta)] * self.k\n",
        "        else:\n",
        "            bounds = [(self.min_theta, self.max_theta)] * self.k + \n",
        "                      (self.min_Lambda, self.max_Lambda)]\n",
        "    if self.optim_p:\n",
        "        bounds += [(self.min_p, self.max_p)] * self.n_p\n",
        "\n",
        "    self.logtheta_loglambda_p_, _ = self.max_likelihood(bounds)\n",
        "\n",
        "    self.theta = self.logtheta_loglambda_p_[: self.n_theta]\n",
        "    if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n",
        "        self.Lambda = self.logtheta_loglambda_p_[self.n_theta : self.n_theta + 1]\n",
        "        if self.optim_p:\n",
        "            self.p_val = self.logtheta_loglambda_p_[self.n_theta + 1 : self.n_theta + 1 + self.n_p]\n",
        "    elif self.method == \"interpolation\":\n",
        "        self.Lambda = None\n",
        "        if self.optim_p:\n",
        "            self.p_val = self.logtheta_loglambda_p_[self.n_theta : self.n_theta + self.n_p]\n",
        "\n",
        "    self.negLnLike, self.Psi_, self.U_ = self.likelihood(self.logtheta_loglambda_p_)\n",
        "    self._update_log()\n",
        "    return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation vector for a new point: build_psi_vec()\n",
        "\n",
        "- For a new x ∈ R^k, compute ψ(x) ∈ R^n with ψ_i = exp(−D(x, x_i)), using same distance construction and weights 10^θ as for R.\n",
        "\n",
        "Code mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def build_psi_vec(self, x: np.ndarray) -> None:\n",
        "    n = self.X_.shape[0]\n",
        "    theta10 = self._get_theta10_from_logtheta()\n",
        "    D = np.zeros(n)\n",
        "\n",
        "    if self.ordered_mask.any():\n",
        "        X_ordered = self.X_[:, self.ordered_mask]\n",
        "        x_ordered = x[self.ordered_mask]\n",
        "        D += cdist(x_ordered.reshape(1, -1), X_ordered, metric=\"sqeuclidean\", w=theta10[self.ordered_mask]).ravel()\n",
        "\n",
        "    if self.factor_mask.any():\n",
        "        X_factor = self.X_[:, self.factor_mask]\n",
        "        x_factor = x[self.factor_mask]\n",
        "        D += cdist(x_factor.reshape(1, -1), X_factor, metric=self.metric_factorial, w=theta10[self.factor_mask]).ravel()\n",
        "\n",
        "    psi = np.exp(-D)                  # ψ(x)\n",
        "    return psi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction at a new x: _pred() and predict()\n",
        "\n",
        "- Ordinary Kriging predictor (Forrester):\n",
        "  - Mean μ and R^{-1} reused (via stored U).\n",
        "  - Predictor: f(x) = μ + ψ(x)^T R^{-1} (y − 1·μ).\n",
        "  - Predictive variance s^2(x) depends on method:\n",
        "    - interpolation/regression: s^2(x) = σ^2 [1 + λ − ψ(x)^T R^{-1} ψ(x)].\n",
        "    - reinterpolation: uses R_adj = R − λI + εI for the variance term, consistent with a reinterpolation/nugget-adjusted scheme.\n",
        "- Expected improvement (EI): computed when requested using standard normal CDF/PDF via erf; the code returns negative EI for minimization.\n",
        "\n",
        "Code mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def _pred(self, x: np.ndarray) -> float:\n",
        "    y = self.y_.flatten()\n",
        "    # load θ, λ (λ transformed to linear scale), p\n",
        "    if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n",
        "        self.theta = self.logtheta_loglambda_p_[: self.n_theta]\n",
        "        lambda_ = 10.0**self.logtheta_loglambda_p_[self.n_theta : self.n_theta + 1]\n",
        "        if self.optim_p:\n",
        "            self.p_val = self.logtheta_loglambda_p_[self.n_theta + 1 : self.n_theta + 1 + self.n_p]\n",
        "    elif self.method == \"interpolation\":\n",
        "        self.theta = self.logtheta_loglambda_p_[: self.n_theta]\n",
        "        lambda_ = self.eps\n",
        "\n",
        "    U = self.U_; n = self.X_.shape[0]; one = np.ones(n)\n",
        "\n",
        "    # μ and R^{-1}r\n",
        "    y_tilde = np.linalg.solve(U, y); y_tilde = np.linalg.solve(U.T, y_tilde)\n",
        "    one_tilde = np.linalg.solve(U, one); one_tilde = np.linalg.solve(U.T, one_tilde)\n",
        "    mu = (one @ y_tilde) / (one @ one_tilde)\n",
        "\n",
        "    resid = y - one * mu\n",
        "    resid_tilde = np.linalg.solve(U, resid); resid_tilde = np.linalg.solve(U.T, resid_tilde)\n",
        "\n",
        "    psi = self.build_psi_vec(x)\n",
        "\n",
        "    # σ^2 and s^2(x)\n",
        "    SigmaSqr = (resid @ resid_tilde) / n\n",
        "    psi_tilde = np.linalg.solve(U, psi); psi_tilde = np.linalg.solve(U.T, psi_tilde)\n",
        "\n",
        "    if (self.method == \"interpolation\") or (self.method == \"regression\"):\n",
        "        SSqr = SigmaSqr * (1 + lambda_ - psi @ psi_tilde)   # s^2(x)\n",
        "    else:\n",
        "        Psi_adjusted = self.Psi_ - np.eye(n) * lambda_ + np.eye(n) * self.eps\n",
        "        Uint = np.linalg.cholesky(Psi_adjusted)\n",
        "        psi_tilde = np.linalg.solve(Uint, psi); psi_tilde = np.linalg.solve(Uint.T, psi_tilde)\n",
        "        SSqr = SigmaSqr * (1 - psi @ psi_tilde)\n",
        "\n",
        "    s = np.abs(SSqr) ** 0.5\n",
        "    f = mu + psi @ resid_tilde                                  # predictor\n",
        "\n",
        "    if self.return_ei:\n",
        "        yBest = np.min(y)\n",
        "        EITermOne = (yBest - f) * (0.5 + 0.5 * erf((1 / np.sqrt(2)) * ((yBest - f) / s)))\n",
        "        EITermTwo = s * (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((yBest - f) ** 2 / SSqr))\n",
        "        ExpImp = np.log10(EITermOne + EITermTwo + self.eps)     # numerically stable\n",
        "        return float(f), float(s), float(-ExpImp)               # negative EI\n",
        "    else:\n",
        "        return float(f), float(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch prediction: predict()\n",
        "\n",
        "- Ensures X has shape (n_samples, k).\n",
        "- Depending on return flags:\n",
        "  - return_val=\"y\": returns predictions only.\n",
        "  - return_std or \"s\": returns standard deviation(s).\n",
        "  - \"ei\" or \"all\": computes negative EI (and optionally y, s).\n",
        "\n",
        "Key implementation detail: X reshaping to avoid 1D shape bugs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def predict(self, X: np.ndarray, return_std=False, return_val: str = \"y\") -> np.ndarray:\n",
        "    self.return_std = return_std\n",
        "    X = self._reshape_X(X)               # ensures (n_samples, k)\n",
        "    if return_std:\n",
        "        predictions, stds = zip(*[self._pred(x_i)[:2] for x_i in X])\n",
        "        return np.array(predictions), np.array(stds)\n",
        "    if return_val == \"s\":\n",
        "        predictions, stds = zip(*[self._pred(x_i)[:2] for x_i in X])\n",
        "        return np.array(stds)\n",
        "    elif return_val == \"all\":\n",
        "        self.return_std = True; self.return_ei = True\n",
        "        predictions, stds, eis = zip(*[self._pred(x_i) for x_i in X])\n",
        "        return np.array(predictions), np.array(stds), np.array(eis)\n",
        "    elif return_val == \"ei\":\n",
        "        self.return_ei = True\n",
        "        predictions, eis = zip(*[(self._pred(x_i)[0], self._pred(x_i)[2]) for x_i in X])\n",
        "        return np.array(eis)\n",
        "    else:\n",
        "        predictions = [self._pred(x_i)[0] for x_i in X]\n",
        "        return np.array(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method variants and options\n",
        "\n",
        "- interpolation: Uses eps as tiny nugget; predictive variance s^2(x) = σ^2[1 + eps − ψ^T R^{-1} ψ].\n",
        "- regression: Uses optimized λ; same variance as above with λ.\n",
        "- reinterpolation: For prediction variance only, adjusts R by removing λ and adding ε before the variance backsolve; predictor mean still uses R with λ.\n",
        "- isotropic: n_theta = 1; a single θ controls all ordered dimensions; code expands 10^θ to k when building distances.\n",
        "- variable types: ordered_mask includes numeric and int; factor_mask uses a categorical metric and still lives inside exp(−D).\n",
        "- parameters in log10: θ, λ are optimized and stored on log10 scale; transformed to linear (10^·) only at the points where R and s^2 require them.\n",
        "\n",
        "## Relation to Forrester (Ch. 2/3/6)\n",
        "\n",
        "- Correlation form exp(−∑ θ_d (Δx_d)^2) and the ordinary Kriging predictor f(x) = μ + r^T R^{-1}(y−1μ) are textbook.\n",
        "- Concentrated likelihood n/2 log(σ^2) + 1/2 log|R| is used for hyperparameter estimation.\n",
        "- Predictive variance s^2(x) = σ^2(1 + λ − r^T R^{-1} r) reflects nugget (λ) for regression models; interpolation uses a tiny eps. The reinterpolation branch mirrors the “nugget-adjusted” variance computation.\n",
        "- Expected Improvement (EI) follows the usual formula for minimization; the implementation uses erf for Φ and exp for φ, with a log10 stabilization.\n",
        "\n",
        "## Practical notes\n",
        "\n",
        "- Bounds: fit() assembles bounds for [log θ] (k entries), [log λ] (1), and optionally p (n_p).\n",
        "- Stability: Cholesky factorization is used; ill-conditioning penalized by returning a large negLnLike if factorization fails.\n",
        "- Shape safety: predict() reshapes inputs; plot() uses a 1D grid reshaped to (n,1) accordingly.\n",
        "\n",
        "## Kriging 1D demo: \\u03bc, \\u03c3^2, s(x), and EI (Forrester et al., 2008, Ch. 2/3/6)\n",
        "This notebook fits the provided Kriging implementation to a 1D function and visualizes:\n",
        "\n",
        "- The predictor f^ (x)\n",
        "- The estimated mean \\u03bc and variance \\u03c3^2 from the concentrated likelihood\n",
        "- The predictive standard deviation s(x)\n",
        "- The Expected Improvement (EI) curve (for minimization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from spotpython.surrogate.kriging import Kriging\n",
        "\n",
        "# 1D training data (noisy-free quadratic)\n",
        "rng = np.random.default_rng(0)\n",
        "X_train = np.linspace(-1.0, 1.0, 9).reshape(-1, 1)\n",
        "y_train = (X_train[:, 0]**2 + 0.1*X_train[:, 0]).astype(float)\n",
        "\n",
        "# Fit Kriging (regression with nugget optimization)\n",
        "model = Kriging(method=\"regression\", var_type=[\"num\"], seed=124)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"theta(log10):\", model.theta)\n",
        "print(\"Lambda(log10):\", model.Lambda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "# Compute \\u03bc and \\u03c3^2 as in likelihood() (Forrester Ch. 3)\n",
        "y = model.y_.flatten()\n",
        "n = model.X_.shape[0]\n",
        "one = np.ones(n)\n",
        "U = model.U_\n",
        "\n",
        "temp_y = np.linalg.solve(U, y)\n",
        "temp_1 = np.linalg.solve(U, one)\n",
        "vy = np.linalg.solve(U.T, temp_y)\n",
        "v1 = np.linalg.solve(U.T, temp_1)\n",
        "mu = (one @ vy) / (one @ v1)\n",
        "resid = y - one * mu\n",
        "tresid = np.linalg.solve(U, resid)\n",
        "tresid = np.linalg.solve(U.T, tresid)\n",
        "sigma2 = (resid @ tresid) / n\n",
        "print(f\"mu: {mu:.6f}, sigma^2: {sigma2:.6e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "# Prediction grid\n",
        "X_grid = np.linspace(X_train.min(), X_train.max(), 200).reshape(-1, 1)\n",
        "\n",
        "# Predict f^(x) and s(x)\n",
        "y_pred, s_pred = model.predict(X_grid, return_std=True)\n",
        "\n",
        "# EI: model.predict(..., return_val=\"all\") returns (y, s, -log10(EI))\n",
        "y_all, s_all, neg_log10_ei = model.predict(X_grid, return_val=\"all\")\n",
        "EI = 10.0 ** (-neg_log10_ei)\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, figsize=(7, 10), sharex=True)\n",
        "\n",
        "# 1) Predictor with data\n",
        "axs[0].plot(X_grid[:, 0], y_pred, 'k', label='Kriging predictor f^')\n",
        "axs[0].scatter(X_train[:, 0], y_train, c='r', label='data')\n",
        "axs[0].set_ylabel('f^ (x)')\n",
        "axs[0].grid(True)\n",
        "axs[0].legend(loc='best')\n",
        "\n",
        "# 2) Predictive standard deviation\n",
        "axs[1].plot(X_grid[:, 0], s_pred, 'b')\n",
        "axs[1].set_ylabel('s(x)')\n",
        "axs[1].grid(True)\n",
        "axs[1].set_title(f\"mu={mu:.4f}, sigma^2={sigma2:.2e}\")\n",
        "\n",
        "# 3) Expected Improvement (minimization)\n",
        "axs[2].plot(X_grid[:, 0], EI, 'g')\n",
        "axs[2].set_xlabel('x')\n",
        "axs[2].set_ylabel('EI(x)')\n",
        "axs[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}