{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Neural ODEs {#sec-light-neural-ode-601}\n",
        "\n",
        "\n",
        "Neural ODEs are related to Residual Neural Networks (ResNets).\n",
        "We consider ResNets in @sec-resnets.\n",
        "\n",
        "\n",
        "## Neural Ordinary Differential Equations\n",
        "\n",
        "Neural Ordinary Differential Equations (Neural ODEs) are a class of models that are based on ordinary differential equations (ODEs). They are a generalization of ResNets, where the depth of the network is treated as a continuous parameter. Neural ODEs have been introduced by @chen18b.\n",
        "We will consider dynamical systems first.\n",
        "\n",
        "\n",
        "::: {#def-dynamical-system}\n",
        "\n",
        "A dynamical system is a triple\n",
        "$$\n",
        "(\\mathcal{S}, \\mathcal{T}, \\Phi)\n",
        "$$\n",
        "where\n",
        "\n",
        "* $\\mathcal{S}$ is the *state space*\n",
        "* $\\mathcal{T}$ is the *parameter space*, and\n",
        "* $\\Phi: (\\mathcal{T} \\times \\mathcal{S}) \\longrightarrow \\mathcal{S}$ is the evolution.\n",
        "\n",
        ":::\n",
        "\n",
        "@def-dynamical-system is a very general definition that includes all sort of dynamical systems. We deal with ODEs where $\\Phi$ plays the role of the *general solution*: indeed a 1-parameter family of transformations of the state space. $\\mathcal{T}=\\mathbb{R}_{+}$ is the time, and usually, $\\mathcal{S}=\\mathbb{R}^{n}$ is the state space. The evolution takes a point in space (initial value), a point in time, and returns the a point in space.\n",
        "A general solution to an ODE is a function $y: I \\times \\mathbb{R}^{n} ⟶ \\mathbb{R}^{n}$: a 1-parameter (usually time is the parameter) family of transformations of the state space. A 1-parameter family of transformations is often called a *flow*. \n",
        "\n",
        "First-order Ordinary Differential Equations (ODEs) can be defined as follows:\n",
        "\n",
        "::: {#def-ode}\n",
        "\n",
        "### First-Order Ordinary Differential Equation (ODE)\n",
        "\n",
        "$$\n",
        "\\mathbf{\\dot{y}}(t) = f(t, \\mathbf{y}(t)),\\quad \\mathbf{y}(t_0) = y_0,\\quad f: \\mathbb{R} \\times \\mathbb{R}^n \\to \\mathbb{R}^n\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "The solution of the ODE is the function $\\mathbf{y}(t)$ that satisfies the ODE and the initial condition, which can be stated as an initial value problems (IVP), i.e. predict $\\mathbf{y}(t_1)$ given $\\mathbf{y}(t_0)$.\n",
        "\n",
        "::: {#def-ivp}\n",
        "\n",
        "### Initial Value Problem (IVP)\n",
        "\n",
        "$$\n",
        "\\mathbf{y}(t_1) = \\mathbf{y}(t_0) + \\int_{t_0}^{t_1} f(\\mathbf{y}(t), t)\n",
        "\\mathrm{d}t = \\textrm{ODESolve}(\\mathbf{y}(t_0), f, t_0, t_1)\n",
        "$$ {#eq-ivp}\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "The existence and uniqueness of solutions to an IVP is ensured by the Picard-Lindel&ouml;f theorem, provided the RHS of the ODE is *Lipschitz continuous*. Lipschitz continuity is a property that pops up quite often in ODE-related results in ML.\n",
        "\n",
        "::: {#def-lipschitz}\n",
        "### Lipschitz Continuity\n",
        "\n",
        "A function $f: X \\subset \\mathbb{R}^{n} ⟶ \\mathbb{R}^{n}$ is called *Lipschitz continuous* (with constant $\\lambda$) if\n",
        "\n",
        "$$\n",
        "|| f(x_{1}) - f(x_{2}) || \\leq \\lambda ||x_{1} - x_{2}|| \\quad \\forall x_{1},x_{2} \\in X.\n",
        "$$\n",
        "\n",
        "::: \n",
        "Note that Lipschitz continuity is a stronger condition than just continuity.\n",
        "\n",
        "Numerical solvers  can be used  to perform the forward pass and solve the IVP. If we use, for example, Euler's method, we have the following update rule:\n",
        "\n",
        "$$\n",
        "\\mathbf{y}(t+h) = \\mathbf{y}(t) + hf(\\mathbf{y}(t), t)\n",
        "$$ {#eq-euler-update}\n",
        "\n",
        "where $h$ is the step size. The update rule is applied iteratively to solve the IVP. \n",
        "The solution is a discrete approximation of the continuous function $\\mathbf{y}(t)$.\n",
        "\n",
        "@eq-euler-update looks almost identical to a ResNet block (see @eq-residual-connection).\n",
        "This was one of the main motivations for Neural ODEs [@chen18b].\n",
        "\n",
        "ResNets update hidden states by employing residual connections:\n",
        "\n",
        "$$\n",
        "\\mathbf{y}_{l+1} = \\mathbf{y}_l + f(\\mathbf{y}_l, \\theta_l)\n",
        "$$\n",
        "\n",
        "where $f$ is a neural network with parameters $\\theta_l$, and $\\mathbf{y}_l$ and\n",
        "$\\mathbf{y}_{l+1}$ are the hidden states at subsequent layers, $l \\in \\{0,\n",
        "\\ldots, L\\}$.\n",
        "\n",
        "These updates can be seen as Euler discretizations of continuous transformations.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{\\dot{y}} &= f(\\mathbf{y}, t, \\theta)\n",
        "\\\\\n",
        "&\\Bigg\\downarrow \\ \\textrm{Euler Discretization}\n",
        "\\\\\n",
        "\\mathbf{y}_{n+1} &= \\mathbf{y}_n + h f(\\mathbf{y}_n, t_n, \\theta)\n",
        "\\end{align}\n",
        "\n",
        "What happens in a residual network (with step sizes $h$) if we consider the continuous limit of each discrete layer in the network?\n",
        "What happens as we add more layers and take smaller steps?\n",
        "The answer seems rather astounding: instead of having a discrete number of layers between the input and output domains, we allow the evolution of the hidden states to become continuous.\n",
        "\n",
        "![A residual network defines a discrete sequence of finite transformations. Circles represent evaluation locations. Figure credit @chen18b.](./figures_static/resnet_0_viz.png){width=50% #fig-neural-ode}\n",
        "\n",
        "![An ODE network defines a vector field, which continuously transforms the state. Circles represent evaluation locations. Figure credit @chen18b.](./figures_static/odenet_0_viz.png){width=50% #fig-resnet-ode}\n",
        "\n",
        "The main technical difficulty in training continuous-depth networks is performing backpropagation through the ODE solver.\n",
        "Differentiating through the operations of the forward pass is straightforward, but incurs a high memory cost and introduces additional numerical error.\n",
        "\n",
        "@pont87a treated the ODE solver as a black box, and computed gradients using the adjoint sensitivity method.\n",
        "This approach computes gradients by solving a second, augmented ODE backwards in time, and is applicable to all ODE solvers. It scales linearly with problem size, has low memory cost, and explicitly controls numerical error.\n",
        "\n",
        "Consider optimizing a scalar-valued loss function $L()$, whose input is the result of an ODE solver:\n",
        "$$\n",
        "L(y(t_1) = L \\left(y(t_0) + \\int_{t_0}^{t_1} f(y(t), t, \\theta) dt \\right) = L \\left( \\textrm{ODESolve}( y(t_0), f, t_0, t_1, \\theta) \\right)\n",
        "$$ {#eq-ode-loss}\n",
        "\n",
        "@eq-ode-loss is related to {@eq-ivp}.\n",
        "To optimize $L$, we require gradients with respect to $\\theta$. \n",
        "\n",
        "Similar to standard neural networks, we start with determining how the gradient of the loss depends on the hidden state $y(t)$ at each instant.\n",
        "This quantity is called the adjoint $a(t) = \\frac{\\partial{L}}{\\partial y(t)}$.\n",
        "It satisfies the following IVP:\n",
        "\n",
        "$$\n",
        " \\dot{\\mathbf{a}}(t) = -\\mathbf{a}(t)^{\\top} \\frac{\\partial f(\\mathbf{x}(t), t,\n",
        "\\theta)}{\\partial \\mathbf{x}}, \\quad \\mathbf{a}(t_1) = \\frac{\\partial L}{\\partial \\mathbf{x}(t_1)}.\n",
        "$$\n",
        "\n",
        "Its dynamics are given by another ODE, which can be thought of as the instantaneous analog of the chain rule:\n",
        "$$\n",
        "\\frac{d a(t)}{d t} = - a(t)^{T} \\frac{\\partial f(y(t), t, \\theta)}{\\partial y}.\n",
        "$$\n",
        "\n",
        "Thus, starting from the initial (remember we are running backwards) value $\\mathbf{a}(t_1) = \\frac{\\partial L}{\\partial \\mathbf{x}(t_1)}$, we can compute $\\mathbf{a}(t_0) = \\frac{\\partial L}{\\partial \\mathbf{x}(t_0)}$ by another call to an ODE solver.\n",
        "\n",
        "Finally, computing the gradients with respect to the parameters $\\theta$ requires evaluating a third integral, which depends on both $\\mathbf{x}(t)$ and $\\mathbf{a}(t)$:\n",
        "\n",
        "$$\n",
        "\\frac{\\mathrm{d}L}{\\mathrm{d}\\theta} = -\\int_{t_1}^{t_0} \\mathbf{a}(t)^{\\top}\\frac{\\partial f}{\\partial \\theta} \\mathrm{d}t,\n",
        "$$\n",
        "\n",
        "So this method trades off computation for memory---in fact the memory requirement for this gradient calculation is only $\\mathcal{O}(1)$ with respect to the number of layers.\n",
        "The corresponding algorithm is described in @chen18b, see also @fig-adjoint.\n",
        "\n",
        "![Reverse-mode differentiation of an ODE solution. The adjoint sensitivity method solves an augmented ODE backwards in time. The augmented system contains both the original state and the sensitivity of the loss with respect to the state.\tIf the loss depends directly on the state at multiple observation times, the adjoint state must be updated in the direction of the partial derivative of the loss with respect to each observation. Figure credit @chen18b.](./figures_static/AdjointFig_w_L.png){width=90% #fig-adjoint}\n",
        "\n",
        "\n",
        "[Here](https://vaipatel.com/deriving-the-adjoint-equation-for-neural-odes-using-lagrange-multipliers/#:~:text=Luckily%2C%20a%20very%20well%2Dknown,to%20store%20intermediate%20function%20evaluations.) you can find a very good explanation of the following result based on Lagrange multipliers.\n",
        "\n",
        "## Regression Example\n",
        "\n",
        "To illustrate this concept, we will consider a simple regression example.\n",
        "This example is based on the Neural-ODEs tutorial from [Neural Ordinary Differential Equations](https://github.com/manncodes/neural-ODEs/blob/main/chapters/Chapter%203/Neural_Ordinary_Differential_Equations.ipynb), which is provided by @chen18b.\n",
        "We will use the ODE solvers from [Torchdiffeq](https://github.com/rtqichen/torchdiffeq).\n",
        "\n",
        "\n",
        "\n",
        "Neural ODEs, or ODE-Nets, build complex models by chaining together simple building blocks, similar to residual networks. Here, our base layer will define the dynamics of an ODE, which will be interconnected using an ODE solver to form the complete neural network model.\n",
        "\n",
        "\n",
        "### Specifying the Dynamics Layer\n",
        "\n",
        "The dynamics of an ODE can be captured by the equation:\n",
        "\n",
        "$$\n",
        "\\dot y(t) = f(y(t), t,  \\theta), \\qquad y(0) = y_0,\n",
        "$$\n",
        "where the initial value $y_0 \\in \\mathbb{R}^n$.\n",
        "The $\\theta$ parameters were added to the dynamics, so the dynamics function has the dimensions $f : \\mathbb{R}^{n} \\times \\mathbb{R} \\times \\mathbb{R}^{|\\theta|} \\to \\mathbb{R}^n$, where $|\\theta|$ is the number of parameters we've added to $f$.\n",
        "\n",
        "We need the dynamics function to take in the current state $y(t)$ of the ODE, the current time $t$, and some parameters $\\theta$, and output $\\frac{\\partial y(t)}{\\partial t}$, which has the same shape as $y(t)$.\n",
        "They are passed as input to a multi-layer perceptron (MLP). Multiple evaluations of this dynamics layer can be combined using any suitable ODE solver, such as the adaptive-step Dormand-Price solver implemented in the `torchdiffeq` library's `odeint` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchdiffeq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by defining an MLP class to serve as the building block of our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "            if i < len(layer_sizes) - 2:\n",
        "                layers.append(nn.Tanh())\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll define a ResNet class that uses the MLP as its inner component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layer_sizes, depth):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.mlp = MLP(layer_sizes)\n",
        "        self.depth = depth\n",
        "\n",
        "    def forward(self, x):\n",
        "        for _ in range(self.depth):\n",
        "            x = self.mlp(x) + x\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* `ODEFunc` defines how the system evolves over time using the MLP to approximate derivatives $\\dot{y}(t)$. \n",
        "* `ODEBlock` specifies the network structure. It uses `torchdiffeq.odeint` to integrate these dynamics over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ODEFunc(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(ODEFunc, self).__init__()\n",
        "        self.mlp = MLP(layer_sizes)\n",
        "\n",
        "    def forward(self, t, y):\n",
        "        t_expanded = t.expand_as(y)\n",
        "        state_and_time = torch.cat([y, t_expanded], dim=1)\n",
        "        return self.mlp(state_and_time)\n",
        "\n",
        "class ODEBlock(nn.Module):\n",
        "    def __init__(self, odefunc):\n",
        "        super(ODEBlock, self).__init__()\n",
        "        self.odefunc = odefunc\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = torch.tensor([0.0, 1.0])\n",
        "        out = torchdiffeq.odeint(self.odefunc, x, t, atol=1e-3, rtol=1e-3)\n",
        "        return out[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a toy 1D dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "inputs = torch.linspace(-2.0, 2.0, 10).reshape(10, 1)\n",
        "targets = inputs**3 +  0.1 * inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We specify the hyperparameters for the ResNet and ODE-Net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "layer_sizes = [1, 25, 1]\n",
        "param_scale = 1.0\n",
        "step_size = 0.01\n",
        "train_iters = 1000\n",
        "resnet_depth = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize and train the ResNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "resnet = ResNet(layer_sizes, resnet_depth)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=step_size)\n",
        "\n",
        "for _ in range(train_iters):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = resnet(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We need to change the input dimension to 2, to allow time-dependent dynamics.\n",
        "odenet_layer_sizes = [2, 25, 1]\n",
        "\n",
        "# Initialize and train ODE-Net.\n",
        "odefunc = ODEFunc(odenet_layer_sizes)\n",
        "odenet = ODEBlock(odefunc)\n",
        "optimizer = optim.SGD(odenet.parameters(), lr=step_size)\n",
        "\n",
        "for _ in range(train_iters):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = odenet(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, plot the predictions of both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fine_inputs = np.linspace(-3.0, 3.0, 100).reshape(-1, 1)\n",
        "fine_inputs_tensor = torch.from_numpy(fine_inputs).float()\n",
        "plt.figure(figsize=(6, 4), dpi=150)\n",
        "plt.scatter(inputs, targets, color='green', label='Targets')\n",
        "plt.plot(fine_inputs, resnet(fine_inputs_tensor).detach().numpy(), color='blue', label='ResNet predictions')\n",
        "\n",
        "plt.plot(fine_inputs, odenet(fine_inputs_tensor).detach().numpy(), color='red', label='ODE Net predictions')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading\n",
        "\n",
        "Neural ODEs have received a lot of attention in the past few years, ever since their introduction in Neurips 2018. Some of many many work in this field include:\n",
        "\n",
        "* Neural Stochastic Differential Equations (Neural SDEs),\n",
        "* Neural Controlled Differential Equations (Neural CDEs),\n",
        "* Graph ODEs,\n",
        "* Hamiltonial Neural Networks, and\n",
        "* Lagrangian Neural Networks.\n",
        "\n",
        "[Michael Poli](https://zymrael.github.io/) maintains the excellent [Awesome Neural ODE](https://github.com/Zymrael/awesome-neural-ode),\n",
        "a collection of resources regarding the interplay between neural differential equations, dynamical systems, deep learning, control, numerical methods and scientific machine learning.\n",
        "\n",
        "[Torchdyn](https://github.com/DiffEqML/torchdyn) is an excellent library for Neural Differential Equations.\n",
        "\n",
        "[Implicit Layers](https://implicit-layers-tutorial.org/) is a list of tutorials \n",
        "on implicit functions and automatic differentiation, Neural ODEs, and Deep Equilibrium Models.\n",
        "\n",
        "[Understanding Neural ODE's](https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html) is an excellent blogpost on ODEs and Neural ODEs.\n",
        "\n",
        "[Patrick Kidger](https://kidger.site/)'s doctoral dissertation is an excellent textbook on Neural Differential Equations, see @kidg22a."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}