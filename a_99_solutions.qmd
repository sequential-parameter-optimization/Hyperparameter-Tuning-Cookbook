---
execute:
  cache: false
  eval: true
  echo: true
  warning: false
jupyter: python3
---

# Solutions to Selected Exercises

::: {.callout-warning}
* Solutions are incomplete and need to be corrected!
* They serve as a starting point for the final solution.
:::


## Data-Driven Modeling and Optimization


### Histograms

::: {#sol-curve}
###  Density Curve

* We can calculate propabilities.
*  We only need two parameters (the mean and the sd) to form the curve -> Store data more efficently
*  Blanks can be filled
:::

### The Normal Distribution

::: {#sol-2SD}
###  TwoSDAnswer
95%
:::

::: {#sol-2SD1}
###  OneSDAnswer
68%
:::

::: {#sol-2SD2}
###  ThreeSDAnswer
99,7%
:::

::: {#sol-2SD3}
###  DataRangeAnswer
80 - 120
:::

::: {#sol-2SD4}
###  PeakHeightAnswer
low
:::

### The mean, the media, and the mode

### The exponential distribution

### Population and Estimated Parameters

::: {#sol-POP1}

### ProbabilityAnswer
50%
:::


### Calculating the Mean, Variance and Standard Deviation

::: {#sol-CAL1}
### MeanDifferenceAnswer
If we have all the data, $\mu$ is the population mean and x-bar is the sample mean. We don't have the full information.
:::

::: {#sol-CAL2}
### EstimateMeanAnswer
Sum of the values divided by n.
:::

::: {#sol-CAL3}
### SigmaSquaredAnswer
Variance
:::

::: {#sol-CAL4}
### EstimatedSDAnswer
The same as the normal standard deviation, but using n-1.
:::

::: {#sol-CAL5}
### VarianceDifferenceAnswer
$n$ and $n-1$
:::

::: {#sol-MAT1}
### ModelBenefitsAnswer
- Approximation
- Prediction
- Understanding
:::

::: {#sol-SAM1}
### SampleDefinitionAnswer
It's a subset of the data.
:::

### Hypothesis Testing and the Null-Hypothesis

::: {#sol-Hyp1}
### RejectHypothesisAnswer
It means the evidence supports the alternative hypothesis, indicating that the null hypothesis is unlikely to be true.
:::

::: {#sol-Hyp2}
### NullHypothesisAnswer
It's a statement that there is no effect or no difference, and it serves as the default or starting assumption in hypothesis testing.
:::

::: {#sol-Hyp3}
### BetterDrugAnswer
By conducting experiments and statistical tests to compare the new drug's effectiveness against the current standard and demonstrating a significant improvement.
:::

### Alternative Hypotheses, Main Ideas

### p-values: What they are and how to interpret them

::: {#sol-PVal1}
### PValueIntroductionAnswer
We can reject the null hypothesis. We can make a decision.
:::

::: {#sol-PVal2}
### PValueRangeAnswer
It can only be between 0 and 1.
:::

::: {#sol-PVal3}
### PValueRangeAnswer
It can only be between 0 and 1.
:::

::: {#sol-PVal4}
### TypicalPValueAnswer
The chance that we wrongly reject the null hypothesis.
:::

::: {#sol-PVal5}
### FalsePositiveAnswer
If we have a false-positive, we succeed in rejecting the null hypothesis. But in fact/reality, this is false -> False positive.
:::

### How to calculate p-values

::: {#sol-Calc1}
### CalculatePValueAnswer
Probability of specific result, probability of outcome with the same probability, and probability of events with smaller probability.
:::

::: {#sol-Calc2}
### SDCalculationAnswer
7 is the SD.
:::

::: {#sol-Calc3}
### SidedPValueAnswer
If we are not interested in the direction of the change, we use the two-sided. If we want to know about the direction, the one-sided.
:::

::: {#sol-Calc4}
### CoinTestAnswer
TBD
:::


::: {#sol-Calc5}
### BorderPValueAnswer
TBD
:::

::: {#sol-Calc6}
### OneSidedPValueCautionAnswer
If you look in the wrong direction, there is no change.
:::

::: {#sol-Calc7}
### BinomialDistributionAnswer
TBD
:::

### p-hacking: What it is and how to avoid it

::: {#sol-Hack1}
### PHackingWaysAnswer

* Performing repeats until you find one result with a small p-value -> false positive result.
* Increasing the sample size within one experiment when it is close to the threshold.
:::

::: {#sol-Hack2}
### AvoidPHackingAnswer
Specify the number of repeats and the sample sizes at the beginning.
:::

::: {#sol-Hack3}
### MultipleTestingProblemAnswer
TBD
:::

### Covariance


::: {#sol-Cov1}
### CovarianceDefinitionAnswer
Formula
:::


::: {#sol-Cov2}
### CovarianceMeaningAnswer
Large values in the first variable result in large values in the second variable.
:::


::: {#sol-Cov3}
### CovarianceVarianceRelationshipAnswer
Formula
:::


::: {#sol-Cov4}
### HighCovarianceAnswer
No, size doesn't matter.
:::


::: {#sol-Cov5}
### ZeroCovarianceAnswer
No relationship
:::


::: {#sol-Cov6}
### NegativeCovarianceAnswer
Yes
:::


::: {#sol-Cov7}
### NegativeVarianceAnswer
No
:::

### Pearson's Correlation


::: {#sol-Corr1}
### CorrelationValueAnswer
Recalculate
:::


::: {#sol-Corr2}
### CorrelationRangeAnswer
From -1 to 1
:::


::: {#sol-Corr3}
### CorrelationFormulaAnswer
Formula
:::

### Boxplots


::: {#sol-StatPow1}
### UnderstandingStatisticalPower
It is the probability of correctly rejecting the null hypothesis.
:::


::: {#sol-StatPow2}
### DistributionEffectOnPower
Power analysis is not applicable.
:::


::: {#sol-StatPow3}
### IncreasingPower
By taking more samples.
:::


::: {#sol-StatPow4}
### PreventingPHacking
TBD
:::


::: {#sol-StatPow5}
### SampleSizeAndPower
The power will be low.
:::

### Power Analysis

::: {#sol-PowAn1}
### MainFactorsAffectingPower
The overlap (distance of the two means) and sample sizes.
:::

::: {#sol-PowAn2}
### PowerAnalysisOutcome
The sample size needed.
:::


::: {#sol-PowAn3}
### RisksInExperiments
Few experiments lead to very low power, and many experiments might result in p-hacking.
:::


::: {#sol-PowAn4}
### StepsToPerformPowerAnalysis

1. Select power
2. Select threshold for significance (alpha)
3. Estimate the overlap (done by the effect size)
:::

### The Central Limit Theorem

::: {#sol-CenLi1}
### CentralLimitTheoremAnswer
TBD
:::

### Boxplots

::: {#sol-BoxPlo1}
### MedianAnswer
The median.
:::

::: {#sol-BoxPlo2}
### BoxContentAnswer
50% of the data.
:::

### R-squared

::: {#sol-RSqu1}
### RSquaredFormulaAnswer
TBD
:::

::: {#sol-RSqu2}
### NegativeRSquaredAnswer
If you fit a line, no, but there are cases where it could be negative. However, these are usually considered useless.
:::

::: {#sol-RSqu3}
### RSquaredCalculationAnswer
TBD
:::

#### The main ideas of fitting a line to data (The main ideas of least squares and linear regression.)

::: {#sol-FitLin1}
### LeastSquaresAnswer
It is the calculation of the smallest sum of residuals when you fit a model to data.
:::

### Linear Regression

### Multiple Regression

### A Gentle Introduction to Machine Learning


::: {#sol-ML1}
### RegressionVsClassificationAnswer
Regression involves predicting continuous values (e.g., temperature, size), while classification involves predicting discrete values (e.g., categories like cat, dog).
:::

### Maximum Likelihood

::: {#sol-MaxLike1}
### LikelihoodConceptAnswer
The distribution that fits the data best.
:::

### Probability is not Likelihood

::: {#sol-Prob1}
### ProbabilityVsLikelihoodAnswer
Likelihood: Finding the curve that best fits the data. Probability: Calculating the probability of an event given a specific curve.
:::

### Cross Validation


::: {#sol-CroVal1}
### TrainVsTestDataAnswer
Training data is used to fit the model, while testing data is used to evaluate how well the model fits.
:::


::: {#sol-CroVal2}
### SingleValidationIssueAnswer
The performance might not be representative because the data may not be equally distributed between training and testing sets.
:::

::: {#sol-CroVal3}
### FoldDefinitionAnswer
TBD
:::

::: {#sol-CroVal4}
### LeaveOneOutValidationAnswer
Only one data point is used as the test set, and the rest are used as the training set.
:::

### The Confusion Matrix

::: {#sol-ConMat1}
### ConfusionMatrixAnswer
TBD
:::

### Sensitivity and Specificity


::: {#sol-SenSpe1}
### SensitivitySpecificityAnswer1
TBD
:::

::: {#sol-SenSpe2}
### SensitivitySpecificityAnswer2
TBD
:::

### Bias and Variance


::: {#sol-MalLea1}
### BiasAndVarianceAnswer
TBD
:::

### Mutual Information

::: {#sol-MutInf1}
### MutualInformationExampleAnswer
TBD
:::


### Principal Component Analysis (PCA)

::: {#sol-PCA1}
### WhatIsPCAAnswer
A dimension reduction technique that helps discover important variables.
:::


::: {#sol-PCA2}
### screePlotAnswer
It shows how much variation is defined by the data.
:::

::: {#sol-PCA3}
### LeastSquaresInPCAAnswer
No, in the first step it tries to maximize distances.
:::


::: {#sol-PCA4}
### PCAStepsAnswer
1. Calculate mean
2. Shift the data to the center of the coordinate system
3. Fit a line by maximizing the distances
4. Calculate the sum of squared distances
5. Calculate the slope
6. Rotate
:::


::: {#sol-PCA5}
### EigenvaluePC1Answer
Formula (to be specified).
:::

::: {#sol-PCA6}
### DifferencesBetweenPointsAnswer
No, because the first difference is measured on the PC1 scale and it is more important.
:::


::: {#sol-PCA7}
### ScalingInPCAAnswer
Scaling by dividing by the standard deviation (SD).
:::

::: {#sol-PCA8}
### DetermineNumberOfComponentsAnswer
TBD
:::


::: {#sol-PCA9}
### LimitingNumberOfComponentsAnswer

1. The dimension of the problem
2. Number of samples
:::

### t-SNE


::: {#sol-tSNE1}
### WhyUseTSNEAnswer
For dimension reduction and picking out the relevant clusters.
:::


::: {#sol-tSNE2}
### MainIdeaOfTSNEAnswer
To reduce the dimensions of the data by reconstructing the relationships in a lower-dimensional space.
:::


::: {#sol-tSNE3}
### BasicConceptOfTSNEAnswer

1. First, randomly arrange the points in a lower dimension
2. Decide whether to move points left or right, depending on distances in the original dimension
3. Finally, arrange points in the lower dimension similarly to the original dimension
:::


::: {#sol-tSNE4}
### TSNEStepsAnswer

1. Project data to get random points
2. Set up a matrix of distances
3. Calculate the inner variances of the clusters and the Gaussian distribution
4. Do the same with the projected points
5. Move projected points so the second matrix gets more similar to the first matrix
:::

### K-means clustering


::: {#sol-KMeans1}
### HowKMeansWorksAnswer

1. Select the number of clusters
2. Randomly select distinct data points as initial cluster centers
3. Measure the distance between each point and the cluster centers
4. Assign each point to the nearest cluster
5. Repeat the process
:::


::: {#sol-KMeans2}
### QualityOfClustersAnswer
Calculate the within-cluster variation.
:::


::: {#sol-KMeans3}
### IncreasingKAnswer
If k is too high, each point would be its own cluster. If k is too low, you cannot see the structures.
:::

### DBSCAN


::: {#sol-DBSCAN1}
### CorePointInDBSCANAnswer
A point that is close to at least k other points.
:::


::: {#sol-DBSCAN2}
### AddingVsExtendingAnswer
Adding means we add a point and then stop. Extending means we add a point and then look for other neighbors from that point.
:::


::: {#sol-DBSCAN3}
### OutliersInDBSCANAnswer
Points that are not core points and do not belong to existing clusters.
:::

### K-nearest neighbors


::: {#sol-KNN1}
### AdvantagesAndDisadvantagesOfKAnswer

* k = 1: Noise can disturb the process because of possibly incorrect measurements of points.
* k = 100: The majority can be wrong for some groups. It is smoother, but there is less chance to discover the structure of the data.

:::

### Naive Bayes


::: {#sol-NaiveBayes1}
### NaiveBayesFormulaAnswer
TBD
:::


::: {#sol-NaiveBayes2}
### CalculateProbabilitiesAnswer
TBD
:::

### Gaussian Naive Bayes



::: {#sol-GaussianNB1}
### UnderflowProblemAnswer
Small values multiplied together can become smaller than the limits of computer memory, resulting in zero. Using logarithms (e.g., log(1/2) -> -1, log(1/4) -> -2) helps prevent underflow.
:::

