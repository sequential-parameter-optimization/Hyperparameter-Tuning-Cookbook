<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>17&nbsp; Machine Learning and Artificial Intelligence – Hyperparameter Tuning Cookbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./300_hpt_intro.html" rel="next">
<link href="./100_ddmo.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<meta name="twitter:title" content="17&nbsp; Machine Learning and Artificial Intelligence – Hyperparameter Tuning Cookbook">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./200_mlai.html">Machine Learning and AI</a></li><li class="breadcrumb-item"><a href="./200_mlai.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Machine Learning and Artificial Intelligence</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hyperparameter Tuning Cookbook</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sequential-parameter-optimization/spotPython" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Hyperparameter-Tuning-Cookbook.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_optimization_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002_awwe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Aircraft Wing Weight Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003_scipy_optimize_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to <code>scipy.optimize</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004_spot_sklearn_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Using <code>scipy</code> Optimizers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Numerical Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_num_rsm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction: Numerical Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_num_spot_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to spotPython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008_num_spot_multidim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multi-dimensional Functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009_num_spot_anisotropic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Isotropic and Anisotropic Kriging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010_num_spot_sklearn_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Using <code>sklearn</code> Surrogates in <code>spotPython</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_num_spot_sklearn_gaussian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Gaussian Process Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_num_spot_ei.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Expected Improvement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_num_spot_noisy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Handling Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014_num_spot_ocba.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Optimal Computational Budget Allocation in <code>Spot</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_num_spot_correlation_p.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Kriging with Varying Correlation-p</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Data-Driven Modeling and Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_ddmo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Data-Driven Modeling and Optimization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning and AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./200_mlai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Machine Learning and Artificial Intelligence</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Hyperparameter Tuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./300_hpt_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with Sklearn</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./400_spot_hpt_sklearn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">HPT: sklearn</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./401_spot_hpt_sklearn_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">HPT: sklearn SVC on Moons Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with River</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./500_spot_hpt_river.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">HPT: River</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./501_spot_river_gui.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Simplifying Hyperparameter Tuning in Online Machine Learning—The spotRiverGUI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./502_spot_hpt_river_friedman_htr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title"><code>river</code> Hyperparameter Tuning: Hoeffding Tree Regressor with Friedman Drift Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./503_spot_hpt_river_friedman_amfr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">The Friedman Drift Data Set</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with PyTorch Lightning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./600_spot_lightning_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_diabetes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Pytorch Lightning: Hyperparameter Tuning with Diabetes Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./602_spot_lightning_xai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./603_spot_lightning_transformer_introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning Transformer: Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./604_spot_lightning_save_load_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Saving and Loading</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_01_intro_to_notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Introduction to Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_02_git_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Git Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_03_python_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Python Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_04_spot_doc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Documentation of the Sequential Parameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_05_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_99_solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Solutions to Selected Exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#jupyter-notebooks" id="toc-jupyter-notebooks" class="nav-link active" data-scroll-target="#jupyter-notebooks"><span class="header-section-number">17.1</span> Jupyter Notebooks</a></li>
  <li><a href="#videos" id="toc-videos" class="nav-link" data-scroll-target="#videos"><span class="header-section-number">17.2</span> Videos</a>
  <ul class="collapse">
  <li><a href="#june-11th-2024" id="toc-june-11th-2024" class="nav-link" data-scroll-target="#june-11th-2024"><span class="header-section-number">17.2.1</span> June, 11th 2024</a></li>
  <li><a href="#june-18th-2024" id="toc-june-18th-2024" class="nav-link" data-scroll-target="#june-18th-2024"><span class="header-section-number">17.2.2</span> June, 18th 2024</a></li>
  <li><a href="#june-25th-2024" id="toc-june-25th-2024" class="nav-link" data-scroll-target="#june-25th-2024"><span class="header-section-number">17.2.3</span> June, 25th 2024</a></li>
  <li><a href="#cnns" id="toc-cnns" class="nav-link" data-scroll-target="#cnns"><span class="header-section-number">17.2.4</span> CNNs</a></li>
  <li><a href="#rnn" id="toc-rnn" class="nav-link" data-scroll-target="#rnn"><span class="header-section-number">17.2.5</span> RNN</a></li>
  <li><a href="#lstm" id="toc-lstm" class="nav-link" data-scroll-target="#lstm"><span class="header-section-number">17.2.6</span> LSTM</a></li>
  <li><a href="#pytorchlightning" id="toc-pytorchlightning" class="nav-link" data-scroll-target="#pytorchlightning"><span class="header-section-number">17.2.7</span> Pytorch/Lightning</a></li>
  <li><a href="#july-2nd-2024" id="toc-july-2nd-2024" class="nav-link" data-scroll-target="#july-2nd-2024"><span class="header-section-number">17.2.8</span> July, 2nd 2024</a></li>
  <li><a href="#additional-lecture-july-9th-2024" id="toc-additional-lecture-july-9th-2024" class="nav-link" data-scroll-target="#additional-lecture-july-9th-2024"><span class="header-section-number">17.2.9</span> Additional Lecture (July, 9th 2024)?</a></li>
  <li><a href="#additional-videos" id="toc-additional-videos" class="nav-link" data-scroll-target="#additional-videos"><span class="header-section-number">17.2.10</span> Additional Videos</a></li>
  <li><a href="#all-videos-in-a-playlist" id="toc-all-videos-in-a-playlist" class="nav-link" data-scroll-target="#all-videos-in-a-playlist"><span class="header-section-number">17.2.11</span> All Videos in a Playlist</a></li>
  </ul></li>
  <li><a href="#the-statquest-introduction-to-pytorch" id="toc-the-statquest-introduction-to-pytorch" class="nav-link" data-scroll-target="#the-statquest-introduction-to-pytorch"><span class="header-section-number">17.3</span> The StatQuest Introduction to PyTorch</a>
  <ul class="collapse">
  <li><a href="#build-a-simple-neural-network-in-pytorch" id="toc-build-a-simple-neural-network-in-pytorch" class="nav-link" data-scroll-target="#build-a-simple-neural-network-in-pytorch"><span class="header-section-number">17.3.1</span> Build a Simple Neural Network in PyTorch</a></li>
  <li><a href="#use-the-neural-network-and-graph-the-output" id="toc-use-the-neural-network-and-graph-the-output" class="nav-link" data-scroll-target="#use-the-neural-network-and-graph-the-output"><span class="header-section-number">17.3.2</span> Use the Neural Network and Graph the Output</a></li>
  <li><a href="#optimize-train-a-parameter-in-the-neural-network-and-graph-the-output" id="toc-optimize-train-a-parameter-in-the-neural-network-and-graph-the-output" class="nav-link" data-scroll-target="#optimize-train-a-parameter-in-the-neural-network-and-graph-the-output"><span class="header-section-number">17.3.3</span> Optimize (Train) a Parameter in the Neural Network and Graph the Output</a></li>
  </ul></li>
  <li><a href="#build-a-long-short-term-memory-unit-by-hand-using-pytorch-lightning" id="toc-build-a-long-short-term-memory-unit-by-hand-using-pytorch-lightning" class="nav-link" data-scroll-target="#build-a-long-short-term-memory-unit-by-hand-using-pytorch-lightning"><span class="header-section-number">17.4</span> Build a Long Short-Term Memory unit by hand using PyTorch + Lightning</a>
  <ul class="collapse">
  <li><a href="#train-the-lstm-unit-and-use-lightning-and-tensorboard-to-evaluate-part-1---getting-started" id="toc-train-the-lstm-unit-and-use-lightning-and-tensorboard-to-evaluate-part-1---getting-started" class="nav-link" data-scroll-target="#train-the-lstm-unit-and-use-lightning-and-tensorboard-to-evaluate-part-1---getting-started"><span class="header-section-number">17.4.1</span> Train the LSTM unit and use Lightning and TensorBoard to evaluate: Part 1 - Getting Started</a></li>
  <li><a href="#optimizing-training-the-weights-and-biases-in-the-lstm-that-we-made-by-hand-part-2---adding-more-epochs-without-starting-over" id="toc-optimizing-training-the-weights-and-biases-in-the-lstm-that-we-made-by-hand-part-2---adding-more-epochs-without-starting-over" class="nav-link" data-scroll-target="#optimizing-training-the-weights-and-biases-in-the-lstm-that-we-made-by-hand-part-2---adding-more-epochs-without-starting-over"><span class="header-section-number">17.4.2</span> Optimizing (Training) the Weights and Biases in the LSTM that we made by hand: Part 2 - Adding More Epochs without Starting Over</a></li>
  </ul></li>
  <li><a href="#using-and-optimzing-the-pytorch-lstm-nn.lstm" id="toc-using-and-optimzing-the-pytorch-lstm-nn.lstm" class="nav-link" data-scroll-target="#using-and-optimzing-the-pytorch-lstm-nn.lstm"><span class="header-section-number">17.5</span> Using and optimzing the PyTorch LSTM, nn.LSTM()</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./200_mlai.html">Machine Learning and AI</a></li><li class="breadcrumb-item"><a href="./200_mlai.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Machine Learning and Artificial Intelligence</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Machine Learning and Artificial Intelligence</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="jupyter-notebooks" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="jupyter-notebooks"><span class="header-section-number">17.1</span> Jupyter Notebooks</h2>
<ul>
<li>The Jupyter-Notebook version of this file can be found here: <a href="https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/mlai.ipynb">malai.ipynb</a></li>
</ul>
</section>
<section id="videos" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="videos"><span class="header-section-number">17.2</span> Videos</h2>
<section id="june-11th-2024" class="level3" data-number="17.2.1">
<h3 data-number="17.2.1" class="anchored" data-anchor-id="june-11th-2024"><span class="header-section-number">17.2.1</span> June, 11th 2024</h3>
<ul>
<li><a href="https://youtu.be/zxagGtF9MeU?si=4klFloENih3Pw7Ix">Happy Halloween (Neural Networks Are Not Scary)</a></li>
<li><a href="https://youtu.be/CqOfi41LfDw?si=tGfuObKzWonsNLZ1">The Essential Main Ideas of Neural Networks</a></li>
</ul>
</section>
<section id="june-18th-2024" class="level3" data-number="17.2.2">
<h3 data-number="17.2.2" class="anchored" data-anchor-id="june-18th-2024"><span class="header-section-number">17.2.2</span> June, 18th 2024</h3>
<ul>
<li><p><a href="https://youtu.be/wl1myxrtQHQ?si=jcGIAhXkBLUvSqeV">The Chain Rule</a></p></li>
<li><p><a href="https://youtu.be/sDv4f4s2SB8?si=V3XzPVbJNsbZSbNw">Gradient Descent, Step-by-Step</a></p></li>
<li><p><a href="https://youtu.be/IN2XmBhILt4?si=Ldx6rk6mPplQjZZv">Neural Networks Pt. 2: Backpropagation Main Ideas</a></p></li>
</ul>
<section id="gradient-descent" class="level4" data-number="17.2.2.1">
<h4 data-number="17.2.2.1" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">17.2.2.1</span> Gradient Descent</h4>
<div id="exr-GradDesc1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.1 (GradDescStepSize)</strong></span> How is the step size calculated?</p>
</div>
<div id="sol-GradDesc1" class="proof solution">
<p><span class="proof-title"><em>Solution 17.1</em> (GradDescStepSize). </span>learning rate x slope</p>
</div>
<div id="exr-GradDesc2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.2 (GradDescIntercept)</strong></span> How to calculate the new intercept?</p>
</div>
<div id="sol-GradDesc2" class="proof solution">
<p><span class="proof-title"><em>Solution 17.2</em> (GradDescIntercept). </span>Old intercept - step size</p>
</div>
<div id="exr-GradDesc3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.3 (GradDescIntercept)</strong></span> When does the gradient descend stop?</p>
</div>
<div id="sol-GradDesc3" class="proof solution">
<p><span class="proof-title"><em>Solution 17.3</em> (GradDescIntercept). </span>When the step size is small or after a certain number of steps</p>
</div>
</section>
<section id="backpropagation" class="level4" data-number="17.2.2.2">
<h4 data-number="17.2.2.2" class="anchored" data-anchor-id="backpropagation"><span class="header-section-number">17.2.2.2</span> Backpropagation</h4>
<div id="exr-BacPro1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.4 (ChainRuleAndGradientDescent)</strong></span> What are the key components involved in backpropagation?</p>
</div>
<div id="exr-BacPro2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.5 (BackpropagationNaming)</strong></span> Why is it called backpropagation?</p>
</div>
<ul>
<li><p><a href="https://youtu.be/iyn2zdALii8?si=yBsgPec1R1O55f9q">Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.</a></p></li>
<li><p><a href="https://youtu.be/GKZoOHXGcLo?si=Ypv_EDEMMC--8Flj">Backpropagation Details Pt. 2: Going bonkers with The Chain Rule</a></p></li>
<li><p><a href="https://youtu.be/68BZ5f7P94E?si=3hPUkdicWLwFzOGZ">Neural Networks Pt. 3: ReLU In Action!!!</a></p></li>
<li><p><a href="https://youtu.be/83LYR-1IcjA?si=kePw0yRCj-A6MsOH">Neural Networks Pt. 4: Multiple Inputs and Outputs</a></p></li>
<li><p><a href="https://youtu.be/KpKog-L9veg?si=gqXLSbOxwJwYs0hu">Neural Networks Part 5: ArgMax and SoftMax</a></p></li>
<li><p><a href="https://youtu.be/L35fFDpwIM4?si=Q-oglIUJb8wVO9nd">Tensors for Neural Networks, Clearly Explained!!!</a></p></li>
<li><p><a href="https://youtu.be/ZTt9gsGcdDo?si=sKDLZ8nbj4vVi9aj">Essential Matrix Algebra for Neural Networks, Clearly Explained!!!</a></p></li>
<li><p><a href="https://youtu.be/FHdlXe1bSe4?si=Yh5gfWsnjDd2WqxN">The StatQuest Introduction to PyTorch</a></p></li>
</ul>
</section>
<section id="pytorch-links" class="level4" data-number="17.2.2.3">
<h4 data-number="17.2.2.3" class="anchored" data-anchor-id="pytorch-links"><span class="header-section-number">17.2.2.3</span> PyTorch Links</h4>
<ul>
<li><a href="https://lightning.ai/lightning-ai/studios/statquest-introduction-to-coding-neural-networks-with-pytorch?view=public&amp;section=all">StatQuest: Introduction to Coding Neural Networks with PyTorch</a></li>
<li><a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html">ML-AI Pytorch Introduction</a></li>
</ul>
</section>
</section>
<section id="june-25th-2024" class="level3" data-number="17.2.3">
<h3 data-number="17.2.3" class="anchored" data-anchor-id="june-25th-2024"><span class="header-section-number">17.2.3</span> June, 25th 2024</h3>
</section>
<section id="cnns" class="level3" data-number="17.2.4">
<h3 data-number="17.2.4" class="anchored" data-anchor-id="cnns"><span class="header-section-number">17.2.4</span> CNNs</h3>
<section id="neural-networks-part-8-image-classification-with-convolutional-neural-networks-cnns" class="level4" data-number="17.2.4.1">
<h4 data-number="17.2.4.1" class="anchored" data-anchor-id="neural-networks-part-8-image-classification-with-convolutional-neural-networks-cnns"><span class="header-section-number">17.2.4.1</span> <a href="https://youtu.be/HGwBXDKFk9I?si=3yBfpZQ0dXw7s6j9">Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)</a></h4>
<div id="exr-CNN1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.6 (CNNImageRecognition)</strong></span> Why are classical neural networks poor at image recognition?</p>
</div>
<div id="exr-CNN2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.7 (CNNFiltersInitialization)</strong></span> How are the filter values in CNNs initialized and optimized?</p>
</div>
<div id="exr-CNN3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.8 (CNNFilterInitialization)</strong></span> How are the filter values determined in Convolutional Neural Networks (CNNs)?</p>
</div>
<div id="exr-CNN4" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.9 (GenNNStockPrediction)</strong></span> What is a limitation of using classical neural networks for stock market prediction?</p>
</div>
</section>
</section>
<section id="rnn" class="level3" data-number="17.2.5">
<h3 data-number="17.2.5" class="anchored" data-anchor-id="rnn"><span class="header-section-number">17.2.5</span> RNN</h3>
<section id="recurrent-neural-networks-rnns-clearly-explained" class="level4" data-number="17.2.5.1">
<h4 data-number="17.2.5.1" class="anchored" data-anchor-id="recurrent-neural-networks-rnns-clearly-explained"><span class="header-section-number">17.2.5.1</span> <a href="https://youtu.be/AsNTP8Kwu80?si=-JiRYXyOpu-gnhnk">Recurrent Neural Networks (RNNs), Clearly Explained!!!</a></h4>
<div id="exr-RNN1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.10 (RNNUnrolling)</strong></span> How does the unrolling process work in Recurrent Neural Networks (RNNs)?</p>
</div>
<div id="exr-RNN2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.11 (RNNReliability)</strong></span> Why do Recurrent Neural Networks (RNNs) sometimes fail to work reliably?</p>
</div>
</section>
</section>
<section id="lstm" class="level3" data-number="17.2.6">
<h3 data-number="17.2.6" class="anchored" data-anchor-id="lstm"><span class="header-section-number">17.2.6</span> LSTM</h3>
<section id="long-short-term-memory-lstm-clearly-explained" class="level4" data-number="17.2.6.1">
<h4 data-number="17.2.6.1" class="anchored" data-anchor-id="long-short-term-memory-lstm-clearly-explained"><span class="header-section-number">17.2.6.1</span> <a href="https://youtu.be/YCzL96nL7j0?si=DphYdoYgx23Twgz6">Long Short-Term Memory (LSTM), Clearly Explained</a></h4>
<div id="exr-LSTM1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.12 (LSTMSigmoidTanh)</strong></span> What are the differences between the sigmoid and tanh activation functions?</p>
</div>
<div id="exr-LSTM11" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.13 (LSTMSigmoidTanh)</strong></span> What is the ?</p>
</div>
<div id="exr-LSTM2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.14 (LSTMGates)</strong></span> What are the gates in an LSTM network and their functions?</p>
</div>
<div id="exr-LSTM3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.15 (LSTMLongTermInfo)</strong></span> In which gate is long-term information used in an LSTM network?</p>
</div>
<div id="exr-LSTM4" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.16 (LSTMUpdateGates)</strong></span> In which Gates is it updated in an LSTM?</p>
</div>
</section>
</section>
<section id="pytorchlightning" class="level3" data-number="17.2.7">
<h3 data-number="17.2.7" class="anchored" data-anchor-id="pytorchlightning"><span class="header-section-number">17.2.7</span> Pytorch/Lightning</h3>
<section id="introduction-to-coding-neural-networks-with-pytorch-and-lightning" class="level4" data-number="17.2.7.1">
<h4 data-number="17.2.7.1" class="anchored" data-anchor-id="introduction-to-coding-neural-networks-with-pytorch-and-lightning"><span class="header-section-number">17.2.7.1</span> <a href="https://youtu.be/khMzi6xPbuM?si=6aqbmYIIaefKQnWX">Introduction to Coding Neural Networks with PyTorch and Lightning</a></h4>
<div id="exr-PyTorch1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 17.17 (PyTorchRequiresGrad)</strong></span> What does <code>requires_grad</code> mean in PyTorch?</p>
</div>
</section>
</section>
<section id="july-2nd-2024" class="level3" data-number="17.2.8">
<h3 data-number="17.2.8" class="anchored" data-anchor-id="july-2nd-2024"><span class="header-section-number">17.2.8</span> July, 2nd 2024</h3>
<ul>
<li><a href="https://youtu.be/viZrOnJclY0?si=B0gvlx4_ppegZAB-">Word Embedding and Word2Vec, Clearly Explained!!!</a></li>
<li><a href="https://youtu.be/L8HKweZIOmg?si=LzC6wjlC2yE9ZekP">Sequence-to-Sequence (seq2seq) Encoder-Decoder Neural Networks, Clearly Explained!!!</a></li>
<li><a href="https://youtu.be/PSs6nxngL6k?si=jajDsVYk4FQgCgNA">Attention for Neural Networks, Clearly Explained!!!</a></li>
</ul>
</section>
<section id="additional-lecture-july-9th-2024" class="level3" data-number="17.2.9">
<h3 data-number="17.2.9" class="anchored" data-anchor-id="additional-lecture-july-9th-2024"><span class="header-section-number">17.2.9</span> Additional Lecture (July, 9th 2024)?</h3>
<ul>
<li><a href="https://youtu.be/zxQyTK8quyY?si=LGe6J13PJ4s0qKbr">Transformer Neural Networks, ChatGPT’s foundation, Clearly Explained!!!</a></li>
<li><a href="https://youtu.be/bQ5BoolX9Ag?si=cojnYPck8CK6NK8p">Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!</a></li>
<li><a href="https://youtu.be/KphmOJnLAdI?si=JwIK3MhmoHxnuI3G">The matrix math behind transformer neural networks, one step at a time!!!</a></li>
<li><a href="https://youtu.be/Qf06XDYXCXI?si=gIKMOQ0xjAxLo_7_">Word Embedding in PyTorch + Lightning</a></li>
</ul>
</section>
<section id="additional-videos" class="level3" data-number="17.2.10">
<h3 data-number="17.2.10" class="anchored" data-anchor-id="additional-videos"><span class="header-section-number">17.2.10</span> Additional Videos</h3>
<ul>
<li><a href="https://youtu.be/M59JElEPgIg?si=KoZGFEZWVc-PclSU">The SoftMax Derivative, Step-by-Step!!!</a></li>
<li><a href="https://youtu.be/6ArSys5qHAU?si=TxyJi22ELyYl0m3L">Neural Networks Part 6: Cross Entropy</a></li>
<li><a href="https://youtu.be/xBEh66V9gZo?si=kUco4zKdH8CNW23k">Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation</a></li>
</ul>
</section>
<section id="all-videos-in-a-playlist" class="level3" data-number="17.2.11">
<h3 data-number="17.2.11" class="anchored" data-anchor-id="all-videos-in-a-playlist"><span class="header-section-number">17.2.11</span> All Videos in a Playlist</h3>
<ul>
<li>Full Playlist <a href="https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1">ML-AI</a></li>
</ul>
</section>
</section>
<section id="the-statquest-introduction-to-pytorch" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="the-statquest-introduction-to-pytorch"><span class="header-section-number">17.3</span> The StatQuest Introduction to PyTorch</h2>
<p>The following code is taken from <a href="https://lightning.ai/lightning-ai/studios/statquest-introduction-to-coding-neural-networks-with-pytorch?view=public&amp;section=all&amp;tab=files&amp;layout=column&amp;path=cloudspaces%2F01hf54c4fhjc8wwadsd037kjjm&amp;y=3&amp;x=0">The StatQuest Introduction to PyTorch</a>. Attribution goes to Josh Starmer, the creator of StatQuest, see <a href="https://lightning.ai/josh-starmer">Josh Starmer</a>.</p>
<div id="d674e0de" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch <span class="co"># torch provides basic functions, from setting a random seed (for reproducability) to creating tensors.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn <span class="co"># torch.nn allows us to create a neural network.</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F <span class="co"># nn.functional give us access to the activation and loss functions.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> SGD <span class="co"># optim contains many optimizers. Here, we're using SGD, stochastic gradient descent.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co">## matplotlib allows us to draw graphs.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns <span class="co">## seaborn makes it easier to draw nice-looking graphs.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Building a neural network in PyTorch means creating a new class with two methods: init() and forward(). The init() method defines and initializes all of the parameters that we want to use, and the forward() method tells PyTorch what should happen during a forward pass through the neural network.</p>
<section id="build-a-simple-neural-network-in-pytorch" class="level3" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="build-a-simple-neural-network-in-pytorch"><span class="header-section-number">17.3.1</span> Build a Simple Neural Network in PyTorch</h3>
<p><code>__init__()</code> is the class constructor function, and we use it to initialize the weights and biases.</p>
<div id="ea524947" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">## create a neural network class by creating a class that inherits from nn.Module.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicNN(nn.Module):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>): <span class="co"># __init__() is the class constructor function, and we use it to initialize the weights and biases.</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() <span class="co"># initialize an instance of the parent class, nn.Model.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Now create the weights and biases that we need for our neural network.</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Each weight or bias is an nn.Parameter, which gives us the option to optimize the parameter by setting</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">## requires_grad, which is short for "requires gradient", to True. Since we don't need to optimize any of these</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">## parameters now, we set requires_grad=False.</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">##</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">## </span><span class="al">NOTE</span><span class="co">: Because our neural network is already fit to the data, we will input specific values</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">## for each weight and bias. In contrast, if we had not already fit the neural network to the data,</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">## we might start with a random initalization of the weights and biases.</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w00 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">1.7</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b00 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="op">-</span><span class="fl">0.85</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w01 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="op">-</span><span class="fl">40.8</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w10 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">12.6</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b10 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">0.0</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w11 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">2.7</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.final_bias <span class="op">=</span> nn.Parameter(torch.tensor(<span class="op">-</span><span class="fl">16.</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>): <span class="co">## forward() takes an input value and runs it though the neural network </span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                              <span class="co">## illustrated at the top of this notebook. </span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">## the next three lines implement the top of the neural network (using the top node in the hidden layer).</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        input_to_top_relu <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> <span class="va">self</span>.w00 <span class="op">+</span> <span class="va">self</span>.b00</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        top_relu_output <span class="op">=</span> F.relu(input_to_top_relu)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        scaled_top_relu_output <span class="op">=</span> top_relu_output <span class="op">*</span> <span class="va">self</span>.w01</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">## the next three lines implement the bottom of the neural network (using the bottom node in the hidden layer).</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        input_to_bottom_relu <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> <span class="va">self</span>.w10 <span class="op">+</span> <span class="va">self</span>.b10</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        bottom_relu_output <span class="op">=</span> F.relu(input_to_bottom_relu)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        scaled_bottom_relu_output <span class="op">=</span> bottom_relu_output <span class="op">*</span> <span class="va">self</span>.w11</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">## here, we combine both the top and bottom nodes from the hidden layer with the final bias.</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        input_to_final_relu <span class="op">=</span> scaled_top_relu_output <span class="op">+</span> scaled_bottom_relu_output <span class="op">+</span> <span class="va">self</span>.final_bias</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.relu(input_to_final_relu)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output <span class="co"># output is the predicted effectiveness for a drug dose.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we have created the class that defines the neural network, we can create an actual neural network and print out its parameters, just to make sure things are what we expect.</p>
<div id="504971e6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">## create the neural network. </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BasicNN()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">## print out the name and value for each parameter</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, param.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>w00 tensor(1.7000)
b00 tensor(-0.8500)
w01 tensor(-40.8000)
w10 tensor(12.6000)
b10 tensor(0.)
w11 tensor(2.7000)
final_bias tensor(-16.)</code></pre>
</div>
</div>
</section>
<section id="use-the-neural-network-and-graph-the-output" class="level3" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="use-the-neural-network-and-graph-the-output"><span class="header-section-number">17.3.2</span> Use the Neural Network and Graph the Output</h3>
<p>Now that we have a neural network, we can use it on a variety of doses to determine which will be effective. Then we can make a graph of these data, and this graph should match the green bent shape fit to the training data that’s shown at the top of this document. So, let’s start by making a sequence of input doses…</p>
<div id="6057f0e0" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">## now create the different doses we want to run through the neural network.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">## torch.linspace() creates the sequence of numbers between, and including, 0 and 1.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>input_doses <span class="op">=</span> torch.linspace(start<span class="op">=</span><span class="dv">0</span>, end<span class="op">=</span><span class="dv">1</span>, steps<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># now print out the doses to make sure they are what we expect...</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>input_doses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,
        0.9000, 1.0000])</code></pre>
</div>
</div>
<p>Now that we have input_doses, let’s run them through the neural network and graph the output…</p>
<div id="2a7a4926" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">## create the neural network. </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BasicNN() </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">## now run the different doses through the neural network.</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>output_values <span class="op">=</span> model(input_doses)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">## Now draw a graph that shows the effectiveness for each dose.</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">## First, set the style for seaborn so that the graph looks cool.</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">## create the graph (you might not see it at this point, but you will after we save it as a PDF).</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>input_doses, </span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>     y<span class="op">=</span>output_values, </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>     color<span class="op">=</span><span class="st">'green'</span>, </span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>     linewidth<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">## now label the y- and x-axes.</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Effectiveness'</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dose'</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">## optionally, save the graph as a PDF.</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.savefig('BasicNN.pdf')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>Text(0.5, 0, 'Dose')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="200_mlai_files/figure-html/cell-6-output-2.png" width="593" height="435" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The graph shows that the neural network fits the training data. In other words, so far, we don’t have any bugs in our code.</p>
</section>
<section id="optimize-train-a-parameter-in-the-neural-network-and-graph-the-output" class="level3" data-number="17.3.3">
<h3 data-number="17.3.3" class="anchored" data-anchor-id="optimize-train-a-parameter-in-the-neural-network-and-graph-the-output"><span class="header-section-number">17.3.3</span> Optimize (Train) a Parameter in the Neural Network and Graph the Output</h3>
<p>Now that we know how to create and use a simple neural network, and we can graph the output relative to the input, let’s see how to train a neural network. The first thing we need to do is tell PyTorch which parameter (or parameters) we want to train, and we do that by setting requiresgrad=True. In this example, we’ll train finalbias.</p>
<p>Now we create a neural network by creating a class that inherits from nn.Module.</p>
<p>NOTE: This code is the same as before, except we changed the class name to BasicNN_train and we modified final_bias in two ways:</p>
<pre><code>1) we set the value of the tensor to 0, and
2) we set "requires_grad=True".</code></pre>
<p>Now let’s graph the output of BasicNN_train, which is currently not optimized, and compare it to the graph we drew earlier of the optimized neural network.</p>
<div id="4d655fd7" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicNN_train(nn.Module):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>): <span class="co"># __init__ is the class constructor function, and we use it to initialize the weights and biases.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() <span class="co"># initialize an instance of the parent class, nn.Module.</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w00 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">1.7</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b00 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="op">-</span><span class="fl">0.85</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w01 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="op">-</span><span class="fl">40.8</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w10 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">12.6</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b10 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">0.0</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w11 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">2.7</span>), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">## we want to modify final_bias to demonstrate how to optimize it with backpropagation.</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">## The optimal value for final_bias is -16...</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.final_bias = nn.Parameter(torch.tensor(-16.), requires_grad=False)</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">## ...so we set it to 0 and tell Pytorch that it now needs to calculate the gradient for this parameter.</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.final_bias <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">0.</span>), requires_grad<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        input_to_top_relu <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> <span class="va">self</span>.w00 <span class="op">+</span> <span class="va">self</span>.b00</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        top_relu_output <span class="op">=</span> F.relu(input_to_top_relu)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        scaled_top_relu_output <span class="op">=</span> top_relu_output <span class="op">*</span> <span class="va">self</span>.w01</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        input_to_bottom_relu <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> <span class="va">self</span>.w10 <span class="op">+</span> <span class="va">self</span>.b10</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        bottom_relu_output <span class="op">=</span> F.relu(input_to_bottom_relu)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        scaled_bottom_relu_output <span class="op">=</span> bottom_relu_output <span class="op">*</span> <span class="va">self</span>.w11</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        input_to_final_relu <span class="op">=</span> scaled_top_relu_output <span class="op">+</span> scaled_bottom_relu_output <span class="op">+</span> <span class="va">self</span>.final_bias</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.relu(input_to_final_relu)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="3978bb3d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">## create the neural network. </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BasicNN_train() </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">## now run the different doses through the neural network.</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>output_values <span class="op">=</span> model(input_doses)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">## Now draw a graph that shows the effectiveness for each dose.</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">## set the style for seaborn so that the graph looks cool.</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">## create the graph (you might not see it at this point, but you will after we save it as a PDF).</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>input_doses, </span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>             y<span class="op">=</span>output_values.detach(), <span class="co">## </span><span class="al">NOTE</span><span class="co">: because final_bias has a gradident, we call detach() </span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>                                       <span class="co">## to return a new tensor that only has the value and not the gradient.</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>             color<span class="op">=</span><span class="st">'green'</span>, </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>             linewidth<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co">## now label the y- and x-axes.</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Effectiveness'</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dose'</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co">## lastly, save the graph as a PDF.</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.savefig('BasicNN_train.pdf')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Text(0.5, 0, 'Dose')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="200_mlai_files/figure-html/cell-8-output-2.png" width="601" height="435" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The graph shows that when the dose is 0.5, the output from the unoptimized neural network is 17, which is wrong, since the output value should be 1. So, now that we have a parameter we can optimize, let’s create some training data that we can use to optimize it.</p>
<div id="4a978393" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">## create the training data for the neural network.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.tensor([<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">1.</span>])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> torch.tensor([<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="fl">0.</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>..and now let’s use that training data to train (or optimize) final_bias.</p>
<div id="117bae3f" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">## create the neural network we want to train.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BasicNN_train()</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>) <span class="co">## here we're creating an optimizer to train the neural network.</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>                                            <span class="co">## </span><span class="al">NOTE</span><span class="co">: There are a bunch of different ways to optimize a neural network.</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>                                            <span class="co">## In this example, we'll use Stochastic Gradient Descent (SGD). However,</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                                            <span class="co">## another popular algortihm is Adam (which will be covered in a StatQuest).</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final bias, before optimization: "</span> <span class="op">+</span> <span class="bu">str</span>(model.final_bias.data) <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">## this is the optimization loop. Each time the optimizer sees all of the training data is called an "epoch".</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">## we create and initialize total_loss for each epoch so that we can evaluate how well model fits the</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">## training data. At first, when the model doesn't fit the training data very well, total_loss</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">## will be large. However, as gradient descent improves the fit, total_loss will get smaller and smaller.</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">## If total_loss gets really small, we can decide that the model fits the data well enough and stop</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">## optimizing the fit. Otherwise, we can just keep optimizing until we reach the maximum number of epochs. </span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">## this internal loop is where the optimizer sees all of the training data and where we </span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">## calculate the total_loss for all of the training data.</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(inputs)):</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        input_i <span class="op">=</span> inputs[iteration] <span class="co">## extract a single input value (a single dose)...</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        label_i <span class="op">=</span> labels[iteration] <span class="co">## ...and its corresponding label (the effectiveness for the dose).</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        output_i <span class="op">=</span> model(input_i) <span class="co">## calculate the neural network output for the input (the single dose).</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (output_i <span class="op">-</span> label_i)<span class="op">**</span><span class="dv">2</span> <span class="co">## calculate the loss for the single value.</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>                                       <span class="co">## </span><span class="al">NOTE</span><span class="co">: Because output_i = model(input_i), "loss" has a connection to "model"</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>                                       <span class="co">## and the derivative (calculated in the next step) is kept and accumulated</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>                                       <span class="co">## in "model".</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        loss.backward() <span class="co"># backward() calculates the derivative for that single value and adds it to the previous one.</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> <span class="bu">float</span>(loss) <span class="co"># accumulate the total loss for this epoch.</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (total_loss <span class="op">&lt;</span> <span class="fl">0.0001</span>):</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Num steps: "</span> <span class="op">+</span> <span class="bu">str</span>(epoch))</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    optimizer.step() <span class="co">## take a step toward the optimal value.</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad() <span class="co">## This zeroes out the gradient stored in "model". </span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>                          <span class="co">## Remember, by default, gradients are added to the previous step (the gradients are accumulated),</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>                          <span class="co">## and we took advantage of this process to calculate the derivative one data point at a time.</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>                          <span class="co">## </span><span class="al">NOTE</span><span class="co">: "optimizer" has access to "model" because of how it was created with the call </span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>                          <span class="co">## (made earlier): optimizer = SGD(model.parameters(), lr=0.1).</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>                          <span class="co">## ALSO </span><span class="al">NOTE</span><span class="co">: Alternatively, we can zero out the gradient with model.zero_grad().</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Step: "</span> <span class="op">+</span> <span class="bu">str</span>(epoch) <span class="op">+</span> <span class="st">" Final Bias: "</span> <span class="op">+</span> <span class="bu">str</span>(model.final_bias.data) <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>    <span class="co">## now go back to the start of the loop and go through another epoch.</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total loss: "</span> <span class="op">+</span> <span class="bu">str</span>(total_loss))</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Final bias, after optimization: "</span> <span class="op">+</span> <span class="bu">str</span>(model.final_bias.data))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Final bias, before optimization: tensor(0.)

Step: 0 Final Bias: tensor(-3.2020)

Step: 10 Final Bias: tensor(-14.6348)

Step: 20 Final Bias: tensor(-15.8623)

Step: 30 Final Bias: tensor(-15.9941)

Num steps: 34
Total loss: 6.58966600894928e-05
Final bias, after optimization: tensor(-16.0019)</code></pre>
</div>
</div>
<p>So, if everything worked correctly, the optimizer should have converged on final_bias = 16.0019 after 34 steps, or epochs. BAM!</p>
<p>Lastly, let’s graph the output from the optimized neural network and see if it’s the same as what we started with. If so, then the optimization worked.</p>
<div id="765c6e28" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">## run the different doses through the neural network</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>output_values <span class="op">=</span> model(input_doses)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">## set the style for seaborn so that the graph looks cool.</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">"whitegrid"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">## create the graph (you might not see it at this point, but you will after we save it as a PDF).</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>input_doses, </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>     y<span class="op">=</span>output_values.detach(), <span class="co">## </span><span class="al">NOTE</span><span class="co">: we call detach() because final_bias has a gradient</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>     color<span class="op">=</span><span class="st">'green'</span>, </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>     linewidth<span class="op">=</span><span class="fl">2.5</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">## now label the y- and x-axes.</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Effectiveness'</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dose'</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">## lastly, save the graph as a PDF.</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.savefig('BascNN_optimized.pdf')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>Text(0.5, 0, 'Dose')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="200_mlai_files/figure-html/cell-11-output-2.png" width="593" height="435" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>And we see that the optimized model results in the same graph that we started with, so the optimization worked as expected.</p>
</section>
</section>
<section id="build-a-long-short-term-memory-unit-by-hand-using-pytorch-lightning" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="build-a-long-short-term-memory-unit-by-hand-using-pytorch-lightning"><span class="header-section-number">17.4</span> Build a Long Short-Term Memory unit by hand using PyTorch + Lightning</h2>
<p>The following code is based on <a href="https://www.youtube.com/watch?v=RHGiXPuo_pI&amp;t=23s">Long Short-Term Memory with PyTorch + Lightning</a> and <a href="https://lightning.ai/lightning-ai/studios/statquest-long-short-term-memory-lstm-with-pytorch-lightning?view=public&amp;section=all&amp;tab=files&amp;layout=column&amp;path=cloudspaces%2F01henpavmdtqndyk17xpzjdbj6&amp;y=3&amp;x=0">StatQuest: Long Short-Term Memory (LSTM) with PyTorch + Lightning!!!</a>. Attribution goes to Josh Starmer, the creator of StatQuest, see <a href="https://lightning.ai/josh-starmer">Josh Starmer</a>.</p>
<div id="b4425143" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch <span class="co"># torch will allow us to create tensors.</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn <span class="co"># torch.nn allows us to create a neural network.</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F <span class="co"># nn.functional give us access to the activation and loss functions.</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam <span class="co"># optim contains many optimizers. This time we're using Adam</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightning <span class="im">as</span> L <span class="co"># lightning has tons of cool tools that make neural networks easier</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, DataLoader <span class="co"># these are needed for the training data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A Long Short-Term Memory (LSTM) unit is a type of neural network, and that means we need to create a new class. To make it easy to train the LSTM, this class will inherit from LightningModule and we’ll create the following methods:</p>
<ul>
<li><code>init()</code> to initialize the Weights and Biases and keep track of a few other house keeping things.</li>
<li><code>lstm_unit()</code> to do the LSTM math. For example, to calculate the percentage of the long-term memory to remember.</li>
<li><code>forward()</code> to make a forward pass through the unrolled LSTM. In other words forward() calls <code>lstm_unit()</code> for each data point.</li>
<li><code>configure_optimizers()</code> to configure the opimimizer. In the past, we have use SGD (Stochastic Gradient Descent), however, in this tutorial we’ll change things up and use Adam, another popular algorithm for optimizing the Weights and Biases.</li>
<li><code>training_step()</code> to pass the training data to forward(), calculate the loss and to keep track of the loss values in a log file.</li>
</ul>
<div id="08e936d7" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMbyHand(L.LightningModule):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        L.seed_everything(seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">## </span><span class="al">NOTE</span><span class="co">: nn.LSTM() uses random values from a uniform distribution to initialize the tensors</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Here we can do it 2 different ways 1) Normal Distribution and 2) Uniform Distribution</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">## We'll start with the Normal distribution.</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> torch.tensor(<span class="fl">1.0</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">## </span><span class="al">NOTE</span><span class="co">: In this case, I'm only using the normal distribution for the Weights.</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">## All Biases are initialized to 0.</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">##</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">## These are the Weights and Biases in the first stage, which determines what percentage</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">## of the long-term memory the LSTM unit will remember.</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wlr1 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wlr2 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blr1 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">0.</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">## These are the Weights and Biases in the second stage, which determines the new</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">## potential long-term memory and what percentage will be remembered.</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wpr1 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wpr2 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bpr1 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">0.</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wp1 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wp2 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bp1 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">0.</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">## These are the Weights and Biases in the third stage, which determines the</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">## new short-term memory and what percentage will be sent to the output.</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wo1 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wo2 <span class="op">=</span> nn.Parameter(torch.normal(mean<span class="op">=</span>mean, std<span class="op">=</span>std), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bo1 <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">0.</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">## We can also initialize all Weights and Biases using a uniform distribution. This is</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">## how nn.LSTM() does it.</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wlr1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wlr2 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.blr1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wpr1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wpr2 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.bpr1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wp1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wp2 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.bp1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wo1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.wo2 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.bo1 = nn.Parameter(torch.rand(1), requires_grad=True)</span></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> lstm_unit(<span class="va">self</span>, input_value, long_memory, short_memory):</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">## lstm_unit does the math for a single LSTM unit.</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>        <span class="co">## NOTES:</span></span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">## long term memory is also called "cell state"</span></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">## short term memory is also called "hidden state"</span></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>        <span class="co">## 1) The first stage determines what percent of the current long-term memory</span></span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>        <span class="co">##    should be remembered</span></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>        long_remember_percent <span class="op">=</span> torch.sigmoid((short_memory <span class="op">*</span> <span class="va">self</span>.wlr1) <span class="op">+</span></span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>                                              (input_value <span class="op">*</span> <span class="va">self</span>.wlr2) <span class="op">+</span></span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>                                              <span class="va">self</span>.blr1)</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a>        <span class="co">## 2) The second stage creates a new, potential long-term memory and determines what</span></span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>        <span class="co">##    percentage of that to add to the current long-term memory</span></span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>        potential_remember_percent <span class="op">=</span> torch.sigmoid((short_memory <span class="op">*</span> <span class="va">self</span>.wpr1) <span class="op">+</span></span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>                                                   (input_value <span class="op">*</span> <span class="va">self</span>.wpr2) <span class="op">+</span></span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a>                                                   <span class="va">self</span>.bpr1)</span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a>        potential_memory <span class="op">=</span> torch.tanh((short_memory <span class="op">*</span> <span class="va">self</span>.wp1) <span class="op">+</span></span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a>                                      (input_value <span class="op">*</span> <span class="va">self</span>.wp2) <span class="op">+</span></span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a>                                      <span class="va">self</span>.bp1)</span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Once we have gone through the first two stages, we can update the long-term memory</span></span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>        updated_long_memory <span class="op">=</span> ((long_memory <span class="op">*</span> long_remember_percent) <span class="op">+</span></span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a>                       (potential_remember_percent <span class="op">*</span> potential_memory))</span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a>        <span class="co">## 3) The third stage creates a new, potential short-term memory and determines what</span></span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">##    percentage of that should be remembered and used as output.</span></span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a>        output_percent <span class="op">=</span> torch.sigmoid((short_memory <span class="op">*</span> <span class="va">self</span>.wo1) <span class="op">+</span></span>
<span id="cb19-86"><a href="#cb19-86" aria-hidden="true" tabindex="-1"></a>                                       (input_value <span class="op">*</span> <span class="va">self</span>.wo2) <span class="op">+</span></span>
<span id="cb19-87"><a href="#cb19-87" aria-hidden="true" tabindex="-1"></a>                                       <span class="va">self</span>.bo1)</span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a>        updated_short_memory <span class="op">=</span> torch.tanh(updated_long_memory) <span class="op">*</span> output_percent</span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Finally, we return the updated long and short-term memories</span></span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>([updated_long_memory, updated_short_memory])</span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a>        <span class="co">## forward() unrolls the LSTM for the training data by calling lstm_unit() for each day of training data</span></span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a>        <span class="co">## that we have. forward() also keeps track of the long and short-term memories after each day and returns</span></span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a>        <span class="co">## the final short-term memory, which is the 'output' of the LSTM.</span></span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a>        long_memory <span class="op">=</span> <span class="dv">0</span> <span class="co"># long term memory is also called "cell state" and indexed with c0, c1, ..., cN</span></span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a>        short_memory <span class="op">=</span> <span class="dv">0</span> <span class="co"># short term memory is also called "hidden state" and indexed with h0, h1, ..., cN</span></span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a>        day1 <span class="op">=</span> <span class="bu">input</span>[<span class="dv">0</span>]</span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a>        day2 <span class="op">=</span> <span class="bu">input</span>[<span class="dv">1</span>]</span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a>        day3 <span class="op">=</span> <span class="bu">input</span>[<span class="dv">2</span>]</span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a>        day4 <span class="op">=</span> <span class="bu">input</span>[<span class="dv">3</span>]</span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Day 1</span></span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a>        long_memory, short_memory <span class="op">=</span> <span class="va">self</span>.lstm_unit(day1, long_memory, short_memory)</span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Day 2</span></span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a>        long_memory, short_memory <span class="op">=</span> <span class="va">self</span>.lstm_unit(day2, long_memory, short_memory)</span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Day 3</span></span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a>        long_memory, short_memory <span class="op">=</span> <span class="va">self</span>.lstm_unit(day3, long_memory, short_memory)</span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Day 4</span></span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a>        long_memory, short_memory <span class="op">=</span> <span class="va">self</span>.lstm_unit(day4, long_memory, short_memory)</span>
<span id="cb19-116"><a href="#cb19-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-117"><a href="#cb19-117" aria-hidden="true" tabindex="-1"></a>        <span class="co">##### Now return short_memory, which is the 'output' of the LSTM.</span></span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> short_memory</span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>): <span class="co"># this configures the optimizer we want to use for backpropagation.</span></span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return Adam(self.parameters(), lr=0.1) # </span><span class="al">NOTE</span><span class="co">: Setting the learning rate to 0.1 trains way faster than</span></span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a>                                                 <span class="co"># using the default learning rate, lr=0.001, which requires a lot more</span></span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a>                                                 <span class="co"># training. However, if we use the default value, we get</span></span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a>                                                 <span class="co"># the exact same Weights and Biases that I used in</span></span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a>                                                 <span class="co"># the LSTM Clearly Explained StatQuest video. So we'll use the</span></span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a>                                                 <span class="co"># default value.</span></span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Adam(<span class="va">self</span>.parameters())</span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx): <span class="co"># take a step during gradient descent.</span></span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a>        input_i, label_i <span class="op">=</span> batch <span class="co"># collect input</span></span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a>        output_i <span class="op">=</span> <span class="va">self</span>.forward(input_i[<span class="dv">0</span>]) <span class="co"># run input through the neural network</span></span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (output_i <span class="op">-</span> label_i)<span class="op">**</span><span class="dv">2</span> <span class="co">## loss = sum of squared residual</span></span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Logging the loss and the predicted values so we can evaluate the training:</span></span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log(<span class="st">"train_loss"</span>, loss)</span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a>        <span class="co">## </span><span class="al">NOTE</span><span class="co">: Our dataset consists of two sequences of values representing Company A and Company B</span></span>
<span id="cb19-137"><a href="#cb19-137" aria-hidden="true" tabindex="-1"></a>        <span class="co">## For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,</span></span>
<span id="cb19-138"><a href="#cb19-138" aria-hidden="true" tabindex="-1"></a>        <span class="co">## the goal is to predict that the value on Day 5 = 1. We use label_i, the value we want to</span></span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a>        <span class="co">## predict, to keep track of which company we just made a prediction for and</span></span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a>        <span class="co">## log that output value in a company specific file</span></span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (label_i <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.log(<span class="st">"out_0"</span>, output_i)</span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.log(<span class="st">"out_1"</span>, output_i)</span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we have created the class that defines an LSTM, we can use it to create a model and print out the randomly initialized Weights and Biases. Then, just for fun, we’ll see what those random Weights and Biases predict for Company A and Company B. If they are good predictions, then we’re done! However, the chances of getting good predictions from random values is very small.</p>
<div id="1b76ae25" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Create the model object, print out parameters and see how well</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co">## the untrained LSTM can make predictions...</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LSTMbyHand() </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Before optimization, the parameters are..."</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, param.data)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Now let's compare the observed and predicted values..."</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">## </span><span class="al">NOTE</span><span class="co">: To make predictions, we pass in the first 4 days worth of stock values </span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">## in an array for each company. In this case, the only difference between the</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">## input values for Company A and B occurs on the first day. Company A has 0 and</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co">## Company B has 1.</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company A: Observed = 0, Predicted ="</span>, </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>      model(torch.tensor([<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company B: Observed = 1, Predicted ="</span>, </span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>      model(torch.tensor([<span class="fl">1.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before optimization, the parameters are...
wlr1 tensor(0.3367)
wlr2 tensor(0.1288)
blr1 tensor(0.)
wpr1 tensor(0.2345)
wpr2 tensor(0.2303)
bpr1 tensor(0.)
wp1 tensor(-1.1229)
wp2 tensor(-0.1863)
bp1 tensor(0.)
wo1 tensor(2.2082)
wo2 tensor(-0.6380)
bo1 tensor(0.)

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(-0.0377)
Company B: Observed = 1, Predicted = tensor(-0.0383)</code></pre>
</div>
</div>
<p>With the unoptimized paramters, the predicted value for Company A, -0.0377, isn’t terrible, since it is relatively close to the observed value, 0. However, the predicted value for Company B, -0.0383, is terrible, because it is relatively far from the observed value, 1. So, that means we need to train the LSTM.</p>
<section id="train-the-lstm-unit-and-use-lightning-and-tensorboard-to-evaluate-part-1---getting-started" class="level3" data-number="17.4.1">
<h3 data-number="17.4.1" class="anchored" data-anchor-id="train-the-lstm-unit-and-use-lightning-and-tensorboard-to-evaluate-part-1---getting-started"><span class="header-section-number">17.4.1</span> Train the LSTM unit and use Lightning and TensorBoard to evaluate: Part 1 - Getting Started</h3>
<p>Since we are using Lightning training, training the LSTM we created by hand is pretty easy. All we have to do is create the training data and put it into a DataLoader…</p>
<div id="b0edf0c5" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">## create the training data for the neural network.</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.tensor([[<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>], [<span class="fl">1.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>]])</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> torch.tensor([<span class="fl">0.</span>, <span class="fl">1.</span>])</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(inputs, labels)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># show the training data</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (input_i, label_i) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Training data: "</span>, input_i, label_i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training data:  tensor([[0.0000, 0.5000, 0.2500, 1.0000]]) tensor([0.])
Training data:  tensor([[1.0000, 0.5000, 0.2500, 1.0000]]) tensor([1.])</code></pre>
</div>
</div>
<p>…and then create a Lightning Trainer, L.Trainer, and fit it to the training data. NOTE: We are starting with 2000 epochs. This may be enough to successfully optimize all of the parameters, but it might not. We’ll find out after we compare the predictions to the observed values.</p>
<div id="70231c41" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> L.Trainer(max_epochs<span class="op">=</span><span class="dv">2000</span>) <span class="co"># with default learning rate, 0.001 (this tiny learning rate makes learning slow)</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>trainer.fit(model, train_dataloaders<span class="op">=</span>dataloader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"cdb5e3490d22463aaab62a07d883d780","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Now that we’ve trained the model with 2000 epochs, we can see how good the predictions are…</p>
<div id="e172fbb0" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Now let's compare the observed and predicted values..."</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company A: Observed = 0, Predicted ="</span>, model(torch.tensor([<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company B: Observed = 1, Predicted ="</span>, model(torch.tensor([<span class="fl">1.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(0.4342)
Company B: Observed = 1, Predicted = tensor(0.6171)</code></pre>
</div>
</div>
<p>Unfortunately, these predictions are terrible. So it seems like we’ll have to do more training. However, it would be awesome if we could be confident that more training will actually improve the predictions. If not, we can spare ourselves a lot of time, and potentially money, and just give up. So, before we dive into more training, let’s look at the loss values and predictions that we saved in log files with TensorBoard. TensorBoard will graph everything that we logged during training, making it super easy to see if things are headed in the right direction or not.</p>
<p>To get TensorBoard working:</p>
<ul>
<li>First, check to see if the TensorBoard plugin is installed. If it’s not, install it with the following command: pip install tensorboard</li>
<li>Next, run the following command: <code>tensorboard --logdir lightning_logs</code></li>
</ul>
<p>NOTE: If your graphs look messed up and you see a bunch of different lines, instead of just one red line per graph, then check where this notebook is saved for a directory called <code>lightning_logs</code>. Delete <code>lightning_logs</code> and the re-run everything in this notebook. One source of problems with the graphs is that every time we train a model, a new batch of log files is created and stored in lightning_logs and TensorBoard, by default, will plot all of them. You can turn off unwanted log files in TensorBoard, and we’ll do this later on in this notebook, but for now, the easiest thing to do is to start with a clean slate.</p>
<p>Anyway, if we look at the loss (trainloss), we see that it is going down, which is good, but it still has further to go. When we look at the predictions for Company A (out0), we see that they started out pretty good, close to 0, but then got really bad early on in training, shooting all the way up to 0.5, but are starting to get smaller. In contrast, when we look at the predictions for Company B (out_1), we see that they started out really bad, close to 0, but have been getting better ever since and look like they could continue to get better if we kept training.</p>
<p>In summary, the graphs seem to suggest that if we continued training our model, the predictions would improve. So let’s add more epochs to the training.</p>
</section>
<section id="optimizing-training-the-weights-and-biases-in-the-lstm-that-we-made-by-hand-part-2---adding-more-epochs-without-starting-over" class="level3" data-number="17.4.2">
<h3 data-number="17.4.2" class="anchored" data-anchor-id="optimizing-training-the-weights-and-biases-in-the-lstm-that-we-made-by-hand-part-2---adding-more-epochs-without-starting-over"><span class="header-section-number">17.4.2</span> Optimizing (Training) the Weights and Biases in the LSTM that we made by hand: Part 2 - Adding More Epochs without Starting Over</h3>
<p>The good news is that because we’re using Lightning, we can pick up where we left off training without having to start over from scratch. This is because when we train with Lightning, it creates checkpoint files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the Trainer where the checkpoint files are located. This is awesome and will save us a lot of time since we don’t have to retrain the first 2000 epochs. So let’s add an additional 1000 epochs to the training.</p>
<div id="d64212cd" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">## First, find where the most recent checkpoint files are stored</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>path_to_checkpoint <span class="op">=</span> trainer.checkpoint_callback.best_model_path <span class="co">## By default, "best" = "most recent"</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The new trainer will start where the last left off, and the check point data is here: "</span> <span class="op">+</span> </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>      path_to_checkpoint <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Then create a new Lightning Trainer</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> L.Trainer(max_epochs<span class="op">=</span><span class="dv">3000</span>) <span class="co"># Before, max_epochs=2000, so, by setting it to 3000, we're adding 1000 more.</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">## And then call fit() using the path to the most recent checkpoint files</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">## so that we can pick up where we left off.</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>trainer.fit(model, train_dataloaders<span class="op">=</span>dataloader, ckpt_path<span class="op">=</span>path_to_checkpoint)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The new trainer will start where the last left off, and the check point data is here: /Users/bartz/workspace/Hyperparameter-Tuning-Cookbook/lightning_logs/version_221/checkpoints/epoch=1999-step=4000.ckpt
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7cf9510536e9454fbebbc5b8bf19892c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Now that we have added 1000 epochs to the training, let’s check the predictions…</p>
<div id="7c1da564" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Now let's compare the observed and predicted values..."</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company A: Observed = 0, Predicted ="</span>, model(torch.tensor([<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company B: Observed = 1, Predicted ="</span>, model(torch.tensor([<span class="fl">1.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(0.2708)
Company B: Observed = 1, Predicted = tensor(0.7534)</code></pre>
</div>
</div>
<p>The blue lines in each graph represents the values we logged during the extra 1000 epochs. The loss is getting smaller and the predictions for both companies are improving! Hooray!!! However, because it looks like there is even more room for improvement, let’s add 2000 more epochs to the training.</p>
<div id="adfbfa2e" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">## First, find where the most recent checkpoint files are stored</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>path_to_checkpoint <span class="op">=</span> trainer.checkpoint_callback.best_model_path <span class="co">## By default, "best" = "most recent"</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The new trainer will start where the last left off, and the check point data is here: "</span> <span class="op">+</span> </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>      path_to_checkpoint <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Then create a new Lightning Trainer</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> L.Trainer(max_epochs<span class="op">=</span><span class="dv">5000</span>) <span class="co"># Before, max_epochs=3000, so, by setting it to 5000, we're adding 2000 more.</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">## And then call fit() using the path to the most recent checkpoint files</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co">## so that we can pick up where we left off.</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>trainer.fit(model, train_dataloaders<span class="op">=</span>dataloader, ckpt_path<span class="op">=</span>path_to_checkpoint)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The new trainer will start where the last left off, and the check point data is here: /Users/bartz/workspace/Hyperparameter-Tuning-Cookbook/lightning_logs/version_222/checkpoints/epoch=2999-step=6000.ckpt
</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0575e99aee014e108ef7b971b45b2e3c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Now that we have added 2000 more epochs to the training (for a total of 5000 epochs), let’s check the predictions.</p>
<div id="c08820ae" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Now let's compare the observed and predicted values..."</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company A: Observed = 0, Predicted ="</span>, model(torch.tensor([<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company B: Observed = 1, Predicted ="</span>, model(torch.tensor([<span class="fl">1.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(0.0022)
Company B: Observed = 1, Predicted = tensor(0.9693)</code></pre>
</div>
</div>
<p>The prediction for Company A is super close to 0, which is exactly what we want, and the prediction for Company B is close to 1, which is also what we want.</p>
<p>The dark red lines show how things changed when we added an additional 2000 epochs to the training, for a total of 5000 epochs. Now we see that the loss (train_loss) and the predictions for each company appear to be tapering off, suggesting that adding more epochs may not improve the predictions much, so we’re done!</p>
<p>Lastly, let’s print out the final estimates for the Weights and Biases. In theory, they should be the same (within rounding error) as what we used in the StatQuest on Long Short-Term Memory and seen in the diagram of the LSTM unit at the top of this Jupyter notebook.</p>
<div id="7f3b36df" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After optimization, the parameters are..."</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, param.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After optimization, the parameters are...
wlr1 tensor(2.7043)
wlr2 tensor(1.6307)
blr1 tensor(1.6234)
wpr1 tensor(1.9983)
wpr2 tensor(1.6525)
bpr1 tensor(0.6204)
wp1 tensor(1.4122)
wp2 tensor(0.9393)
bp1 tensor(-0.3217)
wo1 tensor(4.3848)
wo2 tensor(-0.1943)
bo1 tensor(0.5935)</code></pre>
</div>
</div>
</section>
</section>
<section id="using-and-optimzing-the-pytorch-lstm-nn.lstm" class="level2" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="using-and-optimzing-the-pytorch-lstm-nn.lstm"><span class="header-section-number">17.5</span> Using and optimzing the PyTorch LSTM, nn.LSTM()</h2>
<p>Now that we know how to create an LSTM unit by hand, train it, and then use it to make good predictions, let’s learn how to take advantage of PyTorch’s <code>nn.LSTM()</code> function. For the most part, using nn.LSTM() allows us to simplify the init() function and the forward() function. The other big difference is that this time, we’re not going to try and recreate the parameter values we used in the StatQuest on Long Short-Term Memory, and that means we can set the learning rate for the Adam to 0.1. This will speed up training a lot. Everything else stays the same.</p>
<div id="4423ce1e" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LightningLSTM(L.LightningModule):</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>): <span class="co"># __init__() is the class constructor function, and we use it to initialize the Weights and Biases.</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() <span class="co"># initialize an instance of the parent class, LightningModule.</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        L.seed_everything(seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">## input_size = number of features (or variables) in the data. In our example</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">##              we only have a single feature (value)</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">## hidden_size = this determines the dimension of the output</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">##               in other words, if we set hidden_size=1, then we have 1 output node</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">##               if we set hidden_size=50, then we hve 50 output nodes (that can then be 50 input</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">##               nodes to a subsequent fully connected neural network.</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(input_size<span class="op">=</span><span class="dv">1</span>, hidden_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">## transpose the input vector</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>        input_trans <span class="op">=</span> <span class="bu">input</span>.view(<span class="bu">len</span>(<span class="bu">input</span>), <span class="dv">1</span>)</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>        lstm_out, temp <span class="op">=</span> <span class="va">self</span>.lstm(input_trans)</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">## lstm_out has the short-term memories for all inputs. We make our prediction with the last one</span></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> lstm_out[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>): <span class="co"># this configures the optimizer we want to use for backpropagation.</span></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Adam(<span class="va">self</span>.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>) <span class="co">## we'll just go ahead and set the learning rate to 0.1</span></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx): <span class="co"># take a step during gradient descent.</span></span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>        input_i, label_i <span class="op">=</span> batch <span class="co"># collect input</span></span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>        output_i <span class="op">=</span> <span class="va">self</span>.forward(input_i[<span class="dv">0</span>]) <span class="co"># run input through the neural network</span></span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (output_i <span class="op">-</span> label_i)<span class="op">**</span><span class="dv">2</span> <span class="co">## loss = squared residual</span></span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log(<span class="st">"train_loss"</span>, loss)</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (label_i <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.log(<span class="st">"out_0"</span>, output_i)</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.log(<span class="st">"out_1"</span>, output_i)</span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s create the model and print out the initial Weights and Biases and predictions.</p>
<div id="9aa61fcd" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LightningLSTM() <span class="co"># First, make model from the class</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co">## print out the name and value for each parameter</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Before optimization, the parameters are..."</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, param.data)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Now let's compare the observed and predicted values..."</span>)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company A: Observed = 0, Predicted ="</span>, model(torch.tensor([<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company B: Observed = 1, Predicted ="</span>, model(torch.tensor([<span class="fl">1.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before optimization, the parameters are...
lstm.weight_ih_l0 tensor([[ 0.7645],
        [ 0.8300],
        [-0.2343],
        [ 0.9186]])
lstm.weight_hh_l0 tensor([[-0.2191],
        [ 0.2018],
        [-0.4869],
        [ 0.5873]])
lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])
lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor([0.6675])
Company B: Observed = 1, Predicted = tensor([0.6665])</code></pre>
</div>
</div>
<p>As expected, the predictions are bad, so we will train the model. However, because we’ve increased the learning rate to 0.1, we only need to train for 300 epochs.</p>
<div id="a594af6f" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">## </span><span class="al">NOTE</span><span class="co">: Because we have set Adam's learning rate to 0.1, we will train much, much faster.</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co">## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co">## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co">## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co">## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> L.Trainer(max_epochs<span class="op">=</span><span class="dv">300</span>, log_every_n_steps<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>trainer.fit(model, train_dataloaders<span class="op">=</span>dataloader)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After optimization, the parameters are..."</span>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, param.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"420ab57b10f346e7ad06ae4a96c8ca2b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After optimization, the parameters are...
lstm.weight_ih_l0 tensor([[3.5364],
        [1.3869],
        [1.5390],
        [1.2488]])
lstm.weight_hh_l0 tensor([[5.2070],
        [2.9577],
        [3.2652],
        [2.0678]])
lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])
lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])</code></pre>
</div>
</div>
<p>Now that training is done, let’s print out the new predictions…</p>
<div id="272aed05" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Now let's compare the observed and predicted values..."</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company A: Observed = 0, Predicted ="</span>, model(torch.tensor([<span class="fl">0.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Company B: Observed = 1, Predicted ="</span>, model(torch.tensor([<span class="fl">1.</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">1.</span>])).detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor([6.8242e-05])
Company B: Observed = 1, Predicted = tensor([0.9809])</code></pre>
</div>
</div>
<p>…and, as we can see, after just 300 epochs, the LSTM is making great predictions. The prediction for Company A is close to the observed value 0 and the prediction for Company B is close to the observed value 1.</p>
<p>Lastly, let’s go back to TensorBoard to see the latest graphs. NOTE: To make it easier to see what we just did, deselect version0, version1 and version2 and make sure version3 is checked on the left-hand side of the page, under where it says Runs. This allows us to just look at the log files from the most recent training, which only went for 300 epochs.</p>
<p>In all three graphs, the loss (trainloss) and the predictions for Company A (out0) and Company B (out_1) started to taper off after 500 steps, or just 250 epochs, suggesting that adding more epochs may not improve the predictions much, so we’re done!</p>


</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"0575e99aee014e108ef7b971b45b2e3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c54e64e369e4c0ca673be9c7294835d","IPY_MODEL_eae3bd18ce5e42788fc9e504b218aa0a","IPY_MODEL_d770a9da7ae644f08ef674083d69184b"],"layout":"IPY_MODEL_51c3fddee57c405ca5a70d5177ddef25","tabbable":null,"tooltip":null}},"0622a4d88c854f39be99fc91dee254fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"16d67bc6cfd1405b9acaba09846076a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7f0fce8532b94c3e871884ba9d6ec609","placeholder":"​","style":"IPY_MODEL_f0a43a3f67c74e32916d11a4a6bc0d46","tabbable":null,"tooltip":null,"value":" 2/2 [00:00&lt;00:00, 90.59it/s, v_num=221]"}},"208083309b3246378a17c88637d77493":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2a132a4641254929bbe2b031b6c742a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"327a29780a574743a05f2ef0ae9f29f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_54f79fea4bc546a6bfbc29135296778c","placeholder":"​","style":"IPY_MODEL_4dbc361d64814de8ac3b0c8419d65a18","tabbable":null,"tooltip":null,"value":" 2/2 [00:00&lt;00:00, 70.88it/s, v_num=222]"}},"36b08253f9434c30a9dd39bcc9211023":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"420ab57b10f346e7ad06ae4a96c8ca2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a40bdfe1d3f348049e42d0f310cfadcd","IPY_MODEL_e47c2f15722348e88957c1085d6c40e0","IPY_MODEL_99c9e89958f04395b1238885e0ee6243"],"layout":"IPY_MODEL_95fb6e377ce1491a9b8751fa92a25370","tabbable":null,"tooltip":null}},"4c80147af44a4ba0931ef0177eb85394":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dbc361d64814de8ac3b0c8419d65a18":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"51b0bb8beba44c199ff97c54b107d3cc":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"51c3fddee57c405ca5a70d5177ddef25":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"5291f7ded66848f59076e1fe9104861a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54f79fea4bc546a6bfbc29135296778c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5656fe2ddef94988ac1d3bd03437957f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5291f7ded66848f59076e1fe9104861a","placeholder":"​","style":"IPY_MODEL_9708f26616dd4df083551241ca54e02e","tabbable":null,"tooltip":null,"value":"Epoch 2999: 100%"}},"5c54e64e369e4c0ca673be9c7294835d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f512267311d442eca43e96ceeeb6a032","placeholder":"​","style":"IPY_MODEL_0622a4d88c854f39be99fc91dee254fe","tabbable":null,"tooltip":null,"value":"Epoch 4999: 100%"}},"7021bbaf06984ffc94a4939db411e20e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75026ca900cd48aa8474510225970360":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78de286253154a5aaf1dc4c21343025c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a3c0450e69f1476f9c285d2738ba5f55","placeholder":"​","style":"IPY_MODEL_f2392fb56f9b4dfda4848d501bec4b1d","tabbable":null,"tooltip":null,"value":"Epoch 1999: 100%"}},"7cf9510536e9454fbebbc5b8bf19892c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5656fe2ddef94988ac1d3bd03437957f","IPY_MODEL_93934ff238844e24bb546b4b6d468637","IPY_MODEL_327a29780a574743a05f2ef0ae9f29f4"],"layout":"IPY_MODEL_a207b4c625c4421faad6d985a4daba90","tabbable":null,"tooltip":null}},"7f0fce8532b94c3e871884ba9d6ec609":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8256bed9163642a2a75bf9176f7cd4d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d1fb5e57eb441fea9aad1dc33098437":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"93934ff238844e24bb546b4b6d468637":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f5f2d0d92a054f6d8b37214eb7823336","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_970816f8b6894eb28a41e83f67e56678","tabbable":null,"tooltip":null,"value":2}},"95fb6e377ce1491a9b8751fa92a25370":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"970816f8b6894eb28a41e83f67e56678":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9708f26616dd4df083551241ca54e02e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"97101795cdb54ea8ac5e7748697620e8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99c9e89958f04395b1238885e0ee6243":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_97101795cdb54ea8ac5e7748697620e8","placeholder":"​","style":"IPY_MODEL_8d1fb5e57eb441fea9aad1dc33098437","tabbable":null,"tooltip":null,"value":" 2/2 [00:00&lt;00:00, 205.15it/s, v_num=224]"}},"a207b4c625c4421faad6d985a4daba90":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"a3c0450e69f1476f9c285d2738ba5f55":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a40bdfe1d3f348049e42d0f310cfadcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4c80147af44a4ba0931ef0177eb85394","placeholder":"​","style":"IPY_MODEL_cb5c33fd76ea4532806ea55477d0ad2f","tabbable":null,"tooltip":null,"value":"Epoch 299: 100%"}},"c22a7e83c0a440cca267aca87dd4081f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9183879a0f14e3eb11c44abc45c7517":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb5c33fd76ea4532806ea55477d0ad2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"cdb5e3490d22463aaab62a07d883d780":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78de286253154a5aaf1dc4c21343025c","IPY_MODEL_d6b8da1a57944c36b33243ae8fecb7d0","IPY_MODEL_16d67bc6cfd1405b9acaba09846076a3"],"layout":"IPY_MODEL_51b0bb8beba44c199ff97c54b107d3cc","tabbable":null,"tooltip":null}},"d6b8da1a57944c36b33243ae8fecb7d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_c22a7e83c0a440cca267aca87dd4081f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8256bed9163642a2a75bf9176f7cd4d1","tabbable":null,"tooltip":null,"value":2}},"d770a9da7ae644f08ef674083d69184b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_75026ca900cd48aa8474510225970360","placeholder":"​","style":"IPY_MODEL_208083309b3246378a17c88637d77493","tabbable":null,"tooltip":null,"value":" 2/2 [00:00&lt;00:00, 70.43it/s, v_num=223]"}},"e47c2f15722348e88957c1085d6c40e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_36b08253f9434c30a9dd39bcc9211023","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a132a4641254929bbe2b031b6c742a9","tabbable":null,"tooltip":null,"value":2}},"eae3bd18ce5e42788fc9e504b218aa0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_7021bbaf06984ffc94a4939db411e20e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9183879a0f14e3eb11c44abc45c7517","tabbable":null,"tooltip":null,"value":2}},"f0a43a3f67c74e32916d11a4a6bc0d46":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f2392fb56f9b4dfda4848d501bec4b1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f512267311d442eca43e96ceeeb6a032":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5f2d0d92a054f6d8b37214eb7823336":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./100_ddmo.html" class="pagination-link" aria-label="Data-Driven Modeling and Optimization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Data-Driven Modeling and Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./300_hpt_intro.html" class="pagination-link" aria-label="Hyperparameter Tuning">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>