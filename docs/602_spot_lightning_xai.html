<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>39&nbsp; Explainable AI with SpotPython and Pytorch – Hyperparameter Tuning Cookbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./603_spot_lightning_transformer_introduction.html" rel="next">
<link href="./601_spot_hpt_light_pinn.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-93074729595c0d1bb0917954c6b21507.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<meta name="twitter:title" content="39&nbsp; Explainable AI with SpotPython and Pytorch – Hyperparameter Tuning Cookbook">
<meta name="twitter:image" content="602_spot_lightning_xai_files/figure-html/plot_attributions_feature_ablation-output-2.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="[39]{.chapter-number}&nbsp; [Explainable AI with SpotPython and Pytorch]{.chapter-title}">
<meta name="citation_fulltext_html_url" content="https://arxiv.org/abs/2307.10262">
<meta name="citation_doi" content="10.48550/arXiv.2307.10262">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="arXiv">
<meta name="citation_reference" content="citation_title=Benchmarking in Optimization: Best Practice and Open Issues;,citation_author=Thomas Bartz-Beielstein;,citation_author=Carola Doerr;,citation_author=Jakob Bossek;,citation_author=Sowmya Chandrasekaran;,citation_author=Tome Eftimov;,citation_author=Andreas Fischbach;,citation_author=Pascal Kerschke;,citation_author=Manuel Lopez-Ibanez;,citation_author=Katherine M. Malan;,citation_author=Jason H. Moore;,citation_author=Boris Naujoks;,citation_author=Patryk Orzechowski;,citation_author=Vanessa Volz;,citation_author=Markus Wagner;,citation_author=Thomas Weise;,citation_publication_date=2020-07;,citation_cover_date=2020-07;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2007.03488;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning With Ray Tune;,citation_author=undefined PyTorch;,citation_publication_date=2023-05;,citation_cover_date=2023-05;,citation_year=2023;,citation_fulltext_html_url=https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html;">
<meta name="citation_reference" content="citation_title=Training a Classifier;,citation_author=undefined PyTorch;,citation_publication_date=2023-05;,citation_cover_date=2023-05;,citation_year=2023;,citation_fulltext_html_url=https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html;">
<meta name="citation_reference" content="citation_title=PyTorch Hyperparameter Tuning with SPOT: Comparison with Ray Tuner and Default Hyperparameters on CIFAR10;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2023-04;,citation_cover_date=2023-04;,citation_year=2023;,citation_fulltext_html_url=https://github.com/sequential-parameter-optimization/spotpython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb;">
<meta name="citation_reference" content="citation_title=Machine Learning in Official Statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.48550/arXiv.1812.10422;">
<meta name="citation_reference" content="citation_title=Qualitätshandbuch der Statistischen Ämter des Bundes und der Länder;,citation_publication_date=2021-03;,citation_cover_date=2021-03;,citation_year=2021;,citation_fulltext_html_url=https://www.destatis.de/DE/Methoden/Qualitaet/qualitaetshandbuch.pdf;,citation_language=de;">
<meta name="citation_reference" content="citation_title=Quality Assurance Framework of the European Statistical System;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://ec.europa.eu/eurostat/documents/64157/4392716/ESS-QAF-V2.0-final.pdf;,citation_language=en;">
<meta name="citation_reference" content="citation_title=Standardisierung der Prozesse: 14 Jahre AG SteP;,citation_author=T. Blumöhr;,citation_author=C. Teichmann;,citation_author=A. Noack;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://www.destatis.de/DE/Methoden/
               WISTA-Wirtschaft-und-Statistik/2017/05/standardisierung-prozesse-052017.html;,citation_volume=5;,citation_journal_title=WISTA - Wirtschaft und Statistik;,citation_publisher=Wiesbaden: Statistisches Bundesamt (Destatis);">
<meta name="citation_reference" content="citation_title=Generic Statistical Business Process Model - GSBPM;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://statswiki.unece.org/display/
               GSBPM/GSBPM+v5.1;,citation_publisher=United Nations Economic Commission for Europe (UNECE);">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Engineering Design via Surrogate Modelling;,citation_author=Alexander Forrester;,citation_author=András Sóbester;,citation_author=Andy Keane;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;">
<meta name="citation_reference" content="citation_title=No Free Lunch Theorems for Optimization;,citation_author=David H Wolpert;,citation_author=William G Macready;,citation_publication_date=1997-04;,citation_cover_date=1997-04;,citation_year=1997;,citation_issue=1;,citation_volume=1;,citation_journal_title=IEEE Transactions on Evolutionary Computation;">
<meta name="citation_reference" content="citation_title=An Introduction to Statistical Learning with Applications in R;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Requirements for papers focusing on new or improved global optimization algorithms;,citation_author=Raphael T. Haftka;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=54;,citation_journal_title=Structural and Multidisciplinary Optimization;">
<meta name="citation_reference" content="citation_title=LightGBM: A Highly Efficient Gradient Boosting Decision Tree;,citation_author=Guolin Ke;,citation_author=Qi Meng;,citation_author=Thomas Finley;,citation_author=Taifeng Wang;,citation_author=Wei Chen;,citation_author=Weidong Ma;,citation_author=Qiwei Ye;,citation_author=Tie-Yan Liu;,citation_editor=I. Guyon;,citation_editor=U. Von Luxburg;,citation_editor=S. Bengio;,citation_editor=H. Wallach;,citation_editor=R. Fergus;,citation_editor=S. Vishwanathan;,citation_editor=R. Garnett;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_conference_title=Advances in Neural Information Processing Systems;,citation_conference=Curran Associates, Inc.;">
<meta name="citation_reference" content="citation_title=Greedy Function Approximation: A Gradient Boosting Machine;,citation_author=Jerome H. Friedman;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_issue=5;,citation_volume=29;,citation_journal_title=The Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Machine Learning in Official Statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.48550/arXiv.1812.10422;">
<meta name="citation_reference" content="citation_title=Official statistics in the era of big data opportunities and threats;,citation_author=Walter J. Radermacher;,citation_publication_date=2018-11-01;,citation_cover_date=2018-11-01;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1007/s41060-018-0124-z;,citation_issue=3;,citation_doi=10.1007/s41060-018-0124-z;,citation_volume=6;,citation_language=en;,citation_journal_title=International Journal of Data Science and Analytics;">
<meta name="citation_reference" content="citation_title=Machine Learning in Official Statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;">
<meta name="citation_reference" content="citation_title=A quality framework for statistical algorithms;,citation_author=Wesley Yung;,citation_author=Siu-Ming Tam;,citation_author=Bart Buelens;,citation_author=Hugh Chipman;,citation_author=Florian Dumpert;,citation_author=Gabriele Ascari;,citation_author=Fabiana Rocci;,citation_author=Joep Burger;,citation_author=InKyung Choi;,citation_publication_date=2022-01-01;,citation_cover_date=2022-01-01;,citation_year=2022;,citation_fulltext_html_url=https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji210875;,citation_issue=1;,citation_doi=10.3233/SJI-210875;,citation_volume=38;,citation_language=en;,citation_journal_title=Statistical Journal of the IAOS;">
<meta name="citation_reference" content="citation_title=A quality framework for statistical algorithms;,citation_author=Wesley Yung;,citation_author=Siu-Ming Tam;,citation_author=Bart Buelens;,citation_author=Hugh Chipman;,citation_author=Florian Dumpert;,citation_author=Gabriele Ascari;,citation_author=Fabiana Rocci;,citation_author=Joep Burger;,citation_author=InKyung Choi;,citation_publication_date=2022-01;,citation_cover_date=2022-01;,citation_year=2022;,citation_issue=1;,citation_volume=38;,citation_journal_title=Statistical Journal of the IAOS;">
<meta name="citation_reference" content="citation_title=Qualitätshandbuch der Statistischen Ämter des Bundes und der Länder;,citation_author=Michael Reichelt;">
<meta name="citation_reference" content="citation_title=Data Science and Official Statistics: Toward a New Data Culture;,citation_author=Stefan Schweinfest;,citation_author=Ronald Jansen;,citation_publication_date=2021-10-28;,citation_cover_date=2021-10-28;,citation_year=2021;,citation_fulltext_html_url=https://hdsr.mitpress.mit.edu/pub/1g514ljw/release/4;,citation_issue=4;,citation_doi=10.1162/99608f92.c1237762;,citation_volume=3;,citation_language=en;,citation_journal_title=Harvard Data Science Review;">
<meta name="citation_reference" content="citation_title=Official Statistics 4.0: Verified Facts for People in the 21st Century;,citation_author=Walter J. Radermacher;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=Detecting Covariate Drift with Explanations;,citation_author=Steffen Castle;,citation_author=Robert Schwarzenberg;,citation_author=Mohsen Pourvali;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13–17, 2021, Proceedings, Part II;,citation_conference=Springer-Verlag;">
<meta name="citation_reference" content="citation_title=Keras;,citation_author=Francois Chollet;,citation_author=others;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_publisher=https:://keras.io;">
<meta name="citation_reference" content="citation_title=TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems;,citation_author=Martin Abadi;,citation_author=Ashish Agarwal;,citation_author=Paul Barham;,citation_author=Eugene Brevdo;,citation_author=Zhifeng Chen;,citation_author=Craig Citro;,citation_author=Greg S. Corrado;,citation_author=Andy Davis;,citation_author=Jeffrey Dean;,citation_author=Matthieu Devin;,citation_author=Sanjay Ghemawat;,citation_author=Ian Goodfellow;,citation_author=Andrew Harp;,citation_author=Geoffrey Irving;,citation_author=Michael Isard;,citation_author=Yangqing Jia;,citation_author=Rafal Jozefowicz;,citation_author=Lukasz Kaiser;,citation_author=Manjunath Kudlur;,citation_author=Josh Levenberg;,citation_author=Dan Mane;,citation_author=Rajat Monga;,citation_author=Sherry Moore;,citation_author=Derek Murray;,citation_author=Chris Olah;,citation_author=Mike Schuster;,citation_author=Jonathon Shlens;,citation_author=Benoit Steiner;,citation_author=Ilya Sutskever;,citation_author=Kunal Talwar;,citation_author=Paul Tucker;,citation_author=Vincent Vanhoucke;,citation_author=Vijay Vasudevan;,citation_author=Fernanda Viegas;,citation_author=Oriol Vinyals;,citation_author=Pete Warden;,citation_author=Martin Wattenberg;,citation_author=Martin Wicke;,citation_author=Yuan Yu;,citation_author=Xiaoqiang Zheng;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=statsmodels: Econometric and statistical modeling with python;,citation_author=Skipper Seabold;,citation_author=Josef Perktold;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=9th Python in Science Conference;">
<meta name="citation_reference" content="citation_title=Scikit-learn: Machine Learning in Python;,citation_author=F. Pedregosa;,citation_author=G. Varoquaux;,citation_author=A. Gramfort;,citation_author=V. Michel;,citation_author=B. Thirion;,citation_author=O. Grisel;,citation_author=M. Blondel;,citation_author=P. Prettenhofer;,citation_author=R. Weiss;,citation_author=V. Dubourg;,citation_author=J. Vanderplas;,citation_author=A. Passos;,citation_author=D. Cournapeau;,citation_author=M. Brucher;,citation_author=M. Perrot;,citation_author=E. Duchesnay;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=12;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Array programming with NumPy;,citation_author=Charles R. Harris;,citation_author=K. Jarrod Millman;,citation_author=Stéfan J. Walt;,citation_author=Ralf Gommers;,citation_author=Pauli Virtanen;,citation_author=David Cournapeau;,citation_author=Eric Wieser;,citation_author=Julian Taylor;,citation_author=Sebastian Berg;,citation_author=Nathaniel J. Smith;,citation_author=Robert Kern;,citation_author=Matti Picus;,citation_author=Stephan Hoyer;,citation_author=Marten H. Kerkwijk;,citation_author=Matthew Brett;,citation_author=Allan Haldane;,citation_author=Jaime Fernández Río;,citation_author=Mark Wiebe;,citation_author=Pearu Peterson;,citation_author=Pierre Gérard-Marchant;,citation_author=Kevin Sheppard;,citation_author=Tyler Reddy;,citation_author=Warren Weckesser;,citation_author=Hameer Abbasi;,citation_author=Christoph Gohlke;,citation_author=Travis E. Oliphant;,citation_publication_date=2020-09;,citation_cover_date=2020-09;,citation_year=2020;,citation_issue=7825;,citation_volume=585;,citation_journal_title=Nature;">
<meta name="citation_reference" content="citation_title=Algorithms for Learning Regression Trees and Ensembles on Evolving Data Streams;,citation_author=Elena Ikonomovska;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_dissertation_institution=Jozef Stefan International Postgraduate School;">
<meta name="citation_reference" content="citation_title=Online Bagging and Boosting;,citation_author=N C Oza;,citation_conference_title=2005 IEEE International Conference on Systems, Man and Cybernetics;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Online bagging and boosting;,citation_author=Nikunj C Oza;,citation_author=Stuart Russell;,citation_editor=T Jaakola;,citation_editor=T Richardson;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=8th Insternational Workshop on Artificial Intelligence and Statistics;">
<meta name="citation_reference" content="citation_title=Event labeling combining ensemble detectors and background knowledge;,citation_author=Hadi Fanaee-T;,citation_author=Joao Gama;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=2;,citation_volume=2;,citation_journal_title=Progress in Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Adaptive random forests for evolving data stream classification;,citation_author=Heitor M. Gomes;,citation_author=Albert Bifet;,citation_author=Jesse Read;,citation_author=Jean Paul Barddal;,citation_author=Fabricio Enembreck;,citation_author=Bernhard Pfharinger;,citation_author=Geoff Holmes;,citation_author=Talel Abdessalem;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=9;,citation_volume=106;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Literate Programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
<meta name="citation_reference" content="citation_title=A Review and Taxonomy of Interactive Optimization Methods in Operations Research;,citation_author=David Meignan;,citation_author=Sigrid Knust;,citation_author=Jean-Marc Frayet;,citation_author=Gilles Pesant;,citation_author=Nicolas Gaud;,citation_publication_date=2015-09;,citation_cover_date=2015-09;,citation_year=2015;,citation_journal_title=ACM Transactions on Interactive Intelligent Systems;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=In a Nutshell – The Sequential Parameter Optimization Toolbox;,citation_author=Thomas Bartz-Beielstein;,citation_author=Martin Zaefferer;,citation_author=Frederik Rehbach;,citation_publication_date=2021-12;,citation_cover_date=2021-12;,citation_year=2021;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Direct search methods: Then and now;,citation_author=R M Lewis;,citation_author=V Torczon;,citation_author=M W Trosset;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_issue=1–2;,citation_volume=124;,citation_journal_title=Journal of Computational and Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization;,citation_author=Lisha Li;,citation_author=Kevin Jamieson;,citation_author=Giulia DeSalvo;,citation_author=Afshin Rostamizadeh;,citation_author=Ameet Talwalkar;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Sequential Parameter Optimization;,citation_author=Thomas Bartz-Beielstein;,citation_author=Christian Lasarczyk;,citation_author=Mike Preuss;,citation_editor=B McKay;,citation_editor=others;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_conference_title=Proceedings 2005 Congress on Evolutionary Computation (CEC’05), Edinburgh, Scotland;,citation_conference=IEEE Press;">
<meta name="citation_reference" content="citation_title=Evolutionary Algorithms;,citation_author=Thomas Bartz-Beielstein;,citation_author=Jürgen Branke;,citation_author=Jörn Mehnen;,citation_author=Olaf Mersmann;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=3;,citation_volume=4;,citation_journal_title=Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Classification and Regression Trees;,citation_author=L Breiman;,citation_author=J H Friedman;,citation_author=R A Olshen;,citation_author=C J Stone;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;">
<meta name="citation_reference" content="citation_title=Continuous inspection schemes;,citation_author=E. S. Page;,citation_publication_date=1954-06;,citation_cover_date=1954-06;,citation_year=1954;,citation_issue=1-2;,citation_volume=41;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=Counting Large Numbers of Events in Small Registers;,citation_author=Robert Morris;,citation_publication_date=1978-10;,citation_cover_date=1978-10;,citation_year=1978;,citation_issue=10;,citation_volume=21;,citation_journal_title=Commun. ACM;">
<meta name="citation_reference" content="citation_title=Approximate Counting: A Detailed Analysis;,citation_author=Philippe Flajolet;,citation_publication_date=1985-03;,citation_cover_date=1985-03;,citation_year=1985;,citation_issue=1;,citation_volume=25;,citation_journal_title=BIT;">
<meta name="citation_reference" content="citation_title=Random Sampling with a Reservoir;,citation_author=Jeffrey S. Vitter;,citation_publication_date=1985-03;,citation_cover_date=1985-03;,citation_year=1985;,citation_issue=1;,citation_volume=11;,citation_journal_title=ACM Trans. Math. Softw.;">
<meta name="citation_reference" content="citation_title=Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem;,citation_author=Michael McCloskey;,citation_author=Neal J. Cohen;,citation_publication_date=1989-01;,citation_cover_date=1989-01;,citation_year=1989;,citation_issue=C;,citation_volume=24;,citation_journal_title=Psychology of Learning and Motivation - Advances in Research and Theory;">
<meta name="citation_reference" content="citation_title=Detection of Abrupt Changes - Theory and Application;,citation_author=Michèle Basseville;,citation_author=Igor V. Nikiforov;,citation_publication_date=1993;,citation_cover_date=1993;,citation_year=1993;">
<meta name="citation_reference" content="citation_title=An Introduction to Computational Learning Theory;,citation_author=Michael J. Kearns;,citation_author=Umesh V. Vazirani;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;">
<meta name="citation_reference" content="citation_title=An Introduction to the Kalman Filter;,citation_author=Greg Welch;,citation_author=Gary Bishop;,citation_publication_date=1995;,citation_cover_date=1995;,citation_year=1995;">
<meta name="citation_reference" content="citation_title=The Space Complexity of Approximating the Frequency Moments;,citation_author=Noga Alon;,citation_author=Yossi Matias;,citation_author=Mario Szegedy;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_conference_title=Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing;,citation_conference=Association for Computing Machinery;,citation_series_title=STOC ’96;">
<meta name="citation_reference" content="citation_title=SPLICE-2 Comparative Evaluation: Electricity Pricing;,citation_author=Michael Harries;,citation_author=U Nsw-cse-tr;,citation_author=New South Wales;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;">
<meta name="citation_reference" content="citation_title=Mining high-speed data streams;,citation_author=Pedro M. Domingos;,citation_author=Geoff Hulten;,citation_editor=Raghu Ramakrishnan;,citation_editor=Salvatore J. Stolfo;,citation_editor=Roberto J. Bayardo;,citation_editor=Ismail Parsa;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_conference_title=Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, Boston, MA, USA, August 20-23, 2000;,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=Mining Time-Changing Data Streams;,citation_author=Geoff Hulten;,citation_author=Laurie Spencer;,citation_author=Pedro Domingos;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’01;">
<meta name="citation_reference" content="citation_title=A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification;,citation_author=W. Nick Street;,citation_author=YongSeog Kim;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’01;">
<meta name="citation_reference" content="citation_title=3D Data Management: Controlling Data Volume, Velocity, and Variety;,citation_author=Douglas Laney;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_technical_report_institution=META Group;">
<meta name="citation_reference" content="citation_title=Models and Issues in Data Stream Systems;,citation_author=Brian Babcock;,citation_author=Shivnath Babu;,citation_author=Mayur Datar;,citation_author=Rajeev Motwani;,citation_author=Jennifer Widom;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_conference_title=Proceedings of the 21st ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems;,citation_conference=ACM;,citation_series_title=PODS ’02;">
<meta name="citation_reference" content="citation_title=A New Approximate Maximal Margin Classification Algorithm;,citation_author=Claudio Gentile;,citation_publication_date=2002-03;,citation_cover_date=2002-03;,citation_year=2002;,citation_volume=2;,citation_journal_title=J. Mach. Learn. Res.;">
<meta name="citation_reference" content="citation_title=The Design and Analysis of Computer Experiments;,citation_author=T J Santner;,citation_author=B J Williams;,citation_author=W I Notz;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;">
<meta name="citation_reference" content="citation_title=Learning with drift detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_conference_title=In SBIA Brazilian Symposium on Artificial Intelligence;,citation_conference=Springer Verlag;">
<meta name="citation_reference" content="citation_title=Learning with Drift Detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_inbook_title=Parallel Problem Solving from Nature - PPSN XIII - 13th International Conference, Ljubljana, Slovenia, September 13-17, 2014. Proceedings;">
<meta name="citation_reference" content="citation_title=Learning with Drift Detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_editor=Ana L. C. Bazzan;,citation_editor=Sofiane Labidi;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_conference_title=Advances in Artificial Intelligence – SBIA 2004;,citation_conference=Springer Berlin Heidelberg;">
<meta name="citation_reference" content="citation_title=Statistical Analysis of Massive Data Streams: Proceedings of a Workshop;,citation_editor=Sallie Keller-McNulty;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;">
<meta name="citation_reference" content="citation_title=Data Mining: Practical Machine Learning Tools and Techniques;,citation_author=Ian H. Witten;,citation_author=Eibe Frank;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_series_title=The Morgan Kaufmann Series in Data Management Systems;">
<meta name="citation_reference" content="citation_title=Towards Generic Pattern Mining;,citation_author=Mohammed Javeed Zaki;,citation_author=Nagender Parimi;,citation_author=Nilanjana De;,citation_author=Feng Gao;,citation_author=Benjarath Phoophakdee;,citation_author=Joe Urban;,citation_author=Vineet Chaoji;,citation_author=Mohammad Al Hasan;,citation_author=Saeed Salem;,citation_editor=Bernhard Ganter;,citation_editor=Robert Godin;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=3403;,citation_conference_title=Formal Concept Analysis, Third International Conference, ICFCA 2005, Lens, France, February 14-18, 2005, Proceedings;,citation_conference=Springer;,citation_series_title=Lecture Notes in Computer Science;">
<meta name="citation_reference" content="citation_title=Mining Data Streams: A Review;,citation_author=Mohamed Medhat Gaber;,citation_author=Arkady Zaslavsky;,citation_author=Shonali Krishnaswamy;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=34;,citation_journal_title=SIGMOD Rec.;">
<meta name="citation_reference" content="citation_title=Early drift detection method;,citation_author=Manuel Baena-Garcıa;,citation_author=José Campo-Ávila;,citation_author=Raúl Fidalgo;,citation_author=Albert Bifet;,citation_author=R Gavalda;,citation_author=Rafael Morales-Bueno;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_volume=6;,citation_conference_title=Fourth international workshop on knowledge discovery from data streams;">
<meta name="citation_reference" content="citation_title=Online Passive-Aggressive Algorithms;,citation_author=Koby Crammer;,citation_author=Ofer Dekel;,citation_author=Joseph Keshet;,citation_author=Shai Shalev-Shwartz;,citation_author=Yoram Singer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=19;,citation_volume=7;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Data Streams – Models and Algorithms;,citation_editor=Charu Aggarwal;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;">
<meta name="citation_reference" content="citation_title=Learning from Time-Changing Data with Adaptive Windowing;,citation_author=Albert Bifet;,citation_author=Ricard Gavaldà;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=Proceedings of the 2007 SIAM International Conference on Data Mining (SDM);">
<meta name="citation_reference" content="citation_title=Learning from time-changing data with adaptive windowing;,citation_author=Albert Bifet;,citation_author=Ricard Gavalda;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_volume=7;,citation_conference_title=Proceedings of the 2007 SIAM international conference on data mining;,citation_conference=SIAM;">
<meta name="citation_reference" content="citation_title=A Survey of Classification Methods in Data Streams;,citation_author=Mohamed Gaber;,citation_author=Arkady Zaslavsky;,citation_author=Shonali Krishnaswamy;,citation_editor=Charu Aggarwal;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_inbook_title=Data Streams – Models and Algorithms;">
<meta name="citation_reference" content="citation_title=Use of Hoeffding trees in concept based data stream mining;,citation_author=Stefan Hoeglinger;,citation_author=Russel Pears;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_journal_title=2007 Third International Conference on Information and Automation for Sustainability;">
<meta name="citation_reference" content="citation_title=Detecting concept drift using statistical testing;,citation_author=Kyosuke Nishida;,citation_author=Koichiro Yamauchi;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=International conference on discovery science;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Olindda: A cluster-based approach for detecting novelty and concept drift in data streams;,citation_author=Eduardo J Spinosa;,citation_author=André Ponce Leon F. de Carvalho;,citation_author=Joao Gama;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=Proceedings of the 2007 ACM symposium on Applied computing;">
<meta name="citation_reference" content="citation_title=Statistical Quality Control;,citation_author=Douglas C Montgomery;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;">
<meta name="citation_reference" content="citation_title=Adaptive Learning from Evolving Data Streams;,citation_author=Albert Bifet;,citation_author=Ricard Gavaldà;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 8th International Symposium on Intelligent Data Analysis: Advances in Intelligent Data Analysis VIII;,citation_conference=Springer-Verlag;,citation_series_title=IDA ’09;">
<meta name="citation_reference" content="citation_title=New Ensemble Methods for Evolving Data Streams;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_author=Richard Kirkby;,citation_author=Ricard Gavaldà;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’09;">
<meta name="citation_reference" content="citation_title=Adaptive concept drift detection;,citation_author=Anton Dries;,citation_author=Ulrich Rückert;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=5-6;,citation_volume=2;,citation_journal_title=Stat. Anal. Data Min.;">
<meta name="citation_reference" content="citation_title=Issues in Evaluation of Stream Learning Algorithms;,citation_author=João Gama;,citation_author=Raquel Sebastião;,citation_author=Pedro Pereira Rodrigues;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’09;">
<meta name="citation_reference" content="citation_title=Stream Data Processing: A Quality of Service Perspective;,citation_author=Qingchun Jiang;,citation_author=Sharma Chakravarthy;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=Probabilistic Counting with Randomized Storage;,citation_author=Benjamin Van Durme;,citation_author=Ashwin Lall;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 21st International Joint Conference on Artificial Intelligence;,citation_conference=Morgan Kaufmann Publishers Inc.;,citation_series_title=IJCAI’09;">
<meta name="citation_reference" content="citation_title=Adaptive Stream Mining: Pattern Learning and Mining from Evolving Data Streams;,citation_author=Albert Bifet;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=207;,citation_series_title=Frontiers in Artificial Intelligence and Applications;">
<meta name="citation_reference" content="citation_title=We’re not in kansas anymore: detecting domain changes in streams;,citation_author=Mark Dredze;,citation_author=Tim Oates;,citation_author=Christine Piatko;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing;">
<meta name="citation_reference" content="citation_title=Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction (Institute of Mathematical Statistics Monographs);,citation_author=Bradley Efron;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;">
<meta name="citation_reference" content="citation_title=Knowledge Discovery from Data Streams;,citation_author=João Gama;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_series_title=Chapman and Hall / CRC Data Mining and Knowledge Discovery Series;">
<meta name="citation_reference" content="citation_title=A DCT based approach for detecting novelty and concept drift in data streams;,citation_author=Morteza Zi Hayat;,citation_author=Mahmoud Reza Hashemi;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=2010 International Conference of Soft Computing and Pattern Recognition;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Benchmarking Stream Clustering Algorithms within the MOA Framework;,citation_author=Philipp Kranen;,citation_author=Hardy Kremer;,citation_author=Timm Jansen;,citation_author=Thomas Seidl;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2010), Washington, DC, USA;">
<meta name="citation_reference" content="citation_title=MOA: Massive Online Analysis;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Richard Kirkby;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=99;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Hellinger distance based drift detection for nonstationary environments;,citation_author=Gregory Ditzler;,citation_author=Robi Polikar;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_conference_title=2011 IEEE symposium on computational intelligence in dynamic and uncertain environments (CIDUE);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Classification and novel class detection in concept-drifting data streams under time constraints;,citation_author=Mohammad Masud;,citation_author=Jing Gao;,citation_author=Latifur Khan;,citation_author=Jiawei Han;,citation_author=Bhavani M Thuraisingham;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=6;,citation_volume=23;,citation_journal_title=IEEE Transactions on Knowledge and Data Engineering;">
<meta name="citation_reference" content="citation_title=New drift detection method for data streams;,citation_author=Parinaz Sobhani;,citation_author=Hamid Beigy;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_conference_title=International conference on adaptive and intelligent systems;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=birch: Dealing With Very Large Datasets Using BIRCH;,citation_author=Lysiane Charest;,citation_author=Justin Harrington;,citation_author=Matias Salibian-Barrera;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Synopses for Massive Data: Samples, Histograms, Wavelets, Sketches;,citation_author=Graham Cormode;,citation_author=Minos Garofalakis;,citation_author=Peter J. Haas;,citation_author=Chris Jermaine;,citation_publication_date=2012-01;,citation_cover_date=2012-01;,citation_year=2012;,citation_issue=1–3;,citation_volume=4;,citation_journal_title=Found. Trends Databases;">
<meta name="citation_reference" content="citation_title=Detection of concept drift for learning from stream data;,citation_author=Jeonghoon Lee;,citation_author=Frederic Magoules;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=2012 IEEE 14th International Conference on High Performance Computing and Communication &amp;amp;amp; 2012 IEEE 9th International Conference on Embedded Software and Systems;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=mlbench: Machine Learning Benchmark Problems;,citation_author=Friedrich Leisch;,citation_author=Evgenia Dimitriadou;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=A unifying view on dataset shift in classification;,citation_author=Jose G. Moreno-Torres;,citation_author=Troy Raeder;,citation_author=Rocı́o Alaı́z-Rodrı́guez;,citation_author=Nitesh V. Chawla;,citation_author=Francisco Herrera;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_volume=45;,citation_journal_title=Pattern Recognit.;">
<meta name="citation_reference" content="citation_title=HadoopStreaming: Utilities for Using R Scripts in Hadoop Streaming;,citation_author=David S. Rosenberg;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Exponentially weighted moving average charts for detecting concept drift;,citation_author=Gordon J Ross;,citation_author=Niall M Adams;,citation_author=Dimitris K Tasoulis;,citation_author=David J Hand;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=2;,citation_volume=33;,citation_journal_title=Pattern recognition letters;">
<meta name="citation_reference" content="citation_title=An efficient method of building an ensemble of classifiers in streaming data;,citation_author=Joung Woo Ryu;,citation_author=Mehmed M Kantardzic;,citation_author=Myung-Won Kim;,citation_author=A Ra Khil;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=International Conference on Big Data Analytics;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=rlecuyer: R Interface to RNG With Multiple Streams;,citation_author=Hana Sevcikova;,citation_author=Tony Rossini;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Machine Learning that Matters;,citation_author=Kiri Wagstaff;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=CD-MOA: Change Detection Framework for Massive Online Analysis;,citation_author=Albert Bifet;,citation_author=Jesse Read;,citation_author=Bernhard Pfahringer;,citation_author=Geoff Holmes;,citation_author=Indrė Žliobaitė;,citation_editor=Allan Tucker;,citation_editor=Frank Höppner;,citation_editor=Arno Siebes;,citation_editor=Stephen Swift;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Advances in Intelligent Data Analysis XII;,citation_conference=Springer Berlin Heidelberg;">
<meta name="citation_reference" content="citation_title=Novelty detection algorithm for data streams multi-class problems;,citation_author=Elaine R Faria;,citation_author=João Gama;,citation_author=André CPLF Carvalho;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 28th annual ACM symposium on applied computing;">
<meta name="citation_reference" content="citation_title=Turning Big Data into Tiny Data: Constant-Size Coresets for k-Means, PCA and Projective Clustering;,citation_author=Dan Feldman;,citation_author=Melanie Schmidt;,citation_author=Christian Sohler;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms;,citation_conference=Society for Industrial; Applied Mathematics;,citation_series_title=SODA ’13;">
<meta name="citation_reference" content="citation_title=On evaluating stream learning algorithms;,citation_author=João Gama;,citation_author=Raquel Sebastião;,citation_author=Pedro Pereira Rodrigues;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=3;,citation_volume=90;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Scalable Strategies for Computing with Massive Data;,citation_author=Michael J. Kane;,citation_author=John Emerson;,citation_author=Stephen Weston;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=14;,citation_volume=55;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=RStorm: Simulate and Develop Streaming Processing in R;,citation_author=Maurits Kaptein;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;">
<meta name="citation_reference" content="citation_title=Drift detection using uncertainty distribution divergence;,citation_author=Patrick Lindstrom;,citation_author=Brian Mac Namee;,citation_author=Sarah Jane Delany;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=1;,citation_volume=4;,citation_journal_title=Evolving Systems;">
<meta name="citation_reference" content="citation_title=Ad Click Prediction: A View from the Trenches;,citation_author=H. Brendan McMahan;,citation_author=Gary Holt;,citation_author=D. Sculley;,citation_author=Michael Young;,citation_author=Dietmar Ebner;,citation_author=Julian Grady;,citation_author=Lan Nie;,citation_author=Todd Phillips;,citation_author=Eugene Davydov;,citation_author=Daniel Golovin;,citation_author=Sharat Chikkerur;,citation_author=Dan Liu;,citation_author=Martin Wattenberg;,citation_author=Arnar Mar Hrafnkelsson;,citation_author=Tom Boulos;,citation_author=Jeremy Kubica;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’13;">
<meta name="citation_reference" content="citation_title=Data Stream Clustering: A Survey.;,citation_author=Jonathan A. Silva;,citation_author=Elaine R. Faria;,citation_author=Rodrigo C. Barros;,citation_author=Eduardo R. Hruschka;,citation_author=Andre Carvalho;,citation_author=Joao Gama;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=1;,citation_volume=46;,citation_journal_title=ACM Computer Surveys;">
<meta name="citation_reference" content="citation_title=ff: Memory-efficient Storage of Large Data on Disk and Fast Access Functions;,citation_author=Daniel Adler;,citation_author=Christian Gläser;,citation_author=Oleg Nenadic;,citation_author=Jens Oehlschlägel;,citation_author=Walter Zucchini;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=factas: Data Mining Methods for Data Streams;,citation_author=Romain Bar;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=streamR: Access to Twitter Streaming API via R;,citation_author=Pablo Barbera;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=DBI: R Database Interface;,citation_author=R Special Interest Group on Databases;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Concept drift detection through resampling;,citation_author=Maayan Harel;,citation_author=Shie Mannor;,citation_author=Ran El-Yaniv;,citation_author=Koby Crammer;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=PCA feature extraction for change detection in multidimensional unlabeled data;,citation_author=Ludmila I Kuncheva;,citation_author=William J Faithfull;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=1;,citation_volume=25;,citation_journal_title=IEEE transactions on neural networks and learning systems;">
<meta name="citation_reference" content="citation_title=Mining of Massive Datasets;,citation_author=Jure Leskovec;,citation_author=Anand Rajaraman;,citation_author=Jeffery D. Ullman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RMOA: Connect R with MOA to perform streaming classifications;,citation_author=Jan Wijffels;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A Survey on Concept Drift Adaptation;,citation_author=João Gama;,citation_author=Indrundefined Žliobaitundefined;,citation_author=Albert Bifet;,citation_author=Mykola Pechenizkiy;,citation_author=Abdelhamid Bouchachia;,citation_publication_date=2014-03;,citation_cover_date=2014-03;,citation_year=2014;,citation_issue=4;,citation_volume=46;,citation_journal_title=ACM Comput. Surv.;">
<meta name="citation_reference" content="citation_title=Graph Stream Algorithms: A Survey;,citation_author=Andrew McGregor;,citation_publication_date=2014-05;,citation_cover_date=2014-05;,citation_year=2014;,citation_issue=1;,citation_volume=43;,citation_journal_title=SIGMOD Rec.;">
<meta name="citation_reference" content="citation_title=RStorm: Developing and Testing Streaming Algorithms in R;,citation_author=Maurits Kaptein;,citation_publication_date=2014-06;,citation_cover_date=2014-06;,citation_year=2014;,citation_issue=1;,citation_volume=6;,citation_journal_title=The R Journal;">
<meta name="citation_reference" content="citation_title=SNAP Datasets: Stanford Large Network Dataset Collection;,citation_author=Jure Leskovec;,citation_author=Andrej Krevl;,citation_publication_date=2014-06;,citation_cover_date=2014-06;,citation_year=2014;,citation_publisher=http://snap.stanford.edu/data;">
<meta name="citation_reference" content="citation_title=Questioning the Lambda Architecture;,citation_author=Jay Kreps;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Sketching as a Tool for Numerical Linear Algebra;,citation_author=David P. Woodruff;,citation_publication_date=2014-10;,citation_cover_date=2014-10;,citation_year=2014;,citation_issue=1–2;,citation_volume=10;,citation_journal_title=Found. Trends Theor. Comput. Sci.;">
<meta name="citation_reference" content="citation_title=Modeling concept drift: A probabilistic graphical model based approach;,citation_author=Hanen Borchani;,citation_author=Ana Maria Martinez;,citation_author=Andrés R. Masegosa;,citation_author=Helge Langseth;,citation_author=Thomas Dyhre Nielsen;,citation_author=Antonio Salmerón;,citation_author=Antonio Fernández;,citation_author=Anders Læsø Madsen;,citation_author=Ramón Sáez;,citation_editor=Elisa Fromont;,citation_editor=Tijl De Bie;,citation_editor=Matthijs Leeuwen;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=Advances in Intelligent Data Analysis XIV;,citation_conference=Springer;,citation_series_title=Lecture Notes in Computer Science;">
<meta name="citation_reference" content="citation_title=twitteR: R Based Twitter Client;,citation_author=Jeff Gentry;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=rEMM: Extensible Markov Model for Data Stream Clustering in R;,citation_author=Michael Hahsler;,citation_author=Margaret H. Dunham;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=streamMOA: Interface for MOA Stream Clustering Algorithms;,citation_author=Michael Hahsler;,citation_author=Matthew Bolanos;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=rstream: Streams of Random Numbers;,citation_author=Josef Leydold;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Big Data: Principles and Best Practices of Scalable Realtime Data Systems;,citation_author=Nathan Marz;,citation_author=James Warren;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=A pca-based change detection framework for multidimensional data streams: Change detection in multidimensional data streams;,citation_author=Abdulhakim A Qahtan;,citation_author=Basma Alharbi;,citation_author=Suojin Wang;,citation_author=Xiangliang Zhang;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;">
<meta name="citation_reference" content="citation_title=Concept Drift Detection for Streaming Data;,citation_author=Heng Wang;,citation_author=Zubin Abraham;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=International Joint Conference on Neural Networks (IJCNN);">
<meta name="citation_reference" content="citation_title=koaning.io: Linear Models Solving Non-Linear Problems;,citation_author=Vincent Warmerdam;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Experimental Algorithmics Applied to On-line Machine Learning;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Gregor Papa;,citation_editor=Marjan Mernik;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=Bioinspired Optimization Methods and their Applications;">
<meta name="citation_reference" content="citation_title=quantmod: Quantitative Financial Modelling Framework;,citation_author=Jeffrey A. Ryan;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=A grid density based framework for classifying streaming data in the presence of concept drift;,citation_author=Tegjyot Singh Sethi;,citation_author=Mehmed Kantardzic;,citation_author=Hanquing Hu;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=46;,citation_journal_title=Journal of Intelligent Information Systems;">
<meta name="citation_reference" content="citation_title=rJava: Low-level R to Java interface;,citation_author=Simon Urbanek;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=SparkR: Scaling R Programs with Spark;,citation_author=Shivaram Venkataraman;,citation_author=Zongheng Yang;,citation_author=Davies Liu;,citation_author=Eric Liang;,citation_author=Hossein Falaki;,citation_author=Xiangrui Meng;,citation_author=Reynold Xin;,citation_author=Ali Ghodsi;,citation_author=Michael Franklin;,citation_author=Ion Stoica;,citation_author=Matei Zaharia;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=Proceedings of the 2016 International Conference on Management of Data;,citation_conference=Association for Computing Machinery;,citation_series_title=SIGMOD ’16;">
<meta name="citation_reference" content="citation_title=koaning.io: Bayesian/Streaming Algorithms;,citation_author=Vincent Warmerdam;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=Outlier Analysis;,citation_author=Charu C Aggarwal;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Video Popularity Prediction in Data Streams Based on Context-Independent Features;,citation_author=Vitor Silva;,citation_author=Ana Trindade Winck;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_conference_title=Proceedings of the Symposium on Applied Computing;,citation_conference=Association for Computing Machinery;,citation_series_title=SAC ’17;">
<meta name="citation_reference" content="citation_title=Einsatz von Machine-Learning-Verfahren in amtlichen Unternehmensstatistiken;,citation_author=Florian Dumpert;,citation_author=Martin Beck;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=2;,citation_volume=11;,citation_journal_title=AStA Wirtschafts- und Sozialstatistisches Archiv;">
<meta name="citation_reference" content="citation_title=Introduction to stream: An Extensible Framework for Data Stream Clustering Research with R;,citation_author=Michael Hahsler;,citation_author=Matthew Bolaños;,citation_author=John Forrest;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=14;,citation_volume=76;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=stream: Infrastructure for Data Stream Mining;,citation_author=Michael Hahsler;,citation_author=Matthew Bolaños;,citation_author=John Forrest;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Design and Analysis of Experiments;,citation_author=D C Montgomery;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Time Series Forecasting in the Presence of Concept Drift: A PSO-based Approach;,citation_author=Gustavo H. F. M. Oliveira;,citation_author=Rodolfo C. Cavalcante;,citation_author=George G. Cabral;,citation_author=Leandro L. Minku;,citation_author=Adriano L. I. Oliveira;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_conference_title=2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI);">
<meta name="citation_reference" content="citation_title=United Nations Global Pulse. Harnessing big data for development and humanitarian action.;,citation_author=United Nations;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=koaning.io: Passive Agressive Algorithms;,citation_author=Vincent Warmerdam;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=On the Reliable Detection of Concept Drift from Streaming Unlabeled Data;,citation_author=Tegjyot Singh Sethi;,citation_author=Mehmed Kantardzic;,citation_publication_date=2017-03;,citation_cover_date=2017-03;,citation_year=2017;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Proof of Concept Machine Learning - Abschlussbericht;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Jörg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_technical_report_institution=Statistisches Bundesamt (Destatis);">
<meta name="citation_reference" content="citation_title=Machine Learning for Data Streams with Practical Examples in MOA;,citation_author=Albert Bifet;,citation_author=Ricard Gavalda;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Lifelong Machine Learning;,citation_author=Zhiyuan Chen;,citation_author=Bing Liu;,citation_author=Ronald Brachman;,citation_author=Peter Stone;,citation_author=Francesca Rossi;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Incremental on-line learning: A review and comparison of state of the art algorithms;,citation_author=Viktor Losing;,citation_author=Barbara Hammer;,citation_author=Heiko Wersing;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=275;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Extremely Fast Decision Tree;,citation_author=Chaitanya Manapragada;,citation_author=Geoffrey I. Webb;,citation_author=Mahsa Salehi;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’18;">
<meta name="citation_reference" content="citation_title=An evaluation of data stream clustering algorithms;,citation_author=Stratos Mansalis;,citation_author=Eirini Ntoutsi;,citation_author=Nikos Pelekis;,citation_author=Yannis Theodoridis;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_volume=11;,citation_journal_title=Statistical Analysis and Data Mining: The ASA Data Science Journal;">
<meta name="citation_reference" content="citation_title=koaning.io: How to win with simple, even linear, models;,citation_author=Vincent Warmerdam;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Anomaly Detection in Manufacturing Systems Using Structured Neural Networks;,citation_author=J. Liu;,citation_author=J. Guo;,citation_author=P. V. Orlik;,citation_author=M. Shibata;,citation_author=D. Nakahara;,citation_author=S. Mii;,citation_author=M. Takac;,citation_publication_date=2018-07;,citation_cover_date=2018-07;,citation_year=2018;,citation_technical_report_institution=MITSUBISHI ELECTRIC RESEARCH LABORATORIES;">
<meta name="citation_reference" content="citation_title=Industrial Internet of Things Based Ransomware Detection Using Stacked Variational Neural Network;,citation_author=Muna Al-Hawawreh;,citation_author=Elena Sitnikova;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=Proceedings of the 3rd International Conference on Big Data and Internet of Things;,citation_conference=Association for Computing Machinery;,citation_series_title=BDIOT 2019;">
<meta name="citation_reference" content="citation_title=Evaluation of Cognitive Architectures for Cyber-Physical Production Systems;,citation_author=Andreas Bunte;,citation_author=Andreas Fischbach;,citation_author=Jan Strohschein;,citation_author=Thomas Bartz-Beielstein;,citation_author=Heide Faeskorn-Woyke;,citation_author=Oliver Niggemann;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=24th IEEE International Conference on Emerging Technologies and Factory Automation, ETFA 2019, Zaragoza, Spain, September 10-13, 2019;">
<meta name="citation_reference" content="citation_title=Nowcasting: Ein Echtzeit-Indikator für die Konjunkturanalyse;,citation_author=Charlotte Senftleben;,citation_author=Till Strohsal;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=koaning.io: The Future of Data Science is Past;,citation_author=Vincent Warmerdam;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Forecasting inflation with online prices;,citation_author=Diego Aparicio;,citation_author=Manuel I. Bertolotto;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_volume=36;,citation_journal_title=International Journal of Forecasting;">
<meta name="citation_reference" content="citation_title=CAAI—a cognitive architecture to introduce artificial intelligence in cyber-physical production systems;,citation_author=Andreas Fischbach;,citation_author=Jan Strohschein;,citation_author=Andreas Bunte;,citation_author=Jörg Stork;,citation_author=Heide Faeskorn-Woyke;,citation_author=Natalia Moriz;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=1;,citation_volume=111;,citation_journal_title=The International Journal of Advanced Manufacturing Technology;">
<meta name="citation_reference" content="citation_title=Delayed labelling evaluation for data streams;,citation_author=Maciej Grzenda;,citation_author=Heitor Murilo Gomes;,citation_author=Albert Bifet;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=5;,citation_volume=34;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Stream Data Mining: Algorithms and Their Probabilistic Properties;,citation_editor=Leszek Rutkowski;,citation_editor=Maciej Jaworski;,citation_editor=Piotr Duda;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_series_title=Studies in Big Data;">
<meta name="citation_reference" content="citation_title=Loghub: A Large Collection of System Log Datasets towards Automated Log Analytics;,citation_author=Shilin He;,citation_author=Jieming Zhu;,citation_author=Pinjia He;,citation_author=Michael R. Lyu;,citation_publication_date=2020-08;,citation_cover_date=2020-08;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Nowcasting German GDP: Foreign factors, financial markets, and model averaging;,citation_author=Paolo Andreini;,citation_author=Thomas Hasenzagl;,citation_author=Lucrezia Reichlin;,citation_author=Charlotte Senftleben-König;,citation_author=Till Strohsal;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=International Journal of Forecasting;">
<meta name="citation_reference" content="citation_title=Modelling the COVID-19 Virus Evolution With Incremental Machine Learning;,citation_author=Andrés L. Suárez-Cetrulo;,citation_author=Ankit Kumar;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Machine Learning for Time-Series with Python: Forecast, predict, and detect;,citation_author=Den Auffarth;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Machine Learning in der amtlichen Statistik - Ergebnisse und Bewertung eines internationalen Projekts;,citation_author=Florian Dumpert;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=Statistisches Bundesamt (Destatis);,citation_technical_report_number=4;">
<meta name="citation_reference" content="citation_title=Recurring concept memory management in data streams: exploiting data stream concept evolution to improve performance and transparency;,citation_author=Ben Halstead;,citation_author=Yun Sing Koh;,citation_author=Patricia Riddle;,citation_author=Russel Pears;,citation_author=Mykola Pechenizkiy;,citation_author=Albert Bifet;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=3;,citation_volume=35;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Soziale Marktwirtschaft in der digitalen Zukunft: Foresight-Bericht Strategischer Vorausschauprozess des BMWi;,citation_author=Dirk Holtmannspötter;,citation_author=Ulrich Heimeshoff;,citation_author=Justus Haucap;,citation_author=Ina Loebert;,citation_author=Christoph Busch;,citation_author=Andreas Hoffknecht;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=VDI Technologiezentrum GmbH im Auftrag des Bundesministerium für Wirtschaft und Energie;">
<meta name="citation_reference" content="citation_title=River: machine learning for streaming data in Python;,citation_author=Jacob Montiel;,citation_author=Max Halford;,citation_author=Saulo Martiello Mastelini;,citation_author=Geoffrey Bolmier;,citation_author=Raphael Sourty;,citation_author=Robin Vaysse;,citation_author=Adil Zouitine;,citation_author=Heitor Murilo Gomes;,citation_author=Jesse Read;,citation_author=Talel Abdessalem;,citation_author=others;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Practical Machine Learning for Streaming Data with Python;,citation_author=Sayan Putatunda;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Infrerring Concept Drift Without Labeled Data;,citation_author=Andrew Reed;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=Cloudera Fast Forward Labs;,citation_technical_report_number=FF21;">
<meta name="citation_reference" content="citation_title=Cognitive capabilities for the CAAI in cyber-physical production systems;,citation_author=Jan Strohschein;,citation_author=Andreas Fischbach;,citation_author=Andreas Bunte;,citation_author=Heide Faeskorn-Woyke;,citation_author=Natalia Moriz;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=The International Journal of Advanced Manufacturing Technology;">
<meta name="citation_reference" content="citation_title=FARF: A Fair and Adaptive Random Forests Classifier;,citation_author=Wenbin Zhang;,citation_author=Albert Bifet;,citation_author=Xiangliang Zhang;,citation_author=Jeremy C. Weiss;,citation_author=Wolfgang Nejdl;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part II;,citation_conference=Springer-Verlag;">
<meta name="citation_reference" content="citation_title=Modelling the COVID-19 virus evolution with Incremental Machine Learning;,citation_author=Andrés L. Suárez-Cetrulo;,citation_author=Ankit Kumar;,citation_author=Luis Miralles-Pechuán;,citation_publication_date=2021-04;,citation_cover_date=2021-04;,citation_year=2021;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Online Learning and Active Learning: A Comparative Study of Passive-Aggressive Algorithm With Support Vector Machine (SVM);,citation_author=K. I Ezukwoke;,citation_author=S. J Zareian;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=3;,citation_volume=21;,citation_journal_title=Journal of Higher Education Theory and Practice;">
<meta name="citation_reference" content="citation_title=Digitale Ordnungspolitik – Wirtschaftspolitik daten- und evidenzbasiert weiterentwickeln;,citation_author=Philipp Steinberg;,citation_author=Nils Börnsen;,citation_author=Dirk Neumann;,citation_publication_date=2021-09;,citation_cover_date=2021-09;,citation_year=2021;,citation_publisher=Wirtschaftsdienst;">
<meta name="citation_reference" content="citation_title=Incremental Unsupervised Domain-Adversarial Training of Neural Networks;,citation_author=Antonio-Javier Gallego;,citation_author=Jorge Calvo-Zaragoza;,citation_author=Robert B. Fisher;,citation_publication_date=2021-11;,citation_cover_date=2021-11;,citation_year=2021;,citation_issue=11;,citation_volume=32;,citation_journal_title=IEEE Transactions on Neural Networks and Learning Systems;">
<meta name="citation_reference" content="citation_title=Incremental learning for property price estimation using location-based services and open data;,citation_author=Francisco Alvarez;,citation_author=Edgar Roman-Rangel;,citation_author=Luis V. Montiel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_volume=107;,citation_journal_title=Engineering Applications of Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Interpretable Machine Learning: Moving from Mythos to Diagnostics;,citation_author=Valerie Chen;,citation_author=Jeffrey Li;,citation_author=Joon Sik Kim;,citation_author=Gregory Plumb;,citation_author=Ameet Talwalkar;,citation_publication_date=2022-01;,citation_cover_date=2022-01;,citation_year=2022;,citation_issue=6;,citation_volume=19;,citation_journal_title=Queue;">
<meta name="citation_reference" content="citation_title=Online Time-series Anomaly Detection: A Survey of Modern Model-based Approaches;,citation_author=Lucas Correia;,citation_author=Jan-Christoph Goos;,citation_author=Anna V. Kononova;,citation_author=Thomas Bäck;,citation_author=Philipp Klein;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_technical_report_institution=Mercedes-Benz, Germany;">
<meta name="citation_reference" content="citation_title=Green Accelerated Hoeffding Tree;,citation_author=Eva Garcia-Martin;,citation_author=Albert Bifet;,citation_author=Niklas Lavesson;,citation_author=Rikard König;,citation_author=Henrik Linusson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Monitoring the Economy in Real Time: Trends and Gaps in Real Activity and Prices;,citation_author=Thomas Hasenzagl;,citation_author=Filippo Pellegrino;,citation_author=Lucrezia Reichlin;,citation_author=Giovanni Ricco;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Efficiently Correcting Machine Learning: Considering the Role of Example Ordering in Human-in-the-Loop Training of Image Classification Models;,citation_author=Geoff Holmes;,citation_author=Eibe Frank;,citation_author=Dale Fletcher;,citation_author=Corey Sterling;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=27th International Conference on Intelligent User Interfaces;,citation_conference=Association for Computing Machinery;,citation_series_title=IUI ’22;">
<meta name="citation_reference" content="citation_title=TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models;,citation_author=Joel Jang;,citation_author=Seonghyeon Ye;,citation_author=Changho Lee;,citation_author=Sohee Yang;,citation_author=Joongbo Shin;,citation_author=Janghoon Han;,citation_author=Gyeonghun Kim;,citation_author=Minjoon Seo;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Fast Mining and Forecasting of Co-Evolving Epidemiological Data Streams;,citation_author=Tasuku Kimura;,citation_author=Yasuko Matsubara;,citation_author=Koki Kawabata;,citation_author=Yasushi Sakurai;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’22;">
<meta name="citation_reference" content="citation_title=Maschine Learning for Streaming Data with Python;,citation_author=Jan Korstanje;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=The Disagreement Problem in Explainable Machine Learning: A Practitioner’s Perspective;,citation_author=Satyapriya Krishna;,citation_author=Tessa Han;,citation_author=Alex Gu;,citation_author=Javin Pombra;,citation_author=Shahin Jabbari;,citation_author=Steven Wu;,citation_author=Himabindu Lakkaraju;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=SparkR: R Front End for ’Apache Spark’;,citation_author=The Apache Software Foundation;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Short-term local predictions of COVID-19 in the United Kingdom using dynamic supervised machine learning algorithms;,citation_author=Xin Wang;,citation_author=Yijia Dong;,citation_author=William David Thompson;,citation_author=Harish Nair;,citation_author=You Li;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=1;,citation_volume=2;,citation_journal_title=Communications Medicine;">
<meta name="citation_reference" content="citation_title=Log-based Anomaly Detection with Deep Learning: How Far Are We?;,citation_author=Van-Hoang Le;,citation_author=Hongyu Zhang;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Reliance on metrics is a fundamental challenge for AI.;,citation_author=Rachel L Thomas;,citation_author=David Uminsky;,citation_publication_date=2022-05;,citation_cover_date=2022-05;,citation_year=2022;,citation_issue=5;,citation_volume=3;,citation_journal_title=Patterns (N Y);">
<meta name="citation_reference" content="citation_title=SMRD.de Benutzerhandbuch;,citation_author=Niyaz Valitov;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_technical_report_institution=Bundesnetzagentur für Elektrizität, Gas, Telekommunikation, Post und Eisenbahnen;">
<meta name="citation_reference" content="citation_title=Use of web scraping and text mining techniques in the Istat survey on “Information and Communication Technology in enterprises”;,citation_author=G. Barcaroli;,citation_author=A. Nurra;,citation_author=M. Scarnò;,citation_author=D. Summa;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Who Makes Mistakes? Using Data Mining Techniques to Analyze Reporting Errors in Total Acres Operated;,citation_author=Jaki S. McCarthy;,citation_author=Morgan S. Earp;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_fulltext_html_url=https://ideas.repec.org/p/ags/unasrr/234367.html;,citation_doi=10.22004/AG.ECON.234367;,citation_journal_title=NASS Research Reports;,citation_publisher=United States Department of Agriculture, National Agricultural Statistics Service;">
<meta name="citation_reference" content="citation_title=Modeling Non-response in National Agricultural Statistics Service (NASS) Surveys Using Classification Trees;,citation_author=Jaki S Mccarthy;,citation_author=Thomas Jacob;,citation_author=Amanda Mccracken;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;">
<meta name="citation_reference" content="citation_title=2007 Census of Agriculture Non-Response Methodology;,citation_author=Will Cecere;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=Exploring Quarterly Agricultural Survey Questionnaire Version Reduction Scenarios;,citation_author=Morgan Earp;,citation_author=Scott Cox;,citation_author=Jody Mcdaniel;,citation_author=Chadd Crouse;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=USE OF MACHINE LEARNING METHODS TO IMPUTE CATEGORICAL DATA;,citation_author=P. Rey;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Innovative Uses of Data Mining Techniques in the Production of Official Statistics;,citation_author=Jaki Mccarthy;,citation_author=Thomas Jacob;,citation_author=Dale Atkinson;">
<meta name="citation_reference" content="citation_title=Automatic Coding of Occupations. Using Machine Learning Algorithms for Occupation Coding in Several German Panel Surveys;,citation_fulltext_html_url=https://www.researchgate.net/publication/266259591_Automatic_Coding_of_Occupations_Using_Machine_Learning_Algorithms_for_Occupation_Coding_in_Several_German_Panel_Surveys;">
<meta name="citation_reference" content="citation_title=Evaluating hourly air quality forecasting in Canada with nonlinear updatable machine learning methods;,citation_author=Huiping Peng;,citation_author=Aranildo R. Lima;,citation_author=Andrew Teakles;,citation_author=Jian Jin;,citation_author=Alex J. Cannon;,citation_author=William W. Hsieh;,citation_publication_date=2017-03-01;,citation_cover_date=2017-03-01;,citation_year=2017;,citation_fulltext_html_url=https://doi.org/10.1007/s11869-016-0414-3;,citation_issue=2;,citation_doi=10.1007/s11869-016-0414-3;,citation_issn=1873-9326;,citation_volume=10;,citation_journal_title=Air Quality, Atmosphere &amp;amp;amp; Health;">
<meta name="citation_reference" content="citation_title=Online Machine Learning in Big Data Streams;,citation_author=András A. Benczúr;,citation_author=Levente Kocsis;,citation_author=Róbert Pálovics;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://arxiv.org/abs/1802.05872;,citation_volume=abs/1802.05872;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=Learn: A Novel incremental learning method for text classification;,citation_author=Guangxu Shan;,citation_author=Shiyao Xu;,citation_author=Li Yang;,citation_author=Shengbin Jia;,citation_author=Yang Xiang;,citation_publication_date=2020-06;,citation_cover_date=2020-06;,citation_year=2020;,citation_doi=10.1016/J.ESWA.2020.113198;,citation_issn=0957-4174;,citation_volume=147;,citation_journal_title=Expert Systems with Applications;,citation_publisher=Pergamon;">
<meta name="citation_reference" content="citation_title=Incremental Real-Time Learning Framework for Sentiment Classification: Indian General Election 2019, A Case Study;,citation_author=Sharmistha Chatterjee;,citation_author=Sushmita Gupta;,citation_publication_date=2021-03;,citation_cover_date=2021-03;,citation_year=2021;,citation_doi=10.1109/ICBDA51983.2021.9402992;,citation_isbn=9780738131672;,citation_journal_title=2021 IEEE 6th International Conference on Big Data Analytics, ICBDA 2021;,citation_publisher=Institute of Electrical; Electronics Engineers Inc.;">
<meta name="citation_reference" content="citation_title=MOA: Massive Online Analysis;,citation_abstract=Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Na¨ıveNa¨ıve Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis , and is released under the GNU GPL license.;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Richard Kirkby;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=11;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Evaluation and Performance Measurement;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning Approaches;,citation_author=Thomas Bartz-Beielstein;,citation_author=Martin Zaefferer;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_inbook_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;">
<meta name="citation_reference" content="citation_title=Drift Detection and&nbsp;Handling;,citation_abstract=Structural changes (“drift”) in the data cause problems for many algorithms. Based on the drift definitions given in Chap. 1, methods for drift detection and handling are discussed. For the algorithms presented in Chap. 2, it is clarified to what extent concept drift is reacted to. In turn, the extent to which catastrophic forgetting is an issue is described in Sect. 4.3. Section&nbsp;3.1 describes three architectures for implementing drift detection algorithms. Basic properties of window-based approaches are presented in Sect.&nbsp;3.2. Section&nbsp;3.4 presents commonly used drift detection techniques. Section&nbsp;3.4 describes how the drift detection techniques introduced in Sect.&nbsp;3.3 are used in Online Machine Learning (OML) algorithms and summarizes the tree-based OML techniques implemented in the River package. Section&nbsp;3.5 introduces scaling methods for handling drift.;,citation_author=Thomas Bartz-Beielstein;,citation_author=Lukas Hans;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1007/978-981-99-7007-0_3;,citation_doi=10.1007/978-981-99-7007-0_3;,citation_isbn=978-981-99-7007-0;,citation_inbook_title=Online Machine Learning: A Practical Guide with Examples in Python;">
<meta name="citation_reference" content="citation_title=Introduction: From Batch to&nbsp;Online Machine Learning;,citation_abstract=Batch Machine Learning (BML), which is also referred to as “offline machine learning”, reaches its limits when dealing with very large amounts of data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. In this chapter, the basic terms and concepts of OML are introduced and the differences to BML are shown.;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1007/978-981-99-7007-0_1;,citation_doi=10.1007/978-981-99-7007-0_1;,citation_isbn=978-981-99-7007-0;,citation_inbook_title=Online Machine Learning: A Practical Guide with Examples in Python;">
<meta name="citation_reference" content="citation_title=AMF: Aggregated Mondrian Forests for Online Learning;,citation_author=Jaouad Mourtada;,citation_author=Stephane Gaiffas;,citation_author=Erwan Scornet;,citation_publication_date=2019-06;,citation_cover_date=2019-06;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1906.10529;,citation_doi=10.48550/arXiv.1906.10529;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Extremely fast decision tree;,citation_abstract=We introduce a novel incremental decision tree learning algorithm, Hoeffding Anytime Tree, that is statistically more efficient than the current state-of-the-art, Hoeffding Tree. We demonstrate that an implementation of Hoeffding Anytime Tree’“Extremely Fast Decision Tree”, a minor modification to the MOA implementation of Hoeffding Tree’obtains significantly superior prequential accuracy on most of the largest classification datasets from the UCI repository. Hoeffding Anytime Tree produces the asymptotic batch tree in the limit, is naturally resilient to concept drift, and can be used as a higher accuracy replacement for Hoeffding Tree in most scenarios, at a small additional computational cost.;,citation_author=Chaitanya Manapragada;,citation_author=Geoffrey I. Webb;,citation_author=Mahsa Salehi;,citation_editor=Chih-Jen  Lin;,citation_editor=Hui  Xiong;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://www.kdd.org/kdd2018/, https://dl.acm.org/doi/proceedings/10.1145/3219819;,citation_doi=10.1145/3219819.3220005;,citation_conference_title=KDD’ 2018 - Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery (ACM);">
<meta name="citation_reference" content="citation_title=Surrogates;,citation_author=Robert B Gramacy;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=UvA Deep Learning Tutorials;,citation_author=Phillip Lippe;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://github.com/phlippe/uvadlc_notebooks/tree/master;">
<meta name="citation_reference" content="citation_title=Attention Is All You Need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017-06;,citation_cover_date=2017-06;,citation_year=2017;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=On the Variance of the Adaptive Learning Rate and Beyond;,citation_author=Liyuan Liu;,citation_author=Haoming Jiang;,citation_author=Pengcheng He;,citation_author=Weizhu Chen;,citation_author=Xiaodong Liu;,citation_author=Jianfeng Gao;,citation_author=Jiawei Han;,citation_publication_date=2019-08;,citation_cover_date=2019-08;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Efficient Transformers: A Survey;,citation_author=Yi Tay;,citation_author=Mostafa Dehghani;,citation_author=Dara Bahri;,citation_author=Donald Metzler;,citation_publication_date=2020-09;,citation_cover_date=2020-09;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding;,citation_author=Jacob Devlin;,citation_author=Ming-Wei Chang;,citation_author=Kenton Lee;,citation_author=Kristina Toutanova;,citation_publication_date=2018-10;,citation_cover_date=2018-10;,citation_year=2018;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale;,citation_author=Alexey Dosovitskiy;,citation_author=Lucas Beyer;,citation_author=Alexander Kolesnikov;,citation_author=Dirk Weissenborn;,citation_author=Xiaohua Zhai;,citation_author=Thomas Unterthiner;,citation_author=Mostafa Dehghani;,citation_author=Matthias Minderer;,citation_author=Georg Heigold;,citation_author=Sylvain Gelly;,citation_author=Jakob Uszkoreit;,citation_author=Neil Houlsby;,citation_publication_date=2020-10;,citation_cover_date=2020-10;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Attention is not Explanation;,citation_author=Sarthak Jain;,citation_author=Byron C. Wallace;,citation_publication_date=2019-02;,citation_cover_date=2019-02;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Attention is not not Explanation;,citation_author=Sarah Wiegreffe;,citation_author=Yuval Pinter;,citation_publication_date=2019-08;,citation_cover_date=2019-08;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Multivariate adaptive regression splines;,citation_author=Jerome H. Friedman;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_issue=1;,citation_volume=19;,citation_journal_title=The annals of statistics;">
<meta name="citation_reference" content="citation_title=Learning model trees from evolving data streams;,citation_author=Elena Ikonomovska;,citation_author=João Gama;,citation_author=Sašo Džeroski;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=1;,citation_volume=23;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=An Introduction to Statistical Learning with Applications in R;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Deep Learning with Python;,citation_author=Francoise Chollet;,citation_author=J. J. Allaire;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=A survey of cross-validation procedures for model selection;,citation_author=Sylvain Arlot;,citation_author=Alain Celisse;,citation_author=others;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=4;,citation_journal_title=Statistics surveys;">
<meta name="citation_reference" content="citation_title=A cross-validatory method for dependent data;,citation_author=PRABIR BURMAN;,citation_author=EDMOND CHOW;,citation_author=DEBORAH NOLAN;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=2;,citation_volume=81;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection;,citation_author=Ron Kohavi;,citation_publication_date=1995;,citation_cover_date=1995;,citation_year=1995;,citation_conference_title=Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2;,citation_conference=Morgan Kaufmann Publishers Inc.;,citation_series_title=IJCAI’95;">
<meta name="citation_reference" content="citation_title=Kriging-based sequential design strategies using fast cross-validation techniques with extensions to multi-fidelity computer codes ;,citation_author=Loic Le Gratiet;,citation_author=Claire Cannamela;,citation_publication_date=2012-10;,citation_cover_date=2012-10;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Cross-Validation of Regression Models;,citation_author=Richard R. Picard;,citation_author=R. Dennis Cook;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_issue=387;,citation_volume=79;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=The Elements of Statistical Learning;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_author=Jerome Friedman;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Deep Residual Learning for Image Recognition;,citation_author=Kaiming He;,citation_author=Xiangyu Zhang;,citation_author=Shaoqing Ren;,citation_author=Jian Sun;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Identity Mappings in Deep Residual Networks;,citation_author=Kaiming He;,citation_author=Xiangyu Zhang;,citation_author=Shaoqing Ren;,citation_author=Jian Sun;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Neural Ordinary Differential Equations;,citation_author=Ricky T. Q. Chen;,citation_author=Yulia Rubanova;,citation_author=Jesse Bettencourt;,citation_author=David Duvenaud;,citation_publication_date=2018-06;,citation_cover_date=2018-06;,citation_year=2018;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Mathematical Theory of Optimal Processes;,citation_author=undefined Pontryagin;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;">
<meta name="citation_reference" content="citation_title=On Neural Differential Equations;,citation_author=Patrick Kidger;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_journal_title=arXiv e-prints;">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./700_lightning_basic.html">Hyperparameter Tuning with PyTorch Lightning</a></li><li class="breadcrumb-item"><a href="./602_spot_lightning_xai.html"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hyperparameter Tuning Cookbook</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sequential-parameter-optimization/spotpython" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Hyperparameter-Tuning-Cookbook.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_optimization_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002_awwe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Aircraft Wing Weight Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003_scipy_optimize_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to <code>scipy.optimize</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004_spot_sklearn_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Using <code>scipy</code> Optimizers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Numerical Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_num_rsm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction: Numerical Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_num_spot_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to spotpython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008_num_spot_multidim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multi-dimensional Functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009_num_spot_anisotropic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Isotropic and Anisotropic Kriging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010_num_spot_sklearn_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Using <code>sklearn</code> Surrogates in <code>spotpython</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_num_spot_sklearn_gaussian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Gaussian Process Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_num_spot_ei.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Expected Improvement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_num_spot_noisy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Handling Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014_num_spot_ocba.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Optimal Computational Budget Allocation in <code>Spot</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_num_spot_correlation_p.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Kriging with Varying Correlation-p</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016_num_spot_factorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Factorial Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017_num_spot_user_function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">User-Specified Functions: Extending the Analytical Class</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Data-Driven Modeling and Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_ddmo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Data-Driven Modeling and Optimization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning and AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./200_mlai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Machine Learning and Artificial Intelligence</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Hyperparameter Tuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./300_hpt_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with Sklearn</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./400_spot_hpt_sklearn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">HPT: sklearn</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./401_spot_hpt_sklearn_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">HPT: sklearn SVC on Moons Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with River</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./500_spot_hpt_river.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">HPT: River</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./501_spot_river_gui.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Simplifying Hyperparameter Tuning in Online Machine Learning—The spotRiverGUI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./502_spot_hpt_river_friedman_htr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title"><code>river</code> Hyperparameter Tuning: Hoeffding Tree Regressor with Friedman Drift Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./503_spot_hpt_river_friedman_amfr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">The Friedman Drift Data Set</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with PyTorch Lightning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./700_lightning_basic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Basic Lightning Module</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./701_lightning_details.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Details of the Lightning Module Integration in spotpython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./702_lightning_user_datamodule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">User Specified Basic Lightning Module With spotpython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./600_spot_lightning_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_diabetes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_user_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning and User Data Sets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_user_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning and User Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning: ResNets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_neural_ode.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Neural ODEs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_neural_ode_example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Neural ODE Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_pinn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Physics Informed Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_pinn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning: Physics Informed Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./602_spot_lightning_xai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./603_spot_lightning_transformer_introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning Transformer: Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./603_spot_lightning_transformer_hpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning of a Transformer Network with PyTorch Lightning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./604_spot_lightning_save_load_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Saving and Loading</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./605_spot_hpt_light_diabetes_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set Using a ResNet Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./606_spot_hpt_light_diabetes_user_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set Using a User Specified ResNet Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./608_spot_hpt_light_condnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning Using a CondNet Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_01_intro_to_notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Introduction to Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_02_git_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Git Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_03_python_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Python Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_04_spot_doc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Documentation of the Sequential Parameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_05_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_06_slurm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Using Slurm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_07_package.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Python Package Building</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_99_solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Solutions to Selected Exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#running-the-hyperparameter-tuning-or-loading-the-existing-model" id="toc-running-the-hyperparameter-tuning-or-loading-the-existing-model" class="nav-link active" data-scroll-target="#running-the-hyperparameter-tuning-or-loading-the-existing-model"><span class="header-section-number">39.1</span> Running the Hyperparameter Tuning or Loading the Existing Model</a></li>
  <li><a href="#results-from-the-hyperparameter-tuning-experiment" id="toc-results-from-the-hyperparameter-tuning-experiment" class="nav-link" data-scroll-target="#results-from-the-hyperparameter-tuning-experiment"><span class="header-section-number">39.2</span> Results from the Hyperparameter Tuning Experiment</a>
  <ul class="collapse">
  <li><a href="#getting-the-best-model-i.e-the-tuned-architecture" id="toc-getting-the-best-model-i.e-the-tuned-architecture" class="nav-link" data-scroll-target="#getting-the-best-model-i.e-the-tuned-architecture"><span class="header-section-number">39.2.1</span> Getting the Best Model, i.e, the Tuned Architecture</a></li>
  </ul></li>
  <li><a href="#training-the-tuned-architecture-on-the-test-data" id="toc-training-the-tuned-architecture-on-the-test-data" class="nav-link" data-scroll-target="#training-the-tuned-architecture-on-the-test-data"><span class="header-section-number">39.3</span> Training the Tuned Architecture on the Test Data</a></li>
  <li><a href="#visualizing-the-neural-network-architecture" id="toc-visualizing-the-neural-network-architecture" class="nav-link" data-scroll-target="#visualizing-the-neural-network-architecture"><span class="header-section-number">39.4</span> Visualizing the Neural Network Architecture</a></li>
  <li><a href="#xai-methods" id="toc-xai-methods" class="nav-link" data-scroll-target="#xai-methods"><span class="header-section-number">39.5</span> XAI Methods</a>
  <ul class="collapse">
  <li><a href="#weights" id="toc-weights" class="nav-link" data-scroll-target="#weights"><span class="header-section-number">39.5.1</span> Weights</a></li>
  <li><a href="#activations" id="toc-activations" class="nav-link" data-scroll-target="#activations"><span class="header-section-number">39.5.2</span> Activations</a></li>
  <li><a href="#gradients" id="toc-gradients" class="nav-link" data-scroll-target="#gradients"><span class="header-section-number">39.5.3</span> Gradients</a></li>
  <li><a href="#getting-the-weights" id="toc-getting-the-weights" class="nav-link" data-scroll-target="#getting-the-weights"><span class="header-section-number">39.5.4</span> Getting the Weights</a></li>
  <li><a href="#getting-the-activations" id="toc-getting-the-activations" class="nav-link" data-scroll-target="#getting-the-activations"><span class="header-section-number">39.5.5</span> Getting the Activations</a></li>
  <li><a href="#getting-the-gradients" id="toc-getting-the-gradients" class="nav-link" data-scroll-target="#getting-the-gradients"><span class="header-section-number">39.5.6</span> Getting the Gradients</a></li>
  </ul></li>
  <li><a href="#feature-attributions" id="toc-feature-attributions" class="nav-link" data-scroll-target="#feature-attributions"><span class="header-section-number">39.6</span> Feature Attributions</a>
  <ul class="collapse">
  <li><a href="#integrated-gradients" id="toc-integrated-gradients" class="nav-link" data-scroll-target="#integrated-gradients"><span class="header-section-number">39.6.1</span> Integrated Gradients</a></li>
  <li><a href="#deep-lift" id="toc-deep-lift" class="nav-link" data-scroll-target="#deep-lift"><span class="header-section-number">39.6.2</span> Deep Lift</a></li>
  <li><a href="#feature-ablation" id="toc-feature-ablation" class="nav-link" data-scroll-target="#feature-ablation"><span class="header-section-number">39.6.3</span> Feature Ablation</a></li>
  </ul></li>
  <li><a href="#conductance" id="toc-conductance" class="nav-link" data-scroll-target="#conductance"><span class="header-section-number">39.7</span> Conductance</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./700_lightning_basic.html">Hyperparameter Tuning with PyTorch Lightning</a></li><li class="breadcrumb-item"><a href="./602_spot_lightning_xai.html"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="configure_spot" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.data.diabetes <span class="im">import</span> Diabetes</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.hyperdict.light_hyper_dict <span class="im">import</span> LightHyperDict</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.fun.hyperlight <span class="im">import</span> HyperLight</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.utils.init <span class="im">import</span> (fun_control_init, surrogate_control_init, design_control_init)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.utils.eda <span class="im">import</span> gen_design_table</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.spot <span class="im">import</span> spot</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.utils.<span class="bu">file</span> <span class="im">import</span> get_experiment_filename</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.hyperparameters.values <span class="im">import</span> set_hyperparameter</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> inf</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>PREFIX<span class="op">=</span><span class="st">"602_12"</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>data_set <span class="op">=</span> Diabetes()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> fun_control_init(</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    save_experiment<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    PREFIX<span class="op">=</span>PREFIX,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    fun_evals<span class="op">=</span>inf,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    max_time<span class="op">=</span><span class="dv">60</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    data_set <span class="op">=</span> data_set,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    core_model_name<span class="op">=</span><span class="st">"light.regression.NNLinearRegressor"</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    hyperdict<span class="op">=</span>LightHyperDict,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    _L_in<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    _L_out<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>fun <span class="op">=</span> HyperLight().fun</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>set_hyperparameter(fun_control, <span class="st">"optimizer"</span>, [ <span class="st">"Adadelta"</span>, <span class="st">"Adam"</span>, <span class="st">"Adamax"</span>])</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>set_hyperparameter(fun_control, <span class="st">"l1"</span>, [<span class="dv">3</span>,<span class="dv">7</span>])</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>set_hyperparameter(fun_control, <span class="st">"epochs"</span>, [<span class="dv">10</span>,<span class="dv">12</span>])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>set_hyperparameter(fun_control, <span class="st">"batch_size"</span>, [<span class="dv">4</span>,<span class="dv">11</span>])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>set_hyperparameter(fun_control, <span class="st">"dropout_prob"</span>, [<span class="fl">0.0</span>, <span class="fl">0.025</span>])</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>set_hyperparameter(fun_control, <span class="st">"patience"</span>, [<span class="dv">2</span>,<span class="dv">9</span>])</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>design_control <span class="op">=</span> design_control_init(init_size<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>spot_tuner <span class="op">=</span> spot.Spot(fun<span class="op">=</span>fun,fun_control<span class="op">=</span>fun_control, design_control<span class="op">=</span>design_control)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>module_name: light
submodule_name: regression
model_name: NNLinearRegressor
Experiment saved to 602_12_exp.pkl</code></pre>
</div>
</div>
<section id="running-the-hyperparameter-tuning-or-loading-the-existing-model" class="level2" data-number="39.1">
<h2 data-number="39.1" class="anchored" data-anchor-id="running-the-hyperparameter-tuning-or-loading-the-existing-model"><span class="header-section-number">39.1</span> Running the Hyperparameter Tuning or Loading the Existing Model</h2>
<div id="run_experiment" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.utils.<span class="bu">file</span> <span class="im">import</span> get_experiment_filename, load_experiment</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>overwrite <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>filename <span class="op">=</span> get_experiment_filename(PREFIX)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(filename) <span class="kw">and</span> <span class="kw">not</span> overwrite:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    (spot_tuner, fun_control, design_control,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    surrogate_control, optimizer_control) <span class="op">=</span> load_experiment(filename)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"File does not exist or overwrite is True. Starting new experiment."</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> spot_tuner.run()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># only needed for spotpython version &lt; 0.16.0</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>fun_control.update({<span class="st">"_L_cond"</span>: <span class="va">None</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>File does not exist or overwrite is True. Starting new experiment.

In fun(): config:
{'act_fn': ELU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.01495680533337224),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 16,
 'lr_mult': np.float64(0.20841207421887742),
 'optimizer': 'Adam',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.022750213555036855),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(8.688381162942978),
 'optimizer': 'Adadelta',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 3588.923095703125, 'hp_metric': 3588.923095703125}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0035182324401420283),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(5.090825861997777),
 'optimizer': 'Adam',
 'patience': 4}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 3321.471923828125, 'hp_metric': 3321.471923828125}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.00972190967750383),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 32,
 'lr_mult': np.float64(7.458466974783385),
 'optimizer': 'Adamax',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4960.18994140625, 'hp_metric': 4960.18994140625}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.020311634799104406),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(4.322386097111191),
 'optimizer': 'Adam',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4306.841796875, 'hp_metric': 4306.841796875}

In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.012759647245418036),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(1.747498108806452),
 'optimizer': 'Adadelta',
 'patience': 64}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 6326.5966796875, 'hp_metric': 6326.5966796875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.007063445400286148),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(6.507632456733881),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 4390.31494140625, 'hp_metric': 4390.31494140625}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0035114607338253704),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(5.090793020607),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 3203.63623046875, 'hp_metric': 3203.63623046875}
spotpython tuning: 3203.63623046875 [----------] 0.02% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(5.020658012285975),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 2992.884033203125, 'hp_metric': 2992.884033203125}
spotpython tuning: 2992.884033203125 [----------] 0.06% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adamax',
 'patience': 4}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.01767046792447351),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(7.586923464752967),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 5075.40087890625, 'hp_metric': 5075.40087890625}
spotpython tuning: 2992.884033203125 [----------] 0.37% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 57593.8828125, 'hp_metric': 57593.8828125}
spotpython tuning: 2992.884033203125 [----------] 0.43% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(8.612355738897312),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 3238.78076171875, 'hp_metric': 3238.78076171875}
spotpython tuning: 2992.884033203125 [----------] 0.47% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(3.6100119061929434e-05),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.602459793862582),
 'optimizer': 'Adadelta',
 'patience': 128}
train_model result: {'val_loss': 5438.15673828125, 'hp_metric': 5438.15673828125}
spotpython tuning: 2992.884033203125 [----------] 0.64% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(7.19353986989614e-06),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(7.286464402882227),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 24363.138671875, 'hp_metric': 24363.138671875}
spotpython tuning: 2992.884033203125 [----------] 0.68% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(7.705354507829712),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 3442.9345703125, 'hp_metric': 3442.9345703125}
spotpython tuning: 2992.884033203125 [----------] 0.72% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adadelta',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': inf, 'hp_metric': inf}

In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.011412931034997357),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(3.273700930745777),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 4540.98388671875, 'hp_metric': 4540.98388671875}
spotpython tuning: 2992.884033203125 [----------] 0.82% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adadelta',
 'patience': 256}
train_model result: {'val_loss': 9.844561716318367e+26, 'hp_metric': 9.844561716318367e+26}
spotpython tuning: 2992.884033203125 [----------] 1.55% 

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.01321934370367682),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 32,
 'lr_mult': np.float64(5.935136795016821),
 'optimizer': 'Adam',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4809.2705078125, 'hp_metric': 4809.2705078125}
spotpython tuning: 2992.884033203125 [----------] 1.68% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(5.380592761693367),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 17268.078125, 'hp_metric': 17268.078125}
spotpython tuning: 2992.884033203125 [----------] 1.70% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(7.199168908780852),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 5336.77490234375, 'hp_metric': 5336.77490234375}
spotpython tuning: 2992.884033203125 [----------] 1.80% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.014956185620791096),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(7.1496782721985),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 5272.4775390625, 'hp_metric': 5272.4775390625}
spotpython tuning: 2992.884033203125 [----------] 1.93% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.014779821121501818),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.614301663531089),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 4912.64794921875, 'hp_metric': 4912.64794921875}
spotpython tuning: 2992.884033203125 [----------] 2.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.6055497031466075),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 3317.019775390625, 'hp_metric': 3317.019775390625}
spotpython tuning: 2992.884033203125 [----------] 2.10% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.000433806853890824),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(5.763951145505976),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 4010.791015625, 'hp_metric': 4010.791015625}
spotpython tuning: 2992.884033203125 [----------] 2.72% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(8.042063346117653),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 8646.171875, 'hp_metric': 8646.171875}
spotpython tuning: 2992.884033203125 [----------] 2.80% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.015905136991423645),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(4.4080262864294255),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 3675.35595703125, 'hp_metric': 3675.35595703125}
spotpython tuning: 2992.884033203125 [----------] 3.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.009076022057472068),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(3.151693256355792),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 6284.78662109375, 'hp_metric': 6284.78662109375}
spotpython tuning: 2992.884033203125 [----------] 3.48% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.868972577682908),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 9928.666015625, 'hp_metric': 9928.666015625}
spotpython tuning: 2992.884033203125 [----------] 3.52% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(6.024570444103123),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 4125.29638671875, 'hp_metric': 4125.29638671875}
spotpython tuning: 2992.884033203125 [----------] 3.59% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(9.114977665218673),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 8034.83349609375, 'hp_metric': 8034.83349609375}
spotpython tuning: 2992.884033203125 [----------] 3.63% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0005872017917114974),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(7.479406200545031),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 23367.705078125, 'hp_metric': 23367.705078125}
spotpython tuning: 2992.884033203125 [----------] 3.66% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.01723851450106455),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(6.575494862356812),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 5082.20068359375, 'hp_metric': 5082.20068359375}
spotpython tuning: 2992.884033203125 [----------] 3.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.016639692495729486),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(6.296778569732192),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 6094.38916015625, 'hp_metric': 6094.38916015625}
spotpython tuning: 2992.884033203125 [----------] 3.91% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0033014801581026255),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(3.030514848128059),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 5921.6328125, 'hp_metric': 5921.6328125}
spotpython tuning: 2992.884033203125 [----------] 4.25% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(8.334742064032286),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 3751.56201171875, 'hp_metric': 3751.56201171875}
spotpython tuning: 2992.884033203125 [----------] 4.28% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.31317306481843),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 5429.72265625, 'hp_metric': 5429.72265625}
spotpython tuning: 2992.884033203125 [----------] 4.52% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(6.813796672747045),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 3237.488525390625, 'hp_metric': 3237.488525390625}
spotpython tuning: 2992.884033203125 [----------] 4.57% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.024672464044256135),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(7.2047065943542545),
 'optimizer': 'Adamax',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 5118.98046875, 'hp_metric': 5118.98046875}
spotpython tuning: 2992.884033203125 [#---------] 5.06% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.024734085155576006),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(7.204611038459472),
 'optimizer': 'Adamax',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 5129.65234375, 'hp_metric': 5129.65234375}
spotpython tuning: 2992.884033203125 [#---------] 5.55% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 23795.5546875, 'hp_metric': 23795.5546875}
spotpython tuning: 2992.884033203125 [#---------] 7.06% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(5.755544353233683),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 3927.895751953125, 'hp_metric': 3927.895751953125}
spotpython tuning: 2992.884033203125 [#---------] 7.36% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.012423451126665451),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 16,
 'lr_mult': np.float64(6.348886590977179),
 'optimizer': 'Adamax',
 'patience': 256}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.005559464884142321),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(8.126712116526356),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 21349.763671875, 'hp_metric': 21349.763671875}
spotpython tuning: 2992.884033203125 [#---------] 7.40% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 4256.98681640625, 'hp_metric': 4256.98681640625}
spotpython tuning: 2992.884033203125 [#---------] 7.54% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 24805.0703125, 'hp_metric': 24805.0703125}
spotpython tuning: 2992.884033203125 [#---------] 7.56% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 18452.880859375, 'hp_metric': 18452.880859375}
spotpython tuning: 2992.884033203125 [#---------] 7.58% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(6.860195981142931),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 7063.02294921875, 'hp_metric': 7063.02294921875}
spotpython tuning: 2992.884033203125 [#---------] 7.69% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(5.855158752079028),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 11512.1201171875, 'hp_metric': 11512.1201171875}
spotpython tuning: 2992.884033203125 [#---------] 7.79% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.00480820176645416),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 22892.078125, 'hp_metric': 22892.078125}
spotpython tuning: 2992.884033203125 [#---------] 8.66% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 26614.47265625, 'hp_metric': 26614.47265625}
spotpython tuning: 2992.884033203125 [#---------] 8.73% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(4.9237622735562505),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 23742.490234375, 'hp_metric': 23742.490234375}
spotpython tuning: 2992.884033203125 [#---------] 8.76% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(3.0780511019000594),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 6432.71875, 'hp_metric': 6432.71875}
spotpython tuning: 2992.884033203125 [#---------] 9.20% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 128,
 'lr_mult': np.float64(3.680427610114171),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 18249.22265625, 'hp_metric': 18249.22265625}
spotpython tuning: 2992.884033203125 [#---------] 9.31% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(9.469464013615735),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 5271.9482421875, 'hp_metric': 5271.9482421875}
spotpython tuning: 2992.884033203125 [#---------] 9.41% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(4.026880894672317),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 26934.91015625, 'hp_metric': 26934.91015625}
spotpython tuning: 2992.884033203125 [#---------] 9.50% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(2.7369455825764484),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': 5093.7724609375, 'hp_metric': 5093.7724609375}
spotpython tuning: 2992.884033203125 [#---------] 9.79% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(0.22213510020491256),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ELU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0015235318404906218),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(1.4179031708163974),
 'optimizer': 'Adadelta',
 'patience': 64}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}
spotpython tuning: 2992.884033203125 [#---------] 9.82% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(0.22213510020491256),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.01944086060673178),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 16,
 'lr_mult': np.float64(4.140214590611724),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 4775.9580078125, 'hp_metric': 4775.9580078125}
spotpython tuning: 2992.884033203125 [#---------] 10.11% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.373848107072167),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 4185.9833984375, 'hp_metric': 4185.9833984375}
spotpython tuning: 2992.884033203125 [#---------] 10.25% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.852245844372615),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 4629.39404296875, 'hp_metric': 4629.39404296875}
spotpython tuning: 2992.884033203125 [#---------] 10.37% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0015433022388792726),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(6.695993987974223),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 22576.53125, 'hp_metric': 22576.53125}
spotpython tuning: 2992.884033203125 [#---------] 10.40% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.257098906133402),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 3200.16796875, 'hp_metric': 3200.16796875}
spotpython tuning: 2992.884033203125 [#---------] 10.42% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.002760923964518974),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 5287.3837890625, 'hp_metric': 5287.3837890625}
spotpython tuning: 2992.884033203125 [#---------] 10.45% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 128,
 'lr_mult': np.float64(5.4887478747474425),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 5380.64453125, 'hp_metric': 5380.64453125}
spotpython tuning: 2992.884033203125 [#---------] 10.57% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(4.605702381979677),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 26881.912109375, 'hp_metric': 26881.912109375}
spotpython tuning: 2992.884033203125 [#---------] 10.67% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 32,
 'lr_mult': np.float64(6.551036464211696),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4944.01806640625, 'hp_metric': 4944.01806640625}
spotpython tuning: 2992.884033203125 [#---------] 10.74% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.007210164175463196),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5272.5341796875, 'hp_metric': 5272.5341796875}
spotpython tuning: 2992.884033203125 [#---------] 10.80% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(9.918374609017633),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 22698.806640625, 'hp_metric': 22698.806640625}
spotpython tuning: 2992.884033203125 [#---------] 10.84% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.02465381601434082),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(5.565757170851244),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 3395.051513671875, 'hp_metric': 3395.051513671875}
spotpython tuning: 2992.884033203125 [#---------] 10.87% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.003638272802802189),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(7.77206289432104),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 5354.837890625, 'hp_metric': 5354.837890625}
spotpython tuning: 2992.884033203125 [#---------] 10.99% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(7.449770949570345),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 3491.460693359375, 'hp_metric': 3491.460693359375}
spotpython tuning: 2992.884033203125 [#---------] 11.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.003986955555849509),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(2.658356599573826),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 4794.29052734375, 'hp_metric': 4794.29052734375}
spotpython tuning: 2992.884033203125 [#---------] 11.24% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.014099248047121565),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(4.873355377514533),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 9235.0400390625, 'hp_metric': 9235.0400390625}
spotpython tuning: 2992.884033203125 [#---------] 11.42% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.004555705514982659),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(2.3979567582169836),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 4596.24755859375, 'hp_metric': 4596.24755859375}
spotpython tuning: 2992.884033203125 [#---------] 11.63% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(4.200604425579572),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 21481.91015625, 'hp_metric': 21481.91015625}
spotpython tuning: 2992.884033203125 [#---------] 11.70% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 64,
 'lr_mult': np.float64(6.551349811986611),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 5443.48388671875, 'hp_metric': 5443.48388671875}
spotpython tuning: 2992.884033203125 [#---------] 11.77% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.014513011010900306),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(6.2881350495200525),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 7739.09326171875, 'hp_metric': 7739.09326171875}
spotpython tuning: 2992.884033203125 [#---------] 11.91% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.014692809128293396),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(6.152764654026773),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 19551.4375, 'hp_metric': 19551.4375}
spotpython tuning: 2992.884033203125 [#---------] 11.93% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.01653962156417528),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 16,
 'lr_mult': np.float64(6.534348461686432),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 5148.02880859375, 'hp_metric': 5148.02880859375}
spotpython tuning: 2992.884033203125 [#---------] 12.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0036028877200651736),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(8.676242150164061),
 'optimizer': 'Adadelta',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0041577409580897205),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(2.2867178578314244),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 12317.798828125, 'hp_metric': 12317.798828125}
spotpython tuning: 2992.884033203125 [#---------] 12.12% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 7639.30078125, 'hp_metric': 7639.30078125}
spotpython tuning: 2992.884033203125 [#---------] 12.32% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(4.302662777756226),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 7348.46923828125, 'hp_metric': 7348.46923828125}
spotpython tuning: 2992.884033203125 [#---------] 12.44% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.81324706874453),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 9569.1240234375, 'hp_metric': 9569.1240234375}
spotpython tuning: 2992.884033203125 [#---------] 12.57% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(7.990550071163799),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 4565.4912109375, 'hp_metric': 4565.4912109375}
spotpython tuning: 2992.884033203125 [#---------] 12.72% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.781324341072863),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 12489.576171875, 'hp_metric': 12489.576171875}
spotpython tuning: 2992.884033203125 [#---------] 12.86% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0003944638772019286),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(6.2651790361774475),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 7791.70263671875, 'hp_metric': 7791.70263671875}
spotpython tuning: 2992.884033203125 [#---------] 12.93% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.004942807166393743),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(1.7283418213709236),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 8219.1025390625, 'hp_metric': 8219.1025390625}
spotpython tuning: 2992.884033203125 [#---------] 13.09% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.000473564207907781),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(5.790971292387361),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 4533.15380859375, 'hp_metric': 4533.15380859375}
spotpython tuning: 2992.884033203125 [#---------] 13.27% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0031121792473572545),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(9.272693383688553),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 6460.76318359375, 'hp_metric': 6460.76318359375}
spotpython tuning: 2992.884033203125 [#---------] 13.34% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 7478.83251953125, 'hp_metric': 7478.83251953125}
spotpython tuning: 2992.884033203125 [#---------] 13.42% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0009399866614031635),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 22395.826171875, 'hp_metric': 22395.826171875}
spotpython tuning: 2992.884033203125 [#---------] 13.44% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 64,
 'lr_mult': np.float64(9.9830054895428),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 6006.17138671875, 'hp_metric': 6006.17138671875}
spotpython tuning: 2992.884033203125 [#---------] 13.52% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(4.0423051817046245),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 4784.33203125, 'hp_metric': 4784.33203125}
spotpython tuning: 2992.884033203125 [##--------] 15.04% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 64,
 'lr_mult': np.float64(4.644540305822232),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 11914.7119140625, 'hp_metric': 11914.7119140625}
spotpython tuning: 2992.884033203125 [##--------] 15.17% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.007836254785677793),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(4.199085892156591),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 3713.60498046875, 'hp_metric': 3713.60498046875}
spotpython tuning: 2992.884033203125 [##--------] 15.46% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.004402321846254648),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(4.066072331210421),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 23971.248046875, 'hp_metric': 23971.248046875}
spotpython tuning: 2992.884033203125 [##--------] 15.49% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(8.877746688780558),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 5195.9755859375, 'hp_metric': 5195.9755859375}
spotpython tuning: 2992.884033203125 [##--------] 15.59% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(4.06939818153295),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 4866.669921875, 'hp_metric': 4866.669921875}
spotpython tuning: 2992.884033203125 [##--------] 15.78% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.980320983928097),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 23935.13671875, 'hp_metric': 23935.13671875}
spotpython tuning: 2992.884033203125 [##--------] 15.81% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(5.17280171858764),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 6323.21630859375, 'hp_metric': 6323.21630859375}
spotpython tuning: 2992.884033203125 [##--------] 16.08% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(4.270271929730382),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 4130.6162109375, 'hp_metric': 4130.6162109375}
spotpython tuning: 2992.884033203125 [##--------] 16.29% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 11615.978515625, 'hp_metric': 11615.978515625}
spotpython tuning: 2992.884033203125 [##--------] 16.36% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(3.6501281853404457),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 5.649682005301364e+28, 'hp_metric': 5.649682005301364e+28}
spotpython tuning: 2992.884033203125 [##--------] 16.37% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(4.257877949764216),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 13796.6875, 'hp_metric': 13796.6875}
spotpython tuning: 2992.884033203125 [##--------] 16.46% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(3.9117689141171983),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 9.244152442595847e+32, 'hp_metric': 9.244152442595847e+32}
spotpython tuning: 2992.884033203125 [##--------] 16.53% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.018939965669536013),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(5.582052466638563),
 'optimizer': 'Adadelta',
 'patience': 128}
train_model result: {'val_loss': 5274.68408203125, 'hp_metric': 5274.68408203125}
spotpython tuning: 2992.884033203125 [##--------] 16.86% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.533269007165738),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4275.11865234375, 'hp_metric': 4275.11865234375}
spotpython tuning: 2992.884033203125 [##--------] 17.21% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.016200222398353697),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(4.301593032233862),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4439.4306640625, 'hp_metric': 4439.4306640625}
spotpython tuning: 2992.884033203125 [##--------] 17.37% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.00037809096566720005),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(6.543718282907647),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 12154.921875, 'hp_metric': 12154.921875}
spotpython tuning: 2992.884033203125 [##--------] 17.41% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(4.165898249649253),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 21944.583984375, 'hp_metric': 21944.583984375}
spotpython tuning: 2992.884033203125 [##--------] 17.45% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.021214396979169697),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(5.224247543550641),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 4541.64697265625, 'hp_metric': 4541.64697265625}
spotpython tuning: 2992.884033203125 [##--------] 17.54% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(6.640572486849613),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 4599.34130859375, 'hp_metric': 4599.34130859375}
spotpython tuning: 2992.884033203125 [##--------] 17.72% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(4.627268977453278),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 8548.404296875, 'hp_metric': 8548.404296875}
spotpython tuning: 2992.884033203125 [##--------] 17.82% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 32,
 'lr_mult': np.float64(0.9658835077954714),
 'optimizer': 'Adamax',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 7351.9091796875, 'hp_metric': 7351.9091796875}
spotpython tuning: 2992.884033203125 [##--------] 18.55% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(3.307349121074289),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 9230.15234375, 'hp_metric': 9230.15234375}
spotpython tuning: 2992.884033203125 [##--------] 18.90% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 128,
 'lr_mult': np.float64(8.374066774164618),
 'optimizer': 'Adam',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.00678475034657533),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(5.585908775036623),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 4463.857421875, 'hp_metric': 4463.857421875}
spotpython tuning: 2992.884033203125 [##--------] 19.01% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.018091192760667625),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(6.619708640132784),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 5140.1318359375, 'hp_metric': 5140.1318359375}
spotpython tuning: 2992.884033203125 [##--------] 19.15% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.9783946047118555),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 5257.4345703125, 'hp_metric': 5257.4345703125}
spotpython tuning: 2992.884033203125 [##--------] 19.35% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(5.733666227752855),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 4875.42333984375, 'hp_metric': 4875.42333984375}
spotpython tuning: 2992.884033203125 [##--------] 19.42% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(6.5678800237116555),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 5061.42919921875, 'hp_metric': 5061.42919921875}
spotpython tuning: 2992.884033203125 [##--------] 19.60% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(2.1905752724643915e-05),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(5.957604018908716),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 7648.5556640625, 'hp_metric': 7648.5556640625}
spotpython tuning: 2992.884033203125 [##--------] 19.79% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.598647703312454),
 'optimizer': 'Adadelta',
 'patience': 128}
train_model result: {'val_loss': 4918.95068359375, 'hp_metric': 4918.95068359375}
spotpython tuning: 2992.884033203125 [##--------] 20.33% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adadelta',
 'patience': 256}
train_model result: {'val_loss': 5352.0869140625, 'hp_metric': 5352.0869140625}
spotpython tuning: 2992.884033203125 [##--------] 20.80% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.326757770321022),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 4856.20068359375, 'hp_metric': 4856.20068359375}
spotpython tuning: 2992.884033203125 [##--------] 20.87% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(5.966758217281224),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 14480.205078125, 'hp_metric': 14480.205078125}
spotpython tuning: 2992.884033203125 [##--------] 20.91% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(2.2262660702096913),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 5006.32666015625, 'hp_metric': 5006.32666015625}
spotpython tuning: 2992.884033203125 [##--------] 21.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(5.5340332398684975),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 18422.140625, 'hp_metric': 18422.140625}
spotpython tuning: 2992.884033203125 [##--------] 21.09% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(3.5892127018247475),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 6.271079321316249e+28, 'hp_metric': 6.271079321316249e+28}
spotpython tuning: 2992.884033203125 [##--------] 21.10% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(2.2945482484718175e-05),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(2.668061426038397),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 7872.9716796875, 'hp_metric': 7872.9716796875}
spotpython tuning: 2992.884033203125 [##--------] 21.12% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(2.9814075547557333),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 4240.41796875, 'hp_metric': 4240.41796875}
spotpython tuning: 2992.884033203125 [##--------] 21.25% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.009945828064216877),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 128,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adamax',
 'patience': 128}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.023308351372023705),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(8.489743718030686),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 7902.9052734375, 'hp_metric': 7902.9052734375}
spotpython tuning: 2992.884033203125 [##--------] 21.34% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 16,
 'lr_mult': np.float64(5.11760864947591),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 5226.7080078125, 'hp_metric': 5226.7080078125}
spotpython tuning: 2992.884033203125 [##--------] 21.76% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(3.8062978996417502),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 7788.19091796875, 'hp_metric': 7788.19091796875}
spotpython tuning: 2992.884033203125 [##--------] 22.16% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(4.994182772995735),
 'optimizer': 'Adam',
 'patience': 8}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.0047885247760120824),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(7.840173863199331),
 'optimizer': 'Adam',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 7369.34765625, 'hp_metric': 7369.34765625}
spotpython tuning: 2992.884033203125 [##--------] 22.72% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(7.874880905824835),
 'optimizer': 'Adamax',
 'patience': 8}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0007195898528649741),
 'epochs': 4096,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(8.366596557917797),
 'optimizer': 'Adadelta',
 'patience': 4}
train_model result: {'val_loss': 10738.728515625, 'hp_metric': 10738.728515625}
spotpython tuning: 2992.884033203125 [##--------] 22.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(9.89753898231643),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 4785.38330078125, 'hp_metric': 4785.38330078125}
spotpython tuning: 2992.884033203125 [##--------] 23.02% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.430660396866088),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 11490.591796875, 'hp_metric': 11490.591796875}
spotpython tuning: 2992.884033203125 [##--------] 23.12% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(7.739833165511935),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 5287.34326171875, 'hp_metric': 5287.34326171875}
spotpython tuning: 2992.884033203125 [##--------] 23.20% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.00941838254217537),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 16,
 'lr_mult': np.float64(5.339421030929378),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 6610.82666015625, 'hp_metric': 6610.82666015625}
spotpython tuning: 2992.884033203125 [##--------] 23.32% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.95453134784784),
 'optimizer': 'Adamax',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 22244.322265625, 'hp_metric': 22244.322265625}
spotpython tuning: 2992.884033203125 [##--------] 23.81% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(4.3152241375442655),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 4878.60888671875, 'hp_metric': 4878.60888671875}
spotpython tuning: 2992.884033203125 [##--------] 23.88% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(6.426311558894637),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 3721.511474609375, 'hp_metric': 3721.511474609375}
spotpython tuning: 2992.884033203125 [##--------] 24.02% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.015620348648508586),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(2.8126745385137673),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 10908.8134765625, 'hp_metric': 10908.8134765625}
spotpython tuning: 2992.884033203125 [##--------] 24.23% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 16,
 'lr_mult': np.float64(5.536943892405179),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 5524.4775390625, 'hp_metric': 5524.4775390625}
spotpython tuning: 2992.884033203125 [##--------] 24.38% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(4.377059115270102),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 4992.6298828125, 'hp_metric': 4992.6298828125}
spotpython tuning: 2992.884033203125 [###-------] 25.38% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(6.664302927113824),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 4772.94921875, 'hp_metric': 4772.94921875}
spotpython tuning: 2992.884033203125 [###-------] 25.68% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 16,
 'lr_mult': np.float64(2.4380607415395676),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ELU(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.02408201853566781),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(4.646323273112424),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': nan, 'hp_metric': nan}
spotpython tuning: 2992.884033203125 [###-------] 25.70% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 16,
 'lr_mult': np.float64(2.4380607415395676),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.01859314667174219),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 64,
 'lr_mult': np.float64(7.739745677974436),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5083.1982421875, 'hp_metric': 5083.1982421875}
spotpython tuning: 2992.884033203125 [###-------] 25.85% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(4.18009445508198),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 7762.701171875, 'hp_metric': 7762.701171875}
spotpython tuning: 2992.884033203125 [###-------] 26.23% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.95211216914313),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 3370.0009765625, 'hp_metric': 3370.0009765625}
spotpython tuning: 2992.884033203125 [###-------] 26.27% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.021763372517512015),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(6.34625779244457),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 16072.8935546875, 'hp_metric': 16072.8935546875}
spotpython tuning: 2992.884033203125 [###-------] 26.29% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(5.310917515222983),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4720.595703125, 'hp_metric': 4720.595703125}
spotpython tuning: 2992.884033203125 [###-------] 26.62% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(5.775637630581458),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 7361.25, 'hp_metric': 7361.25}
spotpython tuning: 2992.884033203125 [###-------] 26.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 16,
 'lr_mult': np.float64(2.085537754759531),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 21262.890625, 'hp_metric': 21262.890625}
spotpython tuning: 2992.884033203125 [###-------] 26.80% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(6.715829675944782),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 6904.48193359375, 'hp_metric': 6904.48193359375}
spotpython tuning: 2992.884033203125 [###-------] 27.13% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adam',
 'patience': 4}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 6.151294767406776e+19, 'hp_metric': 6.151294767406776e+19}
spotpython tuning: 2992.884033203125 [###-------] 27.61% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.02496074567159543),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(6.137469767926032),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 4235.4912109375, 'hp_metric': 4235.4912109375}
spotpython tuning: 2992.884033203125 [###-------] 27.64% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(4.6579378349681155),
 'optimizer': 'Adamax',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 9226.8505859375, 'hp_metric': 9226.8505859375}
spotpython tuning: 2992.884033203125 [###-------] 29.62% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.00641541164042448),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(4.975243681847453),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 5347.53173828125, 'hp_metric': 5347.53173828125}
spotpython tuning: 2992.884033203125 [###-------] 29.68% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(6.387346012109103),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 9472.7490234375, 'hp_metric': 9472.7490234375}
spotpython tuning: 2992.884033203125 [###-------] 29.92% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(4.343364649228563),
 'optimizer': 'Adadelta',
 'patience': 4}
train_model result: {'val_loss': 4258.45361328125, 'hp_metric': 4258.45361328125}
spotpython tuning: 2992.884033203125 [###-------] 30.06% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 10899.4384765625, 'hp_metric': 10899.4384765625}
spotpython tuning: 2992.884033203125 [###-------] 30.11% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(5.984828642217278),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 5067.2607421875, 'hp_metric': 5067.2607421875}
spotpython tuning: 2992.884033203125 [###-------] 30.16% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.014541261096134855),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(5.535739686778065),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4965.0966796875, 'hp_metric': 4965.0966796875}
spotpython tuning: 2992.884033203125 [###-------] 30.32% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0010362257127706014),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(2.9172563684423967),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0013384615476006835),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(8.207269818473167),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 4859.41552734375, 'hp_metric': 4859.41552734375}
spotpython tuning: 2992.884033203125 [###-------] 30.47% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.007223000057256535),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4553.29052734375, 'hp_metric': 4553.29052734375}
spotpython tuning: 2992.884033203125 [###-------] 30.58% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adamax',
 'patience': 128}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4784.365234375, 'hp_metric': 4784.365234375}
spotpython tuning: 2992.884033203125 [###-------] 31.10% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0006305645187573513),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 18325.7109375, 'hp_metric': 18325.7109375}
spotpython tuning: 2992.884033203125 [###-------] 31.14% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(3.7551082108748948),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 15005.8515625, 'hp_metric': 15005.8515625}
spotpython tuning: 2992.884033203125 [###-------] 31.21% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(6.491303951645085),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 14183.0, 'hp_metric': 14183.0}
spotpython tuning: 2992.884033203125 [###-------] 31.26% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(9.048552785520199),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 4046.32080078125, 'hp_metric': 4046.32080078125}
spotpython tuning: 2992.884033203125 [###-------] 31.30% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.244008926701012),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 5310.130859375, 'hp_metric': 5310.130859375}
spotpython tuning: 2992.884033203125 [###-------] 31.32% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(8.497209291530712),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 3604.447509765625, 'hp_metric': 3604.447509765625}
spotpython tuning: 2992.884033203125 [###-------] 31.45% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.019191673575914347),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(4.2168028504483805),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5433.669921875, 'hp_metric': 5433.669921875}
spotpython tuning: 2992.884033203125 [###-------] 31.82% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(6.759941147668669),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 3519.0400390625, 'hp_metric': 3519.0400390625}
spotpython tuning: 2992.884033203125 [###-------] 31.85% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.088901032451557),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5433.89453125, 'hp_metric': 5433.89453125}
spotpython tuning: 2992.884033203125 [###-------] 32.21% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.02033735161903539),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.065676807207679),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': 4815.02490234375, 'hp_metric': 4815.02490234375}
spotpython tuning: 2992.884033203125 [###-------] 32.38% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 5015.19677734375, 'hp_metric': 5015.19677734375}
spotpython tuning: 2992.884033203125 [###-------] 32.92% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.019692645363041283),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(4.334601364740405),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4792.65869140625, 'hp_metric': 4792.65869140625}
spotpython tuning: 2992.884033203125 [###-------] 33.14% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(6.441119779277448),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 3220.712158203125, 'hp_metric': 3220.712158203125}
spotpython tuning: 2992.884033203125 [###-------] 33.19% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(8.515600416945528),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 4997.04833984375, 'hp_metric': 4997.04833984375}
spotpython tuning: 2992.884033203125 [###-------] 34.99% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(6.285676562365634),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 4842.72314453125, 'hp_metric': 4842.72314453125}
spotpython tuning: 2992.884033203125 [####------] 35.09% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.004903375080376155),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.624161941182719),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 22288.693359375, 'hp_metric': 22288.693359375}
spotpython tuning: 2992.884033203125 [####------] 35.16% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.01127011464472023),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.268768061630894),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5092.2568359375, 'hp_metric': 5092.2568359375}
spotpython tuning: 2992.884033203125 [####------] 35.24% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(7.174099485110162),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 16002.935546875, 'hp_metric': 16002.935546875}
spotpython tuning: 2992.884033203125 [####------] 35.31% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(0.6034503767967282),
 'optimizer': 'Adadelta',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 7787.2822265625, 'hp_metric': 7787.2822265625}
spotpython tuning: 2992.884033203125 [####------] 35.90% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.005670672661756215),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(5.8058447719142245),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': 4826.4072265625, 'hp_metric': 4826.4072265625}
spotpython tuning: 2992.884033203125 [####------] 36.10% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(6.301027117466985),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 4847.84716796875, 'hp_metric': 4847.84716796875}
spotpython tuning: 2992.884033203125 [####------] 36.35% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.02275756065688041),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(7.923432639937347),
 'optimizer': 'Adam',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 7488.4599609375, 'hp_metric': 7488.4599609375}
spotpython tuning: 2992.884033203125 [####------] 36.87% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(6.382833055450141),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 4792.5400390625, 'hp_metric': 4792.5400390625}
spotpython tuning: 2992.884033203125 [####------] 37.17% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(1.1463004940260788),
 'optimizer': 'Adadelta',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 5663.330078125, 'hp_metric': 5663.330078125}
spotpython tuning: 2992.884033203125 [####------] 37.70% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.011328397273861594),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.283836066036937),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.020764884712149424),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(2.196592339692124),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 19600.271484375, 'hp_metric': 19600.271484375}
spotpython tuning: 2992.884033203125 [####------] 37.85% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0017278435756466074),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(6.451865961371789),
 'optimizer': 'Adadelta',
 'patience': 128}
train_model result: {'val_loss': 5355.93505859375, 'hp_metric': 5355.93505859375}
spotpython tuning: 2992.884033203125 [####------] 38.02% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(4.261980972080889),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.014732390314606459),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(7.454652174608353),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 4486.30615234375, 'hp_metric': 4486.30615234375}
spotpython tuning: 2992.884033203125 [####------] 38.21% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(8.228433990236882),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': 4474.04736328125, 'hp_metric': 4474.04736328125}
spotpython tuning: 2992.884033203125 [####------] 38.57% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.02258741868114081),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(2.808520179644487),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 5440.85400390625, 'hp_metric': 5440.85400390625}
spotpython tuning: 2992.884033203125 [####------] 38.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.02377505300092261),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(4.922503699510086),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 5496.54443359375, 'hp_metric': 5496.54443359375}
spotpython tuning: 2992.884033203125 [####------] 38.84% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(4.79855331007114),
 'optimizer': 'Adamax',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 8465.34375, 'hp_metric': 8465.34375}
spotpython tuning: 2992.884033203125 [####------] 39.64% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(2.9161695917976926),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 5130.8857421875, 'hp_metric': 5130.8857421875}
spotpython tuning: 2992.884033203125 [####------] 40.24% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 18977.033203125, 'hp_metric': 18977.033203125}
spotpython tuning: 2992.884033203125 [####------] 40.62% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(7.579906447714695),
 'optimizer': 'Adadelta',
 'patience': 4}
train_model result: {'val_loss': 4790.55908203125, 'hp_metric': 4790.55908203125}
spotpython tuning: 2992.884033203125 [####------] 40.70% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(3.2334520854364524),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5220.1513671875, 'hp_metric': 5220.1513671875}
spotpython tuning: 2992.884033203125 [####------] 41.08% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.154363681510339),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 3189.776611328125, 'hp_metric': 3189.776611328125}
spotpython tuning: 2992.884033203125 [####------] 41.13% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.016945987529437163),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(4.315730657784241),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 3389.189453125, 'hp_metric': 3389.189453125}
spotpython tuning: 2992.884033203125 [####------] 41.21% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(4.04698623744256),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5473.00390625, 'hp_metric': 5473.00390625}
spotpython tuning: 2992.884033203125 [####------] 41.57% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.002304555959219196),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(3.07538412026885),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 23200.099609375, 'hp_metric': 23200.099609375}
spotpython tuning: 2992.884033203125 [####------] 41.61% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.015209253717228149),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(3.6220141827723067),
 'optimizer': 'Adam',
 'patience': 64}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 13190.2109375, 'hp_metric': 13190.2109375}
spotpython tuning: 2992.884033203125 [####------] 42.14% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(6.212641326260274),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 6490.8671875, 'hp_metric': 6490.8671875}
spotpython tuning: 2992.884033203125 [####------] 42.28% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.924387755275634),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 3519.650390625, 'hp_metric': 3519.650390625}
spotpython tuning: 2992.884033203125 [####------] 42.35% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(7.692550825222474),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 4815.47509765625, 'hp_metric': 4815.47509765625}
spotpython tuning: 2992.884033203125 [####------] 42.47% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(7.397618490647672),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5308.73779296875, 'hp_metric': 5308.73779296875}
spotpython tuning: 2992.884033203125 [####------] 42.56% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(2.679621846848434),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 3256.5458984375, 'hp_metric': 3256.5458984375}
spotpython tuning: 2992.884033203125 [####------] 42.64% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(5.987436717925516),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 8336.7568359375, 'hp_metric': 8336.7568359375}
spotpython tuning: 2992.884033203125 [####------] 42.95% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.007699437849056122),
 'epochs': 4096,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(6.70881338639559),
 'optimizer': 'Adadelta',
 'patience': 256}
train_model result: {'val_loss': 3860.691650390625, 'hp_metric': 3860.691650390625}
spotpython tuning: 2992.884033203125 [####------] 43.48% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(7.465735055662452),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 3247.62158203125, 'hp_metric': 3247.62158203125}
spotpython tuning: 2992.884033203125 [####------] 43.59% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(2.6796216142009563),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 3268.24609375, 'hp_metric': 3268.24609375}
spotpython tuning: 2992.884033203125 [####------] 43.67% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(4.244262145025929),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 6083.30908203125, 'hp_metric': 6083.30908203125}
spotpython tuning: 2992.884033203125 [####------] 43.69% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(5.694327776318017),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 5322.78076171875, 'hp_metric': 5322.78076171875}
spotpython tuning: 2992.884033203125 [####------] 43.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adadelta',
 'patience': 4}
train_model result: {'val_loss': 3701.197998046875, 'hp_metric': 3701.197998046875}
spotpython tuning: 2992.884033203125 [####------] 43.78% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(7.31070761218279),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 4416.69921875, 'hp_metric': 4416.69921875}
spotpython tuning: 2992.884033203125 [####------] 44.12% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(2.1486808837576756),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 3447.247314453125, 'hp_metric': 3447.247314453125}
spotpython tuning: 2992.884033203125 [####------] 44.20% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(0.11378163097793663),
 'optimizer': 'Adamax',
 'patience': 256}
train_model result: {'val_loss': 3292.150634765625, 'hp_metric': 3292.150634765625}
spotpython tuning: 2992.884033203125 [####------] 44.45% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(5.626946858112541),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 24031.013671875, 'hp_metric': 24031.013671875}
spotpython tuning: 2992.884033203125 [####------] 44.49% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(2.0736080584634355),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 3181.63916015625, 'hp_metric': 3181.63916015625}
spotpython tuning: 2992.884033203125 [####------] 44.53% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 4298.73193359375, 'hp_metric': 4298.73193359375}
spotpython tuning: 2992.884033203125 [#####-----] 46.48% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.681349795756071),
 'optimizer': 'Adamax',
 'patience': 4}
train_model result: {'val_loss': 5437.3994140625, 'hp_metric': 5437.3994140625}
spotpython tuning: 2992.884033203125 [#####-----] 46.51% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adadelta',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 16525.26953125, 'hp_metric': 16525.26953125}
spotpython tuning: 2992.884033203125 [#####-----] 47.04% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 4105.48974609375, 'hp_metric': 4105.48974609375}
spotpython tuning: 2992.884033203125 [#####-----] 47.13% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0038646186365569185),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(6.496318615030838),
 'optimizer': 'Adadelta',
 'patience': 256}
train_model result: {'val_loss': 5086.41259765625, 'hp_metric': 5086.41259765625}
spotpython tuning: 2992.884033203125 [#####-----] 47.84% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_normal',
 'l1': 16,
 'lr_mult': np.float64(4.767637461382271),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 3964.82958984375, 'hp_metric': 3964.82958984375}
spotpython tuning: 2992.884033203125 [#####-----] 47.91% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.683944003653511),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 3501.086669921875, 'hp_metric': 3501.086669921875}
spotpython tuning: 2992.884033203125 [#####-----] 47.94% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(5.279025561785902),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 4952.18408203125, 'hp_metric': 4952.18408203125}
spotpython tuning: 2992.884033203125 [#####-----] 48.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(7.44167915163909),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Sigmoid(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.002494202476358784),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(2.3988272256676066),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5431.93994140625, 'hp_metric': 5431.93994140625}
spotpython tuning: 2992.884033203125 [#####-----] 48.21% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(7.437304890884269),
 'optimizer': 'Adamax',
 'patience': 256}
train_model result: {'val_loss': 5137.32568359375, 'hp_metric': 5137.32568359375}
spotpython tuning: 2992.884033203125 [#####-----] 49.83% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0004596846308073419),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(7.430063474300357),
 'optimizer': 'Adam',
 'patience': 512}
train_model result: {'val_loss': 5380.7607421875, 'hp_metric': 5380.7607421875}
spotpython tuning: 2992.884033203125 [#####-----] 52.99% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.02294523423711508),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(1.7630958291509269),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 3209.365478515625, 'hp_metric': 3209.365478515625}
spotpython tuning: 2992.884033203125 [#####-----] 53.09% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(6.2297862551203504),
 'optimizer': 'Adam',
 'patience': 256}
train_model result: {'val_loss': 4525.01220703125, 'hp_metric': 4525.01220703125}
spotpython tuning: 2992.884033203125 [#####-----] 53.99% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.022131835999330683),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(9.969332496135523),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 3436.44921875, 'hp_metric': 3436.44921875}
spotpython tuning: 2992.884033203125 [#####-----] 54.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(4.436327973764923),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 24090.716796875, 'hp_metric': 24090.716796875}
spotpython tuning: 2992.884033203125 [#####-----] 54.07% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0029862887330858135),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(5.286261522869514),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 6632.48681640625, 'hp_metric': 6632.48681640625}
spotpython tuning: 2992.884033203125 [#####-----] 54.23% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5564.55615234375, 'hp_metric': 5564.55615234375}
spotpython tuning: 2992.884033203125 [#####-----] 54.91% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(3.298993875058985),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5184.2158203125, 'hp_metric': 5184.2158203125}
spotpython tuning: 2992.884033203125 [######----] 55.34% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.010865514074484975),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(0.9655700128831218),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 4786.2392578125, 'hp_metric': 4786.2392578125}
spotpython tuning: 2992.884033203125 [######----] 56.11% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(7.755243154400823),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 4065.531005859375, 'hp_metric': 4065.531005859375}
spotpython tuning: 2992.884033203125 [######----] 56.23% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 3199.947998046875, 'hp_metric': 3199.947998046875}
spotpython tuning: 2992.884033203125 [######----] 56.50% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.8200823917407565),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 6586.787109375, 'hp_metric': 6586.787109375}
spotpython tuning: 2992.884033203125 [######----] 56.61% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 64,
 'lr_mult': np.float64(3.555131218760263),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 4584.77734375, 'hp_metric': 4584.77734375}
spotpython tuning: 2992.884033203125 [######----] 57.00% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.022817402661781307),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(2.0298640441996594),
 'optimizer': 'Adadelta',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4845.923828125, 'hp_metric': 4845.923828125}
spotpython tuning: 2992.884033203125 [######----] 57.57% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.012341055158193587),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(7.258901856585661),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 4121.564453125, 'hp_metric': 4121.564453125}
spotpython tuning: 2992.884033203125 [######----] 57.79% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(5.547987627760082),
 'optimizer': 'Adadelta',
 'patience': 16}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.010073990397081936),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 16,
 'lr_mult': np.float64(9.997084498907238),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 5376.544921875, 'hp_metric': 5376.544921875}
spotpython tuning: 2992.884033203125 [######----] 57.88% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.013615341185269966),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(7.864824834354931),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4241.951171875, 'hp_metric': 4241.951171875}
spotpython tuning: 2992.884033203125 [######----] 58.09% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(4.36917763608596),
 'optimizer': 'Adadelta',
 'patience': 4}
train_model result: {'val_loss': 5028.587890625, 'hp_metric': 5028.587890625}
spotpython tuning: 2992.884033203125 [######----] 58.12% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(5.417494386705421),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 4340.98193359375, 'hp_metric': 4340.98193359375}
spotpython tuning: 2992.884033203125 [######----] 58.78% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 128,
 'lr_mult': np.float64(5.048001964040391),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 5195.42236328125, 'hp_metric': 5195.42236328125}
spotpython tuning: 2992.884033203125 [######----] 58.94% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.01742896162167168),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(3.5386538254499103),
 'optimizer': 'Adadelta',
 'patience': 16}
train_model result: {'val_loss': 6355.70458984375, 'hp_metric': 6355.70458984375}
spotpython tuning: 2992.884033203125 [######----] 59.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(9.143469109865617),
 'optimizer': 'Adadelta',
 'patience': 4}
train_model result: {'val_loss': 23540.24609375, 'hp_metric': 23540.24609375}
spotpython tuning: 2992.884033203125 [######----] 59.06% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.8521992682338935),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 4558.54638671875, 'hp_metric': 4558.54638671875}
spotpython tuning: 2992.884033203125 [######----] 59.92% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.01921940098310375),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 16,
 'lr_mult': np.float64(4.069569807105038),
 'optimizer': 'Adam',
 'patience': 64}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 6997.0654296875, 'hp_metric': 6997.0654296875}
spotpython tuning: 2992.884033203125 [######----] 60.50% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 64,
 'lr_mult': np.float64(4.522772572515887),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 4482.90380859375, 'hp_metric': 4482.90380859375}
spotpython tuning: 2992.884033203125 [######----] 61.18% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.007795340908767948),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(7.284624705085071),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.011297770850601872),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(5.689295045133884),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 4789.93505859375, 'hp_metric': 4789.93505859375}
spotpython tuning: 2992.884033203125 [######----] 61.34% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(9.13463057020492),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 3904.40869140625, 'hp_metric': 3904.40869140625}
spotpython tuning: 2992.884033203125 [######----] 61.36% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(3.0658996335885678),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5097.60009765625, 'hp_metric': 5097.60009765625}
spotpython tuning: 2992.884033203125 [######----] 61.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 32,
 'lr_mult': np.float64(4.360388062560906),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 7619.90234375, 'hp_metric': 7619.90234375}
spotpython tuning: 2992.884033203125 [######----] 61.85% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.023996231017374706),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 64,
 'lr_mult': np.float64(8.66018520695405),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 7541.0048828125, 'hp_metric': 7541.0048828125}
spotpython tuning: 2992.884033203125 [######----] 61.92% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.022738984061051445),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 128,
 'lr_mult': np.float64(8.134535649494888),
 'optimizer': 'Adam',
 'patience': 128}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0009182352898888858),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 128,
 'lr_mult': np.float64(3.068382162434608),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 22066.4765625, 'hp_metric': 22066.4765625}
spotpython tuning: 2992.884033203125 [######----] 62.03% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(8.276138381899433),
 'optimizer': 'Adam',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.011497349198613137),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 32,
 'lr_mult': np.float64(9.916196998143844),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 5738.357421875, 'hp_metric': 5738.357421875}
spotpython tuning: 2992.884033203125 [######----] 62.19% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(9.596228915683655),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 4174.1318359375, 'hp_metric': 4174.1318359375}
spotpython tuning: 2992.884033203125 [######----] 62.26% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(3.511690569985154),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5333.4912109375, 'hp_metric': 5333.4912109375}
spotpython tuning: 2992.884033203125 [######----] 62.66% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0009186428437946106),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(8.285536366217073),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 3208.38720703125, 'hp_metric': 3208.38720703125}
spotpython tuning: 2992.884033203125 [######----] 62.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(1.8770760687222285),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 4789.8515625, 'hp_metric': 4789.8515625}
spotpython tuning: 2992.884033203125 [######----] 63.15% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(9.813252837498883),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 3865.75439453125, 'hp_metric': 3865.75439453125}
spotpython tuning: 2992.884033203125 [######----] 63.31% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.013520486127866311),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(7.698468681860945),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 4567.9130859375, 'hp_metric': 4567.9130859375}
spotpython tuning: 2992.884033203125 [######----] 63.45% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.023500125147417603),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(7.789491136055753),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5435.560546875, 'hp_metric': 5435.560546875}
spotpython tuning: 2992.884033203125 [######----] 63.52% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.024615893667093935),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(4.305398506525837),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 5015.41650390625, 'hp_metric': 5015.41650390625}
spotpython tuning: 2992.884033203125 [######----] 63.65% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.72535233361095),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 4858.38232421875, 'hp_metric': 4858.38232421875}
spotpython tuning: 2992.884033203125 [######----] 64.13% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(6.292400967961244),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 3323.87109375, 'hp_metric': 3323.87109375}
spotpython tuning: 2992.884033203125 [######----] 64.18% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_normal',
 'l1': 32,
 'lr_mult': np.float64(3.415587696459095),
 'optimizer': 'Adadelta',
 'patience': 64}
train_model result: {'val_loss': 9220.0126953125, 'hp_metric': 9220.0126953125}
spotpython tuning: 2992.884033203125 [######----] 64.28% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adamax',
 'patience': 8}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 17053.396484375, 'hp_metric': 17053.396484375}
spotpython tuning: 2992.884033203125 [#######---] 66.46% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0006750262937084443),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(8.352257406960979),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 3439.905029296875, 'hp_metric': 3439.905029296875}
spotpython tuning: 2992.884033203125 [#######---] 66.59% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 4383.267578125, 'hp_metric': 4383.267578125}
spotpython tuning: 2992.884033203125 [#######---] 66.68% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'kaiming_normal',
 'l1': 8,
 'lr_mult': np.float64(1.8164574660731316),
 'optimizer': 'Adadelta',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4939.7919921875, 'hp_metric': 4939.7919921875}
spotpython tuning: 2992.884033203125 [#######---] 67.27% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(4.572488044328203),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 3871.029541015625, 'hp_metric': 3871.029541015625}
spotpython tuning: 2992.884033203125 [#######---] 67.40% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.006070698209521706),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(2.1021127077445163),
 'optimizer': 'Adamax',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 22839.849609375, 'hp_metric': 22839.849609375}
spotpython tuning: 2992.884033203125 [#######---] 67.93% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.011527245886575877),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(6.612927268950293),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 4914.75341796875, 'hp_metric': 4914.75341796875}
spotpython tuning: 2992.884033203125 [#######---] 68.14% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(3.278371387470751),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5186.5947265625, 'hp_metric': 5186.5947265625}
spotpython tuning: 2992.884033203125 [#######---] 70.05% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0002139668688370654),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(5.8764153146132685),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 10535.484375, 'hp_metric': 10535.484375}
spotpython tuning: 2992.884033203125 [#######---] 70.12% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(6.527167562074477),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 5653.857421875, 'hp_metric': 5653.857421875}
spotpython tuning: 2992.884033203125 [#######---] 71.58% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(3.82057695155056),
 'optimizer': 'Adamax',
 'patience': 16}
train_model result: {'val_loss': 4786.72412109375, 'hp_metric': 4786.72412109375}
spotpython tuning: 2992.884033203125 [#######---] 71.91% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(4.910474125737579),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 24643.259765625, 'hp_metric': 24643.259765625}
spotpython tuning: 2992.884033203125 [#######---] 71.96% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0001159957277234173),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(6.693003554206465),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 3330.461181640625, 'hp_metric': 3330.461181640625}
spotpython tuning: 2992.884033203125 [#######---] 72.02% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0002612901537744061),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': np.float64(6.9080110682597775),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 3500.906494140625, 'hp_metric': 3500.906494140625}
spotpython tuning: 2992.884033203125 [#######---] 72.14% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(5.020822641697284),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 2953.669921875, 'hp_metric': 2953.669921875}
spotpython tuning: 2953.669921875 [#######---] 72.17% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 64,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 512}
train_model result: {'val_loss': 6971.6982421875, 'hp_metric': 6971.6982421875}
spotpython tuning: 2953.669921875 [#######---] 72.89% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.011518748188478081),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(8.46414163708489),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 5117.39453125, 'hp_metric': 5117.39453125}
spotpython tuning: 2953.669921875 [#######---] 72.99% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(9.60974872121081),
 'optimizer': 'Adam',
 'patience': 32}
train_model result: {'val_loss': 4202.5595703125, 'hp_metric': 4202.5595703125}
spotpython tuning: 2953.669921875 [#######---] 73.34% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.019349971840910883),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(3.1689615088827594),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 3215.223388671875, 'hp_metric': 3215.223388671875}
spotpython tuning: 2953.669921875 [#######---] 73.45% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(8.105167816127071),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 3626.452880859375, 'hp_metric': 3626.452880859375}
spotpython tuning: 2953.669921875 [#######---] 73.82% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(7.855556127021599),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 4107.11328125, 'hp_metric': 4107.11328125}
spotpython tuning: 2953.669921875 [#######---] 73.93% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 32,
 'lr_mult': np.float64(8.409312234815758),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 3528.630859375, 'hp_metric': 3528.630859375}
spotpython tuning: 2953.669921875 [#######---] 74.01% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0004904753768321083),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(4.83407614878095),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 3298.507080078125, 'hp_metric': 3298.507080078125}
spotpython tuning: 2953.669921875 [#######---] 74.03% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(6.184169730591455),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 4775.23779296875, 'hp_metric': 4775.23779296875}
spotpython tuning: 2953.669921875 [#######---] 74.06% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 8,
 'lr_mult': np.float64(5.420792791683584),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': 4971.888671875, 'hp_metric': 4971.888671875}
spotpython tuning: 2953.669921875 [#######---] 74.31% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': np.float64(7.864123552546208),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 4867.31640625, 'hp_metric': 4867.31640625}
spotpython tuning: 2953.669921875 [#######---] 74.42% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(3.942008482908289),
 'optimizer': 'Adam',
 'patience': 64}
train_model result: {'val_loss': 14964.1884765625, 'hp_metric': 14964.1884765625}
spotpython tuning: 2953.669921875 [#######---] 74.82% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(9.215573110617306),
 'optimizer': 'Adadelta',
 'patience': 128}
train_model result: {'val_loss': 5376.181640625, 'hp_metric': 5376.181640625}
spotpython tuning: 2953.669921875 [########--] 75.51% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(0.1),
 'optimizer': 'Adamax',
 'patience': 64}
train_model result: {'val_loss': 3336.338134765625, 'hp_metric': 3336.338134765625}
spotpython tuning: 2953.669921875 [########--] 75.66% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 128,
 'lr_mult': np.float64(8.889162738767132),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 4878.85205078125, 'hp_metric': 4878.85205078125}
spotpython tuning: 2953.669921875 [########--] 78.79% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0035935279211055954),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.539895009541564),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': ELU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.004791987364983441),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(2.0608749680833824),
 'optimizer': 'Adam',
 'patience': 128}
train_model result: {'val_loss': nan, 'hp_metric': nan}
spotpython tuning: 2953.669921875 [########--] 78.81% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0035935279211055954),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.539895009541564),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 512,
 'dropout_prob': np.float64(0.010023062829759129),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 128,
 'lr_mult': np.float64(2.5638249492308773),
 'optimizer': 'Adam',
 'patience': 64}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 15791.232421875, 'hp_metric': 15791.232421875}
spotpython tuning: 2953.669921875 [########--] 79.59% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 32,
 'dropout_prob': np.float64(0.013480195276229543),
 'epochs': 4096,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(4.370819395259581),
 'optimizer': 'Adam',
 'patience': 16}
train_model result: {'val_loss': 3094.232421875, 'hp_metric': 3094.232421875}
spotpython tuning: 2953.669921875 [########--] 79.64% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.003471160797371509),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(6.3862929881019905),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5244.5966796875, 'hp_metric': 5244.5966796875}
spotpython tuning: 2953.669921875 [########--] 79.73% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.019215462272452775),
 'epochs': 1024,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adamax',
 'patience': 4}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4784.56396484375, 'hp_metric': 4784.56396484375}
spotpython tuning: 2953.669921875 [########--] 80.23% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 1024,
 'dropout_prob': np.float64(0.022951710910339637),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(1.7630986603560133),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 3209.37890625, 'hp_metric': 3209.37890625}
spotpython tuning: 2953.669921875 [########--] 80.33% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 3887.22119140625, 'hp_metric': 3887.22119140625}
spotpython tuning: 2953.669921875 [########--] 80.39% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 256,
 'dropout_prob': np.float64(0.006335722063572085),
 'epochs': 2048,
 'initialization': 'xavier_uniform',
 'l1': 16,
 'lr_mult': np.float64(3.691107949399641),
 'optimizer': 'Adam',
 'patience': 32}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 4820.388671875, 'hp_metric': 4820.388671875}
spotpython tuning: 2953.669921875 [########--] 81.56% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 64,
 'lr_mult': np.float64(5.337101754349744),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5536.7099609375, 'hp_metric': 5536.7099609375}
spotpython tuning: 2953.669921875 [########--] 82.49% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(5.273549859083047),
 'optimizer': 'Adam',
 'patience': 8}
train_model result: {'val_loss': 7033.470703125, 'hp_metric': 7033.470703125}
spotpython tuning: 2953.669921875 [########--] 82.69% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.024095694980770233),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(6.881421454748622),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5031.34130859375, 'hp_metric': 5031.34130859375}
spotpython tuning: 2953.669921875 [#########-] 86.00% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 8,
 'lr_mult': np.float64(5.84481586617735),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 4564.955078125, 'hp_metric': 4564.955078125}
spotpython tuning: 2953.669921875 [#########-] 88.02% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ELU(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.009775595673523554),
 'epochs': 2048,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(3.2350851800755125),
 'optimizer': 'Adam',
 'patience': 512}</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 5583.181640625, 'hp_metric': 5583.181640625}
spotpython tuning: 2953.669921875 [#########-] 89.53% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(4.025572230389219),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 5392.6201171875, 'hp_metric': 5392.6201171875}
spotpython tuning: 2953.669921875 [#########-] 89.68% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(5.569122074644288),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 4598.07568359375, 'hp_metric': 4598.07568359375}
spotpython tuning: 2953.669921875 [#########-] 91.66% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_uniform',
 'l1': 32,
 'lr_mult': np.float64(5.744214600544816),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 24471.20703125, 'hp_metric': 24471.20703125}
spotpython tuning: 2953.669921875 [#########-] 91.69% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'kaiming_normal',
 'l1': 64,
 'lr_mult': np.float64(7.716625432727044),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 4178.07177734375, 'hp_metric': 4178.07177734375}
spotpython tuning: 2953.669921875 [#########-] 94.10% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.011188017592886366),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(2.768140574484449),
 'optimizer': 'Adamax',
 'patience': 512}
train_model result: {'val_loss': 5170.00927734375, 'hp_metric': 5170.00927734375}
spotpython tuning: 2953.669921875 [##########] 95.67% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(5.047645841714484),
 'optimizer': 'Adadelta',
 'patience': 32}
train_model result: {'val_loss': 4149.40673828125, 'hp_metric': 4149.40673828125}
spotpython tuning: 2953.669921875 [##########] 95.91% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.014469335516756068),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': np.float64(4.183368951065941),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 3830.707275390625, 'hp_metric': 3830.707275390625}
spotpython tuning: 2953.669921875 [##########] 96.81% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.423013100805973),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 2944.9208984375, 'hp_metric': 2944.9208984375}
spotpython tuning: 2944.9208984375 [##########] 96.83% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.010229870900442391),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.93588744986161),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 3373.96044921875, 'hp_metric': 3373.96044921875}
spotpython tuning: 2944.9208984375 [##########] 96.85% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(8.208899255039595),
 'optimizer': 'Adamax',
 'patience': 128}
train_model result: {'val_loss': 4026.78662109375, 'hp_metric': 4026.78662109375}
spotpython tuning: 2944.9208984375 [##########] 97.23% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 256,
 'dropout_prob': np.float64(0.0),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(5.876418491405471),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 10836.5234375, 'hp_metric': 10836.5234375}
spotpython tuning: 2944.9208984375 [##########] 97.29% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(10.0),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 4517.58984375, 'hp_metric': 4517.58984375}
spotpython tuning: 2944.9208984375 [##########] 97.32% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.025),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(8.162732221690334),
 'optimizer': 'Adam',
 'patience': 256}
train_model result: {'val_loss': 4693.14013671875, 'hp_metric': 4693.14013671875}
spotpython tuning: 2944.9208984375 [##########] 98.07% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': np.float64(9.440996887124262),
 'optimizer': 'Adam',
 'patience': 4}
train_model result: {'val_loss': 3217.251708984375, 'hp_metric': 3217.251708984375}
spotpython tuning: 2944.9208984375 [##########] 98.09% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': np.float64(0.025),
 'epochs': 2048,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': np.float64(7.349765010277698),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': nan, 'hp_metric': nan}

In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 32,
 'dropout_prob': np.float64(0.017592177457678115),
 'epochs': 2048,
 'initialization': 'kaiming_normal',
 'l1': 32,
 'lr_mult': np.float64(7.991833817919381),
 'optimizer': 'Adadelta',
 'patience': 8}
train_model result: {'val_loss': 4386.2470703125, 'hp_metric': 4386.2470703125}
spotpython tuning: 2944.9208984375 [##########] 98.16% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 128,
 'dropout_prob': np.float64(0.00028799278714635926),
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(6.589256038221469),
 'optimizer': 'Adamax',
 'patience': 8}
train_model result: {'val_loss': 4847.119140625, 'hp_metric': 4847.119140625}
spotpython tuning: 2944.9208984375 [##########] 98.31% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Swish(),
 'batch_norm': False,
 'batch_size': 128,
 'dropout_prob': np.float64(0.006978354853319396),
 'epochs': 2048,
 'initialization': 'xavier_normal',
 'l1': 128,
 'lr_mult': np.float64(4.923821539775095),
 'optimizer': 'Adamax',
 'patience': 32}
train_model result: {'val_loss': 4039.177001953125, 'hp_metric': 4039.177001953125}
spotpython tuning: 2944.9208984375 [##########] 98.75% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
In fun(): config:
{'act_fn': Tanh(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': np.float64(0.025),
 'epochs': 4096,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': np.float64(5.650181060811463),
 'optimizer': 'Adadelta',
 'patience': 512}
train_model result: {'val_loss': 5078.6162109375, 'hp_metric': 5078.6162109375}
spotpython tuning: 2944.9208984375 [##########] 100.00% Done...

Experiment saved to 602_12_res.pkl</code></pre>
</div>
</div>
</section>
<section id="results-from-the-hyperparameter-tuning-experiment" class="level2" data-number="39.2">
<h2 data-number="39.2" class="anchored" data-anchor-id="results-from-the-hyperparameter-tuning-experiment"><span class="header-section-number">39.2</span> Results from the Hyperparameter Tuning Experiment</h2>
<ul>
<li>After the hyperparameter tuning is finished, the following information is available:
<ul>
<li>the <code>spot_tuner</code> object and the associated</li>
<li><code>fun_control</code> dictionary</li>
</ul></li>
</ul>
<div id="print_results" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb375"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb375-1"><a href="#cb375-1" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> spot_tuner.print_results(print_screen<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>min y: 2944.9208984375
l1: 4.0
epochs: 12.0
batch_size: 9.0
act_fn: 3.0
optimizer: 1.0
dropout_prob: 0.0
lr_mult: 5.423013100805973
patience: 2.0
batch_norm: 0.0
initialization: 4.0</code></pre>
</div>
</div>
<div id="cell-602_plot_progress_xai" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb377"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb377-1"><a href="#cb377-1" aria-hidden="true" tabindex="-1"></a>spot_tuner.plot_progress()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/plot_progress_xai-output-1.png" id="plot_progress_xai" width="732" height="261" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="getting-the-best-model-i.e-the-tuned-architecture" class="level3" data-number="39.2.1">
<h3 data-number="39.2.1" class="anchored" data-anchor-id="getting-the-best-model-i.e-the-tuned-architecture"><span class="header-section-number">39.2.1</span> Getting the Best Model, i.e, the Tuned Architecture</h3>
<ul>
<li>The method <code>get_tuned_architecture</code> <a href="https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_tuned_architecture">[DOC]</a> returns the best model architecture found during the hyperparameter tuning.</li>
<li>It returns the transformed values, i.e., <code>batch_size = 2^x</code> if the hyperparameter <code>batch_size</code> was transformed with the <code>transform_power_2_int</code> function.</li>
</ul>
<div id="get_tuned_architecture" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb378"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb378-1"><a href="#cb378-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.hyperparameters.values <span class="im">import</span> get_tuned_architecture</span>
<span id="cb378-2"><a href="#cb378-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pprint</span>
<span id="cb378-3"><a href="#cb378-3" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> get_tuned_architecture(spot_tuner, fun_control)</span>
<span id="cb378-4"><a href="#cb378-4" aria-hidden="true" tabindex="-1"></a>pprint.pprint(config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.423013100805973),
 'optimizer': 'Adam',
 'patience': 4}</code></pre>
</div>
</div>
<ul>
<li>Note: <code>get_tuned_architecture</code> has the option <code>force_minX</code> which does not have any effect in this case.</li>
</ul>
<div id="get_tuned_architecture_force_minx" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb380"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb380-1"><a href="#cb380-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.hyperparameters.values <span class="im">import</span> get_tuned_architecture</span>
<span id="cb380-2"><a href="#cb380-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> get_tuned_architecture(spot_tuner, fun_control, force_minX<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb380-3"><a href="#cb380-3" aria-hidden="true" tabindex="-1"></a>pprint.pprint(config)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'act_fn': LeakyReLU(),
 'batch_norm': False,
 'batch_size': 512,
 'dropout_prob': np.float64(0.0),
 'epochs': 4096,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': np.float64(5.423013100805973),
 'optimizer': 'Adam',
 'patience': 4}</code></pre>
</div>
</div>
</section>
</section>
<section id="training-the-tuned-architecture-on-the-test-data" class="level2" data-number="39.3">
<h2 data-number="39.3" class="anchored" data-anchor-id="training-the-tuned-architecture-on-the-test-data"><span class="header-section-number">39.3</span> Training the Tuned Architecture on the Test Data</h2>
<ul>
<li>Since we are interested in the explainability of the model, we will train the tuned architecture on the test data.</li>
<li><code>spotpythons</code>’s <code>test_model</code> function <a href="https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/light/testmodel/">[DOC]</a> is used to train the model on the test data.</li>
<li>Note: Until now, we do not use any information about the NN’s weights and biases. Only the architecture, which is available as the <code>config</code>, is used.</li>
<li><code>spotpython</code> used the TensorBoard logger to save the training process in the <code>./runs</code> directory. Therefore, we have to enable the TensorBoard logger in the <code>fun_control</code> dictionary. To get a clean start, we remove an existing <code>runs</code> folder.</li>
</ul>
<div id="test_model" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb382"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb382-1"><a href="#cb382-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.light.testmodel <span class="im">import</span> test_model</span>
<span id="cb382-2"><a href="#cb382-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.light.loadmodel <span class="im">import</span> load_light_from_checkpoint</span>
<span id="cb382-3"><a href="#cb382-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb382-4"><a href="#cb382-4" aria-hidden="true" tabindex="-1"></a><span class="co"># if the directory "./runs" exists, delete it</span></span>
<span id="cb382-5"><a href="#cb382-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(<span class="st">"./runs"</span>):</span>
<span id="cb382-6"><a href="#cb382-6" aria-hidden="true" tabindex="-1"></a>    os.system(<span class="st">"rm -r ./runs"</span>)</span>
<span id="cb382-7"><a href="#cb382-7" aria-hidden="true" tabindex="-1"></a>fun_control.update({<span class="st">"tensorboard_log"</span>: <span class="va">True</span>})</span>
<span id="cb382-8"><a href="#cb382-8" aria-hidden="true" tabindex="-1"></a>test_model(config, fun_control)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="test_model-1" class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">        Test metric        </span>┃<span style="font-weight: bold">       DataLoader 0        </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│<span style="color: #008080; text-decoration-color: #008080">         hp_metric         </span>│<span style="color: #800080; text-decoration-color: #800080">     3276.14404296875      </span>│
│<span style="color: #008080; text-decoration-color: #008080">         val_loss          </span>│<span style="color: #800080; text-decoration-color: #800080">     3276.14404296875      </span>│
└───────────────────────────┴───────────────────────────┘
</pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>test_model result: {'val_loss': 3276.14404296875, 'hp_metric': 3276.14404296875}</code></pre>
</div>
<div id="test_model-2" class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(3276.14404296875, 3276.14404296875)</code></pre>
</div>
</div>
<div id="load_model_from_chkpt" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb385"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb385-1"><a href="#cb385-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> load_light_from_checkpoint(config, fun_control)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>config: {'l1': 16, 'epochs': 4096, 'batch_size': 512, 'act_fn': LeakyReLU(), 'optimizer': 'Adam', 'dropout_prob': np.float64(0.0), 'lr_mult': np.float64(5.423013100805973), 'patience': 4, 'batch_norm': False, 'initialization': 'xavier_normal'}
Loading model with 16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TEST from runs/saved_models/16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TEST/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.0, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.0, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.0, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): LeakyReLU()
    (14): Dropout(p=0.0, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): LeakyReLU()
    (17): Dropout(p=0.0, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): LeakyReLU()
    (20): Dropout(p=0.0, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)</code></pre>
</div>
</div>
<section id="details-of-the-training-process-on-the-test-data" class="level4" data-number="39.3.0.1">
<h4 data-number="39.3.0.1" class="anchored" data-anchor-id="details-of-the-training-process-on-the-test-data"><span class="header-section-number">39.3.0.1</span> Details of the Training Process on the Test Data</h4>
<ul>
<li>The <code>test_model</code> method initializes the model with the tuned architecture as follows:</li>
</ul>
<div class="sourceCode" id="cb387"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb387-1"><a href="#cb387-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> fun_control[<span class="st">"core_model"</span>](<span class="op">**</span>config, _L_in<span class="op">=</span>_L_in, _L_out<span class="op">=</span>_L_out, _torchmetric<span class="op">=</span>_torchmetric)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>Then, the Lightning Trainer is initialized with the <code>fun_control</code> dictionary and the model as follows:</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb388-1"><a href="#cb388-1" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> L.Trainer(</span>
<span id="cb388-2"><a href="#cb388-2" aria-hidden="true" tabindex="-1"></a>    default_root_dir<span class="op">=</span>os.path.join(fun_control[<span class="st">"CHECKPOINT_PATH"</span>], config_id),</span>
<span id="cb388-3"><a href="#cb388-3" aria-hidden="true" tabindex="-1"></a>    max_epochs<span class="op">=</span>model.hparams.epochs,</span>
<span id="cb388-4"><a href="#cb388-4" aria-hidden="true" tabindex="-1"></a>    accelerator<span class="op">=</span>fun_control[<span class="st">"accelerator"</span>],</span>
<span id="cb388-5"><a href="#cb388-5" aria-hidden="true" tabindex="-1"></a>    devices<span class="op">=</span>fun_control[<span class="st">"devices"</span>],</span>
<span id="cb388-6"><a href="#cb388-6" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span>TensorBoardLogger(</span>
<span id="cb388-7"><a href="#cb388-7" aria-hidden="true" tabindex="-1"></a>        save_dir<span class="op">=</span>fun_control[<span class="st">"TENSORBOARD_PATH"</span>],</span>
<span id="cb388-8"><a href="#cb388-8" aria-hidden="true" tabindex="-1"></a>        version<span class="op">=</span>config_id,</span>
<span id="cb388-9"><a href="#cb388-9" aria-hidden="true" tabindex="-1"></a>        default_hp_metric<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb388-10"><a href="#cb388-10" aria-hidden="true" tabindex="-1"></a>        log_graph<span class="op">=</span>fun_control[<span class="st">"log_graph"</span>],</span>
<span id="cb388-11"><a href="#cb388-11" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb388-12"><a href="#cb388-12" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb388-13"><a href="#cb388-13" aria-hidden="true" tabindex="-1"></a>        EarlyStopping(monitor<span class="op">=</span><span class="st">"val_loss"</span>, patience<span class="op">=</span>config[<span class="st">"patience"</span>], mode<span class="op">=</span><span class="st">"min"</span>, strict<span class="op">=</span><span class="va">False</span>, verbose<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb388-14"><a href="#cb388-14" aria-hidden="true" tabindex="-1"></a>        ModelCheckpoint(</span>
<span id="cb388-15"><a href="#cb388-15" aria-hidden="true" tabindex="-1"></a>            dirpath<span class="op">=</span>os.path.join(fun_control[<span class="st">"CHECKPOINT_PATH"</span>], config_id), save_last<span class="op">=</span><span class="va">True</span></span>
<span id="cb388-16"><a href="#cb388-16" aria-hidden="true" tabindex="-1"></a>        ), </span>
<span id="cb388-17"><a href="#cb388-17" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb388-18"><a href="#cb388-18" aria-hidden="true" tabindex="-1"></a>    enable_progress_bar<span class="op">=</span>enable_progress_bar,</span>
<span id="cb388-19"><a href="#cb388-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb388-20"><a href="#cb388-20" aria-hidden="true" tabindex="-1"></a>trainer.fit(model<span class="op">=</span>model, datamodule<span class="op">=</span>dm)    </span>
<span id="cb388-21"><a href="#cb388-21" aria-hidden="true" tabindex="-1"></a>test_result <span class="op">=</span> trainer.test(datamodule<span class="op">=</span>dm, ckpt_path<span class="op">=</span><span class="st">"last"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>As shown in the code above, the last checkpoint ist saved.</p></li>
<li><p><code>spotpython</code>’s method <code>load_light_from_checkpoint</code> is used to load the last checkpoint and to get the model’s weights and biases. It requires the <code>fun_control</code> dictionary and the <code>config_id</code> as input to find the correct checkpoint.</p></li>
<li><p>Now, the model is trained and the weights and biases are available.</p></li>
</ul>
</section>
</section>
<section id="visualizing-the-neural-network-architecture" class="level2" data-number="39.4">
<h2 data-number="39.4" class="anchored" data-anchor-id="visualizing-the-neural-network-architecture"><span class="header-section-number">39.4</span> Visualizing the Neural Network Architecture</h2>
<div id="249e2425" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb389"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb389-1"><a href="#cb389-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the device</span></span>
<span id="cb389-2"><a href="#cb389-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.utils.device <span class="im">import</span> getDevice</span>
<span id="cb389-3"><a href="#cb389-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> getDevice()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="viz_net_spotpython" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb390"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb390-1"><a href="#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.plot.xai <span class="im">import</span> viz_net</span>
<span id="cb390-2"><a href="#cb390-2" aria-hidden="true" tabindex="-1"></a>viz_net(model, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./model_architecture.png" class="img-fluid figure-img"></p>
<figcaption>architecture</figcaption>
</figure>
</div>
</section>
<section id="xai-methods" class="level2" data-number="39.5">
<h2 data-number="39.5" class="anchored" data-anchor-id="xai-methods"><span class="header-section-number">39.5</span> XAI Methods</h2>
<ul>
<li><code>spotpython</code> provides methods to explain the model’s predictions. The following neural network elements can be analyzed:</li>
</ul>
<section id="weights" class="level3" data-number="39.5.1">
<h3 data-number="39.5.1" class="anchored" data-anchor-id="weights"><span class="header-section-number">39.5.1</span> Weights</h3>
<ul>
<li>Weights are the parameters of the neural network that are learned from the data during training. They connect neurons between layers and determine the strength and direction of the signal sent from one neuron to another. The network adjusts the weights during training to minimize the error between the predicted output and the actual output.</li>
<li>Interpretation of the weights: A high weight value indicates a strong influence of the input neuron on the output. Positive weights suggest a positive correlation, whereas negative weights suggest an inverse relationship between neurons.</li>
</ul>
</section>
<section id="activations" class="level3" data-number="39.5.2">
<h3 data-number="39.5.2" class="anchored" data-anchor-id="activations"><span class="header-section-number">39.5.2</span> Activations</h3>
<ul>
<li>Activations are the outputs produced by neurons after applying an activation function to the weighted sum of inputs. The activation function (e.g., ReLU, sigmoid, tanh) adds non-linearity to the model, allowing it to learn more complex relationships.</li>
<li>Interpretation of the activations: The value of activations indicates the intensity of the signal passed to the next layer. Certain activation patterns can highlight which features or parts of the data the network is focusing on.</li>
</ul>
</section>
<section id="gradients" class="level3" data-number="39.5.3">
<h3 data-number="39.5.3" class="anchored" data-anchor-id="gradients"><span class="header-section-number">39.5.3</span> Gradients</h3>
<ul>
<li>Gradients are the partial derivatives of the loss function with respect to different parameters (weights) of the network. During backpropagation, gradients are used to update the weights in the direction that reduces the loss by methods like gradient descent.</li>
<li>Interpretation of the gradients: The magnitude of the gradient indicates how much a parameter should change to reduce the error. A large gradient implies a steeper slope and a bigger update, while a small gradient suggests that the parameter is near an optimal point. If gradients are too small (vanishing gradient problem), the network may learn slowly or stop learning. If they are too large (exploding gradient problem), the updates may be unstable.</li>
<li><code>sptpython</code> provides the method <code>get_gradients</code> to get the gradients of the model.</li>
</ul>
<div id="import_xai" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb391"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb391-1"><a href="#cb391-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.plot.xai <span class="im">import</span> (get_activations, get_gradients, get_weights, visualize_weights, visualize_gradients, visualize_mean_activations, visualize_gradient_distributions, visualize_weights_distributions, visualize_activations_distributions)</span>
<span id="cb391-2"><a href="#cb391-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> config[<span class="st">"batch_size"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="getting-the-weights" class="level3" data-number="39.5.4">
<h3 data-number="39.5.4" class="anchored" data-anchor-id="getting-the-weights"><span class="header-section-number">39.5.4</span> Getting the Weights</h3>
<div id="get_weights" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb392"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb392-1"><a href="#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.plot.xai <span class="im">import</span> sort_layers</span>
<span id="cb392-2"><a href="#cb392-2" aria-hidden="true" tabindex="-1"></a>weights, _ <span class="op">=</span> get_weights(model)</span>
<span id="cb392-3"><a href="#cb392-3" aria-hidden="true" tabindex="-1"></a><span class="co"># sort_layers(weights)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="visualize_weights" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb393"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb393-1"><a href="#cb393-1" aria-hidden="true" tabindex="-1"></a>visualize_weights(model, absolute<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"GreenYellowRed"</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3200 values in Layer Layer 0. Geometry: (320, 10)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-2.png" id="visualize_weights-1" width="224" height="505" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>51200 values in Layer Layer 3. Geometry: (160, 320)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-4.png" id="visualize_weights-2" width="500" height="392" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>51200 values in Layer Layer 6. Geometry: (320, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-6.png" id="visualize_weights-3" width="378" height="505" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>51200 values in Layer Layer 9. Geometry: (160, 320)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-8.png" id="visualize_weights-4" width="500" height="392" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>25600 values in Layer Layer 12. Geometry: (160, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-10.png" id="visualize_weights-5" width="513" height="463" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>12800 values in Layer Layer 15. Geometry: (80, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-12.png" id="visualize_weights-6" width="492" height="392" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>6400 values in Layer Layer 18. Geometry: (80, 80)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-14.png" id="visualize_weights-7" width="504" height="463" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>80 values in Layer Layer 21. Geometry: (1, 80)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights-output-16.png" id="visualize_weights-8" width="508" height="174" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-visualize_weights_distributions" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb402"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb402-1"><a href="#cb402-1" aria-hidden="true" tabindex="-1"></a>visualize_weights_distributions(model, color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span><span class="dv">0</span><span class="sc">}</span><span class="ss">"</span>, columns<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>n:8</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_weights_distributions-output-2.png" id="visualize_weights_distributions" width="859" height="459" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="getting-the-activations" class="level3" data-number="39.5.5">
<h3 data-number="39.5.5" class="anchored" data-anchor-id="getting-the-activations"><span class="header-section-number">39.5.5</span> Getting the Activations</h3>
<div id="get_activations" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb404"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb404-1"><a href="#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.plot.xai <span class="im">import</span> get_activations</span>
<span id="cb404-2"><a href="#cb404-2" aria-hidden="true" tabindex="-1"></a>activations, mean_activations, layer_sizes <span class="op">=</span> get_activations(net<span class="op">=</span>model, fun_control<span class="op">=</span>fun_control, batch_size<span class="op">=</span>batch_size, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train &amp; val data.
train samples: 160, val samples: 106 generated for train &amp; val data.
LightDataModule.train_dataloader(). data_train size: 160</code></pre>
</div>
</div>
<div id="visualize_mean_activations" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb406"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb406-1"><a href="#cb406-1" aria-hidden="true" tabindex="-1"></a>visualize_mean_activations(mean_activations, layer_sizes<span class="op">=</span>layer_sizes, absolute<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"GreenYellowRed"</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>320 values in Layer 0. Geometry: (1, 320)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_mean_activations-output-2.png" id="visualize_mean_activations-1" width="512" height="170" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>160 values in Layer 3. Geometry: (1, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_mean_activations-output-4.png" id="visualize_mean_activations-2" width="507" height="172" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>320 values in Layer 6. Geometry: (1, 320)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_mean_activations-output-6.png" id="visualize_mean_activations-3" width="507" height="170" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>160 values in Layer 9. Geometry: (1, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_mean_activations-output-8.png" id="visualize_mean_activations-4" width="507" height="172" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>160 values in Layer 12. Geometry: (1, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_mean_activations-output-10.png" id="visualize_mean_activations-5" width="507" height="172" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>80 values in Layer 15. Geometry: (1, 80)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_mean_activations-output-12.png" id="visualize_mean_activations-6" width="507" height="174" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>80 values in Layer 18. Geometry: (1, 80)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_mean_activations-output-14.png" id="visualize_mean_activations-7" width="507" height="174" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-visualize_activations_distributions" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb414"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb414-1"><a href="#cb414-1" aria-hidden="true" tabindex="-1"></a>visualize_activations_distributions(activations<span class="op">=</span>activations,</span>
<span id="cb414-2"><a href="#cb414-2" aria-hidden="true" tabindex="-1"></a>                                    net<span class="op">=</span>model, color<span class="op">=</span><span class="st">"C0"</span>, columns<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_activations_distributions-output-1.png" id="visualize_activations_distributions" width="891" height="459" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="getting-the-gradients" class="level3" data-number="39.5.6">
<h3 data-number="39.5.6" class="anchored" data-anchor-id="getting-the-gradients"><span class="header-section-number">39.5.6</span> Getting the Gradients</h3>
<div id="get_gradients" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb415"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb415-1"><a href="#cb415-1" aria-hidden="true" tabindex="-1"></a>gradients, _ <span class="op">=</span> get_gradients(net<span class="op">=</span>model, fun_control<span class="op">=</span>fun_control, batch_size<span class="op">=</span>batch_size, device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train &amp; val data.
train samples: 160, val samples: 106 generated for train &amp; val data.
LightDataModule.train_dataloader(). data_train size: 160</code></pre>
</div>
</div>
<div id="visualize_gradients" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb417"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb417-1"><a href="#cb417-1" aria-hidden="true" tabindex="-1"></a>visualize_gradients(model, fun_control, batch_size, absolute<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"GreenYellowRed"</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>), device<span class="op">=</span>device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train &amp; val data.
train samples: 160, val samples: 106 generated for train &amp; val data.
LightDataModule.train_dataloader(). data_train size: 160
3200 values in Layer layers.0.weight. Geometry: (320, 10)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-2.png" id="visualize_gradients-1" width="287" height="505" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>51200 values in Layer layers.3.weight. Geometry: (160, 320)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-4.png" id="visualize_gradients-2" width="509" height="392" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>51200 values in Layer layers.6.weight. Geometry: (320, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-6.png" id="visualize_gradients-3" width="390" height="505" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>51200 values in Layer layers.9.weight. Geometry: (160, 320)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-8.png" id="visualize_gradients-4" width="516" height="392" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>25600 values in Layer layers.12.weight. Geometry: (160, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-10.png" id="visualize_gradients-5" width="526" height="463" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>12800 values in Layer layers.15.weight. Geometry: (80, 160)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-12.png" id="visualize_gradients-6" width="501" height="392" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>6400 values in Layer layers.18.weight. Geometry: (80, 80)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-14.png" id="visualize_gradients-7" width="525" height="463" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>80 values in Layer layers.21.weight. Geometry: (1, 80)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradients-output-16.png" id="visualize_gradients-8" width="523" height="174" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-visualize_gradient_distributions" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb426"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb426-1"><a href="#cb426-1" aria-hidden="true" tabindex="-1"></a>visualize_gradient_distributions(model, fun_control, batch_size<span class="op">=</span>batch_size, color<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span><span class="dv">0</span><span class="sc">}</span><span class="ss">"</span>, device<span class="op">=</span>device, columns<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train &amp; val data.
train samples: 160, val samples: 106 generated for train &amp; val data.
LightDataModule.train_dataloader(). data_train size: 160
n:8</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/visualize_gradient_distributions-output-2.png" id="visualize_gradient_distributions" width="715" height="668" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="feature-attributions" class="level2" data-number="39.6">
<h2 data-number="39.6" class="anchored" data-anchor-id="feature-attributions"><span class="header-section-number">39.6</span> Feature Attributions</h2>
<section id="integrated-gradients" class="level3" data-number="39.6.1">
<h3 data-number="39.6.1" class="anchored" data-anchor-id="integrated-gradients"><span class="header-section-number">39.6.1</span> Integrated Gradients</h3>
<div id="cell-get_attributions_xai" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb428"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb428-1"><a href="#cb428-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.plot.xai <span class="im">import</span> get_attributions, plot_attributions</span>
<span id="cb428-2"><a href="#cb428-2" aria-hidden="true" tabindex="-1"></a>df_att <span class="op">=</span> get_attributions(spot_tuner, fun_control, attr_method<span class="op">=</span><span class="st">"IntegratedGradients"</span>, n_rel<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb428-3"><a href="#cb428-3" aria-hidden="true" tabindex="-1"></a>plot_attributions(df_att, attr_method<span class="op">=</span><span class="st">"IntegratedGradients"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 12789.875, 'hp_metric': 12789.875}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 512, 'act_fn': LeakyReLU(), 'optimizer': 'Adam', 'dropout_prob': np.float64(0.0), 'lr_mult': np.float64(5.423013100805973), 'patience': 4, 'batch_norm': False, 'initialization': 'xavier_normal'}
Loading model with 16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN from runs/saved_models/16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.0, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.0, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.0, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): LeakyReLU()
    (14): Dropout(p=0.0, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): LeakyReLU()
    (17): Dropout(p=0.0, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): LeakyReLU()
    (20): Dropout(p=0.0, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting test data.
test samples: 177 generated for test data.
LightDataModule.test_dataloader(). Test set size: 177</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/get_attributions_xai-output-2.png" id="get_attributions_xai" width="839" height="529" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="deep-lift" class="level3" data-number="39.6.2">
<h3 data-number="39.6.2" class="anchored" data-anchor-id="deep-lift"><span class="header-section-number">39.6.2</span> Deep Lift</h3>
<div id="cell-get_attributions_deep_lift" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb430"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb430-1"><a href="#cb430-1" aria-hidden="true" tabindex="-1"></a>df_lift <span class="op">=</span> get_attributions(spot_tuner, fun_control, attr_method<span class="op">=</span><span class="st">"DeepLift"</span>,n_rel<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb430-2"><a href="#cb430-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_lift)</span>
<span id="cb430-3"><a href="#cb430-3" aria-hidden="true" tabindex="-1"></a>plot_attributions(df_lift,  attr_method<span class="op">=</span><span class="st">"DeepLift"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 12874.1533203125, 'hp_metric': 12874.1533203125}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 512, 'act_fn': LeakyReLU(), 'optimizer': 'Adam', 'dropout_prob': np.float64(0.0), 'lr_mult': np.float64(5.423013100805973), 'patience': 4, 'batch_norm': False, 'initialization': 'xavier_normal'}
Loading model with 16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN from runs/saved_models/16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.0, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.0, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.0, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): LeakyReLU()
    (14): Dropout(p=0.0, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): LeakyReLU()
    (17): Dropout(p=0.0, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): LeakyReLU()
    (20): Dropout(p=0.0, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting test data.
test samples: 177 generated for test data.
LightDataModule.test_dataloader(). Test set size: 177
   Feature Index Feature  DeepLiftAttribution
0              2     bmi            29.064596
1              3      bp            22.551331
2              7  s4_tch            22.081400
3              8  s5_ltg            21.914930
4              6  s3_hdl            21.385828
5              9  s6_glu            19.644268
6              5  s2_ldl            19.536001
7              0     age            18.236382
8              1     sex            17.780630
9              4   s1_tc            16.837126</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/get_attributions_deep_lift-output-2.png" id="get_attributions_deep_lift" width="839" height="529" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="feature-ablation" class="level3" data-number="39.6.3">
<h3 data-number="39.6.3" class="anchored" data-anchor-id="feature-ablation"><span class="header-section-number">39.6.3</span> Feature Ablation</h3>
<div id="get_attributions_feature_ablation" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb432"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb432-1"><a href="#cb432-1" aria-hidden="true" tabindex="-1"></a>df_fl <span class="op">=</span> get_attributions(spot_tuner, fun_control, attr_method<span class="op">=</span><span class="st">"FeatureAblation"</span>,n_rel<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 9828.767578125, 'hp_metric': 9828.767578125}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 512, 'act_fn': LeakyReLU(), 'optimizer': 'Adam', 'dropout_prob': np.float64(0.0), 'lr_mult': np.float64(5.423013100805973), 'patience': 4, 'batch_norm': False, 'initialization': 'xavier_normal'}
Loading model with 16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN from runs/saved_models/16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.0, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.0, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.0, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): LeakyReLU()
    (14): Dropout(p=0.0, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): LeakyReLU()
    (17): Dropout(p=0.0, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): LeakyReLU()
    (20): Dropout(p=0.0, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting test data.
test samples: 177 generated for test data.
LightDataModule.test_dataloader(). Test set size: 177</code></pre>
</div>
</div>
<div id="cell-plot_attributions_feature_ablation" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb434"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb434-1"><a href="#cb434-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_fl)</span>
<span id="cb434-2"><a href="#cb434-2" aria-hidden="true" tabindex="-1"></a>plot_attributions(df_fl, attr_method<span class="op">=</span><span class="st">"FeatureAblation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Feature Index Feature  FeatureAblationAttribution
0              2     bmi                   21.125242
1              7  s4_tch                   16.789080
2              8  s5_ltg                   16.561960
3              6  s3_hdl                   15.699564
4              5  s2_ldl                   14.655724
5              3      bp                   14.344009
6              9  s6_glu                   14.282858
7              4   s1_tc                   11.429279
8              0     age                   10.413883
9              1     sex                    9.101063</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/plot_attributions_feature_ablation-output-2.png" id="plot_attributions_feature_ablation" width="839" height="529" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="conductance" class="level2" data-number="39.7">
<h2 data-number="39.7" class="anchored" data-anchor-id="conductance"><span class="header-section-number">39.7</span> Conductance</h2>
<div id="cell-get_conductance" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb436"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb436-1"><a href="#cb436-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.plot.xai <span class="im">import</span> plot_conductance_last_layer, get_weights_conductance_last_layer</span>
<span id="cb436-2"><a href="#cb436-2" aria-hidden="true" tabindex="-1"></a>weights_last, layer_conductance_last <span class="op">=</span> get_weights_conductance_last_layer(spot_tuner, fun_control)</span>
<span id="cb436-3"><a href="#cb436-3" aria-hidden="true" tabindex="-1"></a>plot_conductance_last_layer(weights_last, layer_conductance_last, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_model result: {'val_loss': 3143.05078125, 'hp_metric': 3143.05078125}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 512, 'act_fn': LeakyReLU(), 'optimizer': 'Adam', 'dropout_prob': np.float64(0.0), 'lr_mult': np.float64(5.423013100805973), 'patience': 4, 'batch_norm': False, 'initialization': 'xavier_normal'}
Loading model with 16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN from runs/saved_models/16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.0, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.0, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.0, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): LeakyReLU()
    (14): Dropout(p=0.0, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): LeakyReLU()
    (17): Dropout(p=0.0, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): LeakyReLU()
    (20): Dropout(p=0.0, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_model result: {'val_loss': 14119.68359375, 'hp_metric': 14119.68359375}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 512, 'act_fn': LeakyReLU(), 'optimizer': 'Adam', 'dropout_prob': np.float64(0.0), 'lr_mult': np.float64(5.423013100805973), 'patience': 4, 'batch_norm': False, 'initialization': 'xavier_normal'}
Loading model with 16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN from runs/saved_models/16_4096_512_LeakyReLU_Adam_0.0_5.423_4_False_xavier_normal_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.0, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.0, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.0, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): LeakyReLU()
    (14): Dropout(p=0.0, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): LeakyReLU()
    (17): Dropout(p=0.0, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): LeakyReLU()
    (20): Dropout(p=0.0, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
Conductance analysis for layer:  Linear(in_features=80, out_features=1, bias=True)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="602_spot_lightning_xai_files/figure-html/get_conductance-output-2.png" id="get_conductance" width="527" height="529" class="figure-img"></p>
</figure>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./601_spot_hpt_light_pinn.html" class="pagination-link" aria-label="Hyperparameter Tuning with PyTorch Lightning: Physics Informed Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning: Physics Informed Neural Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./603_spot_lightning_transformer_introduction.html" class="pagination-link" aria-label="HPT PyTorch Lightning Transformer: Introduction">
        <span class="nav-page-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning Transformer: Introduction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, T. Bartz-Beielstein</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/bartzbeielstein">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>