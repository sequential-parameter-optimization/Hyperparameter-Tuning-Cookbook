[
  {
    "objectID": "037_spot_lightning_save_load_models.html",
    "href": "037_spot_lightning_save_load_models.html",
    "title": "31  Saving and Loading",
    "section": "",
    "text": "31.1 spotPython: Saving and Loading Optimization Experiments\nIn this section, we will show how results from spotPython can be saved and reloaded. Here, spotPython can be used as an optimizer.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-spotpython-as-a-hyperparameter-tuner-37",
    "href": "037_spot_lightning_save_load_models.html#sec-spotpython-as-a-hyperparameter-tuner-37",
    "title": "31  Saving and Loading",
    "section": "31.2 spotPython as a Hyperparameter Tuner",
    "text": "31.2 spotPython as a Hyperparameter Tuner\nIf spotPython is used as a hyperparameter tuner, in addition to the fun_control dictionary a core_model dictionary have to be specified. This will be explained in Section 31.2.2.\nFurthermore, a data set has to be selected and added to the fun_control dictionary. Here, we will use the Diabetes data set.\n\n31.2.1 The Diabetes Data Set\nThe hyperparameter tuning of a PyTorch Lightning network on the Diabetes data set is used as an example. The Diabetes data set is a PyTorch Dataset for regression, which originates from the scikit-learn package, see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes.\nTen baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. The Diabetes data set is described in Table 31.3.\n\n\n\nTable 31.3: The Diabetes data set\n\n\n\n\n\nDescription\nValue\n\n\n\n\nSamples total\n442\n\n\nDimensionality\n10\n\n\nFeatures\nreal, -.2 &lt; x &lt; .2\n\n\nTargets\ninteger 25 - 346\n\n\n\n\n\n\n\nfrom spotPython.utils.device import getDevice\nfrom math import inf\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\n\nMAX_TIME = inf\nFUN_EVALS = 8\nINIT_SIZE = 5\nWORKERS = 0\nPREFIX=\"037\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.4\nTORCH_METRIC = \"mean_squared_error\"\ndataset = Diabetes()\n\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    _torchmetric=TORCH_METRIC,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    data_set=dataset,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    log_level=10,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=TEST_SIZE,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_07_05_21_35_23\nCreated spot_tensorboard_path: runs/spot_logs/037_p040025_2024-07-05_21-35-23 for SummaryWriter()\n\n\n\n\n31.2.2 Adding a core_model to the fun_control Dictionary\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].\nThe core_model dictionary contains the hyperparameters of the model to be tuned. These hyperparameters can be specified and modified with as shown in the following code.\n\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=NetLightRegression,\n                              hyper_dict=LightHyperDict)\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [4, 5])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adam\",\n                \"RAdam\",\n            ])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.05, 1.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\n                \"ReLU\",\n                \"LeakyReLU\"\n            ] )\n\nSetting hyperparameter epochs to value [4, 5].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter batch_size to value [4, 5].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter optimizer to value ['Adam', 'RAdam'].\nVariable type is factor.\nCore type is str.\nCalling modify_hyper_parameter_levels().\nSetting hyperparameter dropout_prob to value [0.01, 0.1].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter lr_mult to value [0.05, 1.0].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter patience to value [2, 3].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter act_fn to value ['ReLU', 'LeakyReLU'].\nVariable type is factor.\nCore type is instance().\nCalling modify_hyper_parameter_levels().\n\n\n\nimport pprint as pp\nfrom spotPython.hyperparameters.values import set_factor_hyperparameter_values\nset_factor_hyperparameter_values(fun_control, \"initialization\", [\"Default\"])\npp.pprint(fun_control)\n\n{'CHECKPOINT_PATH': 'runs/saved_models/',\n 'DATASET_PATH': 'data/',\n 'PREFIX': '037',\n 'RESULTS_PATH': 'results/',\n 'TENSORBOARD_PATH': 'runs/',\n '_L_in': 10,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'accelerator': 'auto',\n 'converters': None,\n 'core_model': &lt;class 'spotPython.light.regression.netlightregression.NetLightRegression'&gt;,\n 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                      'core_model_parameter_type': 'instance()',\n                                      'default': 'ReLU',\n                                      'levels': ['ReLU', 'LeakyReLU'],\n                                      'lower': 0,\n                                      'transform': 'None',\n                                      'type': 'factor',\n                                      'upper': 1},\n                           'batch_size': {'default': 4,\n                                          'lower': 4,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 5},\n                           'dropout_prob': {'default': 0.01,\n                                            'lower': 0.01,\n                                            'transform': 'None',\n                                            'type': 'float',\n                                            'upper': 0.1},\n                           'epochs': {'default': 4,\n                                      'lower': 4,\n                                      'transform': 'transform_power_2_int',\n                                      'type': 'int',\n                                      'upper': 5},\n                           'initialization': {'core_model_parameter_type': 'str',\n                                              'default': 'Default',\n                                              'levels': ['Default'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 0},\n                           'l1': {'default': 3,\n                                  'lower': 3,\n                                  'transform': 'transform_power_2_int',\n                                  'type': 'int',\n                                  'upper': 8},\n                           'lr_mult': {'default': 1.0,\n                                       'lower': 0.05,\n                                       'transform': 'None',\n                                       'type': 'float',\n                                       'upper': 1.0},\n                           'optimizer': {'class_name': 'torch.optim',\n                                         'core_model_parameter_type': 'str',\n                                         'default': 'SGD',\n                                         'levels': ['Adam', 'RAdam'],\n                                         'lower': 0,\n                                         'transform': 'None',\n                                         'type': 'factor',\n                                         'upper': 1},\n                           'patience': {'default': 2,\n                                        'lower': 2,\n                                        'transform': 'transform_power_2_int',\n                                        'type': 'int',\n                                        'upper': 3}},\n 'core_model_hyper_dict_default': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                              'core_model_parameter_type': 'instance()',\n                                              'default': 'ReLU',\n                                              'levels': ['Sigmoid',\n                                                         'Tanh',\n                                                         'ReLU',\n                                                         'LeakyReLU',\n                                                         'ELU',\n                                                         'Swish'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 5},\n                                   'batch_size': {'default': 4,\n                                                  'lower': 1,\n                                                  'transform': 'transform_power_2_int',\n                                                  'type': 'int',\n                                                  'upper': 4},\n                                   'dropout_prob': {'default': 0.01,\n                                                    'lower': 0.0,\n                                                    'transform': 'None',\n                                                    'type': 'float',\n                                                    'upper': 0.25},\n                                   'epochs': {'default': 4,\n                                              'lower': 4,\n                                              'transform': 'transform_power_2_int',\n                                              'type': 'int',\n                                              'upper': 9},\n                                   'initialization': {'core_model_parameter_type': 'str',\n                                                      'default': 'Default',\n                                                      'levels': ['Default',\n                                                                 'Kaiming',\n                                                                 'Xavier'],\n                                                      'lower': 0,\n                                                      'transform': 'None',\n                                                      'type': 'factor',\n                                                      'upper': 2},\n                                   'l1': {'default': 3,\n                                          'lower': 3,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 8},\n                                   'lr_mult': {'default': 1.0,\n                                               'lower': 0.1,\n                                               'transform': 'None',\n                                               'type': 'float',\n                                               'upper': 10.0},\n                                   'optimizer': {'class_name': 'torch.optim',\n                                                 'core_model_parameter_type': 'str',\n                                                 'default': 'SGD',\n                                                 'levels': ['Adadelta',\n                                                            'Adagrad',\n                                                            'Adam',\n                                                            'AdamW',\n                                                            'SparseAdam',\n                                                            'Adamax',\n                                                            'ASGD',\n                                                            'NAdam',\n                                                            'RAdam',\n                                                            'RMSprop',\n                                                            'Rprop',\n                                                            'SGD'],\n                                                 'lower': 0,\n                                                 'transform': 'None',\n                                                 'type': 'factor',\n                                                 'upper': 11},\n                                   'patience': {'default': 2,\n                                                'lower': 2,\n                                                'transform': 'transform_power_2_int',\n                                                'type': 'int',\n                                                'upper': 6}},\n 'core_model_name': None,\n 'counter': 0,\n 'data': None,\n 'data_dir': './data',\n 'data_module': None,\n 'data_set': &lt;spotPython.data.diabetes.Diabetes object at 0x3928e1c10&gt;,\n 'data_set_name': None,\n 'db_dict_name': None,\n 'design': None,\n 'device': 'mps',\n 'devices': 1,\n 'enable_progress_bar': False,\n 'eval': None,\n 'fun_evals': 8,\n 'fun_repeats': 1,\n 'horizon': None,\n 'hyperdict': None,\n 'infill_criterion': 'y',\n 'k_folds': 3,\n 'log_graph': False,\n 'log_level': 10,\n 'loss_function': None,\n 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),\n 'max_surrogate_points': 30,\n 'max_time': inf,\n 'metric_params': {},\n 'metric_river': None,\n 'metric_sklearn': None,\n 'metric_sklearn_name': None,\n 'metric_torch': None,\n 'model_dict': {},\n 'n_points': 1,\n 'n_samples': None,\n 'n_total': None,\n 'noise': False,\n 'num_workers': 0,\n 'ocba_delta': 0,\n 'oml_grace_period': None,\n 'optimizer': None,\n 'path': None,\n 'prep_model': None,\n 'prep_model_name': None,\n 'progress_file': None,\n 'save_model': False,\n 'scenario': None,\n 'seed': 123,\n 'show_batch_interval': 1000000,\n 'show_models': False,\n 'show_progress': True,\n 'shuffle': None,\n 'sigma': 0.0,\n 'spot_tensorboard_path': 'runs/spot_logs/037_p040025_2024-07-05_21-35-23',\n 'spot_writer': &lt;torch.utils.tensorboard.writer.SummaryWriter object at 0x3927bd610&gt;,\n 'target_column': None,\n 'target_type': None,\n 'task': None,\n 'test': None,\n 'test_seed': 1234,\n 'test_size': 0.4,\n 'tolerance_x': 1.4901161193847656e-08,\n 'train': None,\n 'upper': array([ 8.  ,  9.  ,  4.  ,  5.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),\n 'var_name': ['l1',\n              'epochs',\n              'batch_size',\n              'act_fn',\n              'optimizer',\n              'dropout_prob',\n              'lr_mult',\n              'patience',\n              'initialization'],\n 'var_type': ['int',\n              'int',\n              'int',\n              'factor',\n              'factor',\n              'float',\n              'float',\n              'int',\n              'factor'],\n 'verbosity': 0,\n 'weight_coeff': 0.0,\n 'weights': 1.0,\n 'weights_entry': None}\n\n\n\n\n31.2.3 design_control, surrogate_control Dictionaries and the Objective Function\nAfter specifying the design_control and surrogate_control dictionaries, the objective function fun from the class HyperLight [SOURCE] is selected. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\nThen, the hyperparameter tuning can be started.\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init(init_size=INIT_SIZE)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                            n_theta=2)\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=10).fun\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      23630.99609375       │\n│         val_loss          │      23630.99609375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 23630.99609375, 'hp_metric': 23630.99609375}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      23312.27734375       │\n│         val_loss          │      23312.27734375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 23312.27734375, 'hp_metric': 23312.27734375}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       19552.1796875       │\n│         val_loss          │       19552.1796875       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 19552.1796875, 'hp_metric': 19552.1796875}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       23842.2265625       │\n│         val_loss          │       23842.2265625       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 23842.2265625, 'hp_metric': 23842.2265625}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      23955.794921875      │\n│         val_loss          │      23955.794921875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 23955.794921875, 'hp_metric': 23955.794921875}\n\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\n\n\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      19014.068359375      │\n│         val_loss          │      19014.068359375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 19014.068359375, 'hp_metric': 19014.068359375}\nspotPython tuning: 19014.068359375 [########--] 75.00% \n\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      19208.46484375       │\n│         val_loss          │      19208.46484375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 19208.46484375, 'hp_metric': 19208.46484375}\nspotPython tuning: 19014.068359375 [#########-] 87.50% \n\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      19290.615234375      │\n│         val_loss          │      19290.615234375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'val_loss': 19290.615234375, 'hp_metric': 19290.615234375}\nspotPython tuning: 19014.068359375 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x3973fead0&gt;\n\n\nThe tuned hyperparameters can be obtained as a dictionary with the following code.\n\nfrom spotPython.hyperparameters.values import get_tuned_hyperparameters\nget_tuned_hyperparameters(spot_tuner)\n\n{'l1': 7.0,\n 'epochs': 5.0,\n 'batch_size': 4.0,\n 'act_fn': 0.0,\n 'optimizer': 1.0,\n 'dropout_prob': 0.09699240513877071,\n 'lr_mult': 0.9054114496467818,\n 'patience': 3.0,\n 'initialization': 0.0}\n\n\nHere , the numerical levels of the hyperparameters are used as keys in the dictionary. If the fun_control dictionary is used, the names of the hyperparameters are used as keys in the dictionary.\n\nget_tuned_hyperparameters(spot_tuner, fun_control)\n\n{'l1': 7.0,\n 'epochs': 5.0,\n 'batch_size': 4.0,\n 'act_fn': 'ReLU',\n 'optimizer': 'RAdam',\n 'dropout_prob': 0.09699240513877071,\n 'lr_mult': 0.9054114496467818,\n 'patience': 3.0,\n 'initialization': 'Default'}\n\n\n\nPREFIX = fun_control[\"PREFIX\"]\nfilename = get_experiment_filename(PREFIX)\nspot_tuner.save_experiment(filename=filename)\nprint(f\"filename: {filename}\")\n\nfilename: spot_037_experiment.pickle\n\n\nThe results from the experiment are stored in the pickle file spot_037_experiment.pickle. The experiment can be reloaded with the following code.\n\n(spot_tuner_1, fun_control_1, design_control_1,\n    surrogate_control_1, optimizer_control_1) = load_experiment(filename)\n\nPlot the progress of the original experiment are identical to the reloaded experiment.\n\nspot_tuner.plot_progress(log_y=True)\nspot_tuner_1.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, the tuned hyperparameters can be obtained as a dictionary from the reloaded experiment with the following code.\n\nget_tuned_hyperparameters(spot_tuner_1, fun_control_1)\n\n{'l1': 7.0,\n 'epochs': 5.0,\n 'batch_size': 4.0,\n 'act_fn': 'ReLU',\n 'optimizer': 'RAdam',\n 'dropout_prob': 0.09699240513877071,\n 'lr_mult': 0.9054114496467818,\n 'patience': 3.0,\n 'initialization': 'Default'}\n\n\n\n\n\n\n\n\nSummary: Saving and Loading Hyperparameter-Tuning Experiments\n\n\n\n\nIf spotPython is used as an hyperparameter tuner (with an hyperparameter dictionary), experiments can be saved and reloaded with the save_experiment and load_experiment functions.\nThe tuned hyperparameters can be obtained with the get_tuned_hyperparameters function.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-saving-and-loading-pytorch-lightning-models-37",
    "href": "037_spot_lightning_save_load_models.html#sec-saving-and-loading-pytorch-lightning-models-37",
    "title": "31  Saving and Loading",
    "section": "31.3 Saving and Loading PyTorch Lightning Models",
    "text": "31.3 Saving and Loading PyTorch Lightning Models\nSection 31.1 and Section 31.2 explained how to save and load optimization and hyperparameter tuning experiments and how to get the tuned hyperparameters as a dictionary. This section shows how to save and load PyTorch Lightning models.\n\n31.3.1 Get the Tuned Architecture\nIn contrast to the function get_tuned_hyperparameters, the function get_tuned_architecture returns the tuned architecture of the model as a dictionary. Here, the transformations are already applied to the numerical levels of the hyperparameters and the encoding (and types) are the original types of the hyperparameters used by the model. The config dictionary can be passed to the model without any modifications.\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\npprint.pprint(config)\n\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.09699240513877071,\n 'epochs': 32,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.9054114496467818,\n 'optimizer': 'RAdam',\n 'patience': 8}\n\n\nAfter getting the tuned architecture, the model can be created and tested with the following code.\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\n\n\nLightDataModule.setup(): stage: TrainerFn.TESTING\ntest_size: 0.4 used for test dataset.\nLightDataModule.test_dataloader(). Test set size: 177\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      26001.91796875       │\n│         val_loss          │      26001.91796875       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntest_model result: {'val_loss': 26001.91796875, 'hp_metric': 26001.91796875}\n\n\n(26001.91796875, 26001.91796875)\n\n\n\n\n31.3.2 Load a Model from Checkpoint\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 128, 'epochs': 32, 'batch_size': 16, 'act_fn': ReLU(), 'optimizer': 'RAdam', 'dropout_prob': 0.09699240513877071, 'lr_mult': 0.9054114496467818, 'patience': 8, 'initialization': 'Default'}\nLoading model with 128_32_16_ReLU_RAdam_0.097_0.9054_8_Default_TEST from runs/saved_models/128_32_16_ReLU_RAdam_0.097_0.9054_8_Default_TEST/last.ckpt\nModel: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=128, bias=True)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.09699240513877071, inplace=False)\n    (4): Linear(in_features=128, out_features=64, bias=True)\n    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.09699240513877071, inplace=False)\n    (8): Linear(in_features=64, out_features=64, bias=True)\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout(p=0.09699240513877071, inplace=False)\n    (12): Linear(in_features=64, out_features=32, bias=True)\n    (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (14): ReLU()\n    (15): Dropout(p=0.09699240513877071, inplace=False)\n    (16): Linear(in_features=32, out_features=1, bias=True)\n  )\n)\n\n\n\nvars(model_loaded)\n\n{'training': False,\n '_parameters': OrderedDict(),\n '_buffers': OrderedDict(),\n '_non_persistent_buffers_set': set(),\n '_backward_pre_hooks': OrderedDict(),\n '_backward_hooks': OrderedDict(),\n '_is_full_backward_hook': None,\n '_forward_hooks': OrderedDict(),\n '_forward_hooks_with_kwargs': OrderedDict(),\n '_forward_hooks_always_called': OrderedDict(),\n '_forward_pre_hooks': OrderedDict(),\n '_forward_pre_hooks_with_kwargs': OrderedDict(),\n '_state_dict_hooks': OrderedDict(),\n '_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_post_hooks': OrderedDict(),\n '_modules': OrderedDict([('layers',\n               Sequential(\n                 (0): Linear(in_features=10, out_features=128, bias=True)\n                 (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (2): ReLU()\n                 (3): Dropout(p=0.09699240513877071, inplace=False)\n                 (4): Linear(in_features=128, out_features=64, bias=True)\n                 (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (6): ReLU()\n                 (7): Dropout(p=0.09699240513877071, inplace=False)\n                 (8): Linear(in_features=64, out_features=64, bias=True)\n                 (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (10): ReLU()\n                 (11): Dropout(p=0.09699240513877071, inplace=False)\n                 (12): Linear(in_features=64, out_features=32, bias=True)\n                 (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (14): ReLU()\n                 (15): Dropout(p=0.09699240513877071, inplace=False)\n                 (16): Linear(in_features=32, out_features=1, bias=True)\n               ))]),\n 'prepare_data_per_node': True,\n 'allow_zero_length_dataloader_with_multiple_devices': False,\n '_log_hyperparams': True,\n '_dtype': torch.float32,\n '_device': device(type='mps', index=0),\n '_trainer': None,\n '_example_input_array': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n '_current_fx_name': None,\n '_automatic_optimization': True,\n '_param_requires_grad_state': {},\n '_metric_attributes': None,\n '_compiler_ctx': None,\n '_fabric': None,\n '_fabric_optimizers': [],\n '_L_in': 10,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'metric': &lt;function torchmetrics.functional.regression.mse.mean_squared_error(preds: torch.Tensor, target: torch.Tensor, squared: bool = True, num_outputs: int = 1) -&gt; torch.Tensor&gt;,\n '_hparams_name': 'kwargs',\n '_hparams': \"act_fn\":         ReLU()\n \"batch_size\":     16\n \"dropout_prob\":   0.09699240513877071\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.9054114496467818\n \"optimizer\":      RAdam\n \"patience\":       8,\n '_hparams_initial': \"act_fn\":         ReLU()\n \"batch_size\":     16\n \"dropout_prob\":   0.09699240513877071\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.9054114496467818\n \"optimizer\":      RAdam\n \"patience\":       8}\n\n\n\nimport torch\ntorch.save(model_loaded, \"model.pt\")\n\n\nmymodel = torch.load(\"model.pt\")\n\n\n# show all attributes of the model\nvars(mymodel)\n\n{'training': False,\n '_parameters': OrderedDict(),\n '_buffers': OrderedDict(),\n '_non_persistent_buffers_set': set(),\n '_backward_pre_hooks': OrderedDict(),\n '_backward_hooks': OrderedDict(),\n '_is_full_backward_hook': None,\n '_forward_hooks': OrderedDict(),\n '_forward_hooks_with_kwargs': OrderedDict(),\n '_forward_hooks_always_called': OrderedDict(),\n '_forward_pre_hooks': OrderedDict(),\n '_forward_pre_hooks_with_kwargs': OrderedDict(),\n '_state_dict_hooks': OrderedDict(),\n '_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_post_hooks': OrderedDict(),\n '_modules': OrderedDict([('layers',\n               Sequential(\n                 (0): Linear(in_features=10, out_features=128, bias=True)\n                 (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (2): ReLU()\n                 (3): Dropout(p=0.09699240513877071, inplace=False)\n                 (4): Linear(in_features=128, out_features=64, bias=True)\n                 (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (6): ReLU()\n                 (7): Dropout(p=0.09699240513877071, inplace=False)\n                 (8): Linear(in_features=64, out_features=64, bias=True)\n                 (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (10): ReLU()\n                 (11): Dropout(p=0.09699240513877071, inplace=False)\n                 (12): Linear(in_features=64, out_features=32, bias=True)\n                 (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                 (14): ReLU()\n                 (15): Dropout(p=0.09699240513877071, inplace=False)\n                 (16): Linear(in_features=32, out_features=1, bias=True)\n               ))]),\n 'prepare_data_per_node': True,\n 'allow_zero_length_dataloader_with_multiple_devices': False,\n '_log_hyperparams': True,\n '_dtype': torch.float32,\n '_device': device(type='mps', index=0),\n '_trainer': None,\n '_example_input_array': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n '_current_fx_name': None,\n '_automatic_optimization': True,\n '_param_requires_grad_state': {},\n '_metric_attributes': None,\n '_compiler_ctx': None,\n '_fabric': None,\n '_fabric_optimizers': [],\n '_L_in': 10,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'metric': &lt;function torchmetrics.functional.regression.mse.mean_squared_error(preds: torch.Tensor, target: torch.Tensor, squared: bool = True, num_outputs: int = 1) -&gt; torch.Tensor&gt;,\n '_hparams_name': 'kwargs',\n '_hparams': \"act_fn\":         ReLU()\n \"batch_size\":     16\n \"dropout_prob\":   0.09699240513877071\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.9054114496467818\n \"optimizer\":      RAdam\n \"patience\":       8,\n '_hparams_initial': \"act_fn\":         ReLU()\n \"batch_size\":     16\n \"dropout_prob\":   0.09699240513877071\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.9054114496467818\n \"optimizer\":      RAdam\n \"patience\":       8}",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-converting-a-lightning-model-to-a-plain-torch-model-37",
    "href": "037_spot_lightning_save_load_models.html#sec-converting-a-lightning-model-to-a-plain-torch-model-37",
    "title": "31  Saving and Loading",
    "section": "31.4 Converting a Lightning Model to a Plain Torch Model",
    "text": "31.4 Converting a Lightning Model to a Plain Torch Model\n\n31.4.1 The Function get_removed_attributes_and_base_net\nspotPython provides a function to covert a PyTorch Lightning model to a plain PyTorch model. The function get_removed_attributes_and_base_net returns a tuple with the removed attributes and the base net. The base net is a plain PyTorch model. The removed attributes are the attributes of the PyTorch Lightning model that are not part of the base net.\nThis conversion can be reverted.\n\nimport numpy as np\nimport torch\nfrom spotPython.utils.device import getDevice\nfrom torch.utils.data import random_split\nfrom spotPython.utils.classes import get_removed_attributes_and_base_net\nfrom spotPython.hyperparameters.optimizer import optimizer_handler\nremoved_attributes, torch_net = get_removed_attributes_and_base_net(net=mymodel)\n\n\nprint(removed_attributes)\n\n{'_param_requires_grad_state': {}, '_device': device(type='mps', index=0), '_fabric': None, 'metric': &lt;function mean_squared_error at 0x36c453a60&gt;, '_trainer': None, '_log_hyperparams': True, '_hparams': \"act_fn\":         ReLU()\n\"batch_size\":     16\n\"dropout_prob\":   0.09699240513877071\n\"epochs\":         32\n\"initialization\": Default\n\"l1\":             128\n\"lr_mult\":        0.9054114496467818\n\"optimizer\":      RAdam\n\"patience\":       8, '_example_input_array': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'prepare_data_per_node': True, '_current_fx_name': None, '_hparams_name': 'kwargs', '_metric_attributes': None, '_fabric_optimizers': [], '_dtype': torch.float32, '_torchmetric': 'mean_squared_error', '_hparams_initial': \"act_fn\":         ReLU()\n\"batch_size\":     16\n\"dropout_prob\":   0.09699240513877071\n\"epochs\":         32\n\"initialization\": Default\n\"l1\":             128\n\"lr_mult\":        0.9054114496467818\n\"optimizer\":      RAdam\n\"patience\":       8, 'allow_zero_length_dataloader_with_multiple_devices': False, '_L_in': 10, '_compiler_ctx': None, '_automatic_optimization': True, '_L_out': 1}\n\n\n\nprint(torch_net)\n\nNetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=128, bias=True)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.09699240513877071, inplace=False)\n    (4): Linear(in_features=128, out_features=64, bias=True)\n    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.09699240513877071, inplace=False)\n    (8): Linear(in_features=64, out_features=64, bias=True)\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout(p=0.09699240513877071, inplace=False)\n    (12): Linear(in_features=64, out_features=32, bias=True)\n    (13): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (14): ReLU()\n    (15): Dropout(p=0.09699240513877071, inplace=False)\n    (16): Linear(in_features=32, out_features=1, bias=True)\n  )\n)\n\n\n\n\n31.4.2 An Example how to use the Plain Torch Net\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Load the Diabetes dataset from sklearn\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert the data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n\n# Create a PyTorch dataset\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create a PyTorch dataloader\nbatch_size = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\ntorch_net.to(getDevice(\"cpu\"))\n\n# train the net\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(torch_net.parameters(), lr=0.01)\nn_epochs = 100\nlosses = []\nfor epoch in range(n_epochs):\n    for inputs, targets in train_dataloader:\n        targets = targets.view(-1, 1)\n        optimizer.zero_grad()\n        outputs = torch_net(inputs)\n        loss = criterion(outputs, targets)\n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n# visualize the network training\nplt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html",
    "href": "a_01_intro_to_notebooks.html",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "",
    "text": "A.1 Different Notebook cells\nThere are different cells that the notebook is currently supporting:\nAs a default, every cells in jupyter is set to “code”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#different-notebook-cells",
    "href": "a_01_intro_to_notebooks.html#different-notebook-cells",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "",
    "text": "code cells\nmarkdown cells\nraw cells\n\n\n\nA.1.1 Code cells\nThe code cells are used to execute the code. They are following the logic of the choosen kernel. Therefore, it is important to keep in mind which programming language is currently used. Otherwise one might yield an error because of the wrong syntax.\nThe code cells are executed my be ▶ Run button (can be found in the header of the notebook).\n\n\nA.1.2 Markdown cells\nThe markdown cells are a usefull tool to comment the written code. Especially with the help of headers can the code be brought in a more readable format. If you are not familiar with the markdown syntax, you can find a usefull cheat sheet here: Markdown Cheat Sheeet\n\n\nA.1.3 Raw cells\nThe “Raw NBConvert” cell type can be used to render different code formats into HTML or LaTeX by Sphinx. This information is stored in the notebook metadata and converted appropriately.\n\nA.1.3.1 Usage\nTo select a desired format from within Jupyter, select the cell containing your special code and choose options from the following dropdown menus:\n\nSelect “Raw NBConvert”\nSwitch the Cell Toolbar to “Raw Cell Format” (The cell toolbar can be found under View)\nChose the appropriate “Raw NBConvert Format” within the cell\n\nData Science is fun",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#install-packages",
    "href": "a_01_intro_to_notebooks.html#install-packages",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.2 Install Packages",
    "text": "A.2 Install Packages\nBecause python is a heavily used programming language, there are many different packags that can make your life easier. Sadly, there are only a few standard packages that are already included in your python enviroment. If you have the need to install a new package in your enviroment, you can simply do that by exectuing the following code snippet in a code cell\n!pip install numpy\n\nThe ! is used to run the cell as a shell command\npip is package manager for python packages.\nnumpy is the the package you want to install\n\nHint: It is often usefull to restart the kernel after installing a package, otherwise loading the package could lead to an error.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#load-packages",
    "href": "a_01_intro_to_notebooks.html#load-packages",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.3 Load Packages",
    "text": "A.3 Load Packages\nAfter successfully installing the package it is necessary to import them before you can work with them. The import of the packages is done in the following way:\nimport numpy as np\nThe imported packages are often abbreviated. This is because you need to specify where the function is coming from.\nThe most common abbreviations for data science packages are:\n\nAbbreviations for data science packages\n\n\nAbbreviation\nPackage\nImport\n\n\n\n\nnp\nnumpy\nimport numpy as np\n\n\npd\npandas\nimport pandas as pd\n\n\nplt\nmatplotlib\nimport matplotlib.pyplot as plt\n\n\npx\nplotly\nimport plotly.exprss as px\n\n\ntf\ntensorflow\nimport tensorflow as tf\n\n\nsns\nseaborn\nimport seaborn as sns\n\n\ndt\ndatetime\nimport datetime as dt\n\n\npkl\npickle\nimport pickle as pkl",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#functions-in-python",
    "href": "a_01_intro_to_notebooks.html#functions-in-python",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.4 Functions in Python",
    "text": "A.4 Functions in Python\nBecause python is not using Semicolon’s it is import to keep track of indentation in your code. The indentation works as a placeholder for the semicolons. This is especially important if your are defining loops, functions, etc. …\nExample: We are defining a function that calculates the squared sum of its input parameters\n\ndef squared_sum(x,y): \n    z = x**2 + y**2\n    return z\n\nIf you are working with something that needs indentation, it will be already done by the notebook.\nHint: Keep in mind that is good practice to use the return parameter. If you are not using return and a function has multiple paramaters that you would like to return, it will only return the last one defined.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#list-of-useful-jupyter-notebook-shortcuts",
    "href": "a_01_intro_to_notebooks.html#list-of-useful-jupyter-notebook-shortcuts",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.5 List of Useful Jupyter Notebook Shortcuts",
    "text": "A.5 List of Useful Jupyter Notebook Shortcuts\n\nList of useful Jupyter Notebook Shortcuts\n\n\n\n\n\n\n\nFunction\nKeyboard Shortcut\nMenu Tools\n\n\n\n\nSave notebook\nEsc + s\nFile → Save and Checkpoint\n\n\nCreate new Cell\nEsc + a (above),  Esc + b (below)\nInsert → Cell above; Insert → Cell below\n\n\nRun Cell\nCtrl + enter\nCell → Run Cell\n\n\nCopy Cell\nc\nCopy Key\n\n\nPaste Cell\nv\nPaste Key\n\n\nInterrupt Kernel\nEsc + i i\nKernel → Interrupt\n\n\nRestart Kernel\nEsc + 0 0\nKernel → Restart\n\n\n\nIf you combine everything you can create beautiful graphics\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate 100 random data points along 3 dimensions\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\n\n# Map each onto a scatterplot we'll create with Matplotlib\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale)*500)\nax.set(title=\"Some random data, created with the Jupyter Notebook!\")\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html",
    "href": "a_02_git_intro_en.html",
    "title": "Appendix B — Git Introduction",
    "section": "",
    "text": "B.1 Learning Objectives\nIn this learning unit, you will learn how to set up Git as a version control system for a project. The most important Git commands will be explained. You will learn how to track and manage changes to your projects with Git. Specifically:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#learning-objectives",
    "href": "a_02_git_intro_en.html#learning-objectives",
    "title": "Appendix B — Git Introduction",
    "section": "",
    "text": "Initializing a repository: git init\nIgnoring files: .gitignore\nAdding files to the staging area: git add\nChecking status changes: git status\nReviewing history: git log\nCreating a new branch: git branch\nSwitching to the current branch: git switch and git checkout\nMerging two branches: git merge\nResolving conflicts\nReverting changes: git revert\nUploading changes to GitLab: git push\nDownloading changes from GitLab: git pull\nAdvanced: git rebase",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#basics-of-git",
    "href": "a_02_git_intro_en.html#basics-of-git",
    "title": "Appendix B — Git Introduction",
    "section": "B.2 Basics of Git",
    "text": "B.2 Basics of Git\n\nB.2.1 Initializing a Repository: git init\nTo set up Git as a version control system for your project, you need to initialize a new Git repository at the top-level folder, which is the working directory of your project. This is done using the git init command.\nAll files in this folder and its subfolders will automatically become part of the repository. Creating a Git repository is similar to adding an all-powerful passive observer of all things to your project. Git sits there, observes, and takes note of even the smallest changes, such as a single character in a file within a repository with hundreds of files. And it will tell you where these changes occurred if you forget. Once Git is initialized, it monitors all changes made within the working directory, and it tracks the history of events from that point forward. For this purpose, a historical timeline is created for your project, referred to as a “branch,” and the initial branch is named main. So, when someone says they are on the main branch or working on the main branch, it means they are in the historical main timeline of the project. The Git repository, often abbreviated as repo, is a virtual representation of your project, including its history and branches, a book, if you will, where you can look up and retrieve the entire history of the project: you work in your working directory, and the Git repository tracks and stores your work.\n\n\nB.2.2 Ignoring Files: .gitignore\nIt’s useful that Git watches and keeps an eye on everything in your project. However, in most projects, there are files and folders that you don’t need or want to keep an eye on. These may include system files, local project settings, libraries with dependencies, and so on.\nYou can exclude any file or folder from your Git repository by including them in the .gitignore file. In the .gitignore file, you create a list of file names, folder names, and other items that Git should not track, and Git will ignore these items. Hence the name “gitignore.” Do you want to track a file that you previously ignored? Simply remove the mention of the file in the gitignore file, and Git will start tracking it again.\n\n\nB.2.3 Adding Changes to the Staging Area: git add\nThe interesting thing about Git as an all-powerful, passive observer of all things is that it’s very passive. As long as you don’t tell Git what to remember, it will passively observe the changes in the project folder but do nothing.\nWhen you make a change to your project that you want Git to include in the project’s history to take a snapshot of so you can refer back to it later, your personal checkpoint, if you will, you need to first stage the changes in the staging area. What is the staging area? The staging area is where you collect changes to files that you want to include in the project’s history.\nThis is done using the git add command. You can specify which files you want to add by naming them, or you can add all of them using -A. By doing this, you’re telling Git that you’ve made changes and want it to remember these particular changes so you can recall them later if needed. This is important because you can choose which changes you want to stage, and those are the changes that will eventually be transferred to the history.\nNote: When you run git add, the changes are not transferred to the project’s history. They are only transferred to the staging area.\n\nExample B.1 (Example of git add from the beginning)  \n# Create a new directory for your\n# repository and navigate to that directory:\n\nmkdir my-repo\ncd my-repo\n\n# Initialize the repository with git init:\n\ngit init\n\n# Create a .gitignore file for Python code.\n# You can use a template from GitHub:\n\ncurl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore -o .gitignore\n\n# Add your files to the repository using git add:\n\ngit add .\nThis adds all files in the current directory to the repository, except for the files listed in the .gitignore file.\n\n\n\nB.2.4 Transferring Changes to Memory: git commit\nThe power of Git becomes evident when you start transferring changes to the project history. This is done using the git commit command. When you run git commit, you inform Git that the changes in the staging area should be added to the history of the project so that they can be referenced or retrieved later.\nAdditionally, you can add a commit message with the -m option to explain what changes were made. So when you look back at the project history, you can see that you added a new feature.\ngit commit creates a snapshot, an image of the current state of your project at that specific time, and adds it to the branch you are currently working on.\nAs you work on your project and transfer more snapshots, the branch grows and forms a timeline of events. This means you can now look back at every transfer in the branch and see what your code looked like at that time.\nYou can compare any phase of your code with any other phase of your code to find errors, restore deleted code, or do things that would otherwise not be possible, such as resetting the project to a previous state or creating a new timeline from any point.\nSo how often should you add these commits? My rule of thumb is not to commit too often. It’s better to have a Git repository with too many commits than one with too few commits.\n\nExample B.2 (Continuing the example from above:) After adding your files with git add, you can create a commit to save your changes. Use the git commit command with the -m option to specify your commit message:\ngit commit -m \"My first commit message\"\nThis creates a new commit with the added files and the specified commit message.\n\n\n\nB.2.5 Check the Status of Your Repository: git status\nIf you’re wondering what you’ve changed in your project since the last commit snapshot, you can always check the Git status. Git will list every modified file and the current status of each file.\nThis status can be either:\n\nUnchanged (unmodified), meaning nothing has changed since you last transferred it, or\nIt’s been changed (changed) but not staged (staged) to be transferred into the history, or\nSomething has been added to staging (staged) and is ready to be transferred into the history.\n\nWhen you run git status, you get an overview of the current state of your project.\n\nExample B.3 (Continuing the example from above:) The git status command displays the status of your working directory and the staging area. It shows you which files have been modified, which files are staged for commit, and which files are not yet being tracked:\ngit status\ngit status is a useful tool to keep track of your changes and ensure that you have added all the desired files for commit.\n\n\n\nB.2.6 Review Your Repository’s History: git log\n\nExample B.4 (Continuing the example from above:) You can view the history of your commits with the git log command. This command displays a list of all the commits in the current branch, along with information such as the author, date, and commit message:\ngit log\nThere are many options to customize the output of git log. For example, you can use the --pretty option to change the format of the output:\ngit log --pretty=oneline\nThis displays each commit in a single line.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#branches-timelines",
    "href": "a_02_git_intro_en.html#branches-timelines",
    "title": "Appendix B — Git Introduction",
    "section": "B.3 Branches (Timelines)",
    "text": "B.3 Branches (Timelines)\n\nB.3.1 Creating an Alternative Timeline: git branch\nIn the course of developing a project, you often reach a point where you want to add a new feature, but doing so might require changing the existing code in a way that could be challenging to undo later.\nOr maybe you just want to experiment and be able to discard your work if the experiment fails. In such cases, Git allows you to create an alternative timeline called a branch to work in.\nThis new branch has its own name and exists in parallel with the main branch and all other branches in your project.\nDuring development, you can switch between branches and work on different versions of your code concurrently. This way, you can have a stable codebase in the main branch while developing an experimental feature in a separate branch. When you switch from one branch to another, the code you’re working on is automatically reset to the latest commit of the branch you’re currently in.\nIf you’re working in a team, different team members can work on their own branches, creating an entire universe of alternative timelines for your project. When features are completed, they can be seamlessly merged back into the main branch.\n\nExample B.5 (Continuing the example from above:) To create a new branch, you can use the git branch command with the name of the new branch as an argument:\ngit branch my-tests\n\n\n\nB.3.2 The Pointer to the Current Branch: HEAD\nHow does Git know where you are on the timeline, and how can you keep track of your position?\nYou’re always working at the tip (HEAD) of the currently active branch. The HEAD pointer points there quite literally. In a new project archive with just a single main branch and only new commits being added, HEAD always points to the latest commit in the main branch. That’s where you are.\nHowever, if you’re in a repository with multiple branches, meaning multiple alternative timelines, HEAD will point to the latest commit in the branch you’re currently working on.\n\n\nB.3.3 Switching to an Alternative Timeline: git switch\nAs your project grows, and you have multiple branches, you need to be able to switch between these branches. This is where the switch command comes into play.\nAt any time, you can use the git switch command with the name of the branch you want to switch to, and HEAD moves from your current branch to the one you specified.\nIf you’ve made changes to your code before switching, Git will attempt to carry those changes over to the branch you’re switching to. However, if these changes conflict with the target branch, the switch will be canceled.\nTo resolve this issue without losing your changes, return to the original branch, add and commit your recent changes, and then perform the switch.\n\n\nB.3.4 Switching to an Alternative Timeline and Making Changes: git checkout\nTo switch between branches, you can also use the git checkout command. It works similarly to git switch for this purpose: you pass the name of the branch you want to switch to, and HEAD moves to the beginning of that branch.\nBut checkout can do more than just switch to another timeline. With git checkout, you can also move to any commit point in any timeline. In other words, you can travel back in time and work on code from the past.\nTo do this, use git checkout and provide the commit ID. This is an automatically generated, random combination of letters and numbers that identifies each commit. You can retrieve the commit ID using git log. When you run git log, you get a list of all the commits in your repository, starting with the most recent ones.\nWhen you use git checkout with an older commit ID, you check out a commit in the middle of a branch. This disrupts the timeline, as you’re actively attempting to change history. Git doesn’t want you to do that because, much like in a science fiction movie, altering the past might also alter the future. In our case, it would break the version control branch’s coherence.\nTo prevent you from accidentally disrupting time and altering history, checking out an earlier commit in any branch results in the warning “Detached Head,” which sounds rather ominous. The “Detached Head” warning is appropriate because it accurately describes what’s happening. Git literally detaches the head from the branch and sets it aside.\nNow, you’re working outside of time in a space unbound to any timeline, which again sounds rather threatening but is perfectly fine in reality.\nTo continue working on this past code, all you need to do is reattach it to the timeline. You can use git branch to create a new branch, and the detached head will automatically attach to this new branch.\nInstead of breaking the history, you’ve now created a new alternative timeline that starts in the past, allowing you to work safely. You can continue working on the branch as usual.\n\nExample B.6 (Continuing the example from above:) To switch to a new branch, you can use the git checkout command:\ngit checkout meine-tests\nNow you’re using the new branch and can make changes independently from the original branch.\n\n\n\nB.3.5 The Difference Between checkout and switch\nWhat is the difference between git switch and git checkout? git switch and git checkout are two different commands that both serve the purpose of switching between branches. You can use both to switch between branches, but they have an important distinction. git switch is a new command introduced with Git 2.23. git checkout is an older command that has existed since Git 1.6.0. So, git switch and git checkout have different origins. git switch was introduced to separate the purposes of git checkout. git checkout has two different purposes: 1. It can be used to switch between branches, and 2. It can be used to reset files to the state of the last commit.\nHere’s an example: In my project, I made a change since the last commit, but I haven’t staged it yet. Then, I realized that I actually don’t want this change. I want to reset the file to the state before the last commit. As long as I haven’t committed my changes, I can do this with git checkout by targeting the specific file. So, if that file is named main.js, I can say: git checkout main.js. And the file will be reset to the state of the last commit, which makes sense. I’m checking out the file from the last commit.\nBut that’s quite different from switching between the beginning of one branch to another. git switch and git restore were introduced to separate these two operations. git switch is for switching between branches, and git restore is for resetting the specified file to the state of the last commit. If you try to restore a file with git switch, it simply won’t work. It’s not intended for that. As I mentioned earlier, it’s about separating concerns.\n\nExample B.7 (Difference between git switch and git checkout) Here’s an example demonstrating how to initialize a repository and switch between branches:\n# Create a new directory for your repository\n# and navigate to that directory:\nmkdir my-repo\ncd my-repo\n\n# Initialize the repository with git init:\ngit init\n\n# Create a new branch with git branch:\ngit branch my-new-branch\n\n# Switch to the new branch using git switch:\ngit switch my-new-branch\n\n# Alternatively, you can also use git checkout\n# to switch to the new branch:\n\ngit checkout my-new-branch\nBoth commands lead to the same result: You are now on the new branch.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#merging-branches-and-resolving-conflicts",
    "href": "a_02_git_intro_en.html#merging-branches-and-resolving-conflicts",
    "title": "Appendix B — Git Introduction",
    "section": "B.4 Merging Branches and Resolving Conflicts",
    "text": "B.4 Merging Branches and Resolving Conflicts\n\nB.4.1 git merge: Merging Two Timelines\nGit allows you to split your development work into as many branches or alternative timelines as you like, enabling you to work on many different versions of your code simultaneously without losing or overwriting any of your work.\nThis is all well and good, but at some point, you need to bring those various versions of your code back together into one branch. That’s where git merge comes in.\nConsider an example where you have two branches, a main branch and an experimental branch called experimental-branch. In the experimental branch, there is a new feature. To merge these two branches, you set HEAD to the branch where you want to incorporate the code and execute git merge followed by the name of the branch you want to merge. HEAD is a special pointer that points to the current branch. When you run git merge, it combines the code from the branch associated with HEAD with the code from the branch specified by the branch name you provide.\n# Initialize the repository\ngit init\n\n# Create a new branch called \"experimental-branch\"\ngit branch experimental-branch\n\n# Switch to the \"experimental-branch\"\ngit checkout experimental-branch\n\n# Add the new feature here and\n# make a commit\n# ...\n\n# Switch back to the \"main\" branch\ngit checkout main\n\n# Perform the merge\ngit merge experimental-branch\nDuring the merge, matching pieces of code in the branches overlap, and any new code from the branch being merged is added to the project. So now, the main branch also contains the code from the experimental branch, and the events of the two separate timelines have been merged into a single one. What’s interesting is that even though the experimental branch was merged with the main branch, the last commit of the experimental branch remains intact, allowing you to continue working on the experimental branch separately if you wish.\n\n\nB.4.2 Resolving Conflicts When Merging\nMerging branches where there are no code changes at the same place in both branches is a straightforward process. It’s also a rare process. In most cases, there will be some form of conflict between the branches – the same code or the same code area has been modified differently in the different branches. Merging two branches with such conflicts will not work, at least not automatically.\nIn this case, Git doesn’t know how to merge this code. So, when such a situation occurs, it’s marked as a conflict, and the merging process is halted. This might sound more dramatic than it is. When you get a conflict warning, Git is saying there are two different versions here, and Git needs to know which one you want to keep. To help you figure out the conflict, Git combines all the code into a single file and automatically marks the conflicting code as the current change, which is the original code from the branch you’re working on, or as the incoming change, which is the code from the file you’re trying to merge.\nTo resolve this conflict, you’ll edit the file to literally resolve the code conflict. This might mean accepting either the current or incoming change and discarding the other. It could mean combining both changes or something else entirely. It’s up to you. So, you edit the code to resolve the conflict. Once you’ve resolved the conflict by editing the code, you add the new conflict-free version to the staging area with git add and then commit the merged code with git commit. That’s how the conflict is resolved.\nA merge conflict occurs when Git struggles to automatically merge changes from two different branches. This usually happens when changes were made to the same line in the same file in both branches. To resolve a merge conflict, you must manually edit the affected files and choose the desired changes. Git marks the conflict areas in the file with special markings like &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. You can search for these markings and manually select the desired changes. After resolving the conflicts, you can add the changes with git add and create a new commit with git commit to complete the merge.\n\nExample B.8  \n# Perform the merge (this will cause a conflict)\ngit merge experimenteller-branch\n\n# Open the affected file in an editor and manually resolve the conflicts\n# ...\n\n# Add the modified file\ngit add &lt;filename&gt;\n\n# Create a new commit\ngit commit -m \"Resolved conflicts\"\n\n\n\nB.4.3 git revert: Undoing Something\nOne of the most powerful features of any software tool is the “Undo” button. Make a mistake, press “Undo,” and it’s as if it never happened. However, that’s not quite as simple when an all-powerful, passive observer is watching and recording your project’s history. How do you undo something that you’ve added to the history without rewriting the history?\nThe answer is that you can overwrite the history with the git reset command, but that’s quite risky and not a good practice.\nA better solution is to work with the historical timeline and simply place an older version of your code at the top of the branch. This is done with git revert. To make this work, you need to know the commit ID of the commit you want to go back to.\nThe commit ID is a machine-generated set of random numbers and letters, also known as a hash. To get a list of all the commits in the repository, including the commit ID and commit message, you can run git log.\n# Show the list of all operations in the repository\ngit log\nBy the way, it’s a good idea to leave clear and informative commit messages for this reason. This way, you know what happened in your previous commits. Once you’ve found the commit you want to revert to, call that commit ID with git revert, and then the ID. This will create a new commit at the top of the branch with the code from the reference commit. To transfer the code to the branch, add a commit message and save it. Now, the last commit in your branch matches the commit you’re reverting to, and your project’s history remains intact.\n\nExample B.9 (An example with git revert)  \n# Initialize a new repository\ngit init\n\n# Create a new file\necho \"Hello, World\" &gt; file.txt\n\n# Add the file to the repository\ngit add file.txt\n\n# Create a new commit\ngit commit -m \"First commit\"\n\n# Modify the file\necho \"Goodbye, World\" &gt; file.txt\n\n# Add the modified file\ngit add file.txt\n\n# Create a new commit\ngit commit -m \"Second commit\"\n\n# Use git log to find the commit ID of the second commit\ngit log\n\n# Use git revert to undo the changes from the second commit\ngit revert &lt;commit-id&gt;\n\nTo download the students branch from the repository git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git to your local machine, add a file, and upload the changes, you can follow these steps:\n\nExample B.10 (An example with git clone, git checkout, git add, git commit, git push)  \n# Clone the repository to your local machine:\ngit clone git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git\n\n# Change to the cloned repository:\ncd numerische-mathematik-sommersemester2023\n\n# Switch to the students branch:\ngit checkout students\n\n# Create the Test folder if it doesn't exist:\nmkdir Test\n\n# Create the Testdatei.txt file in the Test folder:\ntouch Test/Testdatei.txt\n\n# Add the file with git add:\ngit add Test/Testdatei.txt\n\n# Commit the changes with git commit:\ngit commit -m \"Added Testdatei.txt\"\n\n# Push the changes with git push:\ngit push origin students\nThis will upload the changes to the server and update the students branch in the repository.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#downloading-from-gitlab",
    "href": "a_02_git_intro_en.html#downloading-from-gitlab",
    "title": "Appendix B — Git Introduction",
    "section": "B.5 Downloading from GitLab",
    "text": "B.5 Downloading from GitLab\nTo download changes from a GitLab repository to your local machine, you can use the git pull command. This command downloads the latest changes from the specified remote repository and merges them with your local repository.\nHere is an example:\n\nExample B.11 (An example with git pull)  \n\n# Navigate to the local repository\n# linked to the GitHub repository:\ncd my-local-repository\n\n# Make sure you are in the correct branch:\ngit checkout main\n\n# Download the latest changes from GitHub:\ngit pull origin main\nThis downloads the latest changes from the main branch of the remote repository named “origin” and merges them with your local repository.\n\nIf there are conflicts between the downloaded changes and your local changes, you will need to resolve them manually before proceeding.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#advanced",
    "href": "a_02_git_intro_en.html#advanced",
    "title": "Appendix B — Git Introduction",
    "section": "B.6 Advanced",
    "text": "B.6 Advanced\n\nB.6.1 git rebase: Moving the Base of a Branch\nIn some cases, you may need to “rewrite history.” A common scenario is that you’ve been working on a new feature in a feature branch, and you realize that the work should have actually happened in the main branch.\nTo resolve this issue and make it appear as if the work occurred in the main branch, you can reset the experimental branch. “Rebase” literally means detaching the base of the experimental branch and moving it to the beginning of another branch, giving the branch a new base, thus “rebasing.”\nThis operation is performed from the branch you want to “rebase.” You use git rebase and specify the branch you want to use as the new base. If there are no conflicts between the experimental branch and the branch you want to rebase onto, this process happens automatically.\nIf there are conflicts, Git will guide you through the conflict resolution process for each commit from the rebase branch.\nThis may sound like a lot, but there’s a good reason for it. You are literally rewriting history by transferring commits from one branch to another. To maintain the coherence of the new version history, there should be no conflicts within the commits. So, you need to resolve them one by one until the history is clean. It goes without saying that this can be a fairly labor-intensive process. Therefore, you should not use git rebase frequently.\n\nExample B.12 (An example with git rebase) git rebase is a command used to change the base of a branch. This means that commits from the branch are applied to a new base, which is usually another branch. It can be used to clean up the repository history and avoid merge conflicts.\nHere is an example showing how to use git rebase:\n\nIn this example, we initialize a new Git repository and create a new file. We add the file to the repository and make an initial commit. Then, we create a new branch called “feature” and switch to that branch. We make changes to the file in the feature branch and create a new commit.\nThen, we switch back to the main branch and make changes to the file again. We add the modified file and make another commit.\nTo rebase the feature branch onto the main branch, we first switch to the feature branch and then use the git rebase command with the name of the main branch as an argument. This applies the commits from the feature branch to the main branch and changes the base of the feature branch.\n\n# Initialize a new repository\ngit init\n# Create a new file\necho \"Hello World\" &gt; file.txt\n# Add the file to the repository\ngit add file.txt\n# Create an initial commit\ngit commit -m \"Initial commit\"\n# Create a new branch called \"feature\"\ngit branch feature\n# Switch to the \"feature\" branch\ngit checkout feature\n# Make changes to the file in the \"feature\" branch\necho \"Hello Feature World\" &gt; file.txt\n# Add the modified file\ngit add file.txt\n# Create a new commit in the \"feature\" branch\ngit commit -m \"Feature commit\"\n# Switch back to the \"main\" branch\ngit checkout main\n# Make changes to the file in the \"main\" branch\necho \"Hello Main World\" &gt; file.txt\n# Add the modified file\ngit add file.txt\n# Create a new commit in the \"main\" branch\ngit commit -m \"Main commit\"\n# Use git rebase to rebase the \"feature\" branch\n# onto the \"main\" branch\ngit checkout feature\ngit rebase main",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#exercises",
    "href": "a_02_git_intro_en.html#exercises",
    "title": "Appendix B — Git Introduction",
    "section": "B.7 Exercises",
    "text": "B.7 Exercises\nIn order to be able to carry out this exercise, we provide you with a functional working environment. This can be accessed here. You can log in using your GMID. If you do not have one, you can generate one here. Once you have successfully logged in to the server, you must open a terminal instance. You are now in a position to carry out the exercise.\nAlternatively, you can also carry out the exercise locally on your computer, but then you will need to install git.\n\nB.7.1 Create project folder\nFirst create the test-repo folder via the command line and then navigate to this folder using the corresponding command.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#initialize-repo",
    "href": "a_02_git_intro_en.html#initialize-repo",
    "title": "Appendix B — Git Introduction",
    "section": "B.8 Initialize repo",
    "text": "B.8 Initialize repo\nNow initialize the repository so that the future project, which will be saved in the test-repo folder, and all associated files are versioned.\n\nB.8.1 Do not upload / ignore certain file types\nIn order to carry out this exercise, you must first download a file which you then have git ignore. To do this, download the current examination regulations for the Bachelor’s degree program in Electrical Engineering using the following command curl -o pruefungsordnung.pdf https://www.th-koeln.de/mam/downloads/deutsch/studium/studiengaenge/f07/ordnungen_plaene/f07_bpo_ba_ekb_2021_01_04.pdf.\nThe PDF file has been stored in the root directory of your repo and you must now exclude it from being uploaded so that no changes to this file are tracked. Please note that not only this one PDF file should be ignored, but all PDF files in the repo.\n\n\nB.8.2 Create file and stage it\nIn order to be able to commit a change later and thus make it traceable, it must first be staged. However, as we only have a PDF file so far, which is to be ignored by git, we cannot stage anything. Therefore, in this task, a file test.txt with some string as content is to be created and then staged.\n\n\nB.8.3 Create another file and check status\nTo understand the status function, you should create the file test2.txt and then call the status function of git.\n\n\nB.8.4 Commit changes\nAfter the changes to the test.txt file have been staged and these are now to be transferred to the project process, they must be committed. Therefore, in this step you should perform a corresponding commit in the current branch with the message test-commit. Finally, you should also display the history of the commits.\n\n\nB.8.5 Create a new branch and switch to it\nIn this task, you are to create a new branch with the name change-text in which you will later make changes. You should then switch to this branch.\n\n\nB.8.6 Commit changes in the new branch\nTo be able to merge the new branch into the main branch later, you must first make changes to the test.txt file. To do this, open the file and simply change the character string in this file before saving the changes and closing the file. Before you now commit the file, you should reset the file to the status of the last commit for practice purposes and thus undo the change. After you have done this, open the file test.txt again and change the character string again before saving and closing the file. This time you should commit the file test.txt and then commit it with the message test-commit2.\n\n\nB.8.7 Merge branch into main\nAfter you have committed the change to the test.txt file, you should merge the change-text branch including the change into the main branch so that it is also available there.\n\n\nB.8.8 Resolve merge conflict\nTo simulate a merge conflict, you must first change the content of the test.txt file before you commit the change. Then switch to the branch change-text and change the file test.txt there as well before you commit the change. Now you should try to merge the branch change-text into the main branch and solve the problems that occur in order to be able to perform the merge successfully.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_03_python_intro_en.html",
    "href": "a_03_python_intro_en.html",
    "title": "Appendix C — Python Introduction",
    "section": "",
    "text": "C.1 Recommendations\nBeginner’s Guide to Python",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python Introduction</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html",
    "href": "a_04_spot_doc.html",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "",
    "text": "D.1 An Initial Example\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[\nf(x) = x^2.\n\\]\nfun = analytical().fun_sphere\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\nspot_1 = spot.Spot(fun=fun,\n                   fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                   design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                   surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_1.run()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x372ee7550&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#organization",
    "href": "a_04_spot_doc.html#organization",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.2 Organization",
    "text": "D.2 Organization\nSpot organizes the surrogate based optimization process in four steps:\n\nSelection of the objective function: fun.\nSelection of the initial design: design.\nSelection of the optimization algorithm: optimizer.\nSelection of the surrogate model: surrogate.\n\nFor each of these steps, the user can specify an object:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_sphere\nfrom spotPython.design.spacefilling import spacefilling\ndesign = spacefilling(2)\nfrom scipy.optimize import differential_evolution\noptimizer = differential_evolution\nfrom spotPython.build.kriging import Kriging\nsurrogate = Kriging()\n\nFor each of these steps, the user can specify a dictionary of control parameters.\n\nfun_control\ndesign_control\noptimizer_control\nsurrogate_control\n\nEach of these dictionaries has an initialzaion method, e.g., fun_control_init(). The initialization methods set the default values for the control parameters.\n\n\n\n\n\n\nImportant:\n\n\n\n\nThe specification of an lower bound in fun_control is mandatory.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\nfun_control=fun_control_init(lower=np.array([-1, -1]),\n                            upper=np.array([1, 1]))\ndesign_control=design_control_init()\noptimizer_control=optimizer_control_init()\nsurrogate_control=surrogate_control_init()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#the-spot-object",
    "href": "a_04_spot_doc.html#the-spot-object",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.3 The Spot Object",
    "text": "D.3 The Spot Object\nBased on the definition of the fun, design, optimizer, and surrogate objects, and their corresponding control parameter dictionaries, fun_control, design_control, optimizer_control, and surrogate_control, the spot object can be build as follows:\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       optimizer_control=optimizer_control,\n                       surrogate_control=surrogate_control)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run",
    "href": "a_04_spot_doc.html#run",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.4 Run",
    "text": "D.4 Run\n\nspot_tuner.run()\n\nspotPython tuning: 1.801603872454505e-05 [#######---] 73.33% \nspotPython tuning: 1.801603872454505e-05 [########--] 80.00% \nspotPython tuning: 1.801603872454505e-05 [#########-] 86.67% \nspotPython tuning: 1.801603872454505e-05 [#########-] 93.33% \nspotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x37314c650&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#print-the-results",
    "href": "a_04_spot_doc.html#print-the-results",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.5 Print the Results",
    "text": "D.5 Print the Results\n\nspot_tuner.print_results()\n\nmin y: 1.801603872454505e-05\nx0: 0.0019077911677074135\nx1: 0.003791618596979743\n\n\n[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#show-the-progress",
    "href": "a_04_spot_doc.html#show-the-progress",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.6 Show the Progress",
    "text": "D.6 Show the Progress\n\nspot_tuner.plot_progress()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#visualize-the-surrogate",
    "href": "a_04_spot_doc.html#visualize-the-surrogate",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.7 Visualize the Surrogate",
    "text": "D.7 Visualize the Surrogate\n\nThe plot method of the kriging surrogate is used.\nNote: the plot uses the interval defined by the ranges of the natural variables.\n\n\nspot_tuner.surrogate.plot()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "href": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.8 Run With a Specific Start Design",
    "text": "D.8 Run With a Specific Start Design\nTo pass a specific start design, use the X_start argument of the run method.\n\nspot_x0 = spot.Spot(fun=fun,\n                    fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                    design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                    surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_x0.run(X_start=np.array([0.5, -0.5]))\nspot_x0.plot_progress()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#init-build-initial-design",
    "href": "a_04_spot_doc.html#init-build-initial-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.9 Init: Build Initial Design",
    "text": "D.9 Init: Build Initial Design\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\ngen = spacefilling(2)\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\n\n[[ 8.97647221 13.41926847]\n [ 0.66946019  1.22344228]\n [ 5.23614115 13.78185824]\n [ 5.6149825  11.5851384 ]\n [-1.72963184  1.66516096]\n [-4.26945568  7.1325531 ]\n [ 1.26363761 10.17935555]\n [ 2.88779942  8.05508969]\n [-3.39111089  4.15213772]\n [ 7.30131231  5.22275244]]\n[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975\n  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#replicability",
    "href": "a_04_spot_doc.html#replicability",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.10 Replicability",
    "text": "D.10 Replicability\nSeed\n\ngen = spacefilling(2, seed=123)\nX0 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=345)\nX1 = gen.scipy_lhd(3)\nX2 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=123)\nX3 = gen.scipy_lhd(3)\nX0, X1, X2, X3\n\n(array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]),\n array([[0.78373509, 0.86811887],\n        [0.06692621, 0.6058029 ],\n        [0.41374778, 0.00525456]]),\n array([[0.121357  , 0.69043832],\n        [0.41906219, 0.32838498],\n        [0.86742658, 0.52910374]]),\n array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#surrogates",
    "href": "a_04_spot_doc.html#surrogates",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.11 Surrogates",
    "text": "D.11 Surrogates\n\nD.11.1 A Simple Predictor\nThe code below shows how to use a simple model for prediction. Assume that only two (very costly) measurements are available:\n\nf(0) = 0.5\nf(2) = 2.5\n\nWe are interested in the value at \\(x_0 = 1\\), i.e., \\(f(x_0 = 1)\\), but cannot run an additional, third experiment.\n\nfrom sklearn import linear_model\nX = np.array([[0], [2]])\ny = np.array([0.5, 2.5])\nS_lm = linear_model.LinearRegression()\nS_lm = S_lm.fit(X, y)\nX0 = np.array([[1]])\ny0 = S_lm.predict(X0)\nprint(y0)\n\n[1.5]\n\n\nCentral Idea: Evaluation of the surrogate model S_lm is much cheaper (or / and much faster) than running the real-world experiment \\(f\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#tensorboard-setup",
    "href": "a_04_spot_doc.html#tensorboard-setup",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.12 Tensorboard Setup",
    "text": "D.12 Tensorboard Setup\n\nD.12.1 Tensorboard Configuration\nThe TENSORBOARD_CLEAN argument can be set to True in the fun_control dictionary to archive the TensorBoard folder if it already exists. This is useful if you want to start a hyperparameter tuning process from scratch. If you want to continue a hyperparameter tuning process, set TENSORBOARD_CLEAN to False. Then the TensorBoard folder will not be archived and the old and new TensorBoard files will shown in the TensorBoard dashboard.\n\n\nD.12.2 Starting TensorBoard\nTensorBoard can be started as a background process with the following command, where ./runs is the default directory for the TensorBoard log files:\ntensorboard --logdir=\"./runs\"\n\n\n\n\n\n\nTENSORBOARD_PATH\n\n\n\nThe TensorBoard path can be printed with the following command (after a fun_control object has been created):\n\nfrom spotPython.utils.init import get_tensorboard_path\nget_tensorboard_path(fun_control)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#demotest-objective-function-fails",
    "href": "a_04_spot_doc.html#demotest-objective-function-fails",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.13 Demo/Test: Objective Function Fails",
    "text": "D.13 Demo/Test: Objective Function Fails\nSPOT expects np.nan values from failed objective function values. These are handled. Note: SPOT’s counter considers only successful executions of the objective function.\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nimport numpy as np\nfrom math import inf\n# number of initial points:\nni = 20\n# number of points\nn = 30\n\nfun = analytical().fun_random_error\nfun_control=fun_control_init(\n    lower = np.array([-1]),\n    upper= np.array([1]),\n    fun_evals = n,\n    show_progress=False)\ndesign_control=design_control_init(init_size=ni)\n\nspot_1 = spot.Spot(fun=fun,\n                     fun_control=fun_control,\n                     design_control=design_control)\nspot_1.run()\n# To check whether the run was successfully completed,\n# we compare the number of evaluated points to the specified\n# number of points.\nassert spot_1.y.shape[0] == n\n\n[        nan         nan -0.02203599 -0.21843718  0.78240941         nan\n -0.3923345   0.67234256  0.31802454 -0.68898927 -0.75129705  0.97550354\n  0.41757584         nan  0.82585329         nan -0.49274073         nan\n -0.17991251  0.1481835 ]\n[-1.]\n[nan]\n[-0.14624037]\n[0.166475]\n[nan]\n[-0.3352401]\n[-0.47259301]\n[0.95541987]\n[0.17335968]\n[-0.58552368]\n[-0.20126111]\n[-0.60100809]\n[-0.97897336]\n[-0.2748985]\n[0.8359486]\n[0.99035591]\n[0.01641232]\n[0.5629346]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#handling-results-printing-saving-and-loading",
    "href": "a_04_spot_doc.html#handling-results-printing-saving-and-loading",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.14 Handling Results: Printing, Saving, and Loading",
    "text": "D.14 Handling Results: Printing, Saving, and Loading\nThe results can be printed with the following command:\n\nspot_tuner.print_results(print_screen=False)\n\nThe tuned hyperparameters can be obtained as a dictionary with the following command:\n\nfrom spotPython.hyperparameters.values import get_tuned_hyperparameters\nget_tuned_hyperparameters(spot_tuner, fun_control)\n\nThe results can be saved and reloaded with the following commands:\n\nfrom spotPython.utils.file import save_pickle, load_pickle\nfrom spotPython.utils.init import get_experiment_name\nexperiment_name = get_experiment_name(\"024\")\nSAVE_AND_LOAD = False\nif SAVE_AND_LOAD == True:\n    save_pickle(spot_tuner, experiment_name)\n    spot_tuner = load_pickle(experiment_name)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#spotpython-as-a-hyperparameter-tuner",
    "href": "a_04_spot_doc.html#spotpython-as-a-hyperparameter-tuner",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.15 spotpython as a Hyperparameter Tuner",
    "text": "D.15 spotpython as a Hyperparameter Tuner\n\nD.15.1 Modifying Hyperparameter Levels\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n# levels = [\"LinearRegression\"]\n# modify_hyper_parameter_levels(fun_control, \"leaf_model\", levels)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_05_datasets.html",
    "href": "a_05_datasets.html",
    "title": "Appendix E — Datasets",
    "section": "",
    "text": "E.1 The Friedman Drift Dataset",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "a_05_datasets.html#sec-a-05-friedman",
    "href": "a_05_datasets.html#sec-a-05-friedman",
    "title": "Appendix E — Datasets",
    "section": "",
    "text": "E.1.1 The Friedman Drift Dataset as Implemented in river\nWe will describe the Friedman synthetic dataset with concept drifts [SOURCE], see also Friedman (1991) and Ikonomovska, Gama, and Džeroski (2011). Each observation is composed of ten features. Each feature value is sampled uniformly in [0, 1]. Only the first five features are relevant. The target is defined by different functions depending on the type of the drift. Global Recurring Abrupt drift will be used, i.e., the concept drift appears over the whole instance space.\nThe target is defined by the following function: \\[\ny = 10 \\sin(\\pi x_0 x_1) + 20 (x_2 - 0.5)^2 + 10 x_3 + 5 x_4 + \\epsilon,\n\\] where \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) is normally distributed noise.\nIf the Global Recurring Abrupt drift variant of the Friedman Drift dataset is used, the target function changes at two points in time, namely \\(p_1\\) and \\(p_2\\). At the first point, the concept changes to: \\[\ny = 10 \\sin(\\pi x_3 x_5) + 20 (x_1 - 0.5)^2 + 10 x_0 + 5 x_2 + \\epsilon,\n\\] At the second point of drift the old concept reoccurs. This can be implemented as follows, see https://riverml.xyz/latest/api/datasets/synth/FriedmanDrift/:\ndef __iter__(self):\n    rng = random.Random(self.seed)\n\n    i = 0\n    while True:\n        x = {i: rng.uniform(a=0, b=1) for i in range(10)}\n        y = self._global_recurring_abrupt_gen(x, i) + rng.gauss(mu=0, sigma=1)\n\n        yield x, y\n        i += 1\ndef _global_recurring_abrupt_gen(self, x, index: int):\n    if index &lt; self._change_point1 or index &gt;= self._change_point2:\n        # The initial concept is recurring\n        return (\n            10 * math.sin(math.pi * x[0] * x[1]) + 20 * (x[2] - 0.5) ** 2 + 10 * x[3] + 5 * x[4]\n        )\n    else:\n        # Drift: the positions of the features are swapped\n        return (\n            10 * math.sin(math.pi * x[3] * x[5]) + 20 * (x[1] - 0.5) ** 2 + 10 * x[0] + 5 * x[2]\n        )\nspotpython requires the specification of a train and test data set. These data sets can be generated as follows:\n\nfrom river.datasets import synth\nimport pandas as pd\nimport numpy as np\nfrom spotRiver.utils.data_conversion import convert_to_df\n\nseed = 123\nshuffle = True\nn_train = 6_000\nn_test = 4_000\nn_samples = n_train + n_test\ntarget_column = \"y\"\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=(n_train/4, n_train/2),\n   seed=123\n)\n\ntrain = convert_to_df(dataset, n_total=n_train)\ntrain.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=(n_test/4, n_test/2),\n   seed=123\n)\ntest = convert_to_df(dataset, n_total=n_test)\ntest.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n\n\ndef plot_data_with_drift_points(data, target_column, n_train, title=\"\"):\n    indices = range(len(data))\n    y_values = data[target_column]\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(indices, y_values, label=\"y Value\", color='blue')\n\n    drift_points = [n_train / 4, n_train / 2]\n    for dp in drift_points:\n        plt.axvline(x=dp, color='red', linestyle='--', label=f'Drift Point at {int(dp)}')\n\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    plt.legend(by_label.values(), by_label.keys())\n\n    plt.xlabel('Index')\n    plt.ylabel('Target Value (y)')\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n\nplot_data_with_drift_points(train, target_column, n_train, title=\"Training Data with Drift Points\")\n\n\n\n\n\n\n\n\n\nplot_data_with_drift_points(test, target_column, n_train, title=\"Testing Data with Drift Points\")\n\n\n\n\n\n\n\n\n\n\nE.1.2 The Friedman Drift Data Set from spotpython\nA data generator for the Friedman Drift dataset is implemented in the spotPython package, see friedman.py. The spotPython version is a simplified version of the river implementation. The spotPyton version allows the generation of constant input values for the features. This is useful for visualizing the concept drifts. For the productive use the river version should be used.\nPlotting the first 100 samples of the Friedman Drift dataset, we can not see the concept drifts at \\(p_1\\) and \\(p_2\\). Drift can be visualized by plotting the target values over time for constant features, e,g, if \\(x_0\\) is set to \\(1\\) and all other features are set to \\(0\\). This is illustrated in the following plot.\n\nfrom spotPython.data.friedman import FriedmanDriftDataset\n\ndef plot_friedman_drift_data(n_samples, seed, change_point1, change_point2, constant=True):\n    data_generator = FriedmanDriftDataset(n_samples=n_samples, seed=seed, change_point1=change_point1, change_point2=change_point2, constant=constant)\n    data = [data for data in data_generator]\n    indices = [i for _, _, i in data]\n    values = {f\"x{i}\": [] for i in range(5)}\n    values[\"y\"] = []\n    for x, y, _ in data:\n        for i in range(5):\n            values[f\"x{i}\"].append(x[i])\n        values[\"y\"].append(y)\n\n    plt.figure(figsize=(10, 6))\n    for label, series in values.items():\n        plt.plot(indices, series, label=label)\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.axvline(x=change_point1, color='k', linestyle='--', label='Drift Point 1')\n    plt.axvline(x=change_point2, color='r', linestyle='--', label='Drift Point 2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\nplot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFriedman, Jerome H. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics 19 (1): 1–67.\n\n\nIkonomovska, Elena, João Gama, and Sašo Džeroski. 2011. “Learning Model Trees from Evolving Data Streams.” Data Mining and Knowledge Discovery 23 (1): 128–68.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "a_99_solutions.html",
    "href": "a_99_solutions.html",
    "title": "Appendix F — Solutions to Selected Exercises",
    "section": "",
    "text": "F.1 Data-Driven Modeling and Optimization",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Solutions to Selected Exercises</span>"
    ]
  },
  {
    "objectID": "a_99_solutions.html#data-driven-modeling-and-optimization",
    "href": "a_99_solutions.html#data-driven-modeling-and-optimization",
    "title": "Appendix F — Solutions to Selected Exercises",
    "section": "",
    "text": "F.1.1 Histograms\n\nSolution F.1 (Density Curve). \n\nWe can calculate propabilities.\nWe only need two parameters (the mean and the sd) to form the curve -&gt; Store data more efficently\nBlanks can be filled\n\n\n\n\nF.1.2 The Normal Distribution\n\nSolution F.2 (TwoSDAnswer). 95%\n\n\nSolution F.3 (OneSDAnswer). 68%\n\n\nSolution F.4 (ThreeSDAnswer). 99,7%\n\n\nSolution F.5 (DataRangeAnswer). 80 - 120\n\n\nSolution F.6 (PeakHeightAnswer). low\n\n\n\nF.1.3 The mean, the media, and the mode\n\n\nF.1.4 The exponential distribution\n\n\nF.1.5 Population and Estimated Parameters\n\nSolution F.7 (ProbabilityAnswer). 50%\n\n\n\nF.1.6 Calculating the Mean, Variance and Standard Deviation\n\nSolution F.8 (MeanDifferenceAnswer). If we have all the data, \\(\\mu\\) is the population mean and x-bar is the sample mean. We don’t have the full information.\n\n\nSolution F.9 (EstimateMeanAnswer). Sum of the values divided by n.\n\n\nSolution F.10 (SigmaSquaredAnswer). Variance\n\n\nSolution F.11 (EstimatedSDAnswer). The same as the normal standard deviation, but using n-1.\n\n\nSolution F.12 (VarianceDifferenceAnswer). \\(n\\) and \\(n-1\\)\n\n\nSolution F.13 (ModelBenefitsAnswer). \n\nApproximation\nPrediction\nUnderstanding\n\n\n\nSolution F.14 (SampleDefinitionAnswer). It’s a subset of the data.\n\n\n\nF.1.7 Hypothesis Testing and the Null-Hypothesis\n\nSolution F.15 (RejectHypothesisAnswer). It means the evidence supports the alternative hypothesis, indicating that the null hypothesis is unlikely to be true.\n\n\nSolution F.16 (NullHypothesisAnswer). It’s a statement that there is no effect or no difference, and it serves as the default or starting assumption in hypothesis testing.\n\n\nSolution F.17 (BetterDrugAnswer). By conducting experiments and statistical tests to compare the new drug’s effectiveness against the current standard and demonstrating a significant improvement.\n\n\n\nF.1.8 Alternative Hypotheses, Main Ideas\n\n\nF.1.9 p-values: What they are and how to interpret them\n\nSolution F.18 (PValueIntroductionAnswer). We can reject the null hypothesis. We can make a decision.\n\n\nSolution F.19 (PValueRangeAnswer). It can only be between 0 and 1.\n\n\nSolution F.20 (PValueRangeAnswer). It can only be between 0 and 1.\n\n\nSolution F.21 (TypicalPValueAnswer). The chance that we wrongly reject the null hypothesis.\n\n\nSolution F.22 (FalsePositiveAnswer). If we have a false-positive, we succeed in rejecting the null hypothesis. But in fact/reality, this is false -&gt; False positive.\n\n\n\nF.1.10 How to calculate p-values\n\nSolution F.23 (CalculatePValueAnswer). Probability of specific result, probability of outcome with the same probability, and probability of events with smaller probability.\n\n\nSolution F.24 (SDCalculationAnswer). 7 is the SD.\n\n\nSolution F.25 (SidedPValueAnswer). If we are not interested in the direction of the change, we use the two-sided. If we want to know about the direction, the one-sided.\n\n\nSolution F.26 (CoinTestAnswer). TBD\n\n\nSolution F.27 (BorderPValueAnswer). TBD\n\n\nSolution F.28 (OneSidedPValueCautionAnswer). If you look in the wrong direction, there is no change.\n\n\nSolution F.29 (BinomialDistributionAnswer). TBD\n\n\n\nF.1.11 p-hacking: What it is and how to avoid it\n\nSolution F.30 (PHackingWaysAnswer). \n\nPerforming repeats until you find one result with a small p-value -&gt; false positive result.\nIncreasing the sample size within one experiment when it is close to the threshold.\n\n\n\nSolution F.31 (AvoidPHackingAnswer). Specify the number of repeats and the sample sizes at the beginning.\n\n\nSolution F.32 (MultipleTestingProblemAnswer). TBD\n\n\n\nF.1.12 Covariance\n\nSolution F.33 (CovarianceDefinitionAnswer). Formula\n\n\nSolution F.34 (CovarianceMeaningAnswer). Large values in the first variable result in large values in the second variable.\n\n\nSolution F.35 (CovarianceVarianceRelationshipAnswer). Formula\n\n\nSolution F.36 (HighCovarianceAnswer). No, size doesn’t matter.\n\n\nSolution F.37 (ZeroCovarianceAnswer). No relationship\n\n\nSolution F.38 (NegativeCovarianceAnswer). Yes\n\n\nSolution F.39 (NegativeVarianceAnswer). No\n\n\n\nF.1.13 Pearson’s Correlation\n\nSolution F.40 (CorrelationValueAnswer). Recalculate\n\n\nSolution F.41 (CorrelationRangeAnswer). From -1 to 1\n\n\nSolution F.42 (CorrelationFormulaAnswer). Formula\n\n\n\nF.1.14 Boxplots\n\nSolution F.43 (UnderstandingStatisticalPower). It is the probability of correctly rejecting the null hypothesis.\n\n\nSolution F.44 (DistributionEffectOnPower). Power analysis is not applicable.\n\n\nSolution F.45 (IncreasingPower). By taking more samples.\n\n\nSolution F.46 (PreventingPHacking). TBD\n\n\nSolution F.47 (SampleSizeAndPower). The power will be low.\n\n\n\nF.1.15 Power Analysis\n\nSolution F.48 (MainFactorsAffectingPower). The overlap (distance of the two means) and sample sizes.\n\n\nSolution F.49 (PowerAnalysisOutcome). The sample size needed.\n\n\nSolution F.50 (RisksInExperiments). Few experiments lead to very low power, and many experiments might result in p-hacking.\n\n\nSolution F.51 (StepsToPerformPowerAnalysis). \n\nSelect power\nSelect threshold for significance (alpha)\nEstimate the overlap (done by the effect size)\n\n\n\n\nF.1.16 The Central Limit Theorem\n\nSolution F.52 (CentralLimitTheoremAnswer). TBD\n\n\n\nF.1.17 Boxplots\n\nSolution F.53 (MedianAnswer). The median.\n\n\nSolution F.54 (BoxContentAnswer). 50% of the data.\n\n\n\nF.1.18 R-squared\n\nSolution F.55 (RSquaredFormulaAnswer). TBD\n\n\nSolution F.56 (NegativeRSquaredAnswer). If you fit a line, no, but there are cases where it could be negative. However, these are usually considered useless.\n\n\nSolution F.57 (RSquaredCalculationAnswer). TBD\n\n\nF.1.18.1 The main ideas of fitting a line to data (The main ideas of least squares and linear regression.)\n\nSolution F.58 (LeastSquaresAnswer). It is the calculation of the smallest sum of residuals when you fit a model to data.\n\n\n\n\nF.1.19 Linear Regression\n\n\nF.1.20 Multiple Regression\n\n\nF.1.21 A Gentle Introduction to Machine Learning\n\nSolution F.59 (RegressionVsClassificationAnswer). Regression involves predicting continuous values (e.g., temperature, size), while classification involves predicting discrete values (e.g., categories like cat, dog).\n\n\n\nF.1.22 Maximum Likelihood\n\nSolution F.60 (LikelihoodConceptAnswer). The distribution that fits the data best.\n\n\n\nF.1.23 Probability is not Likelihood\n\nSolution F.61 (ProbabilityVsLikelihoodAnswer). Likelihood: Finding the curve that best fits the data. Probability: Calculating the probability of an event given a specific curve.\n\n\n\nF.1.24 Cross Validation\n\nSolution F.62 (TrainVsTestDataAnswer). Training data is used to fit the model, while testing data is used to evaluate how well the model fits.\n\n\nSolution F.63 (SingleValidationIssueAnswer). The performance might not be representative because the data may not be equally distributed between training and testing sets.\n\n\nSolution F.64 (FoldDefinitionAnswer). TBD\n\n\nSolution F.65 (LeaveOneOutValidationAnswer). Only one data point is used as the test set, and the rest are used as the training set.\n\n\n\nF.1.25 The Confusion Matrix\n\nSolution F.66 (ConfusionMatrixAnswer). TBD\n\n\n\nF.1.26 Sensitivity and Specificity\n\nSolution F.67 (SensitivitySpecificityAnswer1). TBD\n\n\nSolution F.68 (SensitivitySpecificityAnswer2). TBD\n\n\n\nF.1.27 Bias and Variance\n\nSolution F.69 (BiasAndVarianceAnswer). TBD\n\n\n\nF.1.28 Mutual Information\n\nSolution F.70 (MutualInformationExampleAnswer). TBD\n\n\n\nF.1.29 Principal Component Analysis (PCA)\n\nSolution F.71 (WhatIsPCAAnswer). A dimension reduction technique that helps discover important variables.\n\n\nSolution F.72 (screePlotAnswer). It shows how much variation is defined by the data.\n\n\nSolution F.73 (LeastSquaresInPCAAnswer). No, in the first step it tries to maximize distances.\n\n\nSolution F.74 (PCAStepsAnswer). \n\nCalculate mean\nShift the data to the center of the coordinate system\nFit a line by maximizing the distances\nCalculate the sum of squared distances\nCalculate the slope\nRotate\n\n\n\nSolution F.75 (EigenvaluePC1Answer). Formula (to be specified).\n\n\nSolution F.76 (DifferencesBetweenPointsAnswer). No, because the first difference is measured on the PC1 scale and it is more important.\n\n\nSolution F.77 (ScalingInPCAAnswer). Scaling by dividing by the standard deviation (SD).\n\n\nSolution F.78 (DetermineNumberOfComponentsAnswer). TBD\n\n\nSolution F.79 (LimitingNumberOfComponentsAnswer). \n\nThe dimension of the problem\nNumber of samples\n\n\n\n\nF.1.30 t-SNE\n\nSolution F.80 (WhyUseTSNEAnswer). For dimension reduction and picking out the relevant clusters.\n\n\nSolution F.81 (MainIdeaOfTSNEAnswer). To reduce the dimensions of the data by reconstructing the relationships in a lower-dimensional space.\n\n\nSolution F.82 (BasicConceptOfTSNEAnswer). \n\nFirst, randomly arrange the points in a lower dimension\nDecide whether to move points left or right, depending on distances in the original dimension\nFinally, arrange points in the lower dimension similarly to the original dimension\n\n\n\nSolution F.83 (TSNEStepsAnswer). \n\nProject data to get random points\nSet up a matrix of distances\nCalculate the inner variances of the clusters and the Gaussian distribution\nDo the same with the projected points\nMove projected points so the second matrix gets more similar to the first matrix\n\n\n\n\nF.1.31 K-means clustering\n\nSolution F.84 (HowKMeansWorksAnswer). \n\nSelect the number of clusters\nRandomly select distinct data points as initial cluster centers\nMeasure the distance between each point and the cluster centers\nAssign each point to the nearest cluster\nRepeat the process\n\n\n\nSolution F.85 (QualityOfClustersAnswer). Calculate the within-cluster variation.\n\n\nSolution F.86 (IncreasingKAnswer). If k is too high, each point would be its own cluster. If k is too low, you cannot see the structures.\n\n\n\nF.1.32 DBSCAN\n\nSolution F.87 (CorePointInDBSCANAnswer). A point that is close to at least k other points.\n\n\nSolution F.88 (AddingVsExtendingAnswer). Adding means we add a point and then stop. Extending means we add a point and then look for other neighbors from that point.\n\n\nSolution F.89 (OutliersInDBSCANAnswer). Points that are not core points and do not belong to existing clusters.\n\n\n\nF.1.33 K-nearest neighbors\n\nSolution F.90 (AdvantagesAndDisadvantagesOfKAnswer). \n\nk = 1: Noise can disturb the process because of possibly incorrect measurements of points.\nk = 100: The majority can be wrong for some groups. It is smoother, but there is less chance to discover the structure of the data.\n\n\n\n\nF.1.34 Naive Bayes\n\nSolution F.91 (NaiveBayesFormulaAnswer). TBD\n\n\nSolution F.92 (CalculateProbabilitiesAnswer). TBD\n\n\n\nF.1.35 Gaussian Naive Bayes\n\nSolution F.93 (UnderflowProblemAnswer). Small values multiplied together can become smaller than the limits of computer memory, resulting in zero. Using logarithms (e.g., log(1/2) -&gt; -1, log(1/4) -&gt; -2) helps prevent underflow.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Solutions to Selected Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadi, Martin, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\nCraig Citro, Greg S. Corrado, et al. 2016. “TensorFlow: Large-Scale Machine Learning on Heterogeneous\nDistributed Systems.” arXiv e-Prints, March,\narXiv:1603.04467.\n\n\nAggarwal, Charu, ed. 2007. Data Streams – Models and\nAlgorithms. Springer-Verlag.\n\n\nBartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf\nMersmann, eds. 2022. Hyperparameter Tuning for\nMachine and Deep Learning with R - A Practical Guide.\nSpringer.\n\n\nBartz-Beielstein, Thomas. 2023. “PyTorch\nHyperparameter Tuning with SPOT: Comparison with Ray\nTuner and Default Hyperparameters on\nCIFAR10.” https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.\n\n\n———. 2024a. “Evaluation and Performance Measurement.” In,\nedited by Eva Bartz and Thomas Bartz-Beielstein, 47–62. Singapore:\nSpringer Nature Singapore.\n\n\n———. 2024b. “Hyperparameter Tuning.” In, edited by Eva\nBartz and Thomas Bartz-Beielstein, 125–40. Singapore: Springer Nature\nSingapore.\n\n\n———. 2024c. “Introduction: From Batch to Online Machine\nLearning.” In Online Machine Learning: A Practical Guide with\nExamples in Python, edited by Eva Bartz and Thomas\nBartz-Beielstein, 1–11. Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-99-7007-0_1.\n\n\nBartz-Beielstein, Thomas, and Lukas Hans. 2024. “Drift Detection\nand Handling.” In Online Machine Learning: A Practical Guide\nwith Examples in Python, edited by Eva Bartz and Thomas\nBartz-Beielstein, 23–39. Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-99-7007-0_3.\n\n\nBartz-Beielstein, Thomas, and Martin Zaefferer. 2022.\n“Hyperparameter Tuning Approaches.” In Hyperparameter Tuning for Machine and Deep Learning with\nR - A Practical Guide, edited by Eva Bartz, Thomas\nBartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, 67–114. Springer.\n\n\nBifet, Albert. 2010. Adaptive Stream Mining: Pattern Learning and\nMining from Evolving Data Streams. Vol. 207. Frontiers in\nArtificial Intelligence and Applications. IOS Press.\n\n\nBifet, Albert, and Ricard Gavaldà. 2007. “Learning from\nTime-Changing Data with Adaptive Windowing.” In Proceedings\nof the 2007 SIAM International Conference on Data Mining (SDM),\n443–48.\n\n\n———. 2009. “Adaptive Learning from Evolving Data Streams.”\nIn Proceedings of the 8th International Symposium on Intelligent\nData Analysis: Advances in Intelligent Data Analysis VIII, 249–60.\nIDA ’09. Berlin, Heidelberg: Springer-Verlag.\n\n\nBifet, Albert, Geoff Holmes, Richard Kirkby, and Bernhard Pfahringer.\n2010a. “MOA: Massive Online\nAnalysis.” Journal of Machine Learning Research 99:\n1601–4.\n\n\n———. 2010b. “MOA: Massive Online Analysis.” Journal of\nMachine Learning Research 11: 1601–4.\n\n\nChollet, Francoise, and J. J. Allaire. 2018. Deep Learning with\nPython. Manning.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\n“BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding.” arXiv\ne-Prints, October, arXiv:1810.04805.\n\n\nDomingos, Pedro M., and Geoff Hulten. 2000. “Mining High-Speed\nData Streams.” In Proceedings of the Sixth ACM\nSIGKDD International Conference on Knowledge Discovery and\nData Mining, Boston, MA, USA, August 20-23, 2000, edited by Raghu\nRamakrishnan, Salvatore J. Stolfo, Roberto J. Bayardo, and Ismail Parsa,\n71–80. ACM.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.\n2020. “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale.” arXiv\ne-Prints, October, arXiv:2010.11929.\n\n\nDredze, Mark, Tim Oates, and Christine Piatko. 2010. “We’re Not in\nKansas Anymore: Detecting Domain Changes in Streams.” In\nProceedings of the 2010 Conference on Empirical Methods in Natural\nLanguage Processing, 585–95.\n\n\nForrester, Alexander, András Sóbester, and Andy Keane. 2008. Engineering Design via Surrogate Modelling.\nWiley.\n\n\nFriedman, Jerome H. 1991. “Multivariate Adaptive Regression\nSplines.” The Annals of Statistics 19 (1): 1–67.\n\n\nGaber, Mohamed Medhat, Arkady Zaslavsky, and Shonali Krishnaswamy. 2005.\n“Mining Data Streams: A Review.” SIGMOD\nRec. 34: 18–26.\n\n\nGama, João, Pedro Medas, Gladys Castillo, and Pedro Rodrigues. 2004.\n“Learning with Drift Detection.” In Advances in\nArtificial Intelligence – SBIA 2004, edited by Ana L. C. Bazzan and\nSofiane Labidi, 286–95. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nGama, João, Raquel Sebastião, and Pedro Pereira Rodrigues. 2013.\n“On Evaluating Stream Learning Algorithms.” Machine\nLearning 90 (3): 317–46.\n\n\nGramacy, Robert B. 2020. Surrogates. CRC press.\n\n\nHoeglinger, Stefan, and Russel Pears. 2007. “Use of Hoeffding\nTrees in Concept Based Data Stream Mining.” 2007 Third\nInternational Conference on Information and Automation for\nSustainability, 57–62.\n\n\nIkonomovska, Elena. 2012. “Algorithms for Learning Regression\nTrees and Ensembles on Evolving Data Streams.” PhD thesis, Jozef\nStefan International Postgraduate School.\n\n\nIkonomovska, Elena, João Gama, and Sašo Džeroski. 2011. “Learning\nModel Trees from Evolving Data Streams.” Data Mining and\nKnowledge Discovery 23 (1): 128–68.\n\n\nJain, Sarthak, and Byron C. Wallace. 2019. “Attention is not Explanation.” arXiv\ne-Prints, February, arXiv:1902.10186.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2014. An Introduction to Statistical Learning\nwith Applications in R. 7th ed. Springer.\n\n\nKeller-McNulty, Sallie, ed. 2004. Statistical Analysis of Massive\nData Streams: Proceedings of a Workshop. Washington,\nDC: Committee on Applied; Theoretical Statistics, National Research\nCouncil; National Academies Press.\n\n\nLippe, Phillip. 2022. “UvA Deep Learning\nTutorials.”\n\n\nLiu, Liyuan, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu,\nJianfeng Gao, and Jiawei Han. 2019. “On the\nVariance of the Adaptive Learning Rate and Beyond.”\narXiv e-Prints, August, arXiv:1908.03265.\n\n\nManapragada, Chaitanya, Geoffrey I. Webb, and Mahsa Salehi. 2018.\n“Extremely Fast Decision Tree.” In KDD’ 2018 -\nProceedings of the 24th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, edited by Chih-Jen Lin and Hui Xiong,\n1953–62. United States of America: Association for Computing Machinery\n(ACM). https://doi.org/10.1145/3219819.3220005.\n\n\nMasud, Mohammad, Jing Gao, Latifur Khan, Jiawei Han, and Bhavani M\nThuraisingham. 2011. “Classification and Novel Class Detection in\nConcept-Drifting Data Streams Under Time Constraints.” IEEE\nTransactions on Knowledge and Data Engineering 23 (6): 859–74.\n\n\nMontiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey\nBolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021.\n“River: Machine Learning for Streaming Data in Python.”\n\n\nMourtada, Jaouad, Stephane Gaiffas, and Erwan Scornet. 2019.\n“AMF: Aggregated Mondrian Forests for Online\nLearning.” arXiv e-Prints, June,\narXiv:1906.10529. https://doi.org/10.48550/arXiv.1906.10529.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.\nGrisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning\nin Python.” Journal of Machine Learning\nResearch 12: 2825–30.\n\n\nPutatunda, Sayan. 2021. Practical Machine Learning for Streaming\nData with Python. Springer.\n\n\nSantner, T J, B J Williams, and W I Notz. 2003. The Design and Analysis of Computer\nExperiments. Berlin, Heidelberg, New York: Springer.\n\n\nStreet, W. Nick, and YongSeog Kim. 2001. “A Streaming Ensemble\nAlgorithm (SEA) for Large-Scale Classification.” In\nProceedings of the Seventh ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 377–82. KDD ’01. New York, NY,\nUSA: Association for Computing Machinery.\n\n\nTay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020.\n“Efficient Transformers: A Survey.” arXiv\ne-Prints, September, arXiv:2009.06732.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” arXiv\ne-Prints, June, 1–15.\n\n\nWiegreffe, Sarah, and Yuval Pinter. 2019. “Attention is not not Explanation.”\narXiv e-Prints, August, arXiv:1908.04626.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-spotpython-saving-and-loading",
    "href": "037_spot_lightning_save_load_models.html#sec-spotpython-saving-and-loading",
    "title": "31  Saving and Loading",
    "section": "",
    "text": "31.1.1 spotPython as an Optimizer\nIf spotPython is used as an optimizer, no dictionary of hyperparameters has be specified. The fun_control dictionary is sufficient.\n\nimport os\nimport pprint\nfrom spotPython.utils.file import load_experiment\nfrom spotPython.utils.file import get_experiment_filename\nimport numpy as np\nfrom math import inf\nfrom spotPython.spot import spot\nfrom spotPython.utils.init import (\n    fun_control_init,\n    design_control_init,\n    surrogate_control_init,\n    optimizer_control_init)\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_branin\nfun_control = fun_control_init(\n            PREFIX=\"branin\",\n            SUMMARY_WRITER=False,\n            lower = np.array([0, 0]),\n            upper = np.array([10, 10]),\n            fun_evals=8,\n            fun_repeats=1,\n            max_time=inf,\n            noise=False,\n            tolerance_x=0,\n            ocba_delta=0,\n            var_type=[\"num\", \"num\"],\n            infill_criterion=\"ei\",\n            n_points=1,\n            seed=123,\n            log_level=20,\n            show_models=False,\n            show_progress=True)\ndesign_control = design_control_init(\n            init_size=5,\n            repeats=1)\nsurrogate_control = surrogate_control_init(\n            model_fun_evals=10000,\n            min_theta=-3,\n            max_theta=3,\n            n_theta=2,\n            theta_init_zero=True,\n            n_p=1,\n            optim_p=False,\n            var_type=[\"num\", \"num\"],\n            seed=124)\noptimizer_control = optimizer_control_init(\n            max_iter=1000,\n            seed=125)\nspot_tuner = spot.Spot(fun=fun,\n            fun_control=fun_control,\n            design_control=design_control,\n            surrogate_control=surrogate_control,\n            optimizer_control=optimizer_control)\nspot_tuner.run()\nPREFIX = fun_control[\"PREFIX\"]\nfilename = get_experiment_filename(PREFIX)\nspot_tuner.save_experiment(filename=filename)\nprint(f\"filename: {filename}\")\n\nspotPython tuning: 4.7932399644479124 [########--] 75.00% \nspotPython tuning: 2.0379795645847087 [#########-] 87.50% \nspotPython tuning: 1.986328241945829 [##########] 100.00% Done...\n\nfilename: spot_branin_experiment.pickle\n\n\n\n(spot_tuner_1, fun_control_1, design_control_1,\n    surrogate_control_1, optimizer_control_1) = load_experiment(filename)\n\nThe progress of the original experiment is shown in Figure 31.1 and the reloaded experiment in Figure 31.2.\n\nspot_tuner.plot_progress(log_y=True)\n\n\n\n\n\n\n\nFigure 31.1: Progress of the original experiment\n\n\n\n\n\n\nspot_tuner_1.plot_progress(log_y=True)\n\n\n\n\n\n\n\nFigure 31.2: Progress of the reloaded experiment\n\n\n\n\n\nThe results from the original experiment are shown in Table 31.1 and the reloaded experiment in Table 31.2.\n\nspot_tuner.print_results()\n\nmin y: 1.986328241945829\nx0: 10.0\nx1: 3.2107728198306598\n\n\n\n\nTable 31.1\n\n\n\n[['x0', 10.0], ['x1', 3.2107728198306598]]\n\n\n\n\n\n\nspot_tuner_1.print_results()\n\nmin y: 1.986328241945829\nx0: 10.0\nx1: 3.2107728198306598\n\n\n\n\nTable 31.2\n\n\n\n[['x0', 10.0], ['x1', 3.2107728198306598]]\n\n\n\n\n\n\n31.1.1.1 Getting the Tuned Hyperparameters\nThe tuned hyperparameters can be obtained as a dictionary with the following code.\n\nfrom spotPython.hyperparameters.values import get_tuned_hyperparameters\nget_tuned_hyperparameters(spot_tuner=spot_tuner)\n\n{'x0': 10.0, 'x1': 3.2107728198306598}\n\n\n\n\n\n\n\n\nSummary: Saving and Loading Optimization Experiments\n\n\n\n\nIf spotPython is used as an optimizer (without an hyperparameter dictionary), experiments can be saved and reloaded with the save_experiment and load_experiment functions.\nThe tuned hyperparameters can be obtained with the get_tuned_hyperparameters function.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  }
]