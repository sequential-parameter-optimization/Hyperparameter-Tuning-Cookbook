[
  {
    "objectID": "601_spot_hpt_light_diabetes.html",
    "href": "601_spot_hpt_light_diabetes.html",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "",
    "text": "31.1 The Basic Setting\nIn this section, we will show how spotpython can be integrated into the PyTorch Lightning training workflow for a regression task. It demonstrates how easy it is to use spotpython to tune hyperparameters for a PyTorch Lightning model.\nimport os\nfrom math import inf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nAfter importing the necessary libraries, the fun_control dictionary is set up via the fun_control_init function. The fun_control dictionary contains\nThe HyperLight class is used to define the objective function fun. It connects the PyTorch and the spotpython methods and is provided by spotpython.\nfrom spotpython.data.diabetes import Diabetes\nfrom spotpython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotpython.fun.hyperlight import HyperLight\nfrom spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\nfrom spotpython.utils.eda import gen_design_table\nfrom spotpython.spot import spot\nfrom spotpython.utils.file import get_experiment_filename\n\nPREFIX=\"601\"\n\ndata_set = Diabetes()\n\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    save_experiment=True,\n    fun_evals=inf,\n    max_time=1,\n    data_set = data_set,\n    core_model_name=\"light.regression.NNLinearRegressor\",\n    hyperdict=LightHyperDict,\n    _L_in=10,\n    _L_out=1)\n\nfun = HyperLight().fun\n\nmodule_name: light\nsubmodule_name: regression\nmodel_name: NNLinearRegressor\nThe method set_hyperparameter allows the user to modify default hyperparameter settings. Here we modify some hyperparameters to keep the model small and to decrease the tuning time.\nfrom spotpython.hyperparameters.values import set_hyperparameter\nset_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\nset_hyperparameter(fun_control, \"l1\", [3,4])\nset_hyperparameter(fun_control, \"epochs\", [3,7])\nset_hyperparameter(fun_control, \"batch_size\", [4,11])\nset_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\nset_hyperparameter(fun_control, \"patience\", [2,3])\n\ndesign_control = design_control_init(init_size=10)\n\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |     3   |   4     | transform_power_2_int |\n| epochs         | int    | 4         |     3   |   7     | transform_power_2_int |\n| batch_size     | int    | 4         |     4   |  11     | transform_power_2_int |\n| act_fn         | factor | ReLU      |     0   |   5     | None                  |\n| optimizer      | factor | SGD       |     0   |   2     | None                  |\n| dropout_prob   | float  | 0.01      |     0   |   0.025 | None                  |\n| lr_mult        | float  | 1.0       |     0.1 |  10     | None                  |\n| patience       | int    | 2         |     2   |   3     | transform_power_2_int |\n| batch_norm     | factor | 0         |     0   |   1     | None                  |\n| initialization | factor | Default   |     0   |   4     | None                  |\nFinally, a Spot object is created. Calling the method run() starts the hyperparameter tuning process.\nspot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\nres = spot_tuner.run()\n\nExperiment saved to 601_exp.pkl\n\nIn fun(): config:\n{'act_fn': Sigmoid(),\n 'batch_norm': False,\n 'batch_size': 2048,\n 'dropout_prob': np.float64(0.010469763733360567),\n 'epochs': 16,\n 'initialization': 'xavier_uniform',\n 'l1': 16,\n 'lr_mult': np.float64(4.135888451953213),\n 'optimizer': 'Adam',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 23075.166015625, 'hp_metric': 23075.166015625}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.0184251494885258),\n 'epochs': 32,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(3.1418668140600845),\n 'optimizer': 'Adadelta',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 3600.9345703125, 'hp_metric': 3600.9345703125}\n\nIn fun(): config:\n{'act_fn': ELU(),\n 'batch_norm': True,\n 'batch_size': 256,\n 'dropout_prob': np.float64(0.00996276270809942),\n 'epochs': 64,\n 'initialization': 'Default',\n 'l1': 16,\n 'lr_mult': np.float64(8.543578103398445),\n 'optimizer': 'Adadelta',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 4820.0556640625, 'hp_metric': 4820.0556640625}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_norm': True,\n 'batch_size': 512,\n 'dropout_prob': np.float64(0.004305336774252681),\n 'epochs': 8,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(0.3009268823483702),\n 'optimizer': 'Adamax',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 24072.69140625, 'hp_metric': 24072.69140625}\n\nIn fun(): config:\n{'act_fn': Tanh(),\n 'batch_norm': True,\n 'batch_size': 128,\n 'dropout_prob': np.float64(0.021718144359373085),\n 'epochs': 32,\n 'initialization': 'kaiming_uniform',\n 'l1': 16,\n 'lr_mult': np.float64(8.005670267977834),\n 'optimizer': 'Adam',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 22560.013671875, 'hp_metric': 22560.013671875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.023931753071792624),\n 'epochs': 16,\n 'initialization': 'xavier_normal',\n 'l1': 16,\n 'lr_mult': np.float64(1.2532486761645163),\n 'optimizer': 'Adamax',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 4145.4853515625, 'hp_metric': 4145.4853515625}\n\nIn fun(): config:\n{'act_fn': ELU(),\n 'batch_norm': False,\n 'batch_size': 512,\n 'dropout_prob': np.float64(0.0074444117802003025),\n 'epochs': 8,\n 'initialization': 'kaiming_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(9.535342719713716),\n 'optimizer': 'Adam',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 20313.955078125, 'hp_metric': 20313.955078125}\n\nIn fun(): config:\n{'act_fn': Swish(),\n 'batch_norm': True,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.0012790404219919403),\n 'epochs': 128,\n 'initialization': 'kaiming_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(2.4659566199812857),\n 'optimizer': 'Adadelta',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 4210.201171875, 'hp_metric': 4210.201171875}\n\nIn fun(): config:\n{'act_fn': Tanh(),\n 'batch_norm': False,\n 'batch_size': 128,\n 'dropout_prob': np.float64(0.0153979445945591),\n 'epochs': 32,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(6.089028896372417),\n 'optimizer': 'Adamax',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 20617.552734375, 'hp_metric': 20617.552734375}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': True,\n 'batch_size': 1024,\n 'dropout_prob': np.float64(0.013939072152682473),\n 'epochs': 64,\n 'initialization': 'xavier_uniform',\n 'l1': 16,\n 'lr_mult': np.float64(5.8899766345108855),\n 'optimizer': 'Adam',\n 'patience': 8}\ntrain_model result: {'val_loss': 23936.0, 'hp_metric': 23936.0}\n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.014588534295585199),\n 'epochs': 32,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(1.6833211254717404),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 3352.73046875, 'hp_metric': 3352.73046875}\nspotpython tuning: 3352.73046875 [----------] 1.91% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 1024,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 8,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(0.1),\n 'optimizer': 'Adadelta',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 24044.990234375, 'hp_metric': 24044.990234375}\nspotpython tuning: 3352.73046875 [----------] 3.13% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.0030062981463069597),\n 'epochs': 32,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(0.1),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 4103.90087890625, 'hp_metric': 4103.90087890625}\nspotpython tuning: 3352.73046875 [#---------] 5.11% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 32,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(1.0613993717832497),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 3292.369384765625, 'hp_metric': 3292.369384765625}\nspotpython tuning: 3292.369384765625 [#---------] 7.04% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 32,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(0.1),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 4356.8642578125, 'hp_metric': 4356.8642578125}\nspotpython tuning: 3292.369384765625 [#---------] 8.89% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 16,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(1.2022533527679018),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 3483.728759765625, 'hp_metric': 3483.728759765625}\nspotpython tuning: 3292.369384765625 [#---------] 11.38% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 32,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(1.4240550340007212),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 3563.134521484375, 'hp_metric': 3563.134521484375}\nspotpython tuning: 3292.369384765625 [#---------] 13.65% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 32,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(2.179965267494811),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 3734.990478515625, 'hp_metric': 3734.990478515625}\nspotpython tuning: 3292.369384765625 [##--------] 15.71% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 32,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(1.0926472402028964),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 4844.357421875, 'hp_metric': 4844.357421875}\nspotpython tuning: 3292.369384765625 [##--------] 17.43% \n\n\n\nIn fun(): config:\n{'act_fn': Swish(),\n 'batch_norm': True,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 128,\n 'initialization': 'kaiming_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(2.72940453146037),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 4836.9091796875, 'hp_metric': 4836.9091796875}\nspotpython tuning: 3292.369384765625 [##--------] 23.58% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': True,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 8,\n 'initialization': 'kaiming_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(1.8324228882258025),\n 'optimizer': 'Adadelta',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 23321.056640625, 'hp_metric': 23321.056640625}\nspotpython tuning: 3292.369384765625 [###-------] 26.46% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 8,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(1.0965249819663794),\n 'optimizer': 'Adadelta',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 3884.6220703125, 'hp_metric': 3884.6220703125}\nspotpython tuning: 3292.369384765625 [###-------] 28.23% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(2.899194801763045e-19),\n 'epochs': 16,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(1.3104752887649715),\n 'optimizer': 'Adadelta',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 3689.428466796875, 'hp_metric': 3689.428466796875}\nspotpython tuning: 3292.369384765625 [###-------] 30.83% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 128,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(2.3293332817391996),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 2934.782958984375, 'hp_metric': 2934.782958984375}\nspotpython tuning: 2934.782958984375 [####------] 35.07% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 128,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(1.2478656331564257),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 4102.27490234375, 'hp_metric': 4102.27490234375}\nspotpython tuning: 2934.782958984375 [####------] 38.51% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 8,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(2.0272815388566454),\n 'optimizer': 'Adadelta',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 8021.01416015625, 'hp_metric': 8021.01416015625}\nspotpython tuning: 2934.782958984375 [####------] 40.80% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 128,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(3.2783216390884165),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 7338.6650390625, 'hp_metric': 7338.6650390625}\nspotpython tuning: 2934.782958984375 [####------] 43.76% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 8,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(6.010223843300105),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 123626.9453125, 'hp_metric': 123626.9453125}\nspotpython tuning: 2934.782958984375 [#####-----] 46.09% \n\n\n\nIn fun(): config:\n{'act_fn': Swish(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 128,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(2.3395537075714543),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 3291.048828125, 'hp_metric': 3291.048828125}\nspotpython tuning: 2934.782958984375 [#####-----] 48.55% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 8,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(0.3437263180477352),\n 'optimizer': 'Adadelta',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 4609.2099609375, 'hp_metric': 4609.2099609375}\nspotpython tuning: 2934.782958984375 [#####-----] 51.40% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.017306820123647045),\n 'epochs': 8,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(0.3108245302971776),\n 'optimizer': 'Adadelta',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 23894.646484375, 'hp_metric': 23894.646484375}\nspotpython tuning: 2934.782958984375 [#####-----] 54.08% \n\n\n\nIn fun(): config:\n{'act_fn': Swish(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 16,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(2.3508233547376998),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 3500.318359375, 'hp_metric': 3500.318359375}\nspotpython tuning: 2934.782958984375 [######----] 56.95% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 8,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(2.37931834812398),\n 'optimizer': 'Adadelta',\n 'patience': 8}\n\n\ntrain_model result: {'val_loss': 17799.318359375, 'hp_metric': 17799.318359375}\nspotpython tuning: 2934.782958984375 [######----] 59.39% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 2048,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 32,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(1.1920367043207052),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 13467.818359375, 'hp_metric': 13467.818359375}\nspotpython tuning: 2934.782958984375 [######----] 61.25% \n\n\n\nIn fun(): config:\n{'act_fn': Swish(),\n 'batch_norm': False,\n 'batch_size': 64,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 64,\n 'initialization': 'kaiming_normal',\n 'l1': 8,\n 'lr_mult': np.float64(0.5997290653429026),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': 3236.67626953125, 'hp_metric': 3236.67626953125}\nspotpython tuning: 2934.782958984375 [######----] 63.85% \n\n\n\nIn fun(): config:\n{'act_fn': ELU(),\n 'batch_norm': False,\n 'batch_size': 256,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 64,\n 'initialization': 'Default',\n 'l1': 8,\n 'lr_mult': np.float64(9.275384031427247),\n 'optimizer': 'Adadelta',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': nan, 'hp_metric': nan}\n\nIn fun(): config:\n{'act_fn': ELU(),\n 'batch_norm': False,\n 'batch_size': 16,\n 'dropout_prob': np.float64(0.017696270829039733),\n 'epochs': 8,\n 'initialization': 'xavier_normal',\n 'l1': 16,\n 'lr_mult': np.float64(8.731063112119566),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': nan, 'hp_metric': nan}\nspotpython tuning: 2934.782958984375 [#######---] 65.96% \n\n\n\nIn fun(): config:\n{'act_fn': ELU(),\n 'batch_norm': False,\n 'batch_size': 256,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 64,\n 'initialization': 'Default',\n 'l1': 8,\n 'lr_mult': np.float64(9.275384031427247),\n 'optimizer': 'Adadelta',\n 'patience': 4}\ntrain_model result: {'val_loss': nan, 'hp_metric': nan}\n\nIn fun(): config:\n{'act_fn': ELU(),\n 'batch_norm': True,\n 'batch_size': 256,\n 'dropout_prob': np.float64(0.013555351828641655),\n 'epochs': 64,\n 'initialization': 'kaiming_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(6.101081501756519),\n 'optimizer': 'Adam',\n 'patience': 4}\ntrain_model result: {'val_loss': 22613.447265625, 'hp_metric': 22613.447265625}\nspotpython tuning: 2934.782958984375 [#######---] 68.68% \n\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 16,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 128,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(0.1),\n 'optimizer': 'Adadelta',\n 'patience': 4}\n\n\ntrain_model result: {'val_loss': 3250.787353515625, 'hp_metric': 3250.787353515625}\nspotpython tuning: 2934.782958984375 [#########-] 93.48% \n\n\n\nIn fun(): config:\n{'act_fn': Swish(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 64,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(2.7983601260656146),\n 'optimizer': 'Adadelta',\n 'patience': 8}\ntrain_model result: {'val_loss': 3583.208740234375, 'hp_metric': 3583.208740234375}\nspotpython tuning: 2934.782958984375 [##########] 97.66% \n\n\n\nIn fun(): config:\n{'act_fn': ELU(),\n 'batch_norm': False,\n 'batch_size': 2048,\n 'dropout_prob': np.float64(0.0),\n 'epochs': 32,\n 'initialization': 'Default',\n 'l1': 8,\n 'lr_mult': np.float64(10.0),\n 'optimizer': 'Adamax',\n 'patience': 4}\ntrain_model result: {'val_loss': nan, 'hp_metric': nan}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 16,\n 'dropout_prob': np.float64(0.011453003926657934),\n 'epochs': 32,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(7.578885889751239),\n 'optimizer': 'Adamax',\n 'patience': 8}\ntrain_model result: {'val_loss': 3086.393798828125, 'hp_metric': 3086.393798828125}\nspotpython tuning: 2934.782958984375 [##########] 100.00% Done...\n\nExperiment saved to 601_res.pkl",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#sec-basic-setup-601",
    "href": "601_spot_hpt_light_diabetes.html#sec-basic-setup-601",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "",
    "text": "PREFIX: a unique identifier for the experiment\nfun_evals: the number of function evaluations\nmax_time: the maximum run time in minutes\ndata_set: the data set. Here we use the Diabetes data set that is provided by spotpython.\ncore_model_name: the class name of the neural network model. This neural network model is provided by spotpython.\nhyperdict: the hyperparameter dictionary. This dictionary is used to define the hyperparameters of the neural network model. It is also provided by spotpython.\n_L_in: the number of input features. Since the Diabetes data set has 10 features, _L_in is set to 10.\n_L_out: the number of output features. Since we want to predict a single value, _L_out is set to 1.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#looking-at-the-results",
    "href": "601_spot_hpt_light_diabetes.html#looking-at-the-results",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "31.2 Looking at the Results",
    "text": "31.2 Looking at the Results\n\n31.2.1 Tuning Progress\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized with spotpython’s method plot_progress. The black points represent the performace values (score or metric) of hyperparameter configurations from the initial design, whereas the red points represents the hyperparameter configurations found by the surrogate model based optimization.\n\nspot_tuner.plot_progress()\n\n\n\n\n\n\n\n\n\n\n31.2.2 Tuned Hyperparameters and Their Importance\nResults can be printed in tabular form.\n\nfrom spotpython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper | tuned              | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     3.0 |     4.0 | 3.0                | transform_power_2_int |         0.74 | .       |\n| epochs         | int    | 4         |     3.0 |     7.0 | 7.0                | transform_power_2_int |       100.00 | ***     |\n| batch_size     | int    | 4         |     4.0 |    11.0 | 5.0                | transform_power_2_int |         3.95 | *       |\n| act_fn         | factor | ReLU      |     0.0 |     5.0 | ReLU               | None                  |         0.74 | .       |\n| optimizer      | factor | SGD       |     0.0 |     2.0 | Adadelta           | None                  |         0.74 | .       |\n| dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.025              | None                  |         0.74 | .       |\n| lr_mult        | float  | 1.0       |     0.1 |    10.0 | 2.3293332817391996 | None                  |        56.45 | **      |\n| patience       | int    | 2         |     2.0 |     3.0 | 3.0                | transform_power_2_int |         0.74 | .       |\n| batch_norm     | factor | 0         |     0.0 |     1.0 | 0                  | None                  |        46.74 | *       |\n| initialization | factor | Default   |     0.0 |     4.0 | xavier_uniform     | None                  |         4.11 | *       |\n\n\nA histogram can be used to visualize the most important hyperparameters.\n\nspot_tuner.plot_importance(threshold=1.0)\n\n\n\n\n\n\n\n\n\nspot_tuner.plot_important_hyperparameter_contour(max_imp=3)\n\nl1:  0.7432081423526133\nepochs:  100.0\nbatch_size:  3.952042472018898\nact_fn:  0.7432081423526133\noptimizer:  0.7432081423526133\ndropout_prob:  0.7432081423526133\nlr_mult:  56.453758964696696\npatience:  0.7432081423526133\nbatch_norm:  46.7376316948175\ninitialization:  4.1072816639852725\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31.2.3 Get the Tuned Architecture\n\nimport pprint\nfrom spotpython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\npprint.pprint(config)\n\n{'act_fn': ReLU(),\n 'batch_norm': False,\n 'batch_size': 32,\n 'dropout_prob': np.float64(0.025),\n 'epochs': 128,\n 'initialization': 'xavier_uniform',\n 'l1': 8,\n 'lr_mult': np.float64(2.3293332817391996),\n 'optimizer': 'Adadelta',\n 'patience': 8}\n\n\n\n\n31.2.4 Test on the full data set\n\n# set the value of the key \"TENSORBOARD_CLEAN\" to True in the fun_control dictionary and use the update() method to update the fun_control dictionary\nfun_control.update({\"TENSORBOARD_CLEAN\": True})\nfun_control.update({\"tensorboard_log\": True})\n\n\nfrom spotpython.light.testmodel import test_model\nfrom spotpython.utils.init import get_feature_names\n\ntest_model(config, fun_control)\nget_feature_names(fun_control)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3908.829345703125     │\n│         val_loss          │     3908.829345703125     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntest_model result: {'val_loss': 3908.829345703125, 'hp_metric': 3908.829345703125}\n\n\n['age',\n 'sex',\n 'bmi',\n 'bp',\n 's1_tc',\n 's2_ldl',\n 's3_hdl',\n 's4_tch',\n 's5_ltg',\n 's6_glu']",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#cross-validation-with-lightning",
    "href": "601_spot_hpt_light_diabetes.html#cross-validation-with-lightning",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "31.3 Cross Validation With Lightning",
    "text": "31.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nconfig\n\n{'l1': 8,\n 'epochs': 128,\n 'batch_size': 32,\n 'act_fn': ReLU(),\n 'optimizer': 'Adadelta',\n 'dropout_prob': np.float64(0.025),\n 'lr_mult': np.float64(2.3293332817391996),\n 'patience': 8,\n 'batch_norm': False,\n 'initialization': 'xavier_uniform'}\n\n\n\nfrom spotpython.light.cvmodel import cv_model\nfun_control.update({\"k_folds\": 2})\nfun_control.update({\"test_size\": 0.6})\ncv_model(config, fun_control)\n\nk: 0\n\n\ntrain_model result: {'val_loss': 4058.8876953125, 'hp_metric': 4058.8876953125}\nk: 1\ntrain_model result: {'val_loss': 8542.123046875, 'hp_metric': 8542.123046875}\n\n\n6300.50537109375",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#extending-the-basic-setup",
    "href": "601_spot_hpt_light_diabetes.html#extending-the-basic-setup",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "31.4 Extending the Basic Setup",
    "text": "31.4 Extending the Basic Setup\nThis basic setup can be adapted to user-specific needs in many ways. For example, the user can specify a custom data set, a custom model, or a custom loss function. The following sections provide more details on how to customize the hyperparameter tuning process. Before we proceed, we will provide an overview of the basic settings of the hyperparameter tuning process and explain the parameters used so far.\n\n31.4.1 General Experiment Setup\nTo keep track of the different experiments, we use a PREFIX for the experiment name. The PREFIX is used to create a unique experiment name. The PREFIX is also used to create a unique TensorBoard folder, which is used to store the TensorBoard log files.\nspotpython allows the specification of two different types of stopping criteria: first, the number of function evaluations (fun_evals), and second, the maximum run time in seconds (max_time). Here, we will set the number of function evaluations to infinity and the maximum run time to one minute.\nmax_time is set to one minute for demonstration purposes. For real experiments, this value should be increased. Note, the total run time may exceed the specified max_time, because the initial design is always evaluated, even if this takes longer than max_time.\n\n\n31.4.2 Data Setup\nHere, we have provided the Diabetes data set class, which is a subclass of torch.utils.data.Dataset. Data preprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation.\nThe data splitting, i.e., the generation of training, validation, and testing data, is handled by Lightning.\n\n\n31.4.3 Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotpython.\n\n\n31.4.4 Core-Model Setup\nBy using core_model_name = \"light.regression.NNLinearRegressor\", the spotpython model class NetLightRegression [SOURCE] from the light.regression module is selected.\n\n\n31.4.5 Hyperdict Setup\nFor a given core_model_name, the corresponding hyperparameters are automatically loaded from the associated dictionary, which is stored as a JSON file. The JSON file contains hyperparameter type information, names, and bounds. For spotpython models, the hyperparameters are stored in the LightHyperDict, see [SOURCE] Alternatively, you can load a local hyper_dict. The hyperdict uses the default hyperparameter settings. These can be modified as described in ?sec-modifying-hyperparameter-levels.\n\n\n31.4.6 Other Settings\nThere are several additional parameters that can be specified, e.g., since we did not specify a loss function, mean_squared_error is used, which is the default loss function. These will be explained in more detail in the following sections.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#sec-tensorboard-601",
    "href": "601_spot_hpt_light_diabetes.html#sec-tensorboard-601",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "31.5 Tensorboard",
    "text": "31.5 Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard, if the argument tensorboard_log to fun_control_init() is set to True. The Tensorboard log files are stored in the runs folder. To start Tensorboard, run the following command in the terminal:\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#loading-the-saved-experiment-and-getting-the-hyperparameters-of-the-tuned-model",
    "href": "601_spot_hpt_light_diabetes.html#loading-the-saved-experiment-and-getting-the-hyperparameters-of-the-tuned-model",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "31.6 Loading the Saved Experiment and Getting the Hyperparameters of the Tuned Model",
    "text": "31.6 Loading the Saved Experiment and Getting the Hyperparameters of the Tuned Model\nTo get the tuned hyperparameters as a dictionary, the get_experiment_from_PREFIX function can be used.\n\nfrom spotpython.utils.file import load_result\nspot_tuner, fun_control, design_control, surrogate_control, optimizer_control = load_result(PREFIX=PREFIX)\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nconfig\n\nLoaded experiment from 601_res.pkl\n\n\n{'l1': 8,\n 'epochs': 128,\n 'batch_size': 32,\n 'act_fn': ReLU(),\n 'optimizer': 'Adadelta',\n 'dropout_prob': np.float64(0.025),\n 'lr_mult': np.float64(2.3293332817391996),\n 'patience': 8,\n 'batch_norm': False,\n 'initialization': 'xavier_uniform'}",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#using-the-spotgui",
    "href": "601_spot_hpt_light_diabetes.html#using-the-spotgui",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "31.7 Using the spotgui",
    "text": "31.7 Using the spotgui\nThe spotgui [github] provides a convenient way to interact with the hyperparameter tuning process. To obtain the settings from Section 31.1, the spotgui can be started as shown in Figure 31.1.\n\n\n\n\n\n\nFigure 31.1: spotgui",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "601_spot_hpt_light_diabetes.html#summary",
    "href": "601_spot_hpt_light_diabetes.html#summary",
    "title": "31  Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set",
    "section": "31.8 Summary",
    "text": "31.8 Summary\nThis section presented an introduction to the basic setup of hyperparameter tuning with spotpython and PyTorch Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html",
    "href": "003_scipy_optimize_intro.html",
    "title": "3  Introduction to scipy.optimize",
    "section": "",
    "text": "3.1 Derivative-free Optimization Algorithms\nSciPy provides algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, differential equations, statistics and many other classes of problems. SciPy is a collection of mathematical algorithms and convenience functions built on NumPy. It adds significant power to Python by providing the user with high-level commands and classes for manipulating and visualizing data.\nSciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programing, constrained and nonlinear least-squares, root finding, and curve fitting.\nIn this notebook, we will learn how to use the scipy.optimize module to solve optimization problems. See: https://docs.scipy.org/doc/scipy/tutorial/optimize.html\nCommon functions and objects, shared across different SciPy optimize solvers, are shown in Table 3.1.\nWe will introduce unconstrained minimization of multivariate scalar functions in this chapter. The minimize function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in scipy.optimize. To demonstrate the minimization function, consider the problem of minimizing the Rosenbrock function of N variables:\n\\[\nf(J) = \\sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2)^2 + (1 - x_i)^2\n\\]\nThe minimum value of this function is 0, which is achieved when (x_i = 1).\nNote that the Rosenbrock function and its derivatives are included in scipy.optimize. The implementations shown in the following sections provide examples of how to define an objective function as well as its Jacobian and Hessian functions. Objective functions in scipy.optimize expect a numpy array as their first parameter, which is to be optimized and must return a float value. The exact calling signature must be f(x, *args), where x represents a numpy array, and args is a tuple of additional arguments supplied to the objective function.\nSection 3.1.1 and Section 3.1.2 present two approaches that do not need gradient information to find the minimum. They use function evaluations to find the minimum.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#derivative-free-optimization-algorithms",
    "href": "003_scipy_optimize_intro.html#derivative-free-optimization-algorithms",
    "title": "3  Introduction to scipy.optimize",
    "section": "",
    "text": "3.1.1 Nelder-Mead Simplex Algorithm\nThe Nelder Mead is a simple local optimization algorithm. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum. It can be devided into the following steps:\n\nInitialize the simplex\nEvaluate the function at each vertex of the simplex\nOrder the vertices by function value\nReflect the worst point through the centroid of the remaining points\nIf the reflected point is better than the second worst, replace the worst point with the reflected point\nIf the reflected point is worse than the worst point, try contracting the simplex\nIf the reflected point is better than the best point, try expanding the simplex\nIf none of the above steps improve the simplex, shrink the simplex towards the best point\nCheck for convergence\n\nmethod='Nelder-Mead': In the example below, the minimize routine is used with the Nelder-Mead simplex algorithm (selected through the method parameter):\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef rosen(x):\n    \"\"\"The Rosenbrock function\"\"\"\n    return sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n\nx0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\nres = minimize(rosen, x0, method='nelder-mead',\n               options={'xatol': 1e-8, 'disp': True})\n\nprint(res.x)\n\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 339\n         Function evaluations: 571\n[1. 1. 1. 1. 1.]\n\n\nThe simplex algorithm is probably the simplest way to minimize a well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum.\n\n\n3.1.2 Powell’s Method\nAnother optimization algorithm that needs only function calls to find the minimum is Powell’s method, which can be selected by setting the method parameter to 'powell' in the minimize function. This algorithm consists of a conjugate direction method. It performs sequential one-dimensional minimizations along each vector of the directions set, which is updated at each iteration of the main minimization loop. It can be described by the following steps:\n\nInitialization\nMinimization along each direction\nCreate conjugate direction\nLine search along the conjugate direction\nCheck for convergence\n\n\nExample 3.1 To demonstrate how to supply additional arguments to an objective function, let’s consider minimizing the Rosenbrock function with an additional scaling factor \\(a\\) and an offset \\(b\\):\n\\[\nf(J, a, b) = \\sum_{i=1}^{N-1} a (x_{i+1} - x_i^2)^2 + (1 - x_i)^2 + b\n\\]\nYou can achieve this using the minimize routine with the example parameters \\(a=0.5\\) and \\(b=1\\):\n\ndef rosen_with_args(x, a, b):\n    \"\"\"The Rosenbrock function with additional arguments\"\"\"\n    return sum(a * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0) + b\n\nx0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\nres = minimize(rosen_with_args, x0, method='nelder-mead',\n               args=(0.5, 1.), options={'xatol': 1e-8, 'disp': True})\n\nprint(res.x)\n\nOptimization terminated successfully.\n         Current function value: 1.000000\n         Iterations: 319\n         Function evaluations: 525\n[1.         1.         1.         1.         0.99999999]\n\n\nAs an alternative to using the args parameter of minimize, you can wrap the objective function in a new function that accepts only x. This approach is also useful when it is necessary to pass additional parameters to the objective function as keyword arguments.\n\ndef rosen_with_args(x, a, *, b):  # b is a keyword-only argument\n    return sum(a * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0) + b\n\ndef wrapped_rosen_without_args(x):\n    return rosen_with_args(x, 0.5, b=1.)  # pass in `a` and `b`\n\nx0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\nres = minimize(wrapped_rosen_without_args, x0, method='nelder-mead',\n               options={'xatol': 1e-8,})\n\nprint(res.x)\n\n[1.         1.         1.         1.         0.99999999]\n\n\nAnother alternative is to use functools.partial.\n\nfrom functools import partial\n\npartial_rosen = partial(rosen_with_args, a=0.5, b=1.)\nres = minimize(partial_rosen, x0, method='nelder-mead',\n               options={'xatol': 1e-8,})\n\nprint(res.x)\n\n[1.         1.         1.         1.         0.99999999]",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#gradient-based-optimization-algorithms",
    "href": "003_scipy_optimize_intro.html#gradient-based-optimization-algorithms",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.2 Gradient-based Optimization Algorithms",
    "text": "3.2 Gradient-based Optimization Algorithms\n\n3.2.1 An Introductory Example: Broyden-Fletcher-Goldfarb-Shanno Algorithm (BFGS)\nThis section introduces an optimization algorithm that uses gradient information to find the minimum. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (selected by setting method='BFGS') is an optimization algorithm that aims to converge quickly to the solution. This algorithm uses the gradient of the objective function. If the gradient is not provided by the user, it is estimated using first-differences. The BFGS method typically requires fewer function calls compared to the simplex algorithm, even when the gradient needs to be estimated.\n\nExample 3.2 (BFGS) To demonstrate the BFGS algorithm, let’s use the Rosenbrock function again. The gradient of the Rosenbrock function is a vector described by the following mathematical expression:\n\\[\\begin{align}\n\\frac{\\partial f}{\\partial x_j} = \\sum_{i=1}^{N} 200(x_i - x_{i-1}^2)(\\delta_{i,j} - 2x_{i-1}\\delta_{i-1,j}) - 2(1 - x_{i-1})\\delta_{i-1,j} \\\\\n= 200(x_j - x_{j-1}^2) - 400x_j(x_{j+1} - x_j^2) - 2(1 - x_j)\n\\end{align}\\]\nThis expression is valid for interior derivatives, but special cases are:\n\\[\n\\frac{\\partial f}{\\partial x_0} = -400x_0(x_1 - x_0^2) - 2(1 - x_0)\n\\]\n\\[\n\\frac{\\partial f}{\\partial x_{N-1}} = 200(x_{N-1} - x_{N-2}^2)\n\\]\nHere’s a Python function that computes this gradient:\n\ndef rosen_der(x):\n    xm = x[1:-1]\n    xm_m1 = x[:-2]\n    xm_p1 = x[2:]\n    der = np.zeros_like(x)\n    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n    der[-1] = 200*(x[-1]-x[-2]**2)\n    return der\n\nYou can specify this gradient information in the minimize function using the jac parameter as illustrated below:\n\nres = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n               options={'disp': True})\n\nprint(res.x)\n\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 25\n         Function evaluations: 30\n         Gradient evaluations: 30\n[1.00000004 1.0000001  1.00000021 1.00000044 1.00000092]\n\n\n\n\n\n3.2.2 Background and Basics for Gradient-based Optimization\n\n\n3.2.3 Gradient\nThe gradient \\(\\nabla f(J)\\) for a scalar function \\(f(J)\\) with \\(n\\) different variables is defined by its partial derivatives:\n\\[\n\\nabla f(J) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right]\n\\]\n\n\n3.2.4 Jacobian Matrix\nThe Jacobian matrix \\(J(J)\\) for a vector-valued function \\(F(J) = [f_1(J), f_2(J), \\ldots, f_m(J)]\\) is defined as:\n\\(J(J) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\ldots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\ldots & \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\ldots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\\)\nIt consists of the first order partial derivatives and gives therefore an overview about the gradients of a vector valued function.\n\nExample 3.3 (acobian matrix) Consider a vector-valued function \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) defined as follows: \\[f(J) = \\begin{bmatrix} x_1^2 + 2x_2 \\\\ 3x_1 - \\sin(x_2) \\\\ e^{x_1 + x_2} \\end{bmatrix}\\]\nLet’s compute the partial derivatives and construct the Jacobian matrix:\n\\(\\frac{\\partial f_1}{\\partial x_1} = 2x_1, \\quad \\frac{\\partial f_1}{\\partial x_2} = 2\\)\n\\(\\frac{\\partial f_2}{\\partial x_1} = 3, \\quad \\frac{\\partial f_2}{\\partial x_2} = -\\cos(x_2)\\)\n\\(\\frac{\\partial f_3}{\\partial x_1} = e^{x_1 + x_2}, \\quad \\frac{\\partial f_3}{\\partial x_2} = e^{x_1 + x_2}\\)\nSo, the Jacobian matrix is:\n\\[J(J) = \\begin{bmatrix} 2x_1 & 2 \\\\ 3 & -\\cos(x_2) \\\\ e^{x_1 + x_2} & e^{x_1 + x_2} \\end{bmatrix}\\]\nThis Jacobian matrix provides information about how small changes in the input variables \\(x_1\\) and \\(x_2\\) affect the corresponding changes in each component of the output vector.\n\n\n\n3.2.5 Hessian Matrix\nThe Hessian matrix \\(H(J)\\) for a scalar function \\(f(J)\\) is defined as:\n\\(H(J) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\\)\nThe Hessian matrix consists of the second order derivatives of the function. It provides information about the local curvature of the function with respect to changes in the input variables.\n\nExample 3.4 (Hessian matrix) Consider a scalar-valued function: \\[f(J) = x_1^2 + 2x_2^2 + \\sin(x_1   x_2)\\]\nThe Hessian matrix of this scalar-valued function is the matrix of its second-order partial derivatives with respect to the input variables: \\[H(J) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix}\\]\nLet’s compute the second-order partial derivatives and construct the Hessian matrix:\n\\[\\begin{align}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= 2 + \\cos(x_1 x_2) x_2^2\\\\\n\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &= 2x_1  x_2 \\cos(x_1 x_2) - \\sin(x_1  x_2)\\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &= 2x_1  x_2  \\cos(x_1  x_2) - \\sin(x_1  x_2)\\\\\n\\frac{\\partial^2 f}{\\partial x_2^2} &= 4x_2^2 + \\cos(x_1  x_2) x_1^2\n\\end{align}\\]\nSo, the Hessian matrix is:\n\\[H(J) = \\begin{bmatrix} 2 + \\cos(x_1   x_2)   x_2^2 & 2x_1   x_2   \\cos(x_1   x_2) - \\sin(x_1   x_2) \\\\ 2x_1   x_2   \\cos(x_1   x_2) - \\sin(x_1   x_2) & 4x_2^2 + \\cos(x_1   x_2)   x_1^2 \\end{bmatrix}\\]\n\n\n\n3.2.6 Gradient Descent\nIn optimization, the goal is to find the minimum or maximum of a function. Gradient-based optimization methods utilize information about the gradient (or derivative) of the function to guide the search for the optimal solution. This is particularly useful when dealing with complex, high-dimensional functions where an exhaustive search is impractical.\nThe gradient descent method can be divided in the following steps:\n\nInitialize: start with an initial guess for the parameters of the function to be optimized.\nCompute Gradient: Calculate the gradient (partial derivatives) of the function with respect to each parameter at the current point. The gradient indicates the direction of the steepest increase in the function.\nUpdate Parameters: Adjust the parameters in the opposite direction of the gradient, scaled by a learning rate. This step aims to move towards the minimum of the function:\n\n\\(x_{k+1} = x_k - \\alpha \\times \\nabla f(x_{k})\\)\n\\(x_{x}\\) is current parameter vector or point in the parameter space.\n\\(\\alpha\\) is the learning rate, a positive scalar that determines the step size in each iteration.\n\\(\\nabla f(x)\\) is the gradient of the objective function.\n\nIterate: Repeat the above steps until convergence or a predefined number of iterations. Convergence is typically determined when the change in the function value or parameters becomes negligible.\n\n\nExample 3.5 (Gradient Descent) We consider a simple quadratic function as an example: \\[\nf(x) = x^2 + 4x + y^2 + 2y + 4.\n\\]\nWe’ll use gradient descent to find the minimum of this function.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the quadratic function\ndef quadratic_function(x, y):\n    return x**2 + 4*x + y**2 + 2*y + 4\n\n# Define the gradient of the quadratic function\ndef gradient_quadratic_function(x, y):\n    grad_x = 2*x + 4\n    grad_y = 2*y + 2\n    return np.array([grad_x, grad_y])\n\n# Gradient Descent for optimization in 2D\ndef gradient_descent(initial_point, learning_rate, num_iterations):\n    points = [np.array(initial_point)]\n    for _ in range(num_iterations):\n        current_point = points[-1]\n        gradient = gradient_quadratic_function(*current_point)\n        new_point = current_point - learning_rate * gradient\n        points.append(new_point)\n    return points\n\n# Visualization of optimization process with 3D surface and consistent arrow sizes\ndef plot_optimization_process_3d_consistent_arrows(points):\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n\n    x_vals = np.linspace(-10, 2, 100)\n    y_vals = np.linspace(-10, 2, 100)\n    X, Y = np.meshgrid(x_vals, y_vals)\n    Z = quadratic_function(X, Y)\n\n    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n    ax.scatter(*zip(*points), [quadratic_function(*p) for p in points], c='red', label='Optimization Trajectory')\n\n    for i in range(len(points) - 1):  \n        x, y = points[i]\n        dx, dy = points[i + 1] - points[i]\n        dz = quadratic_function(*(points[i + 1])) - quadratic_function(*points[i])\n        gradient_length = 0.5\n\n        ax.quiver(x, y, quadratic_function(*points[i]), dx, dy, dz, color='blue', length=gradient_length, normalize=False, arrow_length_ratio=0.1)\n\n    ax.set_title('Gradient-Based Optimization with 2D Quadratic Function')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('f(x, y)')\n    ax.legend()\n    plt.show()\n\n# Initial guess and parameters\ninitial_guess = [-9.0, -9.0]\nlearning_rate = 0.2\nnum_iterations = 10\n\n# Run gradient descent in 2D and visualize the optimization process with 3D surface and consistent arrow sizes\ntrajectory = gradient_descent(initial_guess, learning_rate, num_iterations)\nplot_optimization_process_3d_consistent_arrows(trajectory)\n\n\n\n\n\n\n\n\n\n\n\n3.2.7 Newton Method\nInitialization: Start with an initial guess for the optimal solution: \\(x_0\\).\nIteration: Repeat the following three steps until convergence or a predefined stopping criterion is met:\n\nCalculate the gradient (\\(\\nabla\\)) and the Hessian matrix (\\(\\nabla^2\\)) of the objective function at the current point: \\[\\nabla f(x_k) \\quad \\text{and} \\quad \\nabla^2 f(x_k)\\]\nUpdate the current solution using the Newton-Raphson update formula \\[\nx_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k),\n\\] where\n* $\\nabla f(x_k)$ is the gradient (first derivative) of the objective function with respect to the variable $x$, evaluated at the current solution $x_k$.\n\n\\(\\nabla^2 f(x_k)\\): The Hessian matrix (second derivative) of the objective function with respect to \\(x\\), evaluated at the current solution \\(x_k\\).\n\\(x_k\\): The current solution or point in the optimization process.\n\\(\\nabla^2 f(x_k)]^{-1}\\): The inverse of the Hessian matrix at the current point, representing the approximation of the curvature of the objective function.\n\\(x_{k+1}\\): The updated solution or point after applying the Newton-Raphson update.\n\nCheck for convergence.\n\n\nExample 3.6 (Newton Method) We want to optimize the Rosenbrock function and use the Hessian and the Jacobian (which is equal to the gradient vector for scalar objective function) to the minimize function.\n\ndef rosenbrock(x):\n    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n\ndef rosenbrock_gradient(x):\n    dfdx0 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n    dfdx1 = 200 * (x[1] - x[0]**2)\n    return np.array([dfdx0, dfdx1])\n\ndef rosenbrock_hessian(x):\n    d2fdx0 = 1200 * x[0]**2 - 400 * x[1] + 2\n    d2fdx1 = -400 * x[0]\n    return np.array([[d2fdx0, d2fdx1], [d2fdx1, 200]])\n\ndef classical_newton_optimization_2d(initial_guess, tol=1e-6, max_iter=100):\n    x = initial_guess.copy()\n\n    for i in range(max_iter):\n        gradient = rosenbrock_gradient(x)\n        hessian = rosenbrock_hessian(x)\n\n        # Solve the linear system H * d = -g for d\n        d = np.linalg.solve(hessian, -gradient)\n\n        # Update x\n        x += d\n\n        # Check for convergence\n        if np.linalg.norm(gradient, ord=np.inf) &lt; tol:\n            break\n\n    return x\n\n# Initial guess\ninitial_guess_2d = np.array([0.0, 0.0])\n\n# Run classical Newton optimization for the 2D Rosenbrock function\nresult_2d = classical_newton_optimization_2d(initial_guess_2d)\n\n# Print the result\nprint(\"Optimal solution:\", result_2d)\nprint(\"Objective value:\", rosenbrock(result_2d))\n\nOptimal solution: [1. 1.]\nObjective value: 0.0\n\n\n\n\n\n3.2.8 BFGS-Algorithm\nBFGS is an optimization algorithm designed for unconstrained optimization problems. It belongs to the class of quasi-Newton methods and is known for its efficiency in finding the minimum of a smooth, unconstrained objective function.\n\n\n3.2.9 Procedure:\n\nInitialization:\n\nStart with an initial guess for the parameters of the objective function.\nInitialize an approximation of the Hessian matrix (inverse) denoted by \\(H\\).\n\n\nIterative Update:\n\nAt each iteration, compute the gradient vector at the current point.\nUpdate the parameters using the BFGS update formula, which involves the inverse Hessian matrix approximation, the gradient, and the difference in parameter vectors between successive iterations: \\[x_{k+1} = x_k - H_k^{-1} \\nabla f(x_k).\\]\nUpdate the inverse Hessian approximation using the BFGS update formula for the inverse Hessian. \\[H_{k+1} = H_k + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta x_k^T \\Delta g_k} - \\frac{H_k g_k g_k^T H_k}{g_k^T H_k g_k},\\] where:\n\\(x_k\\) and \\(x_{k+1}\\) are the parameter vectors at the current and updated iterations, respectively.\n\\(\\nabla f(x_k)\\) is the gradient vector at the current iteration.\n\\(\\Delta x_k = x_{k+1} - x_k\\) is the change in parameter vectors.\n\\(\\Delta g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\) is the change in gradient vectors.\n\nConvergence:\n\nRepeat the iterative update until the optimization converges. Convergence is typically determined by reaching a sufficiently low gradient or parameter change.\n\n\n\nExample 3.7 (BFGS for Rosenbrock)  \n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the 2D Rosenbrock function\ndef rosenbrock(x):\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n\n# Initial guess\ninitial_guess = np.array([0.0, 0.0])\n\n# Minimize the Rosenbrock function using BFGS\nminimize(rosenbrock, initial_guess, method='BFGS')\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 2.8440052847381483e-11\n        x: [ 1.000e+00  1.000e+00]\n      nit: 19\n      jac: [ 3.987e-06 -2.844e-06]\n hess_inv: [[ 4.948e-01  9.896e-01]\n            [ 9.896e-01  1.984e+00]]\n     nfev: 72\n     njev: 24\n\n\n\n\n\n3.2.10 Visualization BFGS for Rosenbrock\nA visualization of the BFGS search process on Rosenbrock’s function can be found here: https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#global-optimization",
    "href": "003_scipy_optimize_intro.html#global-optimization",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.3 Global Optimization",
    "text": "3.3 Global Optimization\nGlobal optimization aims to find the global minimum of a function within given bounds, in the presence of potentially many local minima. Typically, global minimizers efficiently search the parameter space, while using a local minimizer (e.g., minimize) under the hood.\n\n3.3.1 Local vs Global Optimization\n\n3.3.1.1 Local Optimizater:\n\nSeeks the optimum in a specific region of the search space\nTends to exploit the local environment, to find solutions in the immediate area\nHighly sensitive to initial conditions; may converge to different local optima based on the starting point\nOften computationally efficient for low-dimensional problems but may struggle with high-dimensional or complex search spaces\nCommonly used in situations where the objective is to refine and improve existing solutions\n\n\n\n3.3.1.2 Global Optimizer:\n\nExplores the entire search space to find the global optimum\nEmphasize exploration over exploitation, aiming to search broadly and avoid premature convergence to local optima\nAim to mitigate the risk of premature convergence to local optima by employing strategies for global exploration\nLess sensitive to initial conditions, designed to navigate diverse regions of the search space\nEquipped to handle high-dimensional and complex problems, though computational demands may vary depending on the specific algorithm\nPreferred for applications where a comprehensive search of the solution space is crucial, such as in parameter tuning, machine learning, and complex engineering design\n\n\n\nExample 3.8 (Global Optimizers in SciPy) SciPy contains a number of good global optimizers. Here, we’ll use those on the same objective function, namely the (aptly named) eggholder function:\n\ndef eggholder(x):\n    return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1]  + 47))))\n            -x[0] * np.sin(np.sqrt(abs(x[0] - (x[1]  + 47)))))\n\nbounds = [(-512, 512), (-512, 512)]\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.arange(-512, 513)\ny = np.arange(-512, 513)\nxgrid, ygrid = np.meshgrid(x, y)\nxy = np.stack([xgrid, ygrid])\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.view_init(45, -45)\nax.plot_surface(xgrid, ygrid, eggholder(xy), cmap='terrain')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('eggholder(x, y)')\nplt.show()\n\n\n\n\n\n\n\n\nWe now use the global optimizers to obtain the minimum and the function value at the minimum. We’ll store the results in a dictionary so we can compare different optimization results later.\n\nfrom scipy import optimize\nresults = dict()\nresults['shgo'] = optimize.shgo(eggholder, bounds)\nresults['shgo']\n\n message: Optimization terminated successfully.\n success: True\n     fun: -935.3379515605789\n    funl: [-9.353e+02]\n       x: [ 4.395e+02  4.540e+02]\n      xl: [[ 4.395e+02  4.540e+02]]\n     nit: 1\n    nfev: 45\n   nlfev: 40\n   nljev: 10\n   nlhev: 0\n\n\n\nresults['DA'] = optimize.dual_annealing(eggholder, bounds)\nresults['DA']\n\n message: ['Maximum number of iteration reached']\n success: True\n  status: 0\n     fun: -956.9182316246586\n       x: [ 4.824e+02  4.329e+02]\n     nit: 1000\n    nfev: 4127\n    njev: 42\n    nhev: 0\n\n\nAll optimizers return an OptimizeResult, which in addition to the solution contains information on the number of function evaluations, whether the optimization was successful, and more. For brevity, we won’t show the full output of the other optimizers:\n\nresults['DE'] = optimize.differential_evolution(eggholder, bounds)\nresults['DE']\n\n             message: Optimization terminated successfully.\n             success: True\n                 fun: -935.3379515604346\n                   x: [ 4.395e+02  4.540e+02]\n                 nit: 23\n                nfev: 741\n          population: [[ 4.396e+02  4.532e+02]\n                       [ 4.293e+02  4.482e+02]\n                       ...\n                       [ 4.412e+02  4.545e+02]\n                       [ 4.432e+02  4.594e+02]]\n population_energies: [-9.353e+02 -9.087e+02 ... -9.340e+02 -9.282e+02]\n                 jac: [-1.137e-05  3.411e-05]\n\n\nshgo has a second method, which returns all local minima rather than only what it thinks is the global minimum:\n\nresults['shgo_sobol'] = optimize.shgo(eggholder, bounds, n=200, iters=5,\n                                      sampling_method='sobol')\nresults['shgo_sobol']\n\n message: Optimization terminated successfully.\n success: True\n     fun: -959.640662720831\n    funl: [-9.596e+02 -9.353e+02 ... -6.591e+01 -6.387e+01]\n       x: [ 5.120e+02  4.042e+02]\n      xl: [[ 5.120e+02  4.042e+02]\n           [ 4.395e+02  4.540e+02]\n           ...\n           [ 3.165e+01 -8.523e+01]\n           [ 5.865e+01 -5.441e+01]]\n     nit: 5\n    nfev: 3529\n   nlfev: 2327\n   nljev: 634\n   nlhev: 0\n\n\nWe’ll now plot all found minima on a heatmap of the function:\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nim = ax.imshow(eggholder(xy), interpolation='bilinear', origin='lower',\n               cmap='gray')\nax.set_xlabel('x')\nax.set_ylabel('y')\n\ndef plot_point(res, marker='o', color=None):\n    ax.plot(512+res.x[0], 512+res.x[1], marker=marker, color=color, ms=10)\n\nplot_point(results['DE'], color='c')  # differential_evolution - cyan\nplot_point(results['DA'], color='w')  # dual_annealing.        - white\n\n# SHGO produces multiple minima, plot them all (with a smaller marker size)\nplot_point(results['shgo'], color='r', marker='+')\nplot_point(results['shgo_sobol'], color='r', marker='x')\nfor i in range(results['shgo_sobol'].xl.shape[0]):\n    ax.plot(512 + results['shgo_sobol'].xl[i, 0],\n            512 + results['shgo_sobol'].xl[i, 1],\n            'ro', ms=2)\n\nax.set_xlim([-4, 514*2])\nax.set_ylim([-4, 514*2])\nplt.show()\n\n\n\n\n\n\n\n\n\n3.3.2 Dual Annealing Optimization\nThis function implements the Dual-Annealing optimization, which is a variant of the famous simulated annealing optimization.\nSimulated Annealing is a probabilistic optimization algorithm inspired by the annealing process in metallurgy. The algorithm is designed to find a good or optimal global solution to a problem by exploring the solution space in a controlled and adaptive manner.\n\n\n\n\n\n\nAnnealing in Metallurgy\n\n\n\nSimulated Annealing draws inspiration from the physical process of annealing in metallurgy. Just as metals are gradually cooled to achieve a more stable state, Simulated Annealing uses a similar approach to explore solution spaces in the digital world.\n\n\nHeating Phase: In metallurgy, a metal is initially heated to a high temperature. At this elevated temperature, the atoms or molecules in the material become more energetic and chaotic, allowing the material to overcome energy barriers and defects.\nAnalogy Simulated Annealing (Exploration Phase): In Simulated Annealing, the algorithm starts with a high “temperature,” which encourages exploration of the solution space. At this stage, the algorithm is more likely to accept solutions that are worse than the current one, allowing it to escape local optima and explore a broader region of the solution space.\nCooling Phase: The material is then gradually cooled at a controlled rate. As the temperature decreases, the atoms or molecules start to settle into more ordered and stable arrangements. The slow cooling rate is crucial to avoid the formation of defects and to ensure the material reaches a well-organized state.\nAnalogy Simulated Annealing (Exploitation Phase): As the algorithm progresses, the temperature is gradually reduced over time according to a cooling schedule. This reduction simulates the cooling process in metallurgy. With lower temperatures, the algorithm becomes more selective and tends to accept only better solutions, focusing on refining and exploiting the promising regions discovered during the exploration phase.\n\n3.3.2.1 Key Concepts\nTemperature: The temperature is a parameter that controls the likelihood of accepting worse solutions. We start with a high temperature, allowing the algorithm to explore the solution space braodly. The temperature decreases with the iterations of the algorithm.\nCooling Schedule: The temperature parameter is reduced according to this schedule. The analogy to the annealing of metals: a slower cooling rate allows the material to reach a more stable state.\nNeighborhood Exploration: At each iteration, the algorithm explores the neighborhood of the current solution. The neighborhood is defined by small perturbations or changes to the current solution.\nAcceptance Probability: The algorithm evaluates the objective function for the new solution in the neighborhood. If the new solution is better, it is accepted. If the new solution is worse, it may still be accepted with a certain probability. This probability is determined by both the difference in objective function values and the current temperature.\nFor minimization: If: \\[\nf(x_{t}) &gt; f(x_{t+1})\n\\] Then: \\[\nP(accept\\_new\\_point) = 1\n\\]\nIf: \\[\nf(x_{t}) &lt; f(x_{t+1})\n\\] Then: \\[\nP(accept\\_new\\_point) = e^{-(\\frac{f(x_{t+1}) - f(x_{t})}{Tt})}\n\\]\nTermination Criterion: The algorithm continues iterations until a termination condition is met. This could be a fixed number of iterations, reaching a specific temperature threshold, or achieving a satisfactory solution.\n\n\n3.3.2.2 Steps\n1. Initialization: Set an initial temperature (\\(T_{0}\\)) and an initial solution (\\(f(x_{0})\\)). The temperature is typically set high initially to encourage exploration.\n2. Generate a Neighbor: Perturb the current solution to generate a neighboring solution. The perturbation can be random or follow a specific strategy.\n3. Evaluate the Neighbor: Evaluate the objective function for the new solution in the neighborhood.\n4. Accept or Reject the Neighbor: + If the new solution is better (lower cost for minimization problems or higher for maximization problems), accept it as the new current solution. + If the new solution is worse, accept it with a probability determined by an acceptance probability function as mentioned above. The probability is influenced by the difference in objective function values and the current temperature.\n5. Cooling: Reduce the temperature according to a cooling schedule. The cooling schedule defines how fast the temperature decreases over time. Common cooling schedules include exponential or linear decay.\n6. Termination Criterion: Repeat the iterations (2-5) until a termination condition is met. This could be a fixed number of iterations, reaching a specific temperature threshold, or achieving a satisfactory solution.\n\n\n\n3.3.2.3 Scipy Implementation of the Dual Annealing Algorithm\nIn Scipy, we utilize the Dual Annealing optimizer, an extension of the simulated annealing algorithm that is versatile for both discrete and continuous problems.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import dual_annealing\n\ndef rastrigin_function(x):\n    return 20 + x[0]**2 - 10 * np.cos(2 * np.pi * x[0]) + x[1]**2 - 10 * np.cos(2 * np.pi * x[1])\n\n# Define the Rastrigin function for visualization\ndef rastrigin_visualization(x, y):\n    return 20 + x**2 - 10 * np.cos(2 * np.pi * x) + y**2 - 10 * np.cos(2 * np.pi * y)\n\n# Create a meshgrid for visualization\nx_vals = np.linspace(-10, 10, 100)\ny_vals = np.linspace(-10, 10, 100)\nx_mesh, y_mesh = np.meshgrid(x_vals, y_vals)\nz_mesh = rastrigin_visualization(x_mesh, y_mesh)\n\n# Visualize the Rastrigin function\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(x_mesh, y_mesh, z_mesh, levels=50, cmap='viridis')\nplt.colorbar(contour, label='Rastrigin Function Value')\nplt.title('Visualization of the 2D Rastrigin Function')\n\n# Optimize the Rastrigin function using dual annealing\nresult = dual_annealing(func = rastrigin_function,\n                        x0=[5.0,3.0],                       #Initial Guess\n                        bounds= [(-10, 10), (-10, 10)],\n                        initial_temp = 5230,                #Intial Value for temperature\n                        restart_temp_ratio = 2e-05,         #Temperature schedule\n                        seed=42)\n\n# Plot the optimized point\noptimal_x, optimal_y = result.x\nplt.plot(optimal_x, optimal_y, 'ro', label='Optimal Point')\n\n# Set labels and legend\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n# Display the optimization result\nprint(\"Optimal parameters:\", result.x)\nprint(\"Minimum value of the Rastrigin function:\", result.fun)\n\n\n\n\n\n\n\n\nOptimal parameters: [-4.60133247e-09 -4.31928660e-09]\nMinimum value of the Rastrigin function: 7.105427357601002e-15\n\n\n\n\n3.3.3 Differential Evolution\nDifferential Evolution is an algorithm used for finding the global minimum of multivariate functions. It is stochastic in nature (does not use gradient methods), and can search large areas of candidate space, but often requires larger numbers of function evaluations than conventional gradient based techniques.\nDifferential Evolution (DE) is a versatile and global optimization algorithm inspired by natural selection and evolutionary processes. Introduced by Storn and Price in 1997, DE mimics the survival-of-the-fittest principle by evolving a population of candidate solutions through iterative mutation, crossover, and selection operations. This nature-inspired approach enables DE to efficiently explore complex and non-linear solution spaces, making it a widely adopted optimization technique in diverse fields such as engineering, finance, and machine learning.\n\n\n3.3.4 Procedure\nThe procedure boils down to the following steps:\n\nInitialization:\n\nCreate a population of candidate solutions randomly within the specified search space.\n\nMutation:\n\nFor each individual in the population, select three distinct individuals (vectors) randomly.\nGenerate a mutant vector V by combining these three vectors with a scaling factor.\n\nCrossover:\n\nPerform the crossover operation between the target vector U and the mutant vector V. Information from both vectors is used to create a trial vector U´\n\n\n\n\n\n\n\n\nCross-Over Strategies in DE\n\n\n\n\nThere are several crossover strategies in the literature. Two examples are:\n\nBinominal Crossover:\nIn this strategy, each component of the trial vector is selected from the mutant vector with a probability equal to the crossover rate (\\(CR\\)). This means that each element of the trial vector has an independent probability of being replaced by the corresponding element of the mutant vector.\n\\[U'_i =\n      \\begin{cases}\n      V_i, & \\text{if a random number} \\ \\sim U(0, 1) \\leq CR \\ \\text{(Crossover Rate)} \\\\\n      U_i, & \\text{otherwise}\n      \\end{cases}\n\\]\nExponential Crossover:\nIn exponential crossover, the trial vector is constructed by selecting a random starting point and copying elements from the mutant vector with a certain probability. The probability decreases exponentially with the distance from the starting point. This strategy introduces a correlation between neighboring elements in the trial vector.\n\n\n\nSelection:\n\nEvaluate the fitness of the trial vector obtained from the crossover.\nReplace the target vector with the trial vector if its fitness is better.\n\nTermination:\n\nRepeat the mutation, crossover, and selection steps for a predefined number of generations or until convergence criteria are met.\n\nResult:\n\nThe algorithm returns the best-found solution after the specified number of iterations.\n\n\nThe key parameters in DE include the population size, crossover probability, and the scaling factor. Tweak these parameters based on the characteristics of the optimization problem for optimal performance.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n# Define the Rastrigin function\ndef rastrigin(x):\n    A = 10\n    return A * len(x) + sum([(xi**2 - A * np.cos(2 * np.pi * xi)) for xi in x])\n\n# Create a grid for visualization\nx_vals = np.linspace(-5.12, 5.12, 100)\ny_vals = np.linspace(-5.12, 5.12, 100)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = rastrigin(np.vstack([X.ravel(), Y.ravel()]))\n\n# Reshape Z to match the shape of X and Y\nZ = Z.reshape(X.shape)\n\n# Plot the Rastrigin function\nplt.contour(X, Y, Z, levels=50, cmap='viridis', label='Rastrigin Function')\n\n# Initial guess (starting point for the optimization)\ninitial_guess = (4,3,4,2)\n\n# Define the bounds for each variable in the Rastrigin function\nbounds = [(-5.12, 5.12)] * 4  # 4D problem, each variable has bounds (-5.12, 5.12)\n\n# Run the minimize function\nresult = minimize(rastrigin, initial_guess, bounds=bounds, method='L-BFGS-B')\n\n# Extract the optimal solution\noptimal_solution = result.x\n\n# Plot the optimal solution\nplt.scatter(optimal_solution[0], optimal_solution[1], color='red', marker='x', label='Optimal Solution')\n\n# Add labels and legend\nplt.title('Optimization of Rastrigin Function with Minimize')\nplt.xlabel('Variable 1')\nplt.ylabel('Variable 2')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n# Print the optimization result\nprint(\"Optimal Solution:\", optimal_solution)\nprint(\"Optimal Objective Value:\", result.fun)\n\n\n\n\n\n\n\n\nOptimal Solution: [-2.52869119e-08 -2.07795060e-08 -2.52869119e-08 -1.62721002e-08]\nOptimal Objective Value: 3.907985046680551e-13\n\n\n\n\n3.3.5 Other global optimization algorithms\n\n\n3.3.6 DIRECT\nDIviding RECTangles (DIRECT) is a deterministic global optimization algorithm capable of minimizing a black box function with its variables subject to lower and upper bound constraints by sampling potential solutions in the search space\n\n\n3.3.7 SHGO\nSHGO stands for “simplicial homology global optimization”. It is considered appropriate for solving general purpose NLP and blackbox optimization problems to global optimality (low-dimensional problems).\n\n\n3.3.8 Basin-hopping\nBasin-hopping is a two-phase method that combines a global stepping algorithm with local minimization at each step. Designed to mimic the natural process of energy minimization of clusters of atoms, it works well for similar problems with “funnel-like, but rugged” energy landscapes\n\n\n3.4 Project: One-Mass Oscillator Optimization\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n\n3.4.1 Introduction\nIn this project, you will apply various optimization algorithms to fit a one-mass oscillator model to real-world data. The objective is to minimize the sum of the squared residuals between the model predictions and the observed amplitudes of a one-mass oscillator system across different frequencies.\n\n\n3.4.2 One-Mass Oscillator Model\nThe one-mass oscillator is characterized by the following equation, representing the amplitudes of the system:\n\\[\nV(\\omega) = \\frac{F}{\\sqrt{(1 - \\nu^2)^2 + 4D^2\\nu^2}}\n\\]\nHere, \\(\\omega\\) represents the angular frequency of the system, \\(\\nu\\) is the ratio of the excitation frequency to the natural frequency, i.e., \\[\n\\nu = \\frac{\\omega_{\\text{err}}}{\\omega_{\\text{eig}}},\n\\] \\(D\\) is the damping ratio, and \\(F\\) is the force applied to the system.\nThe goal of the project is to determine the optimal values for the parameters \\(\\omega_{\\text{eig}}\\), \\(D\\), and \\(F\\) that result in the best fit of the one-mass oscillator model to the observed amplitudes.\n\n\n3.4.3 The Real-World Data\nThere are two different measurements. J represents the measured frequencies, and N represents the measured amplitudes.\n\ndf1 = pd.read_pickle(\"./data/Hcf.d/df1.pkl\")\ndf2 = pd.read_pickle(\"./data/Hcf.d/df2.pkl\")\ndf1.describe()\n\n\n\n\n\n\n\n\nJ\nN\n\n\n\n\ncount\n33.000000\n33.000000\n\n\nmean\n8148.750252\n10.430887\n\n\nstd\n6.870023\n2.846469\n\n\nmin\n8137.649210\n4.698761\n\n\n25%\n8143.799766\n8.319253\n\n\n50%\n8146.942295\n10.152119\n\n\n75%\n8153.934051\n13.407260\n\n\nmax\n8162.504002\n14.382749\n\n\n\n\n\n\n\n\ndf1.head()\n\n\n\n\n\n\n\n\nJ\nN\n\n\n\n\n14999\n8162.504002\n5.527511\n\n\n15011\n8156.384831\n7.359789\n\n\n15016\n8159.199238\n6.532958\n\n\n15020\n8159.200889\n5.895933\n\n\n15025\n8153.934051\n9.326749\n\n\n\n\n\n\n\n\n# plot the data, i.e., the measured amplitudes as a function of the measured frequencies\nplt.scatter(df1[\"J\"], df1[\"N\"], color=\"black\", label=\"Spektralpunkte\", zorder=5, s=10)\nplt.xlabel(\"Frequency [Hz]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\n\n\n\n\nNote: Low amplitudes distort the fit and are negligible therefore we define a lower threshold for N.\n\nthreshold = 0.4\ndf1.sort_values(\"N\")\nmax_N = max(df1[\"N\"])\ndf1 = df1[df1[\"N\"]&gt;=threshold*max_N]\n\nWe extract the frequency value for maximum value of the amplitude. This serves as the initial value for one decision variable.\n\ndf_max=df1[df1[\"N\"]==max(df1[\"N\"])]\ninitial_Oeig = df_max[\"J\"].values[0]\nmax_N = df_max[\"N\"].values[0]\n\nWe also have to define the other two initial guesses for the damping ratio and the force, e.g.,\n\ninitial_D = 0.006\ninitial_F = 0.120\ninitial_values = [initial_Oeig, initial_D, initial_F]\n\nAdditionally, we define the bounds for the decision variables:\n\nmin_Oerr = min(df1[\"J\"])\nmax_Oerr = max(df1[\"J\"])\n\n\nbounds = [(min_Oerr, max_Oerr), (0, 0.03), (0, 1)]\n\n\n\n3.4.4 Objective Function\nThen we define the objective function:\n\ndef one_mass_oscillator(params, Oerr) -&gt; np.ndarray:\n    # returns amplitudes of the system\n    # Defines the model of a one mass oscilator \n    Oeig, D, F = params\n    nue = Oerr / Oeig\n    V = F / (np.sqrt((1 - nue**2) ** 2 + (4 * D**2 * nue**2)))\n    return V\n\n\ndef objective_function(params, Oerr, amplitudes) -&gt; np.ndarray:\n    # objective function to compare calculated and real amplitudes\n    return np.sum((amplitudes - one_mass_oscillator(params, Oerr)) ** 2)\n\nWe define the options for the optimzer and start the optimization process:\n\noptions = {\n    \"maxfun\": 100000,\n    \"ftol\": 1e-9,\n    \"xtol\": 1e-9,\n    \"stepmx\": 10,\n    \"eta\": 0.25,\n    \"gtol\": 1e-5}\n\n\nJ = np.array(df1[\"J\"]) # measured frequency\nN = np.array(df1[\"N\"]) # measured amplitude\n\n\nresult = minimize(\n    objective_function,\n    initial_values,\n    args=(J, N),\n    method='Nelder-Mead',\n    bounds=bounds,\n    options=options)\n\n\n\n3.4.5 Results\nWe can observe the results:\n\n# map optimized values to variables\nresonant_frequency = result.x[0]\nD = result.x[1]\nF = result.x[2]\n# predict the resonant amplitude with the fitted one mass oscillator.\nX_pred = np.linspace(min_Oerr, max_Oerr, 1000)\nypred_one_mass_oscillator = one_mass_oscillator(result.x, X_pred)\nresonant_amplitude = max(ypred_one_mass_oscillator)\nprint(f\"result: {result}\")\n\nresult:        message: Optimization terminated successfully.\n       success: True\n        status: 0\n           fun: 53.54144061205875\n             x: [ 8.148e+03  7.435e-04  2.153e-02]\n           nit: 93\n          nfev: 169\n final_simplex: (array([[ 8.148e+03,  7.435e-04,  2.153e-02],\n                       [ 8.148e+03,  7.435e-04,  2.153e-02],\n                       [ 8.148e+03,  7.435e-04,  2.153e-02],\n                       [ 8.148e+03,  7.435e-04,  2.153e-02]]), array([ 5.354e+01,  5.354e+01,  5.354e+01,  5.354e+01]))\n\n\nFinally, we can plot the optimized fit and the real values:\n\nplt.scatter(\n    df1[\"J\"],\n    df1[\"N\"],\n    color=\"black\",\n    label=\"Spektralpunkte filtered\",\n    zorder=5,\n    s=10,\n)\n# color the max amplitude point red\nplt.scatter(\n    initial_Oeig,\n    max_N,\n    color=\"red\",\n    label=\"Max Amplitude\",\n    zorder=5,\n    s=10,\n)\n\nplt.plot(\n        X_pred,\n        ypred_one_mass_oscillator,\n        label=\"Alpha\",\n        color=\"blue\",\n        linewidth=1,\n    )\nplt.scatter(\n    resonant_frequency,\n    resonant_amplitude,\n    color=\"blue\",\n    label=\"Max Curve Fit\",\n    zorder=10,\n    s=20,\n)\n\n\n\n\n\n\n\n\n\n\n3.4.6 Exercises:\n\n3.4.6.1 Example Nelder-Mead:\n\nWhat are the steps of the Nelder-Mead algorithm?\nWhat are the advantages and disadvantages of the Nelder-Mead algorithm?\n\n\n\n3.4.6.2 Example Powell’s Method:\n\nWhat are the steps of Powell’s method?\nWhat are the advantages and disadvantages of Powell’s method?\nWhat are similarities between the Nelder-Mead and Powell’s methods?\n\n\n\n3.4.6.3 Example Gradient Descent:\n\nWhat are the steps of the gradient descent algorithm?\nWhat is the learning rate in the gradient descent algorithm?\n\n\n\n3.4.6.4 Example Newton Method:\n\nWhat is the difference between the gradient descent and Newton method?\nWhich of the two methods converges faster?\n\n\n\n3.4.6.5 Example BFGS:\n\nIn which situations is it possible to use algorithms like BFGS, but not the classical Newton method?\nWould you choose Gradient Descent or BFGS for a large-scale optimization problem?\n\n\n\n3.4.6.6 Example Dual Annealing:\n\nWhen should you use Simulated Annealing or Dual Annealing over a local optimization algorithm?\nDescribe the Temperature parameter in Simulated Annealing.\n\n\n\n3.4.6.7 Excample Differential Evolution:\n\nWhat are the key steps in the Differential Evolution algorithm?\nExplain the crossover operation in Differential Evolution.\n\n\n\n\n\n3.5 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#project-one-mass-oscillator-optimization",
    "href": "003_scipy_optimize_intro.html#project-one-mass-oscillator-optimization",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.4 Project: One-Mass Oscillator Optimization",
    "text": "3.4 Project: One-Mass Oscillator Optimization\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n\n3.4.1 Introduction\nIn this project, you will apply various optimization algorithms to fit a one-mass oscillator model to real-world data. The objective is to minimize the sum of the squared residuals between the model predictions and the observed amplitudes of a one-mass oscillator system across different frequencies.\n\n\n3.4.2 One-Mass Oscillator Model\nThe one-mass oscillator is characterized by the following equation, representing the amplitudes of the system:\n\\[\nV(\\omega) = \\frac{F}{\\sqrt{(1 - \\nu^2)^2 + 4D^2\\nu^2}}\n\\]\nHere, \\(\\omega\\) represents the angular frequency of the system, \\(\\nu\\) is the ratio of the excitation frequency to the natural frequency, i.e., \\[\n\\nu = \\frac{\\omega_{\\text{err}}}{\\omega_{\\text{eig}}},\n\\] \\(D\\) is the damping ratio, and \\(F\\) is the force applied to the system.\nThe goal of the project is to determine the optimal values for the parameters \\(\\omega_{\\text{eig}}\\), \\(D\\), and \\(F\\) that result in the best fit of the one-mass oscillator model to the observed amplitudes.\n\n\n3.4.3 The Real-World Data\nThere are two different measurements. J represents the measured frequencies, and N represents the measured amplitudes.\n\ndf1 = pd.read_pickle(\"./data/Hcf.d/df1.pkl\")\ndf2 = pd.read_pickle(\"./data/Hcf.d/df2.pkl\")\ndf1.describe()\n\n\n\n\n\n\n\n\nJ\nN\n\n\n\n\ncount\n33.000000\n33.000000\n\n\nmean\n8148.750252\n10.430887\n\n\nstd\n6.870023\n2.846469\n\n\nmin\n8137.649210\n4.698761\n\n\n25%\n8143.799766\n8.319253\n\n\n50%\n8146.942295\n10.152119\n\n\n75%\n8153.934051\n13.407260\n\n\nmax\n8162.504002\n14.382749\n\n\n\n\n\n\n\n\ndf1.head()\n\n\n\n\n\n\n\n\nJ\nN\n\n\n\n\n14999\n8162.504002\n5.527511\n\n\n15011\n8156.384831\n7.359789\n\n\n15016\n8159.199238\n6.532958\n\n\n15020\n8159.200889\n5.895933\n\n\n15025\n8153.934051\n9.326749\n\n\n\n\n\n\n\n\n# plot the data, i.e., the measured amplitudes as a function of the measured frequencies\nplt.scatter(df1[\"J\"], df1[\"N\"], color=\"black\", label=\"Spektralpunkte\", zorder=5, s=10)\nplt.xlabel(\"Frequency [Hz]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\n\n\n\n\nNote: Low amplitudes distort the fit and are negligible therefore we define a lower threshold for N.\n\nthreshold = 0.4\ndf1.sort_values(\"N\")\nmax_N = max(df1[\"N\"])\ndf1 = df1[df1[\"N\"]&gt;=threshold*max_N]\n\nWe extract the frequency value for maximum value of the amplitude. This serves as the initial value for one decision variable.\n\ndf_max=df1[df1[\"N\"]==max(df1[\"N\"])]\ninitial_Oeig = df_max[\"J\"].values[0]\nmax_N = df_max[\"N\"].values[0]\n\nWe also have to define the other two initial guesses for the damping ratio and the force, e.g.,\n\ninitial_D = 0.006\ninitial_F = 0.120\ninitial_values = [initial_Oeig, initial_D, initial_F]\n\nAdditionally, we define the bounds for the decision variables:\n\nmin_Oerr = min(df1[\"J\"])\nmax_Oerr = max(df1[\"J\"])\n\n\nbounds = [(min_Oerr, max_Oerr), (0, 0.03), (0, 1)]\n\n\n\n3.4.4 Objective Function\nThen we define the objective function:\n\ndef one_mass_oscillator(params, Oerr) -&gt; np.ndarray:\n    # returns amplitudes of the system\n    # Defines the model of a one mass oscilator \n    Oeig, D, F = params\n    nue = Oerr / Oeig\n    V = F / (np.sqrt((1 - nue**2) ** 2 + (4 * D**2 * nue**2)))\n    return V\n\n\ndef objective_function(params, Oerr, amplitudes) -&gt; np.ndarray:\n    # objective function to compare calculated and real amplitudes\n    return np.sum((amplitudes - one_mass_oscillator(params, Oerr)) ** 2)\n\nWe define the options for the optimzer and start the optimization process:\n\noptions = {\n    \"maxfun\": 100000,\n    \"ftol\": 1e-9,\n    \"xtol\": 1e-9,\n    \"stepmx\": 10,\n    \"eta\": 0.25,\n    \"gtol\": 1e-5}\n\n\nJ = np.array(df1[\"J\"]) # measured frequency\nN = np.array(df1[\"N\"]) # measured amplitude\n\n\nresult = minimize(\n    objective_function,\n    initial_values,\n    args=(J, N),\n    method='Nelder-Mead',\n    bounds=bounds,\n    options=options)\n\n\n\n3.4.5 Results\nWe can observe the results:\n\n# map optimized values to variables\nresonant_frequency = result.x[0]\nD = result.x[1]\nF = result.x[2]\n# predict the resonant amplitude with the fitted one mass oscillator.\nX_pred = np.linspace(min_Oerr, max_Oerr, 1000)\nypred_one_mass_oscillator = one_mass_oscillator(result.x, X_pred)\nresonant_amplitude = max(ypred_one_mass_oscillator)\nprint(f\"result: {result}\")\n\nresult:        message: Optimization terminated successfully.\n       success: True\n        status: 0\n           fun: 53.54144061205875\n             x: [ 8.148e+03  7.435e-04  2.153e-02]\n           nit: 93\n          nfev: 169\n final_simplex: (array([[ 8.148e+03,  7.435e-04,  2.153e-02],\n                       [ 8.148e+03,  7.435e-04,  2.153e-02],\n                       [ 8.148e+03,  7.435e-04,  2.153e-02],\n                       [ 8.148e+03,  7.435e-04,  2.153e-02]]), array([ 5.354e+01,  5.354e+01,  5.354e+01,  5.354e+01]))\n\n\nFinally, we can plot the optimized fit and the real values:\n\nplt.scatter(\n    df1[\"J\"],\n    df1[\"N\"],\n    color=\"black\",\n    label=\"Spektralpunkte filtered\",\n    zorder=5,\n    s=10,\n)\n# color the max amplitude point red\nplt.scatter(\n    initial_Oeig,\n    max_N,\n    color=\"red\",\n    label=\"Max Amplitude\",\n    zorder=5,\n    s=10,\n)\n\nplt.plot(\n        X_pred,\n        ypred_one_mass_oscillator,\n        label=\"Alpha\",\n        color=\"blue\",\n        linewidth=1,\n    )\nplt.scatter(\n    resonant_frequency,\n    resonant_amplitude,\n    color=\"blue\",\n    label=\"Max Curve Fit\",\n    zorder=10,\n    s=20,\n)\n\n\n\n\n\n\n\n\n\n\n3.4.6 Exercises:\n\n3.4.6.1 Example Nelder-Mead:\n\nWhat are the steps of the Nelder-Mead algorithm?\nWhat are the advantages and disadvantages of the Nelder-Mead algorithm?\n\n\n\n3.4.6.2 Example Powell’s Method:\n\nWhat are the steps of Powell’s method?\nWhat are the advantages and disadvantages of Powell’s method?\nWhat are similarities between the Nelder-Mead and Powell’s methods?\n\n\n\n3.4.6.3 Example Gradient Descent:\n\nWhat are the steps of the gradient descent algorithm?\nWhat is the learning rate in the gradient descent algorithm?\n\n\n\n3.4.6.4 Example Newton Method:\n\nWhat is the difference between the gradient descent and Newton method?\nWhich of the two methods converges faster?\n\n\n\n3.4.6.5 Example BFGS:\n\nIn which situations is it possible to use algorithms like BFGS, but not the classical Newton method?\nWould you choose Gradient Descent or BFGS for a large-scale optimization problem?\n\n\n\n3.4.6.6 Example Dual Annealing:\n\nWhen should you use Simulated Annealing or Dual Annealing over a local optimization algorithm?\nDescribe the Temperature parameter in Simulated Annealing.\n\n\n\n3.4.6.7 Excample Differential Evolution:\n\nWhat are the key steps in the Differential Evolution algorithm?\nExplain the crossover operation in Differential Evolution.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#jupyter-notebook",
    "href": "003_scipy_optimize_intro.html#jupyter-notebook",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.5 Jupyter Notebook",
    "text": "3.5 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  }
]