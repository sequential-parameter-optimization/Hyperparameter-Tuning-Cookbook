[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "",
    "text": "Preface: Optimization and Hyperparameter Tuning\nThe goal of hyperparameter tuning is to optimize the hyperparameters in a way that improves the performance of the machine learning or deep learning model. Hyperparameters are parameters that are not learned during the training process, but are set before the training process begins. Hyperparameter tuning is an important, but often difficult and computationally intensive task. Changing the architecture of a neural network or the learning rate of an optimizer can have a significant impact on the performance.\nHyperparameter tuning is referred to as “hyperparameter optimization” (HPO) in the literature. However, since we do not consider the optimization, but also the understanding of the hyperparameters, we use the term “hyperparameter tuning” in this book. See also the discussion in Chapter 2 of Bartz et al. (2022), which lays the groundwork and presents an introduction to the process of tuning Machine Learning and Deep Learning hyperparameters and the respective methodology. Since the key elements such as the hyperparameter tuning process and measures of tunability and performance are presented in Bartz et al. (2022), we refer to this chapter for details.\nThe simplest, but also most computationally expensive, hyperparameter tuning approach uses manual search (or trial-and-error (Meignan et al. 2015)). Commonly encountered is simple random search, i.e., random and repeated selection of hyperparameters for evaluation, and lattice search (“grid search”). In addition, methods that perform directed search and other model-free algorithms, i.e., algorithms that do not explicitly rely on a model, e.g., evolution strategies (Bartz-Beielstein et al. 2014) or pattern search (Lewis, Torczon, and Trosset 2000) play an important role. Also, “hyperband”, i.e., a multi-armed bandit strategy that dynamically allocates resources to a set of random configurations and uses successive bisections to stop configurations with poor performance (Li et al. 2016), is very common in hyperparameter tuning. The most sophisticated and efficient approaches are the Bayesian optimization and surrogate model based optimization methods, which are based on the optimization of cost functions determined by simulations or experiments.\nWe consider a surrogate optimization based hyperparameter tuning approach that uses the Python version of the SPOT (“Sequential Parameter Optimization Toolbox”) (Bartz-Beielstein, Lasarczyk, and Preuss 2005), which is suitable for situations where only limited resources are available. This may be due to limited availability and cost of hardware, or due to the fact that confidential data may only be processed locally, e.g., due to legal requirements. Furthermore, in our approach, the understanding of algorithms is seen as a key tool for enabling transparency and explainability. This can be enabled, for example, by quantifying the contribution of machine learning and deep learning components (nodes, layers, split decisions, activation functions, etc.). Understanding the importance of hyperparameters and the interactions between multiple hyperparameters plays a major role in the interpretability and explainability of machine learning models. SPOT provides statistical tools for understanding hyperparameters and their interactions. Last but not least, it should be noted that the SPOT software code is available in the open source spotPython package on github1, allowing replicability of the results. This tutorial describes the Python variant of SPOT, which is called spotPython. The R implementation is described in Bartz et al. (2022). SPOT is an established open source software that has been maintained for more than 15 years (Bartz-Beielstein, Lasarczyk, and Preuss 2005) (Bartz et al. 2022)."
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "Book Structure",
    "text": "Book Structure\nThis document is structured in two parts. The first part describes the surrogate model based optimization process and the second part describes the hyperparameter tuning.\nThe first part is structured as follows: The concept of the hyperparameter tuning software spotPython is described in Chapter 1. This introduction is based on one-dimensional examples. Higher-dimensional examples are presented in Chapter 2. Chapter 3 describes isotropic and anisotorpic kriging. How different surrogate models from scikit-learn can be used as surrogates in spotPython optimization runs is explained in Chapter 4. Chapter 5 describes how different optimizers from the scipy optimize package can be used on the surrogate. The differences between the Kriging implementation in spotPython and the GaussianProcessRegressor in scikit-learn are explained in Chapter 6. Chapter 7 describes the expected improvement approach. How noisy functions can be handled is described in Chapter 8. Chapter 9 demonstrates how noisy functions can be handled with Optimal Computational Budget Allocation (OCBA) by Spot.\nThe second part is structured as follows: Chapter 10 describes the hyperparameter tuning of a support vector classifier from scikit-learn [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVCriver python] with spotPython. Chapter 11 illustrates the hyperparameter tuning of a Hoeffding Adaptive Tree Regressor from river [https://riverml.xyz/0.18.0/api/tree/HoeffdingAdaptiveTreeRegressor/] with spotPython.\nChapter 12 describes the execution of the example from the tutorial “Hyperparameter Tuning with Ray Tune” (PyTorch 2023). The integration of spotPython into the PyTorch training workflow is described in detail in the following sections. Section 12.1 describes the setup of the tuners. Section 12.3 describes the data loading. Section 12.5 describes the model to be tuned. The search space is introduced in Section 12.5.3. Optimizers are presented in Section 12.6.1. How to split the data in train, validation, and test sets is described in Section 12.7.1. The selection of the loss function and metrics is described in Section 12.7.5. Section 12.8.1 describes the preparation of the spotPython call. The objective function is described in Section 12.8.2. How to use results from previous runs and default hyperparameter configurations is described in Section 12.8.3. Starting the tuner is shown in Section 12.8.4. TensorBoard can be used to visualize the results as shown in Section 12.9. Results are discussed and explained in Section 12.10. Section 12.11 presents a summary and an outlook for the execution of the example from the tutorial “Hyperparameter Tuning with Ray Tune”.\nFour more examples are presented in the following sections: Chapter 13 describes the hyperparameter tuning of a random forest classifier from scikit-learn with spotPython. Chapter 14 describes the hyperparameter tuning of an XGBoost classifier from scikit-learn with spotPython. Chapter 15 describes the hyperparameter tuning of a support vector classifier from scikit-learn with spotPython. Chapter 16 describes the hyperparameter tuning of a k-nearest neighbors classifier from scikit-learn with spotPython.\nThis part of the book is concluded with a description of the most recent PyTorch hyperparameter tuning approach, which is the integration of spotPython into the PyTorch Lightning training workflow. This is described in Chapter 17. This is considered as the most effective, efficient, and flexible way to integrate spotPython into the PyTorch training workflow.\n\n\n\n\n\n\nHyperparameter Tuning Reference\n\n\n\n\nThe open access book Bartz et al. (2022) provides a comprehensive overview of hyperparameter tuning. It can be downloaded from https://link.springer.com/book/10.1007/978-981-19-5170-1.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe .ipynb notebook (Bartz-Beielstein 2023) is updated regularly and reflects updates and changes in the spotPython package. It can be downloaded from https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb."
  },
  {
    "objectID": "index.html#software-used-in-this-book",
    "href": "index.html#software-used-in-this-book",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "Software Used in this Book",
    "text": "Software Used in this Book\nspotPython (“Sequential Parameter Optimization Toolbox in Python”) is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. The related open-access book is available here: Hyperparameter Tuning for Machine and Deep Learning with R—A Practical Guide.\nscikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license. The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed.\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs. Lightning is a lightweight PyTorch wrapper for high-performance AI research. It allows you to decouple the research from the engineering.\nRiver is a Python library for online machine learning. It is designed to be used in real-world environments, where not all data is available at once, but streaming in.\nspotRiver provides an interface between spotPython and River.\n\n\n\n\n\nBartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide. Springer.\n\n\nBartz-Beielstein, Thomas. 2023. “PyTorch Hyperparameter Tuning with SPOT: Comparison with Ray Tuner and Default Hyperparameters on CIFAR10.” https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.\n\n\nBartz-Beielstein, Thomas, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann. 2014. “Evolutionary Algorithms.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 4 (3): 178–95.\n\n\nBartz-Beielstein, Thomas, Christian Lasarczyk, and Mike Preuss. 2005. “Sequential Parameter Optimization.” In Proceedings 2005 Congress on Evolutionary Computation (CEC’05), Edinburgh, Scotland, edited by B McKay et al., 773–80. Piscataway NJ: IEEE Press.\n\n\nLewis, R M, V Torczon, and M W Trosset. 2000. “Direct search methods: Then and now.” Journal of Computational and Applied Mathematics 124 (1–2): 191–207.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2016. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” arXiv e-Prints, March, arXiv:1603.06560.\n\n\nMeignan, David, Sigrid Knust, Jean-Marc Frayet, Gilles Pesant, and Nicolas Gaud. 2015. “A Review and Taxonomy of Interactive Optimization Methods in Operations Research.” ACM Transactions on Interactive Intelligent Systems, September.\n\n\nPyTorch. 2023. “Hyperparameter Tuning with Ray Tune.” https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "",
    "text": "https://github.com/sequential-parameter-optimization↩︎"
  }
]