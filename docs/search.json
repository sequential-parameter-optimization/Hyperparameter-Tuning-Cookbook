[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "Book Structure",
    "text": "Book Structure\nThis document is structured in three parts. The first part presents an introduction to optimization. The second part describes numerical methods, and the third part presents hyperparameter tuning.\n\n\n\n\n\n\nHyperparameter Tuning Reference\n\n\n\n\nThe open access book Bartz et al. (2022) provides a comprehensive overview of hyperparameter tuning. It can be downloaded from https://link.springer.com/book/10.1007/978-981-19-5170-1.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe .ipynb notebook (Bartz-Beielstein 2023) is updated regularly and reflects updates and changes in the spotPython package. It can be downloaded from https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#software-used-in-this-book",
    "href": "index.html#software-used-in-this-book",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "Software Used in this Book",
    "text": "Software Used in this Book\nscikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license. The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed.\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs. Lightning is a lightweight PyTorch wrapper for high-performance AI research. It allows you to decouple the research from the engineering.\nRiver is a Python library for online machine learning. It is designed to be used in real-world environments, where not all data is available at once, but streaming in.\nspotPython (“Sequential Parameter Optimization Toolbox in Python”) is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. The related open-access book is available here: Hyperparameter Tuning for Machine and Deep Learning with R—A Practical Guide.\nspotRiver provides an interface between spotPython and River.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "Citation",
    "text": "Citation\nIf this document has been useful to you and you wish to cite it in a scientific publication, please refer to the following paper, which can be found on arXiv: https://arxiv.org/abs/2307.10262.\n@ARTICLE{bart23iArXiv,\n      author = {{Bartz-Beielstein}, Thomas},\n      title = \"{Hyperparameter Tuning Cookbook:\n          A guide for scikit-learn, PyTorch, river, and spotPython}\",\n     journal = {arXiv e-prints},\n    keywords = {Computer Science - Machine Learning,\n      Computer Science - Artificial Intelligence, 90C26, I.2.6, G.1.6},\n         year = 2023,\n        month = jul,\n          eid = {arXiv:2307.10262},\n        pages = {arXiv:2307.10262},\n          doi = {10.48550/arXiv.2307.10262},\narchivePrefix = {arXiv},\n       eprint = {2307.10262},\n primaryClass = {cs.LG},\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230710262B},\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\n\n\n\n\n\n\nBartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide. Springer.\n\n\nBartz-Beielstein, Thomas. 2023. “PyTorch Hyperparameter Tuning with SPOT: Comparison with Ray Tuner and Default Hyperparameters on CIFAR10.” https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "001_optimization_surrogate.html",
    "href": "001_optimization_surrogate.html",
    "title": "1  Introduction: Optimization",
    "section": "",
    "text": "1.1 Optimization, Simulation, and Surrogate Modeling",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Optimization</span>"
    ]
  },
  {
    "objectID": "001_optimization_surrogate.html#optimization-simulation-and-surrogate-modeling",
    "href": "001_optimization_surrogate.html#optimization-simulation-and-surrogate-modeling",
    "title": "1  Introduction: Optimization",
    "section": "",
    "text": "We will consider the interplay between\n\nmathematical models,\nnumerical approximation,\nsimulation,\ncomputer experiments, and\nfield data\n\nExperimental design will play a key role in our developments, but not in the classical regression and response surface methodology sense\nChallenging real-data/real-simulation examples benefiting from modern surrogate modeling methodology\nWe will consider the classical, response surface methodology (RSM) approach, and then move on to more modern approaches\nAll approaches are based on surrogates",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Optimization</span>"
    ]
  },
  {
    "objectID": "001_optimization_surrogate.html#surrogates",
    "href": "001_optimization_surrogate.html#surrogates",
    "title": "1  Introduction: Optimization",
    "section": "1.2 Surrogates",
    "text": "1.2 Surrogates\n\nGathering data is expensive, and sometimes getting exactly the data you want is impossible or unethical\nSurrogate: substitute for the real thing\nIn statistics, draws from predictive equations derived from a fitted model can act as a surrogate for the data-generating mechanism\nBenefits of the surrogate approach:\n\nSurrogate could represent a cheaper way to explore relationships, and entertain “what ifs?”\nSurrogates favor faithful yet pragmatic reproduction of dynamics:\n\ninterpretation,\nestablishing causality, or\nidentification\n\nMany numerical simulators are deterministic, whereas field observations are noisy or have measurement error\n\n\n\n1.2.1 Costs of Simulation\n\nComputer simulations are generally cheaper (but not always!) than physical observation\nSome computer simulations can be just as expensive as field experimentation, but computer modeling is regarded as easier because:\n\nthe experimental apparatus is better understood\nmore aspects may be controlled.\n\n\n\n\n1.2.2 Mathematical Models and Meta-Models\n\nUse of mathematical models leveraging numerical solvers has been commonplace for some time\nMathematical models became more complex, requiring more resources to simulate/solve numerically\nPractitioners increasingly relied on meta-models built off of limited simulation campaigns\n\n\n\n1.2.3 Surrogates = Trained Meta-models\n\nData collected via expensive computer evaluations tuned flexible functional forms that could be used in lieu of further simulation to\n\nsave money or computational resources;\ncope with an inability to perform future runs (expired licenses, off-line or over-impacted supercomputers)\n\nTrained meta-models became known as surrogates\n\n\n\n1.2.4 Computer Experiments\n\nComputer experiment: design, running, and fitting meta-models.\n\nLike an ordinary statistical experiment, except the data are generated by computer codes rather than physical or field observations, or surveys\n\nSurrogate modeling is statistical modeling of computer experiments\n\n\n\n1.2.5 Limits of Mathematical Modeling\n\nMathematical biologists, economists and others had reached the limit of equilibrium-based mathematical modeling with cute closed-form solutions\nStochastic simulations replace deterministic solvers based on FEM, Navier–Stokes or Euler methods\nAgent-based simulation models are used to explore predator-prey (Lotka–Voltera) dynamics, spread of disease, management of inventory or patients in health insurance markets\nConsequence: the distinction between surrogate and statistical model is all but gone\n\n\n\n1.2.6 Example: Why Computer Simulations are Necessary\n\nYou can’t seed a real community with Ebola and watch what happens\nIf there’s (real) field data, say on a historical epidemic, further experimentation may be almost entirely limited to the mathematical and computer modeling side\nClassical statistical methods offer little guidance\n\n\n\n1.2.7 Simulation Requirements\n\nSimulation should\n\nenable rich diagnostics to help criticize that models\nunderstanding its sensitivity to inputs and other configurations\nproviding the ability to optimize and\nrefine both automatically and with expert intervention\n\nAnd it has to do all that while remaining computationally tractable\nOne perspective is so-called response surface methods (RSMs):\na poster child from industrial statistics’ heyday, well before information technology became a dominant industry\n\n\n\n\n\n\n\nGoals\n\n\n\n\nHow to choose models and optimizers for solving real-world problems\nHow to use simulation to understand and improve processes",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Optimization</span>"
    ]
  },
  {
    "objectID": "001_optimization_surrogate.html#jupyter-notebook",
    "href": "001_optimization_surrogate.html#jupyter-notebook",
    "title": "1  Introduction: Optimization",
    "section": "1.3 Jupyter Notebook",
    "text": "1.3 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Optimization</span>"
    ]
  },
  {
    "objectID": "002_awwe.html",
    "href": "002_awwe.html",
    "title": "2  Aircraft Wing Weight Example",
    "section": "",
    "text": "2.1 AWWE Equation\n\\[ W = 0.036 S_W^{0.758} \\times W_{fw}^{0.0035} \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6} \\times  q^{0.006}  \\times \\lambda^{0.04} \\] \\[ \\times \\left( \\frac{100 R_{tc}}{\\cos \\Lambda} \\right)^{-0.3} \\times (N_z W_{dg})^{0.49}\\]",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#awwe-equation",
    "href": "002_awwe.html#awwe-equation",
    "title": "2  Aircraft Wing Weight Example",
    "section": "",
    "text": "Example from Forrester et al. \nUnderstand the weight of an unpainted light aircraft wing as a function of nine design and operational parameters:",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#awwe-parameters-and-equations-part-1",
    "href": "002_awwe.html#awwe-parameters-and-equations-part-1",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.2 AWWE Parameters and Equations (Part 1)",
    "text": "2.2 AWWE Parameters and Equations (Part 1)\n\n\n\nTable 2.1: Aircraft Wing Weight Parameters\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nParameter\nBaseline\nMinimum\nMaximum\n\n\n\n\n\\(S_W\\)\nWing area (\\(ft^2\\))\n174\n150\n200\n\n\n\\(W_{fw}\\)\nWeight of fuel in wing (lb)\n252\n220\n300\n\n\n\\(A\\)\nAspect ratio\n7.52\n6\n10\n\n\n\\(\\Lambda\\)\nQuarter-chord sweep (deg)\n0\n-10\n10\n\n\n\\(q\\)\nDynamic pressure at cruise (\\(lb/ft^2\\))\n34\n16\n45\n\n\n\\(\\lambda\\)\nTaper ratio\n0.672\n0.5\n1\n\n\n\\(R_{tc}\\)\nAerofoil thickness to chord ratio\n0.12\n0.08\n0.18\n\n\n\\(N_z\\)\nUltimate load factor\n3.8\n2.5\n6\n\n\n\\(W_{dg}\\)\nFlight design gross weight (lb)\n2000\n1700\n2500\n\n\n\\(W_p\\)\npaint weight (lb/ft^2)\n0.064\n0.025\n0.08\n\n\n\n\n\n\nThe study begins with a baseline Cessna C172 Skyhawk Aircraft as its reference point. It aims to investigate the impact of wing area and fuel weight on the overall weight of the aircraft. Two crucial parameters in this analysis are the aspect ratio (\\(A\\)), defined as the ratio of the wing’s length to the average chord (thickness of the airfoil), and the taper ratio (\\(\\lambda\\)), which represents the ratio of the maximum to the minimum thickness of the airfoil or the maximum to minimum chord.\nIt’s important to note that the equation used in this context is not a computer simulation but will be treated as one for the purpose of illustration. This approach involves employing a true mathematical equation, even if it’s considered unknown, as a useful tool for generating realistic settings to test the methodology. The functional form of this equation was derived by “calibrating” known physical relationships to curves obtained from existing aircraft data, as referenced in Raymer 2012. Essentially, it acts as a surrogate for actual measurements of aircraft weight.\nExamining the mathematical properties of the AWWE (Aircraft Weight With Wing Area and Fuel Weight Equation), it is evident that the response is highly nonlinear concerning its inputs. While it’s common to apply the logarithm to simplify equations with complex exponents, even when modeling the logarithm, which transforms powers into slope coefficients and products into sums, the response remains nonlinear due to the presence of trigonometric terms. Given the combination of nonlinearity and high input dimension, simple linear and quadratic response surface approximations are likely to be inadequate for this analysis.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#goals-understanding-and-optimization",
    "href": "002_awwe.html#goals-understanding-and-optimization",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.3 Goals: Understanding and Optimization",
    "text": "2.3 Goals: Understanding and Optimization\nThe primary goals of this study revolve around understanding and optimization:\n\nUnderstanding: One of the straightforward objectives is to gain a deep understanding of the input-output relationships in this context. Given the global perspective implied by this setting, it becomes evident that a more sophisticated model is almost necessary. At this stage, let’s focus on this specific scenario to establish a clear understanding.\nOptimization: Another application of this analysis could be optimization. There may be an interest in minimizing the weight of the aircraft, but it’s likely that there will be constraints in place. For example, the presence of wings with a nonzero area is essential for the aircraft to be capable of flying. In situations involving (constrained) optimization, a global perspective and, consequently, the use of flexible modeling are vital.\n\nThe provided Python code serves as a genuine computer implementation that “solves” a mathematical model. It accepts arguments encoded in the unit cube, with defaults used to represent baseline settings, as detailed in the table labeled as Table 2.1. To map values from the interval \\([a, b]\\) to the interval \\([0, 1]\\), the following formula can be employed:\n\\[y = f(x) = \\frac{x - a}{b - a}.\\]\nTo reverse this mapping and obtain the original values, the formula \\[g(y) = a + (b - a) y\\] can be used.\n\nimport numpy as np\n\ndef wingwt(Sw=0.48, Wfw=0.4, A=0.38, L=0.5, q=0.62, l=0.344,  Rtc=0.4, Nz=0.37, Wdg=0.38):\n    # put coded inputs back on natural scale\n    Sw = Sw * (200 - 150) + 150 \n    Wfw = Wfw * (300 - 220) + 220 \n    A = A * (10 - 6) + 6 \n    L = (L * (10 - (-10)) - 10) * np.pi/180\n    q = q * (45 - 16) + 16 \n    l = l * (1 - 0.5) + 0.5  \n    Rtc = Rtc * (0.18 - 0.08) + 0.08\n    Nz = Nz * (6 - 2.5) + 2.5\n    Wdg = Wdg*(2500 - 1700) + 1700\n    # calculation on natural scale\n    W = 0.036 * Sw**0.758 * Wfw**0.0035 * (A/np.cos(L)**2)**0.6 * q**0.006 \n    W = W * l**0.04 * (100*Rtc/np.cos(L))**(-0.3) * (Nz*Wdg)**(0.49)\n    return(W)",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#properties-of-the-python-solver",
    "href": "002_awwe.html#properties-of-the-python-solver",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.4 Properties of the Python “Solver”",
    "text": "2.4 Properties of the Python “Solver”\nThe compute time required by the “wingwt” solver is extremely short and can be considered trivial in terms of computational resources. The approximation error is exceptionally small, effectively approaching machine precision, which indicates the high accuracy of the solver’s results.\nTo simulate time-consuming evaluations, a deliberate delay is introduced by incorporating a sleep(3600) command, which effectively synthesizes a one-hour execution time for a particular evaluation.\nMoving on to the AWWE visualization, plotting in two dimensions is considerably simpler than dealing with nine dimensions. To aid in creating visual representations, the code provided below establishes a grid within the unit square to facilitate the generation of sliced visuals. This involves generating a “meshgrid” as outlined in the code.\n\nimport numpy as np\nx = np.linspace(0, 1, 3)\ny = np.linspace(0, 1, 3)\nX, Y = np.meshgrid(x, y)\nzp = zip(np.ravel(X), np.ravel(Y))\nlist(zp)\n\n[(0.0, 0.0),\n (0.5, 0.0),\n (1.0, 0.0),\n (0.0, 0.5),\n (0.5, 0.5),\n (1.0, 0.5),\n (0.0, 1.0),\n (0.5, 1.0),\n (1.0, 1.0)]\n\n\nThe coding used to transform inputs from natural units is largely a matter of taste, so long as it’s easy to undo for reporting back on original scales\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n# plt.style.use('seaborn-white')\nimport numpy as np\nx = np.linspace(0, 1, 100)\ny = np.linspace(0, 1, 100)\nX, Y = np.meshgrid(x, y)",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#plot-1-load-factor-n_z-and-aspect-ratio-a",
    "href": "002_awwe.html#plot-1-load-factor-n_z-and-aspect-ratio-a",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.5 Plot 1: Load Factor (\\(N_z\\)) and Aspect Ratio (\\(A\\))",
    "text": "2.5 Plot 1: Load Factor (\\(N_z\\)) and Aspect Ratio (\\(A\\))\nWe will vary \\(N_z\\) and \\(A\\), with other inputs fixed at their baseline values.\n\nz = wingwt(A = X, Nz = Y)\nfig = plt.figure(figsize=(7., 5.))\nplt.contourf(X, Y, z, 20, cmap='jet')\nplt.xlabel(\"A\")\nplt.ylabel(\"Nz\")\nplt.title(\"Load factor (Nz) vs. Aspect Ratio (A)\")\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\nContour plots can be refined, e.g., by adding explicit contour lines as shown in the following figure.\n\ncontours = plt.contour(X, Y, z, 4, colors='black')\nplt.clabel(contours, inline=True, fontsize=8)\nplt.xlabel(\"A\")\nplt.ylabel(\"Nz\")\n\nplt.imshow(z, extent=[0, 1, 0, 1], origin='lower',\n           cmap='jet', alpha=0.9)\nplt.colorbar()\n\n\n\n\n\n\n\n\nThe interpretation of the AWWE plot can be summarized as follows:\n\nThe figure displays the weight response as a function of two variables, \\(N_z\\) and \\(A\\), using an image-contour plot.\nThe slight curvature observed in the contours suggests an interaction between these two variables.\nNotably, the range of outputs depicted in the figure, spanning from approximately 160 to 320, nearly encompasses the entire range of outputs observed from various input settings within the full 9-dimensional input space.\nThe plot indicates that aircraft wings tend to be heavier when the aspect ratios (\\(A\\)) are high.\nThis observation aligns with the idea that wings are designed to withstand and accommodate high gravitational forces (\\(g\\)-forces, large \\(N_z\\)), and there may be a compounding effect where larger values of \\(N_z\\) contribute to increased wing weight.\nIt’s plausible that this phenomenon is related to the design considerations of fighter jets, which cannot have the efficient and lightweight glider-like wings typically found in other types of aircraft.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#plot-2-taper-ratio-and-fuel-weight",
    "href": "002_awwe.html#plot-2-taper-ratio-and-fuel-weight",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.6 Plot 2: Taper Ratio and Fuel Weight",
    "text": "2.6 Plot 2: Taper Ratio and Fuel Weight\n\nThe same experiment for two other inputs, e.g., taper ratio \\(\\lambda\\) and fuel weight \\(W_{fw}\\)\n\n\nz = wingwt(Wfw = X,  Nz = Y)\ncontours = plt.contour(X, Y, z, 4, colors='black')\nplt.clabel(contours, inline=True, fontsize=8)\nplt.xlabel(\"WfW\")\nplt.ylabel(\"l\")\n\nplt.imshow(z, extent=[0, 1, 0, 1], origin='lower',\n           cmap='jet', alpha=0.9)\nplt.colorbar();\n\n\n\n\n\n\n\n\n\nInterpretation of Taper Ratio (\\(l\\)) and Fuel Weight (\\(W_{fw}\\))\n\nApparently, neither input has much effect on wing weight:\n\nwith \\(\\lambda\\) having a marginally greater effect, covering less than 4 percent of the span of weights observed in the \\(A \\times N_z\\) plane\n\nThere’s no interaction evident in \\(\\lambda \\times W_{fw}\\)",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#the-big-picture-combining-all-variables",
    "href": "002_awwe.html#the-big-picture-combining-all-variables",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.7 The Big Picture: Combining all Variables",
    "text": "2.7 The Big Picture: Combining all Variables\n\npl = [\"Sw\", \"Wfw\", \"A\", \"L\", \"q\", \"l\",  \"Rtc\", \"Nz\", \"Wdg\"]\n\n\nimport math\n\nZ = []\nZlab = []\nl = len(pl)\n# lc = math.comb(l,2)\nfor i in range(l):\n    for j in range(i+1, l):\n    # for j in range(l):\n        # print(pl[i], pl[j])\n        d = {pl[i]: X, pl[j]: Y}\n        Z.append(wingwt(**d))\n        Zlab.append([pl[i],pl[j]])\n\nNow we can generate all 36 combinations, e.g., our first example is combination p = 19.\n\np = 19\nZlab[p]\n\n['A', 'Nz']\n\n\nTo help interpret outputs from experiments such as this one—to level the playing field when comparing outputs from other pairs of inputs—code below sets up a color palette that can be re-used from one experiment to the next. We use the arguments vmin=180 and vmax =360 to implement comparibility\n\nplt.contourf(X, Y, Z[p], 20, cmap='jet', vmin=180, vmax=360)\nplt.xlabel(Zlab[p][0])\nplt.ylabel(Zlab[p][1])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\nLet’s plot the second example, taper ratio \\(\\lambda\\) and fuel weight \\(W_{fw}\\)\nThis is combination 11:\n\n\np = 11\nZlab[p]\n\n['Wfw', 'l']\n\n\n\nplt.contourf(X, Y, Z[p], 20, cmap='jet', vmin=180, vmax=360)\nplt.xlabel(Zlab[p][0])\nplt.ylabel(Zlab[p][1])\nplt.colorbar()\n\n\n\n\n\n\n\n\n\nUsing a global colormap indicates that these variables have minor effects on the wing weight.\nImportant factors can be detected by visual inspection\nPlotting the Big Picture: we can plot all 36 combinations in one figure.\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\n\nfig = plt.figure(figsize=(20., 20.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(6,6),  # creates 2x2 grid of axes\n                 axes_pad=0.5,  # pad between axes in inch.\n                 share_all=True,\n                 label_mode=\"0\",\n                 ) \ni = 0\nfor ax, im in zip(grid, Z):\n    # Iterating over the grid returns the Axes.\n    ax.set_xlabel(Zlab[i][0])\n    ax.set_ylabel(Zlab[i][1])\n    # ax.set_title(Zlab[i][1] + \" vs. \" + Zlab[i][0])\n    ax.contourf(X, Y, im, 30, cmap = \"jet\",  vmin = 180, vmax = 360)\n    i = i + 1\n       \nplt.show()",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#awwe-landscape",
    "href": "002_awwe.html#awwe-landscape",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.8 AWWE Landscape",
    "text": "2.8 AWWE Landscape\n\nOur Observations\n\nThe load factor \\(N_z\\), which determines the magnitude of the maximum aerodynamic load on the wing, is very active and involved in interactions with other variables.\n\n\nClassic example: the interaction of \\(N_z\\) with the aspect ratio \\(A\\) indicates a heavy wing for high aspect ratios and large \\(g\\)-forces\nThis is the reaon why highly manoeuvrable fighter jets cannot have very efficient, glider wings)\n\n\nAspect ratio \\(A\\) and airfoil thickness to chord ratio \\(R_{tc}\\) have nonlinear interactions.\nMost important variables:\n\n\nUltimate load factor \\(N_z\\), wing area \\(S_w\\), and flight design gross weight\\(W_{dg}\\).\n\n\nLittle impact: dynamic pressure \\(q\\), taper ratio \\(l\\), and quarter-chord sweep \\(L\\).\n\nExpert Knowledge\n\nAircraft designers know that the overall weight of the aircraft and the wing area must be kept to a minimum\nthe latter usually dictated by constraints such as required stall speed, landing distance, turn rate, etc.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#summary-of-the-first-experiments",
    "href": "002_awwe.html#summary-of-the-first-experiments",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.9 Summary of the First Experiments",
    "text": "2.9 Summary of the First Experiments\n\nFirst, we considered two pairs of inputs, out of 36 total pairs\nThen, the “Big Picture”:\n\nFor each pair we evaluated wingwt 10,000 times\n\nDoing the same for all pairs would require 360K evaluations:\n\nnot a reasonable number with a real computer simulation that takes any non-trivial amount of time to evaluate\nOnly 1s per evaluation: \\(&gt;100\\) hours\n\nMany solvers take minutes/hours/days to execute a single run\nAnd: three-way interactions?\nConsequence: a different strategy is needed",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#exercise",
    "href": "002_awwe.html#exercise",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.10 Exercise",
    "text": "2.10 Exercise\n\n2.10.1 Adding Paint Weight\n\nPaint weight is not considered.\nAdd Paint Weight \\(W_p\\) to formula (the updated formula is shown below) and update the functions and plots in the notebook.\n\n\\[ W = 0.036S_W^{0.758} \\times W_{fw}^{0.0035} \\times \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6} \\times q^{0.006} \\times \\lambda^{0.04} \\] \\[ \\times \\left( \\frac{100 R_{tc}}{\\cos \\Lambda} \\right)^{-0.3} \\times (N_z W_{dg})^{0.49} + S_w W_p\\]",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "002_awwe.html#jupyter-notebook",
    "href": "002_awwe.html#jupyter-notebook",
    "title": "2  Aircraft Wing Weight Example",
    "section": "2.11 Jupyter Notebook",
    "text": "2.11 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aircraft Wing Weight Example</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html",
    "href": "003_scipy_optimize_intro.html",
    "title": "3  Introduction to scipy.optimize",
    "section": "",
    "text": "3.1 Derivative-free Optimization Algorithms\nSection 3.1.1 and Section 3.1.2 present two approaches that do not need gradient information to find the minimum. They use function evaluations to find the minimum.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#derivative-free-optimization-algorithms",
    "href": "003_scipy_optimize_intro.html#derivative-free-optimization-algorithms",
    "title": "3  Introduction to scipy.optimize",
    "section": "",
    "text": "3.1.1 Nelder-Mead Simplex Algorithm\nmethod='Nelder-Mead': In the example below, the minimize routine is used with the Nelder-Mead simplex algorithm (selected through the method parameter):\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef rosen(x):\n    \"\"\"The Rosenbrock function\"\"\"\n    return sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n\nx0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\nres = minimize(rosen, x0, method='nelder-mead',\n               options={'xatol': 1e-8, 'disp': True})\n\nprint(res.x)\n\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 339\n         Function evaluations: 571\n[1. 1. 1. 1. 1.]\n\n\nThe simplex algorithm is probably the simplest way to minimize a well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum.\n\n\n3.1.2 Powell’s Method\nAnother optimization algorithm that needs only function calls to find the minimum is Powell’s method, which can be selected by setting the method parameter to 'powell' in the minimize function.\nTo demonstrate how to supply additional arguments to an objective function, let’s consider minimizing the Rosenbrock function with an additional scaling factor \\(a\\) and an offset \\(b\\):\n\\[\nf(J, a, b) = \\sum_{i=1}^{N-1} a (x_{i+1} - x_i^2)^2 + (1 - x_i)^2 + b\n\\]\nYou can achieve this using the minimize routine with the example parameters \\(a=0.5\\) and \\(b=1\\):\n\ndef rosen_with_args(x, a, b):\n    \"\"\"The Rosenbrock function with additional arguments\"\"\"\n    return sum(a * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0) + b\n\nx0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\nres = minimize(rosen_with_args, x0, method='nelder-mead',\n               args=(0.5, 1.), options={'xatol': 1e-8, 'disp': True})\n\nprint(res.x)\n\nOptimization terminated successfully.\n         Current function value: 1.000000\n         Iterations: 319\n         Function evaluations: 525\n[1.         1.         1.         1.         0.99999999]\n\n\nAs an alternative to using the args parameter of minimize, you can wrap the objective function in a new function that accepts only x. This approach is also useful when it is necessary to pass additional parameters to the objective function as keyword arguments.\n\ndef rosen_with_args(x, a, *, b):  # b is a keyword-only argument\n    return sum(a * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0) + b\n\ndef wrapped_rosen_without_args(x):\n    return rosen_with_args(x, 0.5, b=1.)  # pass in `a` and `b`\n\nx0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\nres = minimize(wrapped_rosen_without_args, x0, method='nelder-mead',\n               options={'xatol': 1e-8,})\n\nprint(res.x)\n\n[1.         1.         1.         1.         0.99999999]\n\n\nAnother alternative is to use functools.partial.\n\nfrom functools import partial\n\npartial_rosen = partial(rosen_with_args, a=0.5, b=1.)\nres = minimize(partial_rosen, x0, method='nelder-mead',\n               options={'xatol': 1e-8,})\n\nprint(res.x)\n\n[1.         1.         1.         1.         0.99999999]",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#gradient-based-optimization-algorithms",
    "href": "003_scipy_optimize_intro.html#gradient-based-optimization-algorithms",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.2 Gradient-based optimization algorithms",
    "text": "3.2 Gradient-based optimization algorithms\n\n3.2.1 An Introductory Example: Broyden-Fletcher-Goldfarb-Shanno Algorithm (BFGS)\nThis section introduces an optimization algorithm that uses gradient information to find the minimum. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (selected by setting method='BFGS') is an optimization algorithm that aims to converge quickly to the solution. This algorithm uses the gradient of the objective function. If the gradient is not provided by the user, it is estimated using first-differences. The BFGS method typically requires fewer function calls compared to the simplex algorithm, even when the gradient needs to be estimated.\n\n\n\n\n\n\nExample: BFGS\n\n\n\nTo demonstrate the BFGS algorithm, let’s use the Rosenbrock function again. The gradient of the Rosenbrock function is a vector described by the following mathematical expression:\n\\[\\begin{align}\n\\frac{\\partial f}{\\partial x_j} = \\sum_{i=1}^{N} 200(x_i - x_{i-1}^2)(\\delta_{i,j} - 2x_{i-1}\\delta_{i-1,j}) - 2(1 - x_{i-1})\\delta_{i-1,j} \\\\\n= 200(x_j - x_{j-1}^2) - 400x_j(x_{j+1} - x_j^2) - 2(1 - x_j)\n\\end{align}\\]\nThis expression is valid for interior derivatives, but special cases are:\n\\[\n\\frac{\\partial f}{\\partial x_0} = -400x_0(x_1 - x_0^2) - 2(1 - x_0)\n\\]\n\\[\n\\frac{\\partial f}{\\partial x_{N-1}} = 200(x_{N-1} - x_{N-2}^2)\n\\]\nHere’s a Python function that computes this gradient:\n\ndef rosen_der(x):\n    xm = x[1:-1]\n    xm_m1 = x[:-2]\n    xm_p1 = x[2:]\n    der = np.zeros_like(x)\n    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n    der[-1] = 200*(x[-1]-x[-2]**2)\n    return der\n\nYou can specify this gradient information in the minimize function using the jac parameter as illustrated below:\n\nres = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n               options={'disp': True})\n\nprint(res.x)\n\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 25\n         Function evaluations: 30\n         Gradient evaluations: 30\n[1.00000004 1.0000001  1.00000021 1.00000044 1.00000092]\n\n\n\n\n\n\n3.2.2 Background and Basics for Gradient-based Optimization\n\n\n3.2.3 Gradient\nThe gradient \\(\\nabla f(J)\\) for a scalar function \\(f(J)\\) with \\(n\\) different variables is defined by its partial derivatives:\n\\[\\nabla f(J) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right]\\]\n\n\n3.2.4 Jacobian Matrix\nThe Jacobian matrix \\(J(J)\\) for a vector-valued function \\(F(J) = [f_1(J), f_2(J), \\ldots, f_m(J)]\\) is defined as:\n\\(J(J) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\ldots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\ldots & \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\ldots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\\)\nIt consists of the first order partial derivatives and gives therefore an overview about the gradients of a vector valued function.\n\n\n\n\n\n\nExample: Jacobian matrix\n\n\n\nConsider a vector-valued function \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) defined as follows: \\[f(J) = \\begin{bmatrix} x_1^2 + 2x_2 \\\\ 3x_1 - \\sin(x_2) \\\\ e^{x_1 + x_2} \\end{bmatrix}\\]\nLet’s compute the partial derivatives and construct the Jacobian matrix:\n\\(\\frac{\\partial f_1}{\\partial x_1} = 2x_1, \\quad \\frac{\\partial f_1}{\\partial x_2} = 2\\)\n\\(\\frac{\\partial f_2}{\\partial x_1} = 3, \\quad \\frac{\\partial f_2}{\\partial x_2} = -\\cos(x_2)\\)\n\\(\\frac{\\partial f_3}{\\partial x_1} = e^{x_1 + x_2}, \\quad \\frac{\\partial f_3}{\\partial x_2} = e^{x_1 + x_2}\\)\nSo, the Jacobian matrix is:\n\\[J(J) = \\begin{bmatrix} 2x_1 & 2 \\\\ 3 & -\\cos(x_2) \\\\ e^{x_1 + x_2} & e^{x_1 + x_2} \\end{bmatrix}\\]\nThis Jacobian matrix provides information about how small changes in the input variables \\(x_1\\) and \\(x_2\\) affect the corresponding changes in each component of the output vector.\n\n\n\n\n3.2.5 Hessian Matrix\nThe Hessian matrix \\(H(J)\\) for a scalar function \\(f(J)\\) is defined as:\n\\(H(J) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\\)\nSo, the Hessian matrix consists of the second order dervatives of the function. It provides information about the local curvature of the function with respect to changes in the input variables.\n\n\n\n\n\n\nExample: Hessian matrix\n\n\n\nConsider a scalar-valued function: \\[f(J) = x_1^2 + 2x_2^2 + \\sin(x_1   x_2)\\]\nThe Hessian matrix of this scalar-valued function is the matrix of its second-order partial derivatives with respect to the input variables: \\[H(J) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix}\\]\nLet’s compute the second-order partial derivatives and construct the Hessian matrix:\n\\[\\begin{align}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= 2 + \\cos(x_1 x_2) x_2^2\\\\\n\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &= 2x_1  x_2 \\cos(x_1 x_2) - \\sin(x_1  x_2)\\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &= 2x_1  x_2  \\cos(x_1  x_2) - \\sin(x_1  x_2)\\\\\n\\frac{\\partial^2 f}{\\partial x_2^2} &= 4x_2^2 + \\cos(x_1  x_2) x_1^2\n\\end{align}\\]\nSo, the Hessian matrix is:\n\\[H(J) = \\begin{bmatrix} 2 + \\cos(x_1   x_2)   x_2^2 & 2x_1   x_2   \\cos(x_1   x_2) - \\sin(x_1   x_2) \\\\ 2x_1   x_2   \\cos(x_1   x_2) - \\sin(x_1   x_2) & 4x_2^2 + \\cos(x_1   x_2)   x_1^2 \\end{bmatrix}\\]\n\n\n\n\n3.2.6 Gradient for Optimization\nIn optimization, the goal is to find the minimum or maximum of a function. Gradient-based optimization methods utilize information about the gradient (or derivative) of the function to guide the search for the optimal solution. This is particularly useful when dealing with complex, high-dimensional functions where an exhaustive search is impractical.\nThe gradient descent method can be divided in the following steps:\n\nInitialize: start with an initial guess for the parameters of the function to be optimized.\nCompute Gradient: Calculate the gradient (partial derivatives) of the function with respect to each parameter at the current point. The gradient indicates the direction of the steepest increase in the function.\nUpdate Parameters: Adjust the parameters in the opposite direction of the gradient, scaled by a learning rate. This step aims to move towards the minimum of the function:\n\n\\(x_{k+1} = x_k - \\alpha \\times \\nabla f(x_{k})\\)\n\\(x_{x}\\) is current parameter vector or point in the parameter space.\n\\(\\alpha\\) is the learning rate, a positive scalar that determines the step size in each iteration.\n\\(\\nabla f(x)\\) is the gradient of the objective function.\n\nIterate: Repeat the above steps until convergence or a predefined number of iterations. Convergence is typically determined when the change in the function value or parameters becomes negligible.\n\n\n\n\n\n\n\nExample: Gradient Descent\n\n\n\nLet’s consider a simple quadratic function as an example: \\[f(x) = x^2 + 4x + y^2 + 2y + 4.\\]\nWe’ll use gradient descent to find the minimum of this function.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the quadratic function\ndef quadratic_function(x, y):\n    return x**2 + 4*x + y**2 + 2*y + 4\n\n# Define the gradient of the quadratic function\ndef gradient_quadratic_function(x, y):\n    grad_x = 2*x + 4\n    grad_y = 2*y + 2\n    return np.array([grad_x, grad_y])\n\n# Gradient Descent for optimization in 2D\ndef gradient_descent(initial_point, learning_rate, num_iterations):\n    points = [np.array(initial_point)]\n    \n    for _ in range(num_iterations):\n        current_point = points[-1]\n        gradient = gradient_quadratic_function(*current_point)\n        new_point = current_point - learning_rate * gradient\n        \n        points.append(new_point)\n        \n    return points\n\n# Visualization of optimization process with 3D surface and consistent arrow sizes\ndef plot_optimization_process_3d_consistent_arrows(points):\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n\n    x_vals = np.linspace(-10, 2, 100)\n    y_vals = np.linspace(-10, 2, 100)\n    X, Y = np.meshgrid(x_vals, y_vals)\n    Z = quadratic_function(X, Y)\n\n    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n    ax.scatter(*zip(*points), [quadratic_function(*p) for p in points], c='red', label='Optimization Trajectory')\n\n    for i in range(len(points) - 1):  \n        x, y = points[i]\n        dx, dy = points[i + 1] - points[i]\n        dz = quadratic_function(*(points[i + 1])) - quadratic_function(*points[i])\n        gradient_length = 0.5\n\n        ax.quiver(x, y, quadratic_function(*points[i]), dx, dy, dz, color='blue', length=gradient_length, normalize=False, arrow_length_ratio=0.1)\n\n    ax.set_title('Gradient-Based Optimization with 2D Quadratic Function')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('f(x, y)')\n    ax.legend()\n    plt.show()\n\n# Initial guess and parameters\ninitial_guess = [-9.0, -9.0]\nlearning_rate = 0.2\nnum_iterations = 10\n\n# Run gradient descent in 2D and visualize the optimization process with 3D surface and consistent arrow sizes\ntrajectory = gradient_descent(initial_guess, learning_rate, num_iterations)\nplot_optimization_process_3d_consistent_arrows(trajectory)\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.7 Newton Method\nInitialization: Start with an initial guess for the optimal solution: \\(x_0\\).\nIteration: Repeat the following three steps until convergence or a predefined stopping criterion is met:\n\nCalculate the gradient (\\(\\nabla\\)) and the Hessian matrix (\\(\\nabla^2\\)) of the objective function at the current point: \\[\\nabla f(x_k) \\quad \\text{and} \\quad \\nabla^2 f(x_k)\\]\nUpdate the current solution using the Newton-Raphson update formula \\[x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k),\\] where\n\n\n\\(\\nabla f(x_k)\\) is the gradient (first derivative) of the objective function with respect to the variable \\(x\\), evaluated at the current solution \\(x_k\\).\n\\(\\nabla^2 f(x_k)\\): The Hessian matrix (second derivative) of the objective function with respect to \\(x\\), evaluated at the current solution \\(x_k\\).\n\\(x_k\\): The current solution or point in the optimization process.\n\\(\\nabla^2 f(x_k)]^{-1}\\): The inverse of the Hessian matrix at the current point, representing the approximation of the curvature of the objective function.\n\\(x_{k+1}\\): The updated solution or point after applying the Newton-Raphson update.\n\n\nCheck for convergence.\n\n\n\n\n\n\n\nExample: Newton Method\n\n\n\nWe want to optimize the Rosenbrock function and use the Hessian and the Jacobian (which is equal to the gradient vector for scalar objective function) to the minimize function.\n\ndef rosenbrock(x):\n    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n\ndef rosenbrock_gradient(x):\n    dfdx0 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n    dfdx1 = 200 * (x[1] - x[0]**2)\n    return np.array([dfdx0, dfdx1])\n\ndef rosenbrock_hessian(x):\n    d2fdx0 = 1200 * x[0]**2 - 400 * x[1] + 2\n    d2fdx1 = -400 * x[0]\n    return np.array([[d2fdx0, d2fdx1], [d2fdx1, 200]])\n\ndef classical_newton_optimization_2d(initial_guess, tol=1e-6, max_iter=100):\n    x = initial_guess.copy()\n\n    for i in range(max_iter):\n        gradient = rosenbrock_gradient(x)\n        hessian = rosenbrock_hessian(x)\n\n        # Solve the linear system H * d = -g for d\n        d = np.linalg.solve(hessian, -gradient)\n\n        # Update x\n        x += d\n\n        # Check for convergence\n        if np.linalg.norm(gradient, ord=np.inf) &lt; tol:\n            break\n\n    return x\n\n# Initial guess\ninitial_guess_2d = np.array([0.0, 0.0])\n\n# Run classical Newton optimization for the 2D Rosenbrock function\nresult_2d = classical_newton_optimization_2d(initial_guess_2d)\n\n# Print the result\nprint(\"Optimal solution:\", result_2d)\nprint(\"Objective value:\", rosenbrock(result_2d))\n\nOptimal solution: [1. 1.]\nObjective value: 0.0\n\n\n\n\n\n\n3.2.8 BFGS-Algorithm\nBFGS is an optimization algorithm designed for unconstrained optimization problems. It belongs to the class of quasi-Newton methods and is known for its efficiency in finding the minimum of a smooth, unconstrained objective function.\n\n\n3.2.9 Procedure:\n\nInitialization:\n\nStart with an initial guess for the parameters of the objective function.\nInitialize an approximation of the Hessian matrix (inverse) denoted by \\(H\\).\n\n\nIterative Update:\n\nAt each iteration, compute the gradient vector at the current point.\nUpdate the parameters using the BFGS update formula, which involves the inverse Hessian matrix approximation, the gradient, and the difference in parameter vectors between successive iterations: \\[x_{k+1} = x_k - H_k^{-1} \\nabla f(x_k).\\]\nUpdate the inverse Hessian approximation using the BFGS update formula for the inverse Hessian. \\[H_{k+1} = H_k + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta x_k^T \\Delta g_k} - \\frac{H_k g_k g_k^T H_k}{g_k^T H_k g_k},\\] where:\n\\(x_k\\) and \\(x_{k+1}\\) are the parameter vectors at the current and updated iterations, respectively.\n\\(\\nabla f(x_k)\\) is the gradient vector at the current iteration.\n\\(\\Delta x_k = x_{k+1} - x_k\\) is the change in parameter vectors.\n\\(\\Delta g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\\) is the change in gradient vectors.\n\nConvergence:\n\nRepeat the iterative update until the optimization converges. Convergence is typically determined by reaching a sufficiently low gradient or parameter change.\n\n\n\n\n\n\n\n\nExample: BFGS for Rosenbrock\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the 2D Rosenbrock function\ndef rosenbrock(x):\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n\n# Initial guess\ninitial_guess = np.array([0.0, 0.0])\n\n# Minimize the Rosenbrock function using BFGS\nminimize(rosenbrock, initial_guess, method='BFGS')\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 2.843987518235081e-11\n        x: [ 1.000e+00  1.000e+00]\n      nit: 19\n      jac: [ 3.987e-06 -2.844e-06]\n hess_inv: [[ 4.948e-01  9.896e-01]\n            [ 9.896e-01  1.984e+00]]\n     nfev: 72\n     njev: 24\n\n\n\n\n\n\n3.2.10 Visualization BFGS for Rosenbrock\nA visualization of the BFGS search process on Rosenbrock’s function can be found here: https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif\n\n\n\n\n\n\n\nTasks\n\n\n\n\nIn which situations is it possible to use algorithms like BFGS, but not the classical Newton method?\nInvestigate the Newton-CG method\nUse an objective function of your choice and apply Newton-CG\nCompare the Newton-CG method with the BFGS. What are the similarities and differences between the two algorithms?",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#gradient--and-hessian-based-optimization-algorithms",
    "href": "003_scipy_optimize_intro.html#gradient--and-hessian-based-optimization-algorithms",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.3 Gradient- and Hessian-based optimization algorithms",
    "text": "3.3 Gradient- and Hessian-based optimization algorithms\nSection 3.3.1 presents an optimization algorithm that uses gradient and Hessian information to find the minimum. Section 3.3.2 presents an optimization algorithm that uses gradient and Hessian information to find the minimum. Section 3.3.3 presents an optimization algorithm that uses gradient and Hessian information to find the minimum.\nThe methods Newton-CG, trust-ncg and trust-krylov are suitable for dealing with large-scale problems (problems with thousands of variables). That is because the conjugate gradient algorithm approximately solve the trust-region subproblem (or invert the Hessian) by iterations without the explicit Hessian factorization. Since only the product of the Hessian with an arbitrary vector is needed, the algorithm is specially suited for dealing with sparse Hessians, allowing low storage requirements and significant time savings for those sparse problems.\n\n3.3.1 Newton-Conjugate-Gradient Algorithm\nNewton-Conjugate Gradient algorithm is a modified Newton’s method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian.\n\n\n3.3.2 Trust-Region Newton-Conjugate-Gradient Algorithm\n\n\n3.3.3 Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#global-optimization",
    "href": "003_scipy_optimize_intro.html#global-optimization",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.4 Global Optimization",
    "text": "3.4 Global Optimization\nGlobal optimization aims to find the global minimum of a function within given bounds, in the presence of potentially many local minima. Typically, global minimizers efficiently search the parameter space, while using a local minimizer (e.g., minimize) under the hood. SciPy contains a number of good global optimizers. Here, we’ll use those on the same objective function, namely the (aptly named) eggholder function:\n\ndef eggholder(x):\n    return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1]  + 47))))\n            -x[0] * np.sin(np.sqrt(abs(x[0] - (x[1]  + 47)))))\n\nbounds = [(-512, 512), (-512, 512)]\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.arange(-512, 513)\ny = np.arange(-512, 513)\nxgrid, ygrid = np.meshgrid(x, y)\nxy = np.stack([xgrid, ygrid])\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.view_init(45, -45)\nax.plot_surface(xgrid, ygrid, eggholder(xy), cmap='terrain')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('eggholder(x, y)')\nplt.show()\n\n\n\n\n\n\n\n\nWe now use the global optimizers to obtain the minimum and the function value at the minimum. We’ll store the results in a dictionary so we can compare different optimization results later.\n\nfrom scipy import optimize\nresults = dict()\nresults['shgo'] = optimize.shgo(eggholder, bounds)\nresults['shgo']\n\n message: Optimization terminated successfully.\n success: True\n     fun: -935.3379515605789\n    funl: [-9.353e+02]\n       x: [ 4.395e+02  4.540e+02]\n      xl: [[ 4.395e+02  4.540e+02]]\n     nit: 1\n    nfev: 45\n   nlfev: 40\n   nljev: 10\n   nlhev: 0\n\n\n\nresults['DA'] = optimize.dual_annealing(eggholder, bounds)\nresults['DA']\n\n message: ['Maximum number of iteration reached']\n success: True\n  status: 0\n     fun: -894.5789003905955\n       x: [-4.657e+02  3.857e+02]\n     nit: 1000\n    nfev: 4025\n    njev: 8\n    nhev: 0\n\n\nAll optimizers return an OptimizeResult, which in addition to the solution contains information on the number of function evaluations, whether the optimization was successful, and more. For brevity, we won’t show the full output of the other optimizers:\n\nresults['DE'] = optimize.differential_evolution(eggholder, bounds)\nresults['DE']\n\n message: Optimization terminated successfully.\n success: True\n     fun: -894.578900389627\n       x: [-4.657e+02  3.857e+02]\n     nit: 18\n    nfev: 588\n     jac: [-2.274e-05 -2.274e-05]\n\n\nshgo has a second method, which returns all local minima rather than only what it thinks is the global minimum:\n\nresults['shgo_sobol'] = optimize.shgo(eggholder, bounds, n=200, iters=5,\n                                      sampling_method='sobol')\nresults['shgo_sobol']\n\n message: Optimization terminated successfully.\n success: True\n     fun: -959.640662720831\n    funl: [-9.596e+02 -9.353e+02 ... -6.591e+01 -6.387e+01]\n       x: [ 5.120e+02  4.042e+02]\n      xl: [[ 5.120e+02  4.042e+02]\n           [ 4.395e+02  4.540e+02]\n           ...\n           [ 3.165e+01 -8.523e+01]\n           [ 5.865e+01 -5.441e+01]]\n     nit: 5\n    nfev: 3529\n   nlfev: 2327\n   nljev: 634\n   nlhev: 0\n\n\nWe’ll now plot all found minima on a heatmap of the function:\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nim = ax.imshow(eggholder(xy), interpolation='bilinear', origin='lower',\n               cmap='gray')\nax.set_xlabel('x')\nax.set_ylabel('y')\n\ndef plot_point(res, marker='o', color=None):\n    ax.plot(512+res.x[0], 512+res.x[1], marker=marker, color=color, ms=10)\n\nplot_point(results['DE'], color='c')  # differential_evolution - cyan\nplot_point(results['DA'], color='w')  # dual_annealing.        - white\n\n# SHGO produces multiple minima, plot them all (with a smaller marker size)\nplot_point(results['shgo'], color='r', marker='+')\nplot_point(results['shgo_sobol'], color='r', marker='x')\nfor i in range(results['shgo_sobol'].xl.shape[0]):\n    ax.plot(512 + results['shgo_sobol'].xl[i, 0],\n            512 + results['shgo_sobol'].xl[i, 1],\n            'ro', ms=2)\n\nax.set_xlim([-4, 514*2])\nax.set_ylim([-4, 514*2])\nplt.show()\n\n\n\n\n\n\n\n\n\n3.4.1 Dual Annealing Optimization\nThis function implements the Dual Annealing optimization.\n\n\n3.4.2 Differential Evolution\nDifferential Evolution is an algorithm used for finding the global minimum of multivariate functions. It is stochastic in nature (does not use gradient methods), and can search large areas of candidate space, but often requires larger numbers of function evaluations than conventional gradient based techniques.\n\n\n3.4.3 DIRECT\nDIviding RECTangles (DIRECT) is a deterministic global optimization algorithm capable of minimizing a black box function with its variables subject to lower and upper bound constraints by sampling potential solutions in the search space\n\n\n3.4.4 SHGO\nSHGO stands for “simplicial homology global optimization”. It is considered appropriate for solving general purpose NLP and blackbox optimization problems to global optimality (low-dimensional problems).\n\n\n3.4.5 Basin-hopping\nBasin-hopping is a two-phase method that combines a global stepping algorithm with local minimization at each step. Designed to mimic the natural process of energy minimization of clusters of atoms, it works well for similar problems with “funnel-like, but rugged” energy landscapes",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "003_scipy_optimize_intro.html#jupyter-notebook",
    "href": "003_scipy_optimize_intro.html#jupyter-notebook",
    "title": "3  Introduction to scipy.optimize",
    "section": "3.5 Jupyter Notebook",
    "text": "3.5 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to `scipy.optimize`</span>"
    ]
  },
  {
    "objectID": "004_spot_sklearn_optimization.html",
    "href": "004_spot_sklearn_optimization.html",
    "title": "4  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "",
    "text": "4.1 The Objective Function Branin\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula. Here we will use the Branin function. The 2-dim Branin function is\nfrom spotPython.fun.objectivefunctions import analytical\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical(seed=123).fun_branin",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sequential Parameter Optimization: Using `scipy` Optimizers</span>"
    ]
  },
  {
    "objectID": "004_spot_sklearn_optimization.html#the-objective-function-branin",
    "href": "004_spot_sklearn_optimization.html#the-objective-function-branin",
    "title": "4  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "",
    "text": "$$y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * cos(x1) + s,$$ \nwhere values of a, b, c, r, s and t are: \n$a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10$ and $t = 1 / (8*pi)$.\n\nIt has three global minima:\n\\(f(x) = 0.397887\\) at \\((-\\pi, 12.275)\\), \\((\\pi, 2.275)\\), and \\((9.42478, 2.475)\\).\nInput Domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15].",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sequential Parameter Optimization: Using `scipy` Optimizers</span>"
    ]
  },
  {
    "objectID": "004_spot_sklearn_optimization.html#the-optimizer",
    "href": "004_spot_sklearn_optimization.html#the-optimizer",
    "title": "4  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "4.2 The Optimizer",
    "text": "4.2 The Optimizer\nDifferential Evolution (DE) from the scikit.optimize package, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution is the default optimizer for the search on the surrogate. Other optimiers that are available in spotPython, see https://docs.scipy.org/doc/scipy/reference/optimize.html#global-optimization.\n\ndual_annealing\ndirect\nshgo\nbasinhopping\n\nThese optimizers can be selected as follows:\nsurrogate_control = \"model_optimizer\": differential_evolution\nAs noted above, we will use differential_evolution. The optimizer can use 1000 evaluations. This value will be passed to the differential_evolution method, which has the argument maxiter (int). It defines the maximum number of generations over which the entire differential evolution population is evolved, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution\n\n\n\n\n\n\nTensorBoard\n\n\n\nSimilar to the one-dimensional case, which is discussed in Section 7.5, we can use TensorBoard to monitor the progress of the optimization. We will use a similar code, only the prefix is different:\n\nfun_control=fun_control_init(\n                    lower = lower,\n                    upper = upper,\n                    fun_evals = 20,\n                    PREFIX = \"04_DE_\"\n                    )\nsurrogate_control=surrogate_control_init(\n                    n_theta=len(lower))\n\nCreated spot_tensorboard_path: runs/spot_logs/04_DE__p040025_2024-01-15_00-05-34 for SummaryWriter()\n\n\n\n\n\nspot_de = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    surrogate_control=surrogate_control)\nspot_de.run()\n\nspotPython tuning: 3.8004644561334935 [######----] 55.00% \nspotPython tuning: 3.8004644561334935 [######----] 60.00% \nspotPython tuning: 3.1590379739505225 [######----] 65.00% \nspotPython tuning: 3.1345599589760926 [#######---] 70.00% \nspotPython tuning: 2.8987595919440583 [########--] 75.00% \nspotPython tuning: 0.4124604824941809 [########--] 80.00% \nspotPython tuning: 0.40391426855740775 [########--] 85.00% \nspotPython tuning: 0.3990718447916741 [#########-] 90.00% \nspotPython tuning: 0.3990718447916741 [##########] 95.00% \nspotPython tuning: 0.3990718447916741 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d2194ad0&gt;\n\n\n\n4.2.1 TensorBoard\nIf the prefix argument in fun_control_init()is not None (as above, where the prefix was set to 04_DE_) , we can start TensorBoard in the background with the following command:\ntensorboard --logdir=\"./runs\"\nWe can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\nThe TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model.",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sequential Parameter Optimization: Using `scipy` Optimizers</span>"
    ]
  },
  {
    "objectID": "004_spot_sklearn_optimization.html#print-the-results",
    "href": "004_spot_sklearn_optimization.html#print-the-results",
    "title": "4  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "4.3 Print the Results",
    "text": "4.3 Print the Results\n\nspot_de.print_results()\n\nmin y: 0.3990718447916741\nx0: 3.149600915888656\nx1: 2.2983701643039107\n\n\n[['x0', 3.149600915888656], ['x1', 2.2983701643039107]]",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sequential Parameter Optimization: Using `scipy` Optimizers</span>"
    ]
  },
  {
    "objectID": "004_spot_sklearn_optimization.html#show-the-progress",
    "href": "004_spot_sklearn_optimization.html#show-the-progress",
    "title": "4  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "4.4 Show the Progress",
    "text": "4.4 Show the Progress\n\nspot_de.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_de.surrogate.plot()",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sequential Parameter Optimization: Using `scipy` Optimizers</span>"
    ]
  },
  {
    "objectID": "004_spot_sklearn_optimization.html#exercises",
    "href": "004_spot_sklearn_optimization.html#exercises",
    "title": "4  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\n4.5.1 dual_annealing\n\nDescribe the optimization algorithm, see scipy.optimize.dual_annealing.\nUse the algorithm as an optimizer on the surrogate.\n\n\n\n\n\n\n\nTip: Selecting the Optimizer for the Surrogate\n\n\n\nWe can run spotPython with the dual_annealing optimizer as follows:\n\nspot_da = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    optimizer=dual_annealing,\n                    surrogate_control=surrogate_control)\nspot_da.run()\nspot_da.print_results()\nspot_da.plot_progress(log_y=True)\nspot_da.surrogate.plot()\n\nspotPython tuning: 3.800452934057194 [######----] 55.00% \nspotPython tuning: 3.800452934057194 [######----] 60.00% \nspotPython tuning: 3.1590242778566413 [######----] 65.00% \nspotPython tuning: 3.1341475332648105 [#######---] 70.00% \nspotPython tuning: 2.8915909597236436 [########--] 75.00% \nspotPython tuning: 0.4195069442130439 [########--] 80.00% \nspotPython tuning: 0.401848680281649 [########--] 85.00% \nspotPython tuning: 0.3992571870039132 [#########-] 90.00% \nspotPython tuning: 0.3992571870039132 [##########] 95.00% \nspotPython tuning: 0.3992571870039132 [##########] 100.00% Done...\n\nmin y: 0.3992571870039132\nx0: 3.150936988317143\nx1: 2.2985561477641263\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 direct\n\nDescribe the optimization algorithm\nUse the algorithm as an optimizer on the surrogate\n\n\n\n\n\n\n\nTip: Selecting the Optimizer for the Surrogate\n\n\n\nWe can run spotPython with the direct optimizer as follows:\n\nspot_di = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    optimizer=direct,\n                    surrogate_control=surrogate_control)\nspot_di.run()\nspot_di.print_results()\nspot_di.plot_progress(log_y=True)\nspot_di.surrogate.plot()\n\nspotPython tuning: 3.812970247994418 [######----] 55.00% \nspotPython tuning: 3.812970247994418 [######----] 60.00% \nspotPython tuning: 3.162514679816068 [######----] 65.00% \nspotPython tuning: 3.1189615135325983 [#######---] 70.00% \nspotPython tuning: 2.6597698275013 [########--] 75.00% \nspotPython tuning: 0.3984917773445744 [########--] 80.00% \nspotPython tuning: 0.3984917773445744 [########--] 85.00% \nspotPython tuning: 0.3984917773445744 [#########-] 90.00% \nspotPython tuning: 0.3984917773445744 [##########] 95.00% \nspotPython tuning: 0.3984917773445744 [##########] 100.00% Done...\n\nmin y: 0.3984917773445744\nx0: 3.1378600823045257\nx1: 2.3010973936899863\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3 shgo\n\nDescribe the optimization algorithm\nUse the algorithm as an optimizer on the surrogate\n\n\n\n\n\n\n\nTip: Selecting the Optimizer for the Surrogate\n\n\n\nWe can run spotPython with the direct optimizer as follows:\n\nspot_sh = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    optimizer=shgo,\n                    surrogate_control=surrogate_control)\nspot_sh.run()\nspot_sh.print_results()\nspot_sh.plot_progress(log_y=True)\nspot_sh.surrogate.plot()\n\nspotPython tuning: 3.8004552384813834 [######----] 55.00% \nspotPython tuning: 3.8004552384813834 [######----] 60.00% \nspotPython tuning: 3.1590504084857294 [######----] 65.00% \nspotPython tuning: 3.1341080537914 [#######---] 70.00% \nspotPython tuning: 2.8853849830561646 [########--] 75.00% \nspotPython tuning: 0.4239413355798014 [########--] 80.00% \nspotPython tuning: 0.4016765366794104 [########--] 85.00% \nspotPython tuning: 0.3993233052368623 [#########-] 90.00% \nspotPython tuning: 0.3993233052368623 [##########] 95.00% \nspotPython tuning: 0.3993233052368623 [##########] 100.00% Done...\n\nmin y: 0.3993233052368623\nx0: 3.1514683455618786\nx1: 2.2984189502295536\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.4 basinhopping\n\nDescribe the optimization algorithm\nUse the algorithm as an optimizer on the surrogate\n\n\n\n\n\n\n\nTip: Selecting the Optimizer for the Surrogate\n\n\n\nWe can run spotPython with the direct optimizer as follows:\n\nspot_bh = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    optimizer=basinhopping,\n                    surrogate_control=surrogate_control)\nspot_bh.run()\nspot_bh.print_results()\nspot_bh.plot_progress(log_y=True)\nspot_bh.surrogate.plot()\n\nspotPython tuning: 3.80045375093536 [######----] 55.00% \nspotPython tuning: 3.80045375093536 [######----] 60.00% \nspotPython tuning: 3.159009257538889 [######----] 65.00% \nspotPython tuning: 3.1341512916720102 [#######---] 70.00% \nspotPython tuning: 2.8796407604155867 [########--] 75.00% \nspotPython tuning: 0.414633458827506 [########--] 80.00% \nspotPython tuning: 0.40117926479755717 [########--] 85.00% \nspotPython tuning: 0.3993792812618935 [#########-] 90.00% \nspotPython tuning: 0.3993792812618935 [##########] 95.00% \nspotPython tuning: 0.3993792812618935 [##########] 100.00% Done...\n\nmin y: 0.3993792812618935\nx0: 3.150848989866496\nx1: 2.3006645011798197\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.5 Performance Comparison\nCompare the performance and run time of the 5 different optimizers:\n\ndifferential_evolution\ndual_annealing\ndirect\nshgo\nbasinhopping.\n\nThe Branin function has three global minima:\n\n\\(f(x) = 0.397887\\) at\n\n\\((-\\pi, 12.275)\\),\n\\((\\pi, 2.275)\\), and\n\\((9.42478, 2.475)\\).\n\n\nWhich optima are found by the optimizers?\nDoes the seed argument in fun = analytical(seed=123).fun_branin change this behavior?",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sequential Parameter Optimization: Using `scipy` Optimizers</span>"
    ]
  },
  {
    "objectID": "004_spot_sklearn_optimization.html#jupyter-notebook",
    "href": "004_spot_sklearn_optimization.html#jupyter-notebook",
    "title": "4  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "4.6 Jupyter Notebook",
    "text": "4.6 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Optimization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sequential Parameter Optimization: Using `scipy` Optimizers</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html",
    "href": "005_num_rsm.html",
    "title": "5  Introduction: Numerical Methods",
    "section": "",
    "text": "5.1 Response Surface Methods: What is RSM?\nResponse Surface Methods (RSM) refer to a collection of statistical and mathematical tools that are valuable for developing, improving, and optimizing processes. The overarching theme of RSM involves studying how input variables that control a product or process can potentially influence a response that measures performance or quality characteristics.\nThe advantages of RSM include a rich literature, well-established methods often used in manufacturing, the importance of careful experimental design combined with a well-understood model, and the potential to add significant value to scientific inquiry, process refinement, optimization, and more. However, there are also drawbacks to RSM, such as the use of simple and crude surrogates, the hands-on nature of the methods, and the limitation of local methods.\nRSM is related to various fields, including Design of Experiments (DoE), quality management, reliability, and productivity. Its applications are widespread in industry and manufacturing, focusing on designing, developing, and formulating new products and improving existing ones, as well as from laboratory research. RSM is commonly applied in domains such as materials science, manufacturing, applied chemistry, climate science, and many others.\nAn example of RSM involves studying the relationship between a response variable, such as yield (\\(y\\)) in a chemical process, and two process variables: reaction time (\\(\\xi_1\\)) and reaction temperature (\\(\\xi_2\\)). The provided code illustrates this scenario, following a variation of the so-called “banana function.”\nIn the context of visualization, RSM offers the choice between 3D plots and contour plots. In a 3D plot, the independent variables \\(\\xi_1\\) and \\(\\xi_2\\) are represented, with \\(y\\) as the dependent variable.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef fun_rosen(x1, x2):\n    b = 10\n    return (x1-1)**2 + b*(x2-x1**2)**2\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nx = np.arange(-2.0, 2.0, 0.05)\ny = np.arange(-1.0, 3.0, 0.05)\nX, Y = np.meshgrid(x, y)\nzs = np.array(fun_rosen(np.ravel(X), np.ravel(Y)))\nZ = zs.reshape(X.shape)\n\nax.plot_surface(X, Y, Z)\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('Y')\n\nplt.show()\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-1.0, 3.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_rosen(X1, X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y , 50)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title(\"Rosenbrock's Banana Function\")\n\nText(0.5, 1.0, \"Rosenbrock's Banana Function\")",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#response-surface-methods-what-is-rsm",
    "href": "005_num_rsm.html#response-surface-methods-what-is-rsm",
    "title": "5  Introduction: Numerical Methods",
    "section": "",
    "text": "contour plot example:\n\n\\(x_1\\) and \\(x_2\\) are the independent variables\n\\(y\\) is the dependent variable\n\n\n\n\nVisual inspection: yield is optimized near \\((\\xi_1. \\xi_2)\\)\n\n\n5.1.1 Visualization: Problems in Practice\n\nTrue response surface is unknown in practice\nWhen yield evaluation is not as simple as a toy banana function, but a process requiring care to monitor, reconfigure and run, it’s far too expensive to observe over a dense grid\nAnd, measuring yield may be a noisy/inexact process\nThat’s where stats (RSM) comes in\n\n\n\n5.1.2 RSM: Strategies\n\nRSMs consist of experimental strategies for\nexploring the space of the process (i.e., independent/input) variables (above \\(\\xi_1\\) and \\(\\xi2)\\)\nempirical statistical modeling targeted toward development of an appropriate approximating relationship between the response (yield) and process variables local to a study region of interest\noptimization methods for sequential refinement in search of the levels or values of process variables that produce desirable responses (e.g., that maximize yield or explain variation)\nRSM used for fitting an Empirical Model\nTrue response surface driven by an unknown physical mechanism\nObservations corrupted by noise\nHelpful: fit an empirical model to output collected under different process configurations\nConsider response \\(Y\\) that depends on controllable input variables \\(\\xi_1, \\xi_2, \\ldots, \\xi_m\\)\nRSM: Equations of the Empirical Model\n\n\\(Y=f(\\xi_1, \\xi_2, \\ldots, \\xi_m) + \\epsilon\\)\n\\(\\mathbb{E}\\{Y\\} = \\eta = f(\\xi1_1, \\xi_2, \\ldots, \\xi_m)\\)\n\\(\\epsilon\\) is treated as zero mean idiosyncratic noise possibly representing\n\ninherent variation, or\nthe effect of other systems or\nvariables not under our purview at this time\n\n\n\n\n\n5.1.3 RSM: Noise in the Empirical Model\n\nTypical simplifying assumption: \\(\\epsilon \\sim N(0,\\sigma^2)\\)\nWe seek estimates for \\(f\\) and \\(\\sigma^2\\) from noisy observations \\(Y\\) at inputs \\(\\xi\\)\n\n\n\n5.1.4 RSM: Natural and Coded Variables\n\nInputs \\(\\xi_1, \\xi_2, \\ldots, \\xi_m\\) called natural variables:\n\nexpressed in natural units of measurement, e.g., degrees Celsius, pounds per square inch (psi), etc.\n\nTransformed to coded variables \\(x_1, x_2, \\ldots, x_m\\):\n\nto mitigate hassles and confusion that can arise when working with a multitude of scales of measurement\n\nTypical Transformations offering dimensionless inputs \\(x_1, x_2, \\ldots, x_m\\)\n\nin the unit cube, or\nscaled to have a mean of zero and standard deviation of one, are common choices.\n\nEmpirical model becomes \\(\\eta = f(x_1, x_2, \\ldots, x_m)\\)\n\n\n\n5.1.5 RSM Low-order Polynomials\n\nLow-order polynomial make the following simplifying Assumptions\n\nLearning about \\(f\\) is lots easier if we make some simplifying approximations\nAppealing to Taylor’s theorem, a low-order polynomial in a small, localized region of the input (\\(x\\)) space is one way forward\nClassical RSM:\n\ndisciplined application of local analysis and\nsequential refinement of locality through conservative extrapolation\n\nInherently a hands-on process",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#first-order-models-main-effects-model",
    "href": "005_num_rsm.html#first-order-models-main-effects-model",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.2 First-Order Models (Main Effects Model)",
    "text": "5.2 First-Order Models (Main Effects Model)\n\nFirst-order model (sometimes called main effects model) useful in parts of the input space where it’s believed that there’s little curvature in \\(f\\): \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\]\nFor example: \\[\\eta = 50 + 8 x_1 + 3x_2\\]\nIn practice, such a surface would be obtained by fitting a model to the outcome of a designed experiment\nFirst-Order Model in python Evaluated on a Grid\nEvaluate model on a grid in a double-unit square centered at the origin\nCoded units are chosen arbitrarily, although one can imagine deploying this approximating function nearby \\(x^{(0)} = (0,0)\\)\n\n\ndef fun_1(x1,x2):\n    return 50 + 8*x1 + 3*x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-1.0, 1.0, delta)\nx2 = np.arange(-1.0, 1.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_1(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('First Order Model: $50 + 8x_1 + 3x_2$')\n\nText(0.5, 1.0, 'First Order Model: $50 + 8x_1 + 3x_2$')\n\n\n\n\n\n\n\n\n\n\n5.2.1 First-Order Model Properties\n\nFirst-order model in 2d traces out a plane in \\(y \\times (x_1, x_2)\\) space\nOnly be appropriate for the most trivial of response surfaces, even when applied in a highly localized part of the input space\nAdding curvature is key to most applications:\n\nFirst-order model with interactions induces limited degree of curvature via different rates of change of \\(y\\) as \\(x_1\\) is varied for fixed \\(x_2\\), and vice versa: \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_{12} \\]\n\nFor example \\(\\eta = 50+8x_1+3x_2-4x_1x_2\\)\n\n\n\n5.2.2 First-order Model with Interactions in python\n\nCode below facilitates evaluations for pairs \\((x_1, x_2)\\)\nResponses may be observed over a mesh in the same double-unit square\n\n\ndef fun_11(x1,x2):\n    return 50 + 8 * x1 + 3 * x2 - 4 * x1 * x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_11(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('First Order Model with Interactions')\n\nText(0.5, 1.0, 'First Order Model with Interactions')\n\n\n\n\n\n\n\n\n\n\n\n5.2.3 Observations: First-Order Model with Interactions\n\nMean response \\(\\eta\\) is increasing marginally in both \\(x_1\\) and \\(x_2\\), or conditional on a fixed value of the other until \\(x_1\\) is 0.75\nRate of increase slows as both coordinates grow simultaneously since the coefficient in front of the interaction term \\(x_1 x_2\\) is negative\nCompared to the first-order model (without interactions): surface is far more useful locally\nLeast squares regressions often flag up significant interactions when fit to data collected on a design far from local optima",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#second-order-models",
    "href": "005_num_rsm.html#second-order-models",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.3 Second-Order Models",
    "text": "5.3 Second-Order Models\n\nSecond-order model may be appropriate near local optima where \\(f\\) would have substantial curvature: \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2  + \\beta_{11}x_1^2 + \\beta_{22}x^2 + \\beta_{12} x_1 x_2\\]\nFor example \\[\\eta = 50 + 8 x_1 + 3x_2 - 7x_1^2 - 3 x_2^2 - 4x_1x_2\\]\nImplementation of the Second-Order Model as fun_2().\n\n\ndef fun_2(x1,x2):\n    return 50 + 8 * x1 + 3 * x2 - 7 * x1**2 - 3*x2**2 - 4 * x1 * x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_2(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')\n\nText(0.5, 1.0, 'Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')\n\n\n\n\n\n\n\n\n\n\n5.3.1 Second-Order Models: Properties\n\nNot all second-order models would have a single stationary point (in RSM jargon called “a simple maximum”)\nIn “yield maximizing” setting we’re presuming response surface is concave down from a global viewpoint\n\neven though local dynamics may be more nuanced\n\nExact criteria depend upon the eigenvalues of a certain matrix built from those coefficients\nBox and Draper (2007) provide a diagram categorizing all of the kinds of second-order surfaces in RSM analysis, where finding local maxima is the goal\n\n\n\n5.3.2 Example: Stationary Ridge\n\nExample set of coefficients describing what’s called a stationary ridge is provided by the code below\n\n\ndef fun_ridge(x1, x2):\n    return 80 + 4*x1 + 8*x2 - 3*x1**2 - 12*x2**2 - 12*x1*x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_ridge(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Example of a stationary ridge')\n\nText(0.5, 1.0, 'Example of a stationary ridge')\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Observations: Second-Order Model (Ridge)\n\nRidge: a whole line of stationary points corresponding to maxima\nSituation means that the practitioner has some flexibility when it comes to optimizing:\n\ncan choose the precise setting of \\((x_1, x_2)\\) either arbitrarily or (more commonly) by consulting some tertiary criteria\n\n\n\n\n5.3.4 Example: Rising Ridge\n\nAn example of a rising ridge is implemented by the code below.\n\n\ndef fun_ridge_rise(x1, x2):\n     return 80 - 4*x1 + 12*x2 - 3*x1**2 - 12*x2**2 - 12*x1*x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_ridge_rise(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Rising ridge: $\\\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')\n\nText(0.5, 1.0, 'Rising ridge: $\\\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')\n\n\n\n\n\n\n\n\n\n\n\n5.3.5 Summary: Rising Ridge\n\nThe stationary point is remote to the study region\nCcontinuum of (local) stationary points along any line going through the 2d space, excepting one that lies directly on the ridge\nAlthough estimated response will increase while moving along the axis of symmetry toward its stationary point, this situation indicates\n\neither a poor fit by the approximating second-order function, or\nthat the study region is not yet precisely in the vicinity of a local optima—often both.\n\n\n\n\n5.3.6 Falling Ridge\n\nInversion of a rising ridge is a falling ridge\nSimilarly indicating one is far from local optima, except that the response decreases as you move toward the stationary point\nFinding a falling ridge system can be a back-to-the-drawing-board affair.\n\n\n\n5.3.7 Saddle Point\n\nFinally, we can get what’s called a saddle or minimax system.\n\n\ndef fun_saddle(x1, x2):\n    return 80 + 4*x1 + 8*x2 - 2*x2**2 - 12*x1*x2 \n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_saddle(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Saddle Point: $\\\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')\n\nText(0.5, 1.0, 'Saddle Point: $\\\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')\n\n\n\n\n\n\n\n\n\n\n\n5.3.8 Interpretation: Saddle Points\n\nLikely further data collection, and/or outside expertise, is needed before determining a course of action in this situation\n\n\n\n5.3.9 Summary: Ridge Analysis\n\nFinding a simple maximum, or stationary ridge, represents ideals in the spectrum of second-order approximating functions\nBut getting there can be a bit of a slog\nUsing models fitted from data means uncertainty due to noise, and therefore uncertainty in the type of fitted second-order model\nA ridge analysis attempts to offer a principled approach to navigating uncertainties when one is seeking local maxima\nThe two-dimensional setting exemplified above is convenient for visualization, but rare in practice\nComplications compound when studying the effect of more than two process variables",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#general-rsm-models",
    "href": "005_num_rsm.html#general-rsm-models",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.4 General RSM Models",
    "text": "5.4 General RSM Models\n\nGeneral first-order model on \\(m\\) process variables \\(x_1, x_2, \\cdots, x_m\\) is \\[\\eta = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_m x_m\\]\nGeneral second-order model on \\(m\\) process variables \\[\n\\eta= \\beta_0 + \\sum_{j=1}^m + \\sum_{j=1}^m x_j^2 + \\sum_{j=2}^m \\sum_{k=1}^j \\beta_{kj}x_k x_j.\n\\]\n\n\n5.4.1 Ordinary Least Squares\n\nInference from data is carried out by ordinary least squares (OLS)\nFor an excellent review including R examples, see Sheather (2009)\nOLS and maximum likelihood estimators (MLEs) are in the typical Gaussian linear modeling setup basically equivalent",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#designs",
    "href": "005_num_rsm.html#designs",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.5 Designs",
    "text": "5.5 Designs\n\nImportant: Organize the data collection phase of a response surface study carefully\nDesign: choice of \\(x\\)’s where we plan to observe \\(y\\)’s, for the purpose of approximating \\(f\\)\nAnalyses and designs need to be carefully matched\nWhen using a first-order model, some designs are preferred over others\nWhen using a second-order model to capture curvature, a different sort of design is appropriate\nDesign choices often contain features enabling modeling assumptions to be challenged\n\ne.g., to check if initial impressions are supported by the data ultimately collected\n\n\n\n5.5.1 Different Designs\n\nScreening desings: determine which variables matter so that subsequent experiments may be smaller and/or more focused\nThen there are designs tailored to the form of model (first- or second-order, say) in the screened variables\nAnd then there are more designs still",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#rsm-experimentation",
    "href": "005_num_rsm.html#rsm-experimentation",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.6 RSM Experimentation",
    "text": "5.6 RSM Experimentation\n\n5.6.1 First Step\n\nRSM-based experimentation begins with a first-order model, possibly with interactions\nPresumption: current process operating far from optimal conditions\nCollect data and apply method of steepest ascent (gradient) on fitted surfaces to move to the optimum\n\n\n\n5.6.2 Second Step\n\nEventually, if all goes well after several such carefully iterated refinements, second-order models are used on appropriate designs in order to zero-in on ideal operating conditions\nCareful analysis of the fitted surface:\n\nRidge analysis with further refinement using gradients of, and\nstandard errors associated with, the fitted surfaces, and so on\n\n\n\n\n5.6.3 Third Step\n\nOnce the practitioner is satisfied with the full arc of\n\ndesign(s),\nfit(s), and\ndecision(s):\n\nA small experiment called confirmation test may be performed to check if the predicted optimal settings are realizable in practice",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#rsm-review-and-general-considerations",
    "href": "005_num_rsm.html#rsm-review-and-general-considerations",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.7 RSM: Review and General Considerations",
    "text": "5.7 RSM: Review and General Considerations\n\nFirst Glimpse, RSM seems sensible, and pretty straightforward as quantitative statistics-based analysis goes\nBut: RSM can get complicated, especially when input dimensions are not very low\nDesign considerations are particularly nuanced, since the goal is to obtain reliable estimates of main effects, interaction, and curvature while minimizing sampling effort/expense\nRSM Downside: Inefficiency\n\nDespite intuitive appeal, several RSM downsides become apparent upon reflection\nProblems in practice\nStepwise nature of sequential decision making is inefficient:\n\nNot obvious how to re-use or update analysis from earlier phases, or couple with data from other sources/related experiments\n\n\nRSM Downside: Locality\n\nIn addition to being local in experiment-time (stepwise approach), it’s local in experiment-space\nBalance between\n\nexploration (maybe we’re barking up the wrong tree) and\nexploitation (let’s make things a little better) is modest at best\n\n\nRSM Downside: Expert Knowledge\n\nInterjection of expert knowledge is limited to hunches about relevant variables (i.e., the screening phase), where to initialize search, how to design the experiments\nYet at the same time classical RSMs rely heavily on constant examination throughout stages of modeling and design and on the instincts of seasoned practitioners\n\nRSM Downside: Replicability\n\nParallel analyses, conducted according to the same best intentions, rarely lead to the same designs, model fits and so on\nSometimes that means they lead to different conclusions, which can be cause for concern\n\n\n\n5.7.1 Historical Considerations about RSM\n\nIn spite of those criticisms, however, there was historically little impetus to revise the status quo\nClassical RSM was comfortable in its skin, consistently led to improvements or compelling evidence that none can reasonably be expected\nBut then in the late 20th century came an explosive expansion in computational capability, and with it a means of addressing many of those downsides\n\n\n\n5.7.2 Status Quo\n\nNowadays, field experiments and statistical models, designs and optimizations are coupled with with mathematical models\nSimple equations are not regarded as sufficient to describe real-world systems anymore\nPhysicists figured that out fifty years ago; industrial engineers followed, biologists, social scientists, climate scientists and weather forecasters, etc.\nSystems of equations are required, solved over meshes (e.g., finite elements), or stochastically interacting agents\nGoals for those simulation experiments are as diverse as their underlying dynamics\nOptimization of systems is common, e.g., to identify worst-case scenarios\n\n\n\n5.7.3 The Role of Statistics\n\nSolving systems of equations, or interacting agents, requires computing\nStatistics involved at various stages:\n\nchoosing the mathematical model\nsolving by stochastic simulation (Monte Carlo)\ndesigning the computer experiment\nsmoothing over idiosyncrasies or noise\nfinding optimal conditions, or\ncalibrating mathematical/computer models to data from field experiments\n\n\n\n\n5.7.4 New RSM is needed: DACE\n\nClassical RSMs are not well-suited to any of those tasks, because\n\nthey lack the fidelity required to model these data\ntheir intended application is too local\nthey’re also too hands-on.\n\nOnce computers are involved, a natural inclination is to automate—to remove humans from the loop and set the computer running on the analysis in order to maximize computing throughput, or minimize idle time\nDesign and Analysis of Computer Experiments as a modern extension of RSM\nExperimentation is changing due to advances in machine learning\nGaussian process (GP) regression is the canonical surrogate model\nOrigins in geostatistics (gold mining)\nWide applicability in contexts where prediction is king\nMachine learners exposed GPs as powerful predictors for all sorts of tasks:\nfrom regression to classification,\nactive learning/sequential design,\nreinforcement learning and optimization,\nlatent variable modeling, and so on",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#exercises",
    "href": "005_num_rsm.html#exercises",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.8 Exercises",
    "text": "5.8 Exercises\n\nGenerate 3d Plots for the Contour Plots in this notebook.\nWrite a plot_3d function, that takes the objective function fun as an argument.\n\n\nIt should provide the following interface: plot_3d(fun).\n\n\nWrite a plot_contour function, that takes the objective function fun as an argument:\n\n\nIt should provide the following interface: plot_contour(fun).\n\n\nConsider further arguments that might be useful for both function, e.g., ranges, size, etc.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#jupyter-notebook",
    "href": "005_num_rsm.html#jupyter-notebook",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.9 Jupyter Notebook",
    "text": "5.9 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html",
    "href": "006_num_gp.html",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "",
    "text": "6.1 DACE and RSM\nMathematical models implemented in computer codes are used to circumvent the need for expensive field data collection. These models are particularly useful when dealing with highly nonlinear response surfaces, high signal-to-noise ratios (which often involve deterministic evaluations), and a global scope. As a result, a new approach is required in comparison to Response Surface Methodology (RSM).\nWith the improvement in computing power and simulation fidelity, researchers gain higher confidence and a better understanding of the dynamics in physical, biological, and social systems. However, the expansion of configuration spaces and increasing input dimensions necessitates more extensive designs. High-performance computing (HPC) allows for thousands of runs, whereas previously only tens were possible. This shift towards larger models and training data presents new computational challenges.\nResearch questions for DACE (Design and Analysis of Computer Experiments) include how to design computer experiments that make efficient use of computation and how to meta-model computer codes to save on simulation effort. The choice of surrogate model for computer codes significantly impacts the optimal experiment design, and the preferred model-design pairs can vary depending on the specific goal.\nThe combination of computer simulation, design, and modeling with field data from similar real-world experiments introduces a new category of computer model tuning problems. The ultimate goal is to automate these processes to the greatest extent possible, allowing for the deployment of HPC with minimal human intervention.\nOne of the remaining differences between RSM and DACE lies in how they handle noise. DACE employs replication, a technique that would not be used in a deterministic setting, to separate signal from noise. Traditional RSM is best suited for situations where a substantial proportion of the variability in the data is due to noise, and where the acquisition of data values can be severely limited. Consequently, RSM is better suited for a different class of problems, aligning with its intended purposes.\nTwo very good texts on computer experiments and surrogate modeling are Santner, Williams, and Notz (2003) and Forrester, Sóbester, and Keane (2008). The former is the canonical reference in the statistics literature and the latter is perhaps more popular in engineering.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#background-expectation-mean-standard-deviation",
    "href": "006_num_gp.html#background-expectation-mean-standard-deviation",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.2 Background: Expectation, Mean, Standard Deviation",
    "text": "6.2 Background: Expectation, Mean, Standard Deviation\nThe distribution of a random vector is characterized by some indexes. One of them is the expected value, which is defined as \\[\nE[X] = \\sum_{x \\in D_X} xp_X(x)  \\qquad \\text{if $X$ is discrete}\n\\] \\[\nE[X] = \\int\\limits_{x \\in D_X} xf_X(x)\\mathrm{d}x  \\quad  \\text{if $X$ is continuous.}\n\\]\nThe mean, \\(\\mu\\), of a probability distribution is a measure of its central tendency or location. That is, \\(E(X)\\) is defined as the average of all possible values of \\(X\\), weighted by their probabilities.\n\n\n\n\n\n\nExample: Expectation\n\n\n\nLet \\(X\\) denote the number produced by rolling a fair die. Then \\[\nE(X) = 1 \\times 1/6 + 2 \\times 1/6 + 3 \\times 1/6 + 4 \\times 1/6 + 5 \\times 1/6 + 6\\times 1/6 = 3.5.\n\\]\n\n\n\n6.2.1 Sample Mean\nThe sample mean is an important estimate of the population mean. The sample mean of a sample \\(\\{x_i\\}\\) (\\(i=1,2,\\ldots,n\\)) is defined as \\[\\overline{x}  = \\frac{1}{n} \\sum_i x_i.\\]\n\n\n6.2.2 Variance and Standard Deviation\nIf we are trying to predict the value of a random variable \\(X\\) by its mean \\(\\mu = E(X)\\), the error will be \\(X-\\mu\\). In many situations it is useful to have an idea how large this deviation or error is. Since \\(E(X-\\mu) = E(X) -\\mu = 0\\), it is necessary to use the absolute value or the square of (\\(X-\\mu\\)). The squared error is the first choice, because the derivatives are easier to calculate. These considerations motivate the definition of the variance:\nThe variance of a random variable \\(X\\) is the mean squared deviation of \\(X\\) from its expected value \\(\\mu = E(X)\\). \\[\\begin{equation}\nVar(X) = E[ (X-\\mu)^2].\n\\end{equation}\\]\n\n\n6.2.3 Standard Deviation\nTaking the square root of the variance to get back to the same scale of units as \\(X\\) gives the standard deviation. The standard deviation of \\(X\\) is the square root of the variance of \\(X\\). \\[\\begin{equation}\nsd(X) = \\sqrt{Var(X)}.\n\\end{equation}\\]\n\n\n6.2.4 Calculation of the Standard Deviation with Python\nThe function numpy.std returns the standard deviation, a measure of the spread of a distribution, of the array elements. The argument ddof specifies the Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. By default ddof is zero, i.e., std uses the formula \\[\\begin{equation}  \\sqrt{  \\frac{1}{N} \\sum_i \\left( x_i - \\bar{x} \\right)^2  } \\qquad \\text{with } \\quad \\bar{x} = \\sum_{i=1}^N x_i /N. \\end{equation}\\]\n\n\n\n\n\n\nExample: Standard Deviation with Python\n\n\n\nConsider the array \\([1,2,3]\\): Since \\(\\bar{x} = 2\\), the following value is computed: \\[ \\sqrt{1/3 \\times \\left( (1-2)^2 + (2-2)^2 + (3-2)^2  \\right)} = \\sqrt{2/3}.\\]\n\nimport numpy as np\na = np.array([[1, 2, 3]])\nnp.std(a)\n\n0.816496580927726\n\n\n\n\n\n\n6.2.5 The Empirical Standard Deviation\nThe empirical standard deviation (which uses \\(N-1\\)), \\(\\sqrt{1/2 \\times \\left( (1-2)^2 + (2-2)^2 + (3-2)^2  \\right)} = \\sqrt{2/2}\\), can be calculated as follows:\n\nnp.std(a, ddof=1)\n\n1.0\n\n\n\n\n6.2.6 The Argument “axis”\n\n\n\n\n\n\nAxes along which the standard deviation is computed\n\n\n\n\nWhen you compute np.std with axis=0, it calculates the standard deviation along the vertical axis, meaning it computes the standard deviation for each column of the array.\nOn the other hand, when you compute np.std with axis=1, it calculates the standard deviation along the horizontal axis, meaning it computes the standard deviation for each row of the array.\nIf the axis parameter is not specified, np.std computes the standard deviation of the flattened array.\n\n\n\n\nA = np.array([[1, 2], [3, 4]])\nA\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nnp.std(A)\n\n1.118033988749895\n\n\n\nnp.std(A, axis=0)\n\narray([1., 1.])\n\n\n\nnp.std(A, axis=1)\n\narray([0.5, 0.5])",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#data-types-and-precision-in-python",
    "href": "006_num_gp.html#data-types-and-precision-in-python",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.3 Data Types and Precision in Python",
    "text": "6.3 Data Types and Precision in Python\nWe consider single versus double precision in Python. In single precision, std() can be inaccurate:\n\na = np.zeros((2, 4*4), dtype=np.float32)\na[0, :] = 1.0\na[1, :] = 0.1\na \n\narray([[1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,\n        1. , 1. , 1. ],\n       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n        0.1, 0.1, 0.1]], dtype=float32)\n\n\n\nnp.std(a, axis=0)\n\narray([0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n       0.45, 0.45, 0.45, 0.45, 0.45], dtype=float32)\n\n\n\nnp.std(a, axis=1)\n\narray([0., 0.], dtype=float32)\n\n\n\nabs(0.45 - np.std(a))\n\n1.7881393421514957e-08\n\n\n\n\n\n\n\n\nFloat data types\n\n\n\n\nfloat32 and float64 are data types in numpy that specify the precision of floating point numbers.\nfloat32 is a single-precision floating point number that occupies 32 bits of memory. It has a precision of about 7 decimal digits.\nfloat64 is a double-precision floating point number that occupies 64 bits of memory. It has a precision of about 15 decimal digits.\nThe main difference between float32 and float64 is the precision and memory usage. float64 provides a higher precision but uses more memory, while float32 uses less memory but has a lower precision.\n\n\n\nComputing the standard deviation in float64 is more accurate (result may vary), see https://numpy.org/devdocs/reference/generated/numpy.std.html.\n\nabs(0.45 - np.std(a, dtype=np.float64))\n\n7.450580707946131e-10\n\n\n\n\n\n\n\n\nExample: 32 versus 64 bit\n\n\n\n\nimport numpy as np\n\n# Define a number\nnum = 0.123456789123456789\n\n# Convert to float32 and float64\nnum_float32 = np.float32(num)\nnum_float64 = np.float64(num)\n\n# Print the number in both formats\nprint(\"float32: \", num_float32)\nprint(\"float64: \", num_float64)\n\nfloat32:  0.12345679\nfloat64:  0.12345678912345678\n\n\n\n\nThe float32 data type in numpy represents a single-precision floating point number. It uses 32 bits of memory, which gives it a precision of about 7 decimal digits. On the other hand, float64 represents a double-precision floating point number. It uses 64 bits of memory, which gives it a precision of about 15 decimal digits.\nThe reason float32 shows fewer digits is because it has less precision due to using less memory. The bits of memory are used to store the sign, exponent, and fraction parts of the floating point number, and with fewer bits, you can represent fewer digits accurately.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#distributions-and-random-numbers-in-python",
    "href": "006_num_gp.html#distributions-and-random-numbers-in-python",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.4 Distributions and Random Numbers in Python",
    "text": "6.4 Distributions and Random Numbers in Python\nResults from computers are deterministic, so it sounds like a contradiction in terms to generate random numbers on a computer. Standard computers generate pseudo-randomnumbers, i.e., numbers that behave as if they were drawn randomly.\n\n\n\n\n\n\nDeterministic Random Numbers\n\n\n\n\nIdea: Generate deterministically numbers that look (behave) as if they were drawn randomly.\n\n\n\n\n6.4.1 The Uniform Distribution\nThe probability density function of the uniform distribution is defined as: \\[\nf_X(x) = \\frac{1}{b-a} \\qquad \\text{for $x \\in [a,b]$}.\n\\]\nGenerate 10 random numbers from a uniform distribution between \\(a=0\\) and \\(b=1\\):\n\nimport numpy as np\n# Initialize the random number generator\nrng = np.random.default_rng(seed=123456789)\nn = 10\nx = rng.uniform(low=0.0, high=1.0, size=n)\nx\n\narray([0.02771274, 0.90670006, 0.88139355, 0.62489728, 0.79071481,\n       0.82590801, 0.84170584, 0.47172795, 0.95722878, 0.94659153])\n\n\nGenerate 10,000 random numbers from a uniform distribution between 0 and 10 and plot a histogram of the numbers:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Initialize the random number generator\nrng = np.random.default_rng(seed=123456789)\n\n# Generate random numbers from a uniform distribution\nx = rng.uniform(low=0, high=10, size=10000)\n\n# Plot a histogram of the numbers\nplt.hist(x, bins=50, density=True, edgecolor='black')\nplt.title('Uniform Distribution [0,10]')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.4.2 The Normal Distribution\nThe probability density function of the normal distribution is defined as: \\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right),\n\\tag{6.1}\\] where: \\(\\mu\\) is the mean; \\(\\sigma\\) is the standard deviation.\nTo generate ten random numbers from a normal distribution, the following command can be used.\n\n# generate 10 random numbers between from a normal distribution\nimport numpy as np\nrng = np.random.default_rng()\nn = 10\nmu, sigma = 2, 0.1\nx = rng.normal(mu, sigma, n)\nx\n\narray([2.01365277, 1.91565258, 2.02488765, 1.81862553, 1.99630557,\n       1.9645506 , 2.04757209, 2.20215361, 1.81801555, 1.97304294])\n\n\nVerify the mean:\n\nabs(mu - np.mean(x))\n\n0.02255411169435284\n\n\nNote: To verify the standard deviation, we use ddof = 1 (empirical standard deviation):\n\nabs(sigma - np.std(x, ddof=1))\n\n0.012533756453466008\n\n\nA normally distributed random variable is a random variable whose associated probability distribution is the normal (or Gaussian) distribution. The normal distribution is a continuous probability distribution characterized by a symmetric bell-shaped curve.\nThe distribution is defined by two parameters: the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). The mean indicates the center of the distribution, while the standard deviation measures the spread or dispersion of the distribution.\nThis distribution is widely used in statistics and the natural and social sciences as a simple model for random variables with unknown distributions.\n\nplot_normal_distribution(mu=0, sigma=1, num_samples=10000)\n\n\n\n\n\n\n\n\n\n\n6.4.3 Visualization of the Standard Deviation\nThe standard deviation of normal distributed can be visualized in terms of the histogram of \\(X\\):\n\nabout 68% of the values will lie in the interval within one standard deviation of the mean\n95% lie within two standard deviation of the mean\nand 99.9% lie within 3 standard deviations of the mean.\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.4 Standardization of Random Variables\nTo compare statistical properties of random variables which use different units, it is a common practice to transform these random variables into standardized variables. If a random variable \\(X\\) has expectation \\(E(X) = \\mu\\) and standard deviation \\(sd(X) = \\sigma &gt;0\\), the random variable \\[\nX^{\\ast} = (X-\\mu)/\\sigma\n\\] is called \\(X\\) in standard units. It has \\(E(X^{\\ast}) = 0\\) and \\(sd(X^{\\ast}) =1\\).\n\n\n6.4.5 Realizations of a Normal Distribution\nRealizations of a normal distribution refers to the actual values that you get when you draw samples from a normal distribution. Each sample drawn from the distribution is a realization of that distribution.\nFor example, if you have a normal distribution with a mean of 0 and a standard deviation of 1, each number you draw from that distribution is a realization.\nHere’s a Python example:\n\nimport numpy as np\n\n# Define the parameters of the normal distribution\nmu = 0\nsigma = 1\n\n# Draw 10 samples (realizations) from the normal distribution\nrealizations = np.random.normal(mu, sigma, 10)\n\nprint(realizations)\n\n[ 0.48951662  0.23879586 -0.44811181 -0.610795   -2.02994507  0.60794659\n -0.35410888  0.15258149  0.50127485 -0.78640277]\n\n\nIn this code, np.random.normal generates 10 realizations of a normal distribution with a mean of 0 and a standard deviation of 1. The realizations array contains the actual values drawn from the distribution.\n\n\n6.4.6 The Multivariate Normal Distribution\nThe multivariate normal, multinormal, or Gaussian distribution serves as a generalization of the one-dimensional normal distribution to higher dimensions. We will consider \\(k\\)-dimensional random vectors \\(X = (X_1, X_2, \\ldots, X_k)\\). When drawing samples from this distribution, it results in a set of values represented as \\(\\{x_1, x_2, \\ldots, x_k\\}\\). To fully define this distribution, it is necessary to specify its mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\). These parameters are analogous to the mean, which represents the central location, and the variance (squared standard deviation) of the one-dimensional normal distribution introduced in Equation 6.1.\nIn the context of the multivariate normal distribution, the mean takes the form of a coordinate within an \\(k\\)-dimensional space. This coordinate represents the location where samples are most likely to be generated, akin to the peak of the bell curve in a one-dimensional or univariate normal distribution.\n\n\n\n\n\n\nCovariance of two random variables\n\n\n\nFor two random variables \\(X\\) and \\(Y\\), the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values: \\[\n\\operatorname{cov}(X, Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}\n\\]\nThe covariance within the multivariate normal distribution denotes the extent to which two variables vary together. The elements of the covariance matrix, such as \\(\\Sigma_{ij}\\), represent the covariances between the variables \\(x_i\\) and \\(x_j\\). These covariances describe how the different variables in the distribution are related to each other in terms of their variability.\nThe probability density function (PDF) of the multivariate normal distribution is defined as: \\[\nf_X(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (x-\\mu)^T\\Sigma^{-1} (x-\\mu)\\right),\n\\] where: \\(\\mu\\) is the \\(k \\times 1\\) mean vector; \\(\\Sigma\\) is the \\(k \\times k\\) covariance matrix. The covariance matrix \\(\\Sigma\\) is assumed to be positive definite, so that its determinant is strictly positive.\nFor discrete random variables, covariance can be written as: \\[\n\\operatorname{cov} (X,Y) = \\frac{1}{n}\\sum_{i=1}^n (x_i-E(X)) (y_i-E(Y)).\n\\]\n\n\nFigure 6.1 shows draws from a bivariate normal distribution with \\(\\mu = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\\) and \\(\\Sigma=\\begin{pmatrix} 9 & 4 \\\\ 4 & 9 \\end{pmatrix}\\).\n\nimport numpy as np\nrng = np.random.default_rng()\nimport matplotlib.pyplot as plt\nmean = [0, 0]\ncov = [[9, 4], [4, 9]]  # diagonal covariance\nx, y = rng.multivariate_normal(mean, cov, 1000).T\n# Create a scatter plot of the numbers\nplt.scatter(x, y, s=2)\nplt.axis('equal')\nplt.grid()\nplt.title(f\"Bivariate Normal. Mean zero and positive covariance: {cov}\")\nplt.show()\n\n\n\n\n\n\n\nFigure 6.1: Bivariate Normal. Mean zero and covariance \\(\\Sigma=\\begin{pmatrix} 9 & 4 \\\\ 4 & 9\\end{pmatrix}\\)\n\n\n\n\n\nThe covariance matrix of a bivariate normal distribution determines the shape, orientation, and spread of the distribution in the two-dimensional space.\nThe diagonal elements of the covariance matrix (\\(\\sigma_1^2\\), \\(\\sigma_2^2\\)) are the variances of the individual variables. They determine the spread of the distribution along each axis. A larger variance corresponds to a greater spread along that axis.\nThe off-diagonal elements of the covariance matrix (\\(\\sigma_{12}, \\sigma_{21}\\)) are the covariances between the variables. They determine the orientation and shape of the distribution. If the covariance is positive, the distribution is stretched along the line \\(y=x\\), indicating that the variables tend to increase together. If the covariance is negative, the distribution is stretched along the line \\(y=-x\\), indicating that one variable tends to decrease as the other increases. If the covariance is zero, the variables are uncorrelated and the distribution is axis-aligned.\nIn Figure 6.1, the variances are identical and the variables are correlated (covariance is 4), so the distribution is stretched along the line \\(y=x\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n\n# Parameters\nmu = np.array([0, 0])\ncov = np.array([[9, 4], [4, 9]])\n\n# Create grid and multivariate normal\nx = np.linspace(-10,10,100)\ny = np.linspace(-10,10,100)\nX, Y = np.meshgrid(x,y)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\nrv = multivariate_normal(mu, cov)\n\nfig = plt.figure()\nax = plt.axes(projection='3d')  \nsurf=ax.plot_surface(X, Y, rv.pdf(pos),cmap='viridis',linewidth=0)\nax.set_xlabel('X axis')\nax.set_ylabel('Y axis')\nax.set_zlabel('Z axis')\nax.set_title('Bivariate Normal Distribution')\nfig.colorbar(surf, shrink=0.5, aspect=10)\nplt.show()\n\n\n\n\n\n\n\nFigure 6.2: Bivariate Normal. Mean zero and covariance \\(\\Sigma=\\begin{pmatrix} 9 & 4 \\\\ 4 & 9\\end{pmatrix}\\)\n\n\n\n\n\n\n\n6.4.7 The Bivariate Normal Distribution with Mean Zero and Zero Covariances \\(\\sigma_{12} = \\sigma_{21} = 0\\)\n\\(\\Sigma=\\begin{pmatrix} 9 & 0 \\\\ 0 & 9\\end{pmatrix}\\)\n\n\n\n\n\n\n\n\nFigure 6.3: Bivariate Normal. Mean zero and covariance \\(\\Sigma=\\begin{pmatrix} 9 & 0 \\\\ 0 & 9\\end{pmatrix}\\)\n\n\n\n\n\n\n\n6.4.8 The Bivariate Normal Distribution with Mean Zero and Negative Covariances \\(\\sigma_{12} = \\sigma_{21} = -4\\)\n\\(\\Sigma=\\begin{pmatrix} 9 & -4 \\\\ -4 & 9\\end{pmatrix}\\)\n\n\n\n\n\n\n\n\nFigure 6.4: Bivariate Normal. Mean zero and covariance \\(\\Sigma=\\begin{pmatrix} 9 & -4 \\\\ -4 & 9\\end{pmatrix}\\)",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#cholesky-decomposition-and-positive-definite-matrices",
    "href": "006_num_gp.html#cholesky-decomposition-and-positive-definite-matrices",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.5 Cholesky Decomposition and Positive Definite Matrices",
    "text": "6.5 Cholesky Decomposition and Positive Definite Matrices\nThe covariance matrix must be positive definite for a multivariate normal distribution for a couple of reasons:\n\nSemidefinite vs Definite: A covariance matrix is always symmetric and positive semidefinite. However, for a multivariate normal distribution, it must be positive definite, not just semidefinite. This is because a positive semidefinite matrix can have zero eigenvalues, which would imply that some dimensions in the distribution have zero variance, collapsing the distribution in those dimensions. A positive definite matrix has all positive eigenvalues, ensuring that the distribution has positive variance in all dimensions.\nInvertibility: The multivariate normal distribution’s probability density function involves the inverse of the covariance matrix. If the covariance matrix is not positive definite, it may not be invertible, and the density function would be undefined.\n\nIn summary, the covariance matrix being positive definite ensures that the multivariate normal distribution is well-defined and has positive variance in all dimensions.\n\nimport numpy as np\n\ndef is_positive_definite(matrix):\n    return np.all(np.linalg.eigvals(matrix) &gt; 0)\n\nmatrix = np.array([[9, 4], [4, 9]])\nprint(is_positive_definite(matrix))  # Outputs: True\n\nTrue\n\n\nMore effficent (and check if symmetric) is based on Cholesky decomposition.\n\nimport numpy as np\n\ndef is_pd(K):\n    try:\n        np.linalg.cholesky(K)\n        return True\n    except np.linalg.linalg.LinAlgError as err:\n        if 'Matrix is not positive definite' in err.message:\n            return False\n        else:\n            raise\nmatrix = np.array([[9, 4], [4, 9]])\nprint(is_pd(matrix))  # Outputs: True\n\nTrue\n\n\n\n\n\n\n\n\nExample: Cholesky decomposition.\n\n\n\nlinalg.cholesky computes the Cholesky decomposition of a matrix, i.e., it computes a lower triangular matrix \\(L\\) such that \\(LL^T = A\\). If the matrix is not positive definite, an error (LinAlgError) is raised.\n\nimport numpy as np\n\n# Define a Hermitian, positive-definite matrix\nA = np.array([[9, 4], [4, 9]]) \n\n# Compute the Cholesky decomposition\nL = np.linalg.cholesky(A)\n\nprint(\"L = \\n\", L)\nprint(\"L*LT = \\n\", np.dot(L, L.T))\n\nL = \n [[3.         0.        ]\n [1.33333333 2.68741925]]\nL*LT = \n [[9. 4.]\n [4. 9.]]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#maximum-likelihood-estimation-multivariate-normal-distribution",
    "href": "006_num_gp.html#maximum-likelihood-estimation-multivariate-normal-distribution",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.6 Maximum Likelihood Estimation: Multivariate Normal Distribution",
    "text": "6.6 Maximum Likelihood Estimation: Multivariate Normal Distribution\nConsider the first \\(n\\) terms of an identically and independently distributed (i.i..d.) sequence \\({X^{(j)}}\\) of \\(k\\)-dimensional multivariate normal random vectors, i.e., \\(X^{(j)} \\sim N(\\mu, \\Sigma)\\), \\(j=1,2,\\ldots\\). The joint probability density function of the \\(j\\)-th term of the sequence is \\[\nf_X(x_j) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu)\\right),\n\\]\nwhere: \\(\\mu\\) is the \\(k \\times 1\\) mean vector; \\(\\Sigma\\) is the \\(k \\times k\\) covariance matrix. The covariance matrix \\(\\Sigma\\) is assumed to be positive definite, so that its determinant is strictly positive. We use \\(x_1, \\ldots x_n\\), i.e., the realizations of the first \\(n\\) random vectors in the sequence, to estimate the two unknown parameters \\(\\mu\\) and \\(\\Sigma\\).\nThe likelihood function is defined as the joint probability density function of the observed data, viewed as a function of the unknown parameters. Since the terms in the sequence are independent, their joint density is equal to the product of their marginal densities. As a consequence, the likelihood function can be written as the product of the individual densities:\n\\[\nL(\\mu, \\Sigma) = \\prod_{j=1}^n f_X(x_j) = \\prod_{j=1}^n \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu)\\right)\n\\] \\[\n= \\frac{1}{(2\\pi)^{nk/2} \\det(\\Sigma)^{n/2}} \\exp\\left(-\\frac{1}{2} \\sum_{j=1}^n (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu)\\right).\n\\] The log-likelihood function is \\[\n\\ell(\\mu, \\Sigma) = -\\frac{nk}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\det(\\Sigma)) - \\frac{1}{2} \\sum_{j=1}^n (x_j-\\mu)^T\\Sigma^{-1} (x_j-\\mu).\n\\] The likelihood function is well-defined only if \\(\\det(\\Sigma)&gt;0\\).",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#introduction-to-gaussian-processes",
    "href": "006_num_gp.html#introduction-to-gaussian-processes",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.7 Introduction to Gaussian Processes",
    "text": "6.7 Introduction to Gaussian Processes\nThe concept of GP (Gaussian Process) regression can be understood as a simple extension of linear modeling. It is worth noting that this approach goes by various names and acronyms, including “kriging,” a term derived from geostatistics, as introduced by Matheron in 1963. Additionally, it is referred to as Gaussian spatial modeling or a Gaussian stochastic process, and machine learning (ML) researchers often use the term Gaussian process regression (GPR). In all of these instances, the central focus is on regression. This involves training on both inputs and outputs, with the ultimate objective of making predictions and quantifying uncertainty (referred to as uncertainty quantification or UQ).\nHowever, it’s important to emphasize that GPs are not a universal solution for every problem. Specialized tools may outperform GPs in specific, non-generic contexts, and GPs have their own set of limitations that need to be considered.\n\n6.7.1 Gaussian Process Prior\nIn the context of GP, any finite collection of realizations, which is represented by \\(n\\) observations, is modeled as having a multivariate normal (MVN) distribution. The characteristics of these realizations can be fully described by two key parameters:\n\nTheir mean, denoted as an \\(n\\)-vector \\(\\mu\\).\nThe covariance matrix, denoted as an \\(n \\times n\\) matrix \\(\\Sigma\\). This covariance matrix encapsulates the relationships and variability between the individual realizations within the collection.\n\n\n\n6.7.2 Covariance Function\nThe covariance function is defined by inverse exponentiated squared Euclidean distance: \\[\n\\Sigma(\\vec{x}, \\vec{x}') = \\exp\\{ - || \\vec{x} - \\vec{x}'||^2 \\},\n\\] where \\(\\vec{x}\\) and \\(\\vec{x}'\\) are two points in the \\(k\\)-dimensional input space and \\(\\| \\cdot \\|\\) denotes the Euclidean distance, i.e., \\[\n|| \\vec{x} - \\vec{x}'||^2 = \\sum_{i=1}^k (x_i - x_i')^2.\n\\]\nAn 1-d example is shown in Figure 6.5.\n\nvisualize_inverse_exp_squared_distance(5, 0.0, [0.5, 1, 2.0])\n\n\n\n\n\n\n\nFigure 6.5: One-dim inverse exponentiated squared Euclidean distance\n\n\n\n\n\nThe covariance function is also referred to as the kernel function. The Gaussian kernel uses an additional parameter, \\(\\sigma^2\\), to control the rate of decay. This parameter is referred to as the length scale or the characteristic length scale. The covariance function is then defined as\n\\[\n\\Sigma(\\vec{x}, \\vec{x}') = \\exp\\{ - || \\vec{x} - \\vec{x}'||^2 / (2 \\sigma^2) \\}.\n\\tag{6.2}\\]\nThe covariance decays exponentially fast as \\(\\vec{x}\\) and \\(\\vec{x}'\\) become farther apart. Observe that\n\\[\n\\Sigma(\\vec{x},\\vec{x}) = 1\n\\] and\n\\[\n\\Sigma(\\vec{x}, \\vec{x}') &lt; 1\n\\] for \\(\\vec{x} \\neq \\vec{x}'\\). The function \\(\\Sigma(\\vec{x},\\vec{x}')\\) must be positive definite.\n\n\n\n\n\n\nPositive Definiteness\n\n\n\nPositive definiteness in the context of the covariance matrix \\(\\Sigma_n\\) is a fundamental requirement. It is determined by evaluating \\(\\Sigma(x_i, x_j)\\) at pairs of \\(n\\) \\(\\vec{x}\\)-values, denoted as \\(\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_n\\). The condition for positive definiteness is that for all \\(\\vec{x}\\) vectors that are not equal to zero, the expression \\(\\vec{x}^\\top \\Sigma_n \\vec{x}\\) must be greater than zero. This property is essential when intending to use \\(\\Sigma_n\\) as a covariance matrix in multivariate normal (MVN) analysis. It is analogous to the requirement in univariate Gaussian distributions where the variance parameter, \\(\\sigma^2\\), must be positive.\n\n\nGaussian Processes (GPs) can be effectively utilized to generate random data that follows a smooth functional relationship. The process involves the following steps:\n\nSelect a set of \\(\\vec{x}\\)-values, denoted as \\(\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_n\\).\nDefine the covariance matrix \\(\\Sigma_n\\) by evaluating \\(\\Sigma_n^{ij} = \\Sigma(\\vec{x}_i, \\vec{x}_j)\\) for \\(i, j = 1, 2, \\ldots, n\\).\nGenerate an \\(n\\)-variate realization \\(Y\\) that follows a multivariate normal distribution with a mean of zero and a covariance matrix \\(\\Sigma_n\\), expressed as \\(Y \\sim \\mathcal{N}_n(0, \\Sigma_n)\\).\nVisualize the result by plotting it in the \\(x\\)-\\(y\\) plane.\n\n\n\n6.7.3 Construction of the Covariance Matrix\nHere is an one-dimensional example. The process begins by creating an input grid using \\(\\vec{x}\\)-values. This grid consists of 100 elements, providing the basis for further analysis and visualization.\n\nimport numpy as np\nn = 100\nX = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)\n\nIn the context of this discussion, the construction of the covariance matrix, denoted as \\(\\Sigma_n\\), relies on the concept of inverse exponentiated squared Euclidean distances. However, it’s important to note that a modification is introduced later in the process. Specifically, the diagonal of the covariance matrix is augmented with a small value, represented as “eps” or \\(\\epsilon\\).\nThe reason for this augmentation is that while inverse exponentiated distances theoretically ensure the covariance matrix’s positive definiteness, in practical applications, the matrix can sometimes become numerically ill-conditioned. By adding a small value to the diagonal, such as \\(\\epsilon\\), this ill-conditioning issue is mitigated. In this context, \\(\\epsilon\\) is often referred to as “jitter.”\n\nimport numpy as np\nfrom numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\nfrom numpy.linalg import cholesky, solve\nfrom numpy.random import multivariate_normal\ndef build_Sigma(X, sigma2):\n    n = X.shape[0]\n    k = X.shape[1]\n    D = zeros((k, n, n))\n    for l in range(k):\n        for i in range(n):\n            for j in range(i, n):\n                D[l, i, j] = 1/(2*sigma2[l])*(X[i,l] - X[j,l])**2\n    D = sum(D)\n    D = D + D.T\n    return exp(-D)  \n\n\nsigma2 = np.array([1.0])\nSigma = build_Sigma(X, sigma2)\nnp.round(Sigma[:3,:], 3)\n\narray([[1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783, 0.726,\n        0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278, 0.236,\n        0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044, 0.034,\n        0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003, 0.002,\n        0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   ],\n       [0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783,\n        0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278,\n        0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044,\n        0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003,\n        0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   ],\n       [0.98 , 0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835,\n        0.783, 0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325,\n        0.278, 0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056,\n        0.044, 0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004,\n        0.003, 0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n        0.   ]])\n\n\n\nimport matplotlib.pyplot as plt\nplt.imshow(Sigma, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.7.4 Generation of Random Samples and Plotting the Realizations of the Random Function\nIn the context of the multivariate normal distribution, the next step is to utilize the previously constructed covariance matrix denoted as Sigma. It is used as an essential component in generating random samples from the multivariate normal distribution.\nThe function multivariate_normal is employed for this purpose. It serves as a random number generator specifically designed for the multivariate normal distribution. In this case, the mean of the distribution is set equal to mean, and the covariance matrix is provided as Psi. The argument size specifies the number of realizations, which, in this specific scenario, is set to one.\nBy default, the mean vector is initialized to zero. To match the number of samples, which is equivalent to the number of rows in the X and Sigma matrices, the argument zeros(n) is used, where n represents the number of samples (here taken from the size of the matrix, e.g.,: Sigma.shape[0]).\n\nrng = np.random.default_rng(seed=12345)\nY = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = 1, check_valid=\"raise\").reshape(-1,1)\nY.shape\n\n(100, 1)\n\n\nNow we can plot the results, i.e., a finite realization of the random function \\(Y()\\) under a GP prior with a particular covariance structure. We will plot those X and Y pairs as connected points on an \\(x\\)-\\(y\\) plane.\n\nimport matplotlib.pyplot as plt\nplt.plot(X, Y)\nplt.title(\"Realization of Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\nplt.show()\n\n\n\n\n\n\n\nFigure 6.6: Realization of one random function under a GP prior. sigma2: 1.0\n\n\n\n\n\n\nrng = np.random.default_rng(seed=12345)\nY = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = 3, check_valid=\"raise\")\nplt.plot(X, Y.T)\nplt.title(\"Realization of Three Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\nplt.show()\n\n\n\n\n\n\n\nFigure 6.7: Realization of three random functions under a GP prior. sigma2: 1.0\n\n\n\n\n\n\n\n6.7.5 Properties of the 1d Example\n\n6.7.5.1 Several Bumps:\nIn this analysis, we observe several bumps in the \\(x\\)-range of \\([0,10]\\). These bumps in the function occur because shorter distances exhibit high correlation, while longer distances tend to be essentially uncorrelated. This leads to variations in the function’s behavior:\n\nWhen \\(x\\) and \\(x'\\) are one \\(\\sigma\\) unit apart, the correlation is \\(\\exp\\left(-\\sigma^2 / (2\\sigma^2)\\right) = \\exp(-1/2) \\approx 0.61\\), i.e., a relative high correlation.\n\\(2\\sigma\\) apart means correlation \\(\\exp(− 2^2 /2) \\approx 0.14\\), i.e., only small correlation.\n\\(4\\sigma\\) apart means correlation \\(\\exp(− 4^2 /2) \\approx 0.0003\\), i.e., nearly no correlation—variables are considered independent for almost all practical application.\n\n\n\n6.7.5.2 Smoothness:\nThe function plotted in Figure 6.6 represents only a finite realization, which means that we have data for a limited number of pairs, specifically 100 points. These points appear smooth in a tactile sense because they are closely spaced, and the plot function connects the dots with lines to create the appearance of smoothness. The complete surface, which can be conceptually extended to an infinite realization over a compact domain, is exceptionally smooth in a calculus sense due to the covariance function’s property of being infinitely differentiable.\n\n\n6.7.5.3 Scale of Two:\nRegarding the scale of the \\(Y\\) values, they have a range of approximately \\([-2,2]\\), with a 95% probability of falling within this range. In standard statistical terms, 95% of the data points typically fall within two standard deviations of the mean, which is a common measure of the spread or range of data.\n\nimport numpy as np\nfrom numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\nfrom numpy.random import multivariate_normal\n\ndef build_Sigma(X, sigma2):\n    n = X.shape[0]\n    k = X.shape[1]\n    D = zeros((k, n, n))\n    for l in range(k):\n        for i in range(n):\n            for j in range(i, n):\n                D[l, i, j] = 1/(2*sigma2[l])*(X[i,l] - X[j,l])**2\n    D = sum(D)\n    D = D + D.T\n    return exp(-D)\n\ndef plot_mvn( a=0, b=10, sigma2=1.0, size=1, n=100, show=True):    \n    X = np.linspace(a, b, n, endpoint=False).reshape(-1,1)\n    sigma2 = np.array([sigma2])\n    Sigma = build_Sigma(X, sigma2)\n    rng = np.random.default_rng(seed=12345)\n    Y = rng.multivariate_normal(zeros(Sigma.shape[0]), Sigma, size = size, check_valid=\"raise\")\n    plt.plot(X, Y.T)\n    plt.title(\"Realization of Random Functions under a GP prior.\\n sigma2: {}\".format(sigma2[0]))\n    if show:\n        plt.show()\n\n\nplot_mvn(a=0, b=10, sigma2=10.0, size=3, n=250)\n\n\n\n\n\n\n\nFigure 6.8: Realization of Random Functions under a GP prior. sigma2: 10\n\n\n\n\n\n\nplot_mvn(a=0, b=10, sigma2=0.1, size=3, n=250)\n\n\n\n\n\n\n\nFigure 6.9: Realization of Random Functions under a GP prior. sigma2: 0.1",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#kriging-modeling-basics",
    "href": "006_num_gp.html#kriging-modeling-basics",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.8 Kriging: Modeling Basics",
    "text": "6.8 Kriging: Modeling Basics\n\n6.8.1 The Kriging Idea in a Nutshell\nWe consider observed data of an unknown function \\(f\\) at \\(n\\) points \\(x_1, \\ldots, x_n\\), see Figure 6.10. These measurements a considered as realizations of MVN random variables \\(Y_1, \\ldots, Y_n\\) with mean \\(\\mu\\) and covariance matrix \\(\\Sigma_n\\) as shown in Figure 6.7, Figure 6.8 or Figure 6.9. In Kriging, a more general covariance matrix (or equivalently, a correlation matrix \\(\\Psi\\)) is used, see Equation 6.3. Using a maximum likelihood approach, we can estimate the unknown parameters \\(\\mu\\) and \\(\\Sigma_n\\) from the data so that the likelihood function is maximized.\n\n\n\n\n\n\n\n\nFigure 6.10: Eight measurements of an unknown function\n\n\n\n\n\n\n\n6.8.2 The Kriging Basis Function\n\\(k\\)-dimensional basis functions of the form \\[\n\\psi(\\vec{x}^{(i)}, \\vec{x}^{(j)}) = \\exp \\left( - \\sum_{l=1}^k \\theta_l | x_{l}^{(i)} - x_{l}^{(j)} | ^{p_l} \\right)\n\\tag{6.3}\\] are used in a method known as Kriging. Note, \\(\\vec{x}^{(i)}\\) denotes the \\(k\\)-dim vector \\(\\vec{x}^{(i)}= (x_1^{(i)}, \\ldots, x_k^{(i)})^T\\).\nThe Kriging basis function is related to the 1-dim Gaussian basis function (Equation 6.2), which is defined as \\[\n\\Sigma(\\vec{x}^{(i)}, \\vec{x}^{(j)}) = \\exp\\{ - || \\vec{x}^{(i)} - \\vec{x}^{(j)}||^2 / (2\\sigma^2) \\}.\n\\tag{6.4}\\]\nThere are some differences between Gaussian basis functions and Kriging basis functions:\n\nWhere the Gaussian basis function has \\(1/(2\\sigma^2)\\), the Kriging basis has a vector \\(\\theta = [\\theta_1, \\theta_2, \\ldots, \\theta_k]^T\\).\nThe \\(\\theta\\) vector allows the width of the basis function to vary from dimension to dimension.\nIn the Gaussian basis function, the exponent is fixed at 2, Kriging allows this exponent \\(p_l\\) to vary (typically from 1 to 2).\n\n\n\n6.8.3 The Correlation Coefficient\nIn a bivariate normal distribution, the covariance matrix and the correlation coefficient are closely related. The covariance matrix \\(\\Sigma\\) for a bivariate normal distribution is a \\(2\\times 2\\) matrix that looks like this:\n\\[\n\\Sigma =\n\\begin{pmatrix}\n\\sigma_1^2 & \\sigma_{12}\\\\\n\\sigma_{21} & \\sigma_2^2\n\\end{pmatrix},\n\\] where \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are the variances of \\(X_1\\) and \\(X_2\\), and \\(\\sigma_{12} = \\sigma_{21}\\) is the covariance between \\(X_1\\) and \\(X_2\\).\nThe correlation coefficient, often denoted as \\(\\rho\\), is a normalized measure of the linear relationship between two variables. It is calculated from the covariance and the standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\) (or the square roots of the variances) of \\(X_1\\) and \\(X_2\\) as follows: \\[\n\\rho = \\sigma_{12} / (\\sqrt{\\sigma_1^2} \\times \\sqrt{\\sigma_2^2}) = \\sigma_{12} / (\\sigma_1 \\times \\sigma_2).\n\\]\nSo we can express the correlation coefficient \\(\\rho\\) in terms of the elements of the covariance matrix \\(\\Sigma\\). It can be interpreted as follows: The correlation coefficient ranges from -1 to 1. A value of 1 means that \\(X_1\\) and \\(X_2\\) are perfectly positively correlated, a value of -1 means they are perfectly negatively correlated, and a value of 0 means they are uncorrelated. This gives the same information as the covariance, but on a standardized scale that does not depend on the units of \\(X_1\\) and \\(X_2\\).\n\n\n6.8.4 Covariance Matrix and Correlation Matrix\n\n\n\n\n\n\nCovariance and Correlation (taken from Forrester, Sóbester, and Keane (2008))\n\n\n\nCovariance is a measure of the correlation between two or more sets of random variables.\n\\[\n\\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\n\\]\nFrom the covariance, we can derive the correlation\n\\[\n\\text{Corr}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X\\sigma_Y}.\n\\tag{6.5}\\]\nFor a vector of random variables\n\\[\nY =\n\\begin{pmatrix}\n(Y^{(l)}, \\ldots, Y^{(n)})\n\\end{pmatrix}^T\n\\]\nthe covariance matrix is a matrix of covariances between the random variables\n\\[\n\\Sigma =\n\\text{Cov}(Y, Y) =\n\\begin{pmatrix}\n\\text{Cov}(Y^{(1)}, Y^{(1)}) & \\ldots & \\text{Cov}(Y^{(1)}, Y^{(n)}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(Y^{(n)}, Y^{(1)}) & \\ldots & \\text{Cov}(Y^{(n)}, Y^{(n)})\n\\end{pmatrix},\n\\]\nand from Equation 6.5\n\\[\n\\text{Cov}(Y, Y) = \\sigma_Y^2 \\text{Cor}(Y, Y).\n\\]\n\n\nYou can compute the correlation matrix \\(\\Psi\\) from a covariance matrix \\(\\Sigma\\) in Python using the numpy library. The correlation matrix is computed by dividing each element of the covariance matrix by the product of the standard deviations of the corresponding variables.\nThe function covariance_to_correlation first computes the standard deviations of the variables with np.sqrt(np.diag(cov)). It then computes the correlation matrix by dividing each element of the covariance matrix by the product of the standard deviations of the corresponding variables with cov / np.outer(std_devs, std_devs).\n\nimport numpy as np\n\ndef covariance_to_correlation(cov):\n    # Compute standard deviations\n    std_devs = np.sqrt(np.diag(cov))\n    \n    # Compute correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n    \n    return corr\n\ncov = np.array([[9, -4], [-4, 9]])\nprint(covariance_to_correlation(cov))\n\n[[ 1.         -0.44444444]\n [-0.44444444  1.        ]]\n\n\n\n\n6.8.5 The Kriging Model\nConsider sample data \\(\\vec{X}\\) and \\(\\vec{y}\\) from \\(n\\) locations that are available in matrix form: \\(\\vec{X}\\) is a \\((n \\times k)\\) matrix, where \\(k\\) denotes the problem dimension and \\(\\vec{y}\\) is a \\((n\\times 1)\\) vector.\nThe observed responses \\(\\vec{y}\\) are considered as if they are from a stochastic process, which will be denoted as \\[\n\\begin{pmatrix}\n\\vec{Y}(\\vec{x}^{(1)})\\\\\n\\vdots\\\\\n\\vec{Y}(\\vec{x}^{(n)})\\\\\n\\end{pmatrix}.\n\\]\nThe set of random vectors (also referred to as a random field) has a mean of \\(\\vec{1} \\mu\\), which is a \\((n\\times 1)\\) vector.\n\n\n6.8.6 Correlations\nThe random vectors are correlated with each other using the basis function expression from Equation 6.3: \\[\n\\text{cor} \\left(\\vec{Y}(\\vec{x}^{(i)}),\\vec{Y}(\\vec{x}^{(l)}) \\right) = \\exp\\left\\{ - \\sum_{j=1}^k \\theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\\right\\}.\n\\]\nThe \\((n \\times n)\\) correlation matrix of the observed sample data is\n\\[\n\\vec{\\Psi} = \\begin{pmatrix}\n\\text{cor}\\left(\n\\vec{Y}(\\vec{x}^{(i)}),\n\\vec{Y}(\\vec{x}^{(l)})\n\\right) & \\ldots &\n\\text{cor}\\left(\n\\vec{Y}(\\vec{x}^{(i)}),\n\\vec{Y}(\\vec{x}^{(l)})\n\\right)\\\\\n\\vdots  & \\vdots &  \\vdots\\\\\n\\text{cor}\\left(\n\\vec{Y}(\\vec{x}^{(i)}),\n\\vec{Y}(\\vec{x}^{(l)})\n\\right)&\n\\ldots &\n\\text{cor}\\left(\n\\vec{Y}(\\vec{x}^{(i)}),\n\\vec{Y}(\\vec{x}^{(l)})\n\\right)\n\\end{pmatrix}.\n\\]\nNote: correlations depend on the absolute distances between sample points \\(|x_j^{(n)} - x_j^{(n)}|\\) and the parameters \\(p_j\\) and \\(\\theta_j\\).\nCorrelation is intuitive, because when two points move close together, then \\(|x_l^{(i)} - x_l| \\to 0\\) and \\(\\exp(-|x_l^{(i)} - x_l| \\to 1\\), points show very close correlation and \\(Y(x_l^{(i)}) = Y(x_l)\\).\n\\(\\theta\\) can be seen as a width parameter:\n\nlow \\(\\theta_j\\) means that all points will have a high correlation, with \\(Y(x_j)\\) being similar across the sample.\nhigh \\(\\theta_j\\) means that there is a significant difference between the \\(Y(x_j)\\)’s.\n\\(\\theta_j\\) is a measure of how active the function we are approximating is.\nHigh \\(\\theta_j\\) indicate important parameters, see Figure 6.11.\n\n\nvisualize_inverse_exp_squared_distance(5, 0, theta_values=[0.5, 1, 2.0])\n\n\n\n\n\n\n\nFigure 6.11: Theta set to 1/2, 1, and 2\n\n\n\n\n\n\n\n\n\n\n\nExample: The Correlation Matrix (Detailed Computation)\n\n\n\nLet \\(n=4\\) and \\(k=3\\). The sample plan is represented by the following matrix \\(X\\): \\[\nX = \\begin{pmatrix} x_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\\\\\nx_{31} & x_{32} & x_{33}\\\\\nx_{41} & x_{42} & x_{43}\\\\\n\\end{pmatrix}\n\\]\nTo compute the elements of the matrix \\(\\Psi\\), the following \\(k\\) (one for each of the \\(k\\) dimensions) \\((n,n)\\)-matrices have to be computed: \\[\nD_1 = \\begin{pmatrix} x_{11} - x_{11} & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\\\  x_{21} - x_{11} & x_{21} - x_{21} & x_{21} -x_{31} & x_{21} - x_{41} \\\\ x_{31} - x_{11} & x_{31} - x_{21} & x_{31} -x_{31} & x_{31} - x_{41} \\\\ x_{41} - x_{11} & x_{41} - x_{21} & x_{41} -x_{31} & x_{41} - x_{41} \\\\\n\\end{pmatrix}\n\\]\n\\[\nD_2 = \\begin{pmatrix} x_{12} - x_{12} & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\\\  x_{22} - x_{12} & x_{22} - x_{22} & x_{22} -x_{32} & x_{22} - x_{42} \\\\ x_{32} - x_{12} & x_{32} - x_{22} & x_{32} -x_{32} & x_{32} - x_{42} \\\\ x_{42} - x_{12} & x_{42} - x_{22} & x_{42} -x_{32} & x_{42} - x_{42} \\\\\n\\end{pmatrix}\n\\]\n\\[\nD_3 = \\begin{pmatrix} x_{13} - x_{13} & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\\\  x_{23} - x_{13} & x_{23} - x_{23} & x_{23} -x_{33} & x_{23} - x_{43} \\\\ x_{33} - x_{13} & x_{33} - x_{23} & x_{33} -x_{33} & x_{33} - x_{43} \\\\ x_{43} - x_{13} & x_{43} - x_{23} & x_{43} -x_{33} & x_{43} - x_{43} \\\\\\end{pmatrix}\n\\]\nSince the matrices are symmetric and the main diagonals are zero, it is sufficient to compute the following matrices: \\[\nD_1 = \\begin{pmatrix} 0 & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\\\  0 &  0 & x_{21} -x_{31} & x_{21} - x_{41} \\\\ 0 & 0 & 0 & x_{31} - x_{41} \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n\\] \\[\nD_2 = \\begin{pmatrix} 0 & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\\\  0 & 0 & x_{22} -x_{32} & x_{22} - x_{42} \\\\ 0 & 0 & 0 & x_{32} - x_{42} \\\\ 0 & 0 & 0 & 0 \\\\\n\\end{pmatrix}\n\\]\n\\[\nD_3 = \\begin{pmatrix} 0 & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\\\  0 & 0 & x_{23} -x_{33} & x_{23} - x_{43} \\\\ 0 & 0 & 0 & x_{33} - x_{43} \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n\\]\nWe will consider \\(p_l=2\\). The differences will be squared and multiplied by \\(\\theta_i\\), i.e.:\n\\[\nD_1 = \\theta_1 \\begin{pmatrix} 0 & (x_{11} - x_{21})^2 & (x_{11} -x_{31})^2 & (x_{11} - x_{41})^2 \\\\  0 &  0 & (x_{21} -x_{31})^2 & (x_{21} - x_{41})^2 \\\\ 0 & 0 & 0 & (x_{31} - x_{41})^2 \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n\\]\n\\[\nD_2 = \\theta_2 \\begin{pmatrix} 0 & (x_{12} - x_{22})^2 & (x_{12} -x_{32})^2 & (x_{12} - x_{42})^2 \\\\  0 & 0 & (x_{22} -x_{32})^2 & (x_{22} - x_{42})^2 \\\\ 0 & 0 & 0 & (x_{32} - x_{42})^2 \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n\\]\n\\[\nD_3 = \\theta_3 \\begin{pmatrix} 0 & (x_{13} - x_{23})^2 & (x_{13} -x_{33})^2 & (x_{13} - x_{43})^2 \\\\  0 & 0 & (x_{23} -x_{33})^2 & (x_{23} - x_{43})^2 \\\\ 0 & 0 & 0 & (x_{33} - x_{43})^2 \\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n\\]\nThe sum of the three matrices \\(D=D_1+ D_2 + D_3\\) will be calculated next:\n\\[\n\\begin{pmatrix} 0 &\n\\theta_1  (x_{11} - x_{21})^2 + \\theta_2 (x_{12} - x_{22})^2 + \\theta_3  (x_{13} - x_{23})^2  &\n\\theta_1 (x_{11} -x_{31})^2 + \\theta_2  (x_{12} -x_{32})^2 + \\theta_3  (x_{13} -x_{33})^2 &\n\\theta_1  (x_{11} - x_{41})^2 + \\theta_2  (x_{12} - x_{42})^2 + \\theta_3 (x_{13} - x_{43})^2\n\\\\  0 &  0 &\n\\theta_1  (x_{21} -x_{31})^2 + \\theta_2 (x_{22} -x_{32})^2 + \\theta_3  (x_{23} -x_{33})^2 &\n\\theta_1  x_{21} - x_{41})^2 + \\theta_2  (x_{22} - x_{42})^2 + \\theta_3 (x_{23} - x_{43})^2\n\\\\ 0 & 0 & 0 &\n\\theta_1 (x_{31} - x_{41})^2 + \\theta_2 (x_{32} - x_{42})^2 + \\theta_3 (x_{33} - x_{43})^2\n\\\\ 0 & 0 & 0 & 0 \\\\\\end{pmatrix}\n\\]\nFinally, \\[ \\Psi = \\exp(-D)\\] is computed.\nNext, we will demonstrate how this computation can be implemented in Python.\n\nfrom numpy import (array, zeros, power, ones, exp, multiply,\n                    eye, linspace, mat, spacing, sqrt, arange,\n                    append, ravel)\nfrom numpy.linalg import cholesky, solve\ntheta = np.array([1,2,3])\nX = np.array([ [1,0,0], [0,1,0], [100, 100, 100], [101, 100, 100]])\nX\n\narray([[  1,   0,   0],\n       [  0,   1,   0],\n       [100, 100, 100],\n       [101, 100, 100]])\n\n\n\ndef build_Psi(X, theta):\n    n = X.shape[0]\n    k = X.shape[1]\n    D = zeros((k, n, n))\n    for l in range(k):\n        for i in range(n):\n            for j in range(i, n):\n                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n    D = sum(D)\n    D = D + D.T\n    return exp(-D)  \n\n\nPsi = build_Psi(X, theta)\nPsi\n\narray([[1.        , 0.04978707, 0.        , 0.        ],\n       [0.04978707, 1.        , 0.        , 0.        ],\n       [0.        , 0.        , 1.        , 0.36787944],\n       [0.        , 0.        , 0.36787944, 1.        ]])\n\n\n\n\n\n\n\n\n\n\nExample: The Correlation Matrix (Using Existing Functions)\n\n\n\nThe same result as computed in the previous example can be obtained with existing python functions, e.g., from the package scipy.\n\nfrom scipy.spatial.distance import squareform\nfrom scipy.spatial.distance import pdist\n\ndef build_Psi(X, theta, eps=sqrt(spacing(1))):\n    return exp(- squareform(pdist(X,\n                            metric='sqeuclidean',\n                            out=None,\n                            w=theta))) +  multiply(eye(X.shape[0]),\n                                                   eps)\n\nPsi = build_Psi(X, theta, eps=.0)\nPsi\n\narray([[1.        , 0.04978707, 0.        , 0.        ],\n       [0.04978707, 1.        , 0.        , 0.        ],\n       [0.        , 0.        , 1.        , 0.36787944],\n       [0.        , 0.        , 0.36787944, 1.        ]])\n\n\n\n\n\n\n6.8.7 The Condition Number\nA small value, eps, can be passed to the function build_Psi to improve the condition number. For example, eps=sqrt(spacing(1)) can be used. The numpy function spacing() returns the distance between a number and its nearest adjacent number.\nThe condition number of a matrix is a measure of its sensitivity to small changes in its elements. It is used to estimate how much the output of a function will change if the input is slightly altered.\nA matrix with a low condition number is well-conditioned, which means its behavior is relatively stable, while a matrix with a high condition number is ill-conditioned, meaning its behavior is unstable with respect to numerical precision.\n\nimport numpy as np\n\n# Define a well-conditioned matrix (low condition number)\nA = np.array([[1, 0.1], [0.1, 1]])\nprint(\"Condition number of A: \", np.linalg.cond(A))\n\n# Define an ill-conditioned matrix (high condition number)\nB = np.array([[1, 0.99999999], [0.99999999, 1]])\nprint(\"Condition number of B: \", np.linalg.cond(B))\n\nCondition number of A:  1.2222222222222225\nCondition number of B:  200000000.53159264\n\n\n\nnp.linalg.cond(Psi)\n\n2.163953413738652\n\n\n\n\n6.8.8 MLE to estimate \\(\\theta\\) and \\(p\\)\nWe know what the correlations mean, but how do we estimate the values of \\(\\theta_j\\) and where does our observed data \\(y\\) come in? To estimate the values of \\(\\vec{\\theta}\\) and \\(\\vec{p}\\), they are chosen to maximize the likelihood of \\(\\vec{y}\\), which can be expressed in terms of the sample data \\[L\\left(\\vec{Y}(\\vec{x}^{(1)}), \\ldots, \\vec{Y}(\\vec{x}^{(n)}) | \\mu, \\sigma \\right) = \\frac{1}{(2\\pi \\sigma)^{n/2} |\\vec{\\Psi}|^{1/2}} \\exp\\left\\{ \\frac{-(\\vec{y} - \\vec{1}\\mu)^T \\vec{\\Psi}^{-1}(\\vec{y} - \\vec{1}\\mu) }{2 \\sigma^2}\\right\\},\\] and formulated as the log-likelihood: \\[\\ln(L) = - \\frac{n}{2} \\ln(2\\pi \\sigma) - \\frac{1}{2} \\ln |\\vec{\\Psi}| \\frac{-(\\vec{y} - \\vec{1}\\mu)^T \\vec{\\Psi}^{-1}(\\vec{y} - \\vec{1}\\mu) }{2 \\sigma^2}.\\]\nOptimization of the log-likelihood by taking derivatives with respect to \\(\\mu\\) and \\(\\sigma\\) results in \\[\\hat{\\mu} = \\frac{\\vec{1}^T \\vec{\\Psi}^{-1} \\vec{y}^T}{\\vec{1}^T \\vec{\\Psi}^{-1} \\vec{1}^T}\\] and \\[\\hat{\\sigma} = \\frac{(\\vec{y} - \\vec{1}\\mu)^T \\vec{\\Psi}^{-1}(\\vec{y} - \\vec{1}\\mu)}{n}.\\]\nCombining the equations leads to the concentrated log-likelihood: \\[\\ln(L) = - \\frac{n}{2} \\ln(\\hat{\\sigma}) - \\frac{1}{2} \\ln |\\vec{\\Psi}|. \\tag{6.6}\\]\n\n\n\n\n\n\nNote: The Concentrated Log-Likelihood\n\n\n\n\nThe first term in Equation 6.6 requires information about the measured point (observations) \\(y_i\\).\nTo maximize \\(\\ln(L)\\), optimal values of \\(\\vec{\\theta}\\) and \\(\\vec{p}\\) are determined numerically, because the equation is not differentiable.\n\n\n\n\n\n6.8.9 Tuning \\(\\theta\\) and \\(p\\)\nOptimizers such as Nelder-Mead, Conjugate Gradient, or Simulated Annealing can be used to determine optimal values for \\(\\theta\\) and \\(p\\). After the optimization, the correlation matrix \\(\\Psi\\) is build with the optimized \\(\\theta\\) and \\(p\\) values. This is best (most likely) Kriging model for the given data \\(y\\).",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#kriging-prediction",
    "href": "006_num_gp.html#kriging-prediction",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.9 Kriging Prediction",
    "text": "6.9 Kriging Prediction\n\n6.9.1 The Augmented Correlation Matrix\nWe will use the Kriging correlation \\(\\Psi\\) to predict new values based on the observed data. The matrix algebra involved for calculating the likelihood is the most computationally intensive part of the Kriging process. Care must be taken that the computer code is as efficient as possible.\nBasic elements of the Kriging based surrogate optimization such as interpolation, expected improvement, and regression are presented. The presentation follows the approach described in Forrester, Sóbester, and Keane (2008) and Bartz et al. (2022).\nMain idea for prediction is that the new \\(Y(\\vec{x})\\) should be consistent with the old sample data \\(X\\). For a new prediction \\(\\hat{y}\\) at \\(\\vec{x}\\), the value of \\(\\hat{y}\\) is chosen so that it maximizes the likelihood of the sample data \\(\\vec{X}\\) and the prediction, given the (optimized) correlation parameter \\(\\vec{\\theta}\\) and \\(\\vec{p}\\) from above. The observed data \\(\\vec{y}\\) is augmented with the new prediction \\(\\hat{y}\\) which results in the augmented vector \\(\\vec{\\tilde{y}} = ( \\vec{y}^T, \\hat{y})^T\\). A vector of correlations between the observed data and the new prediction is defined as\n\\[ \\vec{\\psi} = \\begin{pmatrix}\n\\text{cor}\\left(\n\\vec{Y}(\\vec{x}^{(1)}),\n\\vec{Y}(\\vec{x})\n\\right) \\\\\n\\vdots  \\\\\n\\text{cor}\\left(\n\\vec{Y}(\\vec{x}^{(n)}),\n\\vec{Y}(\\vec{x})\n\\right)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\vec{\\psi}^{(1)}\\\\\n\\vdots\\\\\n\\vec{\\psi}^{(n)}\n\\end{pmatrix}.\n\\] The augmented correlation matrix is constructed as \\[ \\tilde{\\vec{\\Psi}} =\n\\begin{pmatrix}\n\\vec{\\Psi} & \\vec{\\psi} \\\\\n\\vec{\\psi}^T & 1\n\\end{pmatrix}.\n\\]\nThe log-likelihood of the augmented data is \\[\n\\ln(L) = - \\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\hat{\\sigma}^2) - \\frac{1}{2} \\ln |\\vec{\\hat{\\Psi}}| -  \\frac{(\\vec{\\tilde{y}} - \\vec{1}\\hat{\\mu})^T \\vec{\\tilde{\\Psi}}^{-1}(\\vec{\\tilde{y}} - \\vec{1}\\hat{\\mu})}{2 \\hat{\\sigma}^2}.\n\\]\nThe MLE for \\(\\hat{y}\\) can be calculated as \\[\n\\hat{y}(\\vec{x}) = \\hat{\\mu} + \\vec{\\psi}^T \\vec{\\tilde{\\Psi}}^{-1} (\\vec{y} - \\vec{1}\\hat{\\mu}).\n\\tag{6.7}\\]\n\n\n6.9.2 Properties of the Predictor\nEquation 6.7 reveals two important properties of the Kriging predictor:\n\nBasis functions: The basis function impacts the vector \\(\\vec{\\psi}\\), which contains the \\(n\\) correlations between the new point \\(\\vec{x}\\) and the observed locations. Values from the \\(n\\) basis functions are added to a mean base term \\(\\mu\\) with weightings \\(\\vec{w} = \\vec{\\tilde{\\Psi}}^{(-1)} (\\vec{y} - \\vec{1}\\hat{\\mu})\\).\nInterpolation: The predictions interpolate the sample data. When calculating the prediction at the \\(i\\)th sample point, \\(\\vec{x}^{(i)}\\), the \\(i\\)th column of \\(\\vec{\\Psi}^{-1}\\) is \\(\\vec{\\psi}\\), and \\(\\vec{\\psi}  \\vec{\\Psi}^{-1}\\) is the \\(i\\)th unit vector. Hence, \\(\\hat{y}(\\vec{x}^{(i)}) = y^{(i)}\\).",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#kriging-example-sinusoid-function",
    "href": "006_num_gp.html#kriging-example-sinusoid-function",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.10 Kriging Example: Sinusoid Function",
    "text": "6.10 Kriging Example: Sinusoid Function\nToy example in 1d where the response is a simple sinusoid measured at eight equally spaced \\(x\\)-locations in the span of a single period of oscillation.\n\n6.10.1 Calculating the Correlation Matrix \\(\\Psi\\)\nThe correlation matrix \\(\\Psi\\) is based on the pairwise squared distances between the input locations. Here we will use \\(n=8\\) sample locations and \\(\\theta\\) is set to 1.0.\n\nn = 8\nX = np.linspace(0, 2*np.pi, n, endpoint=False).reshape(-1,1)\n# theta should be an array (of one value, for the moment, will be changed later)\ntheta = np.array([1.0])\nPsi = build_Psi(X, theta)\n\nEvaluate at sample points\n\ny = np.sin(X)\n\n\nimport matplotlib.pyplot as plt\nplt.plot(X, y, \"bo\")\nplt.title(f\"Sin(x) evaluated at {n} points\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.10.2 Computing the \\(\\psi\\) Vector\nDistances between testing locations \\(x\\) and training data locations \\(X\\).\n\nfrom scipy.spatial.distance import cdist\n\ndef build_psi(X, x, theta, eps=sqrt(spacing(1))):\n    n = X.shape[0]\n    k = X.shape[1]\n    m = x.shape[0]\n    psi = zeros((n, m))\n    theta = theta * ones(k)\n    D = zeros((n, m))\n    D = cdist(x.reshape(-1, k),\n              X.reshape(-1, k),\n              metric='sqeuclidean',\n              out=None,\n              w=theta)\n    print(D.shape)\n    psi = exp(-D)\n    # return psi transpose to be consistent with the literature\n    return(psi.T)\n\n\n\n6.10.3 Predicting at New Locations\nWe would like to predict at \\(m = 100\\) new locations in the interval \\([0, 2\\pi]\\). The new locations are stored in the variable x.\n\nm = 100\nx = np.linspace(0, 2*np.pi, m, endpoint=False).reshape(-1,1)\npsi = build_psi(X, x, theta)\n\n(100, 8)\n\n\nComputation of the predictive equations.\n\nU = cholesky(Psi).T\none = np.ones(n).reshape(-1,1)\nmu = (one.T.dot(solve(U, solve(U.T, y)))) / one.T.dot(solve(U, solve(U.T, one)))\nf = mu * ones(m).reshape(-1,1) + psi.T.dot(solve(U, solve(U.T, y - one * mu)))\n\nTo compute \\(f\\), Equation 6.7 is used.\n\n\n6.10.4 Visualization\n\nimport matplotlib.pyplot as plt\nplt.plot(x, f, color = \"orange\", label=\"Fitted\")\nplt.plot(x, np.sin(x), color = \"grey\", label=\"Original\")\nplt.plot(X, y, \"bo\", label=\"Measurements\")\nplt.title(\"Kriging prediction of sin(x) with {} points.\\n theta: {}\".format(n, theta[0]))\nplt.legend(loc='upper right')\nplt.show()",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#cholesky-example-with-two-points",
    "href": "006_num_gp.html#cholesky-example-with-two-points",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.11 Cholesky Example With Two Points",
    "text": "6.11 Cholesky Example With Two Points\n\n6.11.1 Cholesky Decomposition\nWe consider \\(k=1\\) and \\(n=2\\) sample points. The sample points are located at \\(x_1=1\\) and \\(x_2=5\\). The response values are \\(y_1=2\\) and \\(y_2=10\\). The correlation parameter is \\(\\theta=1\\) and \\(p\\) is set to \\(1\\). Using Equation 6.3, we can compute the correlation matrix \\(\\Psi\\):\n\\[\n\\Psi = \\begin{pmatrix}\n1 & e^{-1}\\\\\ne^{-1} & 1\n\\end{pmatrix}.\n\\]\nTo determine MLE as in Equation 6.7, we need to compute \\(\\Psi^{-1}\\):\n\\[\n\\Psi^{-1} = \\frac{e}{e^2 -1} \\begin{pmatrix}\ne & -1\\\\\n-1 & e\n\\end{pmatrix}.\n\\]\nCholesky-decomposition of \\(\\Psi\\) is recommended to compute \\(\\Psi^{-1}\\). Cholesky decomposition is a decomposition of a positive definite symmetric matrix into the product of a lower triangular matrix \\(L\\), a diagonal matrix \\(D\\) and the transpose of \\(L\\), which is denoted as \\(L^T\\). Consider the following example:\n\\[\nLDL^T=\n\\begin{pmatrix}\n1 & 0 \\\\\nl_{21} & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nd_{11} & 0 \\\\\n0 & d_{22}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & l_{21} \\\\\n0 & 1\n\\end{pmatrix}=\n\\]\n\\[\n\\begin{pmatrix}\nd_{11} & 0 \\\\\nd_{11} l_{21} & d_{22}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & l_{21} \\\\\n0 & 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nd_{11} & d_{11} l_{21} \\\\\nd_{11} l_{21} & d_{11} l_{21}^2 + d_{22}\n\\end{pmatrix}.\n\\tag{6.8}\\]\nUsing Equation 6.8, we can compute the Cholesky decomposition of \\(\\Psi\\):\n\n\\(d_{11} = 1\\),\n\\(l_{21}d_{11} = e^{-1} \\Rightarrow l_{21} = e^{-1}\\), and\n\\(d_{11} l_{21}^2 + d_{22} = 1 \\Rightarrow d_{22} = 1 - e^{-2}\\).\n\nThe Cholesky decomposition of \\(\\Psi\\) is \\[\n\\Psi = \\begin{pmatrix}\n1 & 0\\\\\ne^{-1} & 1\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0\\\\\n0 & 1 - e^{-2}\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & e^{-1}\\\\\n0 & 1\\\\\n\\end{pmatrix}\n= LDL^T\\]\nSome programs use \\(U\\) instead of \\(L\\). The Cholesky decomposition of \\(\\Psi\\) is \\[\n\\Psi = LDL^T = U^TDU.\n\\]\nUsing \\[\n\\sqrt{D} =\\begin{pmatrix}\n1 & 0\\\\\n0 & \\sqrt{1 - e^{-2}}\\\\\n\\end{pmatrix},\n\\] we can write the Cholesky decomposition of \\(\\Psi\\) without a diagonal matrix \\(D\\) as \\[\n\\Psi = \\begin{pmatrix}\n1 & 0\\\\\ne^{-1} & \\sqrt{1 - e^{-2}}\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & e^{-1}\\\\\n0 & \\sqrt{1 - e^{-2}}\\\\\n\\end{pmatrix}\n= U^TU.\n\\]\n\n\n6.11.2 Computation of the Inverse Matrix\nTo compute the inverse of a matrix using the Cholesky decomposition, you can follow these steps:\n\nDecompose the matrix \\(A\\) into \\(L\\) and \\(L^T\\), where \\(L\\) is a lower triangular matrix and \\(L^T\\) is the transpose of \\(L\\).\nCompute \\(L^{-1}\\), the inverse of \\(L\\).\nThe inverse of \\(A\\) is then \\((L^{-1})^T  L^-1\\).\n\nPlease note that this method only applies to symmetric, positive-definite matrices.\nThe inverse of the matrix \\(\\Psi\\) from above is:\n\\[\n\\Psi^{-1} = \\frac{e}{e^2 -1} \\begin{pmatrix}\ne & -1\\\\\n-1 & e\n\\end{pmatrix}.\n\\]\nHere’s an example of how to compute the inverse of a matrix using Cholesky decomposition in Python:\n\nimport numpy as np\nfrom scipy.linalg import cholesky, inv\nE = np.exp(1)\n\n# Psi is a symmetric, positive-definite matrix \nPsi = np.array([[1, 1/E], [1/E, 1]])\nL = cholesky(Psi, lower=True)\nL_inv = inv(L)\n# The inverse of A is (L^-1)^T * L^-1\nPsi_inv = np.dot(L_inv.T, L_inv)\n\nprint(\"Psi:\\n\", Psi)\nprint(\"Psi Inverse:\\n\", Psi_inv)\n\nPsi:\n [[1.         0.36787944]\n [0.36787944 1.        ]]\nPsi Inverse:\n [[ 1.15651764 -0.42545906]\n [-0.42545906  1.15651764]]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "006_num_gp.html#jupyter-notebook",
    "href": "006_num_gp.html#jupyter-notebook",
    "title": "6  Kriging (Gaussian Process Regression)",
    "section": "6.12 Jupyter Notebook",
    "text": "6.12 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository\n\n\n\n\n\n\n\n\n\nBartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide. Springer.\n\n\nForrester, Alexander, András Sóbester, and Andy Keane. 2008. Engineering Design via Surrogate Modelling. Wiley.\n\n\nSantner, T J, B J Williams, and W I Notz. 2003. The Design and Analysis of Computer Experiments. Berlin, Heidelberg, New York: Springer.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Kriging (Gaussian Process Regression)</span>"
    ]
  },
  {
    "objectID": "007_num_spot_intro.html",
    "href": "007_num_spot_intro.html",
    "title": "7  Introduction to spotPython",
    "section": "",
    "text": "7.1 Example: Spot and the Sphere Function\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.utils.init import fun_control_init, design_control_init\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.spot import spot\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to spotPython</span>"
    ]
  },
  {
    "objectID": "007_num_spot_intro.html#example-spot-and-the-sphere-function",
    "href": "007_num_spot_intro.html#example-spot-and-the-sphere-function",
    "title": "7  Introduction to spotPython",
    "section": "",
    "text": "7.1.1 The Objective Function: Sphere\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[\nf(x) = x^2\n\\]\n\nfun = analytical().fun_sphere\n\nWe can apply the function fun to input values and plot the result:\n\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x, y, \"k\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.1.2 The Spot Method as an Optimization Algorithm Using a Surrogate Model\nWe initialize the fun_control dictionary. The fun_control dictionary contains the parameters for the objective function. The fun_control dictionary is passed to the Spot method.\n\nfun_control=fun_control_init(lower = np.array([-1]),\n                     upper = np.array([1]))\nspot_0 = spot.Spot(fun=fun,\n                   fun_control=fun_control)\nspot_0.run()\n\nspotPython tuning: 1.2026789271012512e-09 [#######---] 73.33% \nspotPython tuning: 1.2026789271012512e-09 [########--] 80.00% \nspotPython tuning: 1.2026789271012512e-09 [#########-] 86.67% \nspotPython tuning: 1.2026789271012512e-09 [#########-] 93.33% \nspotPython tuning: 3.7010904275056666e-10 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2a43d4d50&gt;\n\n\nThe method print_results() prints the results, i.e., the best objective function value (“min y”) and the corresponding input value (“x0”).\n\nspot_0.print_results()\n\nmin y: 3.7010904275056666e-10\nx0: 1.9238218284201025e-05\n\n\n[['x0', 1.9238218284201025e-05]]\n\n\nTo plot the search progress, the method plot_progress() can be used. The parameter log_y is used to plot the objective function values on a logarithmic scale.\n\nspot_0.plot_progress(log_y=True)\n\n\n\n\n\n\n\nFigure 7.1: Visualization of the search progress of the Spot method. The black elements (points and line) represent the initial design, before the surrogate is build. The red elements represent the search on the surrogate.\n\n\n\n\n\nIf the dimension of the input space is one, the method plot_model() can be used to visualize the model and the underlying objective function values.\n\nspot_0.plot_model()\n\n\n\n\n\n\n\nFigure 7.2: Visualization of the model and the underlying objective function values.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to spotPython</span>"
    ]
  },
  {
    "objectID": "007_num_spot_intro.html#spot-parameters-fun_evals-init_size-and-show_models",
    "href": "007_num_spot_intro.html#spot-parameters-fun_evals-init_size-and-show_models",
    "title": "7  Introduction to spotPython",
    "section": "7.2 Spot Parameters: fun_evals, init_size and show_models",
    "text": "7.2 Spot Parameters: fun_evals, init_size and show_models\nWe will modify three parameters:\n\nThe number of function evaluations (fun_evals) will be set to 10 (instead of 15, which is the default value) in the fun_control dictionary.\nThe parameter show_models, which visualizes the search process for each single iteration for 1-dim functions, in the fun_control dictionary.\nThe size of the initial design (init_size) in the design_control dictionary.\n\nThe full list of the Spot parameters is shown in code reference on GitHub, see Spot.\n\nfun_control=fun_control_init(lower = np.array([-1]),\n                     upper = np.array([1]),\n                     fun_evals = 10,\n                     show_models = True)               \ndesign_control = design_control_init(init_size=9)\nspot_1 = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   design_control=design_control)\nspot_1.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspotPython tuning: 1.2031167009156832e-09 [##########] 100.00% Done...",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to spotPython</span>"
    ]
  },
  {
    "objectID": "007_num_spot_intro.html#print-the-results",
    "href": "007_num_spot_intro.html#print-the-results",
    "title": "7  Introduction to spotPython",
    "section": "7.3 Print the Results",
    "text": "7.3 Print the Results\n\nspot_1.print_results()\n\nmin y: 1.2031167009156832e-09\nx0: -3.468597268227724e-05\n\n\n[['x0', -3.468597268227724e-05]]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to spotPython</span>"
    ]
  },
  {
    "objectID": "007_num_spot_intro.html#show-the-progress",
    "href": "007_num_spot_intro.html#show-the-progress",
    "title": "7  Introduction to spotPython",
    "section": "7.4 Show the Progress",
    "text": "7.4 Show the Progress\n\nspot_1.plot_progress()",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to spotPython</span>"
    ]
  },
  {
    "objectID": "007_num_spot_intro.html#sec-visualizing-tensorboard-01",
    "href": "007_num_spot_intro.html#sec-visualizing-tensorboard-01",
    "title": "7  Introduction to spotPython",
    "section": "7.5 Visualizing the Optimization and Hyperparameter Tuning Process with TensorBoard",
    "text": "7.5 Visualizing the Optimization and Hyperparameter Tuning Process with TensorBoard\nspotPython supports the visualization of the hyperparameter tuning process with TensorBoard. The following example shows how to use TensorBoard with spotPython.\nFirst, we define an “PREFIX” to identify the hyperparameter tuning process. The PREFIX is used to create a directory for the TensorBoard files.\n\nfun_control = fun_control_init(\n    PREFIX = \"01\",\n    lower = np.array([-1]),\n    upper = np.array([2]))\ndesign_control = design_control_init(init_size=5)\n\nCreated spot_tensorboard_path: runs/spot_logs/01_p040025_2024-01-15_00-11-03 for SummaryWriter()\n\n\nSince the PREFIX is not None, spotPython will log the optimization process in the TensorBoard files.\n\nspot_tuner = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   design_control=design_control)\nspot_tuner.run()\nspot_tuner.print_results()\n\nspotPython tuning: 2.7705278094872058e-05 [####------] 40.00% \nspotPython tuning: 8.061545220547415e-07 [#####-----] 46.67% \nspotPython tuning: 7.385022589686283e-07 [#####-----] 53.33% \nspotPython tuning: 3.677917685242894e-07 [######----] 60.00% \nspotPython tuning: 4.911502304103013e-09 [#######---] 66.67% \nspotPython tuning: 4.911502304103013e-09 [#######---] 73.33% \nspotPython tuning: 4.911502304103013e-09 [########--] 80.00% \nspotPython tuning: 4.911502304103013e-09 [#########-] 86.67% \nspotPython tuning: 4.911502304103013e-09 [#########-] 93.33% \nspotPython tuning: 4.911502304103013e-09 [##########] 100.00% Done...\n\nmin y: 4.911502304103013e-09\nx0: -7.008211115615035e-05\n\n\n[['x0', -7.008211115615035e-05]]\n\n\nNow we can start TensorBoard in the background. The TensorBoard process will read the TensorBoard files and visualize the hyperparameter tuning process. From the terminal, we can start TensorBoard with the following command:\ntensorboard --logdir=\"./runs\"\nlogdir is the directory where the TensorBoard files are stored. In our case, the TensorBoard files are stored in the directory ./runs.\nTensorBoard will start a web server on port 6006. We can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\nThe first TensorBoard visualization shows the objective function values plotted against the wall time. The wall time is the time that has passed since the start of the hyperparameter tuning process. The five initial design points are shown in the upper left region of the plot. The line visualizes the optimization process. \nThe second TensorBoard visualization shows the input values, i.e., \\(x_0\\), plotted against the wall time. \nThe third TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython process.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to spotPython</span>"
    ]
  },
  {
    "objectID": "007_num_spot_intro.html#jupyter-notebook",
    "href": "007_num_spot_intro.html#jupyter-notebook",
    "title": "7  Introduction to spotPython",
    "section": "7.6 Jupyter Notebook",
    "text": "7.6 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to spotPython</span>"
    ]
  },
  {
    "objectID": "008_num_spot_multidim.html",
    "href": "008_num_spot_multidim.html",
    "title": "8  Multi-dimensional Functions",
    "section": "",
    "text": "8.1 Example: Spot and the 3-dim Sphere Function\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.utils.init import fun_control_init, surrogate_control_init\nfrom spotPython.spot import spot",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multi-dimensional Functions</span>"
    ]
  },
  {
    "objectID": "008_num_spot_multidim.html#example-spot-and-the-3-dim-sphere-function",
    "href": "008_num_spot_multidim.html#example-spot-and-the-3-dim-sphere-function",
    "title": "8  Multi-dimensional Functions",
    "section": "",
    "text": "8.1.1 The Objective Function: 3-dim Sphere\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[\nf(x) = \\sum_i^k x_i^2.\n\\]\nIt is avaliable as fun_sphere in the analytical class [SOURCE].\n\nfun = analytical().fun_sphere\n\nHere we will use problem dimension \\(k=3\\), which can be specified by the lower bound arrays. The size of the lower bound array determines the problem dimension. If we select -1.0 * np.ones(3), a three-dimensional function is created. In contrast to the one-dimensional case (Section 7.5), where only one theta value was used, we will use three different theta values (one for each dimension), i.e., we set n_theta=3 in the surrogate_control. The prefix is set to \"03\" to distinguish the results from the one-dimensional case. Again, TensorBoard can be used to monitor the progress of the optimization.\nWe can also add interpreable labels to the dimensions, which will be used in the plots. Therefore, we set var_name=[\"Pressure\", \"Temp\", \"Lambda\"] instead of the default var_name=None, which would result in the labels x_0, x_1, and x_2.\n\nfun_control = fun_control_init(\n              PREFIX=\"03\",\n              lower = -1.0*np.ones(3),\n              upper = np.ones(3),\n              var_name=[\"Pressure\", \"Temp\", \"Lambda\"],\n              show_progress=True)\nsurrogate_control = surrogate_control_init(n_theta=3)\nspot_3 = spot.Spot(fun=fun,\n                  fun_control=fun_control,\n                  surrogate_control=surrogate_control)\nspot_3.run()\n\nCreated spot_tensorboard_path: runs/spot_logs/03_p040025_2024-01-15_00-11-17 for SummaryWriter()\nspotPython tuning: 0.03443324167631616 [#######---] 73.33% \nspotPython tuning: 0.03134655155643102 [########--] 80.00% \nspotPython tuning: 0.0009630181526749273 [#########-] 86.67% \nspotPython tuning: 8.570154459856623e-05 [#########-] 93.33% \nspotPython tuning: 6.496172516667557e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2bb920ed0&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nNow we can start TensorBoard in the background with the following command:\ntensorboard --logdir=\"./runs\"\nand can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\n\n\n\n\n8.1.2 Results\n\n_ = spot_3.print_results()\n\nmin y: 6.496172516667557e-05\nPressure: 0.005280070995399376\nTemp: 0.0019490323308060742\nLambda: 0.005769215581315232\n\n\n\nspot_3.plot_progress()\n\n\n\n\n\n\n\n\n\n\n8.1.3 A Contour Plot\nWe can select two dimensions, say \\(i=0\\) and \\(j=1\\), and generate a contour plot as follows.\n\n\n\n\n\n\nNote:\n\n\n\nWe have specified identical min_z and max_z values to generate comparable plots.\n\n\n\nspot_3.plot_contour(i=0, j=1, min_z=0, max_z=2.25)\n\n\n\n\n\n\n\n\n\nIn a similar manner, we can plot dimension \\(i=0\\) and \\(j=2\\):\n\n\nspot_3.plot_contour(i=0, j=2, min_z=0, max_z=2.25)\n\n\n\n\n\n\n\n\n\nThe final combination is \\(i=1\\) and \\(j=2\\):\n\n\nspot_3.plot_contour(i=1, j=2, min_z=0, max_z=2.25)\n\n\n\n\n\n\n\n\n\nThe three plots look very similar, because the fun_sphere is symmetric.\nThis can also be seen from the variable importance:\n\n\n_ = spot_3.print_importance()\n\nPressure:  95.79368533570627\nTemp:  99.99999999999999\nLambda:  87.19542775477797\n\n\n\nspot_3.plot_importance()\n\n\n\n\n\n\n\n\n\n\n8.1.4 TensorBoard\n\n\n\nTensorBoard visualization of the spotPython process. Objective function values plotted against wall time.\n\n\nThe second TensorBoard visualization shows the input values, i.e., \\(x_0, \\ldots, x_2\\), plotted against the wall time. \nThe third TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython surrogate model.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multi-dimensional Functions</span>"
    ]
  },
  {
    "objectID": "008_num_spot_multidim.html#conclusion",
    "href": "008_num_spot_multidim.html#conclusion",
    "title": "8  Multi-dimensional Functions",
    "section": "8.2 Conclusion",
    "text": "8.2 Conclusion\nBased on this quick analysis, we can conclude that all three dimensions are equally important (as expected, because the analytical function is known).",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multi-dimensional Functions</span>"
    ]
  },
  {
    "objectID": "008_num_spot_multidim.html#exercises",
    "href": "008_num_spot_multidim.html#exercises",
    "title": "8  Multi-dimensional Functions",
    "section": "8.3 Exercises",
    "text": "8.3 Exercises\n\n8.3.1 1. The Three Dimensional fun_cubed\n\nThe input dimension is 3. The search range is \\(-1 \\leq x \\leq 1\\) for all dimensions.\nGenerate contour plots\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?\n\n\n\n\n\n8.3.2 2. The Ten Dimensional fun_wing_wt\n\nThe input dimension is 10. The search range is \\(0 \\leq x \\leq 1\\) for all dimensions.\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?\n\nGenerate contour plots for the three most important variables. Do they confirm your selection?\n\n\n\n\n8.3.3 3. The Three Dimensional fun_runge\n\nThe input dimension is 3. The search range is \\(-5 \\leq x \\leq 5\\) for all dimensions.\nGenerate contour plots\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?\n\n\n\n\n\n8.3.4 4. The Three Dimensional fun_linear\n\nThe input dimension is 3. The search range is \\(-5 \\leq x \\leq 5\\) for all dimensions.\nGenerate contour plots\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?\n\n\n\n\n\n8.3.5 5. The Two Dimensional Rosenbrock Function fun_rosen\n\nThe input dimension is 2. The search range is \\(-5 \\leq x \\leq 10\\) for all dimensions.\nSee Rosenbrock function and Rosenbrock Function for details.\nGenerate contour plots\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multi-dimensional Functions</span>"
    ]
  },
  {
    "objectID": "008_num_spot_multidim.html#selected-solutions",
    "href": "008_num_spot_multidim.html#selected-solutions",
    "title": "8  Multi-dimensional Functions",
    "section": "8.4 Selected Solutions",
    "text": "8.4 Selected Solutions\n\n8.4.1 Solution to Exercise Section 8.3.5: The Two-dimensional Rosenbrock Function fun_rosen\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.utils.init import fun_control_init, surrogate_control_init\nfrom spotPython.spot import spot\n\n\n8.4.1.1 The Objective Function: 2-dim fun_rosen\nThe spotPython package provides several classes of objective functions. We will use the fun_rosen in the analytical class [SOURCE].\n\nfun_rosen = analytical().fun_rosen\n\nHere we will use problem dimension \\(k=2\\), which can be specified by the lower bound arrays. The size of the lower bound array determines the problem dimension. If we select -5.0 * np.ones(2), a two-dimensional function is created. In contrast to the one-dimensional case, where only one theta value is used, we will use \\(k\\) different theta values (one for each dimension), i.e., we set n_theta=3 in the surrogate_control. The prefix is set to \"ROSEN\". Again, TensorBoard can be used to monitor the progress of the optimization.\n\nfun_control = fun_control_init(\n              PREFIX=\"ROSEN\",\n              lower = -5.0*np.ones(2),\n              upper = 10*np.ones(2),\n              show_progress=True,\n              fun_evals=25)\nsurrogate_control = surrogate_control_init(n_theta=2)\nspot_rosen = spot.Spot(fun=fun_rosen,\n                  fun_control=fun_control,\n                  surrogate_control=surrogate_control)\nspot_rosen.run()\n\nCreated spot_tensorboard_path: runs/spot_logs/ROSEN_p040025_2024-01-15_00-11-20 for SummaryWriter()\nspotPython tuning: 90.7801015955818 [####------] 44.00% \nspotPython tuning: 1.0172832635943474 [#####-----] 48.00% \nspotPython tuning: 1.0172832635943474 [#####-----] 52.00% \nspotPython tuning: 1.0172832635943474 [######----] 56.00% \nspotPython tuning: 1.0172832635943474 [######----] 60.00% \nspotPython tuning: 1.0172832635943474 [######----] 64.00% \nspotPython tuning: 1.0172832635943474 [#######---] 68.00% \nspotPython tuning: 1.0172832635943474 [#######---] 72.00% \nspotPython tuning: 1.0172832635943474 [########--] 76.00% \nspotPython tuning: 1.0172832635943474 [########--] 80.00% \nspotPython tuning: 0.9921822630967522 [########--] 84.00% \nspotPython tuning: 0.7147779101762312 [#########-] 88.00% \nspotPython tuning: 0.7147779101762312 [#########-] 92.00% \nspotPython tuning: 0.7147779101762312 [##########] 96.00% \nspotPython tuning: 0.7147779101762312 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2bbcdb410&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nNow we can start TensorBoard in the background with the following command:\ntensorboard --logdir=\"./runs\"\nand can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\n\n\n\n\n8.4.1.2 Results\n\n_ = spot_rosen.print_results()\n\nmin y: 0.7147779101762312\nx0: 0.19951670458655138\nx1: 0.1258327277797004\n\n\n\nspot_rosen.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\n\n8.4.1.3 A Contour Plot\nWe can select two dimensions, say \\(i=0\\) and \\(j=1\\), and generate a contour plot as follows.\n\n\n\n\n\n\nNote:\n\n\n\nFor higher dimensions, it might be useful to have identical min_z and max_z values to generate comparable plots. The default values are min_z=None and max_z=None, which will be replaced by the minimum and maximum values of the objective function.\n\n\n\nmin_z = None\nmax_z = None\nspot_rosen.plot_contour(i=0, j=1, min_z=min_z, max_z=max_z)\n\n\n\n\n\n\n\n\n\nThe variable importance can be calculated as follows:\n\n\n_ = spot_rosen.print_importance()\n\nx0:  100.0\nx1:  1.2641431841859785\n\n\n\nspot_rosen.plot_importance()\n\n\n\n\n\n\n\n\n\n\n8.4.1.4 TensorBoard\nTBD",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multi-dimensional Functions</span>"
    ]
  },
  {
    "objectID": "008_num_spot_multidim.html#jupyter-notebook",
    "href": "008_num_spot_multidim.html#jupyter-notebook",
    "title": "8  Multi-dimensional Functions",
    "section": "8.5 Jupyter Notebook",
    "text": "8.5 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multi-dimensional Functions</span>"
    ]
  },
  {
    "objectID": "009_num_spot_anisotropic.html",
    "href": "009_num_spot_anisotropic.html",
    "title": "9  Isotropic and Anisotropic Kriging",
    "section": "",
    "text": "9.1 Example: Isotropic Spot Surrogate and the 2-dim Sphere Function\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.utils.init import fun_control_init, surrogate_control_init\nPREFIX=\"003\"",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Isotropic and Anisotropic Kriging</span>"
    ]
  },
  {
    "objectID": "009_num_spot_anisotropic.html#sec-spot-2d-sphere-iso",
    "href": "009_num_spot_anisotropic.html#sec-spot-2d-sphere-iso",
    "title": "9  Isotropic and Anisotropic Kriging",
    "section": "",
    "text": "9.1.1 The Objective Function: 2-dim Sphere\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula:\n\\[\nf(x, y) = x^2 + y^2\n\\] The size of the lower bound vector determines the problem dimension. Here we will use np.array([-1, -1]), i.e., a two-dimensional function.\n\nfun = analytical().fun_sphere\nfun_control = fun_control_init(PREFIX=PREFIX,\n                               lower = np.array([-1, -1]),\n                               upper = np.array([1, 1]))\n\nCreated spot_tensorboard_path: runs/spot_logs/003_p040025_2024-01-15_00-11-51 for SummaryWriter()\n\n\nAlthough the default spot surrogate model is an isotropic Kriging model, we will explicitly set the n_theta parameter to a value of 1, so that the same theta value is used for both dimensions. This is done to illustrate the difference between isotropic and anisotropic Kriging models.\n\nsurrogate_control=surrogate_control_init(n_theta=1)\n\n\nspot_2 = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   surrogate_control=surrogate_control)\n\nspot_2.run()\n\nspotPython tuning: 1.801603872454505e-05 [#######---] 73.33% \nspotPython tuning: 1.801603872454505e-05 [########--] 80.00% \nspotPython tuning: 1.801603872454505e-05 [#########-] 86.67% \nspotPython tuning: 1.801603872454505e-05 [#########-] 93.33% \nspotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d3e950d0&gt;\n\n\n\n\n9.1.2 Results\n\nspot_2.print_results()\n\nmin y: 1.801603872454505e-05\nx0: 0.0019077911677074135\nx1: 0.003791618596979743\n\n\n[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]\n\n\n\nspot_2.plot_progress(log_y=True)",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Isotropic and Anisotropic Kriging</span>"
    ]
  },
  {
    "objectID": "009_num_spot_anisotropic.html#example-with-anisotropic-kriging",
    "href": "009_num_spot_anisotropic.html#example-with-anisotropic-kriging",
    "title": "9  Isotropic and Anisotropic Kriging",
    "section": "9.2 Example With Anisotropic Kriging",
    "text": "9.2 Example With Anisotropic Kriging\nAs described in Section 9.1, the default parameter setting of spotPython’s Kriging surrogate uses the same theta value for every dimension. This is referred to as “using an isotropic kernel”. If different theta values are used for each dimension, then an anisotropic kernel is used. To enable anisotropic models in spotPython, the number of theta values should be larger than one. We can use surrogate_control=surrogate_control_init(n_theta=2) to enable this behavior (2 is the problem dimension).\n\nsurrogate_control = surrogate_control_init(n_theta=2)\nspot_2_anisotropic = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    surrogate_control=surrogate_control)\nspot_2_anisotropic.run()\n\nspotPython tuning: 1.783225688095949e-05 [#######---] 73.33% \nspotPython tuning: 1.783225688095949e-05 [########--] 80.00% \nspotPython tuning: 1.783225688095949e-05 [#########-] 86.67% \nspotPython tuning: 3.0185289245739795e-06 [#########-] 93.33% \nspotPython tuning: 3.0185289245739795e-06 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d4929590&gt;\n\n\nThe search progress of the optimization with the anisotropic model can be visualized:\n\nspot_2_anisotropic.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_2_anisotropic.print_results()\n\nmin y: 3.0185289245739795e-06\nx0: -0.0001531610695200253\nx1: -0.0017306272306182697\n\n\n[['x0', -0.0001531610695200253], ['x1', -0.0017306272306182697]]\n\n\n\nspot_2_anisotropic.surrogate.plot()\n\n\n\n\n\n\n\n\n\n9.2.1 Taking a Look at the theta Values\n\n9.2.1.1 theta Values from the spot Model\nWe can check, whether one or several theta values were used. The theta values from the surrogate can be printed as follows:\n\nspot_2_anisotropic.surrogate.theta\n\narray([-0.29237522, -0.13253124])\n\n\n\nSince the surrogate from the isotropic setting was stored as spot_2, we can also take a look at the theta value from this model:\n\n\nspot_2.surrogate.theta\n\narray([-0.04189656])\n\n\n\n\n9.2.1.2 TensorBoard\nNow we can start TensorBoard in the background with the following command:\ntensorboard --logdir=\"./runs\"\nWe can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\nThe TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython surrogate model.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Isotropic and Anisotropic Kriging</span>"
    ]
  },
  {
    "objectID": "009_num_spot_anisotropic.html#exercises",
    "href": "009_num_spot_anisotropic.html#exercises",
    "title": "9  Isotropic and Anisotropic Kriging",
    "section": "9.3 Exercises",
    "text": "9.3 Exercises\n\n9.3.1 1. The Branin Function fun_branin\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-5 \\leq x_1 \\leq 10\\) and \\(0 \\leq x_2 \\leq 15\\).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion: instead of the number of evaluations (which is specified via fun_evals), the time should be used as the termination criterion. This can be done as follows (max_time=1 specifies a run time of one minute):\n\n\nfrom math import inf\nfun_control = fun_control_init(\n              fun_evals=inf,\n              max_time=1)\n\n\n\n9.3.2 2. The Two-dimensional Sin-Cos Function fun_sin_cos\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-2\\pi \\leq x_1 \\leq 2\\pi\\) and \\(-2\\pi \\leq x_2 \\leq 2\\pi\\).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.\n\n\n\n9.3.3 3. The Two-dimensional Runge Function fun_runge\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-5 \\leq x_1 \\leq 5\\) and \\(-5 \\leq x_2 \\leq 5\\).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.\n\n\n\n9.3.4 4. The Ten-dimensional Wing-Weight Function fun_wingwt\n\nDescribe the function.\n\nThe input dimension is 10. The search ranges are between 0 and 1 (values are mapped internally to their natural bounds).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.\n\n\n\n9.3.5 5. The Two-dimensional Rosenbrock Function fun_rosen\n\nDescribe the function.\n\nThe input dimension is 2. The search ranges are between -5 and 10.\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Isotropic and Anisotropic Kriging</span>"
    ]
  },
  {
    "objectID": "009_num_spot_anisotropic.html#selected-solutions",
    "href": "009_num_spot_anisotropic.html#selected-solutions",
    "title": "9  Isotropic and Anisotropic Kriging",
    "section": "9.4 Selected Solutions",
    "text": "9.4 Selected Solutions\n\n9.4.1 Solution to Exercise Section 9.3.5: The Two-dimensional Rosenbrock Function fun_rosen\n\n9.4.1.1 The Two Dimensional fun_rosen: The Isotropic Case\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.utils.init import fun_control_init, surrogate_control_init\nfrom spotPython.spot import spot\n\nThe spotPython package provides several classes of objective functions. We will use the fun_rosen in the analytical class [SOURCE].\n\nfun_rosen = analytical().fun_rosen\n\nHere we will use problem dimension \\(k=2\\), which can be specified by the lower bound arrays. The size of the lower bound array determines the problem dimension.\nThe prefix is set to \"ROSEN\" to distinguish the results from the one-dimensional case. Again, TensorBoard can be used to monitor the progress of the optimization.\n\nfun_control = fun_control_init(\n              PREFIX=\"ROSEN\",\n              lower = np.array([-5, -5]),\n              upper = np.array([10, 10]),\n              show_progress=True)\nsurrogate_control = surrogate_control_init(n_theta=1)\nspot_rosen = spot.Spot(fun=fun_rosen,\n                  fun_control=fun_control,\n                  surrogate_control=surrogate_control)\nspot_rosen.run()\n\nCreated spot_tensorboard_path: runs/spot_logs/ROSEN_p040025_2024-01-15_00-11-54 for SummaryWriter()\nspotPython tuning: 52.87631878551649 [#######---] 73.33% \nspotPython tuning: 52.361867783356715 [########--] 80.00% \nspotPython tuning: 52.361867783356715 [#########-] 86.67% \nspotPython tuning: 43.44273941029301 [#########-] 93.33% \nspotPython tuning: 12.275684138292505 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d4c32a10&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nNow we can start TensorBoard in the background with the following command:\ntensorboard --logdir=\"./runs\"\nand can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\n\n\n\n9.4.1.1.1 Results\n\n_ = spot_rosen.print_results()\n\nmin y: 12.275684138292505\nx0: -2.3708459318321333\nx1: 5.923082873674319\n\n\n\nspot_rosen.plot_progress()\n\n\n\n\n\n\n\n\n\n\n9.4.1.1.2 A Contour Plot\nWe can select two dimensions, say \\(i=0\\) and \\(j=1\\), and generate a contour plot as follows.\n\nmin_z = None\nmax_z = None\nspot_rosen.plot_contour(i=0, j=1, min_z=min_z, max_z=max_z)\n\n\n\n\n\n\n\n\n\nThe variable importance cannot be calculated, because only one theta value was used.\n\n\n\n9.4.1.1.3 TensorBoard\nTBD\n\n\n\n9.4.1.2 The Two Dimensional fun_rosen: The Anisotropic Case\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.utils.init import fun_control_init, surrogate_control_init\nfrom spotPython.spot import spot\n\nThe spotPython package provides several classes of objective functions. We will use the fun_rosen in the analytical class [SOURCE].\n\nfun_rosen = analytical().fun_rosen\n\nHere we will use problem dimension \\(k=2\\), which can be specified by the lower bound arrays. The size of the lower bound array determines the problem dimension.\nWe can also add interpreable labels to the dimensions, which will be used in the plots.\n\nfun_control = fun_control_init(\n              PREFIX=\"ROSEN\",\n              lower = np.array([-5, -5]),\n              upper = np.array([10, 10]),\n              show_progress=True)\nsurrogate_control = surrogate_control_init(n_theta=2)\nspot_rosen = spot.Spot(fun=fun_rosen,\n                  fun_control=fun_control,\n                  surrogate_control=surrogate_control)\nspot_rosen.run()\n\nCreated spot_tensorboard_path: runs/spot_logs/ROSEN_p040025_2024-01-15_00-11-55 for SummaryWriter()\nspotPython tuning: 90.7801015955818 [#######---] 73.33% \nspotPython tuning: 1.0172832635943474 [########--] 80.00% \nspotPython tuning: 1.0172832635943474 [#########-] 86.67% \nspotPython tuning: 1.0172832635943474 [#########-] 93.33% \nspotPython tuning: 1.0172832635943474 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d4e4b950&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nNow we can start TensorBoard in the background with the following command:\ntensorboard --logdir=\"./runs\"\nand can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\n\n\n\n9.4.1.2.1 Results\n\n_ = spot_rosen.print_results()\n\nmin y: 1.0172832635943474\nx0: 0.0028122200003174065\nx1: -0.04784582169505708\n\n\n\nspot_rosen.plot_progress()\n\n\n\n\n\n\n\n\n\n\n9.4.1.2.2 A Contour Plot\nWe can select two dimensions, say \\(i=0\\) and \\(j=1\\), and generate a contour plot as follows.\n\nmin_z = None\nmax_z = None\nspot_rosen.plot_contour(i=0, j=1, min_z=min_z, max_z=max_z)\n\n\n\n\n\n\n\n\n\nThe variable importance can be calculated as follows:\n\n\n_ = spot_rosen.print_importance()\n\nx0:  100.0\nx1:  2.2276773313197755\n\n\n\nspot_rosen.plot_importance()\n\n\n\n\n\n\n\n\n\n\n9.4.1.2.3 TensorBoard\nTBD",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Isotropic and Anisotropic Kriging</span>"
    ]
  },
  {
    "objectID": "009_num_spot_anisotropic.html#jupyter-notebook",
    "href": "009_num_spot_anisotropic.html#jupyter-notebook",
    "title": "9  Isotropic and Anisotropic Kriging",
    "section": "9.5 Jupyter Notebook",
    "text": "9.5 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Isotropic and Anisotropic Kriging</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html",
    "href": "010_num_spot_sklearn_surrogate.html",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "",
    "text": "10.1 Example: Branin Function with spotPython’s Internal Kriging Surrogate",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html#example-branin-function-with-spotpythons-internal-kriging-surrogate",
    "href": "010_num_spot_sklearn_surrogate.html#example-branin-function-with-spotpythons-internal-kriging-surrogate",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "",
    "text": "10.1.1 The Objective Function Branin\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula.\nHere we will use the Branin function:\n  y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,\n  where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2),\n  c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).\nIt has three global minima:\n  f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_branin\n\n\n\n\n\n\n\nTensorBoard\n\n\n\nSimilar to the one-dimensional case, which was introduced in Section Section 7.5, we can use TensorBoard to monitor the progress of the optimization. We will use the same code, only the prefix is different:\n\nfrom spotPython.utils.init import fun_control_init, design_control_init\nPREFIX = \"04\"\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    lower = np.array([-5,-0]),\n    upper = np.array([10,15]),\n    fun_evals=20,\n    max_time=inf)\n\ndesign_control = design_control_init(\n    init_size=10)\n\nCreated spot_tensorboard_path: runs/spot_logs/04_p040025_2024-01-15_00-12-13 for SummaryWriter()\n\n\n\n\n\n\n10.1.2 Running the surrogate model based optimizer Spot:\n\nspot_2 = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   design_control=design_control)\n\n\nspot_2.run()\n\nspotPython tuning: 3.146824136952164 [######----] 55.00% \nspotPython tuning: 3.146824136952164 [######----] 60.00% \nspotPython tuning: 3.146824136952164 [######----] 65.00% \nspotPython tuning: 3.146824136952164 [#######---] 70.00% \nspotPython tuning: 1.1487233101571483 [########--] 75.00% \nspotPython tuning: 1.0236891516766402 [########--] 80.00% \nspotPython tuning: 0.41994270072214057 [########--] 85.00% \nspotPython tuning: 0.40193544341108023 [#########-] 90.00% \nspotPython tuning: 0.3991519598268951 [##########] 95.00% \nspotPython tuning: 0.3991519598268951 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2b3595350&gt;\n\n\n\n\n10.1.3 TensorBoard\nNow we can start TensorBoard in the background with the following command:\ntensorboard --logdir=\"./runs\"\nWe can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\nThe TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model.\n\n\n\n\n10.1.4 Print the Results\n\nspot_2.print_results()\n\nmin y: 0.3991519598268951\nx0: 3.1546575195040987\nx1: 2.285931113926263\n\n\n[['x0', 3.1546575195040987], ['x1', 2.285931113926263]]\n\n\n\n\n10.1.5 Show the Progress and the Surrogate\n\nspot_2.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_2.surrogate.plot()",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html#example-using-surrogates-from-scikit-learn",
    "href": "010_num_spot_sklearn_surrogate.html#example-using-surrogates-from-scikit-learn",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "10.2 Example: Using Surrogates From scikit-learn",
    "text": "10.2 Example: Using Surrogates From scikit-learn\n\nDefault is the spotPython (i.e., the internal) kriging surrogate.\nIt can be called explicitely and passed to Spot.\n\n\nfrom spotPython.build.kriging import Kriging\nS_0 = Kriging(name='kriging', seed=123)\n\n\nAlternatively, models from scikit-learn can be selected, e.g., Gaussian Process, RBFs, Regression Trees, etc.\n\n\n# Needed for the sklearn surrogates:\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport pandas as pd\n\n\nHere are some additional models that might be useful later:\n\n\nS_Tree = DecisionTreeRegressor(random_state=0)\nS_LM = linear_model.LinearRegression()\nS_Ridge = linear_model.Ridge()\nS_RF = RandomForestRegressor(max_depth=2, random_state=0)\n\n\n10.2.1 GaussianProcessRegressor as a Surrogate\n\nTo use a Gaussian Process model from sklearn, that is similar to spotPython’s Kriging, we can proceed as follows:\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\nS_GP = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n\nThe scikit-learn GP model S_GP is selected for Spot as follows:\nsurrogate = S_GP\nWe can check the kind of surogate model with the command isinstance:\n\n\nisinstance(S_GP, GaussianProcessRegressor) \n\nTrue\n\n\n\nisinstance(S_0, Kriging)\n\nTrue\n\n\n\nSimilar to the Spot run with the internal Kriging model, we can call the run with the scikit-learn surrogate:\n\n\nfun = analytical(seed=123).fun_branin\nspot_2_GP = spot.Spot(fun=fun,\n                     fun_control=fun_control,\n                     design_control=design_control,\n                     surrogate = S_GP)\nspot_2_GP.run()\n\nspotPython tuning: 18.865121449825782 [######----] 55.00% \nspotPython tuning: 4.06700305855078 [######----] 60.00% \nspotPython tuning: 3.461906927549384 [######----] 65.00% \nspotPython tuning: 3.461906927549384 [#######---] 70.00% \nspotPython tuning: 1.3280944252046556 [########--] 75.00% \nspotPython tuning: 0.9548334920645392 [########--] 80.00% \nspotPython tuning: 0.9344485781421579 [########--] 85.00% \nspotPython tuning: 0.39916716809341857 [#########-] 90.00% \nspotPython tuning: 0.3982254000779708 [##########] 95.00% \nspotPython tuning: 0.3982254000779708 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2b3c59a90&gt;\n\n\n\nspot_2_GP.plot_progress()\n\n\n\n\n\n\n\n\n\nspot_2_GP.print_results()\n\nmin y: 0.3982254000779708\nx0: 3.1499822680266343\nx1: 2.268811272474469\n\n\n[['x0', 3.1499822680266343], ['x1', 2.268811272474469]]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html#example-one-dimensional-sphere-function-with-spotpythons-kriging",
    "href": "010_num_spot_sklearn_surrogate.html#example-one-dimensional-sphere-function-with-spotpythons-kriging",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "10.3 Example: One-dimensional Sphere Function With spotPython’s Kriging",
    "text": "10.3 Example: One-dimensional Sphere Function With spotPython’s Kriging\n\nIn this example, we will use an one-dimensional function, which allows us to visualize the optimization process.\n\nshow_models= True is added to the argument list.\n\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun_control = fun_control_init(\n    lower = np.array([-1]),\n    upper = np.array([1]),\n    fun_evals=10,\n    max_time=inf,\n    show_models= True,\n    tolerance_x = np.sqrt(np.spacing(1)))\nfun = analytical(seed=123).fun_sphere\ndesign_control = design_control_init(\n    init_size=3)\n\n\nspot_1 = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    design_control=design_control)\nspot_1.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspotPython tuning: 0.03475493366922229 [####------] 40.00% \nspotPython tuning: 0.03475483461229862 [#####-----] 50.00% \nspotPython tuning: 0.03475338954992179 [######----] 60.00% \nspotPython tuning: 0.03437475313644103 [#######---] 70.00% \nspotPython tuning: 0.015290217643803946 [########--] 80.00% \nspotPython tuning: 0.0017932523576966073 [#########-] 90.00% \nspotPython tuning: 8.771851669068651e-06 [##########] 100.00% Done...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.1 Results\n\nspot_1.print_results()\n\nmin y: 8.771851669068651e-06\nx0: 0.002961731194600322\n\n\n[['x0', 0.002961731194600322]]\n\n\n\nspot_1.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nThe method plot_model plots the final surrogate:\n\n\nspot_1.plot_model()",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html#example-sklearn-model-gaussianprocess",
    "href": "010_num_spot_sklearn_surrogate.html#example-sklearn-model-gaussianprocess",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "10.4 Example: Sklearn Model GaussianProcess",
    "text": "10.4 Example: Sklearn Model GaussianProcess\n\nThis example visualizes the search process on the GaussianProcessRegression surrogate from sklearn.\nTherefore surrogate = S_GP is added to the argument list.\n\n\nfun = analytical(seed=123).fun_sphere\nspot_1_GP = spot.Spot(fun=fun,\n                      fun_control=fun_control,\n                      design_control=design_control,\n                      surrogate = S_GP)\nspot_1_GP.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspotPython tuning: 0.004925671374769521 [####------] 40.00% \nspotPython tuning: 0.002612062924748803 [#####-----] 50.00% \nspotPython tuning: 3.6666409852957783e-07 [######----] 60.00% \nspotPython tuning: 4.638244203084832e-08 [#######---] 70.00% \nspotPython tuning: 3.2711094860544125e-09 [########--] 80.00% \nspotPython tuning: 2.2493573831304313e-10 [#########-] 90.00% \nspotPython tuning: 2.2493573831304313e-10 [##########] 100.00% Done...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspot_1_GP.print_results()\n\nmin y: 2.2493573831304313e-10\nx0: 1.499785779079943e-05\n\n\n[['x0', 1.499785779079943e-05]]\n\n\n\nspot_1_GP.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_1_GP.plot_model()",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html#exercises",
    "href": "010_num_spot_sklearn_surrogate.html#exercises",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\n10.5.1 1. A decision tree regressor: DecisionTreeRegressor\n\nDescribe the surrogate model. Use the information from the scikit-learn documentation.\nUse the surrogate as the model for optimization.\n\n\n\n10.5.2 2. A random forest regressor: RandomForestRegressor\n\nDescribe the surrogate model. Use the information from the scikit-learn documentation.\nUse the surrogate as the model for optimization.\n\n\n\n10.5.3 3. Ordinary least squares Linear Regression: LinearRegression\n\nDescribe the surrogate model. Use the information from the scikit-learn documentation.\nUse the surrogate as the model for optimization.\n\n\n\n10.5.4 4. Linear least squares with l2 regularization: Ridge\n\nDescribe the surrogate model. Use the information from the scikit-learn documentation.\nUse the surrogate as the model for optimization.\n\n\n\n10.5.5 5. Gradient Boosting: HistGradientBoostingRegressor\n\nDescribe the surrogate model. Use the information from the scikit-learn documentation.\nUse the surrogate as the model for optimization.\n\n\n\n10.5.6 6. Comparison of Surrogates\n\nUse the following two objective functions\n\nthe 1-dim sphere function fun_sphere and\nthe two-dim Branin function fun_branin:\n\nfor a comparison of the performance of the five different surrogates:\n\nspotPython’s internal Kriging\nDecisionTreeRegressor\nRandomForestRegressor\nlinear_model.LinearRegression\nlinear_model.Ridge.\n\nGenerate a table with the results (number of function evaluations, best function value, and best parameter vector) for each surrogate and each function as shown in Table 10.1.\n\n\n\n\nTable 10.1: Result table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsurrogate\nfun\nfun_evals\nmax_time\nx_0\nmin_y\nComments\n\n\n\n\nKriging\nfun_sphere\n10\ninf\n\n\n\n\n\nKriging\nfun_branin\n10\ninf\n\n\n\n\n\nDecisionTreeRegressor\nfun_sphere\n10\ninf\n\n\n\n\n\n…\n…\n…\n…\n\n\n\n\n\nRidge\nfun_branin\n10\ninf\n\n\n\n\n\n\n\n\n\n\nDiscuss the results. Which surrogate is the best for which function? Why?",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html#selected-solutions",
    "href": "010_num_spot_sklearn_surrogate.html#selected-solutions",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "10.6 Selected Solutions",
    "text": "10.6 Selected Solutions\n\n10.6.1 Solution to Exercise Section 10.5.5: Gradient Boosting\n\n10.6.1.1 Branin: Using SPOT\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.utils.init import fun_control_init, design_control_init\nfrom spotPython.spot import spot\n\n\nThe Objective Function Branin\n\n\nfun = analytical().fun_branin\nPREFIX = \"BRANIN\"\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    lower = np.array([-5,-0]),\n    upper = np.array([10,15]),\n    fun_evals=20,\n    max_time=inf)\n\ndesign_control = design_control_init(\n    init_size=10)\n\nCreated spot_tensorboard_path: runs/spot_logs/BRANIN_p040025_2024-01-15_00-12-27 for SummaryWriter()\n\n\n\nRunning the surrogate model based optimizer Spot:\n\n\nspot_2 = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   design_control=design_control)\nspot_2.run()\n\nspotPython tuning: 3.146824136952164 [######----] 55.00% \nspotPython tuning: 3.146824136952164 [######----] 60.00% \nspotPython tuning: 3.146824136952164 [######----] 65.00% \nspotPython tuning: 3.146824136952164 [#######---] 70.00% \nspotPython tuning: 1.1487233101571483 [########--] 75.00% \nspotPython tuning: 1.0236891516766402 [########--] 80.00% \nspotPython tuning: 0.41994270072214057 [########--] 85.00% \nspotPython tuning: 0.40193544341108023 [#########-] 90.00% \nspotPython tuning: 0.3991519598268951 [##########] 95.00% \nspotPython tuning: 0.3991519598268951 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2b8088950&gt;\n\n\n\nPrint the results\n\n\nspot_2.print_results()\n\nmin y: 0.3991519598268951\nx0: 3.1546575195040987\nx1: 2.285931113926263\n\n\n[['x0', 3.1546575195040987], ['x1', 2.285931113926263]]\n\n\n\nShow the optimization progress:\n\n\nspot_2.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nGenerate a surrogate model plot:\n\n\nspot_2.surrogate.plot()\n\n\n\n\n\n\n\n\n\n\n10.6.1.2 Branin: Using Surrogates From scikit-learn\n\nThe HistGradientBoostingRegressor model from scikit-learn is selected:\n\n\n# Needed for the sklearn surrogates:\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nimport pandas as pd\nS_XGB = HistGradientBoostingRegressor()\n\n\nThe scikit-learn XGB model S_XGB is selected for Spot as follows: surrogate = S_XGB.\nSimilar to the Spot run with the internal Kriging model, we can call the run with the scikit-learn surrogate:\n\n\nfun = analytical(seed=123).fun_branin\nspot_2_XGB = spot.Spot(fun=fun,\n                     fun_control=fun_control,\n                     design_control=design_control,\n                     surrogate = S_XGB)\nspot_2_XGB.run()\n\nspotPython tuning: 30.69410528614059 [######----] 55.00% \nspotPython tuning: 30.69410528614059 [######----] 60.00% \nspotPython tuning: 30.69410528614059 [######----] 65.00% \nspotPython tuning: 30.69410528614059 [#######---] 70.00% \nspotPython tuning: 1.3263745845108854 [########--] 75.00% \nspotPython tuning: 1.3263745845108854 [########--] 80.00% \nspotPython tuning: 1.3263745845108854 [########--] 85.00% \nspotPython tuning: 1.3263745845108854 [#########-] 90.00% \nspotPython tuning: 1.3263745845108854 [##########] 95.00% \nspotPython tuning: 1.3263745845108854 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2b8984090&gt;\n\n\n\nPrint the Results\n\n\nspot_2_XGB.print_results()\n\nmin y: 1.3263745845108854\nx0: -2.872730773493426\nx1: 10.874313833535739\n\n\n[['x0', -2.872730773493426], ['x1', 10.874313833535739]]\n\n\n\nShow the Progress\n\n\nspot_2_XGB.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nSince the sklearn model does not provide a plot method, we cannot generate a surrogate model plot.\n\n\n\n10.6.1.3 One-dimensional Sphere Function With spotPython’s Kriging\n\nIn this example, we will use an one-dimensional function, which allows us to visualize the optimization process.\n\nshow_models= True is added to the argument list.\n\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun_control = fun_control_init(\n    lower = np.array([-1]),\n    upper = np.array([1]),\n    fun_evals=10,\n    max_time=inf,\n    show_models= True,\n    tolerance_x = np.sqrt(np.spacing(1)))\nfun = analytical(seed=123).fun_sphere\ndesign_control = design_control_init(\n    init_size=3)\n\n\nspot_1 = spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    design_control=design_control)\nspot_1.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspotPython tuning: 0.03475493366922229 [####------] 40.00% \nspotPython tuning: 0.03475483461229862 [#####-----] 50.00% \nspotPython tuning: 0.03475338954992179 [######----] 60.00% \nspotPython tuning: 0.03437475313644103 [#######---] 70.00% \nspotPython tuning: 0.015290217643803946 [########--] 80.00% \nspotPython tuning: 0.0017932523576966073 [#########-] 90.00% \nspotPython tuning: 8.771851669068651e-06 [##########] 100.00% Done...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrint the Results\n\n\nspot_1.print_results()\n\nmin y: 8.771851669068651e-06\nx0: 0.002961731194600322\n\n\n[['x0', 0.002961731194600322]]\n\n\n\nShow the Progress\n\n\nspot_1.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nThe method plot_model plots the final surrogate:\n\n\nspot_1.plot_model()\n\n\n\n\n\n\n\n\n\n\n10.6.1.4 One-dimensional Sphere Function With Sklearn Model HistGradientBoostingRegressor\n\nThis example visualizes the search process on the HistGradientBoostingRegressor surrogate from sklearn.\nTherefore surrogate = S_XGB is added to the argument list.\n\n\nfun_control = fun_control_init(\n    lower = np.array([-1]),\n    upper = np.array([1]),\n    fun_evals=10,\n    max_time=inf,\n    show_models= True,\n    tolerance_x = np.sqrt(np.spacing(1)))\nfun = analytical(seed=123).fun_sphere\ndesign_control = design_control_init(\n    init_size=3)\nspot_1_XGB = spot.Spot(fun=fun,\n                      fun_control=fun_control,\n                      design_control=design_control,\n                      surrogate = S_XGB)\nspot_1_XGB.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspotPython tuning: 0.0018828816523185745 [####------] 40.00% \nspotPython tuning: 0.0018828816523185745 [#####-----] 50.00% \nspotPython tuning: 0.0018828816523185745 [######----] 60.00% \nspotPython tuning: 0.0018828816523185745 [#######---] 70.00% \nspotPython tuning: 0.0018828816523185745 [########--] 80.00% \nspotPython tuning: 0.0018828816523185745 [#########-] 90.00% \nspotPython tuning: 0.0018828816523185745 [##########] 100.00% Done...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspot_1_XGB.print_results()\n\nmin y: 0.0018828816523185745\nx0: 0.04339218423078717\n\n\n[['x0', 0.04339218423078717]]\n\n\n\nspot_1_XGB.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_1_XGB.plot_model()",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "010_num_spot_sklearn_surrogate.html#jupyter-notebook",
    "href": "010_num_spot_sklearn_surrogate.html#jupyter-notebook",
    "title": "10  Using sklearn Surrogates in spotPython",
    "section": "10.7 Jupyter Notebook",
    "text": "10.7 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using `sklearn` Surrogates in `spotPython`</span>"
    ]
  },
  {
    "objectID": "011_num_spot_sklearn_gaussian.html",
    "href": "011_num_spot_sklearn_gaussian.html",
    "title": "11  Sequential Parameter Optimization: Gaussian Process Models",
    "section": "",
    "text": "11.1 Gaussian Processes Regression: Basic Introductory scikit-learn Example",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sequential Parameter Optimization:  Gaussian Process Models</span>"
    ]
  },
  {
    "objectID": "011_num_spot_sklearn_gaussian.html#gaussian-processes-regression-basic-introductory-scikit-learn-example",
    "href": "011_num_spot_sklearn_gaussian.html#gaussian-processes-regression-basic-introductory-scikit-learn-example",
    "title": "11  Sequential Parameter Optimization: Gaussian Process Models",
    "section": "",
    "text": "This is the example from scikit-learn: https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html\nAfter fitting our model, we see that the hyperparameters of the kernel have been optimized.\nNow, we will use our kernel to compute the mean prediction of the full dataset and plot the 95% confidence interval.\n\n\n11.1.1 Train and Test Data\n\nX = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\ny = np.squeeze(X * np.sin(X))\nrng = np.random.RandomState(1)\ntraining_indices = rng.choice(np.arange(y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]\n\n\n\n11.1.2 Building the Surrogate With Sklearn\n\nThe model building with sklearn consisits of three steps:\n\nInstantiating the model, then\nfitting the model (using fit), and\nmaking predictions (using predict)\n\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\ngaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\ngaussian_process.fit(X_train, y_train)\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n\n\n\n11.1.3 Plotting the SklearnModel\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n\n\n\n\n\n11.1.4 The spotPython Version\n\nThe spotPython version is very similar:\n\nInstantiating the model, then\nfitting the model and\nmaking predictions (using predict).\n\n\n\nS = Kriging(name='kriging',  seed=123, log_level=50, cod_type=\"norm\")\nS.fit(X_train, y_train)\nS_mean_prediction, S_std_prediction, S_ei = S.predict(X, return_val=\"all\")\n\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, S_mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    S_mean_prediction - 1.96 * S_std_prediction,\n    S_mean_prediction + 1.96 * S_std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"spotPython Version: Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n\n\n\n\n\n11.1.5 Visualizing the Differences Between the spotPython and the sklearn Model Fits\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, S_mean_prediction, label=\"spotPython Mean prediction\")\nplt.plot(X, mean_prediction, label=\"Sklearn Mean Prediction\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Comparing Mean Predictions\")",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sequential Parameter Optimization:  Gaussian Process Models</span>"
    ]
  },
  {
    "objectID": "011_num_spot_sklearn_gaussian.html#exercises",
    "href": "011_num_spot_sklearn_gaussian.html#exercises",
    "title": "11  Sequential Parameter Optimization: Gaussian Process Models",
    "section": "11.2 Exercises",
    "text": "11.2 Exercises\n\n11.2.1 Schonlau Example Function\n\nThe Schonlau Example Function is based on sample points only (there is no analytical function description available):\n\n\nX = np.linspace(start=0, stop=13, num=1_000).reshape(-1, 1)\nX_train = np.array([1., 2., 3., 4., 12.]).reshape(-1,1)\ny_train = np.array([0., -1.75, -2, -0.5, 5.])\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Since there is no analytical function available, you might be interested in adding some points and describe the effects.\n\n\n\n11.2.2 Forrester Example Function\n\nThe Forrester Example Function is defined as follows:\nf(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\nData points are generated as follows:\n\n\nfrom spotPython.utils.init import fun_control_init\nX = np.linspace(start=-0.5, stop=1.5, num=1_000).reshape(-1, 1)\nX_train = np.array([0.0, 0.175, 0.225, 0.3, 0.35, 0.375, 0.5,1]).reshape(-1,1)\nfun = analytical().fun_forrester\nfun_control = fun_control_init(sigma = 0.1)\ny = fun(X, fun_control=fun_control)\ny_train = fun(X_train, fun_control=fun_control)\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Modify the noise level (\"sigma\"), e.g., use a value of 0.2, and compare the two models.\n\n\nfun_control = fun_control_init(sigma = 0.2)\n\n\n\n11.2.3 fun_runge Function (1-dim)\n\nThe Runge function is defined as follows:\nf(x) = 1/ (1 + sum(x_i))^2\nData points are generated as follows:\n\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = fun_control_init(sigma = 0.025)\nX_train = gen.scipy_lhd(10, lower=lower, upper = upper).reshape(-1,1)\ny_train = fun(X, fun_control=fun_control)\nX = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\ny = fun(X, fun_control=fun_control)\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Modify the noise level (\"sigma\"), e.g., use a value of 0.05, and compare the two models.\n\n\nfun_control = fun_control_init(sigma = 0.5)\n\n\n\n11.2.4 fun_cubed (1-dim)\n\nThe Cubed function is defined as follows:\nnp.sum(X[i]** 3)\nData points are generated as follows:\n\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nfun_control = fun_control_init(sigma = 0.025,\n                lower = np.array([-10]),\n                upper = np.array([10]))\nfun = analytical().fun_cubed\nX_train = gen.scipy_lhd(10, lower=lower, upper = upper).reshape(-1,1)\ny_train = fun(X, fun_control=fun_control)\nX = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\ny = fun(X, fun_control=fun_control)\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Modify the noise level (\"sigma\"), e.g., use a value of 0.05, and compare the two models.\n\n\nfun_control = fun_control_init(sigma = 0.025)\n\n\n\n11.2.5 The Effect of Noise\nHow does the behavior of the spotPython fit changes when the argument noise is set to True, i.e.,\nS = Kriging(name='kriging',  seed=123, n_theta=1, noise=True)\nis used?",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sequential Parameter Optimization:  Gaussian Process Models</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html",
    "href": "012_num_spot_ei.html",
    "title": "12  Expected Improvement",
    "section": "",
    "text": "12.1 Example: Spot and the 1-dim Sphere Function\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.utils.init import fun_control_init, surrogate_control_init, design_control_init\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#example-spot-and-the-1-dim-sphere-function",
    "href": "012_num_spot_ei.html#example-spot-and-the-1-dim-sphere-function",
    "title": "12  Expected Improvement",
    "section": "",
    "text": "12.1.1 The Objective Function: 1-dim Sphere\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[f(x) = x^2 \\]\n\n\nfun = analytical().fun_sphere\n\n\nThe size of the lower bound vector determines the problem dimension.\nHere we will use np.array([-1]), i.e., a one-dim function.\n\n\n\n\n\n\n\nTensorBoard\n\n\n\nSimilar to the one-dimensional case, which was introduced in Section Section 7.5, we can use TensorBoard to monitor the progress of the optimization. We will use the same code, only the prefix is different:\n\nfrom spotPython.utils.init import fun_control_init\nPREFIX = \"07_Y\"\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    fun_evals = 25,\n    lower = np.array([-1]),\n    upper = np.array([1]),\n    tolerance_x = np.sqrt(np.spacing(1)),)\ndesign_control = design_control_init(init_size=10)\n\nCreated spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-13-37 for SummaryWriter()\n\n\n\n\n\nspot_1 = spot.Spot(\n            fun=fun,\n            fun_control=fun_control,\n            design_control=design_control)\nspot_1.run()\n\nspotPython tuning: 1.2026789271012512e-09 [####------] 44.00% \nspotPython tuning: 1.2026789271012512e-09 [#####-----] 48.00% \nspotPython tuning: 1.2026789271012512e-09 [#####-----] 52.00% \nspotPython tuning: 1.2026789271012512e-09 [######----] 56.00% \nspotPython tuning: 3.7010904275056666e-10 [######----] 60.00% \nspotPython tuning: 3.7010904275056666e-10 [######----] 64.00% \nspotPython tuning: 3.7010904275056666e-10 [#######---] 68.00% \nspotPython tuning: 3.7010904275056666e-10 [#######---] 72.00% \nspotPython tuning: 3.7010904275056666e-10 [########--] 76.00% \nspotPython tuning: 3.7010904275056666e-10 [########--] 80.00% \nspotPython tuning: 3.7010904275056666e-10 [########--] 84.00% \nspotPython tuning: 3.7010904275056666e-10 [#########-] 88.00% \nspotPython tuning: 2.802111689321758e-11 [#########-] 92.00% \nspotPython tuning: 2.802111689321758e-11 [##########] 96.00% \nspotPython tuning: 2.802111689321758e-11 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d6296390&gt;\n\n\n\n\n12.1.2 Results\n\nspot_1.print_results()\n\nmin y: 2.802111689321758e-11\nx0: -5.293497604912803e-06\n\n\n[['x0', -5.293497604912803e-06]]\n\n\n\nspot_1.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#same-but-with-ei-as-infill_criterion",
    "href": "012_num_spot_ei.html#same-but-with-ei-as-infill_criterion",
    "title": "12  Expected Improvement",
    "section": "12.2 Same, but with EI as infill_criterion",
    "text": "12.2 Same, but with EI as infill_criterion\n\nPREFIX = \"07_EI_ISO\"\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    lower = np.array([-1]),\n    upper = np.array([1]),\n    fun_evals = 25,\n    tolerance_x = np.sqrt(np.spacing(1)),\n    infill_criterion = \"ei\")\n\nCreated spot_tensorboard_path: runs/spot_logs/07_EI_ISO_p040025_2024-01-15_00-13-39 for SummaryWriter()\n\n\n\nspot_1_ei = spot.Spot(fun=fun,\n                     fun_control=fun_control)\nspot_1_ei.run()\n\nspotPython tuning: 9.993558891826623e-09 [####------] 44.00% \nspotPython tuning: 9.993558891826623e-09 [#####-----] 48.00% \nspotPython tuning: 9.993558891826623e-09 [#####-----] 52.00% \nspotPython tuning: 9.993558891826623e-09 [######----] 56.00% \nspotPython tuning: 3.016921825539976e-12 [######----] 60.00% \nspotPython tuning: 3.016921825539976e-12 [######----] 64.00% \nspotPython tuning: 3.016921825539976e-12 [#######---] 68.00% \nspotPython tuning: 3.016921825539976e-12 [#######---] 72.00% \nspotPython tuning: 3.016921825539976e-12 [########--] 76.00% \nspotPython tuning: 3.016921825539976e-12 [########--] 80.00% \nspotPython tuning: 3.016921825539976e-12 [########--] 84.00% \nspotPython tuning: 3.016921825539976e-12 [#########-] 88.00% \nspotPython tuning: 3.016921825539976e-12 [#########-] 92.00% \nspotPython tuning: 3.016921825539976e-12 [##########] 96.00% \nspotPython tuning: 3.016921825539976e-12 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d641c6d0&gt;\n\n\n\nspot_1_ei.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_1_ei.print_results()\n\nmin y: 3.016921825539976e-12\nx0: 1.7369288487269638e-06\n\n\n[['x0', 1.7369288487269638e-06]]\n\n\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model. Expected improvement, isotropic Kriging.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#non-isotropic-kriging",
    "href": "012_num_spot_ei.html#non-isotropic-kriging",
    "title": "12  Expected Improvement",
    "section": "12.3 Non-isotropic Kriging",
    "text": "12.3 Non-isotropic Kriging\n\nPREFIX = \"07_EI_NONISO\"\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    lower = np.array([-1, -1]),\n    upper = np.array([1, 1]),\n    fun_evals = 25,\n    tolerance_x = np.sqrt(np.spacing(1)),\n    infill_criterion = \"ei\")\nsurrogate_control = surrogate_control_init(\n    n_theta=2,\n    noise=False,\n    )\n\nCreated spot_tensorboard_path: runs/spot_logs/07_EI_NONISO_p040025_2024-01-15_00-13-41 for SummaryWriter()\n\n\n\nspot_2_ei_noniso = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   surrogate_control=surrogate_control)\nspot_2_ei_noniso.run()\n\nspotPython tuning: 2.035369116580917e-05 [####------] 44.00% \nspotPython tuning: 2.035369116580917e-05 [#####-----] 48.00% \nspotPython tuning: 2.035369116580917e-05 [#####-----] 52.00% \nspotPython tuning: 1.0764759208059285e-05 [######----] 56.00% \nspotPython tuning: 1.0764759208059285e-05 [######----] 60.00% \nspotPython tuning: 1.2512039520452527e-07 [######----] 64.00% \nspotPython tuning: 1.2512039520452527e-07 [#######---] 68.00% \nspotPython tuning: 1.2512039520452527e-07 [#######---] 72.00% \nspotPython tuning: 1.2512039520452527e-07 [########--] 76.00% \nspotPython tuning: 1.2512039520452527e-07 [########--] 80.00% \nspotPython tuning: 1.2512039520452527e-07 [########--] 84.00% \nspotPython tuning: 1.2512039520452527e-07 [#########-] 88.00% \nspotPython tuning: 1.2512039520452527e-07 [#########-] 92.00% \nspotPython tuning: 1.2512039520452527e-07 [##########] 96.00% \nspotPython tuning: 1.2512039520452527e-07 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d656a890&gt;\n\n\n\nspot_2_ei_noniso.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_2_ei_noniso.print_results()\n\nmin y: 1.2512039520452527e-07\nx0: -0.00023903776922459534\nx1: 0.0002607323150065108\n\n\n[['x0', -0.00023903776922459534], ['x1', 0.0002607323150065108]]\n\n\n\nspot_2_ei_noniso.surrogate.plot()\n\n\n\n\n\n\n\n\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model. Expected improvement, isotropic Kriging.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#using-sklearn-surrogates",
    "href": "012_num_spot_ei.html#using-sklearn-surrogates",
    "title": "12  Expected Improvement",
    "section": "12.4 Using sklearn Surrogates",
    "text": "12.4 Using sklearn Surrogates\n\n12.4.1 The spot Loop\nThe spot loop consists of the following steps:\n\nInit: Build initial design \\(X\\)\nEvaluate initial design on real objective \\(f\\): \\(y = f(X)\\)\nBuild surrogate: \\(S = S(X,y)\\)\nOptimize on surrogate: \\(X_0 =  \\text{optimize}(S)\\)\nEvaluate on real objective: \\(y_0 = f(X_0)\\)\nImpute (Infill) new points: \\(X = X \\cup X_0\\), \\(y = y \\cup y_0\\).\nGot 3.\n\nThe spot loop is implemented in R as follows:\n\n\n\nVisual representation of the model based search with SPOT. Taken from: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67–114.\n\n\n\n\n12.4.2 spot: The Initial Model\n\n12.4.2.1 Example: Modifying the initial design size\nThis is the “Example: Modifying the initial design size” from Chapter 4.5.1 in [bart21i].\n\nspot_ei = spot.Spot(fun=fun,\n                fun_control=fun_control_init(\n                lower = np.array([-1,-1]),\n                upper= np.array([1,1])), \n                design_control = design_control_init(init_size=5))\nspot_ei.run()\n\nspotPython tuning: 0.1377171852680486 [####------] 40.00% \nspotPython tuning: 0.008763557388693657 [#####-----] 46.67% \nspotPython tuning: 0.002832279071142736 [#####-----] 53.33% \nspotPython tuning: 0.0008138662965600185 [######----] 60.00% \nspotPython tuning: 0.00036637583790222027 [#######---] 66.67% \nspotPython tuning: 0.00036006945938022686 [#######---] 73.33% \nspotPython tuning: 0.0003591078890308837 [########--] 80.00% \nspotPython tuning: 0.00032713515580249373 [#########-] 86.67% \nspotPython tuning: 0.0002785854368057176 [#########-] 93.33% \nspotPython tuning: 0.0001638494253170647 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d66ea690&gt;\n\n\n\nspot_ei.plot_progress()\n\n\n\n\n\n\n\n\n\nnp.min(spot_1.y), np.min(spot_ei.y)\n\n(2.802111689321758e-11, 0.0001638494253170647)\n\n\n\n\n\n12.4.3 Init: Build Initial Design\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\ngen = spacefilling(2)\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\n\n[[ 8.97647221 13.41926847]\n [ 0.66946019  1.22344228]\n [ 5.23614115 13.78185824]\n [ 5.6149825  11.5851384 ]\n [-1.72963184  1.66516096]\n [-4.26945568  7.1325531 ]\n [ 1.26363761 10.17935555]\n [ 2.88779942  8.05508969]\n [-3.39111089  4.15213772]\n [ 7.30131231  5.22275244]]\n[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975\n  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]\n\n\n\nS = Kriging(name='kriging',  seed=123)\nS.fit(X, y)\nS.plot()\n\n\n\n\n\n\n\n\n\ngen = spacefilling(2, seed=123)\nX0 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=345)\nX1 = gen.scipy_lhd(3)\nX2 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=123)\nX3 = gen.scipy_lhd(3)\nX0, X1, X2, X3\n\n(array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]),\n array([[0.78373509, 0.86811887],\n        [0.06692621, 0.6058029 ],\n        [0.41374778, 0.00525456]]),\n array([[0.121357  , 0.69043832],\n        [0.41906219, 0.32838498],\n        [0.86742658, 0.52910374]]),\n array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]))\n\n\n\n\n12.4.4 Evaluate\n\n\n12.4.5 Build Surrogate\n\n\n12.4.6 A Simple Predictor\nThe code below shows how to use a simple model for prediction.\n\nAssume that only two (very costly) measurements are available:\n\nf(0) = 0.5\nf(2) = 2.5\n\nWe are interested in the value at \\(x_0 = 1\\), i.e., \\(f(x_0 = 1)\\), but cannot run an additional, third experiment.\n\n\nfrom sklearn import linear_model\nX = np.array([[0], [2]])\ny = np.array([0.5, 2.5])\nS_lm = linear_model.LinearRegression()\nS_lm = S_lm.fit(X, y)\nX0 = np.array([[1]])\ny0 = S_lm.predict(X0)\nprint(y0)\n\n[1.5]\n\n\n\nCentral Idea:\n\nEvaluation of the surrogate model S_lm is much cheaper (or / and much faster) than running the real-world experiment \\(f\\).",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#gaussian-processes-regression-basic-introductory-example",
    "href": "012_num_spot_ei.html#gaussian-processes-regression-basic-introductory-example",
    "title": "12  Expected Improvement",
    "section": "12.5 Gaussian Processes regression: basic introductory example",
    "text": "12.5 Gaussian Processes regression: basic introductory example\nThis example was taken from scikit-learn. After fitting our model, we see that the hyperparameters of the kernel have been optimized. Now, we will use our kernel to compute the mean prediction of the full dataset and plot the 95% confidence interval.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math as m\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nX = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\ny = np.squeeze(X * np.sin(X))\nrng = np.random.RandomState(1)\ntraining_indices = rng.choice(np.arange(y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\ngaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\ngaussian_process.fit(X_train, y_train)\ngaussian_process.kernel_\n\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n\n\n\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport matplotlib.pyplot as plt\nrng = np.random.RandomState(1)\nX = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\ny = np.squeeze(X * np.sin(X))\ntraining_indices = rng.choice(np.arange(y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]\n\n\nS = Kriging(name='kriging',  seed=123, log_level=50, cod_type=\"norm\")\nS.fit(X_train, y_train)\n\nmean_prediction, std_prediction, ei = S.predict(X, return_val=\"all\")\n\nstd_prediction\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"spotPython Version: Gaussian process regression on noise-free dataset\")",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#the-surrogate-using-scikit-learn-models",
    "href": "012_num_spot_ei.html#the-surrogate-using-scikit-learn-models",
    "title": "12  Expected Improvement",
    "section": "12.6 The Surrogate: Using scikit-learn models",
    "text": "12.6 The Surrogate: Using scikit-learn models\nDefault is the internal kriging surrogate.\n\nS_0 = Kriging(name='kriging', seed=123)\n\nModels from scikit-learn can be selected, e.g., Gaussian Process:\n\n# Needed for the sklearn surrogates:\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport pandas as pd\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\nS_GP = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n\nand many more:\n\n\nS_Tree = DecisionTreeRegressor(random_state=0)\nS_LM = linear_model.LinearRegression()\nS_Ridge = linear_model.Ridge()\nS_RF = RandomForestRegressor(max_depth=2, random_state=0) \n\n\nThe scikit-learn GP model S_GP is selected.\n\n\nS = S_GP\n\n\nisinstance(S, GaussianProcessRegressor)\n\nTrue\n\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_branin\nfun_control = fun_control_init(\n    lower = np.array([-5,-0]),\n    upper = np.array([10,15]),\n    fun_evals = 15)    \ndesign_control = design_control_init(init_size=5)\nspot_GP = spot.Spot(fun=fun, \n                    fun_control=fun_control,\n                    surrogate=S, \n                    design_control=design_control)\nspot_GP.run()\n\nspotPython tuning: 24.51465459019188 [####------] 40.00% \nspotPython tuning: 11.003077541587748 [#####-----] 46.67% \nspotPython tuning: 11.003077541587748 [#####-----] 53.33% \nspotPython tuning: 7.281227279299504 [######----] 60.00% \nspotPython tuning: 7.281227279299504 [#######---] 66.67% \nspotPython tuning: 7.281227279299504 [#######---] 73.33% \nspotPython tuning: 2.9519489314482 [########--] 80.00% \nspotPython tuning: 2.9519489314482 [#########-] 86.67% \nspotPython tuning: 2.104972804244822 [#########-] 93.33% \nspotPython tuning: 1.9431600962086772 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2dcde8e90&gt;\n\n\n\nspot_GP.y\n\narray([ 69.32459936, 152.38491454, 107.92560483,  24.51465459,\n        76.73500031,  86.30425303,  11.00307754,  16.11742138,\n         7.28122728,  21.82317903,  10.96088904,   2.95194893,\n         3.02910742,   2.1049728 ,   1.9431601 ])\n\n\n\nspot_GP.plot_progress()\n\n\n\n\n\n\n\n\n\nspot_GP.print_results()\n\nmin y: 1.9431600962086772\nx0: 10.0\nx1: 2.9985482809555464\n\n\n[['x0', 10.0], ['x1', 2.9985482809555464]]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#additional-examples",
    "href": "012_num_spot_ei.html#additional-examples",
    "title": "12  Expected Improvement",
    "section": "12.7 Additional Examples",
    "text": "12.7 Additional Examples\n\n# Needed for the sklearn surrogates:\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport pandas as pd\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\nS_GP = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\n\nS_K = Kriging(name='kriging',\n              seed=123,\n              log_level=50,\n              infill_criterion = \"y\",\n              n_theta=1,\n              noise=False,\n              cod_type=\"norm\")\nfun = analytical().fun_sphere\n\nfun_control = fun_control_init(\n    lower = np.array([-1,-1]),\n    upper = np.array([1,1]),\n    fun_evals = 25)\n\nspot_S_K = spot.Spot(fun=fun,\n                     fun_control=fun_control,\n                     surrogate=S_K,\n                     design_control=design_control,\n                     surrogate_control=surrogate_control)\nspot_S_K.run()\n\nspotPython tuning: 0.13771718778810743 [##--------] 24.00% \nspotPython tuning: 0.008768000187888899 [###-------] 28.00% \nspotPython tuning: 0.0028300907437246053 [###-------] 32.00% \nspotPython tuning: 0.0008148020998531609 [####------] 36.00% \nspotPython tuning: 0.00036681248440550095 [####------] 40.00% \nspotPython tuning: 0.00035607605553701025 [####------] 44.00% \nspotPython tuning: 0.00035607605553701025 [#####-----] 48.00% \nspotPython tuning: 0.00033033596693814263 [#####-----] 52.00% \nspotPython tuning: 0.0002774179969789593 [######----] 56.00% \nspotPython tuning: 0.00016886412273302311 [######----] 60.00% \nspotPython tuning: 2.0349536932144563e-05 [######----] 64.00% \nspotPython tuning: 1.6621220007683266e-06 [#######---] 68.00% \nspotPython tuning: 4.905822935561126e-07 [#######---] 72.00% \nspotPython tuning: 4.7634545282279014e-07 [########--] 76.00% \nspotPython tuning: 3.966290585455581e-07 [########--] 80.00% \nspotPython tuning: 1.9602185212475464e-07 [########--] 84.00% \nspotPython tuning: 1.7115221726800905e-07 [#########-] 88.00% \nspotPython tuning: 1.7115221726800905e-07 [#########-] 92.00% \nspotPython tuning: 1.7115221726800905e-07 [##########] 96.00% \nspotPython tuning: 1.7115221726800905e-07 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2dd210490&gt;\n\n\n\nspot_S_K.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_S_K.surrogate.plot()\n\n\n\n\n\n\n\n\n\nspot_S_K.print_results()\n\nmin y: 1.7115221726800905e-07\nx0: 0.0003105897139994429\nx1: 0.0002732878460995902\n\n\n[['x0', 0.0003105897139994429], ['x1', 0.0002732878460995902]]\n\n\n\n12.7.1 Optimize on Surrogate\n\n\n12.7.2 Evaluate on Real Objective\n\n\n12.7.3 Impute / Infill new Points",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#tests",
    "href": "012_num_spot_ei.html#tests",
    "title": "12  Expected Improvement",
    "section": "12.8 Tests",
    "text": "12.8 Tests\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom spotPython.fun.objectivefunctions import analytical\n\nfun_sphere = analytical().fun_sphere\n\nfun_control = fun_control_init(\n                    lower=np.array([-1, -1]),\n                    upper=np.array([1, 1]),\n                    n_points = 2)\nspot_1 = spot.Spot(\n    fun=fun_sphere,\n    fun_control=fun_control,\n)\n\n# (S-2) Initial Design:\nspot_1.X = spot_1.design.scipy_lhd(\n    spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n)\nprint(spot_1.X)\n\n# (S-3): Eval initial design:\nspot_1.y = spot_1.fun(spot_1.X)\nprint(spot_1.y)\n\nspot_1.fit_surrogate()\nX0 = spot_1.suggest_new_X()\nprint(X0)\nassert X0.size == spot_1.n_points * spot_1.k\n\n[[ 0.86352963  0.7892358 ]\n [-0.24407197 -0.83687436]\n [ 0.36481882  0.8375811 ]\n [ 0.415331    0.54468512]\n [-0.56395091 -0.77797854]\n [-0.90259409 -0.04899292]\n [-0.16484832  0.35724741]\n [ 0.05170659  0.07401196]\n [-0.78548145 -0.44638164]\n [ 0.64017497 -0.30363301]]\n[1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n 0.15480068 0.00815134 0.81623768 0.502017  ]\n[[0.00159092 0.00410652]\n [0.00190779 0.00379162]]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#ei-the-famous-schonlau-example",
    "href": "012_num_spot_ei.html#ei-the-famous-schonlau-example",
    "title": "12  Expected Improvement",
    "section": "12.9 EI: The Famous Schonlau Example",
    "text": "12.9 EI: The Famous Schonlau Example\n\nX_train0 = np.array([1, 2, 3, 4, 12]).reshape(-1,1)\nX_train = np.linspace(start=0, stop=10, num=5).reshape(-1, 1)\n\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX_train = np.array([1., 2., 3., 4., 12.]).reshape(-1,1)\ny_train = np.array([0., -1.75, -2, -0.5, 5.])\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False, cod_type=\"norm\")\nS.fit(X_train, y_train)\n\nX = np.linspace(start=0, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nif True:\n    plt.fill_between(\n        X.ravel(),\n        mean_prediction - 2 * std_prediction,\n        mean_prediction + 2 * std_prediction,\n        alpha=0.5,\n        label=r\"95% confidence interval\",\n    )\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n\n\n\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n# plt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, -ei, label=\"Expected Improvement\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n\n\n\n\nS.log\n\n{'negLnLike': array([1.20788205]),\n 'theta': array([-0.9900252]),\n 'p': [],\n 'Lambda': []}",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#ei-the-forrester-example",
    "href": "012_num_spot_ei.html#ei-the-forrester-example",
    "title": "12  Expected Improvement",
    "section": "12.10 EI: The Forrester Example",
    "text": "12.10 EI: The Forrester Example\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\n\n# exact x locations are unknown:\nX_train = np.array([0.0, 0.175, 0.225, 0.3, 0.35, 0.375, 0.5,1]).reshape(-1,1)\n\nfun = analytical().fun_forrester\nfun_control = fun_control_init(\n    PREFIX=\"07_EI_FORRESTER\",\n    sigma=1.0,\n    seed=123,)\ny_train = fun(X_train, fun_control=fun_control)\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False, cod_type=\"norm\")\nS.fit(X_train, y_train)\n\nX = np.linspace(start=0, stop=1, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nif True:\n    plt.fill_between(\n        X.ravel(),\n        mean_prediction - 2 * std_prediction,\n        mean_prediction + 2 * std_prediction,\n        alpha=0.5,\n        label=r\"95% confidence interval\",\n    )\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n\nCreated spot_tensorboard_path: runs/spot_logs/07_EI_FORRESTER_p040025_2024-01-15_00-14-02 for SummaryWriter()\n\n\n\n\n\n\n\n\n\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n# plt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, -ei, label=\"Expected Improvement\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#noise",
    "href": "012_num_spot_ei.html#noise",
    "title": "12  Expected Improvement",
    "section": "12.11 Noise",
    "text": "12.11 Noise\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_sphere\nfun_control = fun_control_init(\n    PREFIX=\"07_Y\",\n    sigma=2.0,\n    seed=123,)\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\ny.shape\nX_train = X.reshape(-1,1)\ny_train = y\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression on noisy dataset\")\n\nCreated spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-14-03 for SummaryWriter()\n[[ 0.63529627]\n [-4.10764204]\n [-0.44071975]\n [ 9.63125638]\n [-8.3518118 ]\n [-3.62418901]\n [ 4.15331   ]\n [ 3.4468512 ]\n [ 6.36049088]\n [-7.77978539]]\n[-1.57464135 16.13714981  2.77008442 93.14904827 71.59322218 14.28895359\n 15.9770567  12.96468767 39.82265329 59.88028242]\n\n\n\n\n\n\n\n\n\n\nS.log\n\n{'negLnLike': array([26.18505386]),\n 'theta': array([-1.10547474]),\n 'p': [],\n 'Lambda': []}\n\n\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression with nugget on noisy dataset\")\n\n\n\n\n\n\n\n\n\nS.log\n\n{'negLnLike': array([21.82059174]),\n 'theta': array([-2.96946062]),\n 'p': [],\n 'Lambda': array([4.28985898e-05])}",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#cubic-function",
    "href": "012_num_spot_ei.html#cubic-function",
    "title": "12  Expected Improvement",
    "section": "12.12 Cubic Function",
    "text": "12.12 Cubic Function\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_cubed\nfun_control = fun_control_init(\n    PREFIX=\"07_Y\",\n    sigma=10.0,\n    seed=123,)\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\ny.shape\nX_train = X.reshape(-1,1)\ny_train = y\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Cubed: Gaussian process regression on noisy dataset\")\n\nCreated spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-14-03 for SummaryWriter()\n[[ 0.63529627]\n [-4.10764204]\n [-0.44071975]\n [ 9.63125638]\n [-8.3518118 ]\n [-3.62418901]\n [ 4.15331   ]\n [ 3.4468512 ]\n [ 6.36049088]\n [-7.77978539]]\n[ 2.56406437e-01 -6.93071067e+01 -8.56027124e-02  8.93405931e+02\n -5.82561927e+02 -4.76028022e+01  7.16445311e+01  4.09512920e+01\n  2.57319028e+02 -4.70871982e+02]\n\n\n\n\n\n\n\n\n\n\nS = Kriging(name='kriging',  seed=123, log_level=0, n_theta=1, noise=True)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Cubed: Gaussian process with nugget regression on noisy dataset\")\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = fun_control_init(\n    PREFIX=\"07_Y\",\n    sigma=0.25,\n    seed=123,)\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\ny.shape\nX_train = X.reshape(-1,1)\ny_train = y\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noisy dataset\")\n\nCreated spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-14-04 for SummaryWriter()\n[[ 0.63529627]\n [-4.10764204]\n [-0.44071975]\n [ 9.63125638]\n [-8.3518118 ]\n [-3.62418901]\n [ 4.15331   ]\n [ 3.4468512 ]\n [ 6.36049088]\n [-7.77978539]]\n[0.712453   0.05595118 0.83735691 0.0106654  0.01413372 0.07074765\n 0.05479457 0.07763503 0.02412205 0.01625354]\n\n\n\n\n\n\n\n\n\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression with nugget on noisy dataset\")",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#modifying-lambda-search-space",
    "href": "012_num_spot_ei.html#modifying-lambda-search-space",
    "title": "12  Expected Improvement",
    "section": "12.13 Modifying Lambda Search Space",
    "text": "12.13 Modifying Lambda Search Space\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True,\n            min_Lambda=0.1,\n            max_Lambda=10)\nS.fit(X_train, y_train)\n\nprint(f\"Lambda: {S.Lambda}\")\n\nLambda: 0.1\n\n\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression with nugget on noisy dataset. Modified Lambda search space.\")",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "012_num_spot_ei.html#factors",
    "href": "012_num_spot_ei.html#factors",
    "title": "12  Expected Improvement",
    "section": "12.14 Factors",
    "text": "12.14 Factors\n\n[\"num\"] * 3\n\n['num', 'num', 'num']\n\n\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\nimport numpy as np\n\n\ngen = spacefilling(2)\nn = 30\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin_factor\n#fun = analytical(sigma=0).fun_sphere\n\nX0 = gen.scipy_lhd(n, lower=lower, upper = upper)\nX1 = np.random.randint(low=1, high=3, size=(n,))\nX = np.c_[X0, X1]\ny = fun(X)\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\nS.fit(X, y)\nSf = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"factor\"])\nSf.fit(X, y)\nn = 50\nX0 = gen.scipy_lhd(n, lower=lower, upper = upper)\nX1 = np.random.randint(low=1, high=3, size=(n,))\nX = np.c_[X0, X1]\ny = fun(X)\ns=np.sum(np.abs(S.predict(X)[0] - y))\nsf=np.sum(np.abs(Sf.predict(X)[0] - y))\nsf - s\n\n-55.49075685088792\n\n\n\n# vars(S)\n\n\n# vars(Sf)",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Expected Improvement</span>"
    ]
  },
  {
    "objectID": "013_num_spot_noisy.html",
    "href": "013_num_spot_noisy.html",
    "title": "13  Handling Noise",
    "section": "",
    "text": "13.1 Example: Spot and the Noisy Sphere Function\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nimport matplotlib.pyplot as plt\nfrom spotPython.utils.init import fun_control_init\nfrom spotPython.utils.file import get_spot_tensorboard_path\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n\nPREFIX = \"08\"",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Handling Noise</span>"
    ]
  },
  {
    "objectID": "013_num_spot_noisy.html#example-spot-and-the-noisy-sphere-function",
    "href": "013_num_spot_noisy.html#example-spot-and-the-noisy-sphere-function",
    "title": "13  Handling Noise",
    "section": "",
    "text": "13.1.1 The Objective Function: Noisy Sphere\nThe spotPython package provides several classes of objective functions, which return a one-dimensional output \\(y=f(x)\\) for a given input \\(x\\) (independent variable). Several objective functions allow one- or multidimensional input, some also combinations of real-valued and categorial input values.\nAn objective function is considered as “analytical” if it can be described by a closed mathematical formula, e.g., \\[\nf(x, y) = x^2 + y^2.\n\\]\nTo simulate measurement errors, adding artificial noise to the function value \\(y\\) is a common practice, e.g.,:\n\\[\nf(x, y) = x^2 + y^2 + \\epsilon.\n\\]\nUsually, noise is assumed to be normally distributed with mean \\(\\mu=0\\) and standard deviation \\(\\sigma\\). spotPython uses numpy’s scale parameter, which specifies the standard deviation (spread or “width”) of the distribution is used. This must be a non-negative value, see https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html.\n\n\n\n\n\n\nExample: The sphere function without noise\n\n\n\nThe default setting does not use any noise.\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_sphere\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: The sphere function with noise\n\n\n\nNoise can be added to the sphere function as follows:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical(seed=123, sigma=0.02).fun_sphere\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n13.1.2 Reproducibility: Noise Generation and Seed Handling\nspotPython provides two mechanisms for generating random noise:\n\nThe seed is initialized once, i.e., when the objective function is instantiated. This can be done using the following call: fun = analytical(sigma=0.02, seed=123).fun_sphere.\nThe seed is set every time the objective function is called. This can be done using the following call: y = fun(x, sigma=0.02, seed=123).\n\nThese two different ways lead to different results as explained in the following tables:\n\n\n\n\n\n\nExample: Noise added to the sphere function\n\n\n\nSince sigma is set to 0.02, noise is added to the function:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical(sigma=0.02, seed=123).fun_sphere\nx = np.array([1]).reshape(-1,1)\nfor i in range(3):\n    print(f\"{i}: {fun(x)}\")\n\n0: [0.98021757]\n1: [0.99264427]\n2: [1.02575851]\n\n\nThe seed is set once. Every call to fun() results in a different value. The whole experiment can be repeated, the initial seed is used to generate the same sequence as shown below:\n\n\n\n\n\n\n\n\nExample: Noise added to the sphere function\n\n\n\nSince sigma is set to 0.02, noise is added to the function:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical(sigma=0.02, seed=123).fun_sphere\nx = np.array([1]).reshape(-1,1)\nfor i in range(3):\n    print(f\"{i}: {fun(x)}\")\n\n0: [0.98021757]\n1: [0.99264427]\n2: [1.02575851]\n\n\n\n\nIf spotPython is used as a hyperparameter tuner, it is important that only one realization of the noise function is optimized. This behaviour can be accomplished by passing the same seed via the dictionary fun_control to every call of the objective function fun as shown below:\n\n\n\n\n\n\nExample: The same noise added to the sphere function\n\n\n\nSince sigma is set to 0.02, noise is added to the function:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_sphere\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    sigma=0.02)\ny = fun(x, fun_control=fun_control)\nx = np.array([1]).reshape(-1,1)\nfor i in range(3):\n    print(f\"{i}: {fun(x)}\")\n\nCreated spot_tensorboard_path: runs/spot_logs/08_p040025_2024-01-15_00-14-48 for SummaryWriter()\n0: [0.98021757]\n1: [0.98021757]\n2: [0.98021757]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Handling Noise</span>"
    ]
  },
  {
    "objectID": "013_num_spot_noisy.html#spotpythons-noise-handling-approaches",
    "href": "013_num_spot_noisy.html#spotpythons-noise-handling-approaches",
    "title": "13  Handling Noise",
    "section": "13.2 spotPython’s Noise Handling Approaches",
    "text": "13.2 spotPython’s Noise Handling Approaches\nThe following setting will be used for the next steps:\n\nfun = analytical().fun_sphere\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    sigma=0.02,\n)\n\nCreated spot_tensorboard_path: runs/spot_logs/08_p040025_2024-01-15_00-14-48 for SummaryWriter()\n\n\nspotPython is adopted as follows to cope with noisy functions:\n\nfun_repeats is set to a value larger than 1 (here: 2)\nnoise is set to true. Therefore, a nugget (Lambda) term is added to the correlation matrix\ninit size (of the design_control dictionary) is set to a value larger than 1 (here: 3)\n\n\nspot_1_noisy = spot.Spot(fun=fun,\n                   fun_control=fun_control_init(\n                                    lower = np.array([-1]),\n                                    upper = np.array([1]),\n                                    fun_evals = 20,\n                                    fun_repeats = 2,\n                                    noise = True,\n                                    show_models=True),\n                   design_control=design_control_init(init_size=3, repeats=2),\n                   surrogate_control=surrogate_control_init(noise=True))\n\n\nspot_1_noisy.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspotPython tuning: 0.034754930918721325 [####------] 40.00% \nspotPython tuning: 0.03339769765455772 [#####-----] 50.00% \nspotPython tuning: 0.015786142156557437 [######----] 60.00% \nspotPython tuning: 0.0005791214064992311 [#######---] 70.00% \nspotPython tuning: 3.5506618676925576e-05 [########--] 80.00% \nspotPython tuning: 5.325186406243954e-07 [#########-] 90.00% \nspotPython tuning: 4.335518265610199e-07 [##########] 100.00% Done...",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Handling Noise</span>"
    ]
  },
  {
    "objectID": "013_num_spot_noisy.html#print-the-results",
    "href": "013_num_spot_noisy.html#print-the-results",
    "title": "13  Handling Noise",
    "section": "13.3 Print the Results",
    "text": "13.3 Print the Results\n\nspot_1_noisy.print_results()\n\nmin y: 4.335518265610199e-07\nx0: 0.0006584465252099216\nmin mean y: 4.335518265610199e-07\nx0: 0.0006584465252099216\n\n\n[['x0', 0.0006584465252099216], ['x0', 0.0006584465252099216]]\n\n\n\nspot_1_noisy.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX + \"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Handling Noise</span>"
    ]
  },
  {
    "objectID": "013_num_spot_noisy.html#noise-and-surrogates-the-nugget-effect",
    "href": "013_num_spot_noisy.html#noise-and-surrogates-the-nugget-effect",
    "title": "13  Handling Noise",
    "section": "13.4 Noise and Surrogates: The Nugget Effect",
    "text": "13.4 Noise and Surrogates: The Nugget Effect\n\n13.4.1 The Noisy Sphere\n\n13.4.1.1 The Data\n\nWe prepare some data first:\n\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_sphere\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    sigma=4)\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\ny = fun(X, fun_control=fun_control)\nX_train = X.reshape(-1,1)\ny_train = y\n\nCreated spot_tensorboard_path: runs/spot_logs/08_p040025_2024-01-15_00-14-54 for SummaryWriter()\n\n\n\nA surrogate without nugget is fitted to these data:\n\n\nS = Kriging(name='kriging',\n            n_theta=1,\n            noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression on noisy dataset\")\n\n\n\n\n\n\n\n\n\nIn comparison to the surrogate without nugget, we fit a surrogate with nugget to the data:\n\n\nS_nug = Kriging(name='kriging',\n            n_theta=1,\n            noise=True)\nS_nug.fit(X_train, y_train)\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S_nug.predict(X_axis, return_val=\"all\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression with nugget on noisy dataset\")\n\n\n\n\n\n\n\n\n\nThe value of the nugget term can be extracted from the model as follows:\n\n\nS.Lambda\n\n\nS_nug.Lambda\n\n0.00055921881757264\n\n\n\nWe see:\n\nthe first model S has no nugget,\nwhereas the second model has a nugget value (Lambda) larger than zero.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Handling Noise</span>"
    ]
  },
  {
    "objectID": "013_num_spot_noisy.html#exercises",
    "href": "013_num_spot_noisy.html#exercises",
    "title": "13  Handling Noise",
    "section": "13.5 Exercises",
    "text": "13.5 Exercises\n\n13.5.1 Noisy fun_cubed\n\nAnalyse the effect of noise on the fun_cubed function with the following settings:\n\n\nfun = analytical().fun_cubed\nfun_control = fun_control_init(\n    sigma=10)\nlower = np.array([-10])\nupper = np.array([10])\n\n\n\n13.5.2 fun_runge\n\nAnalyse the effect of noise on the fun_runge function with the following settings:\n\n\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = fun_control_init(\n    sigma=0.25)\n\n\n\n13.5.3 fun_forrester\n\nAnalyse the effect of noise on the fun_forrester function with the following settings:\n\n\nlower = np.array([0])\nupper = np.array([1])\nfun = analytical().fun_forrester\nfun_control = fun_control_init(\n    sigma=5)\n\n\n\n13.5.4 fun_xsin\n\nAnalyse the effect of noise on the fun_xsin function with the following settings:\n\n\nlower = np.array([-1.])\nupper = np.array([1.])\nfun = analytical().fun_xsin\nfun_control = fun_control_init(    \n    sigma=0.5)",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Handling Noise</span>"
    ]
  },
  {
    "objectID": "014_num_spot_ocba.html",
    "href": "014_num_spot_ocba.html",
    "title": "14  Optimal Computational Budget Allocation in Spot",
    "section": "",
    "text": "14.1 Example: Spot, OCBA, and the Noisy Sphere Function\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nimport matplotlib.pyplot as plt\nfrom spotPython.utils.init import fun_control_init\nfrom spotPython.utils.file import get_spot_tensorboard_path\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n\nPREFIX = \"09\"",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Optimal Computational Budget Allocation in `Spot`</span>"
    ]
  },
  {
    "objectID": "014_num_spot_ocba.html#example-spot-ocba-and-the-noisy-sphere-function",
    "href": "014_num_spot_ocba.html#example-spot-ocba-and-the-noisy-sphere-function",
    "title": "14  Optimal Computational Budget Allocation in Spot",
    "section": "",
    "text": "14.1.1 The Objective Function: Noisy Sphere\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function with noise, i.e., a function that can be described by a (closed) formula: \\[f(x) = x^2 + \\epsilon\\]\nSince sigma is set to 0.1, noise is added to the function:\n\nfun = analytical().fun_sphere\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    sigma=0.1)\n\nCreated spot_tensorboard_path: runs/spot_logs/09_p040025_2024-01-15_00-15-13 for SummaryWriter()\n\n\nA plot illustrates the noise:\n\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x, fun_control=fun_control)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\n\n\n\n\n\n\n\n\nSpot is adopted as follows to cope with noisy functions:\n\nfun_repeats is set to a value larger than 1 (here: 2)\nnoise is set to true. Therefore, a nugget (Lambda) term is added to the correlation matrix\ninit size (of the design_control dictionary) is set to a value larger than 1 (here: 2)\n\n\nspot_1_noisy = spot.Spot(fun=fun,\n                   fun_control=fun_control_init( \n                   lower = np.array([-1]),\n                   upper = np.array([1]),\n                   fun_evals = 20,\n                   fun_repeats = 2,\n                   infill_criterion=\"ei\",\n                   noise = True,\n                   tolerance_x=0.0,\n                   ocba_delta = 1,                   \n                   show_models=True),\n                   design_control=design_control_init(init_size=3, repeats=2),\n                   surrogate_control=surrogate_control_init(noise=True))\n\n\nspot_1_noisy.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspotPython tuning: 0.03475493366922229 [####------] 40.00% \nspotPython tuning: 0.0004463018568303854 [#####-----] 50.00% \nspotPython tuning: 0.0004463018568303854 [######----] 60.00% \nspotPython tuning: 0.0001590474610240226 [#######---] 70.00% \nspotPython tuning: 4.2454542934289965e-09 [########--] 80.00% \nspotPython tuning: 2.2370853591440457e-10 [#########-] 90.00% \nspotPython tuning: 2.2370853591440457e-10 [##########] 100.00% Done...",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Optimal Computational Budget Allocation in `Spot`</span>"
    ]
  },
  {
    "objectID": "014_num_spot_ocba.html#print-the-results",
    "href": "014_num_spot_ocba.html#print-the-results",
    "title": "14  Optimal Computational Budget Allocation in Spot",
    "section": "14.2 Print the Results",
    "text": "14.2 Print the Results\n\nspot_1_noisy.print_results()\n\nmin y: 2.2370853591440457e-10\nx0: -1.4956889245909544e-05\nmin mean y: 2.2370853591440457e-10\nx0: -1.4956889245909544e-05\n\n\n[['x0', -1.4956889245909544e-05], ['x0', -1.4956889245909544e-05]]\n\n\n\nspot_1_noisy.plot_progress(log_y=False)",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Optimal Computational Budget Allocation in `Spot`</span>"
    ]
  },
  {
    "objectID": "014_num_spot_ocba.html#noise-and-surrogates-the-nugget-effect",
    "href": "014_num_spot_ocba.html#noise-and-surrogates-the-nugget-effect",
    "title": "14  Optimal Computational Budget Allocation in Spot",
    "section": "14.3 Noise and Surrogates: The Nugget Effect",
    "text": "14.3 Noise and Surrogates: The Nugget Effect\n\n14.3.1 The Noisy Sphere\n\n14.3.1.1 The Data\nWe prepare some data first:\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_sphere\nfun_control = fun_control_init(    \n    sigma=2,\n    seed=125)\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\ny = fun(X, fun_control=fun_control)\nX_train = X.reshape(-1,1)\ny_train = y\n\nA surrogate without nugget is fitted to these data:\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression on noisy dataset\")\n\n\n\n\n\n\n\n\nIn comparison to the surrogate without nugget, we fit a surrogate with nugget to the data:\n\nS_nug = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True)\nS_nug.fit(X_train, y_train)\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S_nug.predict(X_axis, return_val=\"all\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression with nugget on noisy dataset\")\n\n\n\n\n\n\n\n\nThe value of the nugget term can be extracted from the model as follows:\n\nS.Lambda\n\n\nS_nug.Lambda\n\n8.374496269458742e-05\n\n\nWe see:\n\nthe first model S has no nugget,\nwhereas the second model has a nugget value (Lambda) larger than zero.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Optimal Computational Budget Allocation in `Spot`</span>"
    ]
  },
  {
    "objectID": "014_num_spot_ocba.html#exercises",
    "href": "014_num_spot_ocba.html#exercises",
    "title": "14  Optimal Computational Budget Allocation in Spot",
    "section": "14.4 Exercises",
    "text": "14.4 Exercises\n\n14.4.1 Noisy fun_cubed\nAnalyse the effect of noise on the fun_cubed function with the following settings:\n\nfun = analytical().fun_cubed\nfun_control = fun_control_init(    \n    sigma=10,\n    seed=123)\nlower = np.array([-10])\nupper = np.array([10])\n\n\n\n14.4.2 fun_runge\nAnalyse the effect of noise on the fun_runge function with the following settings:\n\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = fun_control_init(    \n    sigma=0.25,\n    seed=123)\n\n\n\n14.4.3 fun_forrester\nAnalyse the effect of noise on the fun_forrester function with the following settings:\n\nlower = np.array([0])\nupper = np.array([1])\nfun = analytical().fun_forrester\nfun_control = {\"sigma\": 5,\n               \"seed\": 123}\n\n\n\n14.4.4 fun_xsin\nAnalyse the effect of noise on the fun_xsin function with the following settings:\n\nlower = np.array([-1.])\nupper = np.array([1.])\nfun = analytical().fun_xsin\nfun_control = fun_control_init(    \n    sigma=0.5,\n    seed=123)",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Optimal Computational Budget Allocation in `Spot`</span>"
    ]
  },
  {
    "objectID": "015_num_spot_correlation_p.html",
    "href": "015_num_spot_correlation_p.html",
    "title": "15  Kriging with Varying Correlation-p",
    "section": "",
    "text": "15.1 Example: Spot Surrogate and the 2-dim Sphere Function\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.utils.init import fun_control_init, surrogate_control_init\nPREFIX=\"015\"",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kriging with Varying Correlation-p</span>"
    ]
  },
  {
    "objectID": "015_num_spot_correlation_p.html#example-spot-surrogate-and-the-2-dim-sphere-function",
    "href": "015_num_spot_correlation_p.html#example-spot-surrogate-and-the-2-dim-sphere-function",
    "title": "15  Kriging with Varying Correlation-p",
    "section": "",
    "text": "15.1.1 The Objective Function: 2-dim Sphere\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[f(x, y) = x^2 + y^2\\]\nThe size of the lower bound vector determines the problem dimension.\nHere we will use np.array([-1, -1]), i.e., a two-dim function.\n\n\nfun = analytical().fun_sphere\nfun_control = fun_control_init(PREFIX=PREFIX,\n                               lower = np.array([-1, -1]),\n                               upper = np.array([1, 1]))\n\nCreated spot_tensorboard_path: runs/spot_logs/015_p040025_2024-01-15_00-15-38 for SummaryWriter()\n\n\n\nAlthough the default spot surrogate model is an isotropic Kriging model, we will explicitly set the theta parameter to a value of 1 for both dimensions. This is done to illustrate the difference between isotropic and anisotropic Kriging models.\n\n\nsurrogate_control=surrogate_control_init(n_p=1,\n                                         p_val=2.0,)\n\n\nspot_2 = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   surrogate_control=surrogate_control)\n\nspot_2.run()\n\nspotPython tuning: 1.801603872454505e-05 [#######---] 73.33% \nspotPython tuning: 1.801603872454505e-05 [########--] 80.00% \nspotPython tuning: 1.801603872454505e-05 [#########-] 86.67% \nspotPython tuning: 1.801603872454505e-05 [#########-] 93.33% \nspotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d6795510&gt;\n\n\n\n\n15.1.2 Results\n\nspot_2.print_results()\n\nmin y: 1.801603872454505e-05\nx0: 0.0019077911677074135\nx1: 0.003791618596979743\n\n\n[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]\n\n\n\nspot_2.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_2.surrogate.plot()",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kriging with Varying Correlation-p</span>"
    ]
  },
  {
    "objectID": "015_num_spot_correlation_p.html#example-with-modified-p",
    "href": "015_num_spot_correlation_p.html#example-with-modified-p",
    "title": "15  Kriging with Varying Correlation-p",
    "section": "15.2 Example With Modified p",
    "text": "15.2 Example With Modified p\n\nWe can use set p to a value other than 2 to obtain a different Kriging model.\n\n\nsurrogate_control = surrogate_control_init(n_p=1,\n                                           p_val=1.0)\nspot_2_p1= spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    surrogate_control=surrogate_control)\nspot_2_p1.run()\n\nspotPython tuning: 1.801603872454505e-05 [#######---] 73.33% \nspotPython tuning: 1.801603872454505e-05 [########--] 80.00% \nspotPython tuning: 1.801603872454505e-05 [#########-] 86.67% \nspotPython tuning: 1.801603872454505e-05 [#########-] 93.33% \nspotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d7625b50&gt;\n\n\n\nThe search progress of the optimization with the anisotropic model can be visualized:\n\n\nspot_2_p1.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_2_p1.print_results()\n\nmin y: 1.801603872454505e-05\nx0: 0.0019077911677074135\nx1: 0.003791618596979743\n\n\n[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]\n\n\n\nspot_2_p1.surrogate.plot()\n\n\n\n\n\n\n\n\n\n15.2.1 Taking a Look at the p Values\n\n15.2.1.1 p Values from the spot Model\n\nWe can check, which p values the spot model has used:\nThe p values from the surrogate can be printed as follows:\n\n\nspot_2_p1.surrogate.p\n\narray([1.])\n\n\n\nSince the surrogate from the isotropic setting was stored as spot_2, we can also take a look at the theta value from this model:\n\n\nspot_2.surrogate.p\n\narray([2.])",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kriging with Varying Correlation-p</span>"
    ]
  },
  {
    "objectID": "015_num_spot_correlation_p.html#optimization-of-the-p-values",
    "href": "015_num_spot_correlation_p.html#optimization-of-the-p-values",
    "title": "15  Kriging with Varying Correlation-p",
    "section": "15.3 Optimization of the p Values",
    "text": "15.3 Optimization of the p Values\n\nsurrogate_control = surrogate_control_init(n_p=1,\n                                           optim_p=True)\nspot_2_pm= spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    surrogate_control=surrogate_control)\nspot_2_pm.run()\n\nspotPython tuning: 1.893023485380876e-05 [#######---] 73.33% \nspotPython tuning: 1.893023485380876e-05 [########--] 80.00% \nspotPython tuning: 1.893023485380876e-05 [#########-] 86.67% \nspotPython tuning: 1.893023485380876e-05 [#########-] 93.33% \nspotPython tuning: 1.893023485380876e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d721a190&gt;\n\n\n\nspot_2_pm.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_2_pm.print_results()\n\nmin y: 1.893023485380876e-05\nx0: 0.0017549984724977892\nx1: 0.003981232876300906\n\n\n[['x0', 0.0017549984724977892], ['x1', 0.003981232876300906]]\n\n\n\nspot_2_pm.surrogate.plot()\n\n\n\n\n\n\n\n\n\nspot_2_pm.surrogate.p\n\narray([1.77398298])",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kriging with Varying Correlation-p</span>"
    ]
  },
  {
    "objectID": "015_num_spot_correlation_p.html#optimization-of-multiple-p-values",
    "href": "015_num_spot_correlation_p.html#optimization-of-multiple-p-values",
    "title": "15  Kriging with Varying Correlation-p",
    "section": "15.4 Optimization of Multiple p Values",
    "text": "15.4 Optimization of Multiple p Values\n\nsurrogate_control = surrogate_control_init(n_p=2,\n                                           optim_p=True)\nspot_2_pmo= spot.Spot(fun=fun,\n                    fun_control=fun_control,\n                    surrogate_control=surrogate_control)\nspot_2_pmo.run()\n\nspotPython tuning: 2.162397189403005e-05 [#######---] 73.33% \nspotPython tuning: 2.162397189403005e-05 [########--] 80.00% \nspotPython tuning: 2.162397189403005e-05 [#########-] 86.67% \nspotPython tuning: 2.162397189403005e-05 [#########-] 93.33% \nspotPython tuning: 2.162397189403005e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2d7f10a50&gt;\n\n\n\nspot_2_pmo.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\nspot_2_pmo.print_results()\n\nmin y: 2.162397189403005e-05\nx0: 0.0018245082309241386\nx1: 0.00427728203527896\n\n\n[['x0', 0.0018245082309241386], ['x1', 0.00427728203527896]]\n\n\n\nspot_2_pmo.surrogate.plot()\n\n\n\n\n\n\n\n\n\nspot_2_pmo.surrogate.p\n\narray([1.09037777, 1.76346322])",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kriging with Varying Correlation-p</span>"
    ]
  },
  {
    "objectID": "015_num_spot_correlation_p.html#exercises",
    "href": "015_num_spot_correlation_p.html#exercises",
    "title": "15  Kriging with Varying Correlation-p",
    "section": "15.5 Exercises",
    "text": "15.5 Exercises\n\n15.5.1 fun_branin\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-5 \\leq x_1 \\leq 10\\) and \\(0 \\leq x_2 \\leq 15\\).\n\nCompare the results from spotPython runs with different options for p.\nModify the termination criterion: instead of the number of evaluations (which is specified via fun_evals), the time should be used as the termination criterion. This can be done as follows (max_time=1 specifies a run time of one minute):\n\n\nfun_evals=inf,\nmax_time=1,\n\n\n\n15.5.2 fun_sin_cos\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-2\\pi \\leq x_1 \\leq 2\\pi\\) and \\(-2\\pi \\leq x_2 \\leq 2\\pi\\).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.\n\n\n\n15.5.3 fun_runge\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-5 \\leq x_1 \\leq 5\\) and \\(-5 \\leq x_2 \\leq 5\\).\n\nCompare the results from spotPython runs with different options for p.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.\n\n\n\n15.5.4 fun_wingwt\n\nDescribe the function.\n\nThe input dimension is 10. The search ranges are between 0 and 1 (values are mapped internally to their natural bounds).\n\nCompare the results from spotPython runs with different options for p.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kriging with Varying Correlation-p</span>"
    ]
  },
  {
    "objectID": "015_num_spot_correlation_p.html#jupyter-notebook",
    "href": "015_num_spot_correlation_p.html#jupyter-notebook",
    "title": "15  Kriging with Varying Correlation-p",
    "section": "15.6 Jupyter Notebook",
    "text": "15.6 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kriging with Varying Correlation-p</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html",
    "href": "025_spot_hpt_river_friedman_amfr.html",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "18.1 Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, size of the data set, and the experiment name.\nMAX_TIME = 30\nINIT_SIZE = 10\nPREFIX=\"025RIVER\"\nK = 0.1",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#sec-setup-51",
    "href": "025_spot_hpt_river_friedman_amfr.html#sec-setup-51",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "MAX_TIME: The maximum run time in seconds for the hyperparameter tuning process.\nINIT_SIZE: The initial design size for the hyperparameter tuning process.\nPREFIX: The prefix for the experiment name.\nK: The factor that determines the number of samples in the data set.\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nK is the multiplier for the number of samples. If it is set to 1, then 100_000samples are taken. It is set to 0.1 for demonstration purposes. For real experiments, this should be increased to at least 1.\n\n\n\n\n\nThis notebook exemplifies hyperparameter tuning with SPOT (spotPython and spotRiver).\nThe hyperparameter software SPOT is available in Python. It was developed in R (statistical programming language), see Open Access book “Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide”, available here: https://link.springer.com/book/10.1007/978-981-19-5170-1.\nThis notebook demonstrates hyperparameter tuning for river. It is based on the notebook “Incremental decision trees in river: the Hoeffding Tree case”, see: https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters.\nHere we will use the river AMFRegressor functions, see: https://riverml.xyz/0.19.0/api/forest/AMFRegressor/.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#initialization-of-the-fun_control-dictionary",
    "href": "025_spot_hpt_river_friedman_amfr.html#initialization-of-the-fun_control-dictionary",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.2 Initialization of the fun_control Dictionary",
    "text": "18.2 Initialization of the fun_control Dictionary\nspotPython supports the visualization of the hyperparameter tuning process with TensorBoard. The following example shows how to use TensorBoard with spotPython.\nFirst, we define an “experiment name” to identify the hyperparameter tuning process. The experiment name is also used to create a directory for the TensorBoard files.\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    max_time=MAX_TIME,\n    fun_evals=inf,\n    tolerance_x=np.sqrt(np.spacing(1)))\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_17_09_44_49\nCreated spot_tensorboard_path: runs/spot_logs/025RIVER_maans14_2024-01-17_09-44-49 for SummaryWriter()\n\n\n\n\n\n\n\n\nTip: TensorBoard\n\n\n\n\nSince the spot_tensorboard_path argument is not None, which is the default, spotPython will log the optimization process in the TensorBoard folder.\nSection 18.8.3 describes how to start TensorBoard and access the TensorBoard dashboard.\nThe TENSORBOARD_CLEAN argument is set to True to archive the TensorBoard folder if it already exists. This is useful if you want to start a hyperparameter tuning process from scratch. If you want to continue a hyperparameter tuning process, set TENSORBOARD_CLEAN to False. Then the TensorBoard folder will not be archived and the old and new TensorBoard files will shown in the TensorBoard dashboard.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#load-data-the-friedman-drift-data",
    "href": "025_spot_hpt_river_friedman_amfr.html#load-data-the-friedman-drift-data",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.3 Load Data: The Friedman Drift Data",
    "text": "18.3 Load Data: The Friedman Drift Data\nWe will use the Friedman synthetic dataset with concept drifts [SOURCE]. Each observation is composed of ten features. Each feature value is sampled uniformly in [0, 1]. Only the first five features are relevant. The target is defined by different functions depending on the type of the drift. Global Recurring Abrupt drift will be used, i.e., the concept drift appears over the whole instance space. There are two points of concept drift. At the second point of drift the old concept reoccurs.\nThe following parameters are used to generate and handle the data set:\n\nhorizon: The prediction horizon in hours.\nn_samples: The number of samples in the data set.\np_1: The position of the first concept drift.\np_2: The position of the second concept drift.\nposition: The position of the concept drifts.\nn_train: The number of samples used for training.\n\n\nhorizon = 7*24\nn_samples = int(K*100_000)\np_1 = int(K*25_000)\np_2 = int(K*50_000)\nposition=(p_1, p_2)\nn_train = 1_000\n\n\nfrom river.datasets import synth\nimport pandas as pd\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n   seed=123\n)\n\n\nWe will use spotRiver’s convert_to_df function [SOURCE] to convert the river data set to a pandas data frame.\n\n\nfrom spotRiver.utils.data_conversion import convert_to_df\ntarget_column = \"y\"\ndf = convert_to_df(dataset, target_column=target_column, n_total=n_samples)\n\n\nAdd column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column of the dataframe.\nThen split the data frame into a training and test data set. The train and test data sets are stored in the fun_control dictionary.\n\n\nfrom spotPython.hyperparameters.values import set_control_key_value\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\nset_control_key_value(control_dict=fun_control,\n                        key=\"train\",\n                        value=df[:n_train],\n                        replace=True)\nset_control_key_value(fun_control, \"test\", df[n_train:], True)\nset_control_key_value(fun_control, \"n_samples\", n_samples, replace=True)\nset_control_key_value(fun_control, \"target_column\", target_column, replace=True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#specification-of-the-preprocessing-model",
    "href": "025_spot_hpt_river_friedman_amfr.html#specification-of-the-preprocessing-model",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.4 Specification of the Preprocessing Model",
    "text": "18.4 Specification of the Preprocessing Model\n\nWe use the StandardScaler [SOURCE] from river as the preprocessing model. The StandardScaler is used to standardize the data set, i.e., it has zero mean and unit variance.\n\n\nfrom river import preprocessing\nprep_model = preprocessing.StandardScaler()\nset_control_key_value(fun_control, \"prep_model\", prep_model, replace=True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "href": "025_spot_hpt_river_friedman_amfr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.5 SelectSelect Model (algorithm) and core_model_hyper_dict",
    "text": "18.5 SelectSelect Model (algorithm) and core_model_hyper_dict\nspotPython hyperparameter tuning approach uses two components:\n\na model (class) and\nan associated hyperparameter dictionary.\n\nThe corresponding hyperparameters are loaded from the associated dictionary, which is stored as a JSON file [SOURCE]. The JSON file contains hyperparameter type information, names, and bounds.\nThe method add_core_model_to_fun_control adds the model and the hyperparameter dictionary to the fun_control dictionary.\nAlternatively, you can load a local hyper_dict. Simply set river_hyper_dict.json as the filename. If filenameis set to None, which is the default, the hyper_dict [SOURCE] is loaded from the spotRiver package.\n\nfrom river.forest import AMFRegressor\nfrom spotRiver.data.river_hyper_dict import RiverHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(core_model=AMFRegressor,\n                              fun_control=fun_control,\n                              hyper_dict=RiverHyperDict,\n                              filename=None)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "025_spot_hpt_river_friedman_amfr.html#modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.6 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "18.6 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nAfter the core_model and the core_model_hyper_dict are added to the fun_control dictionary, the hyperparameter tuning can be started. However, in some settings, the user wants to modify the hyperparameters of the core_model_hyper_dict. This can be done with the modify_hyper_parameter_bounds and modify_hyper_parameter_levels functions [SOURCE].\nThe following code shows how hyperparameter of type numeric and integer (boolean) can be modified. The modify_hyper_parameter_bounds function is used to modify the bounds of the hyperparameter delta and merit_preprune. Similar option exists for the modify_hyper_parameter_levels function to modify the levels of categorical hyperparameters.\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n# modify_hyper_parameter_bounds(fun_control, \"n_estimators\", bounds=[2,100])\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\nset_control_hyperparameter_value(fun_control, \"n_estimators\", [2, 100])\n\n::: {.callout-note} #### Note: Active and Inactive Hyperparameters Hyperparameters can be excluded from the tuning procedure by selecting identical values for the lower and upper bounds.\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name            | type   |   default |   lower |   upper | transform   |\n|-----------------|--------|-----------|---------|---------|-------------|\n| n_estimators    | int    |        10 |     2   |     100 | None        |\n| step            | float  |         1 |     0.1 |      10 | None        |\n| use_aggregation | factor |         1 |     0   |       1 | None        |",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#selection-of-the-objective-loss-function",
    "href": "025_spot_hpt_river_friedman_amfr.html#selection-of-the-objective-loss-function",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.7 Selection of the Objective (Loss) Function",
    "text": "18.7 Selection of the Objective (Loss) Function\nThe metric_sklearn is used for the sklearn based evaluation via eval_oml_horizon [SOURCE]. Here we use the mean_absolute_error [SOURCE] as the objective function.\n\n\n\n\n\n\nNote: Additional metrics\n\n\n\nspotRiver also supports additional metrics. For example, the metric_river is used for the river based evaluation via eval_oml_iter_progressive [SOURCE]. The metric_river is implemented to simulate the behaviour of the “original” river metrics.\n\n\nspotRiver provides information about the model’ s score (metric), memory, and time. The hyperparamter tuner requires a single objective. Therefore, a weighted sum of the metric, memory, and time is computed. The weights are defined in the weights array.\n\n\n\n\n\n\nNote: Weights\n\n\n\nThe weights provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased.\n\n\nThe oml_grace_period defines the number of observations that are used for the initial training of the model. The step defines the iteration number at which to yield results. This only takes into account the predictions, and not the training steps. The weight_coeff defines a multiplier for the results: results are multiplied by (step/n_steps)**weight_coeff, where n_steps is the total number of iterations. Results from the beginning have a lower weight than results from the end if weight_coeff &gt; 1. If weight_coeff == 0, all results have equal weight. Note, that the weight_coeff is only used internally for the tuner and does not affect the results that are used for the evaluation or comparisons.\n\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\n\nweights = np.array([1, 1/1000, 1/1000])*10_000.0\noml_grace_period = 2\nstep = 100\nweight_coeff = 1.0\n\n# fun_control.update({\n#                \"horizon\": horizon,\n#                \"oml_grace_period\": oml_grace_period,\n#                \"weights\": weights,\n#                \"step\": step,\n#                \"weight_coeff\": weight_coeff,\n#                \"metric_sklearn\": mean_absolute_error\n#                })\nset_control_key_value(control_dict=fun_control,\n                        key=\"horizon\",\n                        value=horizon,\n                        replace=True)\nset_control_key_value(fun_control, \"oml_grace_period\", oml_grace_period, True)\nset_control_key_value(fun_control, \"weights\", weights, True)\nset_control_key_value(fun_control, \"step\", step, True)\nset_control_key_value(fun_control, \"weight_coeff\", weight_coeff, True)\nset_control_key_value(fun_control, \"metric_sklearn\", mean_absolute_error, True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#calling-the-spot-function",
    "href": "025_spot_hpt_river_friedman_amfr.html#calling-the-spot-function",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.8 Calling the SPOT Function",
    "text": "18.8 Calling the SPOT Function\n\n18.8.1 The Objective Function\nThe objective function fun_oml_horizon [SOURCE] is selected next.\n\nfrom spotRiver.fun.hyperriver import HyperRiver\nfun = HyperRiver().fun_oml_horizon\n\nThe following code snippet shows how to get the default hyperparameters as an array, so that they can be passed to the Spot function.\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nX_start = get_default_hyperparameters_as_array(fun_control)\n\n\n\n18.8.2 Run the Spot Optimizer\nThe class Spot [SOURCE] is the hyperparameter tuning workhorse. It is initialized with the following parameters:\n\nfun: the objective function\nfun_control: the dictionary with the control parameters for the objective function\ndesign: the experimental design\ndesign_control: the dictionary with the control parameters for the experimental design\nsurrogate: the surrogate model\nsurrogate_control: the dictionary with the control parameters for the surrogate model\noptimizer: the optimizer\noptimizer_control: the dictionary with the control parameters for the optimizer\n\n\n\n\n\n\n\nNote: Total run time\n\n\n\nThe total run time may exceed the specified max_time, because the initial design (here: init_size = INIT_SIZE as specified above) is always evaluated, even if this takes longer than max_time.\n\n\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init()\nset_control_key_value(control_dict=design_control,\n                        key=\"init_size\",\n                        value=INIT_SIZE,\n                        replace=True)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                           n_theta=2)\n\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   design_control=design_control,\n                   surrogate_control=surrogate_control)\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: 2.6545266529839697 [##--------] 16.06% \nspotPython tuning: 2.6545266529839697 [###-------] 33.93% \nspotPython tuning: 2.651839372234021 [#####-----] 47.22% \nspotPython tuning: 2.646670546350594 [######----] 60.23% \nspotPython tuning: 2.646670546350594 [#######---] 74.65% \nspotPython tuning: 2.646670546350594 [#########-] 89.59% \nspotPython tuning: 2.646670546350594 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x31325bc10&gt;\n\n\n\n\n18.8.3 TensorBoard\nNow we can start TensorBoard in the background with the following command, where ./runs is the default directory for the TensorBoard log files:\ntensorboard --logdir=\"./runs\"\n\n\n\n\n\n\nTip: TENSORBOARD_PATH\n\n\n\nThe TensorBoard path can be printed with the following command:\n\nfrom spotPython.utils.file import get_tensorboard_path\nget_tensorboard_path(fun_control)\n\n'runs/'\n\n\n\n\nWe can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\nThe TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate [SOURCE] is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model.\n\n\n\n\n18.8.4 Results\nAfter the hyperparameter tuning run is finished, the results can be saved and reloaded with the following commands:\n\nfrom spotPython.utils.file import save_pickle, get_experiment_name,  load_pickle\nexperiment_name = get_experiment_name(PREFIX)\nSAVE_AND_LOAD = False\nif SAVE_AND_LOAD == True:\n    from spotPython.utils.file import save_pickle, get_experiment_name,  load_pickle\n    save_pickle(spot_tuner, experiment_name)\n    spot_tuner = load_pickle(experiment_name)\n\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The black points represent the performace values (score or metric) of hyperparameter configurations from the initial design, whereas the red points represents the hyperparameter configurations found by the surrogate model based optimization.\n\nspot_tuner.plot_progress(log_y=True, filename=\"./figures/\" + experiment_name+\"_progress.pdf\")\n\n\n\n\n\n\n\n\nResults can also be printed in tabular form.\n\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name            | type   |   default |   lower |   upper |             tuned | transform   |   importance | stars   |\n|-----------------|--------|-----------|---------|---------|-------------------|-------------|--------------|---------|\n| n_estimators    | int    |      10.0 |     2.0 |     100 |              73.0 | None        |        93.70 | **      |\n| step            | float  |       1.0 |     0.1 |      10 | 3.362202688570396 | None        |        26.96 | *       |\n| use_aggregation | factor |       1.0 |     0.0 |       1 |               0.0 | None        |       100.00 | ***     |\n\n\nA histogram can be used to visualize the most important hyperparameters.\n\nspot_tuner.plot_importance(threshold=0.0025, filename=\"./figures/\" + experiment_name+\"_importance.pdf\")",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#the-larger-data-set",
    "href": "025_spot_hpt_river_friedman_amfr.html#the-larger-data-set",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.9 The Larger Data Set",
    "text": "18.9 The Larger Data Set\nAfter the hyperparameter were tuned on a small data set, we can now apply the hyperparameter configuration to a larger data set. The following code snippet shows how to generate the larger data set.\n\n\n\n\n\n\nCaution: Increased Friedman-Drift Data Set\n\n\n\n\nThe Friedman-Drift Data Set is increased by a factor of two to show the transferability of the hyperparameter tuning results.\nLarger values of K lead to a longer run time.\n\n\n\n\nK = 0.2\nn_samples = int(K*100_000)\np_1 = int(K*25_000)\np_2 = int(K*50_000)\nposition=(p_1, p_2)\n\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n   seed=123\n)\n\nThe larger data set is converted to a Pandas data frame and passed to the fun_control dictionary.\n\ndf = convert_to_df(dataset, target_column=target_column, n_total=n_samples)\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\n# fun_control.update({\"train\": df[:n_train],\n#                     \"test\": df[n_train:],\n#                     \"n_samples\": n_samples,\n#                     \"target_column\": target_column})\nset_control_key_value(fun_control, \"train\", df[:n_train], True)\nset_control_key_value(fun_control, \"test\", df[n_train:], True)\nset_control_key_value(fun_control, \"n_samples\", n_samples, True)\nset_control_key_value(fun_control, \"target_column\", target_column, True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#get-default-hyperparameters",
    "href": "025_spot_hpt_river_friedman_amfr.html#get-default-hyperparameters",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.10 Get Default Hyperparameters",
    "text": "18.10 Get Default Hyperparameters\nThe default hyperparameters, whihc will be used for a comparion with the tuned hyperparameters, can be obtained with the following commands:\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nX_start = get_default_hyperparameters_as_array(fun_control)\nmodel_default = get_one_core_model_from_X(X_start, fun_control)\n\n\n\n\n\n\n\nNote: spotPython tunes numpy arrays\n\n\n\n\nspotPython tunes numpy arrays, i.e., the hyperparameters are stored in a numpy array.\n\n\n\nThe model with the default hyperparameters can be trained and evaluated with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import eval_oml_horizon\n\ndf_eval_default, df_true_default = eval_oml_horizon(\n                    model=model_default,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\nThe three performance criteria, i.e., scaoe (metric), runtime, and memory consumption, can be visualized with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\ndf_labels=[\"default\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])\n\n\n\n\n\n\n\n\n\n18.10.1 Show Predictions\n\nSelect a subset of the data set for the visualization of the predictions:\n\nWe use the mean, \\(m\\), of the data set as the center of the visualization.\nWe use 100 data points, i.e., \\(m \\pm 50\\) as the visualization window.\n\n\n\nm = fun_control[\"test\"].shape[0]\na = int(m/2)-50\nb = int(m/2)\n\n\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b]], target_column=target_column,  df_labels=df_labels)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#get-spot-results",
    "href": "025_spot_hpt_river_friedman_amfr.html#get-spot-results",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.11 Get SPOT Results",
    "text": "18.11 Get SPOT Results\nIn a similar way, we can obtain the hyperparameters found by spotPython.\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\n\n\ndf_eval_spot, df_true_spot = eval_oml_horizon(\n                    model=model_spot,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\n\ndf_labels=[\"default\", \"spot\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"], filename=\"./figures/\" + experiment_name+\"_metrics.pdf\")\n\n\n\n\n\n\n\n\n\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b], df_true_spot[a:b]], target_column=target_column,  df_labels=df_labels, filename=\"./figures/\" + experiment_name+\"_predictions.pdf\")\n\n\n\n\n\n\n\n\n\nfrom spotPython.plot.validation import plot_actual_vs_predicted\nplot_actual_vs_predicted(y_test=df_true_default[target_column], y_pred=df_true_default[\"Prediction\"], title=\"Default\")\nplot_actual_vs_predicted(y_test=df_true_spot[target_column], y_pred=df_true_spot[\"Prediction\"], title=\"SPOT\")",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#detailed-hyperparameter-plots",
    "href": "025_spot_hpt_river_friedman_amfr.html#detailed-hyperparameter-plots",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.12 Detailed Hyperparameter Plots",
    "text": "18.12 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nn_estimators:  93.7035823898178\nstep:  26.955503932088465\nuse_aggregation:  100.0",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#parallel-coordinates-plots",
    "href": "025_spot_hpt_river_friedman_amfr.html#parallel-coordinates-plots",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.13 Parallel Coordinates Plots",
    "text": "18.13 Parallel Coordinates Plots\n\nspot_tuner.parallel_plot()",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#plot-all-combinations-of-hyperparameters",
    "href": "025_spot_hpt_river_friedman_amfr.html#plot-all-combinations-of-hyperparameters",
    "title": "18  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "18.14 Plot all Combinations of Hyperparameters",
    "text": "18.14 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html",
    "href": "031_spot_lightning_linear_diabetes.html",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "",
    "text": "17.1 Step 1: Setup\nfrom spotPython.utils.device import getDevice\nfrom math import inf\n\nMAX_TIME = 1\nFUN_EVALS = inf\nINIT_SIZE = 5\nWORKERS = 0\nPREFIX=\"031\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.1",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-setup-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-setup-31",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter MAX_TIME specifies the maximum run time in seconds.\nThe parameter INIT_SIZE specifies the initial design size.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "031_spot_lightning_linear_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "17.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    log_level=10,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=0.1,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_15_02_47_53\nCreated spot_tensorboard_path: runs/spot_logs/031_p040025_2024-01-15_02-47-53 for SummaryWriter()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-3-loading-the-diabetes-data-set",
    "href": "031_spot_lightning_linear_diabetes.html#step-3-loading-the-diabetes-data-set",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.3 Step 3: Loading the Diabetes Data Set",
    "text": "17.3 Step 3: Loading the Diabetes Data Set\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes()\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n442\n\n\n\n\n\n\n\n\nNote: Data Set and Data Loader\n\n\n\n\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,\n          0.0199, -0.0176],\n        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,\n         -0.0683, -0.0922],\n        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,\n          0.0029, -0.0259],\n        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,\n          0.0227, -0.0094],\n        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,\n         -0.0320, -0.0466]])\nTargets: tensor([151.,  75., 141., 206., 135.])",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-preprocessing-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-preprocessing-31",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.4 Step 4: Preprocessing",
    "text": "17.4 Step 4: Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-selection-of-the-algorithm-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-selection-of-the-algorithm-31",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict",
    "text": "17.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\nHere we simply add the NN Model to the fun_control dictionary by calling the function add_core_model_to_fun_control:\n\n\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=NetLightRegression,\n                              hyper_dict=LightHyperDict)\n\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-modification-of-hyperparameters-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-modification-of-hyperparameters-31",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "17.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9]) and\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"l1\", [4, 6])\nset_control_hyperparameter_value(fun_control, \"epochs\", [9, 10])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adadelta\",\n                \"Adagrad\",\n                \"Adam\",\n                \"AdamW\",\n                \"Adamax\",                \n                \"NAdam\",\n                \"RAdam\",\n                \"RMSprop\",\n                \"Rprop\"\n            ])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [5, 7])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\n                \"Sigmoid\",\n                \"ReLU\",\n                \"LeakyReLU\",\n                \"Swish\"\n            ] )\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |    4    |     6   | transform_power_2_int |\n| epochs         | int    | 4         |    9    |    10   | transform_power_2_int |\n| batch_size     | int    | 4         |    4    |     5   | transform_power_2_int |\n| act_fn         | factor | ReLU      |    0    |     3   | None                  |\n| optimizer      | factor | SGD       |    0    |     8   | None                  |\n| dropout_prob   | float  | 0.01      |    0.01 |     0.1 | None                  |\n| lr_mult        | float  | 1.0       |    0.5  |     5   | None                  |\n| patience       | int    | 2         |    5    |     7   | transform_power_2_int |\n| initialization | factor | Default   |    0    |     2   | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n\n\n\n\nNote: Hyperparameters of the Tuned Model and the fun_control Dictionary\n\n\n\nThe updated fun_control dictionary can be shown with the command fun_control[\"core_model_hyper_dict\"].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "031_spot_lightning_linear_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "17.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n17.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n17.7.2 Loss Function\nThe loss function is specified in the configurable network class [SOURCE] We will use MSE.\n\n\n17.7.3 Metric\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-8-calling-the-spot-function",
    "href": "031_spot_lightning_linear_diabetes.html#step-8-calling-the-spot-function",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.8 Step 8: Calling the SPOT Function",
    "text": "17.8 Step 8: Calling the SPOT Function\n\n17.8.1 Preparing the SPOT Call\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init(init_size=INIT_SIZE)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                            n_theta=2)\n\n\n\n\n\n\n\nNote: Modifying Values in the Control Dictionaries\n\n\n\n\nThe values in the control dictionaries can be modified with the function set_control_key_value [SOURCE], for example:\n\nset_control_key_value(control_dict=surrogate_control,\n                        key=\"noise\",\n                        value=True,\n                        replace=True)                       \nset_control_key_value(control_dict=surrogate_control,\n                        key=\"n_theta\",\n                        value=2,\n                        replace=True)      \n\n\n\n\n\n17.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=50).fun\n\n\n\n17.8.3 Showing the fun_control Dictionary\n\nimport pprint\npprint.pprint(fun_control)\n\n{'CHECKPOINT_PATH': 'runs/saved_models/',\n 'DATASET_PATH': 'data/',\n 'RESULTS_PATH': 'results/',\n 'TENSORBOARD_PATH': 'runs/',\n '_L_in': 10,\n '_L_out': 1,\n 'accelerator': 'auto',\n 'core_model': &lt;class 'spotPython.light.regression.netlightregression.NetLightRegression'&gt;,\n 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                      'core_model_parameter_type': 'instance()',\n                                      'default': 'ReLU',\n                                      'levels': ['Sigmoid',\n                                                 'ReLU',\n                                                 'LeakyReLU',\n                                                 'Swish'],\n                                      'lower': 0,\n                                      'transform': 'None',\n                                      'type': 'factor',\n                                      'upper': 3},\n                           'batch_size': {'default': 4,\n                                          'lower': 4,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 5},\n                           'dropout_prob': {'default': 0.01,\n                                            'lower': 0.01,\n                                            'transform': 'None',\n                                            'type': 'float',\n                                            'upper': 0.1},\n                           'epochs': {'default': 4,\n                                      'lower': 9,\n                                      'transform': 'transform_power_2_int',\n                                      'type': 'int',\n                                      'upper': 10},\n                           'initialization': {'core_model_parameter_type': 'str',\n                                              'default': 'Default',\n                                              'levels': ['Default',\n                                                         'Kaiming',\n                                                         'Xavier'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 2},\n                           'l1': {'default': 3,\n                                  'lower': 4,\n                                  'transform': 'transform_power_2_int',\n                                  'type': 'int',\n                                  'upper': 6},\n                           'lr_mult': {'default': 1.0,\n                                       'lower': 0.5,\n                                       'transform': 'None',\n                                       'type': 'float',\n                                       'upper': 5.0},\n                           'optimizer': {'class_name': 'torch.optim',\n                                         'core_model_parameter_type': 'str',\n                                         'default': 'SGD',\n                                         'levels': ['Adadelta',\n                                                    'Adagrad',\n                                                    'Adam',\n                                                    'AdamW',\n                                                    'Adamax',\n                                                    'NAdam',\n                                                    'RAdam',\n                                                    'RMSprop',\n                                                    'Rprop'],\n                                         'lower': 0,\n                                         'transform': 'None',\n                                         'type': 'factor',\n                                         'upper': 8},\n                           'patience': {'default': 2,\n                                        'lower': 5,\n                                        'transform': 'transform_power_2_int',\n                                        'type': 'int',\n                                        'upper': 7}},\n 'counter': 0,\n 'data': None,\n 'data_dir': './data',\n 'data_module': None,\n 'data_set': &lt;spotPython.data.diabetes.Diabetes object at 0x2abd9b950&gt;,\n 'design': None,\n 'device': 'mps',\n 'devices': 1,\n 'enable_progress_bar': False,\n 'eval': None,\n 'fun_evals': inf,\n 'fun_repeats': 1,\n 'infill_criterion': 'y',\n 'k_folds': 3,\n 'log_level': 10,\n 'loss_function': None,\n 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),\n 'max_time': 1,\n 'metric_params': {},\n 'metric_river': None,\n 'metric_sklearn': None,\n 'metric_torch': None,\n 'model_dict': {},\n 'n_points': 1,\n 'n_samples': None,\n 'noise': False,\n 'num_workers': 0,\n 'ocba_delta': 0,\n 'optimizer': None,\n 'path': None,\n 'prep_model': None,\n 'save_model': False,\n 'seed': 123,\n 'show_batch_interval': 1000000,\n 'show_models': False,\n 'show_progress': True,\n 'shuffle': None,\n 'sigma': 0.0,\n 'spot_tensorboard_path': 'runs/spot_logs/031_p040025_2024-01-15_02-47-53',\n 'spot_writer': &lt;torch.utils.tensorboard.writer.SummaryWriter object at 0x2abd26cd0&gt;,\n 'target_column': None,\n 'task': None,\n 'test': None,\n 'test_seed': 1234,\n 'test_size': 0.1,\n 'tolerance_x': 1.4901161193847656e-08,\n 'train': None,\n 'upper': array([ 8.  ,  9.  ,  4.  ,  5.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),\n 'var_name': ['l1',\n              'epochs',\n              'batch_size',\n              'act_fn',\n              'optimizer',\n              'dropout_prob',\n              'lr_mult',\n              'patience',\n              'initialization'],\n 'var_type': ['int',\n              'int',\n              'int',\n              'factor',\n              'factor',\n              'float',\n              'float',\n              'int',\n              'factor'],\n 'verbosity': 0,\n 'weights': 1.0}\n\n\n\n\n17.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE].\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3704.84814453125, 'hp_metric': 3704.84814453125}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3881.30419921875, 'hp_metric': 3881.30419921875}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2666.55078125, 'hp_metric': 2666.55078125}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5838.70849609375, 'hp_metric': 5838.70849609375}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3217.487060546875, 'hp_metric': 3217.487060546875}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3745.490478515625, 'hp_metric': 3745.490478515625}\nspotPython tuning: 2666.55078125 [###-------] 34.33% \ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3020.19091796875, 'hp_metric': 3020.19091796875}\nspotPython tuning: 2666.55078125 [#########-] 85.51% \ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 4797.9697265625, 'hp_metric': 4797.9697265625}\nspotPython tuning: 2666.55078125 [##########] 100.00% Done...\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3704.84814453125      │\n│         val_loss          │     3704.84814453125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3881.30419921875      │\n│         val_loss          │     3881.30419921875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       2666.55078125       │\n│         val_loss          │       2666.55078125       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     5838.70849609375      │\n│         val_loss          │     5838.70849609375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3217.487060546875     │\n│         val_loss          │     3217.487060546875     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3745.490478515625     │\n│         val_loss          │     3745.490478515625     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3020.19091796875      │\n│         val_loss          │     3020.19091796875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      4797.9697265625      │\n│         val_loss          │      4797.9697265625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2af5bda10&gt;",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-tensorboard-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-tensorboard-31",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.9 Step 9: Tensorboard",
    "text": "17.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-results-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-results-31",
    "title": "17  HPT PyTorch Lightning: Diabetes",
    "section": "17.10 Step 10: Results",
    "text": "17.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX +\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     4.0 |     6.0 | 5.0                 | transform_power_2_int |         0.02 |         |\n| epochs         | int    | 4         |     9.0 |    10.0 | 9.0                 | transform_power_2_int |         1.88 | *       |\n| batch_size     | int    | 4         |     4.0 |     5.0 | 4.0                 | transform_power_2_int |        57.05 | **      |\n| act_fn         | factor | ReLU      |     0.0 |     3.0 | Swish               | None                  |         0.11 | .       |\n| optimizer      | factor | SGD       |     0.0 |     8.0 | RMSprop             | None                  |        27.01 | *       |\n| dropout_prob   | float  | 0.01      |    0.01 |     0.1 | 0.08951095646892425 | None                  |         0.46 | .       |\n| lr_mult        | float  | 1.0       |     0.5 |     5.0 | 3.9340958113143683  | None                  |        29.97 | *       |\n| patience       | int    | 2         |     5.0 |     7.0 | 7.0                 | transform_power_2_int |         1.20 | *       |\n| initialization | factor | Default   |     0.0 |     2.0 | Xavier              | None                  |       100.00 | ***     |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + PREFIX + \"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n17.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nprint(config)\n\n{'l1': 32, 'epochs': 512, 'batch_size': 16, 'act_fn': Swish(), 'optimizer': 'RMSprop', 'dropout_prob': 0.08951095646892425, 'lr_mult': 3.9340958113143683, 'patience': 128, 'initialization': 'Xavier'}\n\n\n\nTest on the full data set\n\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule: train_dataloader(). Training set size: 71\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\nLightDataModule: test_dataloader(). Training set size: 266\nLightDataModule: test_dataloader(). batch_size: 16\nLightDataModule: test_dataloader(). num_workers: 0\ntest_model result: {'val_loss': 5430.9619140625, 'hp_metric': 5430.9619140625}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      5430.9619140625      │\n│         val_loss          │      5430.9619140625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n(5430.9619140625, 5430.9619140625)\n\n\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\n\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 32, 'epochs': 512, 'batch_size': 16, 'act_fn': Swish(), 'optimizer': 'RMSprop', 'dropout_prob': 0.08951095646892425, 'lr_mult': 3.9340958113143683, 'patience': 128, 'initialization': 'Xavier'}\nLoading model with 32_512_16_Swish_RMSprop_0.0895_3.9341_128_Xavier_TEST from runs/saved_models/32_512_16_Swish_RMSprop_0.0895_3.9341_128_Xavier_TEST/last.ckpt\nModel: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=32, bias=True)\n    (1): Swish()\n    (2): Dropout(p=0.08951095646892425, inplace=False)\n    (3): Linear(in_features=32, out_features=16, bias=True)\n    (4): Swish()\n    (5): Dropout(p=0.08951095646892425, inplace=False)\n    (6): Linear(in_features=16, out_features=16, bias=True)\n    (7): Swish()\n    (8): Dropout(p=0.08951095646892425, inplace=False)\n    (9): Linear(in_features=16, out_features=8, bias=True)\n    (10): Swish()\n    (11): Dropout(p=0.08951095646892425, inplace=False)\n    (12): Linear(in_features=8, out_features=1, bias=True)\n  )\n)\n\n\n\nfilename = \"./figures/\" + PREFIX\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nepochs:  1.8807805068719348\nbatch_size:  57.05142175062917\nact_fn:  0.1107426719028105\noptimizer:  27.012920682264415\ndropout_prob:  0.45724124818032186\nlr_mult:  29.965265752518814\npatience:  1.201168496292324\ninitialization:  100.0\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17.10.2 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n                                                \nParallel coordinates plots\n\n\n\n\n17.10.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.cvmodel import cv_model\nset_control_key_value(control_dict=fun_control,\n                        key=\"k_folds\",\n                        value=2,\n                        replace=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"test_size\",\n                        value=0.6,\n                        replace=True)\ncv_model(config, fun_control)\n\nk: 0\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 3624.390869140625, 'hp_metric': 3624.390869140625}\nk: 1\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 3789.607177734375, 'hp_metric': 3789.607177734375}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3624.390869140625     │\n│         val_loss          │     3624.390869140625     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3789.607177734375     │\n│         val_loss          │     3789.607177734375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n3706.9990234375\n\n\n\n\n17.10.4 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n17.10.5 Visualizing the Activation Distribution (Under Development)\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11)\nmodel\n\nNetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=32, bias=True)\n    (1): Swish()\n    (2): Dropout(p=0.08951095646892425, inplace=False)\n    (3): Linear(in_features=32, out_features=16, bias=True)\n    (4): Swish()\n    (5): Dropout(p=0.08951095646892425, inplace=False)\n    (6): Linear(in_features=16, out_features=16, bias=True)\n    (7): Swish()\n    (8): Dropout(p=0.08951095646892425, inplace=False)\n    (9): Linear(in_features=16, out_features=8, bias=True)\n    (10): Swish()\n    (11): Dropout(p=0.08951095646892425, inplace=False)\n    (12): Linear(in_features=8, out_features=11, bias=True)\n  )\n)\n\n\n\n# from spotPython.utils.eda import visualize_activations\n# visualize_activations(model, color=f\"C{0}\")",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html",
    "href": "032_spot_lightning_rnn_diabetes.html",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "",
    "text": "18.1 Step 1: Setup\nfrom spotPython.utils.device import getDevice\nfrom math import inf\nMAX_TIME = 1\nFUN_EVALS = inf\nINIT_SIZE = 5\nWORKERS = 0\nPREFIX=\"032\"\nDEVICE = getDevice()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-setup-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-setup-32",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter MAX_TIME specifies the maximum run time in seconds.\nThe parameter INIT_SIZE specifies the initial design size.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nFUN_EVALS is set to infinity.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\nPREFIX is set to “032”. This is used for the experiment name and the name of the log file.\nDEVICE is set to the device that is returned by getDevice(), e.g., gpu.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "032_spot_lightning_rnn_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "18.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\n\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    log_level=10,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=0.1,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    verbosity=1\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_15_02_56_36\nCreated spot_tensorboard_path: runs/spot_logs/032_p040025_2024-01-15_02-56-36 for SummaryWriter()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-3-loading-the-diabetes-data-set",
    "href": "032_spot_lightning_rnn_diabetes.html#step-3-loading-the-diabetes-data-set",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.3 Step 3: Loading the Diabetes Data Set",
    "text": "18.3 Step 3: Loading the Diabetes Data Set\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes()\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n442\n\n\n\n\n\n\n\n\nNote: Data Set and Data Loader\n\n\n\n\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,\n          0.0199, -0.0176],\n        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,\n         -0.0683, -0.0922],\n        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,\n          0.0029, -0.0259],\n        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,\n          0.0227, -0.0094],\n        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,\n         -0.0320, -0.0466]])\nTargets: tensor([151.,  75., 141., 206., 135.])",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-preprocessing-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-preprocessing-32",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.4 Step 4: Preprocessing",
    "text": "18.4 Step 4: Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-selection-of-the-algorithm-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-selection-of-the-algorithm-32",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict",
    "text": "18.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\nHere we simply add the NN Model to the fun_control dictionary by calling the function add_core_model_to_fun_control:\n\n\nfrom spotPython.light.regression.rnnlightregression import RNNLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=RNNLightRegression,\n                              hyper_dict=LightHyperDict)\n\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].\n\n\n\n\n\n\nNote: User specified models and hyperparameter dictionaries\n\n\n\n\nThe user can specify a model and a hyperparameter dictionary in a subfolder, e.g., userRNN in the current working directory.\nThe model and the hyperparameter dictionary are imported with the following code:\n\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nimport sys\nsys.path.insert(0, './userRNN')\nimport userrnn\nimport user_hyper_dict\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=userrnn.RNNLightRegression,\n                              hyper_dict=user_hyper_dict.UserHyperDict)\n\nExample files can be found in the userRNN folder.\nThese files can be modified by the user.\nThey can be used without re-compilation of the spotPython source code, if they are located in a subfolder of the current working directory.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-modification-of-hyperparameters-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-modification-of-hyperparameters-32",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "18.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9]) and\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"l1\", [3, 8])\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [2, 6])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adadelta\",\n                \"Adagrad\",\n                \"Adam\",\n                \"Adamax\"])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.25])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [3, 9])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\"ReLU\"] )\nset_control_hyperparameter_value(fun_control, \"initialization\",[\"Default\"] )\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |    3    |    8    | transform_power_2_int |\n| epochs         | int    | 4         |    7    |    9    | transform_power_2_int |\n| batch_size     | int    | 4         |    2    |    6    | transform_power_2_int |\n| act_fn         | factor | ReLU      |    0    |    0    | None                  |\n| optimizer      | factor | SGD       |    0    |    3    | None                  |\n| dropout_prob   | float  | 0.01      |    0.01 |    0.25 | None                  |\n| lr_mult        | float  | 1.0       |    0.5  |    5    | None                  |\n| patience       | int    | 2         |    3    |    9    | transform_power_2_int |\n| initialization | factor | Default   |    0    |    0    | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n\n\n\n\nNote: Hyperparameters of the Tuned Model and the fun_control Dictionary\n\n\n\nThe updated fun_control dictionary can be shown with the command fun_control[\"core_model_hyper_dict\"].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "032_spot_lightning_rnn_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "18.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n18.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n18.7.2 Loss Function\nThe loss function is specified in the configurable network class [SOURCE] We will use MSE.\n\n\n18.7.3 Metric\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-8-calling-the-spot-function",
    "href": "032_spot_lightning_rnn_diabetes.html#step-8-calling-the-spot-function",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.8 Step 8: Calling the SPOT Function",
    "text": "18.8 Step 8: Calling the SPOT Function\n\n18.8.1 Preparing the SPOT Call\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init()\nset_control_key_value(control_dict=design_control,\n                        key=\"init_size\",\n                        value=INIT_SIZE,\n                        replace=True)\n\nsurrogate_control = surrogate_control_init()\nset_control_key_value(control_dict=surrogate_control,\n                        key=\"noise\",\n                        value=True,\n                        replace=True)                       \nset_control_key_value(control_dict=surrogate_control,\n                        key=\"n_theta\",\n                        value=2,\n                        replace=True)      \n\n\n\n18.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=10).fun\n\n\n\n18.8.3 Showing the fun_control Dictionary\n\nimport pprint\npprint.pprint(fun_control)\n\n{'CHECKPOINT_PATH': 'runs/saved_models/',\n 'DATASET_PATH': 'data/',\n 'RESULTS_PATH': 'results/',\n 'TENSORBOARD_PATH': 'runs/',\n '_L_in': 10,\n '_L_out': 1,\n 'accelerator': 'auto',\n 'core_model': &lt;class 'spotPython.light.regression.rnnlightregression.RNNLightRegression'&gt;,\n 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                      'core_model_parameter_type': 'instance()',\n                                      'default': 'ReLU',\n                                      'levels': ['ReLU'],\n                                      'lower': 0,\n                                      'transform': 'None',\n                                      'type': 'factor',\n                                      'upper': 0},\n                           'batch_size': {'default': 4,\n                                          'lower': 2,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 6},\n                           'dropout_prob': {'default': 0.01,\n                                            'lower': 0.01,\n                                            'transform': 'None',\n                                            'type': 'float',\n                                            'upper': 0.25},\n                           'epochs': {'default': 4,\n                                      'lower': 7,\n                                      'transform': 'transform_power_2_int',\n                                      'type': 'int',\n                                      'upper': 9},\n                           'initialization': {'core_model_parameter_type': 'str',\n                                              'default': 'Default',\n                                              'levels': ['Default'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 0},\n                           'l1': {'default': 3,\n                                  'lower': 3,\n                                  'transform': 'transform_power_2_int',\n                                  'type': 'int',\n                                  'upper': 8},\n                           'lr_mult': {'default': 1.0,\n                                       'lower': 0.5,\n                                       'transform': 'None',\n                                       'type': 'float',\n                                       'upper': 5.0},\n                           'optimizer': {'class_name': 'torch.optim',\n                                         'core_model_parameter_type': 'str',\n                                         'default': 'SGD',\n                                         'levels': ['Adadelta',\n                                                    'Adagrad',\n                                                    'Adam',\n                                                    'Adamax'],\n                                         'lower': 0,\n                                         'transform': 'None',\n                                         'type': 'factor',\n                                         'upper': 3},\n                           'patience': {'default': 2,\n                                        'lower': 3,\n                                        'transform': 'transform_power_2_int',\n                                        'type': 'int',\n                                        'upper': 9}},\n 'counter': 0,\n 'data': None,\n 'data_dir': './data',\n 'data_module': None,\n 'data_set': &lt;spotPython.data.diabetes.Diabetes object at 0x2a532f290&gt;,\n 'design': None,\n 'device': 'mps',\n 'devices': 1,\n 'enable_progress_bar': False,\n 'eval': None,\n 'fun_evals': inf,\n 'fun_repeats': 1,\n 'infill_criterion': 'y',\n 'k_folds': 3,\n 'log_level': 10,\n 'loss_function': None,\n 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),\n 'max_time': 1,\n 'metric_params': {},\n 'metric_river': None,\n 'metric_sklearn': None,\n 'metric_torch': None,\n 'model_dict': {},\n 'n_points': 1,\n 'n_samples': None,\n 'noise': False,\n 'num_workers': 0,\n 'ocba_delta': 0,\n 'optimizer': None,\n 'path': None,\n 'prep_model': None,\n 'save_model': False,\n 'seed': 123,\n 'show_batch_interval': 1000000,\n 'show_models': False,\n 'show_progress': True,\n 'shuffle': None,\n 'sigma': 0.0,\n 'spot_tensorboard_path': 'runs/spot_logs/032_p040025_2024-01-15_02-56-36',\n 'spot_writer': &lt;torch.utils.tensorboard.writer.SummaryWriter object at 0x2ac671810&gt;,\n 'target_column': None,\n 'task': None,\n 'test': None,\n 'test_seed': 1234,\n 'test_size': 0.1,\n 'tolerance_x': 1.4901161193847656e-08,\n 'train': None,\n 'upper': array([ 8.  ,  9.  ,  4.  ,  1.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),\n 'var_name': ['l1',\n              'epochs',\n              'batch_size',\n              'act_fn',\n              'optimizer',\n              'dropout_prob',\n              'lr_mult',\n              'patience',\n              'initialization'],\n 'var_type': ['int',\n              'int',\n              'int',\n              'factor',\n              'factor',\n              'float',\n              'float',\n              'int',\n              'factor'],\n 'verbosity': 1,\n 'weights': 1.0}\n\n\n\npprint.pprint(design_control)\n\n{'init_size': 5, 'repeats': 1}\n\n\n\npprint.pprint(surrogate_control)\n\n{'log_level': 50,\n 'max_Lambda': 1,\n 'max_theta': 2.0,\n 'min_Lambda': 1e-09,\n 'min_theta': -3.0,\n 'model_fun_evals': 10000,\n 'model_optimizer': &lt;function differential_evolution at 0x171b3e520&gt;,\n 'n_p': 1,\n 'n_theta': 2,\n 'noise': True,\n 'optim_p': False,\n 'p_val': 2.0,\n 'seed': 124,\n 'theta_init_zero': True,\n 'var_type': None}\n\n\n\n\n18.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE].\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.19355651674791854,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 16,\n 'lr_mult': 1.5691149440098038,\n 'optimizer': 'Adam',\n 'patience': 32}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5832.97998046875, 'hp_metric': 5832.97998046875}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.09424169914869776,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 3.35818256351233,\n 'optimizer': 'Adadelta',\n 'patience': 512}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3652.889404296875, 'hp_metric': 3652.889404296875}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 4,\n 'dropout_prob': 0.21164199382623602,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.9336514668325573,\n 'optimizer': 'Adamax',\n 'patience': 16}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 4\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 4\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2414.205078125, 'hp_metric': 2414.205078125}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 8,\n 'dropout_prob': 0.05728504399550885,\n 'epochs': 128,\n 'initialization': 'Default',\n 'l1': 64,\n 'lr_mult': 4.575980093998586,\n 'optimizer': 'Adam',\n 'patience': 32}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 8\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 8\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5862.02099609375, 'hp_metric': 5862.02099609375}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.14352914208400058,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 8,\n 'lr_mult': 2.4204853123355816,\n 'optimizer': 'Adagrad',\n 'patience': 128}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3681.71923828125, 'hp_metric': 3681.71923828125}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 4,\n 'dropout_prob': 0.2116436615948213,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.9336534446907111,\n 'optimizer': 'Adamax',\n 'patience': 16}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 4\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 4\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3089.423828125, 'hp_metric': 3089.423828125}\nspotPython tuning: 2414.205078125 [########--] 75.34% \n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 4,\n 'dropout_prob': 0.21163960937948934,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.9336299317038221,\n 'optimizer': 'Adamax',\n 'patience': 16}\ntrain_model(): Test set size: 45\ntrain_model(): Train set size: 359\ntrain_model(): Batch size: 4\nLightDataModule: train_dataloader(). Training set size: 359\nLightDataModule: train_dataloader(). batch_size: 4\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3275.68994140625, 'hp_metric': 3275.68994140625}\nspotPython tuning: 2414.205078125 [##########] 100.00% Done...\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     5832.97998046875      │\n│         val_loss          │     5832.97998046875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3652.889404296875     │\n│         val_loss          │     3652.889404296875     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      2414.205078125       │\n│         val_loss          │      2414.205078125       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     5862.02099609375      │\n│         val_loss          │     5862.02099609375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3681.71923828125      │\n│         val_loss          │     3681.71923828125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      3089.423828125       │\n│         val_loss          │      3089.423828125       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3275.68994140625      │\n│         val_loss          │     3275.68994140625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2b89c9190&gt;",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-tensorboard-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-tensorboard-32",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.9 Step 9: Tensorboard",
    "text": "18.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-results-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-results-32",
    "title": "18  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "18.10 Step 10: Results",
    "text": "18.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX + \"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     3.0 |     8.0 | 7.0                 | transform_power_2_int |         0.00 |         |\n| epochs         | int    | 4         |     7.0 |     9.0 | 9.0                 | transform_power_2_int |         0.00 |         |\n| batch_size     | int    | 4         |     2.0 |     6.0 | 2.0                 | transform_power_2_int |         0.00 |         |\n| act_fn         | factor | ReLU      |     0.0 |     0.0 | ReLU                | None                  |         0.00 |         |\n| optimizer      | factor | SGD       |     0.0 |     3.0 | Adamax              | None                  |         0.37 | .       |\n| dropout_prob   | float  | 0.01      |    0.01 |    0.25 | 0.21164199382623602 | None                  |         0.00 |         |\n| lr_mult        | float  | 1.0       |     0.5 |     5.0 | 0.9336514668325573  | None                  |         0.00 |         |\n| patience       | int    | 2         |     3.0 |     9.0 | 4.0                 | transform_power_2_int |       100.00 | ***     |\n| initialization | factor | Default   |     0.0 |     0.0 | Default             | None                  |         0.00 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + PREFIX + \"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n18.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nprint(config)\n\n{'l1': 128, 'epochs': 512, 'batch_size': 4, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.21164199382623602, 'lr_mult': 0.9336514668325573, 'patience': 16, 'initialization': 'Default'}\n\n\n\nTest on the full data set\n\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule: train_dataloader(). Training set size: 71\nLightDataModule: train_dataloader(). batch_size: 4\nLightDataModule: train_dataloader(). num_workers: 0\nLightDataModule: test_dataloader(). Training set size: 266\nLightDataModule: test_dataloader(). batch_size: 4\nLightDataModule: test_dataloader(). num_workers: 0\ntest_model result: {'val_loss': 3773.025634765625, 'hp_metric': 3773.025634765625}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3773.025634765625     │\n│         val_loss          │     3773.025634765625     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n(3773.025634765625, 3773.025634765625)\n\n\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\n\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 128, 'epochs': 512, 'batch_size': 4, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.21164199382623602, 'lr_mult': 0.9336514668325573, 'patience': 16, 'initialization': 'Default'}\nLoading model with 128_512_4_ReLU_Adamax_0.2116_0.9337_16_Default_TEST from runs/saved_models/128_512_4_ReLU_Adamax_0.2116_0.9337_16_Default_TEST/last.ckpt\nModel: RNNLightRegression(\n  (rnn_layer): RNN(10, 128, batch_first=True)\n  (fc): Linear(in_features=128, out_features=128, bias=True)\n  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n  (dropout1): Dropout(p=0.21164199382623602, inplace=False)\n  (dropout2): Dropout(p=0.0, inplace=False)\n  (dropout3): Dropout(p=0.0, inplace=False)\n  (activation_fct): ReLU()\n)\n\n\n\nfilename = \"./figures/\" + PREFIX\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\noptimizer:  0.3650734171034668\npatience:  100.0\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n18.10.2 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n                                                \nParallel coordinates plots\n\n\n\n\n18.10.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.cvmodel import cv_model\nset_control_key_value(control_dict=fun_control,\n                        key=\"k_folds\",\n                        value=2,\n                        replace=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"test_size\",\n                        value=0.1,\n                        replace=True)\ncv_model(config, fun_control)\n\nk: 0\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 3405.75830078125, 'hp_metric': 3405.75830078125}\nk: 1\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 3102.15576171875, 'hp_metric': 3102.15576171875}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3405.75830078125      │\n│         val_loss          │     3405.75830078125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3102.15576171875      │\n│         val_loss          │     3102.15576171875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n3253.95703125\n\n\n\n\n18.10.4 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n18.10.5 Visualizing the Activation Distribution (Under Development)\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11)\nmodel\n\nRNNLightRegression(\n  (rnn_layer): RNN(64, 128, batch_first=True)\n  (fc): Linear(in_features=128, out_features=128, bias=True)\n  (output_layer): Linear(in_features=128, out_features=11, bias=True)\n  (dropout1): Dropout(p=0.21164199382623602, inplace=False)\n  (dropout2): Dropout(p=0.0, inplace=False)\n  (dropout3): Dropout(p=0.0, inplace=False)\n  (activation_fct): ReLU()\n)\n\n\n\n# from spotPython.utils.eda import visualize_activations\n# visualize_activations(model, color=f\"C{0}\")",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html",
    "href": "033_spot_lightning_linear_sensitive.html",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "",
    "text": "21.1 Step 1: Setup\nfrom spotPython.utils.device import getDevice\nfrom math import inf\n\nMAX_TIME = 10\nFUN_EVALS = inf\nFUN_REPEATS = 2\nOCBA_DELTA = 1\nREPEATS = 2\nINIT_SIZE = 10\nWORKERS = 0\nPREFIX=\"032\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.3",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-setup-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-setup-33",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter MAX_TIME specifies the maximum run time in seconds.\nThe parameter INIT_SIZE specifies the initial design size.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "033_spot_lightning_linear_sensitive.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "21.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfun_control = fun_control_init(\n    _L_in=133,\n    _L_out=1,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    fun_repeats=FUN_REPEATS,\n    log_level=10,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    ocba_delta = OCBA_DELTA,\n    show_progress=True,\n    test_size=TEST_SIZE,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    verbosity=1,\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_12_00_05_22\nCreated spot_tensorboard_path: runs/spot_logs/032_maans14_2024-01-12_00-05-22 for SummaryWriter()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-3-loading-the-user-specified-data-set",
    "href": "033_spot_lightning_linear_sensitive.html#step-3-loading-the-user-specified-data-set",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.3 Step 3: Loading the User Specified Data Set",
    "text": "21.3 Step 3: Loading the User Specified Data Set\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.pkldataset import PKLDataset\nimport torch\ndataset = PKLDataset(directory=\"./userData/\",\n                     filename=\"data_sensitive.pkl\",\n                     target_column='N',\n                     feature_type=torch.float32,\n                     target_type=torch.float32,\n                     rmNA=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n2381\n\n\n\n\n\n\n\n\nNote: Data Set and Data Loader\n\n\n\n\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 5\nInputs Shape: torch.Size([5, 133])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[6.2000e+01, 5.0100e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 2.0000e+00,\n         1.8000e+01, 1.2310e+03, 1.6000e+01, 1.8950e+03, 4.0000e+00, 1.3000e+01,\n         1.6070e+03, 6.3700e+02, 5.1600e+02, 5.1000e+01, 1.0000e+00, 1.0000e+00,\n         1.0000e+00, 6.0000e+00, 0.0000e+00, 4.5000e+01, 4.6000e+01, 2.0000e+00,\n         1.4210e+03, 1.2490e+03, 1.8890e+03, 1.8880e+03, 1.8670e+03, 5.5500e+02,\n         1.8040e+03, 1.0000e+00, 1.8000e+01, 1.1880e+03, 7.7300e+02, 7.7400e+02,\n         7.8200e+02, 7.8000e+02, 6.2600e+02, 7.5700e+02, 7.5100e+02, 3.4700e+02,\n         7.5000e+02, 7.7900e+02, 6.2900e+02, 5.9000e+01, 7.1500e+02, 7.1700e+02,\n         7.3000e+02, 6.5600e+02, 2.1000e+01, 5.4200e+02, 1.7300e+02, 4.6500e+02,\n         1.1000e+02, 3.9900e+02, 7.7800e+02, 7.7700e+02, 6.2900e+02, 7.8200e+02,\n         7.8200e+02, 6.7500e+02, 6.7500e+02, 7.7100e+02, 7.7000e+01, 7.9000e+01,\n         7.7900e+02, 7.7900e+02, 7.0500e+02, 6.6100e+02, 7.2900e+02, 7.3400e+02,\n         4.7800e+02, 7.7300e+02, 7.7400e+02, 7.7300e+02, 7.7400e+02, 7.7100e+02,\n         7.7200e+02, 7.7200e+02, 5.9000e+01, 7.1500e+02, 0.0000e+00, 7.2400e+02,\n         7.7100e+02, 1.3000e+02, 4.9700e+02, 0.0000e+00, 7.0100e+02, 1.9800e+02,\n         7.8200e+02, 1.4900e+02, 7.6300e+02, 7.0900e+02, 7.1700e+02, 7.3200e+02,\n         6.2000e+01, 7.2000e+02, 7.7400e+02, 6.6000e+01, 7.8300e+02, 7.8300e+02,\n         4.0100e+02, 7.2500e+02, 1.2600e+02, 5.0000e+00, 6.4300e+02, 3.2000e+01,\n         3.2000e+01, 3.0000e+01, 7.4400e+02, 6.1100e+02, 7.3100e+02, 7.8300e+02,\n         7.7600e+02, 7.8200e+02, 7.5700e+02, 7.8200e+02, 7.8300e+02, 7.8200e+02,\n         2.9600e+02, 2.9600e+02, 6.9500e+02, 5.6900e+02, 7.4800e+02, 7.8200e+02,\n         7.8300e+02, 4.0500e+02, 1.3400e+02, 7.1700e+02, 6.6100e+02, 3.6000e+01,\n         3.6000e+01],\n        [5.6000e+01, 4.9800e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 2.0000e+00,\n         2.1000e+01, 1.2040e+03, 1.6000e+01, 1.8680e+03, 4.0000e+00, 1.2000e+01,\n         5.1400e+02, 7.1500e+02, 6.5100e+02, 4.8000e+01, 4.7000e+01, 5.5000e+01,\n         4.8000e+01, 2.9000e+01, 0.0000e+00, 6.7000e+01, 6.3000e+01, 2.0000e+00,\n         2.3620e+03, 2.3560e+03, 1.8030e+03, 1.8950e+03, 1.9590e+03, 7.0400e+02,\n         1.4150e+03, 4.0000e+00, 2.1000e+01, 1.1690e+03, 7.7300e+02, 7.7400e+02,\n         7.8200e+02, 7.8000e+02, 6.2600e+02, 7.5700e+02, 7.5100e+02, 3.4700e+02,\n         7.5000e+02, 7.7900e+02, 6.2900e+02, 5.9000e+01, 7.1500e+02, 7.1700e+02,\n         7.3000e+02, 6.5600e+02, 2.1000e+01, 5.4200e+02, 1.7300e+02, 4.6500e+02,\n         1.1000e+02, 3.9900e+02, 7.7800e+02, 7.7700e+02, 6.2900e+02, 7.8200e+02,\n         7.8200e+02, 6.7500e+02, 6.7500e+02, 7.7100e+02, 7.7000e+01, 7.9000e+01,\n         7.7900e+02, 7.7900e+02, 7.0500e+02, 6.6100e+02, 7.2900e+02, 7.3400e+02,\n         4.7800e+02, 7.7300e+02, 7.7400e+02, 7.7300e+02, 7.7400e+02, 7.7100e+02,\n         7.7200e+02, 7.7200e+02, 5.9000e+01, 7.1500e+02, 0.0000e+00, 7.2400e+02,\n         7.7100e+02, 1.3000e+02, 4.9700e+02, 0.0000e+00, 7.0100e+02, 1.9800e+02,\n         7.8200e+02, 1.4900e+02, 7.6300e+02, 7.0900e+02, 7.1700e+02, 7.3200e+02,\n         6.2000e+01, 7.2000e+02, 7.7400e+02, 6.6000e+01, 7.8300e+02, 7.8300e+02,\n         4.0100e+02, 7.2500e+02, 1.2600e+02, 5.0000e+00, 6.4300e+02, 3.2000e+01,\n         3.2000e+01, 3.0000e+01, 7.4400e+02, 6.1100e+02, 7.3100e+02, 7.8300e+02,\n         7.7600e+02, 7.8200e+02, 7.5700e+02, 7.8200e+02, 7.8300e+02, 7.8200e+02,\n         2.9600e+02, 2.9600e+02, 6.9500e+02, 5.6900e+02, 7.4800e+02, 7.8200e+02,\n         7.8300e+02, 4.0500e+02, 1.3400e+02, 7.1700e+02, 6.6100e+02, 3.4000e+01,\n         3.4000e+01],\n        [1.0000e+01, 4.6800e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 1.0000e+00,\n         2.4000e+01, 1.1970e+03, 1.8000e+01, 2.2500e+03, 3.0000e+00, 1.6000e+01,\n         5.9900e+02, 6.9200e+02, 6.3200e+02, 5.6000e+01, 4.5000e+01, 5.1000e+01,\n         4.2000e+01, 2.7000e+01, 0.0000e+00, 2.8000e+01, 3.0000e+01, 1.0000e+00,\n         2.2200e+02, 2.9600e+02, 2.2540e+03, 2.2530e+03, 2.2450e+03, 7.3800e+02,\n         2.1920e+03, 2.0000e+00, 2.4000e+01, 1.1600e+03, 7.7300e+02, 7.7400e+02,\n         7.8200e+02, 7.8000e+02, 6.2600e+02, 7.5700e+02, 7.5100e+02, 3.4700e+02,\n         7.5000e+02, 7.7900e+02, 6.2900e+02, 5.9000e+01, 7.1500e+02, 7.1700e+02,\n         7.3000e+02, 6.5600e+02, 2.1000e+01, 5.4200e+02, 1.7300e+02, 4.6500e+02,\n         1.1000e+02, 3.9900e+02, 7.7800e+02, 7.7700e+02, 6.2900e+02, 7.8200e+02,\n         7.8200e+02, 6.7500e+02, 6.7500e+02, 7.7100e+02, 7.7000e+01, 7.9000e+01,\n         7.7900e+02, 7.7900e+02, 7.0500e+02, 6.6100e+02, 7.2900e+02, 7.3400e+02,\n         4.7800e+02, 7.7300e+02, 7.7400e+02, 7.7300e+02, 7.7400e+02, 7.7100e+02,\n         7.7200e+02, 7.7200e+02, 5.9000e+01, 7.1500e+02, 0.0000e+00, 7.2400e+02,\n         7.7100e+02, 1.3000e+02, 4.9700e+02, 0.0000e+00, 7.0100e+02, 1.9800e+02,\n         7.8200e+02, 1.4900e+02, 7.6300e+02, 7.0900e+02, 7.1700e+02, 7.3200e+02,\n         6.2000e+01, 7.2000e+02, 7.7400e+02, 6.6000e+01, 7.8300e+02, 7.8300e+02,\n         4.0100e+02, 7.2500e+02, 1.2600e+02, 5.0000e+00, 6.4300e+02, 3.2000e+01,\n         3.2000e+01, 3.0000e+01, 7.4400e+02, 6.1100e+02, 7.3100e+02, 7.8300e+02,\n         7.7600e+02, 7.8200e+02, 7.5700e+02, 7.8200e+02, 7.8300e+02, 7.8200e+02,\n         2.9600e+02, 2.9600e+02, 6.9500e+02, 5.6900e+02, 7.4800e+02, 7.8200e+02,\n         7.8300e+02, 4.0500e+02, 1.3400e+02, 7.1700e+02, 6.6100e+02, 3.9000e+01,\n         3.9000e+01],\n        [3.7000e+01, 3.6600e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 0.0000e+00,\n         3.5000e+01, 1.1860e+03, 1.6000e+01, 1.8620e+03, 4.0000e+00, 1.2000e+01,\n         1.2020e+03, 1.3080e+03, 1.1710e+03, 4.8000e+01, 4.7000e+01, 5.5000e+01,\n         4.8000e+01, 2.9000e+01, 0.0000e+00, 6.7000e+01, 6.3000e+01, 2.0000e+00,\n         1.3200e+02, 1.1200e+02, 1.8770e+03, 1.8690e+03, 1.8450e+03, 1.0800e+03,\n         4.7700e+02, 3.0000e+00, 3.5000e+01, 1.1290e+03, 7.6300e+02, 7.7700e+02,\n         7.7700e+02, 7.7400e+02, 1.5100e+02, 7.4700e+02, 7.1700e+02, 3.5200e+02,\n         7.2800e+02, 7.5200e+02, 6.2800e+02, 5.1000e+01, 7.0400e+02, 7.0700e+02,\n         7.1500e+02, 6.4200e+02, 2.0000e+01, 5.4000e+02, 1.7500e+02, 4.7000e+02,\n         1.0900e+02, 4.0000e+02, 7.7300e+02, 7.7000e+02, 6.2800e+02, 7.7500e+02,\n         7.7500e+02, 2.4200e+02, 2.4200e+02, 7.6500e+02, 8.0000e+01, 1.1000e+02,\n         7.5200e+02, 7.5200e+02, 2.6800e+02, 2.3800e+02, 7.1700e+02, 7.2900e+02,\n         8.0000e+00, 7.6300e+02, 7.7100e+02, 7.6300e+02, 7.7700e+02, 7.6500e+02,\n         7.7400e+02, 7.7400e+02, 5.1000e+01, 7.0400e+02, 0.0000e+00, 7.3300e+02,\n         7.7600e+02, 1.3200e+02, 4.5000e+01, 3.0000e+00, 1.4100e+02, 7.0100e+02,\n         7.7400e+02, 2.5500e+02, 7.3400e+02, 2.7500e+02, 7.0700e+02, 7.1500e+02,\n         7.3000e+01, 7.1300e+02, 7.7000e+02, 8.1000e+01, 7.6700e+02, 7.8200e+02,\n         2.0000e+00, 7.8100e+02, 1.5400e+02, 1.1000e+01, 6.1700e+02, 4.3000e+01,\n         6.4000e+01, 5.2000e+01, 7.2000e+02, 1.1000e+01, 3.7300e+02, 7.7400e+02,\n         6.8500e+02, 7.7700e+02, 7.4600e+02, 7.7800e+02, 7.6900e+02, 7.7400e+02,\n         2.5100e+02, 2.5100e+02, 6.8500e+02, 5.7100e+02, 4.7300e+02, 7.6700e+02,\n         7.8200e+02, 2.0000e+00, 1.0000e+00, 7.0700e+02, 1.4000e+02, 3.4000e+01,\n         3.4000e+01],\n        [7.0000e+01, 7.2700e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 7.0000e+00,\n         5.7000e+01, 1.1300e+03, 1.9000e+01, 2.2730e+03, 2.0000e+00, 1.8000e+01,\n         8.7000e+01, 3.8500e+02, 2.9300e+02, 5.9000e+01, 4.9000e+01, 5.6000e+01,\n         4.7000e+01, 4.0000e+00, 0.0000e+00, 7.6000e+01, 6.8000e+01, 3.0000e+00,\n         1.8500e+02, 1.9800e+02, 2.2710e+03, 2.2740e+03, 2.2690e+03, 4.2200e+02,\n         1.4580e+03, 2.0000e+00, 5.7000e+01, 1.0940e+03, 7.6700e+02, 7.6500e+02,\n         7.5900e+02, 7.5000e+02, 3.0100e+02, 7.1100e+02, 6.8500e+02, 5.6200e+02,\n         7.1000e+02, 7.0400e+02, 6.2600e+02, 5.8000e+01, 6.8000e+02, 6.9300e+02,\n         6.9200e+02, 6.3200e+02, 2.2000e+01, 5.4100e+02, 2.1200e+02, 4.7300e+02,\n         1.1100e+02, 4.0300e+02, 7.5900e+02, 7.6000e+02, 6.2600e+02, 7.4200e+02,\n         7.4200e+02, 4.0000e+02, 4.0000e+02, 7.5400e+02, 1.6000e+02, 1.8900e+02,\n         7.0400e+02, 7.0400e+02, 4.2800e+02, 3.7700e+02, 7.0100e+02, 7.0900e+02,\n         8.8000e+01, 7.6700e+02, 7.6000e+02, 7.6700e+02, 7.6600e+02, 7.5400e+02,\n         7.6100e+02, 7.6100e+02, 5.8000e+01, 6.8000e+02, 0.0000e+00, 7.2600e+02,\n         7.7200e+02, 2.1600e+02, 1.8700e+02, 6.9000e+01, 3.1200e+02, 5.6600e+02,\n         7.4000e+02, 3.0600e+02, 7.0600e+02, 4.3200e+02, 6.9300e+02, 6.9100e+02,\n         9.1000e+01, 6.9300e+02, 7.5900e+02, 1.1700e+02, 7.2400e+02, 7.5600e+02,\n         3.1000e+01, 7.5200e+02, 1.7900e+02, 4.3000e+01, 5.7300e+02, 9.2000e+01,\n         1.0900e+02, 9.7000e+01, 6.9700e+02, 1.6400e+02, 5.1600e+02, 7.5500e+02,\n         7.1800e+02, 7.6000e+02, 7.5200e+02, 7.4600e+02, 7.3000e+02, 7.3800e+02,\n         2.1200e+02, 2.1200e+02, 6.4900e+02, 5.7200e+02, 5.8000e+02, 7.2200e+02,\n         7.5600e+02, 3.2000e+01, 2.5000e+01, 6.9300e+02, 3.2600e+02, 4.1000e+01,\n         4.1000e+01]])\nTargets: tensor([ 558.,  660.,  636., 1220.,  308.])",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-preprocessing-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-preprocessing-33",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.4 Step 4: Preprocessing",
    "text": "21.4 Step 4: Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-selection-of-the-algorithm-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-selection-of-the-algorithm-33",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict",
    "text": "21.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\nHere we simply add the NN Model to the fun_control dictionary by calling the function add_core_model_to_fun_control:\n\nWe can use aconfiguration from the spotPython package:\n\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=NetLightRegression,\n                              hyper_dict=LightHyperDict)\n\n\nAlternatively, we can use a userr configuration from the subdirectory userModel:\n\n\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nimport sys\nsys.path.insert(0, './userModel')\nimport netlightregression\nimport light_hyper_dict\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=netlightregression.NetLightRegression,\n                              hyper_dict=light_hyper_dict.LightHyperDict)\n\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-modification-of-hyperparameters-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-modification-of-hyperparameters-33",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "21.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9]) and\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 7])\n\n\n\n\n\n\n\n\n\n\nNote: Pre-experimental Results\n\n\n\n\nThe following hyperparameters {Table 21.1} have generated acceptable results (obtained in in pre-experimental runs):\n\n\n\n\nTable 21.1: Table 1: Pre-experimental results for the user specified data set. The test set size is 715, the train set size is 1167, and the batch size is 16.\n\n\n\n\n\nHyperparameter\nValue\n\n\n\n\nact_fn\nLeakyReLU\n\n\nbatch_size\n16\n\n\ndropout_prob\n0.01\n\n\nepochs\n512\n\n\ninitialization\nDefault\n\n\nl1\n128\n\n\nlr_mult\n0.5\n\n\noptimizer\nAdagrad\n\n\npatience\n16\n\n\n\n\n\n\nTherefore, we will use these values as the starting poing for the hyperparameter tuning.\n\n\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"l1\", [5, 9])\nset_control_hyperparameter_value(fun_control, \"epochs\", [5, 10])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [3, 6])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adadelta\",\n                \"Adamax\",\n                \"Adagrad\"\n            ])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.005, 0.25])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.25, 5.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [3,5])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\n                \"ReLU\",\n                \"LeakyReLU\",\n            ] )\nset_control_hyperparameter_value(fun_control, \"initialization\",[\"Default\"] )\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |   5     |    9    | transform_power_2_int |\n| epochs         | int    | 4         |   5     |   10    | transform_power_2_int |\n| batch_size     | int    | 4         |   3     |    6    | transform_power_2_int |\n| act_fn         | factor | ReLU      |   0     |    1    | None                  |\n| optimizer      | factor | SGD       |   0     |    2    | None                  |\n| dropout_prob   | float  | 0.01      |   0.005 |    0.25 | None                  |\n| lr_mult        | float  | 1.0       |   0.25  |    5    | None                  |\n| patience       | int    | 2         |   3     |    5    | transform_power_2_int |\n| initialization | factor | Default   |   0     |    0    | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n\n\n\n\nNote: Hyperparameters of the Tuned Model and the fun_control Dictionary\n\n\n\nThe updated fun_control dictionary can be shown with the command fun_control[\"core_model_hyper_dict\"].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "033_spot_lightning_linear_sensitive.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "21.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n21.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n21.7.2 Loss Function\nThe loss function is specified in the configurable network class [SOURCE] We will use MSE.\n\n\n21.7.3 Metric\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-8-calling-the-spot-function",
    "href": "033_spot_lightning_linear_sensitive.html#step-8-calling-the-spot-function",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.8 Step 8: Calling the SPOT Function",
    "text": "21.8 Step 8: Calling the SPOT Function\n\n21.8.1 Preparing the SPOT Call\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init(init_size=INIT_SIZE,\n                                     repeats=REPEATS,)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                            n_theta=2,\n                                            min_Lambda=1e-6,\n                                            max_Lambda=10,\n                                            log_level=10,)\n\n\n\n\n\n\n\nNote: Modifying Values in the Control Dictionaries\n\n\n\n\nThe values in the control dictionaries can be modified with the function set_control_key_value [SOURCE], for example:\n\nset_control_key_value(control_dict=surrogate_control,\n                        key=\"noise\",\n                        value=True,\n                        replace=True)                       \nset_control_key_value(control_dict=surrogate_control,\n                        key=\"n_theta\",\n                        value=2,\n                        replace=True)      \n\n\n\n\n\n21.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=10).fun\n\n\n\n21.8.3 Showing the fun_control Dictionary\n\nimport pprint\npprint.pprint(fun_control)\n\n{'CHECKPOINT_PATH': 'runs/saved_models/',\n 'DATASET_PATH': 'data/',\n 'RESULTS_PATH': 'results/',\n 'TENSORBOARD_PATH': 'runs/',\n '_L_in': 133,\n '_L_out': 1,\n 'accelerator': 'auto',\n 'core_model': &lt;class 'netlightregression.NetLightRegression'&gt;,\n 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                      'core_model_parameter_type': 'instance()',\n                                      'default': 'ReLU',\n                                      'levels': ['ReLU', 'LeakyReLU'],\n                                      'lower': 0,\n                                      'transform': 'None',\n                                      'type': 'factor',\n                                      'upper': 1},\n                           'batch_size': {'default': 4,\n                                          'lower': 3,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 6},\n                           'dropout_prob': {'default': 0.01,\n                                            'lower': 0.005,\n                                            'transform': 'None',\n                                            'type': 'float',\n                                            'upper': 0.25},\n                           'epochs': {'default': 4,\n                                      'lower': 5,\n                                      'transform': 'transform_power_2_int',\n                                      'type': 'int',\n                                      'upper': 10},\n                           'initialization': {'core_model_parameter_type': 'str',\n                                              'default': 'Default',\n                                              'levels': ['Default'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 0},\n                           'l1': {'default': 3,\n                                  'lower': 5,\n                                  'transform': 'transform_power_2_int',\n                                  'type': 'int',\n                                  'upper': 9},\n                           'lr_mult': {'default': 1.0,\n                                       'lower': 0.25,\n                                       'transform': 'None',\n                                       'type': 'float',\n                                       'upper': 5.0},\n                           'optimizer': {'class_name': 'torch.optim',\n                                         'core_model_parameter_type': 'str',\n                                         'default': 'SGD',\n                                         'levels': ['Adadelta',\n                                                    'Adamax',\n                                                    'Adagrad'],\n                                         'lower': 0,\n                                         'transform': 'None',\n                                         'type': 'factor',\n                                         'upper': 2},\n                           'patience': {'default': 2,\n                                        'lower': 3,\n                                        'transform': 'transform_power_2_int',\n                                        'type': 'int',\n                                        'upper': 5}},\n 'counter': 0,\n 'data': None,\n 'data_dir': './data',\n 'data_module': None,\n 'data_set': &lt;spotPython.data.pkldataset.PKLDataset object at 0x2f6868f50&gt;,\n 'design': None,\n 'device': 'mps',\n 'devices': 1,\n 'enable_progress_bar': False,\n 'eval': None,\n 'fun_evals': inf,\n 'fun_repeats': 2,\n 'infill_criterion': 'y',\n 'k_folds': 3,\n 'log_level': 10,\n 'loss_function': None,\n 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),\n 'max_time': 10,\n 'metric_params': {},\n 'metric_river': None,\n 'metric_sklearn': None,\n 'metric_torch': None,\n 'model_dict': {},\n 'n_points': 1,\n 'n_samples': None,\n 'noise': False,\n 'num_workers': 0,\n 'ocba_delta': 1,\n 'optimizer': None,\n 'path': None,\n 'prep_model': None,\n 'save_model': False,\n 'seed': 123,\n 'show_batch_interval': 1000000,\n 'show_models': False,\n 'show_progress': True,\n 'shuffle': None,\n 'sigma': 0.0,\n 'spot_tensorboard_path': 'runs/spot_logs/032_maans14_2024-01-12_00-05-22',\n 'spot_writer': &lt;torch.utils.tensorboard.writer.SummaryWriter object at 0x2a1a506d0&gt;,\n 'target_column': None,\n 'task': None,\n 'test': None,\n 'test_seed': 1234,\n 'test_size': 0.3,\n 'tolerance_x': 1.4901161193847656e-08,\n 'train': None,\n 'upper': array([ 8.  ,  9.  ,  4.  ,  5.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),\n 'var_name': ['l1',\n              'epochs',\n              'batch_size',\n              'act_fn',\n              'optimizer',\n              'dropout_prob',\n              'lr_mult',\n              'patience',\n              'initialization'],\n 'var_type': ['int',\n              'int',\n              'int',\n              'factor',\n              'factor',\n              'float',\n              'float',\n              'int',\n              'factor'],\n 'verbosity': 1,\n 'weights': 1.0}\n\n\n\n\n21.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE].\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.20560368458693354,\n 'epochs': 128,\n 'initialization': 'Default',\n 'l1': 64,\n 'lr_mult': 4.561411125937148,\n 'optimizer': 'Adamax',\n 'patience': 8}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 9691.404296875, 'hp_metric': 9691.404296875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.20560368458693354,\n 'epochs': 128,\n 'initialization': 'Default',\n 'l1': 64,\n 'lr_mult': 4.561411125937148,\n 'optimizer': 'Adamax',\n 'patience': 8}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 82330.359375, 'hp_metric': 82330.359375}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.02426280739731016,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.322803150188513,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5680.0966796875, 'hp_metric': 5680.0966796875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.02426280739731016,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.322803150188513,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2287.1826171875, 'hp_metric': 2287.1826171875}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 8,\n 'dropout_prob': 0.07443535216560176,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 32,\n 'lr_mult': 2.863550239963548,\n 'optimizer': 'Adamax',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 8\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 8\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 45320.10546875, 'hp_metric': 45320.10546875}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 8,\n 'dropout_prob': 0.07443535216560176,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 32,\n 'lr_mult': 2.863550239963548,\n 'optimizer': 'Adamax',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 8\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 8\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 44621.0390625, 'hp_metric': 44621.0390625}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.18567441685948113,\n 'epochs': 64,\n 'initialization': 'Default',\n 'l1': 64,\n 'lr_mult': 4.089366778039889,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 109158.8515625, 'hp_metric': 109158.8515625}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.18567441685948113,\n 'epochs': 64,\n 'initialization': 'Default',\n 'l1': 64,\n 'lr_mult': 4.089366778039889,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 151683.59375, 'hp_metric': 151683.59375}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.13080206803532848,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 2.0077669995722074,\n 'optimizer': 'Adamax',\n 'patience': 8}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 8908.8837890625, 'hp_metric': 8908.8837890625}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.13080206803532848,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 2.0077669995722074,\n 'optimizer': 'Adamax',\n 'patience': 8}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 16831.779296875, 'hp_metric': 16831.779296875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.11983781472185623,\n 'epochs': 1024,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.7181246235247182,\n 'optimizer': 'Adadelta',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 170177.1875, 'hp_metric': 170177.1875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.11983781472185623,\n 'epochs': 1024,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.7181246235247182,\n 'optimizer': 'Adadelta',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 32\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 32\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 4781.1748046875, 'hp_metric': 4781.1748046875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.09366384017851244,\n 'epochs': 64,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 3.301460285082441,\n 'optimizer': 'Adadelta',\n 'patience': 32}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 363511.125, 'hp_metric': 363511.125}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.09366384017851244,\n 'epochs': 64,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 3.301460285082441,\n 'optimizer': 'Adadelta',\n 'patience': 32}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 18567.58203125, 'hp_metric': 18567.58203125}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.03540914713540524,\n 'epochs': 32,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 3.7811076839948083,\n 'optimizer': 'Adamax',\n 'patience': 32}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2381.155517578125, 'hp_metric': 2381.155517578125}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.03540914713540524,\n 'epochs': 32,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 3.7811076839948083,\n 'optimizer': 'Adamax',\n 'patience': 32}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 1633.8154296875, 'hp_metric': 1633.8154296875}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.23933451103594558,\n 'epochs': 128,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.9693203335370014,\n 'optimizer': 'Adamax',\n 'patience': 8}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 19519.423828125, 'hp_metric': 19519.423828125}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.23933451103594558,\n 'epochs': 128,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.9693203335370014,\n 'optimizer': 'Adamax',\n 'patience': 8}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 6648.47900390625, 'hp_metric': 6648.47900390625}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.15827496187066842,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 512,\n 'lr_mult': 2.4208305977937328,\n 'optimizer': 'Adamax',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 6104.76220703125, 'hp_metric': 6104.76220703125}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.15827496187066842,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 512,\n 'lr_mult': 2.4208305977937328,\n 'optimizer': 'Adamax',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 16\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 16\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2992.532958984375, 'hp_metric': 2992.532958984375}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.04825016876158548,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4769991072624946,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2337.91748046875, 'hp_metric': 2337.91748046875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.04825016876158548,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4769991072624946,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3264.527099609375, 'hp_metric': 3264.527099609375}\nspotPython tuning: 1633.8154296875 [#---------] 6.76% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05037009219038317,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.490628647450869,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 1807.1495361328125, 'hp_metric': 1807.1495361328125}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05037009219038317,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.490628647450869,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3191.741943359375, 'hp_metric': 3191.741943359375}\nspotPython tuning: 1633.8154296875 [#---------] 12.74% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05260472442202331,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.505002772751222,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 8475.75, 'hp_metric': 8475.75}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05260472442202331,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.505002772751222,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5522.3935546875, 'hp_metric': 5522.3935546875}\nspotPython tuning: 1633.8154296875 [##--------] 16.49% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05413475936399615,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5148817736749298,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3902.89501953125, 'hp_metric': 3902.89501953125}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05413475936399615,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5148817736749298,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2615.920654296875, 'hp_metric': 2615.920654296875}\nspotPython tuning: 1633.8154296875 [##--------] 19.94% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.055933023612466014,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5264365695558901,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3183.96435546875, 'hp_metric': 3183.96435546875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.055933023612466014,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5264365695558901,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 56039.2890625, 'hp_metric': 56039.2890625}\nspotPython tuning: 1633.8154296875 [##--------] 23.98% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05263943958400707,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5052098265780478,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2725.66357421875, 'hp_metric': 2725.66357421875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05263943958400707,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5052098265780478,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 6538.43310546875, 'hp_metric': 6538.43310546875}\nspotPython tuning: 1633.8154296875 [###-------] 27.02% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05316632156290128,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5086702767555806,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5130.96630859375, 'hp_metric': 5130.96630859375}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05316632156290128,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5086702767555806,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2350.8046875, 'hp_metric': 2350.8046875}\nspotPython tuning: 1633.8154296875 [###-------] 30.43% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.053771679137023694,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5124802181853274,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 10390.2138671875, 'hp_metric': 10390.2138671875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.053771679137023694,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5124802181853274,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2203.74755859375, 'hp_metric': 2203.74755859375}\nspotPython tuning: 1633.8154296875 [####------] 35.85% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05402750909623282,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5141703050361923,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3288.8427734375, 'hp_metric': 3288.8427734375}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05402750909623282,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5141703050361923,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5342.2548828125, 'hp_metric': 5342.2548828125}\nspotPython tuning: 1633.8154296875 [####------] 39.76% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05448797256538013,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5170610907476367,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 1620.1785888671875, 'hp_metric': 1620.1785888671875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05448797256538013,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5170610907476367,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3572.1640625, 'hp_metric': 3572.1640625}\nspotPython tuning: 1620.1785888671875 [####------] 43.95% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.055174351358467405,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5215060127606514,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2750.79541015625, 'hp_metric': 2750.79541015625}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.055174351358467405,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5215060127606514,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5175.3466796875, 'hp_metric': 5175.3466796875}\nspotPython tuning: 1620.1785888671875 [#####-----] 48.21% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.055696958537016866,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.524867959583039,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5328.39404296875, 'hp_metric': 5328.39404296875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.055696958537016866,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.524867959583039,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5937.96533203125, 'hp_metric': 5937.96533203125}\nspotPython tuning: 1620.1785888671875 [#####-----] 54.13% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05589887172269804,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.526150803701278,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2388.19189453125, 'hp_metric': 2388.19189453125}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05589887172269804,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.526150803701278,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 7502.37548828125, 'hp_metric': 7502.37548828125}\nspotPython tuning: 1620.1785888671875 [######----] 60.92% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.0562113034404144,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5280954720377455,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 36894.0625, 'hp_metric': 36894.0625}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.0562113034404144,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5280954720377455,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 5805.9248046875, 'hp_metric': 5805.9248046875}\nspotPython tuning: 1620.1785888671875 [######----] 63.95% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.052806491168032314,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5067848529726446,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 9569.8994140625, 'hp_metric': 9569.8994140625}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.052806491168032314,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5067848529726446,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3403.76953125, 'hp_metric': 3403.76953125}\nspotPython tuning: 1620.1785888671875 [#######---] 71.76% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05245253149077113,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5040814155528057,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 4728.78515625, 'hp_metric': 4728.78515625}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05245253149077113,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.5040814155528057,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 12009.7314453125, 'hp_metric': 12009.7314453125}\nspotPython tuning: 1620.1785888671875 [########--] 77.66% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05196894703679841,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.501019508422615,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3601.85400390625, 'hp_metric': 3601.85400390625}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05196894703679841,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.501019508422615,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 12909.5517578125, 'hp_metric': 12909.5517578125}\nspotPython tuning: 1620.1785888671875 [########--] 81.80% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.051474986340905554,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4979255024445266,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2890.673583984375, 'hp_metric': 2890.673583984375}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.051474986340905554,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4979255024445266,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 4002.3369140625, 'hp_metric': 4002.3369140625}\nspotPython tuning: 1620.1785888671875 [#########-] 85.38% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05080741682078949,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4935919826787676,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 3963.92041015625, 'hp_metric': 3963.92041015625}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.05080741682078949,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4935919826787676,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 6113.154296875, 'hp_metric': 6113.154296875}\nspotPython tuning: 1620.1785888671875 [#########-] 89.13% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.050055475077940365,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4887675150407989,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 6603.4912109375, 'hp_metric': 6603.4912109375}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.050055475077940365,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4887675150407989,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2470.803955078125, 'hp_metric': 2470.803955078125}\nspotPython tuning: 1620.1785888671875 [#########-] 92.73% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.04912791894409341,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4828174957876712,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 9845.26171875, 'hp_metric': 9845.26171875}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.04912791894409341,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.4828174957876712,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 8919.93359375, 'hp_metric': 8919.93359375}\nspotPython tuning: 1620.1785888671875 [##########] 97.02% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.04899003257201811,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.481927837832117,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2008.3822021484375, 'hp_metric': 2008.3822021484375}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.04899003257201811,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 1.481927837832117,\n 'optimizer': 'Adagrad',\n 'patience': 16}\ntrain_model(): Test set size: 715\ntrain_model(): Train set size: 1167\ntrain_model(): Batch size: 64\nLightDataModule: train_dataloader(). Training set size: 1167\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\ntrain_model result: {'val_loss': 2171.0625, 'hp_metric': 2171.0625}\nspotPython tuning: 1620.1785888671875 [##########] 100.00% Done...\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      9691.404296875       │\n│         val_loss          │      9691.404296875       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       82330.359375        │\n│         val_loss          │       82330.359375        │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      5680.0966796875      │\n│         val_loss          │      5680.0966796875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      2287.1826171875      │\n│         val_loss          │      2287.1826171875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      45320.10546875       │\n│         val_loss          │      45320.10546875       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       44621.0390625       │\n│         val_loss          │       44621.0390625       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      109158.8515625       │\n│         val_loss          │      109158.8515625       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       151683.59375        │\n│         val_loss          │       151683.59375        │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      8908.8837890625      │\n│         val_loss          │      8908.8837890625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      16831.779296875      │\n│         val_loss          │      16831.779296875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │        170177.1875        │\n│         val_loss          │        170177.1875        │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      4781.1748046875      │\n│         val_loss          │      4781.1748046875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │        363511.125         │\n│         val_loss          │        363511.125         │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      18567.58203125       │\n│         val_loss          │      18567.58203125       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2381.155517578125     │\n│         val_loss          │     2381.155517578125     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      1633.8154296875      │\n│         val_loss          │      1633.8154296875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      19519.423828125      │\n│         val_loss          │      19519.423828125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     6648.47900390625      │\n│         val_loss          │     6648.47900390625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     6104.76220703125      │\n│         val_loss          │     6104.76220703125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2992.532958984375     │\n│         val_loss          │     2992.532958984375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2337.91748046875      │\n│         val_loss          │     2337.91748046875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3264.527099609375     │\n│         val_loss          │     3264.527099609375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1807.1495361328125     │\n│         val_loss          │    1807.1495361328125     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3191.741943359375     │\n│         val_loss          │     3191.741943359375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │          8475.75          │\n│         val_loss          │          8475.75          │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      5522.3935546875      │\n│         val_loss          │      5522.3935546875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3902.89501953125      │\n│         val_loss          │     3902.89501953125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2615.920654296875     │\n│         val_loss          │     2615.920654296875     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3183.96435546875      │\n│         val_loss          │     3183.96435546875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       56039.2890625       │\n│         val_loss          │       56039.2890625       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2725.66357421875      │\n│         val_loss          │     2725.66357421875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     6538.43310546875      │\n│         val_loss          │     6538.43310546875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     5130.96630859375      │\n│         val_loss          │     5130.96630859375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       2350.8046875        │\n│         val_loss          │       2350.8046875        │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     10390.2138671875      │\n│         val_loss          │     10390.2138671875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2203.74755859375      │\n│         val_loss          │     2203.74755859375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      3288.8427734375      │\n│         val_loss          │      3288.8427734375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      5342.2548828125      │\n│         val_loss          │      5342.2548828125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1620.1785888671875     │\n│         val_loss          │    1620.1785888671875     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       3572.1640625        │\n│         val_loss          │       3572.1640625        │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2750.79541015625      │\n│         val_loss          │     2750.79541015625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      5175.3466796875      │\n│         val_loss          │      5175.3466796875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     5328.39404296875      │\n│         val_loss          │     5328.39404296875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     5937.96533203125      │\n│         val_loss          │     5937.96533203125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2388.19189453125      │\n│         val_loss          │     2388.19189453125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     7502.37548828125      │\n│         val_loss          │     7502.37548828125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │        36894.0625         │\n│         val_loss          │        36894.0625         │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      5805.9248046875      │\n│         val_loss          │      5805.9248046875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      9569.8994140625      │\n│         val_loss          │      9569.8994140625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       3403.76953125       │\n│         val_loss          │       3403.76953125       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       4728.78515625       │\n│         val_loss          │       4728.78515625       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     12009.7314453125      │\n│         val_loss          │     12009.7314453125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3601.85400390625      │\n│         val_loss          │     3601.85400390625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     12909.5517578125      │\n│         val_loss          │     12909.5517578125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2890.673583984375     │\n│         val_loss          │     2890.673583984375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      4002.3369140625      │\n│         val_loss          │      4002.3369140625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3963.92041015625      │\n│         val_loss          │     3963.92041015625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      6113.154296875       │\n│         val_loss          │      6113.154296875       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      6603.4912109375      │\n│         val_loss          │      6603.4912109375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2470.803955078125     │\n│         val_loss          │     2470.803955078125     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       9845.26171875       │\n│         val_loss          │       9845.26171875       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       8919.93359375       │\n│         val_loss          │       8919.93359375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2008.3822021484375     │\n│         val_loss          │    2008.3822021484375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │         2171.0625         │\n│         val_loss          │         2171.0625         │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x2fd506c50&gt;",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-tensorboard-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-tensorboard-33",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.9 Step 9: Tensorboard",
    "text": "21.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-results-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-results-33",
    "title": "21  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "21.10 Step 10: Results",
    "text": "21.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed.\n\nif spot_tuner.noise:\n    print(spot_tuner.min_mean_X)\n    print(spot_tuner.min_mean_y)\nelse:\n    print(spot_tuner.min_X)\n    print(spot_tuner.min_y)\n\n[8.         8.         6.         1.         2.         0.05448797\n 1.51706109 4.        ]\n1620.1785888671875\n\n\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX +\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     5.0 |     9.0 | 8.0                 | transform_power_2_int |         0.00 |         |\n| epochs         | int    | 4         |     5.0 |    10.0 | 8.0                 | transform_power_2_int |        79.92 | **      |\n| batch_size     | int    | 4         |     3.0 |     6.0 | 6.0                 | transform_power_2_int |         0.00 |         |\n| act_fn         | factor | ReLU      |     0.0 |     1.0 | LeakyReLU           | None                  |         0.01 |         |\n| optimizer      | factor | SGD       |     0.0 |     2.0 | Adagrad             | None                  |       100.00 | ***     |\n| dropout_prob   | float  | 0.01      |   0.005 |    0.25 | 0.05448797256538013 | None                  |         0.02 |         |\n| lr_mult        | float  | 1.0       |    0.25 |     5.0 | 1.5170610907476367  | None                  |         0.00 |         |\n| patience       | int    | 2         |     3.0 |     5.0 | 4.0                 | transform_power_2_int |         0.00 |         |\n| initialization | factor | Default   |     0.0 |     0.0 | Default             | None                  |         0.00 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + PREFIX + \"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n21.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nprint(config)\n\n{'l1': 256, 'epochs': 256, 'batch_size': 64, 'act_fn': LeakyReLU(), 'optimizer': 'Adagrad', 'dropout_prob': 0.05448797256538013, 'lr_mult': 1.5170610907476367, 'patience': 16, 'initialization': 'Default'}\n\n\n\nTest on the full data set\n\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule: train_dataloader(). Training set size: 381\nLightDataModule: train_dataloader(). batch_size: 64\nLightDataModule: train_dataloader(). num_workers: 0\nLightDataModule: test_dataloader(). Training set size: 1429\nLightDataModule: test_dataloader(). batch_size: 64\nLightDataModule: test_dataloader(). num_workers: 0\ntest_model result: {'val_loss': 3984.07421875, 'hp_metric': 3984.07421875}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       3984.07421875       │\n│         val_loss          │       3984.07421875       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n(3984.07421875, 3984.07421875)\n\n\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\n\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 256, 'epochs': 256, 'batch_size': 64, 'act_fn': LeakyReLU(), 'optimizer': 'Adagrad', 'dropout_prob': 0.05448797256538013, 'lr_mult': 1.5170610907476367, 'patience': 16, 'initialization': 'Default'}\nLoading model with 256_256_64_LeakyReLU_Adagrad_0.0545_1.5171_16_Default_TEST from runs/saved_models/256_256_64_LeakyReLU_Adagrad_0.0545_1.5171_16_Default_TEST/last.ckpt\nModel: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=133, out_features=256, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.05448797256538013, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.05448797256538013, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.05448797256538013, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.05448797256538013, inplace=False)\n    (12): Linear(in_features=64, out_features=1, bias=True)\n  )\n)\n\n\n\nfilename = \"./figures/\" + PREFIX\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nepochs:  79.92198502293363\noptimizer:  100.0\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n21.10.2 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n                                                \nParallel coordinates plots\n\n\n\n\n21.10.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.cvmodel import cv_model\nset_control_key_value(control_dict=fun_control,\n                        key=\"k_folds\",\n                        value=2,\n                        replace=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"test_size\",\n                        value=0.6,\n                        replace=True)\ncv_model(config, fun_control)\n\nk: 0\nTrain Dataset Size: 1190\nVal Dataset Size: 1191\ntrain_model result: {'val_loss': 4500.970703125, 'hp_metric': 4500.970703125}\nk: 1\nTrain Dataset Size: 1191\nVal Dataset Size: 1190\ntrain_model result: {'val_loss': 4963.96240234375, 'hp_metric': 4963.96240234375}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      4500.970703125       │\n│         val_loss          │      4500.970703125       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     4963.96240234375      │\n│         val_loss          │     4963.96240234375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n4732.466552734375\n\n\n\n\n21.10.4 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n21.10.5 Visualizing the Activation Distribution (Under Development)\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11)\nmodel\n\nNetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=256, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.05448797256538013, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.05448797256538013, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.05448797256538013, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.05448797256538013, inplace=False)\n    (12): Linear(in_features=64, out_features=11, bias=True)\n  )\n)\n\n\n\n# from spotPython.utils.eda import visualize_activations\n# visualize_activations(model, color=f\"C{0}\")",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html",
    "href": "a_01_intro_to_notebooks.html",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "",
    "text": "A.1 Different Notebook cells\nThere are different cells that the notebook is currently supporting:\nAs a default, every cells in jupyter is set to “code”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#different-notebook-cells",
    "href": "a_01_intro_to_notebooks.html#different-notebook-cells",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "",
    "text": "code cells\nmarkdown cells\nraw cells\n\n\n\nA.1.1 Code cells\nThe code cells are used to execute the code. They are following the logic of the choosen kernel. Therefore, it is important to keep in mind which programming language is currently used. Otherwise one might yield an error because of the wrong syntax.\nThe code cells are executed my be ▶ Run button (can be found in the header of the notebook).\n\n\nA.1.2 Markdown cells\nThe markdown cells are a usefull tool to comment the written code. Especially with the help of headers can the code be brought in a more readable format. If you are not familiar with the markdown syntax, you can find a usefull cheat sheet here: Markdown Cheat Sheeet\n\n\nA.1.3 Raw cells\nThe “Raw NBConvert” cell type can be used to render different code formats into HTML or LaTeX by Sphinx. This information is stored in the notebook metadata and converted appropriately.\n\nA.1.3.1 Usage\nTo select a desired format from within Jupyter, select the cell containing your special code and choose options from the following dropdown menus:\n\nSelect “Raw NBConvert”\nSwitch the Cell Toolbar to “Raw Cell Format” (The cell toolbar can be found under View)\nChose the appropriate “Raw NBConvert Format” within the cell\n\nData Science is fun",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#install-packages",
    "href": "a_01_intro_to_notebooks.html#install-packages",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.2 Install Packages",
    "text": "A.2 Install Packages\nBecause python is a heavily used programming language, there are many different packags that can make your life easier. Sadly, there are only a few standard packages that are already included in your python enviroment. If you have the need to install a new package in your enviroment, you can simply do that by exectuing the following code snippet in a code cell\n!pip install numpy\n\nThe ! is used to run the cell as a shell command\npip is package manager for python packages.\nnumpy is the the package you want to install\n\nHint: It is often usefull to restart the kernel after installing a package, otherwise loading the package could lead to an error.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#load-packages",
    "href": "a_01_intro_to_notebooks.html#load-packages",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.3 Load Packages",
    "text": "A.3 Load Packages\nAfter successfully installing the package it is necessary to import them before you can work with them. The import of the packages is done in the following way:\nimport numpy as np\nThe imported packages are often abbreviated. This is because you need to specify where the function is coming from.\nThe most common abbreviations for data science packages are:\n\nAbbreviations for data science packages\n\n\nAbbreviation\nPackage\nImport\n\n\n\n\nnp\nnumpy\nimport numpy as np\n\n\npd\npandas\nimport pandas as pd\n\n\nplt\nmatplotlib\nimport matplotlib.pyplot as plt\n\n\npx\nplotly\nimport plotly.exprss as px\n\n\ntf\ntensorflow\nimport tensorflow as tf\n\n\nsns\nseaborn\nimport seaborn as sns\n\n\ndt\ndatetime\nimport datetime as dt\n\n\npkl\npickle\nimport pickle as pkl",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#functions-in-python",
    "href": "a_01_intro_to_notebooks.html#functions-in-python",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.4 Functions in Python",
    "text": "A.4 Functions in Python\nBecause python is not using Semicolon’s it is import to keep track of indentation in your code. The indentation works as a placeholder for the semicolons. This is especially important if your are defining loops, functions, etc. …\nExample: We are defining a function that calculates the squared sum of its input parameters\n\ndef squared_sum(x,y): \n    z = x**2 + y**2\n    return z\n\nIf you are working with something that needs indentation, it will be already done by the notebook.\nHint: Keep in mind that is good practice to use the return parameter. If you are not using return and a function has multiple paramaters that you would like to return, it will only return the last one defined.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#list-of-useful-jupyter-notebook-shortcuts",
    "href": "a_01_intro_to_notebooks.html#list-of-useful-jupyter-notebook-shortcuts",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.5 List of Useful Jupyter Notebook Shortcuts",
    "text": "A.5 List of Useful Jupyter Notebook Shortcuts\n\nList of useful Jupyter Notebook Shortcuts\n\n\n\n\n\n\n\nFunction\nKeyboard Shortcut\nMenu Tools\n\n\n\n\nSave notebook\nEsc + s\nFile → Save and Checkpoint\n\n\nCreate new Cell\nEsc + a (above),  Esc + b (below)\nInsert → Cell above; Insert → Cell below\n\n\nRun Cell\nCtrl + enter\nCell → Run Cell\n\n\nCopy Cell\nc\nCopy Key\n\n\nPaste Cell\nv\nPaste Key\n\n\nInterrupt Kernel\nEsc + i i\nKernel → Interrupt\n\n\nRestart Kernel\nEsc + 0 0\nKernel → Restart\n\n\n\nIf you combine everything you can create beautiful graphics\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate 100 random data points along 3 dimensions\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\n\n# Map each onto a scatterplot we'll create with Matplotlib\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale)*500)\nax.set(title=\"Some random data, created with the Jupyter Notebook!\")\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html",
    "href": "a_02_git_intro_en.html",
    "title": "Appendix B — Git Introduction",
    "section": "",
    "text": "B.1 Learning Objectives\nIn this learning unit, you will learn how to set up Git as a version control system for a project. The most important Git commands will be explained. You will learn how to track and manage changes to your projects with Git. Specifically:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#learning-objectives",
    "href": "a_02_git_intro_en.html#learning-objectives",
    "title": "Appendix B — Git Introduction",
    "section": "",
    "text": "Initializing a repository: git init\nIgnoring files: .gitignore\nAdding files to the staging area: git add\nChecking status changes: git status\nReviewing history: git log\nCreating a new branch: git branch\nSwitching to the current branch: git switch and git checkout\nMerging two branches: git merge\nResolving conflicts\nReverting changes: git revert\nUploading changes to GitLab: git push\nDownloading changes from GitLab: git pull\nAdvanced: git rebase",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#basics-of-git",
    "href": "a_02_git_intro_en.html#basics-of-git",
    "title": "Appendix B — Git Introduction",
    "section": "B.2 Basics of Git",
    "text": "B.2 Basics of Git\n\nB.2.1 Initializing a Repository: git init\nTo set up Git as a version control system for your project, you need to initialize a new Git repository at the top-level folder, which is the working directory of your project. This is done using the git init command.\nAll files in this folder and its subfolders will automatically become part of the repository. Creating a Git repository is similar to adding an all-powerful passive observer of all things to your project. Git sits there, observes, and takes note of even the smallest changes, such as a single character in a file within a repository with hundreds of files. And it will tell you where these changes occurred if you forget. Once Git is initialized, it monitors all changes made within the working directory, and it tracks the history of events from that point forward. For this purpose, a historical timeline is created for your project, referred to as a “branch,” and the initial branch is named main. So, when someone says they are on the main branch or working on the main branch, it means they are in the historical main timeline of the project. The Git repository, often abbreviated as repo, is a virtual representation of your project, including its history and branches, a book, if you will, where you can look up and retrieve the entire history of the project: you work in your working directory, and the Git repository tracks and stores your work.\n\n\nB.2.2 Ignoring Files: .gitignore\nIt’s useful that Git watches and keeps an eye on everything in your project. However, in most projects, there are files and folders that you don’t need or want to keep an eye on. These may include system files, local project settings, libraries with dependencies, and so on.\nYou can exclude any file or folder from your Git repository by including them in the .gitignore file. In the .gitignore file, you create a list of file names, folder names, and other items that Git should not track, and Git will ignore these items. Hence the name “gitignore.” Do you want to track a file that you previously ignored? Simply remove the mention of the file in the gitignore file, and Git will start tracking it again.\n\n\nB.2.3 Adding Changes to the Staging Area: git add\nThe interesting thing about Git as an all-powerful, passive observer of all things is that it’s very passive. As long as you don’t tell Git what to remember, it will passively observe the changes in the project folder but do nothing.\nWhen you make a change to your project that you want Git to include in the project’s history to take a snapshot of so you can refer back to it later, your personal checkpoint, if you will, you need to first stage the changes in the staging area. What is the staging area? The staging area is where you collect changes to files that you want to include in the project’s history.\nThis is done using the git add command. You can specify which files you want to add by naming them, or you can add all of them using -A. By doing this, you’re telling Git that you’ve made changes and want it to remember these particular changes so you can recall them later if needed. This is important because you can choose which changes you want to stage, and those are the changes that will eventually be transferred to the history.\n\nNote: When you run git add, the changes are not transferred to the project’s history. They are only transferred to the staging area.\n\n\n\n\n\n\n\nExample of git add from the beginning\n\n\n\n# Create a new directory for your\n# repository and navigate to that directory:\n\nmkdir my-repo\ncd my-repo\n\n# Initialize the repository with git init:\n\ngit init\n\n# Create a .gitignore file for Python code.\n# You can use a template from GitHub:\n\ncurl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore -o .gitignore\n\n# Add your files to the repository using git add:\n\ngit add .\nThis adds all files in the current directory to the repository, except for the files listed in the .gitignore file.\n\n\n\n\nB.2.4 Transferring Changes to Memory: git commit\nThe power of Git becomes evident when you start transferring changes to the project history. This is done using the git commit command. When you run git commit, you inform Git that the changes in the staging area should be added to the history of the project so that they can be referenced or retrieved later.\nAdditionally, you can add a commit message with the -m option to explain what changes were made. So when you look back at the project history, you can see that you added a new feature.\ngit commit creates a snapshot, an image of the current state of your project at that specific time, and adds it to the branch you are currently working on.\nAs you work on your project and transfer more snapshots, the branch grows and forms a timeline of events. This means you can now look back at every transfer in the branch and see what your code looked like at that time.\nYou can compare any phase of your code with any other phase of your code to find errors, restore deleted code, or do things that would otherwise not be possible, such as resetting the project to a previous state or creating a new timeline from any point.\nSo how often should you add these commits? My rule of thumb is not to commit too often. It’s better to have a Git repository with too many commits than one with too few commits.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nAfter adding your files with git add, you can create a commit to save your changes. Use the git commit command with the -m option to specify your commit message:\ngit commit -m \"My first commit message\"\nThis creates a new commit with the added files and the specified commit message.\n\n\n\n\nB.2.5 Check the Status of Your Repository: git status\nIf you’re wondering what you’ve changed in your project since the last commit snapshot, you can always check the Git status. Git will list every modified file and the current status of each file.\nThis status can be either:\n\nUnchanged (unmodified), meaning nothing has changed since you last transferred it, or\nIt’s been changed (changed) but not staged (staged) to be transferred into the history, or\nSomething has been added to staging (staged) and is ready to be transferred into the history.\n\nWhen you run git status, you get an overview of the current state of your project.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nThe git status command displays the status of your working directory and the staging area. It shows you which files have been modified, which files are staged for commit, and which files are not yet being tracked:\ngit status\ngit status is a useful tool to keep track of your changes and ensure that you have added all the desired files for commit.\n\n\n\n\nB.2.6 Review Your Repository’s History: git log\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nYou can view the history of your commits with the git log command. This command displays a list of all the commits in the current branch, along with information such as the author, date, and commit message:\ngit log\nThere are many options to customize the output of git log. For example, you can use the --pretty option to change the format of the output:\ngit log --pretty=oneline\nThis displays each commit in a single line.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#branches-timelines",
    "href": "a_02_git_intro_en.html#branches-timelines",
    "title": "Appendix B — Git Introduction",
    "section": "B.3 Branches (Timelines)",
    "text": "B.3 Branches (Timelines)\n\nB.3.1 Creating an Alternative Timeline: git branch\nIn the course of developing a project, you often reach a point where you want to add a new feature, but doing so might require changing the existing code in a way that could be challenging to undo later.\nOr maybe you just want to experiment and be able to discard your work if the experiment fails. In such cases, Git allows you to create an alternative timeline called a branch to work in.\nThis new branch has its own name and exists in parallel with the main branch and all other branches in your project.\nDuring development, you can switch between branches and work on different versions of your code concurrently. This way, you can have a stable codebase in the main branch while developing an experimental feature in a separate branch. When you switch from one branch to another, the code you’re working on is automatically reset to the latest commit of the branch you’re currently in.\nIf you’re working in a team, different team members can work on their own branches, creating an entire universe of alternative timelines for your project. When features are completed, they can be seamlessly merged back into the main branch.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nTo create a new branch, you can use the git branch command with the name of the new branch as an argument:\ngit branch my-tests\n\n\n\n\nB.3.2 The Pointer to the Current Branch: HEAD\nHow does Git know where you are on the timeline, and how can you keep track of your position?\nYou’re always working at the tip (HEAD) of the currently active branch. The HEAD pointer points there quite literally. In a new project archive with just a single main branch and only new commits being added, HEAD always points to the latest commit in the main branch. That’s where you are.\nHowever, if you’re in a repository with multiple branches, meaning multiple alternative timelines, HEAD will point to the latest commit in the branch you’re currently working on.\n\n\nB.3.3 Switching to an Alternative Timeline: git switch\nAs your project grows, and you have multiple branches, you need to be able to switch between these branches. This is where the switch command comes into play.\nAt any time, you can use the git switch command with the name of the branch you want to switch to, and HEAD moves from your current branch to the one you specified.\nIf you’ve made changes to your code before switching, Git will attempt to carry those changes over to the branch you’re switching to. However, if these changes conflict with the target branch, the switch will be canceled.\nTo resolve this issue without losing your changes, return to the original branch, add and commit your recent changes, and then perform the switch.\n\n\nB.3.4 Switching to an Alternative Timeline and Making Changes: git checkout\nTo switch between branches, you can also use the git checkout command. It works similarly to git switch for this purpose: you pass the name of the branch you want to switch to, and HEAD moves to the beginning of that branch.\nBut checkout can do more than just switch to another timeline. With git checkout, you can also move to any commit point in any timeline. In other words, you can travel back in time and work on code from the past.\nTo do this, use git checkout and provide the commit ID. This is an automatically generated, random combination of letters and numbers that identifies each commit. You can retrieve the commit ID using git log. When you run git log, you get a list of all the commits in your repository, starting with the most recent ones.\nWhen you use git checkout with an older commit ID, you check out a commit in the middle of a branch. This disrupts the timeline, as you’re actively attempting to change history. Git doesn’t want you to do that because, much like in a science fiction movie, altering the past might also alter the future. In our case, it would break the version control branch’s coherence.\nTo prevent you from accidentally disrupting time and altering history, checking out an earlier commit in any branch results in the warning “Detached Head,” which sounds rather ominous. The “Detached Head” warning is appropriate because it accurately describes what’s happening. Git literally detaches the head from the branch and sets it aside.\nNow, you’re working outside of time in a space unbound to any timeline, which again sounds rather threatening but is perfectly fine in reality.\nTo continue working on this past code, all you need to do is reattach it to the timeline. You can use git branch to create a new branch, and the detached head will automatically attach to this new branch.\nInstead of breaking the history, you’ve now created a new alternative timeline that starts in the past, allowing you to work safely. You can continue working on the branch as usual.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nTo switch to a new branch, you can use the git checkout command:\ngit checkout meine-tests\nNow you’re using the new branch and can make changes independently from the original branch.\n\n\n\n\nB.3.5 The Difference Between checkout and switch\nWhat is the difference between git switch and git checkout? git switch and git checkout are two different commands that both serve the purpose of switching between branches. You can use both to switch between branches, but they have an important distinction. git switch is a new command introduced with Git 2.23. git checkout is an older command that has existed since Git 1.6.0. So, git switch and git checkout have different origins. git switch was introduced to separate the purposes of git checkout. git checkout has two different purposes: 1. It can be used to switch between branches, and 2. It can be used to reset files to the state of the last commit.\nHere’s an example: In my project, I made a change since the last commit, but I haven’t staged it yet. Then, I realized that I actually don’t want this change. I want to reset the file to the state before the last commit. As long as I haven’t committed my changes, I can do this with git checkout by targeting the specific file. So, if that file is named main.js, I can say: git checkout main.js. And the file will be reset to the state of the last commit, which makes sense. I’m checking out the file from the last commit.\nBut that’s quite different from switching between the beginning of one branch to another. git switch and git restore were introduced to separate these two operations. git switch is for switching between branches, and git restore is for resetting the specified file to the state of the last commit. If you try to restore a file with git switch, it simply won’t work. It’s not intended for that. As I mentioned earlier, it’s about separating concerns.\n:::{.callout-note} #### Difference Between checkout and switch git checkout and git switch are both commands for switching between branches in a Git repository. The main difference between the two commands is that git switch is a newer command specifically designed for branch switching, while git checkout is an older command that can be used for various tasks, including branch switching.\nHere’s an example demonstrating how to initialize a repository and switch between branches:\n# Create a new directory for your repository\n# and navigate to that directory:\nmkdir my-repo\ncd my-repo\n\n# Initialize the repository with git init:\ngit init\n\n# Create a new branch with git branch:\ngit branch my-new-branch\n\n# Switch to the new branch using git switch:\ngit switch my-new-branch\n\n# Alternatively, you can also use git checkout\n# to switch to the new branch:\n\ngit checkout my-new-branch\nBoth commands lead to the same result: You are now on the new branch.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#merging-branches-and-resolving-conflicts",
    "href": "a_02_git_intro_en.html#merging-branches-and-resolving-conflicts",
    "title": "Appendix B — Git Introduction",
    "section": "B.4 Merging Branches and Resolving Conflicts",
    "text": "B.4 Merging Branches and Resolving Conflicts\n\nB.4.1 git merge: Merging Two Timelines\nGit allows you to split your development work into as many branches or alternative timelines as you like, enabling you to work on many different versions of your code simultaneously without losing or overwriting any of your work.\nThis is all well and good, but at some point, you need to bring those various versions of your code back together into one branch. That’s where git merge comes in.\nConsider an example where you have two branches, a main branch and an experimental branch called experimental-branch. In the experimental branch, there is a new feature. To merge these two branches, you set HEAD to the branch where you want to incorporate the code and execute git merge followed by the name of the branch you want to merge. HEAD is a special pointer that points to the current branch. When you run git merge, it combines the code from the branch associated with HEAD with the code from the branch specified by the branch name you provide.\n# Initialize the repository\ngit init\n\n# Create a new branch called \"experimental-branch\"\ngit branch experimental-branch\n\n# Switch to the \"experimental-branch\"\ngit checkout experimental-branch\n\n# Add the new feature here and\n# make a commit\n# ...\n\n# Switch back to the \"main\" branch\ngit checkout main\n\n# Perform the merge\ngit merge experimental-branch\nDuring the merge, matching pieces of code in the branches overlap, and any new code from the branch being merged is added to the project. So now, the main branch also contains the code from the experimental branch, and the events of the two separate timelines have been merged into a single one. What’s interesting is that even though the experimental branch was merged with the main branch, the last commit of the experimental branch remains intact, allowing you to continue working on the experimental branch separately if you wish.\n\n\nB.4.2 Resolving Conflicts When Merging\nMerging branches where there are no code changes at the same place in both branches is a straightforward process. It’s also a rare process. In most cases, there will be some form of conflict between the branches – the same code or the same code area has been modified differently in the different branches. Merging two branches with such conflicts will not work, at least not automatically.\nIn this case, Git doesn’t know how to merge this code. So, when such a situation occurs, it’s marked as a conflict, and the merging process is halted. This might sound more dramatic than it is. When you get a conflict warning, Git is saying there are two different versions here, and Git needs to know which one you want to keep. To help you figure out the conflict, Git combines all the code into a single file and automatically marks the conflicting code as the current change, which is the original code from the branch you’re working on, or as the incoming change, which is the code from the file you’re trying to merge.\nTo resolve this conflict, you’ll edit the file to literally resolve the code conflict. This might mean accepting either the current or incoming change and discarding the other. It could mean combining both changes or something else entirely. It’s up to you. So, you edit the code to resolve the conflict. Once you’ve resolved the conflict by editing the code, you add the new conflict-free version to the staging area with git add and then commit the merged code with git commit. That’s how the conflict is resolved.\nA merge conflict occurs when Git struggles to automatically merge changes from two different branches. This usually happens when changes were made to the same line in the same file in both branches. To resolve a merge conflict, you must manually edit the affected files and choose the desired changes. Git marks the conflict areas in the file with special markings like &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. You can search for these markings and manually select the desired changes. After resolving the conflicts, you can add the changes with git add and create a new commit with git commit to complete the merge.\nHere’s an example:\n# Perform the merge (this will cause a conflict)\ngit merge experimenteller-branch\n\n# Open the affected file in an editor and manually resolve the conflicts\n# ...\n\n# Add the modified file\ngit add &lt;filename&gt;\n\n# Create a new commit\ngit commit -m \"Resolved conflicts\"\n\n\nB.4.3 git revert: Undoing Something\nOne of the most powerful features of any software tool is the “Undo” button. Make a mistake, press “Undo,” and it’s as if it never happened. However, that’s not quite as simple when an all-powerful, passive observer is watching and recording your project’s history. How do you undo something that you’ve added to the history without rewriting the history?\nThe answer is that you can overwrite the history with the git reset command, but that’s quite risky and not a good practice.\nA better solution is to work with the historical timeline and simply place an older version of your code at the top of the branch. This is done with git revert. To make this work, you need to know the commit ID of the commit you want to go back to.\nThe commit ID is a machine-generated set of random numbers and letters, also known as a hash. To get a list of all the commits in the repository, including the commit ID and commit message, you can run git log.\n# Show the list of all operations in the repository\ngit log\nBy the way, it’s a good idea to leave clear and informative commit messages for this reason. This way, you know what happened in your previous commits. Once you’ve found the commit you want to revert to, call that commit ID with git revert, and then the ID. This will create a new commit at the top of the branch with the code from the reference commit. To transfer the code to the branch, add a commit message and save it. Now, the last commit in your branch matches the commit you’re reverting to, and your project’s history remains intact.\n\n\n\n\n\n\nAn example with git revert\n\n\n\n# Initialize a new repository\ngit init\n\n# Create a new file\necho \"Hello, World\" &gt; file.txt\n\n# Add the file to the repository\ngit add file.txt\n\n# Create a new commit\ngit commit -m \"First commit\"\n\n# Modify the file\necho \"Goodbye, World\" &gt; file.txt\n\n# Add the modified file\ngit add file.txt\n\n# Create a new commit\ngit commit -m \"Second commit\"\n\n# Use git log to find the commit ID of the second commit\ngit log\n\n# Use git revert to undo the changes from the second commit\ngit revert &lt;commit-id&gt;\n\n\nTo download the students branch from the repository git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git to your local machine, add a file, and upload the changes, you can follow these steps:\n\n\n\n\n\n\nAn example with git clone, git checkout, git add, git commit, git push\n\n\n\n# Clone the repository to your local machine:\ngit clone git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git\n\n# Change to the cloned repository:\ncd numerische-mathematik-sommersemester2023\n\n# Switch to the students branch:\ngit checkout students\n\n# Create the Test folder if it doesn't exist:\nmkdir Test\n\n# Create the Testdatei.txt file in the Test folder:\ntouch Test/Testdatei.txt\n\n# Add the file with git add:\ngit add Test/Testdatei.txt\n\n# Commit the changes with git commit:\ngit commit -m \"Added Testdatei.txt\"\n\n# Push the changes with git push:\ngit push origin students\nThis will upload the changes to the server and update the students branch in the repository.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#downloading-from-gitlab",
    "href": "a_02_git_intro_en.html#downloading-from-gitlab",
    "title": "Appendix B — Git Introduction",
    "section": "B.5 Downloading from GitLab",
    "text": "B.5 Downloading from GitLab\nTo download changes from a GitLab repository to your local machine, you can use the git pull command. This command downloads the latest changes from the specified remote repository and merges them with your local repository.\nHere is an example:\n\n\n\n\n\n\nAn example with git pull\n\n\n\n\n# Navigate to the local repository\n# linked to the GitHub repository:\ncd my-local-repository\n\n# Make sure you are in the correct branch:\ngit checkout main\n\n# Download the latest changes from GitHub:\ngit pull origin main\nThis downloads the latest changes from the main branch of the remote repository named “origin” and merges them with your local repository.\n\n\n\nIf there are conflicts between the downloaded changes and your local changes, you will need to resolve them manually before proceeding.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#advanced",
    "href": "a_02_git_intro_en.html#advanced",
    "title": "Appendix B — Git Introduction",
    "section": "B.6 Advanced",
    "text": "B.6 Advanced\n\nB.6.1 git rebase: Moving the Base of a Branch\nIn some cases, you may need to “rewrite history.” A common scenario is that you’ve been working on a new feature in a feature branch, and you realize that the work should have actually happened in the main branch.\nTo resolve this issue and make it appear as if the work occurred in the main branch, you can reset the experimental branch. “Rebase” literally means detaching the base of the experimental branch and moving it to the beginning of another branch, giving the branch a new base, thus “rebasing.”\nThis operation is performed from the branch you want to “rebase.” You use git rebase and specify the branch you want to use as the new base. If there are no conflicts between the experimental branch and the branch you want to rebase onto, this process happens automatically.\nIf there are conflicts, Git will guide you through the conflict resolution process for each commit from the rebase branch.\nThis may sound like a lot, but there’s a good reason for it. You are literally rewriting history by transferring commits from one branch to another. To maintain the coherence of the new version history, there should be no conflicts within the commits. So, you need to resolve them one by one until the history is clean. It goes without saying that this can be a fairly labor-intensive process. Therefore, you should not use git rebase frequently.\n\n\n\n\n\n\nAn example with git rebase\n\n\n\ngit rebase is a command used to change the base of a branch. This means that commits from the branch are applied to a new base, which is usually another branch. It can be used to clean up the repository history and avoid merge conflicts.\nHere is an example showing how to use git rebase:\n\nIn this example, we initialize a new Git repository and create a new file. We add the file to the repository and make an initial commit. Then, we create a new branch called “feature” and switch to that branch. We make changes to the file in the feature branch and create a new commit.\nThen, we switch back to the main branch and make changes to the file again. We add the modified file and make another commit.\nTo rebase the feature branch onto the main branch, we first switch to the feature branch and then use the git rebase command with the name of the main branch as an argument. This applies the commits from the feature branch to the main branch and changes the base of the feature branch.\n\n# Initialize a new repository\ngit init\n# Create a new file\necho \"Hello World\" &gt; file.txt\n# Add the file to the repository\ngit add file.txt\n# Create an initial commit\ngit commit -m \"Initial commit\"\n# Create a new branch called \"feature\"\ngit branch feature\n# Switch to the \"feature\" branch\ngit checkout feature\n# Make changes to the file in the \"feature\" branch\necho \"Hello Feature World\" &gt; file.txt\n# Add the modified file\ngit add file.txt\n# Create a new commit in the \"feature\" branch\ngit commit -m \"Feature commit\"\n# Switch back to the \"main\" branch\ngit checkout main\n# Make changes to the file in the \"main\" branch\necho \"Hello Main World\" &gt; file.txt\n# Add the modified file\ngit add file.txt\n# Create a new commit in the \"main\" branch\ngit commit -m \"Main commit\"\n# Use git rebase to rebase the \"feature\" branch\n# onto the \"main\" branch\ngit checkout feature\ngit rebase main",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#exercises",
    "href": "a_02_git_intro_en.html#exercises",
    "title": "Appendix B — Git Introduction",
    "section": "B.7 Exercises",
    "text": "B.7 Exercises\nIn order to be able to carry out this exercise, we provide you with a functional working environment. This can be accessed here. You can log in using your GMID. If you do not have one, you can generate one here. Once you have successfully logged in to the server, you must open a terminal instance. You are now in a position to carry out the exercise.\nAlternatively, you can also carry out the exercise locally on your computer, but then you will need to install git.\n\nB.7.1 Create project folder\nFirst create the test-repo folder via the command line and then navigate to this folder using the corresponding command.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#initialize-repo",
    "href": "a_02_git_intro_en.html#initialize-repo",
    "title": "Appendix B — Git Introduction",
    "section": "B.8 Initialize repo",
    "text": "B.8 Initialize repo\nNow initialize the repository so that the future project, which will be saved in the test-repo folder, and all associated files are versioned.\n\nB.8.1 Do not upload / ignore certain file types\nIn order to carry out this exercise, you must first download a file which you then have git ignore. To do this, download the current examination regulations for the Bachelor’s degree program in Electrical Engineering using the following command curl -o pruefungsordnung.pdf https://www.th-koeln.de/mam/downloads/deutsch/studium/studiengaenge/f07/ordnungen_plaene/f07_bpo_ba_ekb_2021_01_04.pdf.\nThe PDF file has been stored in the root directory of your repo and you must now exclude it from being uploaded so that no changes to this file are tracked. Please note that not only this one PDF file should be ignored, but all PDF files in the repo.\n\n\nB.8.2 Create file and stage it\nIn order to be able to commit a change later and thus make it traceable, it must first be staged. However, as we only have a PDF file so far, which is to be ignored by git, we cannot stage anything. Therefore, in this task, a file test.txt with some string as content is to be created and then staged.\n\n\nB.8.3 Create another file and check status\nTo understand the status function, you should create the file test2.txt and then call the status function of git.\n\n\nB.8.4 Commit changes\nAfter the changes to the test.txt file have been staged and these are now to be transferred to the project process, they must be committed. Therefore, in this step you should perform a corresponding commit in the current branch with the message test-commit. Finally, you should also display the history of the commits.\n\n\nB.8.5 Create a new branch and switch to it\nIn this task, you are to create a new branch with the name change-text in which you will later make changes. You should then switch to this branch.\n\n\nB.8.6 Commit changes in the new branch\nTo be able to merge the new branch into the main branch later, you must first make changes to the test.txt file. To do this, open the file and simply change the character string in this file before saving the changes and closing the file. Before you now commit the file, you should reset the file to the status of the last commit for practice purposes and thus undo the change. After you have done this, open the file test.txt again and change the character string again before saving and closing the file. This time you should commit the file test.txt and then commit it with the message test-commit2.\n\n\nB.8.7 Merge branch into main\nAfter you have committed the change to the test.txt file, you should merge the change-text branch including the change into the main branch so that it is also available there.\n\n\nB.8.8 Resolve merge conflict\nTo simulate a merge conflict, you must first change the content of the test.txt file before you commit the change. Then switch to the branch change-text and change the file test.txt there as well before you commit the change. Now you should try to merge the branch change-text into the main branch and solve the problems that occur in order to be able to perform the merge successfully.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_03_python_intro_en.html",
    "href": "a_03_python_intro_en.html",
    "title": "Appendix C — Python Introduction",
    "section": "",
    "text": "C.1 Recommendations\nBeginner’s Guide to Python",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python Introduction</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html",
    "href": "a_04_spot_doc.html",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "",
    "text": "D.1 An Initial Example\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[\nf(x) = x^2.\n\\]\nfun = analytical().fun_sphere\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\nspot_1 = spot.Spot(fun=fun,\n                   fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                   design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                   surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_1.run()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x105860490&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#organization",
    "href": "a_04_spot_doc.html#organization",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.2 Organization",
    "text": "D.2 Organization\nSpot organizes the surrogate based optimization process in four steps:\n\nSelection of the objective function: fun.\nSelection of the initial design: design.\nSelection of the optimization algorithm: optimizer.\nSelection of the surrogate model: surrogate.\n\nFor each of these steps, the user can specify an object:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_sphere\nfrom spotPython.design.spacefilling import spacefilling\ndesign = spacefilling(2)\nfrom scipy.optimize import differential_evolution\noptimizer = differential_evolution\nfrom spotPython.build.kriging import Kriging\nsurrogate = Kriging()\n\nFor each of these steps, the user can specify a dictionary of control parameters.\n\nfun_control\ndesign_control\noptimizer_control\nsurrogate_control\n\nEach of these dictionaries has an initialzaion method, e.g., fun_control_init(). The initialization methods set the default values for the control parameters.\n\n\n\n\n\n\nImportant:\n\n\n\n\nThe specification of an lower bound in fun_control is mandatory.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\nfun_control=fun_control_init(lower=np.array([-1, -1]),\n                            upper=np.array([1, 1]))\ndesign_control=design_control_init()\noptimizer_control=optimizer_control_init()\nsurrogate_control=surrogate_control_init()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#the-spot-object",
    "href": "a_04_spot_doc.html#the-spot-object",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.3 The Spot Object",
    "text": "D.3 The Spot Object\nBased on the definition of the fun, design, optimizer, and surrogate objects, and their corresponding control parameter dictionaries, fun_control, design_control, optimizer_control, and surrogate_control, the spot object can be build as follows:\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       optimizer_control=optimizer_control,\n                       surrogate_control=surrogate_control)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run",
    "href": "a_04_spot_doc.html#run",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.4 Run",
    "text": "D.4 Run\n\nspot_tuner.run()\n\nspotPython tuning: 1.801603872454505e-05 [#######---] 73.33% \nspotPython tuning: 1.801603872454505e-05 [########--] 80.00% \nspotPython tuning: 1.801603872454505e-05 [#########-] 86.67% \nspotPython tuning: 1.801603872454505e-05 [#########-] 93.33% \nspotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x108688a50&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#print-the-results",
    "href": "a_04_spot_doc.html#print-the-results",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.5 Print the Results",
    "text": "D.5 Print the Results\n\nspot_tuner.print_results()\n\nmin y: 1.801603872454505e-05\nx0: 0.0019077911677074135\nx1: 0.003791618596979743\n\n\n[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#show-the-progress",
    "href": "a_04_spot_doc.html#show-the-progress",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.6 Show the Progress",
    "text": "D.6 Show the Progress\n\nspot_tuner.plot_progress()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#visualize-the-surrogate",
    "href": "a_04_spot_doc.html#visualize-the-surrogate",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.7 Visualize the Surrogate",
    "text": "D.7 Visualize the Surrogate\n\nThe plot method of the kriging surrogate is used.\nNote: the plot uses the interval defined by the ranges of the natural variables.\n\n\nspot_tuner.surrogate.plot()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "href": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.8 Run With a Specific Start Design",
    "text": "D.8 Run With a Specific Start Design\nTo pass a specific start design, use the X_start argument of the run method.\n\nspot_x0 = spot.Spot(fun=fun,\n                    fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                    design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                    surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_x0.run(X_start=np.array([0.5, -0.5]))\nspot_x0.plot_progress()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#init-build-initial-design",
    "href": "a_04_spot_doc.html#init-build-initial-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.9 Init: Build Initial Design",
    "text": "D.9 Init: Build Initial Design\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\ngen = spacefilling(2)\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\n\n[[ 8.97647221 13.41926847]\n [ 0.66946019  1.22344228]\n [ 5.23614115 13.78185824]\n [ 5.6149825  11.5851384 ]\n [-1.72963184  1.66516096]\n [-4.26945568  7.1325531 ]\n [ 1.26363761 10.17935555]\n [ 2.88779942  8.05508969]\n [-3.39111089  4.15213772]\n [ 7.30131231  5.22275244]]\n[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975\n  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#replicability",
    "href": "a_04_spot_doc.html#replicability",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.10 Replicability",
    "text": "D.10 Replicability\nSeed\n\ngen = spacefilling(2, seed=123)\nX0 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=345)\nX1 = gen.scipy_lhd(3)\nX2 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=123)\nX3 = gen.scipy_lhd(3)\nX0, X1, X2, X3\n\n(array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]),\n array([[0.78373509, 0.86811887],\n        [0.06692621, 0.6058029 ],\n        [0.41374778, 0.00525456]]),\n array([[0.121357  , 0.69043832],\n        [0.41906219, 0.32838498],\n        [0.86742658, 0.52910374]]),\n array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#surrogates",
    "href": "a_04_spot_doc.html#surrogates",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.11 Surrogates",
    "text": "D.11 Surrogates\n\nD.11.1 A Simple Predictor\nThe code below shows how to use a simple model for prediction. Assume that only two (very costly) measurements are available:\n\nf(0) = 0.5\nf(2) = 2.5\n\nWe are interested in the value at \\(x_0 = 1\\), i.e., \\(f(x_0 = 1)\\), but cannot run an additional, third experiment.\n\nfrom sklearn import linear_model\nX = np.array([[0], [2]])\ny = np.array([0.5, 2.5])\nS_lm = linear_model.LinearRegression()\nS_lm = S_lm.fit(X, y)\nX0 = np.array([[1]])\ny0 = S_lm.predict(X0)\nprint(y0)\n\n[1.5]\n\n\nCentral Idea: Evaluation of the surrogate model S_lm is much cheaper (or / and much faster) than running the real-world experiment \\(f\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#demotest-objective-function-fails",
    "href": "a_04_spot_doc.html#demotest-objective-function-fails",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.12 Demo/Test: Objective Function Fails",
    "text": "D.12 Demo/Test: Objective Function Fails\nSPOT expects np.nan values from failed objective function values. These are handled. Note: SPOT’s counter considers only successful executions of the objective function.\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nimport numpy as np\nfrom math import inf\n# number of initial points:\nni = 20\n# number of points\nn = 30\n\nfun = analytical().fun_random_error\nfun_control=fun_control_init(\n    lower = np.array([-1]),\n    upper= np.array([1]),\n    fun_evals = n,\n    show_progress=False)\ndesign_control=design_control_init(init_size=ni)\n\nspot_1 = spot.Spot(fun=fun,\n                     fun_control=fun_control,\n                     design_control=design_control)\nspot_1.run()\n# To check whether the run was successfully completed,\n# we compare the number of evaluated points to the specified\n# number of points.\nassert spot_1.y.shape[0] == n\n\n[        nan         nan -0.02203599 -0.21843718  0.78240941         nan\n -0.3923345   0.67234256  0.31802454 -0.68898927 -0.75129705  0.97550354\n  0.41757584         nan  0.82585329         nan -0.49274073         nan\n -0.17991251  0.1481835 ]\n[-1.]\n[nan]\n[-0.14624037]\n[0.166475]\n[nan]\n[-0.3352401]\n[-0.47259301]\n[0.95541987]\n[0.17335968]\n[-0.58552368]\n[-0.20126111]\n[-0.60100809]\n[-0.97897336]\n[-0.2748985]\n[0.8359486]\n[0.99035591]\n[0.01641232]\n[0.5629346]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf\nMersmann, eds. 2022. Hyperparameter Tuning for\nMachine and Deep Learning with R - A Practical Guide.\nSpringer.\n\n\nBartz-Beielstein, Thomas. 2023. “PyTorch\nHyperparameter Tuning with SPOT: Comparison with Ray\nTuner and Default Hyperparameters on\nCIFAR10.” https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.\n\n\nForrester, Alexander, András Sóbester, and Andy Keane. 2008. Engineering Design via Surrogate Modelling.\nWiley.\n\n\nSantner, T J, B J Williams, and W I Notz. 2003. The Design and Analysis of Computer\nExperiments. Berlin, Heidelberg, New York: Springer.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html",
    "href": "023_spot_river_gui.html",
    "title": "17  The spotriver GUI",
    "section": "",
    "text": "17.1 Starting the GUI\nThe GUI can be started by executing the spotRiverGUI.py file in the spotRiver/gui directory.\nAfter the GUI window has opened, the user can select the task:\nDepending on the task, the user can select the data set, the preprocessing model, the core model, and the evaluation function.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#hyperparameter-tuning",
    "href": "023_spot_river_gui.html#hyperparameter-tuning",
    "title": "17  The spotriver GUI",
    "section": "17.2 Hyperparameter Tuning",
    "text": "17.2 Hyperparameter Tuning\nCalls run_spot_river_experiment from spotRiver.tuner.run.py with the following parameters:\n\nMAX_TIME,\nINIT_SIZE\nPREFIX\nhorizon\nn_total\nperc_train\noml_grace_period\ndata_set\nprepmodel\ncoremodel\n\n\n17.2.1 The run_spot_river_experiment Method\nrun_spot_river_experiment calls the tuner spot after processing the following steps:\n\nGenerate an experiment name.\nInitialize the fun_control dictionary.\nSelect the data set based on the data_set parameter and generate a data frame.\nSplits the data into training and test sets.\nSets the oml_grace_period parameter.\nSelect the preprocessing model based on the prepmodel parameter.\nSets the weights for the evaluation function and the weight coeffient.\nLoads the coremodel based on the coremodel parameter with hyperparameters set to the values specified in the RiverHyperDict dictionary.\nDetermines the default hyperparameters.\nSelects the evaluation function: HyperRiver.fun_oml_horizon.\nDetermines hyperparameter types, names, lower and upper bounds for the spot tuner.\nStarts tensorboard as a background process.\nStarts the spot tuner.\n\nWhen the tuner is finished, the following steps are performed:\n\nThe tensorboard process is terminated.\nThe spot_tuner object and the fun_control dictionary are returned.\n\nAfter the tuner is finished, the following information is available:\n\n\n17.2.2 Binary Classification",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#analysis",
    "href": "023_spot_river_gui.html#analysis",
    "title": "17  The spotriver GUI",
    "section": "17.6 Analysis",
    "text": "17.6 Analysis",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "022_spot_hpt_river.html",
    "href": "022_spot_hpt_river.html",
    "title": "16  HPT: River",
    "section": "",
    "text": "16.1 Introduction to River",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>HPT: River</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#internal-methods",
    "href": "023_spot_river_gui.html#internal-methods",
    "title": "17  The spotriver GUI",
    "section": "17.7 Internal Methods",
    "text": "17.7 Internal Methods\nThe spotriver GUI uses the following internal methods:\n\nrun_spot_river_experiment\nfun_oml_horizon\nevaluate_model\neval_oml_horizon\n\n\n17.7.1 The run_spot_river_experiment Method\nrun_spot_river_experiment calls the tuner spot after processing the following steps:\n\nGenerate an experiment name.\nInitialize the fun_control dictionary.\nSelect the data set based on the data_set parameter and generate a data frame.\nSplits the data into training and test sets.\nSets the oml_grace_period parameter.\nSelect the preprocessing model based on the prepmodel parameter.\nSets the weights for the evaluation function and the weight coeffient.\nLoads the coremodel based on the coremodel parameter with hyperparameters set to the values specified in the RiverHyperDict dictionary.\nDetermines the default hyperparameters.\nSelects the evaluation function: HyperRiver.fun_oml_horizon.\nDetermines hyperparameter types, names, lower and upper bounds for the spot tuner.\nStarts tensorboard as a background process.\nStarts the spot tuner.\n\nWhen the tuner is finished, the following steps are performed:\n\nThe tensorboard process is terminated.\nThe spot_tuner object and the fun_control dictionary are returned.\n\nAfter the tuner is finished, the following information is available:\nThe run_spot_river_experiment method is located in spotRiver.tuner.run.py and is called by the GUI. It calls the fun_oml_horizon evaluation function and the spot tuner.\n\n\n17.7.2 The fun_oml_horizon Method\nThe fun_oml_horizon method is located in spotRiver.hyperriver.py file. It calls the evaluate_model method, which in turn calls the eval_oml_horizon method from the spotRiver.evaluation.eval_bml.py file.\n\n\n17.7.3 The evaluate_model Method\n\n\n17.7.4 The eval_oml_horizon Method",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#starting-the-gui",
    "href": "023_spot_river_gui.html#starting-the-gui",
    "title": "17  The spotriver GUI",
    "section": "",
    "text": "&gt;&gt; python spotRiverGUI.py\n\n\n\nspotriver GUI\n\n\n\n\nBinary Classification\nRegression",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#binary-classification",
    "href": "023_spot_river_gui.html#binary-classification",
    "title": "17  The spotriver GUI",
    "section": "17.2 Binary Classification",
    "text": "17.2 Binary Classification\nIf the Binary Classification task is selected, the user can select pre-specified data sets from River the Data drop-down menu.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#regression",
    "href": "023_spot_river_gui.html#regression",
    "title": "17  The spotriver GUI",
    "section": "17.3 Regression",
    "text": "17.3 Regression",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#starting-a-new-experiment",
    "href": "023_spot_river_gui.html#starting-a-new-experiment",
    "title": "17  The spotriver GUI",
    "section": "17.4 Starting a New Experiment",
    "text": "17.4 Starting a New Experiment\nAn experiment can be started by clicking on the Run Experiment button. The GUI calls run_spot_river_experiment from spotRiver.tuner.run.py. Output will be shown in the shell window from which the GUI was started.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#starting-tensorboard",
    "href": "023_spot_river_gui.html#starting-tensorboard",
    "title": "17  The spotriver GUI",
    "section": "17.3 Starting Tensorboard",
    "text": "17.3 Starting Tensorboard\nTensorboard is automatically started when an experiment is started. The tensorboard process can be observed in a browser by opening the http://localhost:6006 page.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#stopping-tensorboard",
    "href": "023_spot_river_gui.html#stopping-tensorboard",
    "title": "17  The spotriver GUI",
    "section": "17.4 Stopping Tensorboard",
    "text": "17.4 Stopping Tensorboard\nspotPython.utils.tensorboard provides the methods start_tensorboard and stop_tensorboard to start and stop tensorboard as a background process. These will be used in future versions of the GUI to start and stop tensorboard. Currently, only the start_tensorboard method is used to start tensorboard as a background process.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  },
  {
    "objectID": "023_spot_river_gui.html#starting-and-stopping-tensorboard",
    "href": "023_spot_river_gui.html#starting-and-stopping-tensorboard",
    "title": "17  The spotriver GUI",
    "section": "17.5 Starting and Stopping Tensorboard",
    "text": "17.5 Starting and Stopping Tensorboard\nTensorboard is automatically started when an experiment is started. The tensorboard process can be observed in a browser by opening the http://localhost:6006 page.\nspotPython.utils.tensorboard provides the methods start_tensorboard and stop_tensorboard to start and stop tensorboard as a background process. These will be used in future versions of the GUI to start and stop tensorboard. Currently, only the start_tensorboard method is used to start tensorboard as a background process.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The spotriver GUI</span>"
    ]
  }
]