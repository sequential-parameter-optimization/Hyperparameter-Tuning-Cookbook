[
  {
    "objectID": "005_num_rsm.html",
    "href": "005_num_rsm.html",
    "title": "5  Introduction: Numerical Methods",
    "section": "",
    "text": "5.1 Response Surface Methods: What is RSM?\nThis part deals with numerical implementations of optimization methods. The goal is to understand the implementation of optimization methods and to solve real-world problems numerically and efficiently. We will focus on the implementation of surrogate models, because they are the most efficient way to solve real-world problems.\nStarting point is the well-established response surface methodology. It will be extended to the design and analysis of computer experiments (DACE). The DACE methodology is a modern extension of the response surface methodology. It is based on the use of surrogate models, which are used to replace the real-world problem with a simpler problem. The simpler problem is then solved numerically. The solution of the simpler problem is then used to solve the real-world problem.\nResponse Surface Methods (RSM) refer to a collection of statistical and mathematical tools that are valuable for developing, improving, and optimizing processes. The overarching theme of RSM involves studying how input variables that control a product or process can potentially influence a response that measures performance or quality characteristics.\nThe advantages of RSM include a rich literature, well-established methods often used in manufacturing, the importance of careful experimental design combined with a well-understood model, and the potential to add significant value to scientific inquiry, process refinement, optimization, and more. However, there are also drawbacks to RSM, such as the use of simple and crude surrogates, the hands-on nature of the methods, and the limitation of local methods.\nRSM is related to various fields, including Design of Experiments (DoE), quality management, reliability, and productivity. Its applications are widespread in industry and manufacturing, focusing on designing, developing, and formulating new products and improving existing ones, as well as from laboratory research. RSM is commonly applied in domains such as materials science, manufacturing, applied chemistry, climate science, and many others.\nAn example of RSM involves studying the relationship between a response variable, such as yield (\\(y\\)) in a chemical process, and two process variables: reaction time (\\(\\xi_1\\)) and reaction temperature (\\(\\xi_2\\)). The provided code illustrates this scenario, following a variation of the so-called “banana function.”\nIn the context of visualization, RSM offers the choice between 3D plots and contour plots. In a 3D plot, the independent variables \\(\\xi_1\\) and \\(\\xi_2\\) are represented, with \\(y\\) as the dependent variable.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef fun_rosen(x1, x2):\n    b = 10\n    return (x1-1)**2 + b*(x2-x1**2)**2\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nx = np.arange(-2.0, 2.0, 0.05)\ny = np.arange(-1.0, 3.0, 0.05)\nX, Y = np.meshgrid(x, y)\nzs = np.array(fun_rosen(np.ravel(X), np.ravel(Y)))\nZ = zs.reshape(X.shape)\n\nax.plot_surface(X, Y, Z)\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('Y')\n\nplt.show()\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-1.0, 3.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_rosen(X1, X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y , 50)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title(\"Rosenbrock's Banana Function\")\n\nText(0.5, 1.0, \"Rosenbrock's Banana Function\")",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#sec-rsm-intro",
    "href": "005_num_rsm.html#sec-rsm-intro",
    "title": "5  Introduction: Numerical Methods",
    "section": "",
    "text": "contour plot example:\n\n\\(x_1\\) and \\(x_2\\) are the independent variables\n\\(y\\) is the dependent variable\n\n\n\n\nVisual inspection: yield is optimized near \\((\\xi_1. \\xi_2)\\)\n\n\n5.1.1 Visualization: Problems in Practice\n\nTrue response surface is unknown in practice\nWhen yield evaluation is not as simple as a toy banana function, but a process requiring care to monitor, reconfigure and run, it’s far too expensive to observe over a dense grid\nAnd, measuring yield may be a noisy/inexact process\nThat’s where stats (RSM) comes in\n\n\n\n5.1.2 RSM: Strategies\n\nRSMs consist of experimental strategies for\nexploring the space of the process (i.e., independent/input) variables (above \\(\\xi_1\\) and \\(\\xi2)\\)\nempirical statistical modeling targeted toward development of an appropriate approximating relationship between the response (yield) and process variables local to a study region of interest\noptimization methods for sequential refinement in search of the levels or values of process variables that produce desirable responses (e.g., that maximize yield or explain variation)\nRSM used for fitting an Empirical Model\nTrue response surface driven by an unknown physical mechanism\nObservations corrupted by noise\nHelpful: fit an empirical model to output collected under different process configurations\nConsider response \\(Y\\) that depends on controllable input variables \\(\\xi_1, \\xi_2, \\ldots, \\xi_m\\)\nRSM: Equations of the Empirical Model\n\n\\(Y=f(\\xi_1, \\xi_2, \\ldots, \\xi_m) + \\epsilon\\)\n\\(\\mathbb{E}\\{Y\\} = \\eta = f(\\xi1_1, \\xi_2, \\ldots, \\xi_m)\\)\n\\(\\epsilon\\) is treated as zero mean idiosyncratic noise possibly representing\n\ninherent variation, or\nthe effect of other systems or\nvariables not under our purview at this time\n\n\n\n\n\n5.1.3 RSM: Noise in the Empirical Model\n\nTypical simplifying assumption: \\(\\epsilon \\sim N(0,\\sigma^2)\\)\nWe seek estimates for \\(f\\) and \\(\\sigma^2\\) from noisy observations \\(Y\\) at inputs \\(\\xi\\)\n\n\n\n5.1.4 RSM: Natural and Coded Variables\n\nInputs \\(\\xi_1, \\xi_2, \\ldots, \\xi_m\\) called natural variables:\n\nexpressed in natural units of measurement, e.g., degrees Celsius, pounds per square inch (psi), etc.\n\nTransformed to coded variables \\(x_1, x_2, \\ldots, x_m\\):\n\nto mitigate hassles and confusion that can arise when working with a multitude of scales of measurement\n\nTypical Transformations offering dimensionless inputs \\(x_1, x_2, \\ldots, x_m\\)\n\nin the unit cube, or\nscaled to have a mean of zero and standard deviation of one, are common choices.\n\nEmpirical model becomes \\(\\eta = f(x_1, x_2, \\ldots, x_m)\\)\n\n\n\n5.1.5 RSM Low-order Polynomials\n\nLow-order polynomial make the following simplifying Assumptions\n\nLearning about \\(f\\) is lots easier if we make some simplifying approximations\nAppealing to Taylor’s theorem, a low-order polynomial in a small, localized region of the input (\\(x\\)) space is one way forward\nClassical RSM:\n\ndisciplined application of local analysis and\nsequential refinement of locality through conservative extrapolation\n\nInherently a hands-on process",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#first-order-models-main-effects-model",
    "href": "005_num_rsm.html#first-order-models-main-effects-model",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.2 First-Order Models (Main Effects Model)",
    "text": "5.2 First-Order Models (Main Effects Model)\n\nFirst-order model (sometimes called main effects model) useful in parts of the input space where it’s believed that there’s little curvature in \\(f\\): \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\]\nFor example: \\[\\eta = 50 + 8 x_1 + 3x_2\\]\nIn practice, such a surface would be obtained by fitting a model to the outcome of a designed experiment\nFirst-Order Model in python Evaluated on a Grid\nEvaluate model on a grid in a double-unit square centered at the origin\nCoded units are chosen arbitrarily, although one can imagine deploying this approximating function nearby \\(x^{(0)} = (0,0)\\)\n\n\ndef fun_1(x1,x2):\n    return 50 + 8*x1 + 3*x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-1.0, 1.0, delta)\nx2 = np.arange(-1.0, 1.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_1(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('First Order Model: $50 + 8x_1 + 3x_2$')\n\nText(0.5, 1.0, 'First Order Model: $50 + 8x_1 + 3x_2$')\n\n\n\n\n\n\n\n\n\n\n5.2.1 First-Order Model Properties\n\nFirst-order model in 2d traces out a plane in \\(y \\times (x_1, x_2)\\) space\nOnly be appropriate for the most trivial of response surfaces, even when applied in a highly localized part of the input space\nAdding curvature is key to most applications:\n\nFirst-order model with interactions induces limited degree of curvature via different rates of change of \\(y\\) as \\(x_1\\) is varied for fixed \\(x_2\\), and vice versa: \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_{12} \\]\n\nFor example \\(\\eta = 50+8x_1+3x_2-4x_1x_2\\)\n\n\n\n5.2.2 First-order Model with Interactions in python\n\nCode below facilitates evaluations for pairs \\((x_1, x_2)\\)\nResponses may be observed over a mesh in the same double-unit square\n\n\ndef fun_11(x1,x2):\n    return 50 + 8 * x1 + 3 * x2 - 4 * x1 * x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_11(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('First Order Model with Interactions')\n\nText(0.5, 1.0, 'First Order Model with Interactions')\n\n\n\n\n\n\n\n\n\n\n\n5.2.3 Observations: First-Order Model with Interactions\n\nMean response \\(\\eta\\) is increasing marginally in both \\(x_1\\) and \\(x_2\\), or conditional on a fixed value of the other until \\(x_1\\) is 0.75\nRate of increase slows as both coordinates grow simultaneously since the coefficient in front of the interaction term \\(x_1 x_2\\) is negative\nCompared to the first-order model (without interactions): surface is far more useful locally\nLeast squares regressions often flag up significant interactions when fit to data collected on a design far from local optima",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#second-order-models",
    "href": "005_num_rsm.html#second-order-models",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.3 Second-Order Models",
    "text": "5.3 Second-Order Models\n\nSecond-order model may be appropriate near local optima where \\(f\\) would have substantial curvature: \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2  + \\beta_{11}x_1^2 + \\beta_{22}x^2 + \\beta_{12} x_1 x_2\\]\nFor example \\[\\eta = 50 + 8 x_1 + 3x_2 - 7x_1^2 - 3 x_2^2 - 4x_1x_2\\]\nImplementation of the Second-Order Model as fun_2().\n\n\ndef fun_2(x1,x2):\n    return 50 + 8 * x1 + 3 * x2 - 7 * x1**2 - 3*x2**2 - 4 * x1 * x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_2(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')\n\nText(0.5, 1.0, 'Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')\n\n\n\n\n\n\n\n\n\n\n5.3.1 Second-Order Models: Properties\n\nNot all second-order models would have a single stationary point (in RSM jargon called “a simple maximum”)\nIn “yield maximizing” setting we’re presuming response surface is concave down from a global viewpoint\n\neven though local dynamics may be more nuanced\n\nExact criteria depend upon the eigenvalues of a certain matrix built from those coefficients\nBox and Draper (2007) provide a diagram categorizing all of the kinds of second-order surfaces in RSM analysis, where finding local maxima is the goal\n\n\n\n5.3.2 Example: Stationary Ridge\n\nExample set of coefficients describing what’s called a stationary ridge is provided by the code below\n\n\ndef fun_ridge(x1, x2):\n    return 80 + 4*x1 + 8*x2 - 3*x1**2 - 12*x2**2 - 12*x1*x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_ridge(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Example of a stationary ridge')\n\nText(0.5, 1.0, 'Example of a stationary ridge')\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Observations: Second-Order Model (Ridge)\n\nRidge: a whole line of stationary points corresponding to maxima\nSituation means that the practitioner has some flexibility when it comes to optimizing:\n\ncan choose the precise setting of \\((x_1, x_2)\\) either arbitrarily or (more commonly) by consulting some tertiary criteria\n\n\n\n\n5.3.4 Example: Rising Ridge\n\nAn example of a rising ridge is implemented by the code below.\n\n\ndef fun_ridge_rise(x1, x2):\n     return 80 - 4*x1 + 12*x2 - 3*x1**2 - 12*x2**2 - 12*x1*x2\n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_ridge_rise(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Rising ridge: $\\\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')\n\nText(0.5, 1.0, 'Rising ridge: $\\\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')\n\n\n\n\n\n\n\n\n\n\n\n5.3.5 Summary: Rising Ridge\n\nThe stationary point is remote to the study region\nCcontinuum of (local) stationary points along any line going through the 2d space, excepting one that lies directly on the ridge\nAlthough estimated response will increase while moving along the axis of symmetry toward its stationary point, this situation indicates\n\neither a poor fit by the approximating second-order function, or\nthat the study region is not yet precisely in the vicinity of a local optima—often both.\n\n\n\n\n5.3.6 Falling Ridge\n\nInversion of a rising ridge is a falling ridge\nSimilarly indicating one is far from local optima, except that the response decreases as you move toward the stationary point\nFinding a falling ridge system can be a back-to-the-drawing-board affair.\n\n\n\n5.3.7 Saddle Point\n\nFinally, we can get what’s called a saddle or minimax system.\n\n\ndef fun_saddle(x1, x2):\n    return 80 + 4*x1 + 8*x2 - 2*x2**2 - 12*x1*x2 \n\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ndelta = 0.025\nx1 = np.arange(-2.0, 2.0, delta)\nx2 = np.arange(-2.0, 2.0, delta)\nX1, X2 = np.meshgrid(x1, x2)\nY = fun_saddle(X1,X2)\nfig, ax = plt.subplots()\nCS = ax.contour(X1, X2, Y, 20)\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Saddle Point: $\\\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')\n\nText(0.5, 1.0, 'Saddle Point: $\\\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')\n\n\n\n\n\n\n\n\n\n\n\n5.3.8 Interpretation: Saddle Points\n\nLikely further data collection, and/or outside expertise, is needed before determining a course of action in this situation\n\n\n\n5.3.9 Summary: Ridge Analysis\n\nFinding a simple maximum, or stationary ridge, represents ideals in the spectrum of second-order approximating functions\nBut getting there can be a bit of a slog\nUsing models fitted from data means uncertainty due to noise, and therefore uncertainty in the type of fitted second-order model\nA ridge analysis attempts to offer a principled approach to navigating uncertainties when one is seeking local maxima\nThe two-dimensional setting exemplified above is convenient for visualization, but rare in practice\nComplications compound when studying the effect of more than two process variables",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#general-rsm-models",
    "href": "005_num_rsm.html#general-rsm-models",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.4 General RSM Models",
    "text": "5.4 General RSM Models\n\nGeneral first-order model on \\(m\\) process variables \\(x_1, x_2, \\cdots, x_m\\) is \\[\\eta = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_m x_m\\]\nGeneral second-order model on \\(m\\) process variables \\[\n\\eta= \\beta_0 + \\sum_{j=1}^m + \\sum_{j=1}^m x_j^2 + \\sum_{j=2}^m \\sum_{k=1}^j \\beta_{kj}x_k x_j.\n\\]\n\n\n5.4.1 Ordinary Least Squares\n\nInference from data is carried out by ordinary least squares (OLS)\nFor an excellent review including R examples, see Sheather (2009)\nOLS and maximum likelihood estimators (MLEs) are in the typical Gaussian linear modeling setup basically equivalent",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#general-linear-regression",
    "href": "005_num_rsm.html#general-linear-regression",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.5 General Linear Regression",
    "text": "5.5 General Linear Regression\nWe are considering a model, which can be written in the form\n\\[\nY = X \\beta + \\epsilon,\n\\] where \\(Y\\) is an \\((n \\times 1)\\) vector of observations (responses), \\(X\\) is an \\((n \\times p)\\) matrix of known form, \\(\\beta\\) is a \\((1 \\times p)\\) vector of unknown parameters, and \\(\\epsilon\\) is an \\((n \\times 1)\\) vector of errors. Furthermore, \\(E(\\epsilon) = 0\\), \\(Var(\\epsilon) = \\sigma^2 I\\) and the \\(\\epsilon_i\\) are uncorrelated.\nUsing the normal equations \\[\n(X'X)b = X'Y,\n\\]\nthe solution is given by\n\\[\nb = (X'X)^{-1}X'Y.\n\\]\n\nExample 5.1 (Linear Regression)  \n\nimport numpy as np\nn = 8\nX = np.linspace(0, 2*np.pi, n, endpoint=False).reshape(-1,1)\nprint(np.round(X, 2))\ny = np.sin(X)\nprint(np.round(y, 2))\n# fit an OLS model to the data, predict the response based on the 1ßß x values\nm = 100\nx = np.linspace(0, 2*np.pi, m, endpoint=False).reshape(-1,1)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\ny_pred = model.predict(x)\n# visualize the data and the fitted model\nimport matplotlib.pyplot as plt\nplt.scatter(X, y, color='black')\nplt.plot(x, y_pred, color='blue', linewidth=1)\n# add the ground truth (sine function) in orange\nplt.plot(x, np.sin(x), color='orange', linewidth=1)\nplt.show()\n\n[[0.  ]\n [0.79]\n [1.57]\n [2.36]\n [3.14]\n [3.93]\n [4.71]\n [5.5 ]]\n[[ 0.  ]\n [ 0.71]\n [ 1.  ]\n [ 0.71]\n [ 0.  ]\n [-0.71]\n [-1.  ]\n [-0.71]]",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#designs",
    "href": "005_num_rsm.html#designs",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.6 Designs",
    "text": "5.6 Designs\n\nImportant: Organize the data collection phase of a response surface study carefully\nDesign: choice of \\(x\\)’s where we plan to observe \\(y\\)’s, for the purpose of approximating \\(f\\)\nAnalyses and designs need to be carefully matched\nWhen using a first-order model, some designs are preferred over others\nWhen using a second-order model to capture curvature, a different sort of design is appropriate\nDesign choices often contain features enabling modeling assumptions to be challenged\n\ne.g., to check if initial impressions are supported by the data ultimately collected\n\n\n\n5.6.1 Different Designs\n\nScreening desings: determine which variables matter so that subsequent experiments may be smaller and/or more focused\nThen there are designs tailored to the form of model (first- or second-order, say) in the screened variables\nAnd then there are more designs still",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#rsm-experimentation",
    "href": "005_num_rsm.html#rsm-experimentation",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.7 RSM Experimentation",
    "text": "5.7 RSM Experimentation\n\n5.7.1 First Step\n\nRSM-based experimentation begins with a first-order model, possibly with interactions\nPresumption: current process operating far from optimal conditions\nCollect data and apply method of steepest ascent (gradient) on fitted surfaces to move to the optimum\n\n\n\n5.7.2 Second Step\n\nEventually, if all goes well after several such carefully iterated refinements, second-order models are used on appropriate designs in order to zero-in on ideal operating conditions\nCareful analysis of the fitted surface:\n\nRidge analysis with further refinement using gradients of, and\nstandard errors associated with, the fitted surfaces, and so on\n\n\n\n\n5.7.3 Third Step\n\nOnce the practitioner is satisfied with the full arc of\n\ndesign(s),\nfit(s), and\ndecision(s):\n\nA small experiment called confirmation test may be performed to check if the predicted optimal settings are realizable in practice",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#rsm-review-and-general-considerations",
    "href": "005_num_rsm.html#rsm-review-and-general-considerations",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.8 RSM: Review and General Considerations",
    "text": "5.8 RSM: Review and General Considerations\n\nFirst Glimpse, RSM seems sensible, and pretty straightforward as quantitative statistics-based analysis goes\nBut: RSM can get complicated, especially when input dimensions are not very low\nDesign considerations are particularly nuanced, since the goal is to obtain reliable estimates of main effects, interaction, and curvature while minimizing sampling effort/expense\nRSM Downside: Inefficiency\n\nDespite intuitive appeal, several RSM downsides become apparent upon reflection\nProblems in practice\nStepwise nature of sequential decision making is inefficient:\n\nNot obvious how to re-use or update analysis from earlier phases, or couple with data from other sources/related experiments\n\n\nRSM Downside: Locality\n\nIn addition to being local in experiment-time (stepwise approach), it’s local in experiment-space\nBalance between\n\nexploration (maybe we’re barking up the wrong tree) and\nexploitation (let’s make things a little better) is modest at best\n\n\nRSM Downside: Expert Knowledge\n\nInterjection of expert knowledge is limited to hunches about relevant variables (i.e., the screening phase), where to initialize search, how to design the experiments\nYet at the same time classical RSMs rely heavily on constant examination throughout stages of modeling and design and on the instincts of seasoned practitioners\n\nRSM Downside: Replicability\n\nParallel analyses, conducted according to the same best intentions, rarely lead to the same designs, model fits and so on\nSometimes that means they lead to different conclusions, which can be cause for concern\n\n\n\n5.8.1 Historical Considerations about RSM\n\nIn spite of those criticisms, however, there was historically little impetus to revise the status quo\nClassical RSM was comfortable in its skin, consistently led to improvements or compelling evidence that none can reasonably be expected\nBut then in the late 20th century came an explosive expansion in computational capability, and with it a means of addressing many of those downsides\n\n\n\n5.8.2 Status Quo\n\nNowadays, field experiments and statistical models, designs and optimizations are coupled with with mathematical models\nSimple equations are not regarded as sufficient to describe real-world systems anymore\nPhysicists figured that out fifty years ago; industrial engineers followed, biologists, social scientists, climate scientists and weather forecasters, etc.\nSystems of equations are required, solved over meshes (e.g., finite elements), or stochastically interacting agents\nGoals for those simulation experiments are as diverse as their underlying dynamics\nOptimization of systems is common, e.g., to identify worst-case scenarios\n\n\n\n5.8.3 The Role of Statistics\n\nSolving systems of equations, or interacting agents, requires computing\nStatistics involved at various stages:\n\nchoosing the mathematical model\nsolving by stochastic simulation (Monte Carlo)\ndesigning the computer experiment\nsmoothing over idiosyncrasies or noise\nfinding optimal conditions, or\ncalibrating mathematical/computer models to data from field experiments\n\n\n\n\n5.8.4 New RSM is needed: DACE\n\nClassical RSMs are not well-suited to any of those tasks, because\n\nthey lack the fidelity required to model these data\ntheir intended application is too local\nthey’re also too hands-on.\n\nOnce computers are involved, a natural inclination is to automate—to remove humans from the loop and set the computer running on the analysis in order to maximize computing throughput, or minimize idle time\nDesign and Analysis of Computer Experiments as a modern extension of RSM\nExperimentation is changing due to advances in machine learning\nGaussian process (GP) regression is the canonical surrogate model\nOrigins in geostatistics (gold mining)\nWide applicability in contexts where prediction is king\nMachine learners exposed GPs as powerful predictors for all sorts of tasks:\nfrom regression to classification,\nactive learning/sequential design,\nreinforcement learning and optimization,\nlatent variable modeling, and so on",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#exercises",
    "href": "005_num_rsm.html#exercises",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\nGenerate 3d Plots for the Contour Plots in this notebook.\nWrite a plot_3d function, that takes the objective function fun as an argument.\n\n\nIt should provide the following interface: plot_3d(fun).\n\n\nWrite a plot_contour function, that takes the objective function fun as an argument:\n\n\nIt should provide the following interface: plot_contour(fun).\n\n\nConsider further arguments that might be useful for both function, e.g., ranges, size, etc.",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  },
  {
    "objectID": "005_num_rsm.html#jupyter-notebook",
    "href": "005_num_rsm.html#jupyter-notebook",
    "title": "5  Introduction: Numerical Methods",
    "section": "5.10 Jupyter Notebook",
    "text": "5.10 Jupyter Notebook\n\n\n\n\n\n\nNote\n\n\n\n\nThe Jupyter-Notebook of this lecture is available on GitHub in the Hyperparameter-Tuning-Cookbook Repository",
    "crumbs": [
      "Numerical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction: Numerical Methods</span>"
    ]
  }
]