[
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html",
    "href": "024_spot_hpt_river_friedman_hatr.html",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "22.1 The Friedman Drift Data Set\nWe will use the Friedman synthetic dataset with concept drifts, which is described in detail in Section E.1. The following parameters are used to generate and handle the data set:\nWe will use spotRiver’s convert_to_df function [SOURCE] to convert the river data set to a pandas data frame. Then we add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column of the dataframe.\nThis data generation is independently repeated for the training and test data sets, because the data sets are generated with concept drifts and the usual train-test split would not work.\nfrom river.datasets import synth\nimport pandas as pd\nimport numpy as np\nfrom spotRiver.utils.data_conversion import convert_to_df\n\nseed = 123\nshuffle = True\nn_train = 6_000\nn_test = 4_000\nn_samples = n_train + n_test\ntarget_column = \"y\"\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=(n_train/4, n_train/2),\n   seed=123\n)\n\ntrain = convert_to_df(dataset, n_total=n_train)\ntrain.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\ntrain.describe()\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\ny\n\n\n\n\ncount\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n6000.000000\n\n\nmean\n0.492992\n0.494934\n0.507111\n0.499008\n0.498263\n0.498717\n0.504220\n0.500993\n0.502120\n0.500886\n14.336770\n\n\nstd\n0.291592\n0.292346\n0.289594\n0.284814\n0.292419\n0.287102\n0.288827\n0.290182\n0.288721\n0.292779\n4.967536\n\n\nmin\n0.000183\n0.000128\n0.000047\n0.000232\n0.000076\n0.000007\n0.000023\n0.000217\n0.000072\n0.000088\n0.182076\n\n\n25%\n0.233542\n0.239627\n0.260951\n0.254245\n0.237365\n0.256443\n0.255483\n0.248906\n0.257864\n0.241014\n10.803346\n\n\n50%\n0.488280\n0.492370\n0.510660\n0.499166\n0.500711\n0.498509\n0.503142\n0.501054\n0.497220\n0.501474\n14.305293\n\n\n75%\n0.749727\n0.749006\n0.758401\n0.741691\n0.755608\n0.747869\n0.754169\n0.753107\n0.750948\n0.759222\n17.872150\n\n\nmax\n0.999800\n0.999835\n0.999884\n0.999880\n0.999868\n0.999847\n0.999904\n0.999913\n0.999918\n0.999851\n28.960869\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=(n_test/4, n_test/2),\n   seed=123\n)\ntest = convert_to_df(dataset, n_total=n_test)\ntest.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n\ntest.describe()\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\ny\n\n\n\n\ncount\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n4000.000000\n\n\nmean\n0.491759\n0.497671\n0.504094\n0.501948\n0.501125\n0.497826\n0.505303\n0.500875\n0.501379\n0.501893\n14.350881\n\n\nstd\n0.291205\n0.292693\n0.289846\n0.286430\n0.293268\n0.287358\n0.289319\n0.290001\n0.290804\n0.292795\n4.954976\n\n\nmin\n0.000316\n0.000128\n0.000047\n0.000232\n0.000076\n0.000007\n0.000023\n0.000327\n0.000109\n0.000848\n0.182076\n\n\n25%\n0.233376\n0.243008\n0.256440\n0.253511\n0.238230\n0.254148\n0.257751\n0.251306\n0.255416\n0.241123\n10.895122\n\n\n50%\n0.488008\n0.498058\n0.507394\n0.503101\n0.507231\n0.497874\n0.503893\n0.501620\n0.492703\n0.504333\n14.242195\n\n\n75%\n0.744795\n0.749106\n0.758884\n0.745345\n0.758035\n0.747054\n0.754285\n0.752760\n0.750897\n0.763104\n17.846398\n\n\nmax\n0.999800\n0.999835\n0.999884\n0.999880\n0.999868\n0.999847\n0.999904\n0.999913\n0.999918\n0.999851\n28.357853",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#the-friedman-drift-data-set",
    "href": "024_spot_hpt_river_friedman_hatr.html#the-friedman-drift-data-set",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "horizon: The prediction horizon in hours.\nn_samples: The number of samples in the data set.\np_1: The position of the first concept drift.\np_2: The position of the second concept drift.\nposition: The position of the concept drifts.\nn_train: The number of samples used for training.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#setup",
    "href": "024_spot_hpt_river_friedman_hatr.html#setup",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.2 Setup",
    "text": "22.2 Setup\n\n22.2.1 General Experiment Setup\nTo keep track of the different experiments, we use a PREFIX for the experiment name. The PREFIX is used to create a unique experiment name. The PREFIX is also used to create a unique TensorBoard folder, which is used to store the TensorBoard log files.\nspotpython allows the specification of two different types of stopping criteria: first, the number of function evaluations (fun_evals), and second, the maximum run time in seconds (max_time). Here, we will set the number of function evaluations to infinity and the maximum run time to one minute.\nFurthermore, we set the initial design size (init_size) to 10. The initial design is used to train the surrogate model. The surrogate model is used to predict the performance of the hyperparameter configurations. The initial design is also used to train the first model. Since the init_size referres to the experimental design, it is set in the design_control dictionary.\nmax_time is set to one minute for demonstration purposes and init_size is set to 10 for demonstration purposes. For real experiments, these values should be increased. Note, the total run time may exceed the specified max_time, because the initial design (here: init_size = INIT_SIZE as specified above) is always evaluated, even if this takes longer than max_time.\n\n\n\n\n\n\nSummary: General Experiment Setup\n\n\n\nThe following parameters are used:\n\nPREFIX = \"024\"\nfun_evals = inf\nmax_time = 1\n\n\n\n\n\n22.2.2 Data Setup\nWe use the StandardScaler [SOURCE] from river as the data-preprocessing model. The StandardScaler is used to standardize the data set, i.e., it has zero mean and unit variance.\nThe names of the training and test data sets are train and test, respectively. Both must use the same column names. The column names were set to x1 to x10 for the features and y for the target column during the data set generation in Section 22.1. Therefore, the target_column is set to y.\n\n\n\n\n\n\nSummary: Data Setup\n\n\n\nThe following parameters are used:\n\nprep_model_name = \"StandardScaler\"\ntest = test\ntrain = train\ntarget_column = target_column\n\n\n\n\n\n22.2.3 Evaluation Setup\nHere we use the mean_absolute_error [SOURCE] as the evaluation metric. Internally, this metric is passed to the objective (or loss) function fun_oml_horizon [SOURCE] and further to the evaluation function eval_oml_horizon [SOURCE].\nspotRiver also supports additional metrics. For example, the metric_river is used for the river based evaluation via eval_oml_iter_progressive [SOURCE]. The metric_river is implemented to simulate the behaviour of the “original” river metrics.\n\n\n\n\n\n\nSummary: Evaluation Setup\n\n\n\nThe following parameters are used:\n\nmetric_sklearn_name = \"mean_absolute_error\"\n\n\n\n\n\n22.2.4 River-Specific Setup\nIn the online-machine-learning (OML) setup, the model is trained on a fixed number of observations and then evaluated on a fixed number of observations. The horizon defines the number of observations that are used for the evaluation. Here, a horizon of 7*24 is used, which corresponds to one week of data.\nThe oml_grace_period defines the number of observations that are used for the initial training of the model. Tis value is relatively small, since the online-machine-learning is trained on the incoming data and the model is updated continuously. However, it needs a certain number of observations to start the training process. Therefoere, this short training period aka oml_grace_period is set to the horizon, i.e., the number of observations that are used for the evaluation. In this case, we use a horizon of 7*24.\nThe weight_coeff defines a multiplier for the results: results are multiplied by (step/n_steps)**weight_coeff, where n_steps is the total number of iterations. Results from the beginning have a lower weight than results from the end if weight_coeff &gt; 1. If weight_coeff == 0, all results have equal weight. Note, that the weight_coeff is only used internally for the tuner and does not affect the results that are used for the evaluation or comparisons.\nThe weights provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased. spotRiver stores information about the model’ s score (metric), memory, and time. The hyperparamter tuner requires a single objective. Therefore, a weighted sum of the metric, memory, and time is computed. The weights are defined in the weights array. The weights provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased.\n\n\n\n\n\n\nSummary: River-Specific Setup\n\n\n\nThe following parameters are used:\n\nhorizon = 7*24\noml_grace_period = 7*24\nweight_coeff = 1.0\nweights = np.array([1, 0.01, 0.01])\n\n\n\n\n\n22.2.5 Select Model (algorithm)\nBy using core_model_name = \"tree.HoeffdingAdaptiveTreeRegressor\", the river model class HoeffdingAdaptiveTreeRegressor [SOURCE] from the tree module is selected. For a given core_model_name, the corresponding hyperparameters are automatically loaded from the associated dictionary, which is stored as a JSON file [SOURCE]. The JSON file contains hyperparameter type information, names, and bounds. For river models, the hyperparameters are stored in the RiverHyperDict, see [SOURCE]\nAlternatively, you can load a local hyper_dict. Simply set river_hyper_dict.json as the filename. If filenameis set to None, which is the default, the hyper_dict [SOURCE] is loaded from the spotRiver package.\n\n\n\n\n\n\nSummary: Model Setup\n\n\n\nThe following parameters are used:\n\nfrom spotRiver.fun.hyperriver import HyperRiver\nfrom spotRiver.hyperdict.river_hyper_dict import RiverHyperDict\ncore_model_name = \"tree.HoeffdingAdaptiveTreeRegressor\"\nhyperdict = RiverHyperDict\n\n\n\n\n\n22.2.6 Objective Function Setup\nThe loss function (metric) values are passed to the objective function fun_oml_horizon, which compbines information about the required memory and time as described in Section 22.2.4.\n\n\n\n\n\n\nSummary: Objective Function Setup\n\n\n\nThe following parameters are used:\n\nfun = HyperRiver().fun_oml_horizon\n\n\n\n\n\n22.2.7 Setting up the Dictionaries\nAltogether, the fun_control, design_control, surrogate_control, and optimize_control dictionaries are initialized as follows:\n\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n\nfun = HyperRiver().fun_oml_horizon\n\nfun_control = fun_control_init(\n    PREFIX=\"024\",\n    fun_evals=inf,\n    max_time=1,\n\n    prep_model_name=\"StandardScaler\",\n    test=test,\n    train=train,\n    target_column=target_column,\n\n    metric_sklearn_name=\"mean_absolute_error\",\n    horizon=7*24,\n    oml_grace_period=7*24,\n    weight_coeff=1.0,\n    weights=np.array([1, 0.01, 0.01]),\n\n    core_model_name=\"tree.HoeffdingAdaptiveTreeRegressor\",\n    hyperdict=RiverHyperDict,\n\n    noise=True,\n    seed=123,\n   )\n\n\ndesign_control = design_control_init(\n    init_size=10,\n)\n\nsurrogate_control = surrogate_control_init(\n    noise=True,\n    n_theta=2,\n    min_Lambda=1e-3,\n    max_Lambda=10,\n)\n\noptimizer_control = optimizer_control_init()\n\nCreated spot_tensorboard_path: runs/spot_logs/024_p040025_2024-06-20_21-46-24 for SummaryWriter()\n\n\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n# levels = [\"LinearRegression\"]\n# modify_hyper_parameter_levels(fun_control, \"leaf_model\", levels)\n\nThe specified experimental design can be checked with the following command:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name                   | type   | default          |   lower |    upper | transform              |\n|------------------------|--------|------------------|---------|----------|------------------------|\n| grace_period           | int    | 200              |  10     | 1000     | None                   |\n| max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n| delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n| tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n| leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n| model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n| splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n| min_samples_split      | int    | 5                |   2     |   10     | None                   |\n| bootstrap_sampling     | factor | 0                |   0     |    1     | None                   |\n| drift_window_threshold | int    | 300              | 100     |  500     | None                   |\n| switch_significance    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| binary_split           | factor | 0                |   0     |    1     | None                   |\n| max_size               | float  | 500.0            | 100     | 1000     | None                   |\n| memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n| stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n| remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n| merit_preprune         | factor | 1                |   0     |    1     | None                   |\n\n\n\n\n22.2.8 Run the Spot Optimizer\nThe class Spot [SOURCE] is the hyperparameter tuning workhorse. It is initialized with the following parameters, which were specified above.\n\nfun: the objective function\nfun_control: the dictionary with the control parameters for the objective function\ndesign_control: the dictionary with the control parameters for the experimental design\nsurrogate_control: the dictionary with the control parameters for the surrogate model\noptimizer_control: the dictionary with the control parameters for the optimizer\n\nspotpython allows maximum flexibility in the definition of the hyperparameter tuning setup. Alternative surrogate models, optimizers, and experimental designs can be used. Thus, interfaces for the surrogate model, experimental design, and optimizer are provided. The default surrogate model is the kriging model, the default optimizer is the differential evolution, and default experimental design is the Latin hypercube design.\n\n\n\n\n\n\nSummary: Spot Setup\n\n\n\nThe following parameters are used for the Spot setup. These were specified above:\n\nfun = fun\nfun_control = fun_control\ndesign_control = design_control\nsurrogate_control = surrogate_control\noptimizer_control = optimizer_control\n\n\n\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(\n    fun=fun,\n    fun_control=fun_control,\n    design_control=design_control,\n    surrogate_control=surrogate_control,\n    optimizer_control=optimizer_control,\n)\nres = spot_tuner.run()\n\nspotPython tuning: 2.2479797473288734 [----------] 4.83% \nspotPython tuning: 2.2479797473288734 [#---------] 6.92% \nspotPython tuning: 2.2479797473288734 [#---------] 10.97% \nspotPython tuning: 2.2479797473288734 [#---------] 12.93% \nspotPython tuning: 2.2479797473288734 [##--------] 20.26% \nspotPython tuning: 2.2479797473288734 [##--------] 22.52% \nspotPython tuning: 2.2479797473288734 [##--------] 24.60% \nspotPython tuning: 2.2479797473288734 [###-------] 27.64% \nspotPython tuning: 2.2479797473288734 [###-------] 32.17% \nspotPython tuning: 2.2479797473288734 [####------] 35.57% \nspotPython tuning: 2.2479797473288734 [####------] 38.59% \nspotPython tuning: 2.2479797473288734 [#####-----] 53.59% \nspotPython tuning: 2.2479797473288734 [######----] 64.15% \nspotPython tuning: 2.2479797473288734 [#######---] 69.52% \nspotPython tuning: 2.2479797473288734 [########--] 82.54% \nspotPython tuning: 2.1705781758218436 [##########] 99.30% \nspotPython tuning: 2.1705781758218436 [##########] 100.00% Done...",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "href": "024_spot_hpt_river_friedman_hatr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.3 SelectSelect Model (algorithm) and core_model_hyper_dict",
    "text": "22.3 SelectSelect Model (algorithm) and core_model_hyper_dict\nspotPython hyperparameter tuning approach uses two components:\n\na model (class) and\nan associated hyperparameter dictionary.\n\nHere, the river model class HoeffdingAdaptiveTreeRegressor [SOURCE] is selected.\nThe corresponding hyperparameters are loaded from the associated dictionary, which is stored as a JSON file [SOURCE]. The JSON file contains hyperparameter type information, names, and bounds.\nThe method add_core_model_to_fun_control adds the model and the hyperparameter dictionary to the fun_control dictionary.\nAlternatively, you can load a local hyper_dict. Simply set river_hyper_dict.json as the filename. If filenameis set to None, which is the default, the hyper_dict [SOURCE] is loaded from the spotRiver package.\nWe use the StandardScaler [SOURCE] from river as the preprocessing model. The StandardScaler is used to standardize the data set, i.e., it has zero mean and unit variance.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#objective-function",
    "href": "024_spot_hpt_river_friedman_hatr.html#objective-function",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.4 Objective Function",
    "text": "22.4 Objective Function\n\n22.4.1 The Objective Function\nThe objective function fun_oml_horizon [SOURCE] is selected next.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#selection-of-the-objective-loss-function",
    "href": "024_spot_hpt_river_friedman_hatr.html#selection-of-the-objective-loss-function",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.5 Selection of the Objective (Loss) Function",
    "text": "22.5 Selection of the Objective (Loss) Function\nThe metric_sklearn is used for the sklearn based evaluation via eval_oml_horizon [SOURCE]. Here we use the mean_absolute_error [SOURCE] as the objective function.\n\n\n\n\n\n\nNote: Additional metrics\n\n\n\nspotRiver also supports additional metrics. For example, the metric_river is used for the river based evaluation via eval_oml_iter_progressive [SOURCE]. The metric_river is implemented to simulate the behaviour of the “original” river metrics.\n\n\nspotRiver provides information about the model’ s score (metric), memory, and time. The hyperparamter tuner requires a single objective. Therefore, a weighted sum of the metric, memory, and time is computed. The weights are defined in the weights array.\n\n\n\n\n\n\nNote: Weights\n\n\n\nThe weights provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased.\n\n\nThe oml_grace_period defines the number of observations that are used for the initial training of the model. The step defines the iteration number at which to yield results. This only takes into account the predictions, and not the training steps. The weight_coeff defines a multiplier for the results: results are multiplied by (step/n_steps)**weight_coeff, where n_steps is the total number of iterations. Results from the beginning have a lower weight than results from the end if weight_coeff &gt; 1. If weight_coeff == 0, all results have equal weight. Note, that the weight_coeff is only used internally for the tuner and does not affect the results that are used for the evaluation or comparisons.\n\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\nfrom spotRiver.fun.hyperriver import HyperRiver\nfrom spotRiver.hyperdict.river_hyper_dict import RiverHyperDict\n\n# experiment setup\nPREFIX = \"024\"\nfun_evals = inf\nmax_time = 1\ninit_size = 10\n\n# evaluation setup\nhorizon = 7*24\nmetric_sklearn_name = \"mean_absolute_error\"\noml_grace_period = horizon\nweight_coeff = 1.0\nweights = np.array([1, 0.01, 0.01])\nfun = HyperRiver().fun_oml_horizon\n\n# data preparation\nprep_model_name = \"StandardScaler\"\n\n# model setup\ncore_model_name = \"tree.HoeffdingAdaptiveTreeRegressor\"\n# core_model_name = \"tree.HoeffdingTreeRegressor\"\nhyperdict = RiverHyperDict\n\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    core_model_name=core_model_name,\n    fun_evals=fun_evals,\n    horizon=horizon,\n    hyperdict=hyperdict,\n    max_time=max_time,\n    metric_sklearn_name=metric_sklearn_name,\n    noise=True,\n    oml_grace_period=oml_grace_period,\n    prep_model_name=prep_model_name,\n    seed=seed,\n    target_column=target_column,\n    test=test,\n    train=train,\n    weight_coeff=weight_coeff,\n    weights=weights,)\n\n\ndesign_control = design_control_init(\n    init_size=init_size,\n)\n\nsurrogate_control = surrogate_control_init(\n    noise=True,\n    n_theta=2,\n    min_Lambda=1e-3,\n    max_Lambda=10,\n)\n\noptimizer_control = optimizer_control_init()\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_06_20_09_20_31\nCreated spot_tensorboard_path: runs/spot_logs/024_p040025_2024-06-20_09-20-31 for SummaryWriter()\n\n\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n# levels = [\"LinearRegression\"]\n# modify_hyper_parameter_levels(fun_control, \"leaf_model\", levels)\n\n\nfrom spotPython.utils.eda import gen_design_table\nfrom spotPython.spot import spot\nprint(gen_design_table(fun_control))\n\n| name                   | type   | default          |   lower |    upper | transform              |\n|------------------------|--------|------------------|---------|----------|------------------------|\n| grace_period           | int    | 200              |  10     | 1000     | None                   |\n| max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n| delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n| tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n| leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n| model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n| splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n| min_samples_split      | int    | 5                |   2     |   10     | None                   |\n| bootstrap_sampling     | factor | 0                |   0     |    1     | None                   |\n| drift_window_threshold | int    | 300              | 100     |  500     | None                   |\n| switch_significance    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| binary_split           | factor | 0                |   0     |    1     | None                   |\n| max_size               | float  | 500.0            | 100     | 1000     | None                   |\n| memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n| stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n| remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n| merit_preprune         | factor | 1                |   0     |    1     | None                   |\n\n\n\n22.5.1 Run the Spot Optimizer\nThe class Spot [SOURCE] is the hyperparameter tuning workhorse. It is initialized with the following parameters:\n\nfun: the objective function\nfun_control: the dictionary with the control parameters for the objective function\ndesign: the experimental design\ndesign_control: the dictionary with the control parameters for the experimental design\nsurrogate: the surrogate model\nsurrogate_control: the dictionary with the control parameters for the surrogate model\noptimizer: the optimizer\noptimizer_control: the dictionary with the control parameters for the optimizer\n\n\n\n\n\n\n\nNote: Total run time\n\n\n\nThe total run time may exceed the specified max_time, because the initial design (here: init_size = INIT_SIZE as specified above) is always evaluated, even if this takes longer than max_time.\n\n\n\nspot_tuner = spot.Spot(\n    fun=fun,\n    fun_control=fun_control,\n    design_control=design_control,\n    surrogate_control=surrogate_control,\n    optimizer_control=optimizer_control,\n)\nres = spot_tuner.run()\n\nspotPython tuning: 2.2515055392695573 [----------] 4.81% \nspotPython tuning: 2.2515055392695573 [#---------] 6.79% \nspotPython tuning: 2.2515055392695573 [#---------] 10.75% \nspotPython tuning: 2.2515055392695573 [#---------] 12.66% \nspotPython tuning: 2.2515055392695573 [##--------] 19.86% \nspotPython tuning: 2.2515055392695573 [##--------] 22.06% \nspotPython tuning: 2.2515055392695573 [##--------] 24.09% \nspotPython tuning: 2.2515055392695573 [###-------] 27.09% \nspotPython tuning: 2.2515055392695573 [###-------] 31.53% \nspotPython tuning: 2.2515055392695573 [###-------] 34.87% \nspotPython tuning: 2.2515055392695573 [####------] 37.89% \nspotPython tuning: 2.2515055392695573 [#####-----] 51.60% \nspotPython tuning: 2.2515055392695573 [######----] 62.06% \nspotPython tuning: 2.2515055392695573 [#######---] 67.36% \nspotPython tuning: 2.2515055392695573 [########--] 80.10% \nspotPython tuning: 2.1705886575489273 [##########] 96.05% \nspotPython tuning: 2.1705886575489273 [##########] 100.00% Done...",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#results",
    "href": "024_spot_hpt_river_friedman_hatr.html#results",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.3 Results",
    "text": "22.3 Results\n\n22.3.1 TensorBoard\nNow we can start TensorBoard in the background with the following command, where ./runs is the default directory for the TensorBoard log files:\ntensorboard --logdir=\"./runs\"\n:::{.callout-tip} #### Tip: TENSORBOARD_PATH The TensorBoard path can be printed with the following command:\n\nfrom spotPython.utils.init import get_tensorboard_path\nget_tensorboard_path(fun_control)\n\n'runs/'\n\n\n\nspot_tuner.print_results(print_screen=False)\n\n[['grace_period', 369.0],\n ['max_depth', 10.0],\n ['delta', 2.687369912822594e-07],\n ['tau', 0.03965702590085796],\n ['leaf_prediction', 1.0],\n ['leaf_model', 0.0],\n ['model_selector_decay', 0.912608961480265],\n ['splitter', 2.0],\n ['min_samples_split', 5.0],\n ['bootstrap_sampling', 0.0],\n ['drift_window_threshold', 256.0],\n ['switch_significance', 0.010066722690551406],\n ['binary_split', 0.0],\n ['max_size', 714.4837650420958],\n ['memory_estimate_period', 6.0],\n ['stop_mem_management', 0.0],\n ['remove_poor_attrs', 0.0],\n ['merit_preprune', 1.0]]\n\n\nThe tuned hyperparameters can be obtained as a dictionary with the following command:\n\nfrom spotPython.hyperparameters.values import get_tuned_hyperparameters\nget_tuned_hyperparameters(spot_tuner, fun_control)\n\n{'grace_period': 369.0,\n 'max_depth': 10.0,\n 'delta': 2.687369912822594e-07,\n 'tau': 0.03965702590085796,\n 'leaf_prediction': 'model',\n 'leaf_model': 'LinearRegression',\n 'model_selector_decay': 0.912608961480265,\n 'splitter': 'QOSplitter',\n 'min_samples_split': 5.0,\n 'bootstrap_sampling': 0,\n 'drift_window_threshold': 256.0,\n 'switch_significance': 0.010066722690551406,\n 'binary_split': 0,\n 'max_size': 714.4837650420958,\n 'memory_estimate_period': 6.0,\n 'stop_mem_management': 0,\n 'remove_poor_attrs': 0,\n 'merit_preprune': 1}\n\n\nThe results can be saved and reloaded with the following commands:\n\nfrom spotPython.utils.file import save_pickle, load_pickle\nfrom spotPython.utils.init import get_experiment_name\nexperiment_name = get_experiment_name(\"024\")\nSAVE_AND_LOAD = False\nif SAVE_AND_LOAD == True:\n    save_pickle(spot_tuner, experiment_name)\n    spot_tuner = load_pickle(experiment_name)\n\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The black points represent the performace values (score or metric) of hyperparameter configurations from the initial design, whereas the red points represents the hyperparameter configurations found by the surrogate model based optimization.\n\nspot_tuner.plot_progress(log_y=True, filename=None)\n\n\n\n\n\n\n\n\nResults can also be printed in tabular form.\n\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name                   | type   | default          |   lower |   upper | tuned                 | transform              |   importance | stars   |\n|------------------------|--------|------------------|---------|---------|-----------------------|------------------------|--------------|---------|\n| grace_period           | int    | 200              |    10.0 |  1000.0 | 369.0                 | None                   |        14.00 | *       |\n| max_depth              | int    | 20               |     2.0 |    20.0 | 10.0                  | transform_power_2_int  |         0.01 |         |\n| delta                  | float  | 1e-07            |   1e-08 |   1e-06 | 2.687369912822594e-07 | None                   |         0.01 |         |\n| tau                    | float  | 0.05             |    0.01 |     0.1 | 0.03965702590085796   | None                   |       100.00 | ***     |\n| leaf_prediction        | factor | mean             |     0.0 |     2.0 | model                 | None                   |         3.51 | *       |\n| leaf_model             | factor | LinearRegression |     0.0 |     2.0 | LinearRegression      | None                   |        85.31 | **      |\n| model_selector_decay   | float  | 0.95             |     0.9 |    0.99 | 0.912608961480265     | None                   |         0.87 | .       |\n| splitter               | factor | EBSTSplitter     |     0.0 |     2.0 | QOSplitter            | None                   |         0.02 |         |\n| min_samples_split      | int    | 5                |     2.0 |    10.0 | 5.0                   | None                   |        65.94 | **      |\n| bootstrap_sampling     | factor | 0                |     0.0 |     1.0 | 0                     | None                   |        38.41 | *       |\n| drift_window_threshold | int    | 300              |   100.0 |   500.0 | 256.0                 | None                   |         0.00 |         |\n| switch_significance    | float  | 0.05             |    0.01 |     0.1 | 0.010066722690551406  | None                   |         1.99 | *       |\n| binary_split           | factor | 0                |     0.0 |     1.0 | 0                     | None                   |         0.00 |         |\n| max_size               | float  | 500.0            |   100.0 |  1000.0 | 714.4837650420958     | None                   |        32.90 | *       |\n| memory_estimate_period | int    | 6                |     3.0 |     8.0 | 6.0                   | transform_power_10_int |         0.01 |         |\n| stop_mem_management    | factor | 0                |     0.0 |     1.0 | 0                     | None                   |         1.93 | *       |\n| remove_poor_attrs      | factor | 0                |     0.0 |     1.0 | 0                     | None                   |         0.00 |         |\n| merit_preprune         | factor | 1                |     0.0 |     1.0 | 1                     | None                   |         0.12 | .       |\n\n\nA histogram can be used to visualize the most important hyperparameters.\n\nspot_tuner.plot_importance(threshold=0.1)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#get-default-hyperparameters",
    "href": "024_spot_hpt_river_friedman_hatr.html#get-default-hyperparameters",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.4 Get Default Hyperparameters",
    "text": "22.4 Get Default Hyperparameters\nThe default hyperparameters, which will be used for a comparion with the tuned hyperparameters, can be obtained with the following commands:\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nX_start = get_default_hyperparameters_as_array(fun_control)\nmodel_default = get_one_core_model_from_X(X_start, fun_control, default=True)\nmodel_default\n\nHoeffdingAdaptiveTreeRegressorHoeffdingAdaptiveTreeRegressor (\n  grace_period=200\n  max_depth=1048576\n  delta=1e-07\n  tau=0.05\n  leaf_prediction=\"mean\"\n  leaf_model=LinearRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.01\n      )\n    )\n    loss=Squared ()\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n  model_selector_decay=0.95\n  nominal_attributes=None\n  splitter=EBSTSplitter ()\n  min_samples_split=5\n  bootstrap_sampling=0\n  drift_window_threshold=300\n  drift_detector=ADWIN (\n    delta=0.002\n    clock=32\n    max_buckets=5\n    min_window_length=5\n    grace_period=10\n  )\n  switch_significance=0.05\n  binary_split=0\n  max_size=500.\n  memory_estimate_period=1000000\n  stop_mem_management=0\n  remove_poor_attrs=0\n  merit_preprune=1\n  seed=None\n)\n\n\n\n\n\n\n\n\n\nNote: spotPython tunes numpy arrays\n\n\n\n\nspotPython tunes numpy arrays, i.e., the hyperparameters are stored in a numpy array.\n\n\n\nThe model with the default hyperparameters can be trained and evaluated with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import eval_oml_horizon\n\ndf_eval_default, df_true_default = eval_oml_horizon(\n                    model=model_default,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\nThe three performance criteria, i.e., score (metric), runtime, and memory consumption, can be visualized with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\ndf_labels=[\"default\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])\n\n\n\n\n\n\n\n\n\n22.4.1 Show Predictions\n\nSelect a subset of the data set for the visualization of the predictions:\n\nWe use the mean, \\(m\\), of the data set as the center of the visualization.\nWe use 100 data points, i.e., \\(m \\pm 50\\) as the visualization window.\n\n\n\nm = fun_control[\"test\"].shape[0]\na = int(m/2)-50\nb = int(m/2)+50\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b]], target_column=target_column,  df_labels=df_labels)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#get-spot-results",
    "href": "024_spot_hpt_river_friedman_hatr.html#get-spot-results",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.5 Get SPOT Results",
    "text": "22.5 Get SPOT Results\nIn a similar way, we can obtain the hyperparameters found by spotPython.\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\n\n\ndf_eval_spot, df_true_spot = eval_oml_horizon(\n                    model=model_spot,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\n\ndf_labels=[\"default\", \"spot\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])\n\n\n\n\n\n\n\n\n\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b], df_true_spot[a:b]], target_column=target_column,  df_labels=df_labels)\n\n\n\n\n\n\n\n\n\nfrom spotPython.plot.validation import plot_actual_vs_predicted\nplot_actual_vs_predicted(y_test=df_true_default[target_column], y_pred=df_true_default[\"Prediction\"], title=\"Default\")\nplot_actual_vs_predicted(y_test=df_true_spot[target_column], y_pred=df_true_spot[\"Prediction\"], title=\"SPOT\")",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#visualize-regression-trees",
    "href": "024_spot_hpt_river_friedman_hatr.html#visualize-regression-trees",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.6 Visualize Regression Trees",
    "text": "22.6 Visualize Regression Trees\n\ndataset_f = dataset.take(n_samples)\nprint(f\"n_samples: {n_samples}\")\nfor x, y in dataset_f:\n    model_default.learn_one(x, y)\n\nn_samples: 10000\n\n\n\n\n\n\n\n\nCaution: Large Trees\n\n\n\n\nSince the trees are large, the visualization is suppressed by default.\nTo visualize the trees, uncomment the following line.\n\n\n\n\n# model_default.draw()\n\n\nmodel_default.summary\n\n{'n_nodes': 45,\n 'n_branches': 22,\n 'n_leaves': 23,\n 'n_active_leaves': 37,\n 'n_inactive_leaves': 0,\n 'height': 9,\n 'total_observed_weight': 14168.0,\n 'n_alternate_trees': 4,\n 'n_pruned_alternate_trees': 2,\n 'n_switch_alternate_trees': 0}\n\n\n\n22.6.1 Spot Model\n\nprint(f\"n_samples: {n_samples}\")\ndataset_f = dataset.take(n_samples)\nfor x, y in dataset_f:\n    model_spot.learn_one(x, y)\n\nn_samples: 10000\n\n\n\n\n\n\n\n\nCaution: Large Trees\n\n\n\n\nSince the trees are large, the visualization is suppressed by default.\nTo visualize the trees, uncomment the following line.\n\n\n\n\n# model_spot.draw()\n\n\nmodel_spot.summary\n\n{'n_nodes': 29,\n 'n_branches': 14,\n 'n_leaves': 15,\n 'n_active_leaves': 25,\n 'n_inactive_leaves': 0,\n 'height': 6,\n 'total_observed_weight': 14168.0,\n 'n_alternate_trees': 2,\n 'n_pruned_alternate_trees': 1,\n 'n_switch_alternate_trees': 0}\n\n\n\nfrom spotPython.utils.eda import compare_two_tree_models\nprint(compare_two_tree_models(model_default, model_spot))\n\n| Parameter                |   Default |   Spot |\n|--------------------------|-----------|--------|\n| n_nodes                  |        45 |     29 |\n| n_branches               |        22 |     14 |\n| n_leaves                 |        23 |     15 |\n| n_active_leaves          |        37 |     25 |\n| n_inactive_leaves        |         0 |      0 |\n| height                   |         9 |      6 |\n| total_observed_weight    |     14168 |  14168 |\n| n_alternate_trees        |         4 |      2 |\n| n_pruned_alternate_trees |         2 |      1 |\n| n_switch_alternate_trees |         0 |      0 |",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#detailed-hyperparameter-plots",
    "href": "024_spot_hpt_river_friedman_hatr.html#detailed-hyperparameter-plots",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.7 Detailed Hyperparameter Plots",
    "text": "22.7 Detailed Hyperparameter Plots\n\nspot_tuner.plot_important_hyperparameter_contour(max_imp=3)\n\ngrace_period:  14.003273596561183\nmax_depth:  0.008893420909658247\ndelta:  0.00779643859999098\ntau:  100.0\nleaf_prediction:  3.5140062154588354\nleaf_model:  85.31418086809722\nmodel_selector_decay:  0.8674596274256202\nsplitter:  0.02154647406371429\nmin_samples_split:  65.94008220387273\nbootstrap_sampling:  38.41094805486372\ndrift_window_threshold:  0.004702562625188429\nswitch_significance:  1.9893983856587265\nbinary_split:  0.002356865111162426\nmax_size:  32.89766188970352\nmemory_estimate_period:  0.006442788495298308\nstop_mem_management:  1.9330561501200645\nremove_poor_attrs:  0.003752156649412206\nmerit_preprune:  0.12072075698753675\nimpo: [['grace_period', 14.003273596561183], ['max_depth', 0.008893420909658247], ['delta', 0.00779643859999098], ['tau', 100.0], ['leaf_prediction', 3.5140062154588354], ['leaf_model', 85.31418086809722], ['model_selector_decay', 0.8674596274256202], ['splitter', 0.02154647406371429], ['min_samples_split', 65.94008220387273], ['bootstrap_sampling', 38.41094805486372], ['drift_window_threshold', 0.004702562625188429], ['switch_significance', 1.9893983856587265], ['binary_split', 0.002356865111162426], ['max_size', 32.89766188970352], ['memory_estimate_period', 0.006442788495298308], ['stop_mem_management', 1.9330561501200645], ['remove_poor_attrs', 0.003752156649412206], ['merit_preprune', 0.12072075698753675]]\nindices: [3, 5, 8, 9, 13, 0, 4, 11, 15, 6, 17, 7, 1, 2, 14, 10, 16, 12]\nindices after max_imp selection: [3, 5, 8]",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#parallel-coordinates-plots",
    "href": "024_spot_hpt_river_friedman_hatr.html#parallel-coordinates-plots",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.8 Parallel Coordinates Plots",
    "text": "22.8 Parallel Coordinates Plots\n\nspot_tuner.parallel_plot()",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "a_05_datasets.html",
    "href": "a_05_datasets.html",
    "title": "Appendix E — Datasets",
    "section": "",
    "text": "E.1 The Friedman Drift Dataset",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "a_05_datasets.html#sec-a-05-friedman",
    "href": "a_05_datasets.html#sec-a-05-friedman",
    "title": "Appendix E — Datasets",
    "section": "",
    "text": "E.1.1 The Friedman Drift Dataset as Implemented in river\nWe will describe the Friedman synthetic dataset with concept drifts [SOURCE], see also Friedman (1991) and Ikonomovska, Gama, and Džeroski (2011). Each observation is composed of ten features. Each feature value is sampled uniformly in [0, 1]. Only the first five features are relevant. The target is defined by different functions depending on the type of the drift. Global Recurring Abrupt drift will be used, i.e., the concept drift appears over the whole instance space.\nThe target is defined by the following function: \\[\ny = 10 \\sin(\\pi x_0 x_1) + 20 (x_2 - 0.5)^2 + 10 x_3 + 5 x_4 + \\epsilon,\n\\] where \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) is normally distributed noise.\nIf the Global Recurring Abrupt drift variant of the Friedman Drift dataset is used, the target function changes at two points in time, namely \\(p_1\\) and \\(p_2\\). At the first point, the concept changes to: \\[\ny = 10 \\sin(\\pi x_3 x_5) + 20 (x_1 - 0.5)^2 + 10 x_0 + 5 x_2 + \\epsilon,\n\\] At the second point of drift the old concept reoccurs. This can be implemented as follows, see https://riverml.xyz/latest/api/datasets/synth/FriedmanDrift/:\ndef __iter__(self):\n    rng = random.Random(self.seed)\n\n    i = 0\n    while True:\n        x = {i: rng.uniform(a=0, b=1) for i in range(10)}\n        y = self._global_recurring_abrupt_gen(x, i) + rng.gauss(mu=0, sigma=1)\n\n        yield x, y\n        i += 1\ndef _global_recurring_abrupt_gen(self, x, index: int):\n    if index &lt; self._change_point1 or index &gt;= self._change_point2:\n        # The initial concept is recurring\n        return (\n            10 * math.sin(math.pi * x[0] * x[1]) + 20 * (x[2] - 0.5) ** 2 + 10 * x[3] + 5 * x[4]\n        )\n    else:\n        # Drift: the positions of the features are swapped\n        return (\n            10 * math.sin(math.pi * x[3] * x[5]) + 20 * (x[1] - 0.5) ** 2 + 10 * x[0] + 5 * x[2]\n        )\nspotpython requires the specification of a train and test data set. These data sets can be generated as follows:\n\nfrom river.datasets import synth\nimport pandas as pd\nimport numpy as np\nfrom spotRiver.utils.data_conversion import convert_to_df\n\nseed = 123\nshuffle = True\nn_train = 6_000\nn_test = 4_000\nn_samples = n_train + n_test\ntarget_column = \"y\"\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=(n_train/4, n_train/2),\n   seed=123\n)\n\ntrain = convert_to_df(dataset, n_total=n_train)\ntrain.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=(n_test/4, n_test/2),\n   seed=123\n)\ntest = convert_to_df(dataset, n_total=n_test)\ntest.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n\n\ndef plot_data_with_drift_points(data, target_column, n_train, title=\"\"):\n    indices = range(len(data))\n    y_values = data[target_column]\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(indices, y_values, label=\"y Value\", color='blue')\n\n    drift_points = [n_train / 4, n_train / 2]\n    for dp in drift_points:\n        plt.axvline(x=dp, color='red', linestyle='--', label=f'Drift Point at {int(dp)}')\n\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    plt.legend(by_label.values(), by_label.keys())\n\n    plt.xlabel('Index')\n    plt.ylabel('Target Value (y)')\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n\nplot_data_with_drift_points(train, target_column, n_train, title=\"Training Data with Drift Points\")\n\n\n\n\n\n\n\n\n\nplot_data_with_drift_points(test, target_column, n_train, title=\"Testing Data with Drift Points\")\n\n\n\n\n\n\n\n\n\n\nE.1.2 The Friedman Drift Data Set from spotpython\nA data generator for the Friedman Drift dataset is implemented in the spotPython package, see friedman.py. The spotPython version is a simplified version of the river implementation. The spotPyton version allows the generation of constant input values for the features. This is useful for visualizing the concept drifts. For the productive use the river version should be used.\nPlotting the first 100 samples of the Friedman Drift dataset, we can not see the concept drifts at \\(p_1\\) and \\(p_2\\). Drift can be visualized by plotting the target values over time for constant features, e,g, if \\(x_0\\) is set to \\(1\\) and all other features are set to \\(0\\). This is illustrated in the following plot.\n\nfrom spotPython.data.friedman import FriedmanDriftDataset\n\ndef plot_friedman_drift_data(n_samples, seed, change_point1, change_point2, constant=True):\n    data_generator = FriedmanDriftDataset(n_samples=n_samples, seed=seed, change_point1=change_point1, change_point2=change_point2, constant=constant)\n    data = [data for data in data_generator]\n    indices = [i for _, _, i in data]\n    values = {f\"x{i}\": [] for i in range(5)}\n    values[\"y\"] = []\n    for x, y, _ in data:\n        for i in range(5):\n            values[f\"x{i}\"].append(x[i])\n        values[\"y\"].append(y)\n\n    plt.figure(figsize=(10, 6))\n    for label, series in values.items():\n        plt.plot(indices, series, label=label)\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.axvline(x=change_point1, color='k', linestyle='--', label='Drift Point 1')\n    plt.axvline(x=change_point2, color='r', linestyle='--', label='Drift Point 2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\nplot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFriedman, Jerome H. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics 19 (1): 1–67.\n\n\nIkonomovska, Elena, João Gama, and Sašo Džeroski. 2011. “Learning Model Trees from Evolving Data Streams.” Data Mining and Knowledge Discovery 23 (1): 128–68.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#sec-the-friedman-drift-data-set-24",
    "href": "024_spot_hpt_river_friedman_hatr.html#sec-the-friedman-drift-data-set-24",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "position: The positions of the concept drifts.\nn_train: The number of samples used for training.\nn_test: The number of samples used for testing.\nseed: The seed for the random number generator.\ntarget_column: The name of the target column.\ndrift_type: The type of the concept drift.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#further-setup-arguments",
    "href": "024_spot_hpt_river_friedman_hatr.html#further-setup-arguments",
    "title": "22  ’river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "22.3 Further Setup Arguments",
    "text": "22.3 Further Setup Arguments\nThe TENSORBOARD_CLEAN argument is set to True to archive the TensorBoard folder if it already exists. This is useful if you want to start a hyperparameter tuning process from scratch. If you want to continue a hyperparameter tuning process, set TENSORBOARD_CLEAN to False. Then the TensorBoard folder will not be archived and the old and new TensorBoard files will shown in the TensorBoard dashboard.\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n# levels = [\"LinearRegression\"]\n# modify_hyper_parameter_levels(fun_control, \"leaf_model\", levels)\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name                   | type   | default          |   lower |    upper | transform              |\n|------------------------|--------|------------------|---------|----------|------------------------|\n| grace_period           | int    | 200              |  10     | 1000     | None                   |\n| max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n| delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n| tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n| leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n| model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n| splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n| min_samples_split      | int    | 5                |   2     |   10     | None                   |\n| bootstrap_sampling     | factor | 0                |   0     |    1     | None                   |\n| drift_window_threshold | int    | 300              | 100     |  500     | None                   |\n| switch_significance    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| binary_split           | factor | 0                |   0     |    1     | None                   |\n| max_size               | float  | 500.0            | 100     | 1000     | None                   |\n| memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n| stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n| remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n| merit_preprune         | factor | 1                |   0     |    1     | None                   |\n\n\n\n22.3.1 Run the Spot Optimizer\nThe class Spot [SOURCE] is the hyperparameter tuning workhorse. It is initialized with the following parameters:\n\nfun: the objective function\nfun_control: the dictionary with the control parameters for the objective function\ndesign: the experimental design\ndesign_control: the dictionary with the control parameters for the experimental design\nsurrogate: the surrogate model\nsurrogate_control: the dictionary with the control parameters for the surrogate model\noptimizer: the optimizer\noptimizer_control: the dictionary with the control parameters for the optimizer\n\n\n\n\n\n\n\nNote: Total run time\n\n\n\nThe total run time may exceed the specified max_time, because the initial design (here: init_size = INIT_SIZE as specified above) is always evaluated, even if this takes longer than max_time.\n\n\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(\n    fun=fun,\n    fun_control=fun_control,\n    design_control=design_control,\n    surrogate_control=surrogate_control,\n    optimizer_control=optimizer_control,\n)\nres = spot_tuner.run()\n\nspotPython tuning: 2.2768242097406106 [#---------] 5.16% \nspotPython tuning: 2.2768242097406106 [#---------] 7.27% \nspotPython tuning: 2.2768242097406106 [#---------] 11.45% \nspotPython tuning: 2.2768242097406106 [#---------] 13.45% \nspotPython tuning: 2.2768242097406106 [##--------] 20.87% \nspotPython tuning: 2.2768242097406106 [##--------] 23.13% \nspotPython tuning: 2.2768242097406106 [###-------] 25.23% \nspotPython tuning: 2.2768242097406106 [###-------] 28.40% \nspotPython tuning: 2.2768242097406106 [###-------] 32.99% \nspotPython tuning: 2.2768242097406106 [####------] 36.54% \nspotPython tuning: 2.2768242097406106 [####------] 39.67% \nspotPython tuning: 2.2768242097406106 [######----] 55.44% \nspotPython tuning: 2.2768242097406106 [#######---] 66.55% \nspotPython tuning: 2.2768242097406106 [#######---] 72.06% \nspotPython tuning: 2.2768242097406106 [#########-] 85.45% \nspotPython tuning: 2.17062028430776 [##########] 100.00% Done...",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>'`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html",
    "href": "a_04_spot_doc.html",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "",
    "text": "D.1 An Initial Example\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[\nf(x) = x^2.\n\\]\nfun = analytical().fun_sphere\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\nspot_1 = spot.Spot(fun=fun,\n                   fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                   design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                   surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_1.run()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x104052b50&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#organization",
    "href": "a_04_spot_doc.html#organization",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.2 Organization",
    "text": "D.2 Organization\nSpot organizes the surrogate based optimization process in four steps:\n\nSelection of the objective function: fun.\nSelection of the initial design: design.\nSelection of the optimization algorithm: optimizer.\nSelection of the surrogate model: surrogate.\n\nFor each of these steps, the user can specify an object:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_sphere\nfrom spotPython.design.spacefilling import spacefilling\ndesign = spacefilling(2)\nfrom scipy.optimize import differential_evolution\noptimizer = differential_evolution\nfrom spotPython.build.kriging import Kriging\nsurrogate = Kriging()\n\nFor each of these steps, the user can specify a dictionary of control parameters.\n\nfun_control\ndesign_control\noptimizer_control\nsurrogate_control\n\nEach of these dictionaries has an initialzaion method, e.g., fun_control_init(). The initialization methods set the default values for the control parameters.\n\n\n\n\n\n\nImportant:\n\n\n\n\nThe specification of an lower bound in fun_control is mandatory.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\nfun_control=fun_control_init(lower=np.array([-1, -1]),\n                            upper=np.array([1, 1]))\ndesign_control=design_control_init()\noptimizer_control=optimizer_control_init()\nsurrogate_control=surrogate_control_init()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#the-spot-object",
    "href": "a_04_spot_doc.html#the-spot-object",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.3 The Spot Object",
    "text": "D.3 The Spot Object\nBased on the definition of the fun, design, optimizer, and surrogate objects, and their corresponding control parameter dictionaries, fun_control, design_control, optimizer_control, and surrogate_control, the spot object can be build as follows:\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       optimizer_control=optimizer_control,\n                       surrogate_control=surrogate_control)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run",
    "href": "a_04_spot_doc.html#run",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.4 Run",
    "text": "D.4 Run\n\nspot_tuner.run()\n\nspotPython tuning: 1.801603872454505e-05 [#######---] 73.33% \nspotPython tuning: 1.801603872454505e-05 [########--] 80.00% \nspotPython tuning: 1.801603872454505e-05 [#########-] 86.67% \nspotPython tuning: 1.801603872454505e-05 [#########-] 93.33% \nspotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x3804a7250&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#print-the-results",
    "href": "a_04_spot_doc.html#print-the-results",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.5 Print the Results",
    "text": "D.5 Print the Results\n\nspot_tuner.print_results()\n\nmin y: 1.801603872454505e-05\nx0: 0.0019077911677074135\nx1: 0.003791618596979743\n\n\n[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#show-the-progress",
    "href": "a_04_spot_doc.html#show-the-progress",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.6 Show the Progress",
    "text": "D.6 Show the Progress\n\nspot_tuner.plot_progress()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#visualize-the-surrogate",
    "href": "a_04_spot_doc.html#visualize-the-surrogate",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.7 Visualize the Surrogate",
    "text": "D.7 Visualize the Surrogate\n\nThe plot method of the kriging surrogate is used.\nNote: the plot uses the interval defined by the ranges of the natural variables.\n\n\nspot_tuner.surrogate.plot()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "href": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.8 Run With a Specific Start Design",
    "text": "D.8 Run With a Specific Start Design\nTo pass a specific start design, use the X_start argument of the run method.\n\nspot_x0 = spot.Spot(fun=fun,\n                    fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                    design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                    surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_x0.run(X_start=np.array([0.5, -0.5]))\nspot_x0.plot_progress()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#init-build-initial-design",
    "href": "a_04_spot_doc.html#init-build-initial-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.9 Init: Build Initial Design",
    "text": "D.9 Init: Build Initial Design\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\ngen = spacefilling(2)\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\n\n[[ 8.97647221 13.41926847]\n [ 0.66946019  1.22344228]\n [ 5.23614115 13.78185824]\n [ 5.6149825  11.5851384 ]\n [-1.72963184  1.66516096]\n [-4.26945568  7.1325531 ]\n [ 1.26363761 10.17935555]\n [ 2.88779942  8.05508969]\n [-3.39111089  4.15213772]\n [ 7.30131231  5.22275244]]\n[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975\n  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#replicability",
    "href": "a_04_spot_doc.html#replicability",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.10 Replicability",
    "text": "D.10 Replicability\nSeed\n\ngen = spacefilling(2, seed=123)\nX0 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=345)\nX1 = gen.scipy_lhd(3)\nX2 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=123)\nX3 = gen.scipy_lhd(3)\nX0, X1, X2, X3\n\n(array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]),\n array([[0.78373509, 0.86811887],\n        [0.06692621, 0.6058029 ],\n        [0.41374778, 0.00525456]]),\n array([[0.121357  , 0.69043832],\n        [0.41906219, 0.32838498],\n        [0.86742658, 0.52910374]]),\n array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#surrogates",
    "href": "a_04_spot_doc.html#surrogates",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.11 Surrogates",
    "text": "D.11 Surrogates\n\nD.11.1 A Simple Predictor\nThe code below shows how to use a simple model for prediction. Assume that only two (very costly) measurements are available:\n\nf(0) = 0.5\nf(2) = 2.5\n\nWe are interested in the value at \\(x_0 = 1\\), i.e., \\(f(x_0 = 1)\\), but cannot run an additional, third experiment.\n\nfrom sklearn import linear_model\nX = np.array([[0], [2]])\ny = np.array([0.5, 2.5])\nS_lm = linear_model.LinearRegression()\nS_lm = S_lm.fit(X, y)\nX0 = np.array([[1]])\ny0 = S_lm.predict(X0)\nprint(y0)\n\n[1.5]\n\n\nCentral Idea: Evaluation of the surrogate model S_lm is much cheaper (or / and much faster) than running the real-world experiment \\(f\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#demotest-objective-function-fails",
    "href": "a_04_spot_doc.html#demotest-objective-function-fails",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.12 Demo/Test: Objective Function Fails",
    "text": "D.12 Demo/Test: Objective Function Fails\nSPOT expects np.nan values from failed objective function values. These are handled. Note: SPOT’s counter considers only successful executions of the objective function.\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nimport numpy as np\nfrom math import inf\n# number of initial points:\nni = 20\n# number of points\nn = 30\n\nfun = analytical().fun_random_error\nfun_control=fun_control_init(\n    lower = np.array([-1]),\n    upper= np.array([1]),\n    fun_evals = n,\n    show_progress=False)\ndesign_control=design_control_init(init_size=ni)\n\nspot_1 = spot.Spot(fun=fun,\n                     fun_control=fun_control,\n                     design_control=design_control)\nspot_1.run()\n# To check whether the run was successfully completed,\n# we compare the number of evaluated points to the specified\n# number of points.\nassert spot_1.y.shape[0] == n\n\n[        nan         nan -0.02203599 -0.21843718  0.78240941         nan\n -0.3923345   0.67234256  0.31802454 -0.68898927 -0.75129705  0.97550354\n  0.41757584         nan  0.82585329         nan -0.49274073         nan\n -0.17991251  0.1481835 ]\n[-1.]\n[nan]\n[-0.14624037]\n[0.166475]\n[nan]\n[-0.3352401]\n[-0.47259301]\n[0.95541987]\n[0.17335968]\n[-0.58552368]\n[-0.20126111]\n[-0.60100809]\n[-0.97897336]\n[-0.2748985]\n[0.8359486]\n[0.99035591]\n[0.01641232]\n[0.5629346]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  }
]