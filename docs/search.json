[
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html",
    "href": "024_spot_hpt_river_friedman_hatr.html",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "20.1 Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, size of the data set, and the experiment name.\nMAX_TIME = 1\nINIT_SIZE = 5\nPREFIX=\"24-river\"\nK = 0.1",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#sec-setup-13",
    "href": "024_spot_hpt_river_friedman_hatr.html#sec-setup-13",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "MAX_TIME: The maximum run time in seconds for the hyperparameter tuning process.\nINIT_SIZE: The initial design size for the hyperparameter tuning process.\nPREFIX: The prefix for the experiment name.\nK: The factor that determines the number of samples in the data set.\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nK is the multiplier for the number of samples. If it is set to 1, then 100_000samples are taken. It is set to 0.1 for demonstration purposes. For real experiments, this should be increased to at least 1.\n\n\n\n\n\nThis notebook exemplifies hyperparameter tuning with SPOT (spotPython and spotRiver).\nThe hyperparameter software SPOT is available in Python. It was developed in R (statistical programming language), see Open Access book “Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide”, available here: https://link.springer.com/book/10.1007/978-981-19-5170-1.\nThis notebook demonstrates hyperparameter tuning for river. It is based on the notebook “Incremental decision trees in river: the Hoeffding Tree case”, see: https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters.\nHere we will use the river HTR and HATR functions as in “Incremental decision trees in river: the Hoeffding Tree case”, see: https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#initialization-of-the-fun_control-dictionary",
    "href": "024_spot_hpt_river_friedman_hatr.html#initialization-of-the-fun_control-dictionary",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.2 Initialization of the fun_control Dictionary",
    "text": "20.2 Initialization of the fun_control Dictionary\nspotPython supports the visualization of the hyperparameter tuning process with TensorBoard. The following example shows how to use TensorBoard with spotPython. The fun_control dictionary is the central data structure that is used to control the optimization process. It is initialized as follows:\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    max_time=MAX_TIME,\n    fun_evals=inf,\n    tolerance_x = np.sqrt(np.spacing(1)))\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_03_19_21_52_50\nCreated spot_tensorboard_path: runs/spot_logs/24-river_p040025_2024-03-19_21-52-50 for SummaryWriter()\n\n\n\n\n\n\n\n\nTip: TensorBoard\n\n\n\n\nSince the spot_tensorboard_path argument is not None, which is the default, spotPython will log the optimization process in the TensorBoard folder.\nSection 21.8.3 describes how to start TensorBoard and access the TensorBoard dashboard.\nThe TENSORBOARD_CLEAN argument is set to True to archive the TensorBoard folder if it already exists. This is useful if you want to start a hyperparameter tuning process from scratch. If you want to continue a hyperparameter tuning process, set TENSORBOARD_CLEAN to False. Then the TensorBoard folder will not be archived and the old and new TensorBoard files will shown in the TensorBoard dashboard.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#load-data-the-friedman-drift-data",
    "href": "024_spot_hpt_river_friedman_hatr.html#load-data-the-friedman-drift-data",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.3 Load Data: The Friedman Drift Data",
    "text": "20.3 Load Data: The Friedman Drift Data\nWe will use the Friedman synthetic dataset with concept drifts [SOURCE]. Each observation is composed of ten features. Each feature value is sampled uniformly in [0, 1]. Only the first five features are relevant. The target is defined by different functions depending on the type of the drift. Global Recurring Abrupt drift will be used, i.e., the concept drift appears over the whole instance space. There are two points of concept drift. At the second point of drift the old concept reoccurs.\nThe following parameters are used to generate and handle the data set:\n\nhorizon: The prediction horizon in hours.\nn_samples: The number of samples in the data set.\np_1: The position of the first concept drift.\np_2: The position of the second concept drift.\nposition: The position of the concept drifts.\nn_train: The number of samples used for training.\n\n\nhorizon = 7*24\nn_samples = int(K*100_000)\np_1 = int(K*25_000)\np_2 = int(K*50_000)\nposition=(p_1, p_2)\nn_train = 1_000\n\n\nfrom river.datasets import synth\nimport pandas as pd\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n   seed=123\n)\n\n\nWe will use spotRiver’s convert_to_df function [SOURCE] to convert the river data set to a pandas data frame.\n\n\nfrom spotRiver.utils.data_conversion import convert_to_df\ntarget_column = \"y\"\ndf = convert_to_df(dataset, target_column=target_column, n_total=n_samples)\n\n\nAdd column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column of the dataframe.\nThen split the data frame into a training and test data set. The train and test data sets are stored in the fun_control dictionary.\n\n\nfrom spotPython.hyperparameters.values import set_control_key_value\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\nset_control_key_value(control_dict=fun_control,\n                        key=\"train\",\n                        value=df[:n_train],\n                        replace=True)\nset_control_key_value(fun_control, \"test\", df[n_train:], True)\nset_control_key_value(fun_control, \"n_samples\", n_samples, replace=True)\nset_control_key_value(fun_control, \"target_column\", target_column, replace=True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#specification-of-the-preprocessing-model",
    "href": "024_spot_hpt_river_friedman_hatr.html#specification-of-the-preprocessing-model",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.4 Specification of the Preprocessing Model",
    "text": "20.4 Specification of the Preprocessing Model\n\nWe use the StandardScaler [SOURCE] from river as the preprocessing model. The StandardScaler is used to standardize the data set, i.e., it has zero mean and unit variance.\n\n\nfrom river import preprocessing\nprep_model = preprocessing.StandardScaler()\nset_control_key_value(fun_control, \"prep_model\", prep_model, replace=True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "href": "024_spot_hpt_river_friedman_hatr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.5 SelectSelect Model (algorithm) and core_model_hyper_dict",
    "text": "20.5 SelectSelect Model (algorithm) and core_model_hyper_dict\nspotPython hyperparameter tuning approach uses two components:\n\na model (class) and\nan associated hyperparameter dictionary.\n\nHere, the river model class HoeffdingAdaptiveTreeRegressor [SOURCE] is selected.\nThe corresponding hyperparameters are loaded from the associated dictionary, which is stored as a JSON file [SOURCE]. The JSON file contains hyperparameter type information, names, and bounds.\nThe method add_core_model_to_fun_control adds the model and the hyperparameter dictionary to the fun_control dictionary.\nAlternatively, you can load a local hyper_dict. Simply set river_hyper_dict.json as the filename. If filenameis set to None, which is the default, the hyper_dict [SOURCE] is loaded from the spotRiver package.\n\nfrom river.tree import HoeffdingAdaptiveTreeRegressor\nfrom spotRiver.data.river_hyper_dict import RiverHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\ncore_model  = HoeffdingAdaptiveTreeRegressor\nadd_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=RiverHyperDict,\n                              filename=None)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "024_spot_hpt_river_friedman_hatr.html#modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.6 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "20.6 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nAfter the core_model and the core_model_hyper_dict are added to the fun_control dictionary, the hyperparameter tuning can be started. However, in some settings, the user wants to modify the hyperparameters of the core_model_hyper_dict. This can be done with the modify_hyper_parameter_bounds and modify_hyper_parameter_levels functions [SOURCE].\nThe following code shows how hyperparameter of type numeric and integer (boolean) can be modified. The modify_hyper_parameter_bounds function is used to modify the bounds of the hyperparameter delta and merit_preprune. Similar option exists for the modify_hyper_parameter_levels function to modify the levels of categorical hyperparameters.\n\n# from spotPython.hyperparameters.values import set_control_hyperparameter_value\n# set_control_hyperparameter_value(fun_control, \"delta\", [1e-10, 1e-6])\n# set_control_hyperparameter_value(fun_control, \"merit_preprune\", [0, 0])\n\n\n\n\n\n\n\nNote: Active and Inactive Hyperparameters\n\n\n\nHyperparameters can be excluded from the tuning procedure by selecting identical values for the lower and upper bounds. For example, the hyperparameter merit_preprune is excluded from the tuning procedure by setting the bounds to [0, 0].\n\n\nspotPython’s method gen_design_table summarizes the experimental design that is used for the hyperparameter tuning:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name                   | type   | default          |   lower |    upper | transform              |\n|------------------------|--------|------------------|---------|----------|------------------------|\n| grace_period           | int    | 200              |  10     | 1000     | None                   |\n| max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n| delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n| tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n| leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n| model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n| splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n| min_samples_split      | int    | 5                |   2     |   10     | None                   |\n| bootstrap_sampling     | factor | 0                |   0     |    1     | None                   |\n| drift_window_threshold | int    | 300              | 100     |  500     | None                   |\n| switch_significance    | float  | 0.05             |   0.01  |    0.1   | None                   |\n| binary_split           | factor | 0                |   0     |    1     | None                   |\n| max_size               | float  | 500.0            | 100     | 1000     | None                   |\n| memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n| stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n| remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n| merit_preprune         | factor | 1                |   0     |    1     | None                   |",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#selection-of-the-objective-loss-function",
    "href": "024_spot_hpt_river_friedman_hatr.html#selection-of-the-objective-loss-function",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.7 Selection of the Objective (Loss) Function",
    "text": "20.7 Selection of the Objective (Loss) Function\nThe metric_sklearn is used for the sklearn based evaluation via eval_oml_horizon [SOURCE]. Here we use the mean_absolute_error [SOURCE] as the objective function.\n\n\n\n\n\n\nNote: Additional metrics\n\n\n\nspotRiver also supports additional metrics. For example, the metric_river is used for the river based evaluation via eval_oml_iter_progressive [SOURCE]. The metric_river is implemented to simulate the behaviour of the “original” river metrics.\n\n\nspotRiver provides information about the model’ s score (metric), memory, and time. The hyperparamter tuner requires a single objective. Therefore, a weighted sum of the metric, memory, and time is computed. The weights are defined in the weights array.\n\n\n\n\n\n\nNote: Weights\n\n\n\nThe weights provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased.\n\n\nThe oml_grace_period defines the number of observations that are used for the initial training of the model. The step defines the iteration number at which to yield results. This only takes into account the predictions, and not the training steps. The weight_coeff defines a multiplier for the results: results are multiplied by (step/n_steps)**weight_coeff, where n_steps is the total number of iterations. Results from the beginning have a lower weight than results from the end if weight_coeff &gt; 1. If weight_coeff == 0, all results have equal weight. Note, that the weight_coeff is only used internally for the tuner and does not affect the results that are used for the evaluation or comparisons.\n\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\n\nweights = np.array([1, 1/1000, 1/1000])*10_000.0\noml_grace_period = 2\nstep = 100\nweight_coeff = 1.0\n\nset_control_key_value(control_dict=fun_control,\n                        key=\"horizon\",\n                        value=horizon,\n                        replace=True)\nset_control_key_value(fun_control, \"oml_grace_period\", oml_grace_period, True)\nset_control_key_value(fun_control, \"weights\", weights, True)\nset_control_key_value(fun_control, \"step\", step, True)\nset_control_key_value(fun_control, \"weight_coeff\", weight_coeff, True)\nset_control_key_value(fun_control, \"metric_sklearn\", mean_absolute_error, True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#calling-the-spot-function",
    "href": "024_spot_hpt_river_friedman_hatr.html#calling-the-spot-function",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.8 Calling the SPOT Function",
    "text": "20.8 Calling the SPOT Function\n\n20.8.1 The Objective Function\nThe objective function fun_oml_horizon [SOURCE] is selected next.\n\nfrom spotRiver.fun.hyperriver import HyperRiver\nfun = HyperRiver().fun_oml_horizon\n\nThe following code snippet shows how to get the default hyperparameters as an array, so that they can be passed to the Spot function.\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nX_start = get_default_hyperparameters_as_array(fun_control)\n\n\n\n20.8.2 Run the Spot Optimizer\nThe class Spot [SOURCE] is the hyperparameter tuning workhorse. It is initialized with the following parameters:\n\nfun: the objective function\nfun_control: the dictionary with the control parameters for the objective function\ndesign: the experimental design\ndesign_control: the dictionary with the control parameters for the experimental design\nsurrogate: the surrogate model\nsurrogate_control: the dictionary with the control parameters for the surrogate model\noptimizer: the optimizer\noptimizer_control: the dictionary with the control parameters for the optimizer\n\n\n\n\n\n\n\nNote: Total run time\n\n\n\nThe total run time may exceed the specified max_time, because the initial design (here: init_size = INIT_SIZE as specified above) is always evaluated, even if this takes longer than max_time.\n\n\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init()\nset_control_key_value(control_dict=design_control,\n                        key=\"init_size\",\n                        value=INIT_SIZE,\n                        replace=True)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                           n_theta=2)\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   design_control=design_control,\n                   surrogate_control=surrogate_control)\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: 21681.32108277045 [#---------] 12.07% \nspotPython tuning: 21681.32108277045 [##--------] 16.43% \nspotPython tuning: 21681.32108277045 [##--------] 23.77% \nspotPython tuning: 21681.32108277045 [#####-----] 49.96% \nspotPython tuning: 21681.32108277045 [##########] 95.96% \nspotPython tuning: 21681.32108277045 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x3a49e0310&gt;\n\n\n\n\n20.8.3 TensorBoard\nNow we can start TensorBoard in the background with the following command, where ./runs is the default directory for the TensorBoard log files:\ntensorboard --logdir=\"./runs\"\n\n\n\n\n\n\nTip: TENSORBOARD_PATH\n\n\n\nThe TensorBoard path can be printed with the following command:\n\nfrom spotPython.utils.init import get_tensorboard_path\nget_tensorboard_path(fun_control)\n\n'runs/'\n\n\n\n\nWe can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\nThe TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate [SOURCE] is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model.\n\n\n\n\n20.8.4 Results\nAfter the hyperparameter tuning run is finished, results can be printed:\n\nspot_tuner.print_results(print_screen=False)\n\n[['grace_period', 271.0],\n ['max_depth', 13.0],\n ['delta', 1.6436874519102299e-07],\n ['tau', 0.07868130740742395],\n ['leaf_prediction', 1.0],\n ['leaf_model', 0.0],\n ['model_selector_decay', 0.973379790035513],\n ['splitter', 2.0],\n ['min_samples_split', 9.0],\n ['bootstrap_sampling', 1.0],\n ['drift_window_threshold', 379.0],\n ['switch_significance', 0.023590637180761667],\n ['binary_split', 0.0],\n ['max_size', 241.5226665924828],\n ['memory_estimate_period', 6.0],\n ['stop_mem_management', 0.0],\n ['remove_poor_attrs', 1.0],\n ['merit_preprune', 1.0]]\n\n\nThe tuned hyperparameters can be obtained as a dictionary with the following command:\n\nfrom spotPython.hyperparameters.values import get_tuned_hyperparameters\nget_tuned_hyperparameters(spot_tuner, fun_control)\n\n{'grace_period': 271.0,\n 'max_depth': 13.0,\n 'delta': 1.6436874519102299e-07,\n 'tau': 0.07868130740742395,\n 'leaf_prediction': 'model',\n 'leaf_model': 'LinearRegression',\n 'model_selector_decay': 0.973379790035513,\n 'splitter': 'QOSplitter',\n 'min_samples_split': 9.0,\n 'bootstrap_sampling': 1,\n 'drift_window_threshold': 379.0,\n 'switch_significance': 0.023590637180761667,\n 'binary_split': 0,\n 'max_size': 241.5226665924828,\n 'memory_estimate_period': 6.0,\n 'stop_mem_management': 0,\n 'remove_poor_attrs': 1,\n 'merit_preprune': 1}\n\n\nThe results can be saved and reloaded with the following commands:\n\nfrom spotPython.utils.file import save_pickle, load_pickle\nfrom spotPython.utils.init import get_experiment_name\nexperiment_name = get_experiment_name(PREFIX)\nSAVE_AND_LOAD = False\nif SAVE_AND_LOAD == True:\n    save_pickle(spot_tuner, experiment_name)\n    spot_tuner = load_pickle(experiment_name)\n\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The black points represent the performace values (score or metric) of hyperparameter configurations from the initial design, whereas the red points represents the hyperparameter configurations found by the surrogate model based optimization.\n\nspot_tuner.plot_progress(log_y=True, filename=\"./figures/\" + experiment_name+\"_progress.pdf\")\n\n\n\n\n\n\n\n\nResults can also be printed in tabular form.\n\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name                   | type   | default          |   lower |   upper | tuned                  | transform              |   importance | stars   |\n|------------------------|--------|------------------|---------|---------|------------------------|------------------------|--------------|---------|\n| grace_period           | int    | 200              |    10.0 |  1000.0 | 271.0                  | None                   |         0.15 | .       |\n| max_depth              | int    | 20               |     2.0 |    20.0 | 13.0                   | transform_power_2_int  |         0.02 |         |\n| delta                  | float  | 1e-07            |   1e-08 |   1e-06 | 1.6436874519102299e-07 | None                   |         0.02 |         |\n| tau                    | float  | 0.05             |    0.01 |     0.1 | 0.07868130740742395    | None                   |         0.03 |         |\n| leaf_prediction        | factor | mean             |     0.0 |     2.0 | model                  | None                   |         3.10 | *       |\n| leaf_model             | factor | LinearRegression |     0.0 |     2.0 | LinearRegression       | None                   |         0.00 |         |\n| model_selector_decay   | float  | 0.95             |     0.9 |    0.99 | 0.973379790035513      | None                   |         0.02 |         |\n| splitter               | factor | EBSTSplitter     |     0.0 |     2.0 | QOSplitter             | None                   |         0.32 | .       |\n| min_samples_split      | int    | 5                |     2.0 |    10.0 | 9.0                    | None                   |         1.05 | *       |\n| bootstrap_sampling     | factor | 0                |     0.0 |     1.0 | 1                      | None                   |        13.44 | *       |\n| drift_window_threshold | int    | 300              |   100.0 |   500.0 | 379.0                  | None                   |        13.77 | *       |\n| switch_significance    | float  | 0.05             |    0.01 |     0.1 | 0.023590637180761667   | None                   |       100.00 | ***     |\n| binary_split           | factor | 0                |     0.0 |     1.0 | 0                      | None                   |         0.11 | .       |\n| max_size               | float  | 500.0            |   100.0 |  1000.0 | 241.5226665924828      | None                   |         1.00 | *       |\n| memory_estimate_period | int    | 6                |     3.0 |     8.0 | 6.0                    | transform_power_10_int |         0.01 |         |\n| stop_mem_management    | factor | 0                |     0.0 |     1.0 | 0                      | None                   |         0.15 | .       |\n| remove_poor_attrs      | factor | 0                |     0.0 |     1.0 | 1                      | None                   |         4.75 | *       |\n| merit_preprune         | factor | 1                |     0.0 |     1.0 | 1                      | None                   |         0.70 | .       |\n\n\nA histogram can be used to visualize the most important hyperparameters.\n\nspot_tuner.plot_importance(threshold=0.0025, filename=\"./figures/\" + experiment_name+\"_importance.pdf\")",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#the-larger-data-set",
    "href": "024_spot_hpt_river_friedman_hatr.html#the-larger-data-set",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.9 The Larger Data Set",
    "text": "20.9 The Larger Data Set\nAfter the hyperparamter were tuned on a small data set, we can now apply the hyperparameter configuration to a larger data set. The following code snippet shows how to generate the larger data set.\n\n\n\n\n\n\nCaution: Increased Friedman-Drift Data Set\n\n\n\n\nThe Friedman-Drift Data Set is increased by a factor of two to show the transferability of the hyperparameter tuning results.\nLarger values of K lead to a longer run time.\n\n\n\n\nK = 0.2\nn_samples = int(K*100_000)\np_1 = int(K*25_000)\np_2 = int(K*50_000)\nposition=(p_1, p_2)\n\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n   seed=123\n)\n\nThe larger data set is converted to a Pandas data frame and passed to the fun_control dictionary.\n\ndf = convert_to_df(dataset, target_column=target_column, n_total=n_samples)\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\nset_control_key_value(fun_control, \"train\", df[:n_train], True)\nset_control_key_value(fun_control, \"test\", df[n_train:], True)\nset_control_key_value(fun_control, \"n_samples\", n_samples, True)\nset_control_key_value(fun_control, \"target_column\", target_column, True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#get-default-hyperparameters",
    "href": "024_spot_hpt_river_friedman_hatr.html#get-default-hyperparameters",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.10 Get Default Hyperparameters",
    "text": "20.10 Get Default Hyperparameters\nThe default hyperparameters, whihc will be used for a comparion with the tuned hyperparameters, can be obtained with the following commands:\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nX_start = get_default_hyperparameters_as_array(fun_control)\nmodel_default = get_one_core_model_from_X(X_start, fun_control, default=True)\nmodel_default\n\nHoeffdingAdaptiveTreeRegressorHoeffdingAdaptiveTreeRegressor (\n  grace_period=200\n  max_depth=1048576\n  delta=1e-07\n  tau=0.05\n  leaf_prediction=\"mean\"\n  leaf_model=LinearRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.01\n      )\n    )\n    loss=Squared ()\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n  model_selector_decay=0.95\n  nominal_attributes=None\n  splitter=EBSTSplitter ()\n  min_samples_split=5\n  bootstrap_sampling=0\n  drift_window_threshold=300\n  drift_detector=ADWIN (\n    delta=0.002\n    clock=32\n    max_buckets=5\n    min_window_length=5\n    grace_period=10\n  )\n  switch_significance=0.05\n  binary_split=0\n  max_size=500.\n  memory_estimate_period=1000000\n  stop_mem_management=0\n  remove_poor_attrs=0\n  merit_preprune=1\n  seed=None\n)\n\n\n\n\n\n\n\n\n\nNote: spotPython tunes numpy arrays\n\n\n\n\nspotPython tunes numpy arrays, i.e., the hyperparameters are stored in a numpy array.\n\n\n\nThe model with the default hyperparameters can be trained and evaluated with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import eval_oml_horizon\n\ndf_eval_default, df_true_default = eval_oml_horizon(\n                    model=model_default,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\nThe three performance criteria, i.e., score (metric), runtime, and memory consumption, can be visualized with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\ndf_labels=[\"default\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])\n\n\n\n\n\n\n\n\n\n20.10.1 Show Predictions\n\nSelect a subset of the data set for the visualization of the predictions:\n\nWe use the mean, \\(m\\), of the data set as the center of the visualization.\nWe use 100 data points, i.e., \\(m \\pm 50\\) as the visualization window.\n\n\n\nm = fun_control[\"test\"].shape[0]\na = int(m/2)-50\nb = int(m/2)\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b]], target_column=target_column,  df_labels=df_labels)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#get-spot-results",
    "href": "024_spot_hpt_river_friedman_hatr.html#get-spot-results",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.11 Get SPOT Results",
    "text": "20.11 Get SPOT Results\nIn a similar way, we can obtain the hyperparameters found by spotPython.\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\n\n\ndf_eval_spot, df_true_spot = eval_oml_horizon(\n                    model=model_spot,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\n\ndf_labels=[\"default\", \"spot\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"], filename=\"./figures/\" + experiment_name+\"_metrics.pdf\")\n\n\n\n\n\n\n\n\n\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b], df_true_spot[a:b]], target_column=target_column,  df_labels=df_labels, filename=\"./figures/\" + experiment_name+\"_predictions.pdf\")\n\n\n\n\n\n\n\n\n\nfrom spotPython.plot.validation import plot_actual_vs_predicted\nplot_actual_vs_predicted(y_test=df_true_default[target_column], y_pred=df_true_default[\"Prediction\"], title=\"Default\")\nplot_actual_vs_predicted(y_test=df_true_spot[target_column], y_pred=df_true_spot[\"Prediction\"], title=\"SPOT\")",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#visualize-regression-trees",
    "href": "024_spot_hpt_river_friedman_hatr.html#visualize-regression-trees",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.12 Visualize Regression Trees",
    "text": "20.12 Visualize Regression Trees\n\ndataset_f = dataset.take(n_samples)\nprint(f\"n_samples: {n_samples}\")\nfor x, y in dataset_f:\n    model_default.learn_one(x, y)\n\nn_samples: 20000\n\n\n\n\n\n\n\n\nCaution: Large Trees\n\n\n\n\nSince the trees are large, the visualization is suppressed by default.\nTo visualize the trees, uncomment the following line.\n\n\n\n\n# model_default.draw()\n\n\nmodel_default.summary\n\n{'n_nodes': 35,\n 'n_branches': 17,\n 'n_leaves': 18,\n 'n_active_leaves': 96,\n 'n_inactive_leaves': 0,\n 'height': 6,\n 'total_observed_weight': 39002.0,\n 'n_alternate_trees': 21,\n 'n_pruned_alternate_trees': 6,\n 'n_switch_alternate_trees': 2}\n\n\n\n20.12.1 Spot Model\n\nprint(f\"n_samples: {n_samples}\")\ndataset_f = dataset.take(n_samples)\nfor x, y in dataset_f:\n    model_spot.learn_one(x, y)\n\nn_samples: 20000\n\n\n\n\n\n\n\n\nCaution: Large Trees\n\n\n\n\nSince the trees are large, the visualization is suppressed by default.\nTo visualize the trees, uncomment the following line.\n\n\n\n\n# model_spot.draw()\n\n\nmodel_spot.summary\n\n{'n_nodes': 55,\n 'n_branches': 27,\n 'n_leaves': 28,\n 'n_active_leaves': 104,\n 'n_inactive_leaves': 0,\n 'height': 9,\n 'total_observed_weight': 39002.0,\n 'n_alternate_trees': 20,\n 'n_pruned_alternate_trees': 3,\n 'n_switch_alternate_trees': 1}\n\n\n\nfrom spotPython.utils.eda import compare_two_tree_models\nprint(compare_two_tree_models(model_default, model_spot))\n\n| Parameter                |   Default |   Spot |\n|--------------------------|-----------|--------|\n| n_nodes                  |        35 |     55 |\n| n_branches               |        17 |     27 |\n| n_leaves                 |        18 |     28 |\n| n_active_leaves          |        96 |    104 |\n| n_inactive_leaves        |         0 |      0 |\n| height                   |         6 |      9 |\n| total_observed_weight    |     39002 |  39002 |\n| n_alternate_trees        |        21 |     20 |\n| n_pruned_alternate_trees |         6 |      3 |\n| n_switch_alternate_trees |         2 |      1 |",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#detailed-hyperparameter-plots",
    "href": "024_spot_hpt_river_friedman_hatr.html#detailed-hyperparameter-plots",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.13 Detailed Hyperparameter Plots",
    "text": "20.13 Detailed Hyperparameter Plots\n\nfilename = None\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\ngrace_period:  0.15273103691910975\nmax_depth:  0.0220247779116996\ndelta:  0.01864052988596182\ntau:  0.025639280486689935\nleaf_prediction:  3.100722029092928\nleaf_model:  0.00288539300315935\nmodel_selector_decay:  0.021137267823553917\nsplitter:  0.318996784359951\nmin_samples_split:  1.0549920529628907\nbootstrap_sampling:  13.435001153269173\ndrift_window_threshold:  13.771729566869347\nswitch_significance:  100.0\nbinary_split:  0.11074108800880438\nmax_size:  1.0035816656565333\nmemory_estimate_period:  0.007642027514462318\nstop_mem_management:  0.1534240262228379\nremove_poor_attrs:  4.749613178664494\nmerit_preprune:  0.7001482817882747\nimpo: [['grace_period', 0.15273103691910975], ['max_depth', 0.0220247779116996], ['delta', 0.01864052988596182], ['tau', 0.025639280486689935], ['leaf_prediction', 3.100722029092928], ['leaf_model', 0.00288539300315935], ['model_selector_decay', 0.021137267823553917], ['splitter', 0.318996784359951], ['min_samples_split', 1.0549920529628907], ['bootstrap_sampling', 13.435001153269173], ['drift_window_threshold', 13.771729566869347], ['switch_significance', 100.0], ['binary_split', 0.11074108800880438], ['max_size', 1.0035816656565333], ['memory_estimate_period', 0.007642027514462318], ['stop_mem_management', 0.1534240262228379], ['remove_poor_attrs', 4.749613178664494], ['merit_preprune', 0.7001482817882747]]\nindices: [11, 10, 9, 16, 4, 8, 13, 17, 7, 15, 0, 12, 3, 1, 6, 2, 14, 5]\nindices after max_imp selection: [11, 10, 9, 16, 4, 8, 13, 17, 7, 15, 0, 12, 3, 1, 6, 2, 14, 5]",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#parallel-coordinates-plots",
    "href": "024_spot_hpt_river_friedman_hatr.html#parallel-coordinates-plots",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.14 Parallel Coordinates Plots",
    "text": "20.14 Parallel Coordinates Plots\n\nspot_tuner.parallel_plot()",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "024_spot_hpt_river_friedman_hatr.html#plot-all-combinations-of-hyperparameters",
    "href": "024_spot_hpt_river_friedman_hatr.html#plot-all-combinations-of-hyperparameters",
    "title": "20  river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data",
    "section": "20.15 Plot all Combinations of Hyperparameters",
    "text": "20.15 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html",
    "href": "025_spot_hpt_river_friedman_amfr.html",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "21.1 Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, size of the data set, and the experiment name.\nMAX_TIME = 1\nINIT_SIZE = 5\nPREFIX=\"025RIVER\"\nK = 0.1",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#sec-setup-51",
    "href": "025_spot_hpt_river_friedman_amfr.html#sec-setup-51",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "",
    "text": "MAX_TIME: The maximum run time in seconds for the hyperparameter tuning process.\nINIT_SIZE: The initial design size for the hyperparameter tuning process.\nPREFIX: The prefix for the experiment name.\nK: The factor that determines the number of samples in the data set.\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nK is the multiplier for the number of samples. If it is set to 1, then 100_000samples are taken. It is set to 0.1 for demonstration purposes. For real experiments, this should be increased to at least 1.\n\n\n\n\n\nThis notebook exemplifies hyperparameter tuning with SPOT (spotPython and spotRiver).\nThe hyperparameter software SPOT is available in Python. It was developed in R (statistical programming language), see Open Access book “Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide”, available here: https://link.springer.com/book/10.1007/978-981-19-5170-1.\nThis notebook demonstrates hyperparameter tuning for river. It is based on the notebook “Incremental decision trees in river: the Hoeffding Tree case”, see: https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters.\nHere we will use the river AMFRegressor functions, see: https://riverml.xyz/0.19.0/api/forest/AMFRegressor/.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#initialization-of-the-fun_control-dictionary",
    "href": "025_spot_hpt_river_friedman_amfr.html#initialization-of-the-fun_control-dictionary",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.2 Initialization of the fun_control Dictionary",
    "text": "21.2 Initialization of the fun_control Dictionary\nspotPython supports the visualization of the hyperparameter tuning process with TensorBoard. The following example shows how to use TensorBoard with spotPython.\nFirst, we define an “experiment name” to identify the hyperparameter tuning process. The experiment name is also used to create a directory for the TensorBoard files.\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    max_time=MAX_TIME,\n    fun_evals=inf,\n    tolerance_x=np.sqrt(np.spacing(1)))\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_03_17_21_08_13\nCreated spot_tensorboard_path: runs/spot_logs/025RIVER_p040025_2024-03-17_21-08-13 for SummaryWriter()\n\n\n\n\n\n\n\n\nTip: TensorBoard\n\n\n\n\nSince the spot_tensorboard_path argument is not None, which is the default, spotPython will log the optimization process in the TensorBoard folder.\nSection 21.8.3 describes how to start TensorBoard and access the TensorBoard dashboard.\nThe TENSORBOARD_CLEAN argument is set to True to archive the TensorBoard folder if it already exists. This is useful if you want to start a hyperparameter tuning process from scratch. If you want to continue a hyperparameter tuning process, set TENSORBOARD_CLEAN to False. Then the TensorBoard folder will not be archived and the old and new TensorBoard files will shown in the TensorBoard dashboard.",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#load-data-the-friedman-drift-data",
    "href": "025_spot_hpt_river_friedman_amfr.html#load-data-the-friedman-drift-data",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.3 Load Data: The Friedman Drift Data",
    "text": "21.3 Load Data: The Friedman Drift Data\nWe will use the Friedman synthetic dataset with concept drifts [SOURCE]. Each observation is composed of ten features. Each feature value is sampled uniformly in [0, 1]. Only the first five features are relevant. The target is defined by different functions depending on the type of the drift. Global Recurring Abrupt drift will be used, i.e., the concept drift appears over the whole instance space. There are two points of concept drift. At the second point of drift the old concept reoccurs.\nThe following parameters are used to generate and handle the data set:\n\nhorizon: The prediction horizon in hours.\nn_samples: The number of samples in the data set.\np_1: The position of the first concept drift.\np_2: The position of the second concept drift.\nposition: The position of the concept drifts.\nn_train: The number of samples used for training.\n\n\nhorizon = 7*24\nn_samples = int(K*100_000)\np_1 = int(K*25_000)\np_2 = int(K*50_000)\nposition=(p_1, p_2)\nn_train = 1_000\n\n\nfrom river.datasets import synth\nimport pandas as pd\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n   seed=123\n)\n\n\nWe will use spotRiver’s convert_to_df function [SOURCE] to convert the river data set to a pandas data frame.\n\n\nfrom spotRiver.utils.data_conversion import convert_to_df\ntarget_column = \"y\"\ndf = convert_to_df(dataset, target_column=target_column, n_total=n_samples)\n\n\nAdd column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column of the dataframe.\nThen split the data frame into a training and test data set. The train and test data sets are stored in the fun_control dictionary.\n\n\nfrom spotPython.hyperparameters.values import set_control_key_value\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\nset_control_key_value(control_dict=fun_control,\n                        key=\"train\",\n                        value=df[:n_train],\n                        replace=True)\nset_control_key_value(fun_control, \"test\", df[n_train:], True)\nset_control_key_value(fun_control, \"n_samples\", n_samples, replace=True)\nset_control_key_value(fun_control, \"target_column\", target_column, replace=True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#specification-of-the-preprocessing-model",
    "href": "025_spot_hpt_river_friedman_amfr.html#specification-of-the-preprocessing-model",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.4 Specification of the Preprocessing Model",
    "text": "21.4 Specification of the Preprocessing Model\n\nWe use the StandardScaler [SOURCE] from river as the preprocessing model. The StandardScaler is used to standardize the data set, i.e., it has zero mean and unit variance.\n\n\nfrom river import preprocessing\nprep_model = preprocessing.StandardScaler()\nset_control_key_value(fun_control, \"prep_model\", prep_model, replace=True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "href": "025_spot_hpt_river_friedman_amfr.html#selectselect-model-algorithm-and-core_model_hyper_dict",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.5 SelectSelect Model (algorithm) and core_model_hyper_dict",
    "text": "21.5 SelectSelect Model (algorithm) and core_model_hyper_dict\nspotPython hyperparameter tuning approach uses two components:\n\na model (class) and\nan associated hyperparameter dictionary.\n\nThe corresponding hyperparameters are loaded from the associated dictionary, which is stored as a JSON file [SOURCE]. The JSON file contains hyperparameter type information, names, and bounds.\nThe method add_core_model_to_fun_control adds the model and the hyperparameter dictionary to the fun_control dictionary.\nAlternatively, you can load a local hyper_dict. Simply set river_hyper_dict.json as the filename. If filenameis set to None, which is the default, the hyper_dict [SOURCE] is loaded from the spotRiver package.\n\nfrom river.forest import AMFRegressor\nfrom spotRiver.data.river_hyper_dict import RiverHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(core_model=AMFRegressor,\n                              fun_control=fun_control,\n                              hyper_dict=RiverHyperDict,\n                              filename=None)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "025_spot_hpt_river_friedman_amfr.html#modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.6 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "21.6 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nAfter the core_model and the core_model_hyper_dict are added to the fun_control dictionary, the hyperparameter tuning can be started. However, in some settings, the user wants to modify the hyperparameters of the core_model_hyper_dict. This can be done with the modify_hyper_parameter_bounds and modify_hyper_parameter_levels functions [SOURCE].\nThe following code shows how hyperparameter of type numeric and integer (boolean) can be modified. The modify_hyper_parameter_bounds function is used to modify the bounds of the hyperparameter delta and merit_preprune. Similar option exists for the modify_hyper_parameter_levels function to modify the levels of categorical hyperparameters.\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n# modify_hyper_parameter_bounds(fun_control, \"n_estimators\", bounds=[2,100])\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\nset_control_hyperparameter_value(fun_control, \"n_estimators\", [2, 100])\n\nSetting hyperparameter n_estimators to value [2, 100].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\n\n\n::: {.callout-note} #### Note: Active and Inactive Hyperparameters Hyperparameters can be excluded from the tuning procedure by selecting identical values for the lower and upper bounds.\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name            | type   |   default |   lower |   upper | transform   |\n|-----------------|--------|-----------|---------|---------|-------------|\n| n_estimators    | int    |        10 |     2   |     100 | None        |\n| step            | float  |         1 |     0.1 |      10 | None        |\n| use_aggregation | factor |         1 |     0   |       1 | None        |",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#selection-of-the-objective-loss-function",
    "href": "025_spot_hpt_river_friedman_amfr.html#selection-of-the-objective-loss-function",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.7 Selection of the Objective (Loss) Function",
    "text": "21.7 Selection of the Objective (Loss) Function\nThe metric_sklearn is used for the sklearn based evaluation via eval_oml_horizon [SOURCE]. Here we use the mean_absolute_error [SOURCE] as the objective function.\n\n\n\n\n\n\nNote: Additional metrics\n\n\n\nspotRiver also supports additional metrics. For example, the metric_river is used for the river based evaluation via eval_oml_iter_progressive [SOURCE]. The metric_river is implemented to simulate the behaviour of the “original” river metrics.\n\n\nspotRiver provides information about the model’ s score (metric), memory, and time. The hyperparamter tuner requires a single objective. Therefore, a weighted sum of the metric, memory, and time is computed. The weights are defined in the weights array.\n\n\n\n\n\n\nNote: Weights\n\n\n\nThe weights provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased.\n\n\nThe oml_grace_period defines the number of observations that are used for the initial training of the model. The step defines the iteration number at which to yield results. This only takes into account the predictions, and not the training steps. The weight_coeff defines a multiplier for the results: results are multiplied by (step/n_steps)**weight_coeff, where n_steps is the total number of iterations. Results from the beginning have a lower weight than results from the end if weight_coeff &gt; 1. If weight_coeff == 0, all results have equal weight. Note, that the weight_coeff is only used internally for the tuner and does not affect the results that are used for the evaluation or comparisons.\n\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\n\nweights = np.array([1, 1/1000, 1/1000])*10_000.0\noml_grace_period = 2\nstep = 100\nweight_coeff = 1.0\n\n# fun_control.update({\n#                \"horizon\": horizon,\n#                \"oml_grace_period\": oml_grace_period,\n#                \"weights\": weights,\n#                \"step\": step,\n#                \"weight_coeff\": weight_coeff,\n#                \"metric_sklearn\": mean_absolute_error\n#                })\nset_control_key_value(control_dict=fun_control,\n                        key=\"horizon\",\n                        value=horizon,\n                        replace=True)\nset_control_key_value(fun_control, \"oml_grace_period\", oml_grace_period, True)\nset_control_key_value(fun_control, \"weights\", weights, True)\nset_control_key_value(fun_control, \"step\", step, True)\nset_control_key_value(fun_control, \"weight_coeff\", weight_coeff, True)\nset_control_key_value(fun_control, \"metric_sklearn\", mean_absolute_error, True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#calling-the-spot-function",
    "href": "025_spot_hpt_river_friedman_amfr.html#calling-the-spot-function",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.8 Calling the SPOT Function",
    "text": "21.8 Calling the SPOT Function\n\n21.8.1 The Objective Function\nThe objective function fun_oml_horizon [SOURCE] is selected next.\n\nfrom spotRiver.fun.hyperriver import HyperRiver\nfun = HyperRiver().fun_oml_horizon\n\nThe following code snippet shows how to get the default hyperparameters as an array, so that they can be passed to the Spot function.\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nX_start = get_default_hyperparameters_as_array(fun_control)\n\n\n\n21.8.2 Run the Spot Optimizer\nThe class Spot [SOURCE] is the hyperparameter tuning workhorse. It is initialized with the following parameters:\n\nfun: the objective function\nfun_control: the dictionary with the control parameters for the objective function\ndesign: the experimental design\ndesign_control: the dictionary with the control parameters for the experimental design\nsurrogate: the surrogate model\nsurrogate_control: the dictionary with the control parameters for the surrogate model\noptimizer: the optimizer\noptimizer_control: the dictionary with the control parameters for the optimizer\n\n\n\n\n\n\n\nNote: Total run time\n\n\n\nThe total run time may exceed the specified max_time, because the initial design (here: init_size = INIT_SIZE as specified above) is always evaluated, even if this takes longer than max_time.\n\n\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init()\nset_control_key_value(control_dict=design_control,\n                        key=\"init_size\",\n                        value=INIT_SIZE,\n                        replace=True)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                           n_theta=2)\n\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                   fun_control=fun_control,\n                   design_control=design_control,\n                   surrogate_control=surrogate_control)\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: 26501.510309156187 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x3896dca50&gt;\n\n\n\n\n21.8.3 TensorBoard\nNow we can start TensorBoard in the background with the following command, where ./runs is the default directory for the TensorBoard log files:\ntensorboard --logdir=\"./runs\"\n\n\n\n\n\n\nTip: TENSORBOARD_PATH\n\n\n\nThe TensorBoard path can be printed with the following command:\n\nfrom spotPython.utils.init import get_tensorboard_path\nget_tensorboard_path(fun_control)\n\n'runs/'\n\n\n\n\nWe can access the TensorBoard web server with the following URL:\nhttp://localhost:6006/\nThe TensorBoard plot illustrates how spotPython can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate \\(\\theta\\) of the Kriging surrogate [SOURCE] is plotted against the number of optimization steps.\n\n\n\nTensorBoard visualization of the spotPython optimization process and the surrogate model.\n\n\n\n\n21.8.4 Results\nAfter the hyperparameter tuning run is finished, the results can be saved and reloaded with the following commands:\n\nfrom spotPython.utils.file import save_pickle,  load_pickle\nfrom spotPython.utils.init import get_experiment_name\nexperiment_name = get_experiment_name(PREFIX)\nSAVE_AND_LOAD = False\nif SAVE_AND_LOAD == True:\n    save_pickle(spot_tuner, experiment_name)\n    spot_tuner = load_pickle(experiment_name)\n\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The black points represent the performace values (score or metric) of hyperparameter configurations from the initial design, whereas the red points represents the hyperparameter configurations found by the surrogate model based optimization.\n\nspot_tuner.plot_progress(log_y=True, filename=\"./figures/\" + experiment_name+\"_progress.pdf\")\n\n\n\n\n\n\n\n\nResults can also be printed in tabular form.\n\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name            | type   |   default |   lower |   upper |            tuned | transform   |   importance | stars   |\n|-----------------|--------|-----------|---------|---------|------------------|-------------|--------------|---------|\n| n_estimators    | int    |      10.0 |     2.0 |     100 |             57.0 | None        |       100.00 | ***     |\n| step            | float  |       1.0 |     0.1 |      10 | 3.71170631585164 | None        |         5.27 | *       |\n| use_aggregation | factor |       1.0 |     0.0 |       1 |              1.0 | None        |         5.27 | *       |\n\n\nA histogram can be used to visualize the most important hyperparameters.\n\nspot_tuner.plot_importance(threshold=0.0025, filename=\"./figures/\" + experiment_name+\"_importance.pdf\")",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#the-larger-data-set",
    "href": "025_spot_hpt_river_friedman_amfr.html#the-larger-data-set",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.9 The Larger Data Set",
    "text": "21.9 The Larger Data Set\nAfter the hyperparameter were tuned on a small data set, we can now apply the hyperparameter configuration to a larger data set. The following code snippet shows how to generate the larger data set.\n\n\n\n\n\n\nCaution: Increased Friedman-Drift Data Set\n\n\n\n\nThe Friedman-Drift Data Set is increased by a factor of two to show the transferability of the hyperparameter tuning results.\nLarger values of K lead to a longer run time.\n\n\n\n\nK = 0.2\nn_samples = int(K*100_000)\np_1 = int(K*25_000)\np_2 = int(K*50_000)\nposition=(p_1, p_2)\n\n\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n   seed=123\n)\n\nThe larger data set is converted to a Pandas data frame and passed to the fun_control dictionary.\n\ndf = convert_to_df(dataset, target_column=target_column, n_total=n_samples)\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\n# fun_control.update({\"train\": df[:n_train],\n#                     \"test\": df[n_train:],\n#                     \"n_samples\": n_samples,\n#                     \"target_column\": target_column})\nset_control_key_value(fun_control, \"train\", df[:n_train], True)\nset_control_key_value(fun_control, \"test\", df[n_train:], True)\nset_control_key_value(fun_control, \"n_samples\", n_samples, True)\nset_control_key_value(fun_control, \"target_column\", target_column, True)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#get-default-hyperparameters",
    "href": "025_spot_hpt_river_friedman_amfr.html#get-default-hyperparameters",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.10 Get Default Hyperparameters",
    "text": "21.10 Get Default Hyperparameters\nThe default hyperparameters, whihc will be used for a comparion with the tuned hyperparameters, can be obtained with the following commands:\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nX_start = get_default_hyperparameters_as_array(fun_control)\nmodel_default = get_one_core_model_from_X(X_start, fun_control, default=True)\n\n\n\n\n\n\n\nNote: spotPython tunes numpy arrays\n\n\n\n\nspotPython tunes numpy arrays, i.e., the hyperparameters are stored in a numpy array.\n\n\n\nThe model with the default hyperparameters can be trained and evaluated with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import eval_oml_horizon\n\ndf_eval_default, df_true_default = eval_oml_horizon(\n                    model=model_default,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\nThe three performance criteria, i.e., scaoe (metric), runtime, and memory consumption, can be visualized with the following commands:\n\nfrom spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\ndf_labels=[\"default\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])\n\n\n\n\n\n\n\n\n\n21.10.1 Show Predictions\n\nSelect a subset of the data set for the visualization of the predictions:\n\nWe use the mean, \\(m\\), of the data set as the center of the visualization.\nWe use 100 data points, i.e., \\(m \\pm 50\\) as the visualization window.\n\n\n\nm = fun_control[\"test\"].shape[0]\na = int(m/2)-50\nb = int(m/2)\n\n\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b]], target_column=target_column,  df_labels=df_labels)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#get-spot-results",
    "href": "025_spot_hpt_river_friedman_amfr.html#get-spot-results",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.11 Get SPOT Results",
    "text": "21.11 Get SPOT Results\nIn a similar way, we can obtain the hyperparameters found by spotPython.\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\n\n\ndf_eval_spot, df_true_spot = eval_oml_horizon(\n                    model=model_spot,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\n\ndf_labels=[\"default\", \"spot\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"], filename=\"./figures/\" + experiment_name+\"_metrics.pdf\")\n\n\n\n\n\n\n\n\n\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b], df_true_spot[a:b]], target_column=target_column,  df_labels=df_labels, filename=\"./figures/\" + experiment_name+\"_predictions.pdf\")\n\n\n\n\n\n\n\n\n\nfrom spotPython.plot.validation import plot_actual_vs_predicted\nplot_actual_vs_predicted(y_test=df_true_default[target_column], y_pred=df_true_default[\"Prediction\"], title=\"Default\")\nplot_actual_vs_predicted(y_test=df_true_spot[target_column], y_pred=df_true_spot[\"Prediction\"], title=\"SPOT\")",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#detailed-hyperparameter-plots",
    "href": "025_spot_hpt_river_friedman_amfr.html#detailed-hyperparameter-plots",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.12 Detailed Hyperparameter Plots",
    "text": "21.12 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nn_estimators:  100.0\nstep:  5.274876204780701\nuse_aggregation:  5.274876204780701\nimpo: [['n_estimators', 100.0], ['step', 5.274876204780701], ['use_aggregation', 5.274876204780701]]\nindices: [0, 1, 2]\nindices after max_imp selection: [0, 1, 2]",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#parallel-coordinates-plots",
    "href": "025_spot_hpt_river_friedman_amfr.html#parallel-coordinates-plots",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.13 Parallel Coordinates Plots",
    "text": "21.13 Parallel Coordinates Plots\n\nspot_tuner.parallel_plot()",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "025_spot_hpt_river_friedman_amfr.html#plot-all-combinations-of-hyperparameters",
    "href": "025_spot_hpt_river_friedman_amfr.html#plot-all-combinations-of-hyperparameters",
    "title": "21  river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data",
    "section": "21.14 Plot all Combinations of Hyperparameters",
    "text": "21.14 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)",
    "crumbs": [
      "Hyperparameter Tuning with River",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>`river` Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span>"
    ]
  },
  {
    "objectID": "030_spot_lightning_data.html",
    "href": "030_spot_lightning_data.html",
    "title": "22  HPT PyTorch Lightning: Data",
    "section": "",
    "text": "22.1 Setup\nimport torch\nfrom spotPython.utils.device import getDevice\nfrom math import inf\nWORKERS = 0\nPREFIX=\"030\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.4",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>HPT PyTorch Lightning: Data</span>"
    ]
  },
  {
    "objectID": "030_spot_lightning_data.html#sec-setup-30",
    "href": "030_spot_lightning_data.html#sec-setup-30",
    "title": "22  HPT PyTorch Lightning: Data",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>HPT PyTorch Lightning: Data</span>"
    ]
  },
  {
    "objectID": "030_spot_lightning_data.html#initialization-of-the-fun_control-dictionary",
    "href": "030_spot_lightning_data.html#initialization-of-the-fun_control-dictionary",
    "title": "22  HPT PyTorch Lightning: Data",
    "section": "22.2 Initialization of the fun_control Dictionary",
    "text": "22.2 Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    _torchmetric=\"mean_squared_error\",\n    PREFIX=PREFIX,\n    device=DEVICE,\n    enable_progress_bar=False,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=TEST_SIZE,\n    )\n\nCreated spot_tensorboard_path: runs/spot_logs/030_p040025_2024-03-17_21-59-54 for SummaryWriter()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>HPT PyTorch Lightning: Data</span>"
    ]
  },
  {
    "objectID": "030_spot_lightning_data.html#loading-the-diabetes-data-set",
    "href": "030_spot_lightning_data.html#loading-the-diabetes-data-set",
    "title": "22  HPT PyTorch Lightning: Data",
    "section": "22.3 Loading the Diabetes Data Set",
    "text": "22.3 Loading the Diabetes Data Set\nHere, we load the Diabetes data set from spotPython’s data module.\n\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes(target_type=torch.float)\nprint(len(dataset))\n\n442\n\n\n\n22.3.1 Data Set and Data Loader\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,\n          0.0199, -0.0176],\n        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,\n         -0.0683, -0.0922],\n        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,\n          0.0029, -0.0259],\n        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,\n          0.0227, -0.0094],\n        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,\n         -0.0320, -0.0466]])\nTargets: tensor([151.,  75., 141., 206., 135.])\n\n\n\n\n22.3.2 Preparing Training, Validation, and Test Data\nThe following code shows how to split the data into training, validation, and test sets. Then a Lightning Trainer is used to train (fit) the model, validate it, and test it.\n\nfrom torch.utils.data import DataLoader\nfrom spotPython.data.diabetes import Diabetes\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom torch import nn\nimport lightning as L\nimport torch\nBATCH_SIZE = 8\ndataset = Diabetes(target_type=torch.float)\ntrain1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\ntrain_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\nval_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\nbatch_x, batch_y = next(iter(train_loader))\nprint(f\"batch_x.shape: {batch_x.shape}\")\nprint(f\"batch_y.shape: {batch_y.shape}\")\nnet_light_base = NetLightRegression(l1=128,\n                                    epochs=10,\n                                    batch_size=BATCH_SIZE,\n                                    initialization='Default',\n                                    act_fn=nn.ReLU(),\n                                    optimizer='Adam',\n                                    dropout_prob=0.1,\n                                    lr_mult=0.1,\n                                    patience=5,\n                                    _L_in=10,\n                                    _L_out=1,\n                                    _torchmetric=\"mean_squared_error\")\ntrainer = L.Trainer(max_epochs=10,  enable_progress_bar=False)\ntrainer.fit(net_light_base, train_loader)\ntrainer.validate(net_light_base, val_loader)\ntrainer.test(net_light_base, test_loader)\n\nbatch_x.shape: torch.Size([8, 10])\nbatch_y.shape: torch.Size([8])\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      32421.490234375      │\n│         val_loss          │      32421.490234375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       25781.0234375       │\n│         val_loss          │       25781.0234375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n[{'val_loss': 25781.0234375, 'hp_metric': 25781.0234375}]\n\n\n\n\n22.3.3 Dataset for spotPython\nspotPython handles the data set, which is added to the fun_control dictionary with the key data_set as follows:\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes(target_type=torch.float)\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n442\n\n\nIf the data set is in the fun_control dictionary, it is used to create a LightDataModule object. This object is used to create the data loaders for the training, validation, and test sets. Therefore, the following information must be provided in the fun_control dictionary:\n\ndata_set: the data set\nbatch_size: the batch size\nnum_workers: the number of workers\ntest_size: the size of the test set\ntest_seed: the seed for the test set\n\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfun_control = fun_control_init(\n    data_set=dataset,\n    device=\"cpu\",\n    enable_progress_bar=False,\n    num_workers=0,\n    show_progress=True,\n    test_size=0.4,\n    test_seed=42,    \n    )\n\n\nfrom spotPython.data.lightdatamodule import LightDataModule\ndm = LightDataModule(\n    dataset=fun_control[\"data_set\"],\n    batch_size=8,\n    num_workers=fun_control[\"num_workers\"],\n    test_size=fun_control[\"test_size\"],\n    test_seed=fun_control[\"test_seed\"],\n)\ndm.setup()\nprint(f\"train_model(): Test set size: {len(dm.data_test)}\")\nprint(f\"train_model(): Train set size: {len(dm.data_train)}\")\n\nLightDataModule.setup(): stage: None\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\ntest_size: 0.4 used for test dataset.\ntest_size: 0.4 used for predict dataset.\ntrain_model(): Test set size: 177\ntrain_model(): Train set size: 160",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>HPT PyTorch Lightning: Data</span>"
    ]
  },
  {
    "objectID": "030_spot_lightning_data.html#the-lightdatamodule",
    "href": "030_spot_lightning_data.html#the-lightdatamodule",
    "title": "22  HPT PyTorch Lightning: Data",
    "section": "22.4 The LightDataModule",
    "text": "22.4 The LightDataModule\nThe steps described above are handled by the LightDataModule class. This class is used to create the data loaders for the training, validation, and test sets. The LightDataModule class is part of the spotPython package. The LightDataModule class provides the following methods:\n\nprepare_data(): This method is used to prepare the data set.\nsetup(): This method is used to create the data loaders for the training, validation, and test sets.\ntrain_dataloader(): This method is used to return the data loader for the training set.\nval_dataloader(): This method is used to return the data loader for the validation set.\ntest_dataloader(): This method is used to return the data loader for the test set.\npredict_dataloader(): This method is used to return the data loader for the prediction set.\n\n\n22.4.1 The prepare_data() Method\nThe prepare_data() method is used to prepare the data set. This method is called only once and on a single process. It can be used to download the data set. In our case, the data set is already available, so this method uses a simple pass statement.\n\n\n22.4.2 The setup() Method\nSplits the data for use in training, validation, and testing. It uses torch.utils.data.random_split() to split the data. Splitting is based on the test_size and test_seed. The test_size can be a float or an int.\n\n22.4.2.1 Determine the Sizes of the Data Sets\n\nfrom torch.utils.data import random_split\ndata_full = dataset\ntest_size = fun_control[\"test_size\"]\ntest_seed=fun_control[\"test_seed\"]\n# if test_size is float, then train_size is 1 - test_size\nif isinstance(test_size, float):\n    full_train_size = round(1.0 - test_size, 2)\n    val_size = round(full_train_size * test_size, 2)\n    train_size = round(full_train_size - val_size, 2)\nelse:\n    # if test_size is int, then train_size is len(data_full) - test_size\n    full_train_size = len(data_full) - test_size\n    val_size = int(full_train_size * test_size / len(data_full))\n    train_size = full_train_size - val_size\n\nprint(f\"LightDataModule setup(): full_train_size: {full_train_size}\")\nprint(f\"LightDataModule setup(): val_size: {val_size}\")\nprint(f\"LightDataModule setup(): train_size: {train_size}\")\nprint(f\"LightDataModule setup(): test_size: {test_size}\")\n\nLightDataModule setup(): full_train_size: 0.6\nLightDataModule setup(): val_size: 0.24\nLightDataModule setup(): train_size: 0.36\nLightDataModule setup(): test_size: 0.4\n\n\nstage is used to define the data set to be returned. The stage can be None, fit, test, or predict. If stage is None, the method returns the training (fit), testing (test) and prediction (predict) data sets.\n\n\n22.4.2.2 Stage “fit”\n\nstage = \"fit\"\nif stage == \"fit\" or stage is None:\n    generator_fit = torch.Generator().manual_seed(test_seed)\n    data_train, data_val, _ = random_split(data_full, [train_size, val_size, test_size], generator=generator_fit)\nprint(f\"LightDataModule setup(): Train set size: {len(data_train)}\")\nprint(f\"LightDataModule setup(): Validation set size: {len(data_val)}\")\n\nLightDataModule setup(): Train set size: 160\nLightDataModule setup(): Validation set size: 106\n\n\n\n\n22.4.2.3 Stage “test”\n\nstage = \"test\"\nif stage == \"test\" or stage is None:\n    generator_test = torch.Generator().manual_seed(test_seed)\n    data_test, _ = random_split(data_full, [test_size, full_train_size], generator=generator_test)\nprint(f\"LightDataModule setup(): Test set size: {len(data_test)}\")\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nLightDataModule setup(): Test set size: 177\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,\n         -0.0283,  0.0445],\n        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,\n         -0.0225,  0.0072],\n        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,\n         -0.0741, -0.0591],\n        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,\n         -0.0514, -0.0591],\n        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,\n         -0.0181, -0.0135]])\nTargets: tensor([158.,  49., 142.,  96.,  59.])\n\n\n\n\n22.4.2.4 Stage “predict”\nPrediction and testing use the same data set.\n\nstage = \"predict\"\nif stage == \"predict\" or stage is None:\n    generator_predict = torch.Generator().manual_seed(test_seed)\n    data_predict, _ = random_split(\n        data_full, [test_size, full_train_size], generator=generator_predict\n    )\nprint(f\"LightDataModule setup(): Predict set size: {len(data_predict)}\")\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(data_predict, batch_size=batch_size, shuffle=False)\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nLightDataModule setup(): Predict set size: 177\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,\n         -0.0283,  0.0445],\n        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,\n         -0.0225,  0.0072],\n        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,\n         -0.0741, -0.0591],\n        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,\n         -0.0514, -0.0591],\n        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,\n         -0.0181, -0.0135]])\nTargets: tensor([158.,  49., 142.,  96.,  59.])\n\n\n\n\n\n22.4.3 The train_dataloader() Method\nReturns the training dataloader, i.e., a Pytorch DataLoader instance using the training dataset. It simply returns a DataLoader with the data_train set that was created in the setup() method as described in Section 22.4.2.2.\n\ndef train_dataloader(self) -&gt; DataLoader:\n    return DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers)\n\nThe train_dataloader() method can be used as follows:\n\nfrom spotPython.data.lightdatamodule import LightDataModule\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes(target_type=torch.float)\ndata_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\ndata_module.setup()\nprint(f\"Training set size: {len(data_module.data_train)}\")\ndl = data_module.train_dataloader()\n# Iterate over the data in the DataLoader\nfor batch in dl:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nLightDataModule.setup(): stage: None\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\ntest_size: 0.4 used for test dataset.\ntest_size: 0.4 used for predict dataset.\nTraining set size: 160\nLightDataModule.train_dataloader(). data_train size: 160\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,\n         -0.0283,  0.0445],\n        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,\n         -0.0225,  0.0072],\n        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,\n         -0.0741, -0.0591],\n        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,\n         -0.0514, -0.0591],\n        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,\n         -0.0181, -0.0135]])\nTargets: tensor([158.,  49., 142.,  96.,  59.])\n\n\n\n\n22.4.4 The val_dataloader() Method\nReturns the validation dataloader, i.e., a Pytorch DataLoader instance using the validation dataset. It simply returns a DataLoader with the data_val set that was created in the setup() method as desccribed in Section 22.4.2.2.\n\ndef val_dataloader(self) -&gt; DataLoader:\n    return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=self.num_workers)\n\nThe val_dataloader() method can be used as follows:\n\nfrom spotPython.data.lightdatamodule import LightDataModule\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes(target_type=torch.float)\ndata_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\ndata_module.setup()\nprint(f\"Validation set size: {len(data_module.data_val)}\")\ndl = data_module.val_dataloader()\n# Iterate over the data in the DataLoader\nfor batch in dl:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nLightDataModule.setup(): stage: None\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\ntest_size: 0.4 used for test dataset.\ntest_size: 0.4 used for predict dataset.\nValidation set size: 106\nLightDataModule.val_dataloader(). Val. set size: 106\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0163, -0.0446,  0.0736, -0.0412, -0.0043, -0.0135, -0.0139, -0.0011,\n          0.0429,  0.0445],\n        [ 0.0453, -0.0446,  0.0714,  0.0012, -0.0098, -0.0010,  0.0155, -0.0395,\n         -0.0412, -0.0715],\n        [ 0.0308,  0.0507,  0.0326,  0.0494, -0.0401, -0.0436, -0.0692,  0.0343,\n          0.0630,  0.0031],\n        [ 0.0235,  0.0507, -0.0396, -0.0057, -0.0484, -0.0333,  0.0118, -0.0395,\n         -0.1016, -0.0674],\n        [-0.0091,  0.0507,  0.0013, -0.0022,  0.0796,  0.0701,  0.0339, -0.0026,\n          0.0267,  0.0818]])\nTargets: tensor([275., 141., 208.,  78., 142.])\n\n\n\n\n22.4.5 The test_dataloader() Method\nReturns the test dataloader, i.e., a Pytorch DataLoader instance using the test dataset. It simply returns a DataLoader with the data_test set that was created in the setup() method as described in Section 22.4.2.3.\n\ndef test_dataloader(self) -&gt; DataLoader:\n    return DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers)\n\nThe test_dataloader() method can be used as follows:\n\nfrom spotPython.data.lightdatamodule import LightDataModule\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes(target_type=torch.float)\ndata_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\ndata_module.setup()\nprint(f\"Test set size: {len(data_module.data_test)}\")\ndl = data_module.test_dataloader()\n# Iterate over the data in the DataLoader\nfor batch in dl:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nLightDataModule.setup(): stage: None\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\ntest_size: 0.4 used for test dataset.\ntest_size: 0.4 used for predict dataset.\nTest set size: 177\nLightDataModule.test_dataloader(). Test set size: 177\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,\n         -0.0283,  0.0445],\n        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,\n         -0.0225,  0.0072],\n        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,\n         -0.0741, -0.0591],\n        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,\n         -0.0514, -0.0591],\n        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,\n         -0.0181, -0.0135]])\nTargets: tensor([158.,  49., 142.,  96.,  59.])\n\n\n\n\n22.4.6 The predict_dataloader() Method\nReturns the prediction dataloader, i.e., a Pytorch DataLoader instance using the prediction dataset. It simply returns a DataLoader with the data_predict set that was created in the setup() method as described in Section 22.4.2.4.\n\n\n\n\n\n\nWarning\n\n\n\nThe batch_size is set to the length of the data_predict set.\n\n\n\ndef predict_dataloader(self) -&gt; DataLoader:\n    return DataLoader(self.data_predict, batch_size=len(self.data_predict), num_workers=self.num_workers)\n\nThe predict_dataloader() method can be used as follows:\n\nfrom spotPython.data.lightdatamodule import LightDataModule\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes(target_type=torch.float)\ndata_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\ndata_module.setup()\nprint(f\"Test set size: {len(data_module.data_predict)}\")\ndl = data_module.predict_dataloader()\n# Iterate over the data in the DataLoader\nfor batch in dl:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nLightDataModule.setup(): stage: None\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\ntest_size: 0.4 used for test dataset.\ntest_size: 0.4 used for predict dataset.\nTest set size: 177\nLightDataModule.predict_dataloader(). Predict set size: 177\nBatch Size: 177\nInputs Shape: torch.Size([177, 10])\nTargets Shape: torch.Size([177])\n---------------\nInputs: tensor([[ 0.0562, -0.0446, -0.0579,  ..., -0.0214, -0.0283,  0.0445],\n        [ 0.0018, -0.0446, -0.0709,  ..., -0.0395, -0.0225,  0.0072],\n        [-0.0527, -0.0446,  0.0542,  ..., -0.0395, -0.0741, -0.0591],\n        ...,\n        [ 0.0090, -0.0446, -0.0321,  ..., -0.0764, -0.0119, -0.0384],\n        [-0.0273, -0.0446, -0.0666,  ..., -0.0395, -0.0358, -0.0094],\n        [ 0.0817,  0.0507,  0.0067,  ...,  0.0919,  0.0547,  0.0072]])\nTargets: tensor([158.,  49., 142.,  96.,  59.,  74., 137., 136.,  39.,  66., 310., 198.,\n        235., 116.,  55., 177.,  59., 246.,  53., 135.,  88., 198., 186., 217.,\n         51., 118., 153., 180.,  51., 229.,  84.,  72., 237., 142., 185.,  91.,\n         88., 148., 179., 144.,  25.,  89.,  42.,  60., 124., 170., 215., 263.,\n        178., 245., 202.,  97., 321.,  71., 123., 220., 132., 243.,  61., 102.,\n        187.,  70., 242., 134.,  63.,  72.,  88., 219., 127., 146., 122., 143.,\n        220., 293.,  59., 317.,  60., 140.,  65., 277.,  90.,  96., 109., 190.,\n         90.,  52., 160., 233., 230., 175.,  68., 272., 144.,  70.,  68., 163.,\n         71.,  93., 263., 118., 220.,  90., 232., 120., 163.,  88.,  85.,  52.,\n        181., 232., 212., 332.,  81., 214., 145., 268., 115.,  93.,  64., 156.,\n        128., 200., 281., 103., 220.,  66.,  48., 246.,  42., 150., 125., 109.,\n        129.,  97., 265.,  97., 173., 216., 237., 121.,  42., 151.,  31.,  68.,\n        137., 221., 283., 124., 243., 150.,  69., 306., 182., 252., 132., 258.,\n        121., 110., 292., 101., 275., 141., 208.,  78., 142., 185., 167., 258.,\n        144.,  89., 225., 140., 303., 236.,  87.,  77., 131.])",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>HPT PyTorch Lightning: Data</span>"
    ]
  },
  {
    "objectID": "030_spot_lightning_data.html#using-the-lightdatamodule-in-the-train_model-method",
    "href": "030_spot_lightning_data.html#using-the-lightdatamodule-in-the-train_model-method",
    "title": "22  HPT PyTorch Lightning: Data",
    "section": "22.5 Using the LightDataModule in the train_model() Method",
    "text": "22.5 Using the LightDataModule in the train_model() Method\nFirst, a LightDataModule object is created and the setup() method is called.\n\ndm = LightDataModule(\n    dataset=fun_control[\"data_set\"],\n    batch_size=config[\"batch_size\"],\n    num_workers=fun_control[\"num_workers\"],\n    test_size=fun_control[\"test_size\"],\n    test_seed=fun_control[\"test_seed\"],\n)\ndm.setup()\n\nThen, the Trainer is initialized.\n\n# Init trainer\ntrainer = L.Trainer(\n    default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n    max_epochs=model.hparams.epochs,\n    accelerator=fun_control[\"accelerator\"],\n    devices=fun_control[\"devices\"],\n    logger=TensorBoardLogger(\n        save_dir=fun_control[\"TENSORBOARD_PATH\"],\n        version=config_id,\n        default_hp_metric=True,\n        log_graph=fun_control[\"log_graph\"],\n    ),\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n    ],\n    enable_progress_bar=enable_progress_bar,\n)\n\nNext, the fit() method is called to train the model.\n\n# Pass the datamodule as arg to trainer.fit to override model hooks :)\ntrainer.fit(model=model, datamodule=dm)\n\nFinally, the validate() method is called to validate the model. The validate() method returns the validation loss.\n\n# Test best model on validation and test set\n# result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\nresult = trainer.validate(model=model, datamodule=dm)\n# unlist the result (from a list of one dict)\nresult = result[0]\nreturn result[\"val_loss\"]",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>HPT PyTorch Lightning: Data</span>"
    ]
  },
  {
    "objectID": "030_spot_lightning_data.html#further-information",
    "href": "030_spot_lightning_data.html#further-information",
    "title": "22  HPT PyTorch Lightning: Data",
    "section": "22.6 Further Information",
    "text": "22.6 Further Information\n\n22.6.1 Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>HPT PyTorch Lightning: Data</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html",
    "href": "031_spot_lightning_linear_diabetes.html",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "",
    "text": "23.1 Step 1: Setup\nfrom spotPython.utils.device import getDevice\nfrom math import inf\n\nMAX_TIME = 1\nFUN_EVALS = inf\nINIT_SIZE = 5\nWORKERS = 0\nPREFIX=\"031\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.1\nTORCH_METRIC = \"mean_squared_error\"",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-setup-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-setup-31",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter MAX_TIME specifies the maximum run time in seconds.\nThe parameter INIT_SIZE specifies the initial design size.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "031_spot_lightning_linear_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "23.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    _torchmetric=TORCH_METRIC,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    log_level=10,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=0.1,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_03_17_22_00_07\nCreated spot_tensorboard_path: runs/spot_logs/031_p040025_2024-03-17_22-00-07 for SummaryWriter()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-3-loading-the-diabetes-data-set",
    "href": "031_spot_lightning_linear_diabetes.html#step-3-loading-the-diabetes-data-set",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.3 Step 3: Loading the Diabetes Data Set",
    "text": "23.3 Step 3: Loading the Diabetes Data Set\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes()\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n442\n\n\n\n\n\n\n\n\nNote: Data Set and Data Loader\n\n\n\n\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,\n          0.0199, -0.0176],\n        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,\n         -0.0683, -0.0922],\n        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,\n          0.0029, -0.0259],\n        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,\n          0.0227, -0.0094],\n        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,\n         -0.0320, -0.0466]])\nTargets: tensor([151.,  75., 141., 206., 135.])",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-preprocessing-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-preprocessing-31",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.4 Step 4: Preprocessing",
    "text": "23.4 Step 4: Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-selection-of-the-algorithm-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-selection-of-the-algorithm-31",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict",
    "text": "23.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\nHere we simply add the NN Model to the fun_control dictionary by calling the function add_core_model_to_fun_control:\n\n\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=NetLightRegression,\n                              hyper_dict=LightHyperDict)\n\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-modification-of-hyperparameters-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-modification-of-hyperparameters-31",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "23.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9]) and\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"l1\", [4, 6])\nset_control_hyperparameter_value(fun_control, \"epochs\", [9, 10])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adadelta\",\n                \"Adagrad\",\n                \"Adam\",\n                \"AdamW\",\n                \"Adamax\",                \n                \"NAdam\",\n                \"RAdam\",\n                \"RMSprop\",\n                \"Rprop\"\n            ])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [5, 7])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\n                \"Sigmoid\",\n                \"ReLU\",\n                \"LeakyReLU\",\n                \"Swish\"\n            ] )\n\nSetting hyperparameter l1 to value [4, 6].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter epochs to value [9, 10].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter batch_size to value [4, 5].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter optimizer to value ['Adadelta', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'NAdam', 'RAdam', 'RMSprop', 'Rprop'].\nVariable type is factor.\nCore type is str.\nCalling modify_hyper_parameter_levels().\nSetting hyperparameter dropout_prob to value [0.01, 0.1].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter lr_mult to value [0.5, 5.0].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter patience to value [5, 7].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter act_fn to value ['Sigmoid', 'ReLU', 'LeakyReLU', 'Swish'].\nVariable type is factor.\nCore type is instance().\nCalling modify_hyper_parameter_levels().\n\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |    4    |     6   | transform_power_2_int |\n| epochs         | int    | 4         |    9    |    10   | transform_power_2_int |\n| batch_size     | int    | 4         |    4    |     5   | transform_power_2_int |\n| act_fn         | factor | ReLU      |    0    |     3   | None                  |\n| optimizer      | factor | SGD       |    0    |     8   | None                  |\n| dropout_prob   | float  | 0.01      |    0.01 |     0.1 | None                  |\n| lr_mult        | float  | 1.0       |    0.5  |     5   | None                  |\n| patience       | int    | 2         |    5    |     7   | transform_power_2_int |\n| initialization | factor | Default   |    0    |     2   | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n\n\n\n\nNote: Hyperparameters of the Tuned Model and the fun_control Dictionary\n\n\n\nThe updated fun_control dictionary can be shown with the command fun_control[\"core_model_hyper_dict\"].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "031_spot_lightning_linear_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "23.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n23.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n23.7.2 Loss Function\nThe loss function is specified in the configurable network class [SOURCE] We will use MSE.\n\n\n23.7.3 Metric\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#step-8-calling-the-spot-function",
    "href": "031_spot_lightning_linear_diabetes.html#step-8-calling-the-spot-function",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.8 Step 8: Calling the SPOT Function",
    "text": "23.8 Step 8: Calling the SPOT Function\n\n23.8.1 Preparing the SPOT Call\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init(init_size=INIT_SIZE)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                            n_theta=2)\n\n\n\n\n\n\n\nNote: Modifying Values in the Control Dictionaries\n\n\n\n\nThe values in the control dictionaries can be modified with the function set_control_key_value [SOURCE], for example:\n\nset_control_key_value(control_dict=surrogate_control,\n                        key=\"noise\",\n                        value=True,\n                        replace=True)                       \nset_control_key_value(control_dict=surrogate_control,\n                        key=\"n_theta\",\n                        value=2,\n                        replace=True)      \n\n\n\n\n\n23.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=50).fun\n\n\n\n23.8.3 Showing the fun_control Dictionary\n\nimport pprint\npprint.pprint(fun_control)\n\n{'CHECKPOINT_PATH': 'runs/saved_models/',\n 'DATASET_PATH': 'data/',\n 'PREFIX': '031',\n 'RESULTS_PATH': 'results/',\n 'TENSORBOARD_PATH': 'runs/',\n '_L_in': 10,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'accelerator': 'auto',\n 'converters': None,\n 'core_model': &lt;class 'spotPython.light.regression.netlightregression.NetLightRegression'&gt;,\n 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                      'core_model_parameter_type': 'instance()',\n                                      'default': 'ReLU',\n                                      'levels': ['Sigmoid',\n                                                 'ReLU',\n                                                 'LeakyReLU',\n                                                 'Swish'],\n                                      'lower': 0,\n                                      'transform': 'None',\n                                      'type': 'factor',\n                                      'upper': 3},\n                           'batch_size': {'default': 4,\n                                          'lower': 4,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 5},\n                           'dropout_prob': {'default': 0.01,\n                                            'lower': 0.01,\n                                            'transform': 'None',\n                                            'type': 'float',\n                                            'upper': 0.1},\n                           'epochs': {'default': 4,\n                                      'lower': 9,\n                                      'transform': 'transform_power_2_int',\n                                      'type': 'int',\n                                      'upper': 10},\n                           'initialization': {'core_model_parameter_type': 'str',\n                                              'default': 'Default',\n                                              'levels': ['Default',\n                                                         'Kaiming',\n                                                         'Xavier'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 2},\n                           'l1': {'default': 3,\n                                  'lower': 4,\n                                  'transform': 'transform_power_2_int',\n                                  'type': 'int',\n                                  'upper': 6},\n                           'lr_mult': {'default': 1.0,\n                                       'lower': 0.5,\n                                       'transform': 'None',\n                                       'type': 'float',\n                                       'upper': 5.0},\n                           'optimizer': {'class_name': 'torch.optim',\n                                         'core_model_parameter_type': 'str',\n                                         'default': 'SGD',\n                                         'levels': ['Adadelta',\n                                                    'Adagrad',\n                                                    'Adam',\n                                                    'AdamW',\n                                                    'Adamax',\n                                                    'NAdam',\n                                                    'RAdam',\n                                                    'RMSprop',\n                                                    'Rprop'],\n                                         'lower': 0,\n                                         'transform': 'None',\n                                         'type': 'factor',\n                                         'upper': 8},\n                           'patience': {'default': 2,\n                                        'lower': 5,\n                                        'transform': 'transform_power_2_int',\n                                        'type': 'int',\n                                        'upper': 7}},\n 'core_model_hyper_dict_default': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                              'core_model_parameter_type': 'instance()',\n                                              'default': 'ReLU',\n                                              'levels': ['Sigmoid',\n                                                         'Tanh',\n                                                         'ReLU',\n                                                         'LeakyReLU',\n                                                         'ELU',\n                                                         'Swish'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 5},\n                                   'batch_size': {'default': 4,\n                                                  'lower': 1,\n                                                  'transform': 'transform_power_2_int',\n                                                  'type': 'int',\n                                                  'upper': 4},\n                                   'dropout_prob': {'default': 0.01,\n                                                    'lower': 0.0,\n                                                    'transform': 'None',\n                                                    'type': 'float',\n                                                    'upper': 0.25},\n                                   'epochs': {'default': 4,\n                                              'lower': 4,\n                                              'transform': 'transform_power_2_int',\n                                              'type': 'int',\n                                              'upper': 9},\n                                   'initialization': {'core_model_parameter_type': 'str',\n                                                      'default': 'Default',\n                                                      'levels': ['Default',\n                                                                 'Kaiming',\n                                                                 'Xavier'],\n                                                      'lower': 0,\n                                                      'transform': 'None',\n                                                      'type': 'factor',\n                                                      'upper': 2},\n                                   'l1': {'default': 3,\n                                          'lower': 3,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 8},\n                                   'lr_mult': {'default': 1.0,\n                                               'lower': 0.1,\n                                               'transform': 'None',\n                                               'type': 'float',\n                                               'upper': 10.0},\n                                   'optimizer': {'class_name': 'torch.optim',\n                                                 'core_model_parameter_type': 'str',\n                                                 'default': 'SGD',\n                                                 'levels': ['Adadelta',\n                                                            'Adagrad',\n                                                            'Adam',\n                                                            'AdamW',\n                                                            'SparseAdam',\n                                                            'Adamax',\n                                                            'ASGD',\n                                                            'NAdam',\n                                                            'RAdam',\n                                                            'RMSprop',\n                                                            'Rprop',\n                                                            'SGD'],\n                                                 'lower': 0,\n                                                 'transform': 'None',\n                                                 'type': 'factor',\n                                                 'upper': 11},\n                                   'patience': {'default': 2,\n                                                'lower': 2,\n                                                'transform': 'transform_power_2_int',\n                                                'type': 'int',\n                                                'upper': 6}},\n 'core_model_name': None,\n 'counter': 0,\n 'data': None,\n 'data_dir': './data',\n 'data_module': None,\n 'data_set': &lt;spotPython.data.diabetes.Diabetes object at 0x36b94ed50&gt;,\n 'data_set_name': None,\n 'design': None,\n 'device': 'mps',\n 'devices': 1,\n 'enable_progress_bar': False,\n 'eval': None,\n 'fun_evals': inf,\n 'fun_repeats': 1,\n 'horizon': None,\n 'infill_criterion': 'y',\n 'k_folds': 3,\n 'log_graph': False,\n 'log_level': 10,\n 'loss_function': None,\n 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),\n 'max_surrogate_points': 30,\n 'max_time': 1,\n 'metric_params': {},\n 'metric_river': None,\n 'metric_sklearn': None,\n 'metric_torch': None,\n 'model_dict': {},\n 'n_points': 1,\n 'n_samples': None,\n 'n_total': None,\n 'noise': False,\n 'num_workers': 0,\n 'ocba_delta': 0,\n 'oml_grace_period': None,\n 'optimizer': None,\n 'path': None,\n 'prep_model': None,\n 'save_model': False,\n 'seed': 123,\n 'show_batch_interval': 1000000,\n 'show_models': False,\n 'show_progress': True,\n 'shuffle': None,\n 'sigma': 0.0,\n 'spot_tensorboard_path': 'runs/spot_logs/031_p040025_2024-03-17_22-00-07',\n 'spot_writer': &lt;torch.utils.tensorboard.writer.SummaryWriter object at 0x155e17b10&gt;,\n 'target_column': None,\n 'task': None,\n 'test': None,\n 'test_seed': 1234,\n 'test_size': 0.1,\n 'tolerance_x': 1.4901161193847656e-08,\n 'train': None,\n 'upper': array([ 8.  ,  9.  ,  4.  ,  5.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),\n 'var_name': ['l1',\n              'epochs',\n              'batch_size',\n              'act_fn',\n              'optimizer',\n              'dropout_prob',\n              'lr_mult',\n              'patience',\n              'initialization'],\n 'var_type': ['int',\n              'int',\n              'int',\n              'factor',\n              'factor',\n              'float',\n              'float',\n              'int',\n              'factor'],\n 'verbosity': 0,\n 'weight_coeff': 0.0,\n 'weights': 1.0}\n\n\n\n\n23.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE].\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2462.921875, 'hp_metric': 2462.921875}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2856.306884765625, 'hp_metric': 2856.306884765625}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 3388.148193359375, 'hp_metric': 3388.148193359375}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 5114.5771484375, 'hp_metric': 5114.5771484375}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2705.722412109375, 'hp_metric': 2705.722412109375}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2662.486083984375, 'hp_metric': 2662.486083984375}\nspotPython tuning: 2462.921875 [#####-----] 51.92% \nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2573.8544921875, 'hp_metric': 2573.8544921875}\nspotPython tuning: 2462.921875 [##########] 100.00% Done...\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │        2462.921875        │\n│         val_loss          │        2462.921875        │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2856.306884765625     │\n│         val_loss          │     2856.306884765625     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3388.148193359375     │\n│         val_loss          │     3388.148193359375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      5114.5771484375      │\n│         val_loss          │      5114.5771484375      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2705.722412109375     │\n│         val_loss          │     2705.722412109375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2662.486083984375     │\n│         val_loss          │     2662.486083984375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      2573.8544921875      │\n│         val_loss          │      2573.8544921875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x36ebba910&gt;",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-tensorboard-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-tensorboard-31",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.9 Step 9: Tensorboard",
    "text": "23.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "031_spot_lightning_linear_diabetes.html#sec-results-31",
    "href": "031_spot_lightning_linear_diabetes.html#sec-results-31",
    "title": "23  HPT PyTorch Lightning: Diabetes",
    "section": "23.10 Step 10: Results",
    "text": "23.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX +\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     4.0 |     6.0 | 6.0                 | transform_power_2_int |         0.01 |         |\n| epochs         | int    | 4         |     9.0 |    10.0 | 10.0                | transform_power_2_int |         0.01 |         |\n| batch_size     | int    | 4         |     4.0 |     5.0 | 5.0                 | transform_power_2_int |       100.00 | ***     |\n| act_fn         | factor | ReLU      |     0.0 |     3.0 | ReLU                | None                  |         9.12 | *       |\n| optimizer      | factor | SGD       |     0.0 |     8.0 | AdamW               | None                  |         0.01 |         |\n| dropout_prob   | float  | 0.01      |    0.01 |     0.1 | 0.04938229888019609 | None                  |         0.01 |         |\n| lr_mult        | float  | 1.0       |     0.5 |     5.0 | 2.3689895017756495  | None                  |         0.15 | .       |\n| patience       | int    | 2         |     5.0 |     7.0 | 6.0                 | transform_power_2_int |         0.01 |         |\n| initialization | factor | Default   |     0.0 |     2.0 | Default             | None                  |         0.01 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + PREFIX + \"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n23.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nprint(config)\n\n{'l1': 64, 'epochs': 1024, 'batch_size': 32, 'act_fn': ReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.04938229888019609, 'lr_mult': 2.3689895017756495, 'patience': 64, 'initialization': 'Default'}\n\n\n\nTest on the full data set\n\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.TESTING\ntest_size: 0.1 used for test dataset.\nLightDataModule.test_dataloader(). Test set size: 45\ntest_model result: {'val_loss': 2958.849609375, 'hp_metric': 2958.849609375}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      2958.849609375       │\n│         val_loss          │      2958.849609375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n(2958.849609375, 2958.849609375)\n\n\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\n\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 64, 'epochs': 1024, 'batch_size': 32, 'act_fn': ReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.04938229888019609, 'lr_mult': 2.3689895017756495, 'patience': 64, 'initialization': 'Default'}\nLoading model with 64_1024_32_ReLU_AdamW_0.0494_2.369_64_Default_TEST from runs/saved_models/64_1024_32_ReLU_AdamW_0.0494_2.369_64_Default_TEST/last.ckpt\nModel: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.04938229888019609, inplace=False)\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.04938229888019609, inplace=False)\n    (6): Linear(in_features=32, out_features=32, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.04938229888019609, inplace=False)\n    (9): Linear(in_features=32, out_features=16, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.04938229888019609, inplace=False)\n    (12): Linear(in_features=16, out_features=1, bias=True)\n  )\n)\n\n\n\nfilename = \"./figures/\" + PREFIX\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  0.006487185715991213\nepochs:  0.006487185715991213\nbatch_size:  100.0\nact_fn:  9.11790565173128\noptimizer:  0.006487185715991213\ndropout_prob:  0.006487185715991213\nlr_mult:  0.1529052762416755\npatience:  0.006487185715991213\ninitialization:  0.006487185715991213\nimpo: [['l1', 0.006487185715991213], ['epochs', 0.006487185715991213], ['batch_size', 100.0], ['act_fn', 9.11790565173128], ['optimizer', 0.006487185715991213], ['dropout_prob', 0.006487185715991213], ['lr_mult', 0.1529052762416755], ['patience', 0.006487185715991213], ['initialization', 0.006487185715991213]]\nindices: [2, 3, 6, 0, 1, 4, 5, 7, 8]\nindices after max_imp selection: [2, 3, 6, 0, 1, 4, 5, 7, 8]\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n23.10.2 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n                                                \nParallel coordinates plots\n\n\n\n\n23.10.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.cvmodel import cv_model\nset_control_key_value(control_dict=fun_control,\n                        key=\"k_folds\",\n                        value=2,\n                        replace=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"test_size\",\n                        value=0.6,\n                        replace=True)\ncv_model(config, fun_control)\n\nk: 0\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 3012.21044921875, 'hp_metric': 3012.21044921875}\nk: 1\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 2975.527099609375, 'hp_metric': 2975.527099609375}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3012.21044921875      │\n│         val_loss          │     3012.21044921875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2975.527099609375     │\n│         val_loss          │     2975.527099609375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n2993.8687744140625\n\n\n\n\n23.10.4 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n23.10.5 Visualizing the Activation Distribution (Under Development)\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11, _torchmetric=TORCH_METRIC)\nmodel\n\nNetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.04938229888019609, inplace=False)\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.04938229888019609, inplace=False)\n    (6): Linear(in_features=32, out_features=32, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.04938229888019609, inplace=False)\n    (9): Linear(in_features=32, out_features=16, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.04938229888019609, inplace=False)\n    (12): Linear(in_features=16, out_features=11, bias=True)\n  )\n)\n\n\n\n# from spotPython.utils.eda import visualize_activations\n# visualize_activations(model, color=f\"C{0}\")",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html",
    "href": "032_spot_lightning_rnn_diabetes.html",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "",
    "text": "24.1 Step 1: Setup\nfrom spotPython.utils.device import getDevice\nfrom math import inf\nMAX_TIME = 1\nFUN_EVALS = inf\nINIT_SIZE = 5\nWORKERS = 0\nPREFIX=\"032\"\nDEVICE = getDevice()\nTORCH_METRIC = \"mean_squared_error\"",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-setup-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-setup-32",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter MAX_TIME specifies the maximum run time in seconds.\nThe parameter INIT_SIZE specifies the initial design size.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nFUN_EVALS is set to infinity.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\nPREFIX is set to “032”. This is used for the experiment name and the name of the log file.\nDEVICE is set to the device that is returned by getDevice(), e.g., gpu.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "032_spot_lightning_rnn_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "24.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\n\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    _torchmetric=TORCH_METRIC,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    log_level=10,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=0.1,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    verbosity=1\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_03_17_22_07_57\nCreated spot_tensorboard_path: runs/spot_logs/032_p040025_2024-03-17_22-07-57 for SummaryWriter()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-3-loading-the-diabetes-data-set",
    "href": "032_spot_lightning_rnn_diabetes.html#step-3-loading-the-diabetes-data-set",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.3 Step 3: Loading the Diabetes Data Set",
    "text": "24.3 Step 3: Loading the Diabetes Data Set\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes()\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n442\n\n\n\n\n\n\n\n\nNote: Data Set and Data Loader\n\n\n\n\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 5\nInputs Shape: torch.Size([5, 10])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,\n          0.0199, -0.0176],\n        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,\n         -0.0683, -0.0922],\n        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,\n          0.0029, -0.0259],\n        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,\n          0.0227, -0.0094],\n        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,\n         -0.0320, -0.0466]])\nTargets: tensor([151.,  75., 141., 206., 135.])",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-preprocessing-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-preprocessing-32",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.4 Step 4: Preprocessing",
    "text": "24.4 Step 4: Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-selection-of-the-algorithm-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-selection-of-the-algorithm-32",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict",
    "text": "24.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\nHere we simply add the NN Model to the fun_control dictionary by calling the function add_core_model_to_fun_control:\n\n\nfrom spotPython.light.regression.rnnlightregression import RNNLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=RNNLightRegression,\n                              hyper_dict=LightHyperDict)\n\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].\n\n\n\n\n\n\nNote: User specified models and hyperparameter dictionaries\n\n\n\n\nThe user can specify a model and a hyperparameter dictionary in a subfolder, e.g., userRNN in the current working directory.\nThe model and the hyperparameter dictionary are imported with the following code:\n\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nimport sys\nsys.path.insert(0, './userRNN')\nimport userrnn\nimport user_hyper_dict\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=userrnn.RNNLightRegression,\n                              hyper_dict=user_hyper_dict.UserHyperDict)\n\nExample files can be found in the userRNN folder.\nThese files can be modified by the user.\nThey can be used without re-compilation of the spotPython source code, if they are located in a subfolder of the current working directory.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-modification-of-hyperparameters-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-modification-of-hyperparameters-32",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "24.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9]) and\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"l1\", [3, 8])\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [2, 6])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adadelta\",\n                \"Adagrad\",\n                \"Adam\",\n                \"Adamax\"])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.25])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [3, 9])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\"ReLU\"] )\nset_control_hyperparameter_value(fun_control, \"initialization\",[\"Default\"] )\n\nSetting hyperparameter l1 to value [3, 8].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter epochs to value [7, 9].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter batch_size to value [2, 6].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter optimizer to value ['Adadelta', 'Adagrad', 'Adam', 'Adamax'].\nVariable type is factor.\nCore type is str.\nCalling modify_hyper_parameter_levels().\nSetting hyperparameter dropout_prob to value [0.01, 0.25].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter lr_mult to value [0.5, 5.0].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter patience to value [3, 9].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter act_fn to value ['ReLU'].\nVariable type is factor.\nCore type is instance().\nCalling modify_hyper_parameter_levels().\nSetting hyperparameter initialization to value ['Default'].\nVariable type is factor.\nCore type is str.\nCalling modify_hyper_parameter_levels().\n\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |    3    |    8    | transform_power_2_int |\n| epochs         | int    | 4         |    7    |    9    | transform_power_2_int |\n| batch_size     | int    | 4         |    2    |    6    | transform_power_2_int |\n| act_fn         | factor | ReLU      |    0    |    0    | None                  |\n| optimizer      | factor | SGD       |    0    |    3    | None                  |\n| dropout_prob   | float  | 0.01      |    0.01 |    0.25 | None                  |\n| lr_mult        | float  | 1.0       |    0.5  |    5    | None                  |\n| patience       | int    | 2         |    3    |    9    | transform_power_2_int |\n| initialization | factor | Default   |    0    |    0    | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n\n\n\n\nNote: Hyperparameters of the Tuned Model and the fun_control Dictionary\n\n\n\nThe updated fun_control dictionary can be shown with the command fun_control[\"core_model_hyper_dict\"].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "032_spot_lightning_rnn_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "24.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n24.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n24.7.2 Loss Function\nThe loss function is specified in the configurable network class [SOURCE] We will use MSE.\n\n\n24.7.3 Metric\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#step-8-calling-the-spot-function",
    "href": "032_spot_lightning_rnn_diabetes.html#step-8-calling-the-spot-function",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.8 Step 8: Calling the SPOT Function",
    "text": "24.8 Step 8: Calling the SPOT Function\n\n24.8.1 Preparing the SPOT Call\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init()\nset_control_key_value(control_dict=design_control,\n                        key=\"init_size\",\n                        value=INIT_SIZE,\n                        replace=True)\n\nsurrogate_control = surrogate_control_init()\nset_control_key_value(control_dict=surrogate_control,\n                        key=\"noise\",\n                        value=True,\n                        replace=True)                       \nset_control_key_value(control_dict=surrogate_control,\n                        key=\"n_theta\",\n                        value=2,\n                        replace=True)      \n\n\n\n24.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=10).fun\n\n\n\n24.8.3 Showing the fun_control Dictionary\n\nimport pprint\npprint.pprint(fun_control)\n\n{'CHECKPOINT_PATH': 'runs/saved_models/',\n 'DATASET_PATH': 'data/',\n 'PREFIX': '032',\n 'RESULTS_PATH': 'results/',\n 'TENSORBOARD_PATH': 'runs/',\n '_L_in': 10,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'accelerator': 'auto',\n 'converters': None,\n 'core_model': &lt;class 'spotPython.light.regression.rnnlightregression.RNNLightRegression'&gt;,\n 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                      'core_model_parameter_type': 'instance()',\n                                      'default': 'ReLU',\n                                      'levels': ['ReLU'],\n                                      'lower': 0,\n                                      'transform': 'None',\n                                      'type': 'factor',\n                                      'upper': 0},\n                           'batch_size': {'default': 4,\n                                          'lower': 2,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 6},\n                           'dropout_prob': {'default': 0.01,\n                                            'lower': 0.01,\n                                            'transform': 'None',\n                                            'type': 'float',\n                                            'upper': 0.25},\n                           'epochs': {'default': 4,\n                                      'lower': 7,\n                                      'transform': 'transform_power_2_int',\n                                      'type': 'int',\n                                      'upper': 9},\n                           'initialization': {'core_model_parameter_type': 'str',\n                                              'default': 'Default',\n                                              'levels': ['Default'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 0},\n                           'l1': {'default': 3,\n                                  'lower': 3,\n                                  'transform': 'transform_power_2_int',\n                                  'type': 'int',\n                                  'upper': 8},\n                           'lr_mult': {'default': 1.0,\n                                       'lower': 0.5,\n                                       'transform': 'None',\n                                       'type': 'float',\n                                       'upper': 5.0},\n                           'optimizer': {'class_name': 'torch.optim',\n                                         'core_model_parameter_type': 'str',\n                                         'default': 'SGD',\n                                         'levels': ['Adadelta',\n                                                    'Adagrad',\n                                                    'Adam',\n                                                    'Adamax'],\n                                         'lower': 0,\n                                         'transform': 'None',\n                                         'type': 'factor',\n                                         'upper': 3},\n                           'patience': {'default': 2,\n                                        'lower': 3,\n                                        'transform': 'transform_power_2_int',\n                                        'type': 'int',\n                                        'upper': 9}},\n 'core_model_hyper_dict_default': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                              'core_model_parameter_type': 'instance()',\n                                              'default': 'ReLU',\n                                              'levels': ['Tanh', 'ReLU'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 1},\n                                   'batch_size': {'default': 4,\n                                                  'lower': 1,\n                                                  'transform': 'transform_power_2_int',\n                                                  'type': 'int',\n                                                  'upper': 4},\n                                   'dropout_prob': {'default': 0.01,\n                                                    'lower': 0.0,\n                                                    'transform': 'None',\n                                                    'type': 'float',\n                                                    'upper': 0.25},\n                                   'epochs': {'default': 4,\n                                              'lower': 4,\n                                              'transform': 'transform_power_2_int',\n                                              'type': 'int',\n                                              'upper': 9},\n                                   'initialization': {'core_model_parameter_type': 'str',\n                                                      'default': 'Default',\n                                                      'levels': ['Default',\n                                                                 'Kaiming',\n                                                                 'Xavier'],\n                                                      'lower': 0,\n                                                      'transform': 'None',\n                                                      'type': 'factor',\n                                                      'upper': 2},\n                                   'l1': {'default': 3,\n                                          'lower': 3,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 8},\n                                   'lr_mult': {'default': 1.0,\n                                               'lower': 0.1,\n                                               'transform': 'None',\n                                               'type': 'float',\n                                               'upper': 10.0},\n                                   'optimizer': {'class_name': 'torch.optim',\n                                                 'core_model_parameter_type': 'str',\n                                                 'default': 'SGD',\n                                                 'levels': ['Adadelta',\n                                                            'Adagrad',\n                                                            'Adam',\n                                                            'AdamW',\n                                                            'SparseAdam',\n                                                            'Adamax',\n                                                            'ASGD',\n                                                            'NAdam',\n                                                            'RAdam',\n                                                            'RMSprop',\n                                                            'Rprop',\n                                                            'SGD'],\n                                                 'lower': 0,\n                                                 'transform': 'None',\n                                                 'type': 'factor',\n                                                 'upper': 11},\n                                   'patience': {'default': 2,\n                                                'lower': 2,\n                                                'transform': 'transform_power_2_int',\n                                                'type': 'int',\n                                                'upper': 6}},\n 'core_model_name': None,\n 'counter': 0,\n 'data': None,\n 'data_dir': './data',\n 'data_module': None,\n 'data_set': &lt;spotPython.data.diabetes.Diabetes object at 0x36e04a550&gt;,\n 'data_set_name': None,\n 'design': None,\n 'device': 'mps',\n 'devices': 1,\n 'enable_progress_bar': False,\n 'eval': None,\n 'fun_evals': inf,\n 'fun_repeats': 1,\n 'horizon': None,\n 'infill_criterion': 'y',\n 'k_folds': 3,\n 'log_graph': False,\n 'log_level': 10,\n 'loss_function': None,\n 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),\n 'max_surrogate_points': 30,\n 'max_time': 1,\n 'metric_params': {},\n 'metric_river': None,\n 'metric_sklearn': None,\n 'metric_torch': None,\n 'model_dict': {},\n 'n_points': 1,\n 'n_samples': None,\n 'n_total': None,\n 'noise': False,\n 'num_workers': 0,\n 'ocba_delta': 0,\n 'oml_grace_period': None,\n 'optimizer': None,\n 'path': None,\n 'prep_model': None,\n 'save_model': False,\n 'seed': 123,\n 'show_batch_interval': 1000000,\n 'show_models': False,\n 'show_progress': True,\n 'shuffle': None,\n 'sigma': 0.0,\n 'spot_tensorboard_path': 'runs/spot_logs/032_p040025_2024-03-17_22-07-57',\n 'spot_writer': &lt;torch.utils.tensorboard.writer.SummaryWriter object at 0x36dedb910&gt;,\n 'target_column': None,\n 'task': None,\n 'test': None,\n 'test_seed': 1234,\n 'test_size': 0.1,\n 'tolerance_x': 1.4901161193847656e-08,\n 'train': None,\n 'upper': array([ 8.  ,  9.  ,  4.  ,  1.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),\n 'var_name': ['l1',\n              'epochs',\n              'batch_size',\n              'act_fn',\n              'optimizer',\n              'dropout_prob',\n              'lr_mult',\n              'patience',\n              'initialization'],\n 'var_type': ['int',\n              'int',\n              'int',\n              'factor',\n              'factor',\n              'float',\n              'float',\n              'int',\n              'factor'],\n 'verbosity': 1,\n 'weight_coeff': 0.0,\n 'weights': 1.0}\n\n\n\npprint.pprint(design_control)\n\n{'init_size': 5, 'repeats': 1}\n\n\n\npprint.pprint(surrogate_control)\n\n{'log_level': 50,\n 'max_Lambda': 1,\n 'max_theta': 2.0,\n 'metric_factorial': 'canberra',\n 'min_Lambda': 1e-09,\n 'min_theta': -3.0,\n 'model_fun_evals': 10000,\n 'model_optimizer': &lt;function differential_evolution at 0x16ad6db20&gt;,\n 'n_p': 1,\n 'n_theta': 2,\n 'noise': True,\n 'optim_p': False,\n 'p_val': 2.0,\n 'seed': 124,\n 'theta_init_zero': True,\n 'var_type': None}\n\n\n\n\n24.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE].\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 64,\n 'dropout_prob': 0.19355651674791854,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 16,\n 'lr_mult': 1.5691149440098038,\n 'optimizer': 'Adam',\n 'patience': 32}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 4774.853515625, 'hp_metric': 4774.853515625}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.09424169914869776,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 3.35818256351233,\n 'optimizer': 'Adadelta',\n 'patience': 512}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2993.610595703125, 'hp_metric': 2993.610595703125}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 4,\n 'dropout_prob': 0.21164199382623602,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.9336514668325573,\n 'optimizer': 'Adamax',\n 'patience': 16}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2729.412353515625, 'hp_metric': 2729.412353515625}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 8,\n 'dropout_prob': 0.05728504399550885,\n 'epochs': 128,\n 'initialization': 'Default',\n 'l1': 64,\n 'lr_mult': 4.575980093998586,\n 'optimizer': 'Adam',\n 'patience': 32}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 3437.872314453125, 'hp_metric': 3437.872314453125}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.14352914208400058,\n 'epochs': 256,\n 'initialization': 'Default',\n 'l1': 8,\n 'lr_mult': 2.4204853123355816,\n 'optimizer': 'Adagrad',\n 'patience': 128}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 3862.91357421875, 'hp_metric': 3862.91357421875}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 4,\n 'dropout_prob': 0.25,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.5,\n 'optimizer': 'Adamax',\n 'patience': 8}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 2938.389404296875, 'hp_metric': 2938.389404296875}\nspotPython tuning: 2729.412353515625 [#######---] 66.58% \n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 4,\n 'dropout_prob': 0.11728697856857,\n 'epochs': 512,\n 'initialization': 'Default',\n 'l1': 256,\n 'lr_mult': 0.5,\n 'optimizer': 'Adadelta',\n 'patience': 512}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 39\ntrain_model result: {'val_loss': 4715.65478515625, 'hp_metric': 4715.65478515625}\nspotPython tuning: 2729.412353515625 [##########] 100.00% Done...\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      4774.853515625       │\n│         val_loss          │      4774.853515625       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2993.610595703125     │\n│         val_loss          │     2993.610595703125     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2729.412353515625     │\n│         val_loss          │     2729.412353515625     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3437.872314453125     │\n│         val_loss          │     3437.872314453125     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3862.91357421875      │\n│         val_loss          │     3862.91357421875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2938.389404296875     │\n│         val_loss          │     2938.389404296875     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     4715.65478515625      │\n│         val_loss          │     4715.65478515625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x37a5a7d50&gt;",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-tensorboard-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-tensorboard-32",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.9 Step 9: Tensorboard",
    "text": "24.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "032_spot_lightning_rnn_diabetes.html#sec-results-32",
    "href": "032_spot_lightning_rnn_diabetes.html#sec-results-32",
    "title": "24  HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network",
    "section": "24.10 Step 10: Results",
    "text": "24.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX + \"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     3.0 |     8.0 | 7.0                 | transform_power_2_int |       100.00 | ***     |\n| epochs         | int    | 4         |     7.0 |     9.0 | 9.0                 | transform_power_2_int |         0.01 |         |\n| batch_size     | int    | 4         |     2.0 |     6.0 | 2.0                 | transform_power_2_int |         0.01 |         |\n| act_fn         | factor | ReLU      |     0.0 |     0.0 | ReLU                | None                  |         0.00 |         |\n| optimizer      | factor | SGD       |     0.0 |     3.0 | Adamax              | None                  |         0.01 |         |\n| dropout_prob   | float  | 0.01      |    0.01 |    0.25 | 0.21164199382623602 | None                  |         0.01 |         |\n| lr_mult        | float  | 1.0       |     0.5 |     5.0 | 0.9336514668325573  | None                  |         0.01 |         |\n| patience       | int    | 2         |     3.0 |     9.0 | 4.0                 | transform_power_2_int |         0.01 |         |\n| initialization | factor | Default   |     0.0 |     0.0 | Default             | None                  |         0.00 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + PREFIX + \"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n24.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nprint(config)\n\n{'l1': 128, 'epochs': 512, 'batch_size': 4, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.21164199382623602, 'lr_mult': 0.9336514668325573, 'patience': 16, 'initialization': 'Default'}\n\n\n\nTest on the full data set\n\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.81, val_size: 0.09 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 39\nLightDataModule.train_dataloader(). data_train size: 359\nLightDataModule.setup(): stage: TrainerFn.TESTING\ntest_size: 0.1 used for test dataset.\nLightDataModule.test_dataloader(). Test set size: 45\ntest_model result: {'val_loss': 2566.683837890625, 'hp_metric': 2566.683837890625}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2566.683837890625     │\n│         val_loss          │     2566.683837890625     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n(2566.683837890625, 2566.683837890625)\n\n\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\n\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 128, 'epochs': 512, 'batch_size': 4, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.21164199382623602, 'lr_mult': 0.9336514668325573, 'patience': 16, 'initialization': 'Default'}\nLoading model with 128_512_4_ReLU_Adamax_0.2116_0.9337_16_Default_TEST from runs/saved_models/128_512_4_ReLU_Adamax_0.2116_0.9337_16_Default_TEST/last.ckpt\nModel: RNNLightRegression(\n  (rnn_layer): RNN(10, 128, batch_first=True)\n  (fc): Linear(in_features=128, out_features=128, bias=True)\n  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n  (dropout1): Dropout(p=0.21164199382623602, inplace=False)\n  (dropout2): Dropout(p=0.0, inplace=False)\n  (dropout3): Dropout(p=0.0, inplace=False)\n  (activation_fct): ReLU()\n)\n\n\n\nfilename = \"./figures/\" + PREFIX\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  100.0\nepochs:  0.005967886669723714\nbatch_size:  0.005967886669723714\noptimizer:  0.005967886669723714\ndropout_prob:  0.005967886669723714\nlr_mult:  0.005967886669723714\npatience:  0.005967886669723714\nimpo: [['l1', 100.0], ['epochs', 0.005967886669723714], ['batch_size', 0.005967886669723714], ['optimizer', 0.005967886669723714], ['dropout_prob', 0.005967886669723714], ['lr_mult', 0.005967886669723714], ['patience', 0.005967886669723714]]\nindices: [0, 1, 2, 3, 4, 5, 6]\nindices after max_imp selection: [0, 1, 2, 3, 4, 5, 6]\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n24.10.2 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n                                                \nParallel coordinates plots\n\n\n\n\n24.10.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.cvmodel import cv_model\nset_control_key_value(control_dict=fun_control,\n                        key=\"k_folds\",\n                        value=2,\n                        replace=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"test_size\",\n                        value=0.1,\n                        replace=True)\ncv_model(config, fun_control)\n\nk: 0\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 3284.99951171875, 'hp_metric': 3284.99951171875}\nk: 1\nTrain Dataset Size: 221\nVal Dataset Size: 221\ntrain_model result: {'val_loss': 3183.663818359375, 'hp_metric': 3183.663818359375}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3284.99951171875      │\n│         val_loss          │     3284.99951171875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3183.663818359375     │\n│         val_loss          │     3183.663818359375     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n3234.3316650390625\n\n\n\n\n24.10.4 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n24.10.5 Visualizing the Activation Distribution (Under Development)\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11, _torchmetric=TORCH_METRIC)\nmodel\n\nRNNLightRegression(\n  (rnn_layer): RNN(64, 128, batch_first=True)\n  (fc): Linear(in_features=128, out_features=128, bias=True)\n  (output_layer): Linear(in_features=128, out_features=11, bias=True)\n  (dropout1): Dropout(p=0.21164199382623602, inplace=False)\n  (dropout2): Dropout(p=0.0, inplace=False)\n  (dropout3): Dropout(p=0.0, inplace=False)\n  (activation_fct): ReLU()\n)\n\n\n\n# from spotPython.utils.eda import visualize_activations\n# visualize_activations(model, color=f\"C{0}\")",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html",
    "href": "033_spot_lightning_linear_sensitive.html",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "",
    "text": "25.1 Step 1: Setup\nfrom spotPython.utils.device import getDevice\nfrom math import inf\n\nMAX_TIME = 1\nFUN_EVALS = inf\nFUN_REPEATS = 1\nOCBA_DELTA = 0\nREPEATS = 1\nINIT_SIZE = 3\nWORKERS = 0\nPREFIX=\"033\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.3\nTORCH_METRIC = \"mean_squared_error\"",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-setup-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-setup-33",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter MAX_TIME specifies the maximum run time in seconds.\nThe parameter INIT_SIZE specifies the initial design size.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to a small value for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "033_spot_lightning_linear_sensitive.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "25.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfun_control = fun_control_init(\n    _L_in=6,\n    _L_out=1,\n    _torchmetric=TORCH_METRIC,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    fun_repeats=FUN_REPEATS,\n    log_level=50,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    ocba_delta = OCBA_DELTA,\n    show_progress=True,\n    test_size=TEST_SIZE,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    verbosity=1,\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_03_17_22_26_35\nCreated spot_tensorboard_path: runs/spot_logs/033_p040025_2024-03-17_22-26-35 for SummaryWriter()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-3-loading-the-user-specified-data-set",
    "href": "033_spot_lightning_linear_sensitive.html#step-3-loading-the-user-specified-data-set",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.3 Step 3: Loading the User Specified Data Set",
    "text": "25.3 Step 3: Loading the User Specified Data Set\n\n# from spotPython.hyperparameters.values import set_control_key_value\n# from spotPython.data.pkldataset import PKLDataset\n# import torch\n# dataset = PKLDataset(directory=\"./userData/\",\n#                      filename=\"data_sensitive.pkl\",\n#                      target_column='N',\n#                      feature_type=torch.float32,\n#                      target_type=torch.float32,\n#                      rmNA=True)\n# set_control_key_value(control_dict=fun_control,\n#                         key=\"data_set\",\n#                         value=dataset,\n#                         replace=True)\n# print(len(dataset))\n\n\n\n\n\n\n\nNote: Data Set and Data Loader\n\n\n\n\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n\n# if the package pyhcf is installed then print \"pyhcf is installed\" else print \"pyhcf is not installed\"\ntry:\n    import pyhcf\n    print(\"pyhcf is installed\")\n    from pyhcf.data.loadHcfData import load_hcf_data\n    dataset = load_hcf_data(A=True, H=True,\n                        param_list=['H', 'D', 'L', 'K', 'E', 'I', 'N'],\n                        target='N', rmNA=True, rmMF=True, scale_data=True, return_X_y=False)\nexcept ImportError:\n    print(\"pyhcf is not installed\")\n    from spotPython.data.pkldataset import PKLDataset\n    import torch\n    dataset = PKLDataset(directory=\"./userData/\",\n                        filename=\"data_sensitive.pkl\",\n                        target_column='N',\n                        feature_type=torch.float32,\n                        target_type=torch.float32,\n                        rmNA=True)\n\npyhcf is installed\nLoading data for ['H', 'D', 'L', 'K', 'E', 'I', 'N']...\nVolle Liste mit target:  ['H Drehzahl [1/min]', 'D Flächenverhältnis ', 'L Mode []', 'K Knotendurchmesser []', 'E Drosselstellung ', 'I Ordnung []', 'N AufgewAmplitudeNom [MPa]']\nFeature Liste ohne target:  ['H Drehzahl [1/min]', 'D Flächenverhältnis ', 'L Mode []', 'K Knotendurchmesser []', 'E Drosselstellung ', 'I Ordnung []']\ndf features bevor tensor gebildet wird        H Drehzahl [1/min]  D Flächenverhältnis   L Mode []  \\\n0                    46.0                   2.0        0.0   \n1                   344.0                   2.0        1.0   \n2                   385.0                   2.0        1.0   \n3                   399.0                   2.0        1.0   \n4                   399.0                   2.0        1.0   \n...                   ...                   ...        ...   \n41832                83.0                   5.0        9.0   \n41833                74.0                   5.0        9.0   \n41834                74.0                   5.0        9.0   \n41835                80.0                   5.0        9.0   \n41836                75.0                   5.0        9.0   \n\n       K Knotendurchmesser []  E Drosselstellung   I Ordnung []  \n0                         3.0                 2.0           4.0  \n1                         3.0                 2.0           4.0  \n2                         3.0                 2.0           4.0  \n3                         3.0                 2.0           4.0  \n4                         3.0                 2.0           4.0  \n...                       ...                 ...           ...  \n41832                     3.0                 1.0          19.0  \n41833                     3.0                 1.0          19.0  \n41834                     3.0                 1.0          19.0  \n41835                     3.0                 1.0          19.0  \n41836                     3.0                 1.0          19.0  \n\n[41837 rows x 6 columns]\n\n\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n41837\n\n\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n \n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 5\nInputs Shape: torch.Size([5, 6])\nTargets Shape: torch.Size([5])\n---------------\nInputs: tensor([[0.0033, 0.4000, 0.0000, 0.7500, 1.0000, 0.1667],\n        [0.0246, 0.4000, 0.0435, 0.7500, 1.0000, 0.1667],\n        [0.0275, 0.4000, 0.0435, 0.7500, 1.0000, 0.1667],\n        [0.0285, 0.4000, 0.0435, 0.7500, 1.0000, 0.1667],\n        [0.0285, 0.4000, 0.0435, 0.7500, 1.0000, 0.1667]])\nTargets: tensor([4.5764, 4.9073, 6.2846, 5.5094, 5.6079])",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-preprocessing-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-preprocessing-33",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.4 Step 4: Preprocessing",
    "text": "25.4 Step 4: Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-selection-of-the-algorithm-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-selection-of-the-algorithm-33",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict",
    "text": "25.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\nHere we simply add the NN Model to the fun_control dictionary by calling the function add_core_model_to_fun_control:\n\nWe can use aconfiguration from the spotPython package:\n\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=NetLightRegression,\n                              hyper_dict=LightHyperDict)\n\n\nAlternatively, we can use a userr configuration from the subdirectory userModel:\n\n\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nimport sys\nsys.path.insert(0, './userModel')\nimport netlightregression\nimport light_hyper_dict\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=netlightregression.NetLightRegression,\n                              hyper_dict=light_hyper_dict.LightHyperDict)\n\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-modification-of-hyperparameters-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-modification-of-hyperparameters-33",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "25.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9]) and\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 7])\n\n\n\n\n\n\n\n\n\n\nNote: Pre-experimental Results\n\n\n\n\nThe following hyperparameters {Table 25.1} have generated acceptable results (obtained in in pre-experimental runs):\n\n\n\n\nTable 25.1: Table 1: Pre-experimental results for the user specified data set. The test set size is 715, the train set size is 1167, and the batch size is 16.\n\n\n\n\n\nHyperparameter\nValue\n\n\n\n\nact_fn\nLeakyReLU\n\n\nbatch_size\n16\n\n\ndropout_prob\n0.01\n\n\nepochs\n512\n\n\ninitialization\nDefault\n\n\nl1\n128\n\n\nlr_mult\n0.5\n\n\noptimizer\nAdagrad\n\n\npatience\n16\n\n\n\n\n\n\nTherefore, we will use these values as the starting poing for the hyperparameter tuning.\n\n\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"l1\", [3, 4])\nset_control_hyperparameter_value(fun_control, \"epochs\", [2, 4])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [3, 6])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adadelta\",\n                \"Adamax\",\n                \"Adagrad\"\n            ])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.005, 0.25])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.25, 5.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\n                \"ReLU\",\n                \"LeakyReLU\",\n            ] )\nset_control_hyperparameter_value(fun_control, \"initialization\",[\"Default\"] )\n\nSetting hyperparameter l1 to value [3, 4].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter epochs to value [2, 4].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter batch_size to value [3, 6].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter optimizer to value ['Adadelta', 'Adamax', 'Adagrad'].\nVariable type is factor.\nCore type is str.\nCalling modify_hyper_parameter_levels().\nSetting hyperparameter dropout_prob to value [0.005, 0.25].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter lr_mult to value [0.25, 5.0].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter patience to value [2, 3].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter act_fn to value ['ReLU', 'LeakyReLU'].\nVariable type is factor.\nCore type is instance().\nCalling modify_hyper_parameter_levels().\nSetting hyperparameter initialization to value ['Default'].\nVariable type is factor.\nCore type is str.\nCalling modify_hyper_parameter_levels().\n\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |   3     |    4    | transform_power_2_int |\n| epochs         | int    | 4         |   2     |    4    | transform_power_2_int |\n| batch_size     | int    | 4         |   3     |    6    | transform_power_2_int |\n| act_fn         | factor | ReLU      |   0     |    1    | None                  |\n| optimizer      | factor | SGD       |   0     |    2    | None                  |\n| dropout_prob   | float  | 0.01      |   0.005 |    0.25 | None                  |\n| lr_mult        | float  | 1.0       |   0.25  |    5    | None                  |\n| patience       | int    | 2         |   2     |    3    | transform_power_2_int |\n| initialization | factor | Default   |   0     |    0    | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n\n\n\n\nNote: Hyperparameters of the Tuned Model and the fun_control Dictionary\n\n\n\nThe updated fun_control dictionary can be shown with the command fun_control[\"core_model_hyper_dict\"].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "033_spot_lightning_linear_sensitive.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "25.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n25.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n25.7.2 Loss Function\nThe loss function is specified in the configurable network class [SOURCE] We will use MSE.\n\n\n25.7.3 Metric\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#step-8-calling-the-spot-function",
    "href": "033_spot_lightning_linear_sensitive.html#step-8-calling-the-spot-function",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.8 Step 8: Calling the SPOT Function",
    "text": "25.8 Step 8: Calling the SPOT Function\n\n25.8.1 Preparing the SPOT Call\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init(init_size=INIT_SIZE,\n                                     repeats=REPEATS,)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                            n_theta=2,\n                                            min_Lambda=1e-6,\n                                            max_Lambda=10,\n                                            log_level=50,)\n\n\n\n\n\n\n\nNote: Modifying Values in the Control Dictionaries\n\n\n\n\nThe values in the control dictionaries can be modified with the function set_control_key_value [SOURCE], for example:\n\nset_control_key_value(control_dict=surrogate_control,\n                        key=\"noise\",\n                        value=True,\n                        replace=True)                       \nset_control_key_value(control_dict=surrogate_control,\n                        key=\"n_theta\",\n                        value=2,\n                        replace=True)      \n\n\n\n\n\n25.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=50).fun\n\n\n\n25.8.3 Showing the fun_control Dictionary\n\nimport pprint\npprint.pprint(fun_control)\n\n{'CHECKPOINT_PATH': 'runs/saved_models/',\n 'DATASET_PATH': 'data/',\n 'PREFIX': '033',\n 'RESULTS_PATH': 'results/',\n 'TENSORBOARD_PATH': 'runs/',\n '_L_in': 6,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'accelerator': 'auto',\n 'converters': None,\n 'core_model': &lt;class 'netlightregression.NetLightRegression'&gt;,\n 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                      'core_model_parameter_type': 'instance()',\n                                      'default': 'ReLU',\n                                      'levels': ['ReLU', 'LeakyReLU'],\n                                      'lower': 0,\n                                      'transform': 'None',\n                                      'type': 'factor',\n                                      'upper': 1},\n                           'batch_size': {'default': 4,\n                                          'lower': 3,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 6},\n                           'dropout_prob': {'default': 0.01,\n                                            'lower': 0.005,\n                                            'transform': 'None',\n                                            'type': 'float',\n                                            'upper': 0.25},\n                           'epochs': {'default': 4,\n                                      'lower': 2,\n                                      'transform': 'transform_power_2_int',\n                                      'type': 'int',\n                                      'upper': 4},\n                           'initialization': {'core_model_parameter_type': 'str',\n                                              'default': 'Default',\n                                              'levels': ['Default'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 0},\n                           'l1': {'default': 3,\n                                  'lower': 3,\n                                  'transform': 'transform_power_2_int',\n                                  'type': 'int',\n                                  'upper': 4},\n                           'lr_mult': {'default': 1.0,\n                                       'lower': 0.25,\n                                       'transform': 'None',\n                                       'type': 'float',\n                                       'upper': 5.0},\n                           'optimizer': {'class_name': 'torch.optim',\n                                         'core_model_parameter_type': 'str',\n                                         'default': 'SGD',\n                                         'levels': ['Adadelta',\n                                                    'Adamax',\n                                                    'Adagrad'],\n                                         'lower': 0,\n                                         'transform': 'None',\n                                         'type': 'factor',\n                                         'upper': 2},\n                           'patience': {'default': 2,\n                                        'lower': 2,\n                                        'transform': 'transform_power_2_int',\n                                        'type': 'int',\n                                        'upper': 3}},\n 'core_model_hyper_dict_default': {'act_fn': {'class_name': 'spotPython.torch.activation',\n                                              'core_model_parameter_type': 'instance()',\n                                              'default': 'ReLU',\n                                              'levels': ['Sigmoid',\n                                                         'Tanh',\n                                                         'ReLU',\n                                                         'LeakyReLU',\n                                                         'ELU',\n                                                         'Swish'],\n                                              'lower': 0,\n                                              'transform': 'None',\n                                              'type': 'factor',\n                                              'upper': 5},\n                                   'batch_size': {'default': 4,\n                                                  'lower': 1,\n                                                  'transform': 'transform_power_2_int',\n                                                  'type': 'int',\n                                                  'upper': 4},\n                                   'dropout_prob': {'default': 0.01,\n                                                    'lower': 0.0,\n                                                    'transform': 'None',\n                                                    'type': 'float',\n                                                    'upper': 0.25},\n                                   'epochs': {'default': 4,\n                                              'lower': 4,\n                                              'transform': 'transform_power_2_int',\n                                              'type': 'int',\n                                              'upper': 9},\n                                   'initialization': {'core_model_parameter_type': 'str',\n                                                      'default': 'Default',\n                                                      'levels': ['Default',\n                                                                 'Kaiming',\n                                                                 'Xavier'],\n                                                      'lower': 0,\n                                                      'transform': 'None',\n                                                      'type': 'factor',\n                                                      'upper': 2},\n                                   'l1': {'default': 3,\n                                          'lower': 3,\n                                          'transform': 'transform_power_2_int',\n                                          'type': 'int',\n                                          'upper': 8},\n                                   'lr_mult': {'default': 1.0,\n                                               'lower': 0.1,\n                                               'transform': 'None',\n                                               'type': 'float',\n                                               'upper': 10.0},\n                                   'optimizer': {'class_name': 'torch.optim',\n                                                 'core_model_parameter_type': 'str',\n                                                 'default': 'SGD',\n                                                 'levels': ['Adadelta',\n                                                            'Adagrad',\n                                                            'Adam',\n                                                            'AdamW',\n                                                            'SparseAdam',\n                                                            'Adamax',\n                                                            'ASGD',\n                                                            'NAdam',\n                                                            'RAdam',\n                                                            'RMSprop',\n                                                            'Rprop',\n                                                            'SGD'],\n                                                 'lower': 0,\n                                                 'transform': 'None',\n                                                 'type': 'factor',\n                                                 'upper': 11},\n                                   'patience': {'default': 2,\n                                                'lower': 2,\n                                                'transform': 'transform_power_2_int',\n                                                'type': 'int',\n                                                'upper': 6}},\n 'core_model_name': None,\n 'counter': 0,\n 'data': None,\n 'data_dir': './data',\n 'data_module': None,\n 'data_set': &lt;torch.utils.data.dataset.TensorDataset object at 0x36dc93750&gt;,\n 'data_set_name': None,\n 'design': None,\n 'device': 'mps',\n 'devices': 1,\n 'enable_progress_bar': False,\n 'eval': None,\n 'fun_evals': inf,\n 'fun_repeats': 1,\n 'horizon': None,\n 'infill_criterion': 'y',\n 'k_folds': 3,\n 'log_graph': False,\n 'log_level': 50,\n 'loss_function': None,\n 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),\n 'max_surrogate_points': 30,\n 'max_time': 1,\n 'metric_params': {},\n 'metric_river': None,\n 'metric_sklearn': None,\n 'metric_torch': None,\n 'model_dict': {},\n 'n_points': 1,\n 'n_samples': None,\n 'n_total': None,\n 'noise': False,\n 'num_workers': 0,\n 'ocba_delta': 0,\n 'oml_grace_period': None,\n 'optimizer': None,\n 'path': None,\n 'prep_model': None,\n 'save_model': False,\n 'seed': 123,\n 'show_batch_interval': 1000000,\n 'show_models': False,\n 'show_progress': True,\n 'shuffle': None,\n 'sigma': 0.0,\n 'spot_tensorboard_path': 'runs/spot_logs/033_p040025_2024-03-17_22-26-35',\n 'spot_writer': &lt;torch.utils.tensorboard.writer.SummaryWriter object at 0x35ba11dd0&gt;,\n 'target_column': None,\n 'task': None,\n 'test': None,\n 'test_seed': 1234,\n 'test_size': 0.3,\n 'tolerance_x': 1.4901161193847656e-08,\n 'train': None,\n 'upper': array([ 8.  ,  9.  ,  4.  ,  5.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),\n 'var_name': ['l1',\n              'epochs',\n              'batch_size',\n              'act_fn',\n              'optimizer',\n              'dropout_prob',\n              'lr_mult',\n              'patience',\n              'initialization'],\n 'var_type': ['int',\n              'int',\n              'int',\n              'factor',\n              'factor',\n              'float',\n              'float',\n              'int',\n              'factor'],\n 'verbosity': 1,\n 'weight_coeff': 0.0,\n 'weights': 1.0}\n\n\n\n\n25.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE].\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.020345615289778483,\n 'epochs': 8,\n 'initialization': 'Default',\n 'l1': 16,\n 'lr_mult': 3.5380370864571606,\n 'optimizer': 'Adamax',\n 'patience': 8}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.49, val_size: 0.21 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 8785\nLightDataModule.train_dataloader(). data_train size: 20501\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 8785\ntrain_model result: {'val_loss': 28.42346954345703, 'hp_metric': 28.42346954345703}\n\nIn fun(): config:\n{'act_fn': ReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.23254269132436722,\n 'epochs': 4,\n 'initialization': 'Default',\n 'l1': 8,\n 'lr_mult': 0.6593438339617097,\n 'optimizer': 'Adadelta',\n 'patience': 4}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.49, val_size: 0.21 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 8785\nLightDataModule.train_dataloader(). data_train size: 20501\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 8785\ntrain_model result: {'val_loss': 30.56962776184082, 'hp_metric': 30.56962776184082}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.15478450721867254,\n 'epochs': 16,\n 'initialization': 'Default',\n 'l1': 8,\n 'lr_mult': 2.628500799878493,\n 'optimizer': 'Adagrad',\n 'patience': 8}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.49, val_size: 0.21 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 8785\nLightDataModule.train_dataloader(). data_train size: 20501\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 8785\ntrain_model result: {'val_loss': 30.587919235229492, 'hp_metric': 30.587919235229492}\n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 16,\n 'dropout_prob': 0.11943493649381043,\n 'epochs': 8,\n 'initialization': 'Default',\n 'l1': 16,\n 'lr_mult': 3.5604127261479572,\n 'optimizer': 'Adamax',\n 'patience': 8}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.49, val_size: 0.21 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 8785\nLightDataModule.train_dataloader(). data_train size: 20501\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 8785\ntrain_model result: {'val_loss': 28.177833557128906, 'hp_metric': 28.177833557128906}\nspotPython tuning: 28.177833557128906 [#########-] 87.09% \n\nIn fun(): config:\n{'act_fn': LeakyReLU(),\n 'batch_size': 8,\n 'dropout_prob': 0.25,\n 'epochs': 8,\n 'initialization': 'Default',\n 'l1': 16,\n 'lr_mult': 3.6392374566249317,\n 'optimizer': 'Adamax',\n 'patience': 8}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.49, val_size: 0.21 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 8785\nLightDataModule.train_dataloader(). data_train size: 20501\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 8785\ntrain_model result: {'val_loss': 27.438711166381836, 'hp_metric': 27.438711166381836}\nspotPython tuning: 27.438711166381836 [##########] 100.00% Done...\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     28.42346954345703     │\n│         val_loss          │     28.42346954345703     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     30.56962776184082     │\n│         val_loss          │     30.56962776184082     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    30.587919235229492     │\n│         val_loss          │    30.587919235229492     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    28.177833557128906     │\n│         val_loss          │    28.177833557128906     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    27.438711166381836     │\n│         val_loss          │    27.438711166381836     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x36dc92590&gt;",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-tensorboard-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-tensorboard-33",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.9 Step 9: Tensorboard",
    "text": "25.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "033_spot_lightning_linear_sensitive.html#sec-results-33",
    "href": "033_spot_lightning_linear_sensitive.html#sec-results-33",
    "title": "25  HPT PyTorch Lightning: User Specified Data Set and Regression Model",
    "section": "25.10 Step 10: Results",
    "text": "25.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed.\n\nif spot_tuner.noise:\n    print(spot_tuner.min_mean_X)\n    print(spot_tuner.min_mean_y)\nelse:\n    print(spot_tuner.min_X)\n    print(spot_tuner.min_y)\n\n[4.         3.         3.         1.         1.         0.25\n 3.63923746 3.        ]\n27.438711166381836\n\n\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX +\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper | tuned              | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     3.0 |     4.0 | 4.0                | transform_power_2_int |       100.00 | ***     |\n| epochs         | int    | 4         |     2.0 |     4.0 | 3.0                | transform_power_2_int |         5.44 | *       |\n| batch_size     | int    | 4         |     3.0 |     6.0 | 3.0                | transform_power_2_int |         0.01 |         |\n| act_fn         | factor | ReLU      |     0.0 |     1.0 | LeakyReLU          | None                  |         0.07 |         |\n| optimizer      | factor | SGD       |     0.0 |     2.0 | Adamax             | None                  |         0.72 | .       |\n| dropout_prob   | float  | 0.01      |   0.005 |    0.25 | 0.25               | None                  |         0.01 |         |\n| lr_mult        | float  | 1.0       |    0.25 |     5.0 | 3.6392374566249317 | None                  |        86.97 | **      |\n| patience       | int    | 2         |     2.0 |     3.0 | 3.0                | transform_power_2_int |        69.75 | **      |\n| initialization | factor | Default   |     0.0 |     0.0 | Default            | None                  |         0.00 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + PREFIX + \"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n25.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nprint(config)\n\n{'l1': 16, 'epochs': 8, 'batch_size': 8, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.25, 'lr_mult': 3.6392374566249317, 'patience': 8, 'initialization': 'Default'}\n\n\n\nTest on the full data set\n\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.49, val_size: 0.21 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 8785\nLightDataModule.train_dataloader(). data_train size: 20501\nLightDataModule.setup(): stage: TrainerFn.TESTING\ntest_size: 0.3 used for test dataset.\nLightDataModule.test_dataloader(). Test set size: 12552\ntest_model result: {'val_loss': 27.895092010498047, 'hp_metric': 27.895092010498047}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    27.895092010498047     │\n│         val_loss          │    27.895092010498047     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n(27.895092010498047, 27.895092010498047)\n\n\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\n\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 16, 'epochs': 8, 'batch_size': 8, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.25, 'lr_mult': 3.6392374566249317, 'patience': 8, 'initialization': 'Default'}\nLoading model with 16_8_8_LeakyReLU_Adamax_0.25_3.6392_8_Default_TEST from runs/saved_models/16_8_8_LeakyReLU_Adamax_0.25_3.6392_8_Default_TEST/last.ckpt\nModel: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=6, out_features=16, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.25, inplace=False)\n    (3): Linear(in_features=16, out_features=8, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.25, inplace=False)\n    (6): Linear(in_features=8, out_features=8, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.25, inplace=False)\n    (9): Linear(in_features=8, out_features=4, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.25, inplace=False)\n    (12): Linear(in_features=4, out_features=1, bias=True)\n  )\n)\n\n\n\nfilename = \"./figures/\" + PREFIX\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  100.0\nepochs:  5.442511869428519\nbatch_size:  0.00901964242189605\nact_fn:  0.06529127875903068\noptimizer:  0.7176499331030126\ndropout_prob:  0.00901964242189605\nlr_mult:  86.97380853074058\npatience:  69.75293098967978\nimpo: [['l1', 100.0], ['epochs', 5.442511869428519], ['batch_size', 0.00901964242189605], ['act_fn', 0.06529127875903068], ['optimizer', 0.7176499331030126], ['dropout_prob', 0.00901964242189605], ['lr_mult', 86.97380853074058], ['patience', 69.75293098967978]]\nindices: [0, 6, 7, 1, 4, 3, 2, 5]\nindices after max_imp selection: [0, 6, 7, 1, 4, 3, 2, 5]\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n25.10.2 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n                                                \nParallel coordinates plots\n\n\n\n\n25.10.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.cvmodel import cv_model\nset_control_key_value(control_dict=fun_control,\n                        key=\"k_folds\",\n                        value=2,\n                        replace=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"test_size\",\n                        value=0.6,\n                        replace=True)\ncv_model(config, fun_control)\n\nk: 0\nTrain Dataset Size: 20918\nVal Dataset Size: 20919\ntrain_model result: {'val_loss': 28.190673828125, 'hp_metric': 28.190673828125}\nk: 1\nTrain Dataset Size: 20919\nVal Dataset Size: 20918\ntrain_model result: {'val_loss': 26.77023696899414, 'hp_metric': 26.77023696899414}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      28.190673828125      │\n│         val_loss          │      28.190673828125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     26.77023696899414     │\n│         val_loss          │     26.77023696899414     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n27.48045539855957\n\n\n\n\n25.10.4 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n25.10.5 Visualizing the Activation Distribution (Under Development)\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11, _torchmetric=TORCH_METRIC)\nmodel\n\nNetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=16, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.25, inplace=False)\n    (3): Linear(in_features=16, out_features=8, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.25, inplace=False)\n    (6): Linear(in_features=8, out_features=8, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.25, inplace=False)\n    (9): Linear(in_features=8, out_features=4, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.25, inplace=False)\n    (12): Linear(in_features=4, out_features=11, bias=True)\n  )\n)\n\n\n\n# from spotPython.utils.eda import visualize_activations\n# visualize_activations(model, color=f\"C{0}\")",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>HPT PyTorch Lightning: User Specified Data Set and Regression Model</span>"
    ]
  },
  {
    "objectID": "034_spot_lightning_xai.html",
    "href": "034_spot_lightning_xai.html",
    "title": "26  Explainable AI with SpotPython and Pytorch",
    "section": "",
    "text": "from torch.utils.data import DataLoader\nfrom spotPython.utils.init import fun_control_init\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfrom spotPython.hyperparameters.values import (\n        get_default_hyperparameters_as_array, get_one_config_from_X)\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.plot.xai import (get_activations, get_gradients, get_weights, plot_nn_values_hist, plot_nn_values_scatter, visualize_weights, visualize_gradients, visualize_activations, visualize_gradient_distributions, visualize_weights_distributions)\nfun_control = fun_control_init(\n    _L_in=10, # 10: diabetes\n    _L_out=1,\n    _torchmetric=\"mean_squared_error\",\n    )\ndataset = Diabetes()\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=NetLightRegression,\n                              hyper_dict=LightHyperDict)\nX = get_default_hyperparameters_as_array(fun_control)\nconfig = get_one_config_from_X(X, fun_control)\n_L_in = fun_control[\"_L_in\"]\n_L_out = fun_control[\"_L_out\"]\n_torchmetric = fun_control[\"_torchmetric\"]\nmodel = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\nbatch_size= config[\"batch_size\"]\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n\nget_activations(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n\nnet: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=8, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.01, inplace=False)\n    (3): Linear(in_features=8, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.01, inplace=False)\n    (6): Linear(in_features=4, out_features=4, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.01, inplace=False)\n    (9): Linear(in_features=4, out_features=2, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.01, inplace=False)\n    (12): Linear(in_features=2, out_features=1, bias=True)\n  )\n)\n\n\n{0: array([ 1.43207282e-01,  6.29711570e-03,  1.04200505e-01, -3.79187055e-03,\n        -1.74976081e-01, -7.97475874e-02, -2.00860098e-01,  2.48444706e-01,\n         1.42530382e-01, -2.86848284e-03,  3.61538306e-02, -5.21567464e-02,\n        -2.15294853e-01, -1.26742452e-01, -1.79230243e-01,  2.73077637e-01,\n         1.36738747e-01,  8.57899524e-03,  1.01677172e-01,  3.27537209e-03,\n        -1.92429125e-01, -7.95854479e-02, -1.84092522e-01,  2.72164345e-01,\n         1.51459932e-01,  3.70034464e-02,  4.94864471e-02, -6.36564493e-02,\n        -1.63678646e-01, -1.26617596e-01, -2.05547154e-01,  2.25242034e-01,\n         1.54910132e-01,  4.92911926e-03,  6.90693706e-02, -3.28048766e-02,\n        -1.77523270e-01, -1.17699929e-01, -1.95609123e-01,  2.50784457e-01,\n         1.66618377e-01,  1.22015905e-02,  2.58807391e-02, -8.16192627e-02,\n        -2.00623482e-01, -1.17052861e-01, -1.86843857e-01,  2.40996480e-01,\n         1.80479109e-01,  3.72159779e-02,  3.55244242e-02, -3.60636003e-02,\n        -2.09616765e-01, -1.19843856e-01, -1.44335642e-01,  2.73970872e-01,\n         1.46006003e-01, -1.83095448e-02,  8.83664042e-02,  2.28608660e-02,\n        -1.77115664e-01, -1.37761638e-01, -1.90622538e-01,  2.85049826e-01,\n         1.44436479e-01,  1.36893028e-02,  6.65569007e-02, -2.01072544e-04,\n        -1.99043870e-01, -1.11171007e-01, -1.76820531e-01,  2.78549343e-01,\n         1.31597325e-01,  1.31126130e-02,  5.92438430e-02, -6.50760159e-02,\n        -1.55642599e-01, -1.12090096e-01, -2.32182071e-01,  2.25448370e-01,\n         2.09733546e-01,  4.48576212e-02,  1.76887698e-02, -7.26176202e-02,\n        -1.81560591e-01, -1.18118800e-01, -1.55840069e-01,  2.45131820e-01,\n         1.57539800e-01,  4.57477421e-02,  8.64019766e-02,  1.06538944e-02,\n        -2.25713193e-01, -8.36062431e-02, -1.51326194e-01,  2.42097005e-01,\n         1.46130219e-01, -6.08363748e-03,  4.69235405e-02, -4.06553820e-02,\n        -1.90215483e-01, -1.30105391e-01, -1.91207454e-01,  2.75829673e-01,\n         1.37035578e-01,  1.32784341e-02,  8.11730698e-02, -2.83419956e-02,\n        -1.72134370e-01, -1.05717532e-01, -1.93411276e-01,  2.68321216e-01,\n         1.24822736e-01, -2.49985605e-02,  5.46513647e-02, -3.76937985e-02,\n        -2.02080101e-01, -1.29510283e-01, -1.99880868e-01,  2.84415096e-01,\n         1.36025175e-01,  2.10405458e-02,  1.25923350e-01, -1.76883452e-02,\n        -1.46617338e-01, -1.00234658e-01, -2.21794963e-01,  2.05139220e-01],\n       dtype=float32),\n 3: array([-0.09106569,  0.1583102 ,  0.29874575, -0.05709067, -0.07168067,\n         0.13238074,  0.29310873, -0.04537553, -0.08868651,  0.1509394 ,\n         0.29576218, -0.05088371, -0.07256822,  0.15756652,  0.29804155,\n        -0.06024089, -0.07925774,  0.15159757,  0.29655147, -0.05204486,\n        -0.06510482,  0.14707126,  0.2955585 , -0.05045141, -0.05945834,\n         0.15397522,  0.28643155, -0.03937228, -0.0780265 ,  0.14430483,\n         0.2993904 , -0.04338944, -0.07745007,  0.14382583,  0.29152495,\n        -0.04569359, -0.0820166 ,  0.14775376,  0.30206323, -0.06361473,\n        -0.05014775,  0.166575  ,  0.28808075, -0.04191206, -0.07614301,\n         0.16806597,  0.29809946, -0.05615523, -0.07369395,  0.1361293 ,\n         0.2925982 , -0.04455033, -0.08367016,  0.1473538 ,  0.29441217,\n        -0.05101946, -0.07929114,  0.129256  ,  0.29300398, -0.04631316,\n        -0.09977546,  0.17411752,  0.30642375, -0.07330883], dtype=float32),\n 6: array([ 0.0289472 , -0.15329668,  0.0478624 ,  0.5073338 ,  0.0341417 ,\n        -0.16241008,  0.05829808,  0.5058923 ,  0.0301194 , -0.15560818,\n         0.05099656,  0.5068564 ,  0.02897662, -0.15344843,  0.04822756,\n         0.5072659 ,  0.03012997, -0.15550743,  0.05065322,  0.5069246 ,\n         0.03103477, -0.15709649,  0.05247599,  0.50667256,  0.02730933,\n        -0.15252225,  0.05098768,  0.5065358 ,  0.03256606, -0.1589647 ,\n         0.05305114,  0.5067359 ,  0.03095145, -0.15756463,  0.0541862 ,\n         0.5063291 ,  0.03229896, -0.1581411 ,  0.0514296 ,  0.50702184,\n         0.02454496, -0.14787355,  0.04604906,  0.50718296,  0.02638293,\n        -0.14930864,  0.04427201,  0.5077407 ,  0.03309862, -0.16082473,\n         0.05695034,  0.50603575,  0.03071197, -0.15675312,  0.05250891,\n         0.5066291 ,  0.03489432, -0.16362445,  0.05948593,  0.50574666,\n         0.02671531, -0.14859803,  0.04098555,  0.5084204 ], dtype=float32),\n 9: array([0.04397329, 0.23183572, 0.0411244 , 0.22675759, 0.0430866 ,\n        0.23046201, 0.0438614 , 0.2317175 , 0.04319487, 0.23055825,\n        0.04269706, 0.22967225, 0.04286424, 0.23156166, 0.0426398 ,\n        0.22890632, 0.0421553 , 0.22920491, 0.04312574, 0.22948456,\n        0.04418794, 0.23408437, 0.04489119, 0.23388621, 0.0414625 ,\n        0.22755873, 0.0426609 , 0.22978865, 0.04081305, 0.22611658,\n        0.04594607, 0.23471704], dtype=float32),\n 12: array([-0.30635482, -0.30988604, -0.3073418 , -0.30644947, -0.30726004,\n        -0.30787635, -0.30680704, -0.308307  , -0.3082779 , -0.3078606 ,\n        -0.30507785, -0.3049926 , -0.30935943, -0.3078232 , -0.3103186 ,\n        -0.30425358], dtype=float32)}\n\n\n\nget_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n\n{'layers.0.weight': array([ 0.10417588, -0.04161512,  0.10597267,  0.02180895,  0.12001498,\n         0.02890352,  0.0114617 ,  0.08183316,  0.2495192 ,  0.5108763 ,\n         0.14668094, -0.07902834,  0.00912531,  0.02640062,  0.14108546,\n         0.06816655,  0.14256878, -0.00347906,  0.07373643,  0.23171762,\n         0.08313342, -0.03320929,  0.08456727,  0.01740376,  0.09577317,\n         0.02306532,  0.00914657,  0.06530368,  0.19911884,  0.40768448,\n         0.04405227,  0.03805925,  0.015035  ,  0.0069457 ,  0.0094994 ,\n         0.03021198, -0.01876849,  0.02160798, -0.03238906, -0.02050959,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        -0.05415884,  0.02163482, -0.05509295, -0.01133801, -0.06239325,\n        -0.01502633, -0.0059587 , -0.04254334, -0.12971973, -0.26559377],\n       dtype=float32),\n 'layers.3.weight': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n        -5.8896475e+00, -6.3057983e-01, -2.5641673e+00, -8.9936301e-02,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.0009731e+01,\n         5.1539731e-01,  5.5181418e-02,  2.2438776e-01,  7.8702364e-03,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  8.7594181e-01,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n       dtype=float32),\n 'layers.6.weight': array([ 0.       ,  7.6445217, 15.00777  ,  0.       ,  0.       ,\n         0.       ,  0.       ,  0.       ,  0.       , 11.027902 ,\n        21.650043 ,  0.       ,  0.       ,  3.4587548,  6.7902484,\n         0.       ], dtype=float32),\n 'layers.9.weight': array([ -2.3285935,   0.       ,  -3.9471314, -39.11015  ,  -4.6057267,\n          0.       ,  -7.807034 , -77.35597  ], dtype=float32),\n 'layers.12.weight': array([-12.126857, -64.91129 ], dtype=float32)}\n\n\n\nget_weights(model)\n\n{'Layer 0': array([-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n         0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ,\n        -0.11684369,  0.11833665, -0.2683918 , -0.19186287, -0.11611126,\n        -0.06214499, -0.2412386 ,  0.20706299, -0.07457635,  0.10150522,\n         0.22361842,  0.05891514,  0.08647272,  0.3052416 , -0.1426217 ,\n         0.10016555, -0.14069483,  0.22599205,  0.25255737, -0.29155323,\n         0.2699465 ,  0.1510033 ,  0.13780165,  0.13018301,  0.26287982,\n        -0.04175457, -0.26743335, -0.09074122, -0.2227112 ,  0.02090478,\n        -0.0590421 , -0.16961981, -0.02875188,  0.2995954 , -0.02494261,\n         0.01004025, -0.04931906,  0.04971322,  0.28176293,  0.19337103,\n         0.11224869,  0.06871963,  0.07456425,  0.12216929, -0.04086405,\n        -0.29390487, -0.19555901,  0.26992753,  0.01890203, -0.25616774,\n         0.04987782,  0.26129004, -0.29883513, -0.21289697, -0.12594265,\n         0.0126926 , -0.07375361, -0.03475064, -0.30828732,  0.14808285,\n         0.27756676,  0.19329056, -0.22393112, -0.25491226,  0.13131431,\n         0.00710201,  0.12963155, -0.3090024 , -0.01885444,  0.22301766],\n       dtype=float32),\n 'Layer 3': array([ 0.19455573,  0.1236456 , -0.2711233 ,  0.27280954,  0.11085409,\n         0.24458632, -0.13908438,  0.07495221,  0.3452033 ,  0.2378209 ,\n         0.28354862, -0.07424083,  0.26936427, -0.27691442,  0.03057846,\n        -0.19906998, -0.08245403, -0.0905441 ,  0.02645254,  0.32178298,\n         0.1750386 , -0.00149772,  0.2509683 , -0.1811804 ,  0.18221131,\n        -0.03278595, -0.06152213,  0.0413917 , -0.27085608,  0.04085568,\n         0.1188781 ,  0.302264  ], dtype=float32),\n 'Layer 6': array([ 0.4752962 , -0.24824601,  0.22039747,  0.19587505,  0.13966405,\n         0.39540154, -0.20208222,  0.13140953,  0.00280607, -0.3760708 ,\n        -0.12140697, -0.33391154,  0.22107768,  0.04494798,  0.04898232,\n        -0.15168536], dtype=float32),\n 'Layer 9': array([ 0.07573527, -0.22145915, -0.30541402,  0.03821951, -0.3709231 ,\n        -0.3758251 , -0.3254385 , -0.1698224 ], dtype=float32),\n 'Layer 12': array([0.27389026, 0.5417277 ], dtype=float32)}\n\n\n\nvisualize_activations(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\", cmap=\"BlueWhiteRed\", absolute=False)\n\nnet: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=8, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.01, inplace=False)\n    (3): Linear(in_features=8, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.01, inplace=False)\n    (6): Linear(in_features=4, out_features=4, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.01, inplace=False)\n    (9): Linear(in_features=4, out_features=2, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.01, inplace=False)\n    (12): Linear(in_features=2, out_features=1, bias=True)\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_weights_distributions(model, color=f\"C{0}\")\n\nn:5\n\n\n\n\n\n\n\n\n\n\nvisualize_gradient_distributions(model, fun_control, batch_size=batch_size, color=f\"C{0}\")\n\nn:5\n\n\n\n\n\n\n\n\n\n\nvisualize_weights(model, absolute=True, cmap=\"gray\", figsize=(6, 6))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_gradients(model, fun_control, batch_size, absolute=True, cmap=\"BlueWhiteRed\", figsize=(6, 6))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_activations(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n\nnet: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=8, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.01, inplace=False)\n    (3): Linear(in_features=8, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.01, inplace=False)\n    (6): Linear(in_features=4, out_features=4, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.01, inplace=False)\n    (9): Linear(in_features=4, out_features=2, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.01, inplace=False)\n    (12): Linear(in_features=2, out_features=1, bias=True)\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_activations(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n\nnet: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=8, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.01, inplace=False)\n    (3): Linear(in_features=8, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.01, inplace=False)\n    (6): Linear(in_features=4, out_features=4, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.01, inplace=False)\n    (9): Linear(in_features=4, out_features=2, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.01, inplace=False)\n    (12): Linear(in_features=2, out_features=1, bias=True)\n  )\n)",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Explainable AI with SpotPython and Pytorch</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html",
    "href": "035_spot_lightning_transformer_introduction.html",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "",
    "text": "27.1 Transformer Basics",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#sec-transformer-basics",
    "href": "035_spot_lightning_transformer_introduction.html#sec-transformer-basics",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "",
    "text": "27.1.1 Embedding\nWord embedding is a technique where words or phrases (so-called tokens) from the vocabulary are mapped to vectors of real numbers. These vectors capture the semantic properties of the words. Words that are similar in meaning are mapped to vectors that are close to each other in the vector space, and words that are dissimilar are mapped to vectors that are far apart. Word embeddings are needed for transformers for several reasons:\n\nDimensionality Reduction: Word embeddings reduce the dimensionality of the data. Instead of dealing with high-dimensional sparse vectors (like one-hot encoded vectors), we deal with dense vectors of much lower dimensionality.\nCapturing Semantic Similarities: Word embeddings capture semantic similarities between words. This is crucial for tasks like text classification, sentiment analysis, etc., where the meaning of the words is important.\nHandling Unknown Words: If a word is not present in the training data but appears in the test data, one-hot encoding cannot handle it. But word embeddings can handle such situations by mapping the unknown word to a vector that is similar to known words.\nInput to Neural Networks: Transformers, like other neural networks, work with numerical data. Word embeddings provide a way to convert text data into numerical form that can be fed into these networks.\n\nIn the context of transformers, word embeddings are used as the initial input representation. The transformer then learns more complex representations by considering the context in which each token appears.\n\n27.1.1.1 Neural Network for Embeddings\nIdea for word embeddings: use a relatively simple NN that has one input for every token (word, symbol) in the vocabulary. The output of the NN is a vector of a fixed size, which is the word embedding. The network that is used in this chapter is visualized in Figure 27.1. For simplicity, a 2-dimensional output vector is used in this visualization. The weights of the NN are randomly initialized, and are learned during training.\n\n\n\n\n\n\nFigure 27.1: Transformer. Computation of the self attention. In this example, we consider two inputs, i.e., (1,0) and (0,1). For each input, there are two values, which results in a \\(2 \\times 2\\) matrix. In general, when there are \\(T\\) inputs, a \\(T \\times T\\) matrix will be generated. Figure credits: Starmer, Josh: Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained.\n\n\n\nAll tokens are embedded in this way. For each token there are two numerical values, the embedding vector. The same network is used for embedding all tokens. If a longer input is added, it can be embedded with the same net.\n\n\n27.1.1.2 Positional Encoding for the Embeddings\nPositional encoding is added to the input embeddings to give the model some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed.\nIf a token occurs several times, it is embedded several times and receives different embedding vectors, as the position is taken into account by the positional encoding.\n\n\n\n27.1.2 Attention\nAttention describes how similar is each token to itself and to all other tokens in the input, e.g., in a sentence. The attention mechanism can be implemented as a set of layers in neural networks. There are a lot of different possible definitions of “attention” in the literature, but the one we will use here is the following: the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys (Lippe 2022).\nThe goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to “attend” more than others.\nCalculation of the self-attention:\n\nQueries: Calculate two new values from the (two) values of the embedding vector using an NN, which are referred to as query values.\nKeys: Calculate two new values, called key values, from the (two) values of the embedding vector using an NN.\nDot product: Calculate the dot product of the query values and the key values. This is a measure of the similarity of the query and key values.\nSoftmax: Apply the softmax function to the outputs from the dot product. This is a measure of the attention that a token pays to other tokens.\nValues: Calculate two new values from the (two) values of the embedding vector using an NN, which are referred to as value values.\nThe values are multiplied (weighted) by the values of the softmax function.\nThe weighted values are summed. Now we have the self attention value for the token.\n\n\n\n27.1.3 Self-Attention\nMost attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called “self-attention”. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements’ keys, and returned a different, averaged value vector for each element.\n\n\n27.1.4 Masked Self-Attention\nMasked self-attention is a variant of the self-attention method described in Section 27.1.3. It asks the question: How similar is each token to itself and to all preceding tokens in the input (sentence)? Masked self-attention is an autoregressive mechanism, which means that the attention mechanism is only allowed to look at the tokens that have already been processed. Calculation of the mask self-attention is identical to the self-attention, but the attention is only calculated for the tokens that have already been processed. If the masked self-attention method is applied to the first token, the masked self-attention value is exactly the value of the first token, as it only takes itself into account. For the other tokens, the masked self-attention value is a weighted sum of the values of the previous tokens. The weighting is determined by the similarity of the query values and the key values (dot product and softmax).\n\n\n27.1.5 Generation of Outputs\nTo calculate the output, we use a residual connector that adds the output of the neural network and the output of the masked self-attention method. We thus obtain the residual connection values. The residual connector is used to facilitate training.\nTo generate the next token, we use another neural network that calculates the output from the (two) residual connection values. The input layer of the neural network has the size of the residual connection values, the output layer has the number of tokens in the vocabulary as a dimension.\nIf we now enter the residual connection value of the first token, we receive the token (or the probabilities using Softmax) that is to come next as the output of the neural network. This makes sense even if we already know the second token (as with the first token): We can use it to calculate the error of the neural network and train the network. In addition, the decoder-transformer uses the masked self-attention method to calculate the output, i.e. the encoding and generation of new tokens is done with exactly the same elements of the network.\nNote: ChatGPT does not use a new neural network, but the same network that was already used to calculate the embedding. The network is therefore used for embedding, masked self-attention and calculating the output. In the last calculation, the network is inverted, i.e. it is run in the opposite direction to obtain the tokens and not the embeddings as in the original run.\n\n\n27.1.6 End-Of-Sequence-Token\nThe end-of-sequence token is used to signal the end of the input and also to start generating new tokens after the input. The EOS token recognizes all other tokens, as it comes after all tokens. When generating tokens, it is important to consider the relationships between the input tokens and the generation of new tokens.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#sec-details-implementation",
    "href": "035_spot_lightning_transformer_introduction.html#sec-details-implementation",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "27.2 Details of the Implementation",
    "text": "27.2 Details of the Implementation\nWe will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the (scaled) dot product attention. The variables shown in Table 27.1 are used in the Transformer architecture.\n\n\n\nTable 27.1: Variables used in the Transformer architecture.\n\n\n\n\n\n\n\n\n\n\nSymbol\nVariable\nDescription\n\n\n\n\n\\(Q\\)\nquery\nThe query vectors.\n\n\n\\(K\\)\nkey\nThe key vectors.\n\n\n\\(V\\)\nvalue\nThe value vectors.\n\n\n\\(d_{\\text{model}}\\)\nd_model\nThe dimensionality of the input and output features of the Transformer.\n\n\n\\(d_k\\)\nd_k\nThe hidden dimensionality of the key and query vectors.\n\n\n\\(d_v\\)\nd_v\nThe hidden dimensionality of the value vectors.\n\n\n\\(h\\)\nnum_heads\nThe number of heads in the Multi-Head Attention layer.\n\n\n\\(B\\)\nbatch_size\nThe batch size.\n\n\n\\(T\\)\nseq_length\nThe sequence length.\n\n\n\\(X\\)\nx\nThe input features (input elements in the sequence).\n\n\n\\(W^{Q}\\)\nqkv_proj\nThe weight matrix to transform the input to the query vectors.\n\n\n\\(W^{K}\\)\nqkv_proj\nThe weight matrix to transform the input to the key vectors.\n\n\n\\(W^{V}\\)\nqkv_proj\nThe weight matrix to transform the input to the value vectors.\n\n\n\\(W^{O}\\)\no_proj\nThe weight matrix to transform the concatenated output of the Multi-Head Attention layer to the final output.\n\n\n\\(N\\)\nnum_layers\nThe number of layers in the Transformer.\n\n\n\\(PE_{(pos,i)}\\)\npositional_encoding\nThe positional encoding for position \\(pos\\) and hidden dimensionality \\(i\\).\n\n\n\n\n\n\nSummarizing the ideas from Section 27.1, an attention mechanism has usually four parts we need to specify (Lippe 2022):\n\nQuery: The query is a feature vector that describes what we are looking for in the sequence, i.e., what would we maybe want to pay attention to.\nKeys: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\nScore function: To rate which elements we want to pay attention to, we need to specify a score function \\(f_{attn}\\). The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\nValues: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n\nThe weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write:\n\\[\n\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n\\]\nVisually, we can show the attention over a sequence of words as follows:\n\n\n\nAttention over a sequence of words. For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights. Figure taken from Lippe (2022)\n\n\n\n27.2.1 Dot Product Attention\nOur goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries \\(Q\\in\\mathbb{R}^{T\\times d_k}\\), keys \\(K\\in\\mathbb{R}^{T\\times d_k}\\) and values \\(V\\in\\mathbb{R}^{T\\times d_v}\\) where \\(T\\) is the sequence length, and \\(d_k\\) and \\(d_v\\) are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element \\(i\\) to \\(j\\) is based on its similarity of the query \\(Q_i\\) and key \\(K_j\\), using the dot product as the similarity metric (in Figure 27.1, we considered \\(Q_2\\) and \\(K_1\\) as well as \\(Q_2\\) and \\(K_2\\)). The dot product attention is calculated as follows:\n\\[\n\\text{Attention}(Q,K,V)=\\text{softmax}\\left(QK^T\\right) V\n\\tag{27.1}\\]\nThe matrix multiplication \\(QK^T\\) performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape \\(T\\times T\\). Each row represents the attention logits for a specific element \\(i\\) to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention).\n\n\n27.2.2 Scaled Dot Product Attention\nAn additional aspect is the scaling of the dot product using a scaling factor of \\(1/\\sqrt{d_k}\\). This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. We initialize our layers with the intention of having equal variance throughout the model, and hence, \\(Q\\) and \\(K\\) might also have a variance close to \\(1\\). However, performing a dot product over two vectors with a variance \\(\\sigma^2\\) results in a scalar having \\(d_k\\)-times higher variance:\n\\[\nq_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k\n\\]\nIf we do not scale down the variance back to \\(\\sim\\sigma^2\\), the softmax over the logits will already saturate to \\(1\\) for one random element and \\(0\\) for all others. The gradients through the softmax will be close to zero so that we can’t learn the parameters appropriately. Note that the extra factor of \\(\\sigma^2\\), i.e., having \\(\\sigma^4\\) instead of \\(\\sigma^2\\), is usually not an issue, since we keep the original variance \\(\\sigma^2\\) close to \\(1\\) anyways. Equation 27.1 can be modified as follows to calculate the dot product attention:\n\\[\n\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V.\n\\]\nAnother perspective on this scaled dot product attention mechanism offers the computation graph which is visualized in Figure 27.2.\n\n\n\n\n\n\nFigure 27.2: Scaled dot product attention. Figure credit Vaswani et al. (2017)\n\n\n\nThe block Mask (opt.) in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value.\nAfter we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#sec-transformer-in-lightning",
    "href": "035_spot_lightning_transformer_introduction.html#sec-transformer-in-lightning",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "27.3 Example: Transformer in Lightning",
    "text": "27.3 Example: Transformer in Lightning\nThe following code is based on https://github.com/phlippe/uvadlc_notebooks/tree/master (Author: Phillip Lippe)\nFirst, we import the necessary libraries and download the pretrained models.\n\nimport os\nimport numpy as np\nimport random\nimport math\nimport json\nfrom functools import partial\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import to_rgb\nimport matplotlib\nimport seaborn as sns\n\n## tqdm for loading bars\nfrom tqdm.notebook import tqdm\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\n\n# PyTorch Lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\n\n# Path to the folder where the pretrained models are saved\nCHECKPOINT_PATH = \"../saved_models/tutorial6\"\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\nfrom spotPython.utils.device import getDevice\ndevice = getDevice()\nprint(\"Device:\", device)\n\nDevice: mps\n\n\n\n# Setting the seed\npl.seed_everything(42)\n\n42\n\n\nTwo pre-trained models are downloaded below. Make sure to have adjusted your CHECKPOINT_PATH before running this code if not already done.\n\nimport urllib.request\nfrom urllib.error import HTTPError\n# Github URL where saved models are stored for this tutorial\nbase_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n# Files to download\npretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n\n# Create checkpoint path if it doesn't exist yet\nos.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n\n27.3.1 Downloading the Pretrained Models\n\n# For each file, check whether it already exists. If not, try downloading it.\nfor file_name in pretrained_files:\n    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n    if \"/\" in file_name:\n        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n    if not os.path.isfile(file_path):\n        file_url = base_url + file_name\n        print(f\"Downloading {file_url}...\")\n        try:\n            urllib.request.urlretrieve(file_url, file_path)\n        except HTTPError as e:\n            print(\"Error:\\n\", e)\n\n\n\n27.3.2 The Transformer Architecture\nWe will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module nn.Transformer (documentation) and a tutorial on how to use it for next token prediction. However, we will implement it here ourselves, to get through to the smallest details.\n\n\n27.3.3 Attention Mechanism\n\ndef scaled_dot_product(q, k, v, mask=None):\n    \"\"\"\n    Compute scaled dot product attention.\n    Args:\n        q: Queries\n        k: Keys\n        v: Values\n        mask: Mask to apply to the attention logits\n\n    Returns:\n        Tuple of (Values, Attention weights)\n\n    Examples:\n    &gt;&gt;&gt; seq_len, d_k = 1, 2\n        pl.seed_everything(42)\n        q = torch.randn(seq_len, d_k)\n        k = torch.randn(seq_len, d_k)\n        v = torch.randn(seq_len, d_k)\n        values, attention = scaled_dot_product(q, k, v)\n        print(\"Q\\n\", q)\n        print(\"K\\n\", k)\n        print(\"V\\n\", v)\n        print(\"Values\\n\", values)\n        print(\"Attention\\n\", attention)\n    \"\"\"\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention\n\nNote that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let’s generate a few random queries, keys, and value vectors, and calculate the attention outputs:\n\nseq_len, d_k = 1, 2\npl.seed_everything(42)\nq = torch.randn(seq_len, d_k)\nk = torch.randn(seq_len, d_k)\nv = torch.randn(seq_len, d_k)\nvalues, attention = scaled_dot_product(q, k, v)\nprint(\"Q\\n\", q)\nprint(\"K\\n\", k)\nprint(\"V\\n\", v)\nprint(\"Values\\n\", values)\nprint(\"Attention\\n\", attention)\n\nQ\n tensor([[0.3367, 0.1288]])\nK\n tensor([[0.2345, 0.2303]])\nV\n tensor([[-1.1229, -0.1863]])\nValues\n tensor([[-1.1229, -0.1863]])\nAttention\n tensor([[1.]])\n\n\n\n\n27.3.4 Multi-Head Attention\nThe scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into \\(h\\) sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n\\[\n\\begin{split}\n    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n\\end{split}\n\\]\nWe refer to this as Multi-Head Attention layer with the learnable parameters \\(W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}\\), \\(W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}\\), \\(W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}\\), and \\(W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}\\) (\\(D\\) being the input dimensionality). Expressed in a computational graph, we can visualize it as in Figure 27.3.\n\n\n\n\n\n\nFigure 27.3: Multi-Head Attention. Figure taken from Vaswani et al. (2017)\n\n\n\nHow are we applying a Multi-Head Attention layer in a neural network, where we do not have an arbitrary query, key, and value vector as input? Looking at the computation graph in Figure 27.3, a simple but effective implementation is to set the current feature map in a NN, \\(X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}\\), as \\(Q\\), \\(K\\) and \\(V\\) (\\(B\\) being the batch size, \\(T\\) the sequence length, \\(d_{\\text{model}}\\) the hidden dimensionality of \\(X\\)). The consecutive weight matrices \\(W^{Q}\\), \\(W^{K}\\), and \\(W^{V}\\) can transform \\(X\\) to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below.\nAs a consequence, if the embedding dimension is 4, then 1, 2 or 4 heads can be used, but not 3. If 4 heads are used, then the dimension of the query, key and value vectors is 1. If 2 heads are used, then the dimension of the query, key and value vectors is \\(D=2\\). If 1 head is used, then the dimension of the query, key and value vectors is \\(D=4\\). The number of heads is a hyperparameter that can be adjusted. The number of heads is usually 8 or 16.\n\n# Helper function to support different mask shapes.\n# Output shape supports (batch_size, number of heads, seq length, seq length)\n# If 2D: broadcasted over batch size and number of heads\n# If 3D: broadcasted over number of heads\n# If 4D: leave as is\ndef expand_mask(mask):\n    assert mask.ndim &gt;= 2, \"Mask must be &gt;= 2-dim. with seq_length x seq_length\"\n    if mask.ndim == 3:\n        mask = mask.unsqueeze(1)\n    while mask.ndim &lt; 4:\n        mask = mask.unsqueeze(0)\n    return mask\n\n\nclass MultiheadAttention(nn.Module):\n    \n    def __init__(self, input_dim, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dim. must be 0 modulo number of heads.\"\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Stack all weight matrices 1...h together for efficiency\n        # Note that in many implementations you see \"bias=False\" which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n        \n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        if mask is not None:\n            mask = expand_mask(mask)\n        qkv = self.qkv_proj(x)\n        \n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n        \n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        o = self.o_proj(values)\n        \n        if return_attention:\n            return o, attention\n        else:\n            return o\n\n\n\n27.3.5 Permutation Equivariance\nOne crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. \\(X_1\\leftrightarrow X_2\\) (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look in Section 27.3.8.\n\n\n27.3.6 Transformer Encoder\nNext, we will look at how to apply the multi-head attention block inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. While this structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding, we will focus here on the encoder part. Many advances in NLP have been made using pure encoder-based Transformer models (if interested, models include the BERT-family (Devlin et al. 2018), the Vision Transformer (Dosovitskiy et al. 2020), and more). We will also mainly focus on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as shown in Figure 27.4.\n\n\n\n\n\n\nFigure 27.4: Transformer architecture. Figure credit: Vaswani et al. (2017)\n\n\n\nThe encoder consists of \\(N\\) identical blocks that are applied in sequence. Taking as input \\(x\\), it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates \\[\n\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))\n\\] (\\(x\\) being \\(Q\\), \\(K\\) and \\(V\\) input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons:\n\nSimilar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\nWithout the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position \\(i\\) has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow.\n\n\n\n27.3.7 Layer Normalization and Feed-Forward Network\nThe Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence.\nWe are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate).\nAdditionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear\\(\\to\\)ReLU\\(\\to\\)Linear MLP. The full transformation including the residual connection can be expressed as:\n\\[\n\\begin{split}\n    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n\\end{split}\n\\]\nThis MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to “post-process” the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8\\(\\times\\) larger than \\(d_{\\text{model}}\\), i.e. the dimensionality of the original input \\(x\\). The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\nFinally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization.\n\nclass EncoderBlock(nn.Module):\n    \n    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Dimensionality of the input\n            num_heads - Number of heads to use in the attention block\n            dim_feedforward - Dimensionality of the hidden layer in the MLP\n            dropout - Dropout probability to use in the dropout layers\n        \"\"\"\n        super().__init__()\n        \n        # Attention layer\n        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n        \n        # Two-layer MLP\n        self.linear_net = nn.Sequential(\n            nn.Linear(input_dim, dim_feedforward),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim_feedforward, input_dim)\n        )\n        \n        # Layers to apply in between the main layers\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Attention part\n        attn_out = self.self_attn(x, mask=mask)\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n        \n        # MLP part\n        linear_out = self.linear_net(x)\n        x = x + self.dropout(linear_out)\n        x = self.norm2(x)\n        \n        return x\n\nBased on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called get_attention_maps. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including Jain and Wallace (2019) and Wiegreffe and Pinter (2019)).\n\nclass TransformerEncoder(nn.Module):\n    \n    def __init__(self, num_layers, **block_args):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [EncoderBlock(**block_args) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for l in self.layers:\n            x = l(x, mask=mask)\n        return x\n\n    def get_attention_maps(self, x, mask=None):\n        attention_maps = []\n        for l in self.layers:\n            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n            attention_maps.append(attn_map)\n            x = l(x)\n        return attention_maps\n\n\n\n27.3.8 Positional Encoding\nWe have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. (2017) are sine and cosine functions of different frequencies, as follows:\n\\[\nPE_{(pos,i)} = \\begin{cases}\n    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n\\end{cases}\n\\]\n\\(PE_{(pos,i)}\\) represents the position encoding at position \\(pos\\) in the sequence, and hidden dimensionality \\(i\\). These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see “Positional encoding”), and constitute the position information. We distinguish between even (\\(i \\text{ mod } 2=0\\)) and uneven (\\(i \\text{ mod } 2=1\\)) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent \\(PE_{(pos+k,:)}\\) as a linear function of \\(PE_{(pos,:)}\\), which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from \\(2\\pi\\) to \\(10000\\cdot 2\\pi\\).\nThe positional encoding is implemented below. The code is taken from the PyTorch tutorial https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model about Transformers on NLP and adjusted for our purposes.\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing \n        # the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        # register_buffer =&gt; Tensor which is not a parameter,\n        # but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the \n        # state dict (e.g. when we save the model) \n        self.register_buffer('pe', pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x\n\nTo understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let’s do it below.\n\nmatplotlib.rcParams['lines.linewidth'] = 2.0\nplt.set_cmap('cividis')\nencod_block = PositionalEncoding(d_model=48, max_len=96)\npe = encod_block.pe.squeeze().T.cpu().numpy()\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\npos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\nfig.colorbar(pos, ax=ax)\nax.set_xlabel(\"Position in sequence\")\nax.set_ylabel(\"Hidden dimension\")\nax.set_title(\"Positional encoding over hidden dimensions\")\nax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\nax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nYou can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions \\(1\\), \\(2\\), \\(3\\) and \\(4\\).\n\nsns.set_theme()\nfig, ax = plt.subplots(2, 2, figsize=(12,4))\nax = [a for a_list in ax for a in a_list]\nfor i in range(len(ax)):\n    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\",\n                markersize=6, markeredgecolor=\"black\")\n    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n    ax[i].set_xticks(np.arange(1,17))\n    ax[i].tick_params(axis='both', which='major', labelsize=10)\n    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n    ax[i].set_ylim(-1.2, 1.2)\nfig.subplots_adjust(hspace=0.8)\nsns.reset_orig()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the patterns between the hidden dimension \\(1\\) and \\(2\\) only differ in the starting angle. The wavelength is \\(2\\pi\\), hence the repetition after position \\(6\\). The hidden dimensions \\(2\\) and \\(3\\) have about twice the wavelength.\n\n\n27.3.9 Learning Rate Warm-up\nOne commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations. Thus, we slowly start learning instead of taking very large steps from the beginning. In fact, training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing. Take for instance the following plot by Liu et al. (2019) comparing Adam-vanilla (i.e. Adam without warm-up) vs Adam with a warm-up:\n\n\n\nWarm-up comparison. Figure taken from Liu et al. (2019)\n\n\nClearly, the warm-up is a crucial hyperparameter in the Transformer architecture. Why is it so important? There are currently two common explanations. Firstly, Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like RAdam have been shown to overcome this issue, not requiring warm-up for training Transformers. Secondly, the iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using Pre-Layer Normalization (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques (Adaptive Normalization, Power Normalization).\nNevertheless, many applications and papers still use the original Transformer architecture with Adam, because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations. There are many different schedulers we could use. For instance, the original Transformer paper used an exponential decay scheduler with a warm-up. However, the currently most popular scheduler is the cosine warm-up scheduler, which combines warm-up with a cosine-shaped learning rate decay. We can implement it below, and visualize the learning rate factor over epochs.\n\nclass CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n    \n    def __init__(self, optimizer, warmup, max_iters):\n        self.warmup = warmup\n        self.max_num_iters = max_iters\n        super().__init__(optimizer)\n        \n    def get_lr(self):\n        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n        return [base_lr * lr_factor for base_lr in self.base_lrs]\n    \n    def get_lr_factor(self, epoch):\n        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n        if epoch &lt;= self.warmup:\n            lr_factor *= epoch * 1.0 / self.warmup\n        return lr_factor\n\n\n# Needed for initializing the lr scheduler\np = nn.Parameter(torch.empty(4,4))\noptimizer = optim.Adam([p], lr=1e-3)\nlr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n\n# Plotting\nepochs = list(range(2000))\nsns.set()\nplt.figure(figsize=(8,3))\nplt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\nplt.ylabel(\"Learning rate factor\")\nplt.xlabel(\"Iterations (in batches)\")\nplt.title(\"Cosine Warm-up Learning Rate Scheduler\")\nplt.show()\nsns.reset_orig()\n\n\n\n\n\n\n\n\nIn the first 100 iterations, we increase the learning rate factor from 0 to 1, whereas for all later iterations, we decay it using the cosine wave. Pre-implementations of this scheduler can be found in the popular NLP Transformer library huggingface.\n\n\n27.3.10 PyTorch Lightning Module\nFinally, we can embed the Transformer architecture into a PyTorch lightning module. PyTorch Lightning simplifies our training and test code, as well as structures the code nicely in separate functions. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional [CLS] token to the sequence (CLS stands for classification, i.e., the first token of every sequence is always a special classification token, CLS). However, here we focus on tasks where we have an output per element.\nAdditionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). We also add the learning rate scheduler, which takes a step each iteration instead of once per epoch. This is needed for the warmup and the smooth cosine decay. The training, validation, and test step is left empty for now and will be filled for our task-specific models.\n\nclass TransformerPredictor(pl.LightningModule):\n\n    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Hidden dimensionality of the input\n            model_dim - Hidden dimensionality to use inside the Transformer\n            num_classes - Number of classes to predict per sequence element\n            num_heads - Number of heads to use in the Multi-Head Attention blocks\n            num_layers - Number of encoder blocks to use.\n            lr - Learning rate in the optimizer\n            warmup - Number of warmup steps. Usually between 50 and 500\n            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n            dropout - Dropout to apply inside the model\n            input_dropout - Dropout to apply on the input features\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self._create_model()\n\n    def _create_model(self):\n        # Input dim -&gt; Model dim\n        self.input_net = nn.Sequential(\n            nn.Dropout(self.hparams.input_dropout),\n            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n        )\n        # Positional encoding for sequences\n        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n        # Transformer\n        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n                                              input_dim=self.hparams.model_dim,\n                                              dim_feedforward=2*self.hparams.model_dim,\n                                              num_heads=self.hparams.num_heads,\n                                              dropout=self.hparams.dropout)\n        # Output classifier per sequence lement\n        self.output_net = nn.Sequential(\n            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n            nn.LayerNorm(self.hparams.model_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(self.hparams.dropout),\n            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n        ) \n\n    def forward(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Inputs:\n            x - Input features of shape [Batch, SeqLen, input_dim]\n            mask - Mask to apply on the attention outputs (optional)\n            add_positional_encoding - If True, we add the positional encoding to the input.\n                                      Might not be desired for some tasks.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)\n        x = self.output_net(x)\n        return x\n\n    @torch.no_grad()\n    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Function for extracting the attention matrices of the whole Transformer for a single batch.\n        Input arguments same as the forward pass.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n        return attention_maps\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n        \n        # Apply lr scheduler per step\n        lr_scheduler = CosineWarmupScheduler(optimizer, \n                                             warmup=self.hparams.warmup, \n                                             max_iters=self.hparams.max_iters)\n        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n\n    def training_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def validation_step(self, batch, batch_idx):\n        raise NotImplementedError    \n\n    def test_step(self, batch, batch_idx):\n        raise NotImplementedError",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#experiment-sequence-to-sequence",
    "href": "035_spot_lightning_transformer_introduction.html#experiment-sequence-to-sequence",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "27.4 Experiment: Sequence to Sequence",
    "text": "27.4 Experiment: Sequence to Sequence\nAfter having finished the implementation of the Transformer architecture, we can start experimenting and apply it to various tasks. We will focus on parallel Sequence-to-Sequence.\nA Sequence-to-Sequence task represents a task where the input and the output is a sequence, not necessarily of the same length. Popular tasks in this domain include machine translation and summarization. For this, we usually have a Transformer encoder for interpreting the input sequence, and a decoder for generating the output in an autoregressive manner. Here, however, we will go back to a much simpler example task and use only the encoder. Given a sequence of \\(N\\) numbers between \\(0\\) and \\(M\\), the task is to reverse the input sequence. In Numpy notation, if our input is \\(x\\), the output should be \\(x\\)[::-1]. Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well.\n\n27.4.1 Dataset and Data Loaders\nFirst, let’s create a dataset class below.\n\nclass ReverseDataset(data.Dataset):\n\n    def __init__(self, num_categories, seq_len, size):\n        super().__init__()\n        self.num_categories = num_categories\n        self.seq_len = seq_len\n        self.size = size\n        \n        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n  \n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        inp_data = self.data[idx]\n        labels = torch.flip(inp_data, dims=(0,))\n        return inp_data, labels\n\nWe create an arbitrary number of random sequences of numbers between 0 and num_categories-1. The label is simply the tensor flipped over the sequence dimension. We can create the corresponding data loaders below.\n\ndataset = partial(ReverseDataset, 10, 16)\ntrain_loader = data.DataLoader(dataset(50000),\n                                batch_size=128,\n                                shuffle=True,\n                                drop_last=True,\n                                pin_memory=True)\nval_loader   = data.DataLoader(dataset(1000), batch_size=128)\ntest_loader  = data.DataLoader(dataset(10000), batch_size=128)\n\n\ninp_data, labels = train_loader.dataset[0]\nprint(\"Input data:\", inp_data)\nprint(\"Labels:    \", labels)\n\nInput data: tensor([0, 4, 1, 2, 5, 5, 7, 6, 9, 6, 3, 1, 9, 3, 1, 9])\nLabels:     tensor([9, 1, 3, 9, 1, 3, 6, 9, 6, 7, 5, 5, 2, 1, 4, 0])\n\n\nDuring training, we pass the input sequence through the Transformer encoder and predict the output for each input token. We use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector. Remember that representing the categories as single scalars decreases the expressiveness of the model extremely as \\(0\\) and \\(1\\) are not closer related than \\(0\\) and \\(9\\) in our example. An alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module nn.Embedding. However, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer (self.input_net maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category).\n\n\n27.4.2 The Reverse Predictor Class\nTo implement the training dynamic, we create a new class inheriting from TransformerPredictor and overwriting the training, validation and test step functions, which were left empty in the base class. We also add a _calculate_loss function to calculate the loss and accuracy for a batch.\n\nclass ReversePredictor(TransformerPredictor):\n    \n    def _calculate_loss(self, batch, mode=\"train\"):\n        # Fetch data and transform categories to one-hot vectors\n        inp_data, labels = batch\n        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n        \n        # Perform prediction and calculate loss and accuracy\n        preds = self.forward(inp_data, add_positional_encoding=True)\n        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n        \n        # Logging\n        self.log(f\"{mode}_loss\", loss)\n        self.log(f\"{mode}_acc\", acc)\n        return loss, acc\n        \n    def training_step(self, batch, batch_idx):\n        loss, _ = self._calculate_loss(batch, mode=\"train\")\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        _ = self._calculate_loss(batch, mode=\"val\")\n    \n    def test_step(self, batch, batch_idx):\n        _ = self._calculate_loss(batch, mode=\"test\")\n\nFinally, we can create a training function. We create a pl.Trainer object, running for \\(N\\) epochs, logging in TensorBoard, and saving our best model based on the validation. Afterward, we test our models on the test set.\n\n\n27.4.3 Gradient Clipping\nAn additional parameter we pass to the trainer here is gradient_clip_val. This clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like DeepAI glossary). For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward. In plain PyTorch, you can apply gradient clipping via torch.nn.utils.clip_grad_norm_(...) (see documentation). The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients.\n\n\n27.4.4 Implementation of the Lightning Trainer\nThe Lightning trainer can be implemented as follows:\n\ndef train_reverse(**kwargs):\n    # Create a PyTorch Lightning trainer with the generation callback\n    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n    os.makedirs(root_dir, exist_ok=True)\n    trainer = pl.Trainer(default_root_dir=root_dir, \n                         callbacks=[ModelCheckpoint(save_weights_only=True,\n                                    mode=\"max\", monitor=\"val_acc\")],\n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n                         devices=1,\n                         max_epochs=10,\n                         gradient_clip_val=5)\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n    \n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ReverseTask.ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(\"Found pretrained model, loading...\")\n        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n    else:\n        model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n        trainer.fit(model, train_loader, val_loader)\n        \n    # Test best model on validation and test set\n    val_result = trainer.test(model, val_loader, verbose=False)\n    test_result = trainer.test(model, test_loader, verbose=False)\n    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n    \n    model = model.to(device)\n    return model, result\n\n\n\n27.4.5 Training the Model\nFinally, we can train the model. In this setup, we will use a single encoder block and a single head in the Multi-Head Attention. This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted as an “explanation” of the predictions (compared to the other papers above dealing with deep Transformers).\n\nreverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n                                              model_dim=32,\n                                              num_heads=1,\n                                              num_classes=train_loader.dataset.num_categories,\n                                              num_layers=1,\n                                              dropout=0.0,\n                                              lr=5e-4,\n                                              warmup=50)\n\nFound pretrained model, loading...\n\n\n\n\n\n\n\n\nThe warning of PyTorch Lightning regarding the number of workers can be ignored for now. As the data set is so simple and the __getitem__ finishes a neglectable time, we don’t need subprocesses to provide us the data (in fact, more workers can slow down the training as we have communication overhead among processes/threads). First, let’s print the results:\n\nprint(f\"Val accuracy:  {(100.0 * reverse_result['val_acc']):4.2f}%\")\nprint(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")\n\nVal accuracy:  100.00%\nTest accuracy: 100.00%\n\n\nAs we would have expected, the Transformer can correctly solve the task.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#visualizing-attention-maps",
    "href": "035_spot_lightning_transformer_introduction.html#visualizing-attention-maps",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "27.5 Visualizing Attention Maps",
    "text": "27.5 Visualizing Attention Maps\nHow does the attention in the Multi-Head Attention block looks like for an arbitrary input? Let’s try to visualize it below.\n\ndata_input, labels = next(iter(val_loader))\ninp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\ninp_data = inp_data.to(device)\nattention_maps = reverse_model.get_attention_maps(inp_data)\n\nThe object attention_maps is a list of length \\(N\\) where \\(N\\) is the number of layers. Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below.\n\nattention_maps[0].shape\n\ntorch.Size([128, 1, 16, 16])\n\n\nNext, we will write a plotting function that takes as input the sequences, attention maps, and an index indicating for which batch element we want to visualize the attention map. We will create a plot where over rows, we have different layers, while over columns, we show the different heads. Remember that the softmax has been applied for each row separately.\n\ndef plot_attention_maps(input_data, attn_maps, idx=0):\n    if input_data is not None:\n        input_data = input_data[idx].detach().cpu().numpy()\n    else:\n        input_data = np.arange(attn_maps[0][idx].shape[-1])\n    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n    \n    num_heads = attn_maps[0].shape[0]\n    num_layers = len(attn_maps)\n    seq_len = input_data.shape[0]\n    fig_size = 4 if num_heads == 1 else 3\n    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n    if num_layers == 1:\n        ax = [ax]\n    if num_heads == 1:\n        ax = [[a] for a in ax]\n    for row in range(num_layers):\n        for column in range(num_heads):\n            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n            ax[row][column].set_xticks(list(range(seq_len)))\n            ax[row][column].set_xticklabels(input_data.tolist())\n            ax[row][column].set_yticks(list(range(seq_len)))\n            ax[row][column].set_yticklabels(input_data.tolist())\n            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n    fig.subplots_adjust(hspace=0.5)\n    cax = fig.add_axes([0.95, 0.15, 0.01, 0.7])\n    cbar = fig.colorbar(ax[0][0].imshow(attn_maps[0][0], origin='lower', vmin=0), cax=cax)\n    cbar.set_label('Attention')\n    plt.show()\n\nFinally, we can plot the attention map of our trained Transformer on the reverse task:\n\nplot_attention_maps(data_input, attention_maps, idx=0)\n\n\n\n\n\n\n\n\nThe model has learned to attend to the token that is on the flipped index of itself. Hence, it actually does what we intended it to do. We see that it however also pays some attention to values close to the flipped index. This is because the model doesn’t need the perfect, hard attention to solve this problem, but is fine with this approximate, noisy attention map. The close-by indices are caused by the similarity of the positional encoding, which we also intended with the positional encoding.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#conclusion",
    "href": "035_spot_lightning_transformer_introduction.html#conclusion",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "27.6 Conclusion",
    "text": "27.6 Conclusion\nIn this chapter, we took a closer look at the Multi-Head Attention layer which uses a scaled dot product between queries and keys to find correlations and similarities between input elements. The Transformer architecture is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block. The Transformer is a very important, recent architecture that can be applied to many tasks and datasets. Although it is best known for its success in NLP, there is so much more to it. We have seen its application on sequence-to-sequence tasks. Its property of being permutation-equivariant if we do not provide any positional encodings, allows it to generalize to many settings. Hence, it is important to know the architecture, but also its possible issues such as the gradient problem during the first iterations solved by learning rate warm-up. If you are interested in continuing with the study of the Transformer architecture, please have a look at the blog posts listed in the “Further Reading” section below.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#additional-considerations",
    "href": "035_spot_lightning_transformer_introduction.html#additional-considerations",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "27.7 Additional Considerations",
    "text": "27.7 Additional Considerations\n\n27.7.1 Complexity and Path Length\nWe can compare the self-attention operation with our other common layer competitors for sequence data: convolutions and recurrent neural networks. In Figure 27.5 you can find a table by Vaswani et al. (2017) on the complexity per layer, the number of sequential operations, and maximum path length. The complexity is measured by the upper bound of the number of operations to perform, while the maximum path length represents the maximum number of steps a forward or backward signal has to traverse to reach any other position. The lower this length, the better gradient signals can backpropagate for long-range dependencies. Let’s take a look at the table in Figure 27.5.\n\n\n\n\n\n\nFigure 27.5: Comparison of complexity and path length of different sequence layers. Table taken from Lippe (2022)\n\n\n\n\\(n\\) is the sequence length, \\(d\\) is the representation dimension and \\(k\\) is the kernel size of convolutions. In contrast to recurrent networks, the self-attention layer can parallelize all its operations making it much faster to execute for smaller sequence lengths. However, when the sequence length exceeds the hidden dimensionality, self-attention becomes more expensive than RNNs. One way of reducing the computational cost for long sequences is by restricting the self-attention to a neighborhood of inputs to attend over, denoted by \\(r\\). Nevertheless, there has been recently a lot of work on more efficient Transformer architectures that still allow long dependencies, of which you can find an overview in the paper by Tay et al. (2020) if interested.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "035_spot_lightning_transformer_introduction.html#further-reading",
    "href": "035_spot_lightning_transformer_introduction.html#further-reading",
    "title": "27  HPT PyTorch Lightning Transformer: Introduction",
    "section": "27.8 Further Reading",
    "text": "27.8 Further Reading\nThere are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n\nTransformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\nThe Illustrated Transformer (Jay Alammar, 2018) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\nAttention? Attention! (Lilian Weng, 2018) - A nice blog post summarizing attention mechanisms in many domains including vision.\nIllustrated: Self-Attention (Raimi Karim, 2019) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\nThe Transformer family (Lilian Weng, 2020) - A very detailed blog post reviewing more variants of Transformers besides the original one.\n\n\n\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” arXiv e-Prints, October, arXiv:1810.04805.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv e-Prints, October, arXiv:2010.11929.\n\n\nJain, Sarthak, and Byron C. Wallace. 2019. “Attention is not Explanation.” arXiv e-Prints, February, arXiv:1902.10186.\n\n\nLippe, Phillip. 2022. “UvA Deep Learning Tutorials.”\n\n\nLiu, Liyuan, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. 2019. “On the Variance of the Adaptive Learning Rate and Beyond.” arXiv e-Prints, August, arXiv:1908.03265.\n\n\nTay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. “Efficient Transformers: A Survey.” arXiv e-Prints, September, arXiv:2009.06732.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv e-Prints, June, 1–15.\n\n\nWiegreffe, Sarah, and Yuval Pinter. 2019. “Attention is not not Explanation.” arXiv e-Prints, August, arXiv:1908.04626.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Introduction</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html",
    "href": "036_spot_lightning_transformer_diabetes.html",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "",
    "text": "28.1 Step 1: Setup\nfrom spotPython.utils.device import getDevice\nfrom math import inf\n\nMAX_TIME = 1\nFUN_EVALS = inf\nINIT_SIZE = 5\nWORKERS = 0\nPREFIX=\"036\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.3\nTORCH_METRIC = \"mean_squared_error\"",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#sec-setup-31",
    "href": "036_spot_lightning_transformer_diabetes.html#sec-setup-31",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "",
    "text": "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size, etc.\nThe parameter MAX_TIME specifies the maximum run time in seconds.\nThe parameter INIT_SIZE specifies the initial design size.\nThe parameter WORKERS specifies the number of workers.\nThe prefix PREFIX is used for the experiment name and the name of the log file.\nThe parameter DEVICE specifies the device to use for training.\n\n\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nAlthough there are no .cuda() or .to(device) calls required, because Lightning does these for you, see LIGHTNINGMODULE, we would like to know which device is used. Threrefore, we imitate the LightningModule behaviour which selects the highest device.\nThe method spotPython.utils.device.getDevice() returns the device that is used by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "036_spot_lightning_transformer_diabetes.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "28.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process.\n\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    _torchmetric=TORCH_METRIC,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    log_level=10,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=TEST_SIZE,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    )",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#step-3-loading-the-diabetes-data-set",
    "href": "036_spot_lightning_transformer_diabetes.html#step-3-loading-the-diabetes-data-set",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.3 Step 3: Loading the Diabetes Data Set",
    "text": "28.3 Step 3: Loading the Diabetes Data Set\n\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\ndataset = Diabetes()\nset_control_key_value(control_dict=fun_control,\n                        key=\"data_set\",\n                        value=dataset,\n                        replace=True)\nprint(len(dataset))\n\n\n\n\n\n\n\nNote: Data Set and Data Loader\n\n\n\n\nAs shown below, a DataLoader from torch.utils.data can be used to check the data.\n\n\n# Set batch size for DataLoader\nbatch_size = 5\n# Create DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(f\"Inputs Shape: {inputs.shape}\")\n    print(f\"Targets Shape: {targets.shape}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#sec-preprocessing-31",
    "href": "036_spot_lightning_transformer_diabetes.html#sec-preprocessing-31",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.4 Step 4: Preprocessing",
    "text": "28.4 Step 4: Preprocessing\nPreprocessing is handled by Lightning and PyTorch. It is described in the LIGHTNINGDATAMODULE documentation. Here you can find information about the transforms methods.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#sec-selection-of-the-algorithm-31",
    "href": "036_spot_lightning_transformer_diabetes.html#sec-selection-of-the-algorithm-31",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict",
    "text": "28.5 Step 5: Select the Core Model (algorithm) and core_model_hyper_dict\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\nHere we simply add the NN Model to the fun_control dictionary by calling the function add_core_model_to_fun_control:\n\n\nfrom spotPython.light.regression.transformerlightregression import TransformerLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=TransformerLightRegression,\n                              hyper_dict=LightHyperDict)\n\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#sec-modification-of-hyperparameters-31",
    "href": "036_spot_lightning_transformer_diabetes.html#sec-modification-of-hyperparameters-31",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "28.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [7, 9]) and\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\n# set_control_hyperparameter_value(fun_control, \"l1\", [2, 3])\n# set_control_hyperparameter_value(fun_control, \"epochs\", [5, 7])\n# set_control_hyperparameter_value(fun_control, \"batch_size\", [3, 4])\n# set_control_hyperparameter_value(fun_control, \"optimizer\", [\n#                 \"Adadelta\",\n#                 \"Adagrad\",\n#                 \"Adam\",\n#                 \"Adamax\",                \n#             ])\n# set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n# set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n# set_control_hyperparameter_value(fun_control, \"patience\", [3, 5])\n# set_control_hyperparameter_value(fun_control, \"act_fn\",[\n#                 \"ReLU\",\n#                 \"LeakyReLU\",\n#             ] )\nset_control_hyperparameter_value(fun_control, \"initialization\",[\"Default\"] )\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\nThis allows to check if all information is available and if the information is correct.\n\n\n\n\n\n\nNote: Hyperparameters of the Tuned Model and the fun_control Dictionary\n\n\n\nThe updated fun_control dictionary can be shown with the command fun_control[\"core_model_hyper_dict\"].",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "036_spot_lightning_transformer_diabetes.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "28.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n28.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n28.7.2 Loss Function\nThe loss function is specified in the configurable network class [SOURCE] We will use MSE.\n\n\n28.7.3 Metric\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#step-8-calling-the-spot-function",
    "href": "036_spot_lightning_transformer_diabetes.html#step-8-calling-the-spot-function",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.8 Step 8: Calling the SPOT Function",
    "text": "28.8 Step 8: Calling the SPOT Function\n\n28.8.1 Preparing the SPOT Call\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init(init_size=INIT_SIZE)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                            n_theta=2)\n\n\n\n\n\n\n\nNote: Modifying Values in the Control Dictionaries\n\n\n\n\nThe values in the control dictionaries can be modified with the function set_control_key_value [SOURCE], for example:\n\nset_control_key_value(control_dict=surrogate_control,\n                        key=\"noise\",\n                        value=True,\n                        replace=True)                       \nset_control_key_value(control_dict=surrogate_control,\n                        key=\"n_theta\",\n                        value=2,\n                        replace=True)      \n\n\n\n\n\n28.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=10).fun\n\n\n\n28.8.3 Showing the fun_control Dictionary\n\nimport pprint\npprint.pprint(fun_control)\n\n\n\n28.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE].\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#sec-tensorboard-31",
    "href": "036_spot_lightning_transformer_diabetes.html#sec-tensorboard-31",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.9 Step 9: Tensorboard",
    "text": "28.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\ntensorboard --logdir=\"runs/\"\nFurther information can be found in the PyTorch Lightning documentation for Tensorboard.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "036_spot_lightning_transformer_diabetes.html#sec-results-31",
    "href": "036_spot_lightning_transformer_diabetes.html#sec-results-31",
    "title": "28  HPT PyTorch Lightning Transformer: Diabetes",
    "section": "28.10 Step 10: Results",
    "text": "28.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + PREFIX +\"_progress.png\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n\nspot_tuner.plot_importance(threshold=50,\n    filename=\"./figures/\" + PREFIX + \"_importance.png\")\n\n\n28.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\nprint(config)\n\n\nTest on the full data set\n\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\n\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\n\n# filename = \"./figures/\" + PREFIX\nfilename = None\nspot_tuner.plot_important_hyperparameter_contour(filename=filename, threshold=50)\n\n\n\n28.10.2 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n\n28.10.3 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.cvmodel import cv_model\nset_control_key_value(control_dict=fun_control,\n                        key=\"k_folds\",\n                        value=2,\n                        replace=True)\nset_control_key_value(control_dict=fun_control,\n                        key=\"test_size\",\n                        value=0.6,\n                        replace=True)\ncv_model(config, fun_control)\n\n\n\n28.10.4 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n28.10.5 Visualizing the Activation Distribution (Under Development)\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11, _torchmetric=TORCH_METRIC)\nmodel\n\n\n# from spotPython.utils.eda import visualize_activations\n# visualize_activations(model, color=f\"C{0}\")",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>HPT PyTorch Lightning Transformer: Diabetes</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html",
    "href": "037_spot_lightning_save_load_models.html",
    "title": "29  Saving and Loading",
    "section": "",
    "text": "29.1 spotPython: Saving and Loading Optimization Experiments\nIn this section, we will show how results from spotPython can be saved and reloaded. Here, spotPython can be used as an optimizer.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-spotpython-saving-and-loading",
    "href": "037_spot_lightning_save_load_models.html#sec-spotpython-saving-and-loading",
    "title": "29  Saving and Loading",
    "section": "",
    "text": "29.1.1 spotPython as an Optimizer\nIf spotPython is used as an optimizer, no dictionary of hyperparameters has be specified. The fun_control dictionary is sufficient.\n\nimport os\nimport pprint\nfrom spotPython.utils.file import save_experiment, load_experiment\nimport numpy as np\nfrom math import inf\nfrom spotPython.spot import spot\nfrom spotPython.utils.init import (\n    fun_control_init,\n    design_control_init,\n    surrogate_control_init,\n    optimizer_control_init)\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_branin\nfun_control = fun_control_init(\n            PREFIX=\"branin\",\n            SUMMARY_WRITER=False,\n            lower = np.array([0, 0]),\n            upper = np.array([10, 10]),\n            fun_evals=8,\n            fun_repeats=1,\n            max_time=inf,\n            noise=False,\n            tolerance_x=0,\n            ocba_delta=0,\n            var_type=[\"num\", \"num\"],\n            infill_criterion=\"ei\",\n            n_points=1,\n            seed=123,\n            log_level=20,\n            show_models=False,\n            show_progress=True)\ndesign_control = design_control_init(\n            init_size=5,\n            repeats=1)\nsurrogate_control = surrogate_control_init(\n            model_fun_evals=10000,\n            min_theta=-3,\n            max_theta=3,\n            n_theta=2,\n            theta_init_zero=True,\n            n_p=1,\n            optim_p=False,\n            var_type=[\"num\", \"num\"],\n            seed=124)\noptimizer_control = optimizer_control_init(\n            max_iter=1000,\n            seed=125)\nspot_tuner = spot.Spot(fun=fun,\n            fun_control=fun_control,\n            design_control=design_control,\n            surrogate_control=surrogate_control,\n            optimizer_control=optimizer_control)\nspot_tuner.run()\npkl_name = save_experiment(\n    spot_tuner=spot_tuner,\n    fun_control=fun_control,\n    design_control=None,\n    surrogate_control=None,\n    optimizer_control=None\n)\nprint(f\"pkl_name: {pkl_name}\")\n\nspotPython tuning: 4.7932399644479124 [########--] 75.00% \nspotPython tuning: 2.0379795645847087 [#########-] 87.50% \nspotPython tuning: 1.986328241945829 [##########] 100.00% Done...\n\nExperiment saved as spot_branin_experiment.pickle\npkl_name: spot_branin_experiment.pickle\n\n\n\n(spot_tuner_1, fun_control_1, design_control_1,\n    surrogate_control_1, optimizer_control_1) = load_experiment(pkl_name)\n\nThe progress of the original experiment is shown in Figure 29.1 and the reloaded experiment in Figure 29.2.\n\nspot_tuner.plot_progress(log_y=True)\n\n\n\n\n\n\n\nFigure 29.1: Progress of the original experiment\n\n\n\n\n\n\nspot_tuner_1.plot_progress(log_y=True)\n\n\n\n\n\n\n\nFigure 29.2: Progress of the reloaded experiment\n\n\n\n\n\nThe results from the original experiment are shown in Table 29.1 and the reloaded experiment in Table 29.2.\n\nspot_tuner.print_results()\n\nmin y: 1.986328241945829\nx0: 10.0\nx1: 3.2107728198306598\n\n\n\n\nTable 29.1\n\n\n\n[['x0', 10.0], ['x1', 3.2107728198306598]]\n\n\n\n\n\n\nspot_tuner_1.print_results()\n\nmin y: 1.986328241945829\nx0: 10.0\nx1: 3.2107728198306598\n\n\n\n\nTable 29.2\n\n\n\n[['x0', 10.0], ['x1', 3.2107728198306598]]\n\n\n\n\n\n\n29.1.1.1 Getting the Tuned Hyperparameters\nThe tuned hyperparameters can be obtained as a dictionary with the following code.\n\nfrom spotPython.hyperparameters.values import get_tuned_hyperparameters\nget_tuned_hyperparameters(spot_tuner=spot_tuner)\n\n{'x0': 10.0, 'x1': 3.2107728198306598}\n\n\n\n\n\n\n\n\nSummary: Saving and Loading Optimization Experiments\n\n\n\n\nIf spotPython is used as an optimizer (without an hyperparameter dictionary), experiments can be saved and reloaded with the save_experiment and load_experiment functions.\nThe tuned hyperparameters can be obtained with the get_tuned_hyperparameters function.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-spotpython-as-a-hyperparameter-tuner-37",
    "href": "037_spot_lightning_save_load_models.html#sec-spotpython-as-a-hyperparameter-tuner-37",
    "title": "29  Saving and Loading",
    "section": "29.2 spotPython as a Hyperparameter Tuner",
    "text": "29.2 spotPython as a Hyperparameter Tuner\nIf spotPython is used as a hyperparameter tuner, in addition to the fun_control dictionary a core_model dictionary have to be specified. This will be explained in Section 29.2.2.\nFurthermore, a data set has to be selected and added to the fun_control dictionary. Here, we will use the Diabetes data set.\n\n29.2.1 The Diabetes Data Set\nThe hyperparameter tuning of a PyTorch Lightning network on the Diabetes data set is used as an example. The Diabetes data set is a PyTorch Dataset for regression, which originates from the scikit-learn package, see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes.\nTen baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. The Diabetes data set is described in Table 29.3.\n\n\n\nTable 29.3: The Diabetes data set\n\n\n\n\n\nDescription\nValue\n\n\n\n\nSamples total\n442\n\n\nDimensionality\n10\n\n\nFeatures\nreal, -.2 &lt; x &lt; .2\n\n\nTargets\ninteger 25 - 346\n\n\n\n\n\n\n\nfrom spotPython.utils.device import getDevice\nfrom math import inf\nfrom spotPython.utils.init import fun_control_init\nimport numpy as np\nfrom spotPython.hyperparameters.values import set_control_key_value\nfrom spotPython.data.diabetes import Diabetes\n\nMAX_TIME = 1\nFUN_EVALS = 8\nINIT_SIZE = 5\nWORKERS = 0\nPREFIX=\"037\"\nDEVICE = getDevice()\nDEVICES = 1\nTEST_SIZE = 0.4\nTORCH_METRIC = \"mean_squared_error\"\ndataset = Diabetes()\n\nfun_control = fun_control_init(\n    _L_in=10,\n    _L_out=1,\n    _torchmetric=TORCH_METRIC,\n    PREFIX=PREFIX,\n    TENSORBOARD_CLEAN=True,\n    data_set=dataset,\n    device=DEVICE,\n    enable_progress_bar=False,\n    fun_evals=FUN_EVALS,\n    log_level=50,\n    max_time=MAX_TIME,\n    num_workers=WORKERS,\n    show_progress=True,\n    test_size=TEST_SIZE,\n    tolerance_x=np.sqrt(np.spacing(1)),\n    )\n\nMoving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_03_17_22_49_17\nCreated spot_tensorboard_path: runs/spot_logs/037_p040025_2024-03-17_22-49-17 for SummaryWriter()\n\n\n\n\n29.2.2 Adding a core_model to the fun_control Dictionary\nspotPython includes the NetLightRegression class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\nThe hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE].\nThe core_model dictionary contains the hyperparameters of the model to be tuned. These hyperparameters can be specified and modified with as shown in the following code.\n\nfrom spotPython.light.regression.netlightregression import NetLightRegression\nfrom spotPython.hyperdict.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nadd_core_model_to_fun_control(fun_control=fun_control,\n                              core_model=NetLightRegression,\n                              hyper_dict=LightHyperDict)\nfrom spotPython.hyperparameters.values import set_control_hyperparameter_value\n\nset_control_hyperparameter_value(fun_control, \"epochs\", [4, 5])\nset_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\n                \"Adam\",\n                \"RAdam\",\n            ])\nset_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\nset_control_hyperparameter_value(fun_control, \"lr_mult\", [0.05, 1.0])\nset_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\nset_control_hyperparameter_value(fun_control, \"act_fn\",[\n                \"ReLU\",\n                \"LeakyReLU\"\n            ] )\n\nSetting hyperparameter epochs to value [4, 5].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter batch_size to value [4, 5].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter optimizer to value ['Adam', 'RAdam'].\nVariable type is factor.\nCore type is str.\nCalling modify_hyper_parameter_levels().\nSetting hyperparameter dropout_prob to value [0.01, 0.1].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter lr_mult to value [0.05, 1.0].\nVariable type is float.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter patience to value [2, 3].\nVariable type is int.\nCore type is None.\nCalling modify_hyper_parameter_bounds().\nSetting hyperparameter act_fn to value ['ReLU', 'LeakyReLU'].\nVariable type is factor.\nCore type is instance().\nCalling modify_hyper_parameter_levels().\n\n\n\n\n29.2.3 design_control, surrogate_control Dictionaries and the Objective Function\nAfter specifying the design_control and surrogate_control dictionaries, the objective function fun from the class HyperLight [SOURCE] is selected. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\nThen, the hyperparameter tuning can be started.\n\nfrom spotPython.utils.init import design_control_init, surrogate_control_init\ndesign_control = design_control_init(init_size=INIT_SIZE)\n\nsurrogate_control = surrogate_control_init(noise=True,\n                                            n_theta=2)\nfrom spotPython.fun.hyperlight import HyperLight\nfun = HyperLight(log_level=50).fun\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       surrogate_control=surrogate_control)\nspot_tuner.run()\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 8927.0654296875, 'hp_metric': 8927.0654296875}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 23525.451171875, 'hp_metric': 23525.451171875}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 13652.5859375, 'hp_metric': 13652.5859375}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 23946.33984375, 'hp_metric': 23946.33984375}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 20347.798828125, 'hp_metric': 20347.798828125}\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 3977.69482421875, 'hp_metric': 3977.69482421875}\nspotPython tuning: 3977.69482421875 [########--] 75.00% \nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 4742.162109375, 'hp_metric': 4742.162109375}\nspotPython tuning: 3977.69482421875 [#########-] 87.50% \nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.VALIDATING\nLightDataModule.val_dataloader(). Val. set size: 106\ntrain_model result: {'val_loss': 4107.67431640625, 'hp_metric': 4107.67431640625}\nspotPython tuning: 3977.69482421875 [##########] 100.00% Done...\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      8927.0654296875      │\n│         val_loss          │      8927.0654296875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      23525.451171875      │\n│         val_loss          │      23525.451171875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       13652.5859375       │\n│         val_loss          │       13652.5859375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      23946.33984375       │\n│         val_loss          │      23946.33984375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      20347.798828125      │\n│         val_loss          │      20347.798828125      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     3977.69482421875      │\n│         val_loss          │     3977.69482421875      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │      4742.162109375       │\n│         val_loss          │      4742.162109375       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     4107.67431640625      │\n│         val_loss          │     4107.67431640625      │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x35cca8050&gt;\n\n\nThe tuned hyperparameters can be obtained as a dictionary with the following code.\n\nfrom spotPython.hyperparameters.values import get_tuned_hyperparameters\nget_tuned_hyperparameters(spot_tuner)\n\n{'l1': 7.0,\n 'epochs': 5.0,\n 'batch_size': 5.0,\n 'act_fn': 0.0,\n 'optimizer': 0.0,\n 'dropout_prob': 0.04938283618350054,\n 'lr_mult': 0.4445552938072223,\n 'patience': 3.0,\n 'initialization': 0.0}\n\n\nHere , the numerical levels of the hyperparameters are used as keys in the dictionary. If the fun_control dictionary is used, the names of the hyperparameters are used as keys in the dictionary.\n\nget_tuned_hyperparameters(spot_tuner, fun_control)\n\n{'l1': 7.0,\n 'epochs': 5.0,\n 'batch_size': 5.0,\n 'act_fn': 'ReLU',\n 'optimizer': 'Adam',\n 'dropout_prob': 0.04938283618350054,\n 'lr_mult': 0.4445552938072223,\n 'patience': 3.0,\n 'initialization': 'Default'}\n\n\n\npkl_name = save_experiment(\n    spot_tuner=spot_tuner,\n    fun_control=fun_control,\n    design_control=None,\n    surrogate_control=None,\n    optimizer_control=None\n)\nprint(f\"pkl_name: {pkl_name}\")\n\nExperiment saved as spot_037_experiment.pickle\npkl_name: spot_037_experiment.pickle\n\n\nThe results from the experiment are stored in the pickle file spot_037_experiment.pickle. The experiment can be reloaded with the following code.\n\n(spot_tuner_1, fun_control_1, design_control_1,\n    surrogate_control_1, optimizer_control_1) = load_experiment(pkl_name)\n\nPlot the progress of the original experiment are identical to the reloaded experiment.\n\nspot_tuner.plot_progress(log_y=True)\nspot_tuner_1.plot_progress(log_y=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, the tuned hyperparameters can be obtained as a dictionary from the reloaded experiment with the following code.\n\nget_tuned_hyperparameters(spot_tuner_1, fun_control_1)\n\n{'l1': 7.0,\n 'epochs': 5.0,\n 'batch_size': 5.0,\n 'act_fn': 'ReLU',\n 'optimizer': 'Adam',\n 'dropout_prob': 0.04938283618350054,\n 'lr_mult': 0.4445552938072223,\n 'patience': 3.0,\n 'initialization': 'Default'}\n\n\n\n\n\n\n\n\nSummary: Saving and Loading Hyperparameter-Tuning Experiments\n\n\n\n\nIf spotPython is used as an hyperparameter tuner (with an hyperparameter dictionary), experiments can be saved and reloaded with the save_experiment and load_experiment functions.\nThe tuned hyperparameters can be obtained with the get_tuned_hyperparameters function.",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-saving-and-loading-pytorch-lightning-models-37",
    "href": "037_spot_lightning_save_load_models.html#sec-saving-and-loading-pytorch-lightning-models-37",
    "title": "29  Saving and Loading",
    "section": "29.3 Saving and Loading PyTorch Lightning Models",
    "text": "29.3 Saving and Loading PyTorch Lightning Models\nSection 29.1 and Section 29.2 explained how to save and load optimization and hyperparameter tuning experiments and how to get the tuned hyperparameters as a dictionary. This section shows how to save and load PyTorch Lightning models.\n\n29.3.1 Get the Tuned Architecture\nIn contrast to the function get_tuned_hyperparameters, the function get_tuned_architecture returns the tuned architecture of the model as a dictionary. Here, the transformations are already applied to the numerical levels of the hyperparameters and the encoding (and types) are the original types of the hyperparameters used by the model. The config dictionary can be passed to the model without any modifications.\n\nfrom spotPython.hyperparameters.values import get_tuned_architecture\nconfig = get_tuned_architecture(spot_tuner, fun_control)\npprint.pprint(config)\n\n{'act_fn': ReLU(),\n 'batch_size': 32,\n 'dropout_prob': 0.04938283618350054,\n 'epochs': 32,\n 'initialization': 'Default',\n 'l1': 128,\n 'lr_mult': 0.4445552938072223,\n 'optimizer': 'Adam',\n 'patience': 8}\n\n\nAfter getting the tuned architecture, the model can be created and tested with the following code.\n\nfrom spotPython.light.testmodel import test_model\ntest_model(config, fun_control)\n\nLightDataModule.setup(): stage: TrainerFn.FITTING\ntrain_size: 0.36, val_size: 0.24 used for train & val data.\nLightDataModule.val_dataloader(). Val. set size: 106\nLightDataModule.train_dataloader(). data_train size: 160\nLightDataModule.setup(): stage: TrainerFn.TESTING\ntest_size: 0.4 used for test dataset.\nLightDataModule.test_dataloader(). Test set size: 177\ntest_model result: {'val_loss': 7709.06640625, 'hp_metric': 7709.06640625}\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │       7709.06640625       │\n│         val_loss          │       7709.06640625       │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n(7709.06640625, 7709.06640625)\n\n\n\n\n29.3.2 Load a Model from Checkpoint\n\nfrom spotPython.light.loadmodel import load_light_from_checkpoint\nmodel_loaded = load_light_from_checkpoint(config, fun_control)\n\nconfig: {'l1': 128, 'epochs': 32, 'batch_size': 32, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.04938283618350054, 'lr_mult': 0.4445552938072223, 'patience': 8, 'initialization': 'Default'}\nLoading model with 128_32_32_ReLU_Adam_0.0494_0.4446_8_Default_TEST from runs/saved_models/128_32_32_ReLU_Adam_0.0494_0.4446_8_Default_TEST/last.ckpt\nModel: NetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.04938283618350054, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.04938283618350054, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.04938283618350054, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.04938283618350054, inplace=False)\n    (12): Linear(in_features=32, out_features=1, bias=True)\n  )\n)\n\n\n\nvars(model_loaded)\n\n{'training': False,\n '_parameters': OrderedDict(),\n '_buffers': OrderedDict(),\n '_non_persistent_buffers_set': set(),\n '_backward_pre_hooks': OrderedDict(),\n '_backward_hooks': OrderedDict(),\n '_is_full_backward_hook': None,\n '_forward_hooks': OrderedDict(),\n '_forward_hooks_with_kwargs': OrderedDict(),\n '_forward_hooks_always_called': OrderedDict(),\n '_forward_pre_hooks': OrderedDict(),\n '_forward_pre_hooks_with_kwargs': OrderedDict(),\n '_state_dict_hooks': OrderedDict(),\n '_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_post_hooks': OrderedDict(),\n '_modules': OrderedDict([('layers',\n               Sequential(\n                 (0): Linear(in_features=10, out_features=128, bias=True)\n                 (1): ReLU()\n                 (2): Dropout(p=0.04938283618350054, inplace=False)\n                 (3): Linear(in_features=128, out_features=64, bias=True)\n                 (4): ReLU()\n                 (5): Dropout(p=0.04938283618350054, inplace=False)\n                 (6): Linear(in_features=64, out_features=64, bias=True)\n                 (7): ReLU()\n                 (8): Dropout(p=0.04938283618350054, inplace=False)\n                 (9): Linear(in_features=64, out_features=32, bias=True)\n                 (10): ReLU()\n                 (11): Dropout(p=0.04938283618350054, inplace=False)\n                 (12): Linear(in_features=32, out_features=1, bias=True)\n               ))]),\n 'prepare_data_per_node': True,\n 'allow_zero_length_dataloader_with_multiple_devices': False,\n '_log_hyperparams': True,\n '_dtype': torch.float32,\n '_device': device(type='mps', index=0),\n '_trainer': None,\n '_example_input_array': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n '_current_fx_name': None,\n '_automatic_optimization': True,\n '_param_requires_grad_state': {},\n '_metric_attributes': None,\n '_compiler_ctx': None,\n '_fabric': None,\n '_fabric_optimizers': [],\n '_L_in': 10,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'metric': &lt;function torchmetrics.functional.regression.mse.mean_squared_error(preds: torch.Tensor, target: torch.Tensor, squared: bool = True, num_outputs: int = 1) -&gt; torch.Tensor&gt;,\n '_hparams_name': 'kwargs',\n '_hparams': \"act_fn\":         ReLU()\n \"batch_size\":     32\n \"dropout_prob\":   0.04938283618350054\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.4445552938072223\n \"optimizer\":      Adam\n \"patience\":       8,\n '_hparams_initial': \"act_fn\":         ReLU()\n \"batch_size\":     32\n \"dropout_prob\":   0.04938283618350054\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.4445552938072223\n \"optimizer\":      Adam\n \"patience\":       8}\n\n\n\nimport torch\ntorch.save(model_loaded, \"model.pt\")\n\n\nmymodel = torch.load(\"model.pt\")\n\n\n# show all attributes of the model\nvars(mymodel)\n\n{'training': False,\n '_parameters': OrderedDict(),\n '_buffers': OrderedDict(),\n '_non_persistent_buffers_set': set(),\n '_backward_pre_hooks': OrderedDict(),\n '_backward_hooks': OrderedDict(),\n '_is_full_backward_hook': None,\n '_forward_hooks': OrderedDict(),\n '_forward_hooks_with_kwargs': OrderedDict(),\n '_forward_hooks_always_called': OrderedDict(),\n '_forward_pre_hooks': OrderedDict(),\n '_forward_pre_hooks_with_kwargs': OrderedDict(),\n '_state_dict_hooks': OrderedDict(),\n '_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_post_hooks': OrderedDict(),\n '_modules': OrderedDict([('layers',\n               Sequential(\n                 (0): Linear(in_features=10, out_features=128, bias=True)\n                 (1): ReLU()\n                 (2): Dropout(p=0.04938283618350054, inplace=False)\n                 (3): Linear(in_features=128, out_features=64, bias=True)\n                 (4): ReLU()\n                 (5): Dropout(p=0.04938283618350054, inplace=False)\n                 (6): Linear(in_features=64, out_features=64, bias=True)\n                 (7): ReLU()\n                 (8): Dropout(p=0.04938283618350054, inplace=False)\n                 (9): Linear(in_features=64, out_features=32, bias=True)\n                 (10): ReLU()\n                 (11): Dropout(p=0.04938283618350054, inplace=False)\n                 (12): Linear(in_features=32, out_features=1, bias=True)\n               ))]),\n 'prepare_data_per_node': True,\n 'allow_zero_length_dataloader_with_multiple_devices': False,\n '_log_hyperparams': True,\n '_dtype': torch.float32,\n '_device': device(type='mps', index=0),\n '_trainer': None,\n '_example_input_array': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n '_current_fx_name': None,\n '_automatic_optimization': True,\n '_param_requires_grad_state': {},\n '_metric_attributes': None,\n '_compiler_ctx': None,\n '_fabric': None,\n '_fabric_optimizers': [],\n '_L_in': 10,\n '_L_out': 1,\n '_torchmetric': 'mean_squared_error',\n 'metric': &lt;function torchmetrics.functional.regression.mse.mean_squared_error(preds: torch.Tensor, target: torch.Tensor, squared: bool = True, num_outputs: int = 1) -&gt; torch.Tensor&gt;,\n '_hparams_name': 'kwargs',\n '_hparams': \"act_fn\":         ReLU()\n \"batch_size\":     32\n \"dropout_prob\":   0.04938283618350054\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.4445552938072223\n \"optimizer\":      Adam\n \"patience\":       8,\n '_hparams_initial': \"act_fn\":         ReLU()\n \"batch_size\":     32\n \"dropout_prob\":   0.04938283618350054\n \"epochs\":         32\n \"initialization\": Default\n \"l1\":             128\n \"lr_mult\":        0.4445552938072223\n \"optimizer\":      Adam\n \"patience\":       8}",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "037_spot_lightning_save_load_models.html#sec-converting-a-lightning-model-to-a-plain-torch-model-37",
    "href": "037_spot_lightning_save_load_models.html#sec-converting-a-lightning-model-to-a-plain-torch-model-37",
    "title": "29  Saving and Loading",
    "section": "29.4 Converting a Lightning Model to a Plain Torch Model",
    "text": "29.4 Converting a Lightning Model to a Plain Torch Model\n\n29.4.1 The Function get_removed_attributes_and_base_net\nspotPython provides a function to covert a PyTorch Lightning model to a plain PyTorch model. The function get_removed_attributes_and_base_net returns a tuple with the removed attributes and the base net. The base net is a plain PyTorch model. The removed attributes are the attributes of the PyTorch Lightning model that are not part of the base net.\nThis conversion can be reverted.\n\nimport numpy as np\nimport torch\nfrom spotPython.utils.device import getDevice\nfrom torch.utils.data import random_split\nfrom spotPython.utils.classes import get_removed_attributes_and_base_net\nfrom spotPython.hyperparameters.optimizer import optimizer_handler\nremoved_attributes, torch_net = get_removed_attributes_and_base_net(net=mymodel)\n\n\nprint(removed_attributes)\n\n{'_param_requires_grad_state': {}, '_compiler_ctx': None, '_torchmetric': 'mean_squared_error', 'metric': &lt;function mean_squared_error at 0x302972200&gt;, '_device': device(type='mps', index=0), '_log_hyperparams': True, '_trainer': None, 'prepare_data_per_node': True, '_example_input_array': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), '_fabric_optimizers': [], '_hparams_name': 'kwargs', '_dtype': torch.float32, '_L_in': 10, '_metric_attributes': None, '_L_out': 1, '_hparams_initial': \"act_fn\":         ReLU()\n\"batch_size\":     32\n\"dropout_prob\":   0.04938283618350054\n\"epochs\":         32\n\"initialization\": Default\n\"l1\":             128\n\"lr_mult\":        0.4445552938072223\n\"optimizer\":      Adam\n\"patience\":       8, '_automatic_optimization': True, '_hparams': \"act_fn\":         ReLU()\n\"batch_size\":     32\n\"dropout_prob\":   0.04938283618350054\n\"epochs\":         32\n\"initialization\": Default\n\"l1\":             128\n\"lr_mult\":        0.4445552938072223\n\"optimizer\":      Adam\n\"patience\":       8, '_current_fx_name': None, '_fabric': None, 'allow_zero_length_dataloader_with_multiple_devices': False}\n\n\n\nprint(torch_net)\n\nNetLightRegression(\n  (layers): Sequential(\n    (0): Linear(in_features=10, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.04938283618350054, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.04938283618350054, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.04938283618350054, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.04938283618350054, inplace=False)\n    (12): Linear(in_features=32, out_features=1, bias=True)\n  )\n)\n\n\n\n\n29.4.2 An Example how to use the Plain Torch Net\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Load the Diabetes dataset from sklearn\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert the data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n\n# Create a PyTorch dataset\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create a PyTorch dataloader\nbatch_size = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\ntorch_net.to(getDevice(\"cpu\"))\n\n# train the net\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(torch_net.parameters(), lr=0.01)\nn_epochs = 100\nlosses = []\nfor epoch in range(n_epochs):\n    for inputs, targets in train_dataloader:\n        targets = targets.view(-1, 1)\n        optimizer.zero_grad()\n        outputs = torch_net(inputs)\n        loss = criterion(outputs, targets)\n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n# visualize the network training\nplt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()",
    "crumbs": [
      "Hyperparameter Tuning with PyTorch Lightning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Saving and Loading</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html",
    "href": "a_01_intro_to_notebooks.html",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "",
    "text": "A.1 Different Notebook cells\nThere are different cells that the notebook is currently supporting:\nAs a default, every cells in jupyter is set to “code”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#different-notebook-cells",
    "href": "a_01_intro_to_notebooks.html#different-notebook-cells",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "",
    "text": "code cells\nmarkdown cells\nraw cells\n\n\n\nA.1.1 Code cells\nThe code cells are used to execute the code. They are following the logic of the choosen kernel. Therefore, it is important to keep in mind which programming language is currently used. Otherwise one might yield an error because of the wrong syntax.\nThe code cells are executed my be ▶ Run button (can be found in the header of the notebook).\n\n\nA.1.2 Markdown cells\nThe markdown cells are a usefull tool to comment the written code. Especially with the help of headers can the code be brought in a more readable format. If you are not familiar with the markdown syntax, you can find a usefull cheat sheet here: Markdown Cheat Sheeet\n\n\nA.1.3 Raw cells\nThe “Raw NBConvert” cell type can be used to render different code formats into HTML or LaTeX by Sphinx. This information is stored in the notebook metadata and converted appropriately.\n\nA.1.3.1 Usage\nTo select a desired format from within Jupyter, select the cell containing your special code and choose options from the following dropdown menus:\n\nSelect “Raw NBConvert”\nSwitch the Cell Toolbar to “Raw Cell Format” (The cell toolbar can be found under View)\nChose the appropriate “Raw NBConvert Format” within the cell\n\nData Science is fun",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#install-packages",
    "href": "a_01_intro_to_notebooks.html#install-packages",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.2 Install Packages",
    "text": "A.2 Install Packages\nBecause python is a heavily used programming language, there are many different packags that can make your life easier. Sadly, there are only a few standard packages that are already included in your python enviroment. If you have the need to install a new package in your enviroment, you can simply do that by exectuing the following code snippet in a code cell\n!pip install numpy\n\nThe ! is used to run the cell as a shell command\npip is package manager for python packages.\nnumpy is the the package you want to install\n\nHint: It is often usefull to restart the kernel after installing a package, otherwise loading the package could lead to an error.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#load-packages",
    "href": "a_01_intro_to_notebooks.html#load-packages",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.3 Load Packages",
    "text": "A.3 Load Packages\nAfter successfully installing the package it is necessary to import them before you can work with them. The import of the packages is done in the following way:\nimport numpy as np\nThe imported packages are often abbreviated. This is because you need to specify where the function is coming from.\nThe most common abbreviations for data science packages are:\n\nAbbreviations for data science packages\n\n\nAbbreviation\nPackage\nImport\n\n\n\n\nnp\nnumpy\nimport numpy as np\n\n\npd\npandas\nimport pandas as pd\n\n\nplt\nmatplotlib\nimport matplotlib.pyplot as plt\n\n\npx\nplotly\nimport plotly.exprss as px\n\n\ntf\ntensorflow\nimport tensorflow as tf\n\n\nsns\nseaborn\nimport seaborn as sns\n\n\ndt\ndatetime\nimport datetime as dt\n\n\npkl\npickle\nimport pickle as pkl",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#functions-in-python",
    "href": "a_01_intro_to_notebooks.html#functions-in-python",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.4 Functions in Python",
    "text": "A.4 Functions in Python\nBecause python is not using Semicolon’s it is import to keep track of indentation in your code. The indentation works as a placeholder for the semicolons. This is especially important if your are defining loops, functions, etc. …\nExample: We are defining a function that calculates the squared sum of its input parameters\n\ndef squared_sum(x,y): \n    z = x**2 + y**2\n    return z\n\nIf you are working with something that needs indentation, it will be already done by the notebook.\nHint: Keep in mind that is good practice to use the return parameter. If you are not using return and a function has multiple paramaters that you would like to return, it will only return the last one defined.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_01_intro_to_notebooks.html#list-of-useful-jupyter-notebook-shortcuts",
    "href": "a_01_intro_to_notebooks.html#list-of-useful-jupyter-notebook-shortcuts",
    "title": "Appendix A — Introduction to Jupyter Notebook",
    "section": "A.5 List of Useful Jupyter Notebook Shortcuts",
    "text": "A.5 List of Useful Jupyter Notebook Shortcuts\n\nList of useful Jupyter Notebook Shortcuts\n\n\n\n\n\n\n\nFunction\nKeyboard Shortcut\nMenu Tools\n\n\n\n\nSave notebook\nEsc + s\nFile → Save and Checkpoint\n\n\nCreate new Cell\nEsc + a (above),  Esc + b (below)\nInsert → Cell above; Insert → Cell below\n\n\nRun Cell\nCtrl + enter\nCell → Run Cell\n\n\nCopy Cell\nc\nCopy Key\n\n\nPaste Cell\nv\nPaste Key\n\n\nInterrupt Kernel\nEsc + i i\nKernel → Interrupt\n\n\nRestart Kernel\nEsc + 0 0\nKernel → Restart\n\n\n\nIf you combine everything you can create beautiful graphics\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate 100 random data points along 3 dimensions\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\n\n# Map each onto a scatterplot we'll create with Matplotlib\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale)*500)\nax.set(title=\"Some random data, created with the Jupyter Notebook!\")\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Introduction to Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html",
    "href": "a_02_git_intro_en.html",
    "title": "Appendix B — Git Introduction",
    "section": "",
    "text": "B.1 Learning Objectives\nIn this learning unit, you will learn how to set up Git as a version control system for a project. The most important Git commands will be explained. You will learn how to track and manage changes to your projects with Git. Specifically:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#learning-objectives",
    "href": "a_02_git_intro_en.html#learning-objectives",
    "title": "Appendix B — Git Introduction",
    "section": "",
    "text": "Initializing a repository: git init\nIgnoring files: .gitignore\nAdding files to the staging area: git add\nChecking status changes: git status\nReviewing history: git log\nCreating a new branch: git branch\nSwitching to the current branch: git switch and git checkout\nMerging two branches: git merge\nResolving conflicts\nReverting changes: git revert\nUploading changes to GitLab: git push\nDownloading changes from GitLab: git pull\nAdvanced: git rebase",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#basics-of-git",
    "href": "a_02_git_intro_en.html#basics-of-git",
    "title": "Appendix B — Git Introduction",
    "section": "B.2 Basics of Git",
    "text": "B.2 Basics of Git\n\nB.2.1 Initializing a Repository: git init\nTo set up Git as a version control system for your project, you need to initialize a new Git repository at the top-level folder, which is the working directory of your project. This is done using the git init command.\nAll files in this folder and its subfolders will automatically become part of the repository. Creating a Git repository is similar to adding an all-powerful passive observer of all things to your project. Git sits there, observes, and takes note of even the smallest changes, such as a single character in a file within a repository with hundreds of files. And it will tell you where these changes occurred if you forget. Once Git is initialized, it monitors all changes made within the working directory, and it tracks the history of events from that point forward. For this purpose, a historical timeline is created for your project, referred to as a “branch,” and the initial branch is named main. So, when someone says they are on the main branch or working on the main branch, it means they are in the historical main timeline of the project. The Git repository, often abbreviated as repo, is a virtual representation of your project, including its history and branches, a book, if you will, where you can look up and retrieve the entire history of the project: you work in your working directory, and the Git repository tracks and stores your work.\n\n\nB.2.2 Ignoring Files: .gitignore\nIt’s useful that Git watches and keeps an eye on everything in your project. However, in most projects, there are files and folders that you don’t need or want to keep an eye on. These may include system files, local project settings, libraries with dependencies, and so on.\nYou can exclude any file or folder from your Git repository by including them in the .gitignore file. In the .gitignore file, you create a list of file names, folder names, and other items that Git should not track, and Git will ignore these items. Hence the name “gitignore.” Do you want to track a file that you previously ignored? Simply remove the mention of the file in the gitignore file, and Git will start tracking it again.\n\n\nB.2.3 Adding Changes to the Staging Area: git add\nThe interesting thing about Git as an all-powerful, passive observer of all things is that it’s very passive. As long as you don’t tell Git what to remember, it will passively observe the changes in the project folder but do nothing.\nWhen you make a change to your project that you want Git to include in the project’s history to take a snapshot of so you can refer back to it later, your personal checkpoint, if you will, you need to first stage the changes in the staging area. What is the staging area? The staging area is where you collect changes to files that you want to include in the project’s history.\nThis is done using the git add command. You can specify which files you want to add by naming them, or you can add all of them using -A. By doing this, you’re telling Git that you’ve made changes and want it to remember these particular changes so you can recall them later if needed. This is important because you can choose which changes you want to stage, and those are the changes that will eventually be transferred to the history.\n\nNote: When you run git add, the changes are not transferred to the project’s history. They are only transferred to the staging area.\n\n\n\n\n\n\n\nExample of git add from the beginning\n\n\n\n# Create a new directory for your\n# repository and navigate to that directory:\n\nmkdir my-repo\ncd my-repo\n\n# Initialize the repository with git init:\n\ngit init\n\n# Create a .gitignore file for Python code.\n# You can use a template from GitHub:\n\ncurl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore -o .gitignore\n\n# Add your files to the repository using git add:\n\ngit add .\nThis adds all files in the current directory to the repository, except for the files listed in the .gitignore file.\n\n\n\n\nB.2.4 Transferring Changes to Memory: git commit\nThe power of Git becomes evident when you start transferring changes to the project history. This is done using the git commit command. When you run git commit, you inform Git that the changes in the staging area should be added to the history of the project so that they can be referenced or retrieved later.\nAdditionally, you can add a commit message with the -m option to explain what changes were made. So when you look back at the project history, you can see that you added a new feature.\ngit commit creates a snapshot, an image of the current state of your project at that specific time, and adds it to the branch you are currently working on.\nAs you work on your project and transfer more snapshots, the branch grows and forms a timeline of events. This means you can now look back at every transfer in the branch and see what your code looked like at that time.\nYou can compare any phase of your code with any other phase of your code to find errors, restore deleted code, or do things that would otherwise not be possible, such as resetting the project to a previous state or creating a new timeline from any point.\nSo how often should you add these commits? My rule of thumb is not to commit too often. It’s better to have a Git repository with too many commits than one with too few commits.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nAfter adding your files with git add, you can create a commit to save your changes. Use the git commit command with the -m option to specify your commit message:\ngit commit -m \"My first commit message\"\nThis creates a new commit with the added files and the specified commit message.\n\n\n\n\nB.2.5 Check the Status of Your Repository: git status\nIf you’re wondering what you’ve changed in your project since the last commit snapshot, you can always check the Git status. Git will list every modified file and the current status of each file.\nThis status can be either:\n\nUnchanged (unmodified), meaning nothing has changed since you last transferred it, or\nIt’s been changed (changed) but not staged (staged) to be transferred into the history, or\nSomething has been added to staging (staged) and is ready to be transferred into the history.\n\nWhen you run git status, you get an overview of the current state of your project.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nThe git status command displays the status of your working directory and the staging area. It shows you which files have been modified, which files are staged for commit, and which files are not yet being tracked:\ngit status\ngit status is a useful tool to keep track of your changes and ensure that you have added all the desired files for commit.\n\n\n\n\nB.2.6 Review Your Repository’s History: git log\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nYou can view the history of your commits with the git log command. This command displays a list of all the commits in the current branch, along with information such as the author, date, and commit message:\ngit log\nThere are many options to customize the output of git log. For example, you can use the --pretty option to change the format of the output:\ngit log --pretty=oneline\nThis displays each commit in a single line.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#branches-timelines",
    "href": "a_02_git_intro_en.html#branches-timelines",
    "title": "Appendix B — Git Introduction",
    "section": "B.3 Branches (Timelines)",
    "text": "B.3 Branches (Timelines)\n\nB.3.1 Creating an Alternative Timeline: git branch\nIn the course of developing a project, you often reach a point where you want to add a new feature, but doing so might require changing the existing code in a way that could be challenging to undo later.\nOr maybe you just want to experiment and be able to discard your work if the experiment fails. In such cases, Git allows you to create an alternative timeline called a branch to work in.\nThis new branch has its own name and exists in parallel with the main branch and all other branches in your project.\nDuring development, you can switch between branches and work on different versions of your code concurrently. This way, you can have a stable codebase in the main branch while developing an experimental feature in a separate branch. When you switch from one branch to another, the code you’re working on is automatically reset to the latest commit of the branch you’re currently in.\nIf you’re working in a team, different team members can work on their own branches, creating an entire universe of alternative timelines for your project. When features are completed, they can be seamlessly merged back into the main branch.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nTo create a new branch, you can use the git branch command with the name of the new branch as an argument:\ngit branch my-tests\n\n\n\n\nB.3.2 The Pointer to the Current Branch: HEAD\nHow does Git know where you are on the timeline, and how can you keep track of your position?\nYou’re always working at the tip (HEAD) of the currently active branch. The HEAD pointer points there quite literally. In a new project archive with just a single main branch and only new commits being added, HEAD always points to the latest commit in the main branch. That’s where you are.\nHowever, if you’re in a repository with multiple branches, meaning multiple alternative timelines, HEAD will point to the latest commit in the branch you’re currently working on.\n\n\nB.3.3 Switching to an Alternative Timeline: git switch\nAs your project grows, and you have multiple branches, you need to be able to switch between these branches. This is where the switch command comes into play.\nAt any time, you can use the git switch command with the name of the branch you want to switch to, and HEAD moves from your current branch to the one you specified.\nIf you’ve made changes to your code before switching, Git will attempt to carry those changes over to the branch you’re switching to. However, if these changes conflict with the target branch, the switch will be canceled.\nTo resolve this issue without losing your changes, return to the original branch, add and commit your recent changes, and then perform the switch.\n\n\nB.3.4 Switching to an Alternative Timeline and Making Changes: git checkout\nTo switch between branches, you can also use the git checkout command. It works similarly to git switch for this purpose: you pass the name of the branch you want to switch to, and HEAD moves to the beginning of that branch.\nBut checkout can do more than just switch to another timeline. With git checkout, you can also move to any commit point in any timeline. In other words, you can travel back in time and work on code from the past.\nTo do this, use git checkout and provide the commit ID. This is an automatically generated, random combination of letters and numbers that identifies each commit. You can retrieve the commit ID using git log. When you run git log, you get a list of all the commits in your repository, starting with the most recent ones.\nWhen you use git checkout with an older commit ID, you check out a commit in the middle of a branch. This disrupts the timeline, as you’re actively attempting to change history. Git doesn’t want you to do that because, much like in a science fiction movie, altering the past might also alter the future. In our case, it would break the version control branch’s coherence.\nTo prevent you from accidentally disrupting time and altering history, checking out an earlier commit in any branch results in the warning “Detached Head,” which sounds rather ominous. The “Detached Head” warning is appropriate because it accurately describes what’s happening. Git literally detaches the head from the branch and sets it aside.\nNow, you’re working outside of time in a space unbound to any timeline, which again sounds rather threatening but is perfectly fine in reality.\nTo continue working on this past code, all you need to do is reattach it to the timeline. You can use git branch to create a new branch, and the detached head will automatically attach to this new branch.\nInstead of breaking the history, you’ve now created a new alternative timeline that starts in the past, allowing you to work safely. You can continue working on the branch as usual.\n\n\n\n\n\n\nContinuing the example from above:\n\n\n\nTo switch to a new branch, you can use the git checkout command:\ngit checkout meine-tests\nNow you’re using the new branch and can make changes independently from the original branch.\n\n\n\n\nB.3.5 The Difference Between checkout and switch\nWhat is the difference between git switch and git checkout? git switch and git checkout are two different commands that both serve the purpose of switching between branches. You can use both to switch between branches, but they have an important distinction. git switch is a new command introduced with Git 2.23. git checkout is an older command that has existed since Git 1.6.0. So, git switch and git checkout have different origins. git switch was introduced to separate the purposes of git checkout. git checkout has two different purposes: 1. It can be used to switch between branches, and 2. It can be used to reset files to the state of the last commit.\nHere’s an example: In my project, I made a change since the last commit, but I haven’t staged it yet. Then, I realized that I actually don’t want this change. I want to reset the file to the state before the last commit. As long as I haven’t committed my changes, I can do this with git checkout by targeting the specific file. So, if that file is named main.js, I can say: git checkout main.js. And the file will be reset to the state of the last commit, which makes sense. I’m checking out the file from the last commit.\nBut that’s quite different from switching between the beginning of one branch to another. git switch and git restore were introduced to separate these two operations. git switch is for switching between branches, and git restore is for resetting the specified file to the state of the last commit. If you try to restore a file with git switch, it simply won’t work. It’s not intended for that. As I mentioned earlier, it’s about separating concerns.\n:::{.callout-note} #### Difference Between checkout and switch git checkout and git switch are both commands for switching between branches in a Git repository. The main difference between the two commands is that git switch is a newer command specifically designed for branch switching, while git checkout is an older command that can be used for various tasks, including branch switching.\nHere’s an example demonstrating how to initialize a repository and switch between branches:\n# Create a new directory for your repository\n# and navigate to that directory:\nmkdir my-repo\ncd my-repo\n\n# Initialize the repository with git init:\ngit init\n\n# Create a new branch with git branch:\ngit branch my-new-branch\n\n# Switch to the new branch using git switch:\ngit switch my-new-branch\n\n# Alternatively, you can also use git checkout\n# to switch to the new branch:\n\ngit checkout my-new-branch\nBoth commands lead to the same result: You are now on the new branch.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#merging-branches-and-resolving-conflicts",
    "href": "a_02_git_intro_en.html#merging-branches-and-resolving-conflicts",
    "title": "Appendix B — Git Introduction",
    "section": "B.4 Merging Branches and Resolving Conflicts",
    "text": "B.4 Merging Branches and Resolving Conflicts\n\nB.4.1 git merge: Merging Two Timelines\nGit allows you to split your development work into as many branches or alternative timelines as you like, enabling you to work on many different versions of your code simultaneously without losing or overwriting any of your work.\nThis is all well and good, but at some point, you need to bring those various versions of your code back together into one branch. That’s where git merge comes in.\nConsider an example where you have two branches, a main branch and an experimental branch called experimental-branch. In the experimental branch, there is a new feature. To merge these two branches, you set HEAD to the branch where you want to incorporate the code and execute git merge followed by the name of the branch you want to merge. HEAD is a special pointer that points to the current branch. When you run git merge, it combines the code from the branch associated with HEAD with the code from the branch specified by the branch name you provide.\n# Initialize the repository\ngit init\n\n# Create a new branch called \"experimental-branch\"\ngit branch experimental-branch\n\n# Switch to the \"experimental-branch\"\ngit checkout experimental-branch\n\n# Add the new feature here and\n# make a commit\n# ...\n\n# Switch back to the \"main\" branch\ngit checkout main\n\n# Perform the merge\ngit merge experimental-branch\nDuring the merge, matching pieces of code in the branches overlap, and any new code from the branch being merged is added to the project. So now, the main branch also contains the code from the experimental branch, and the events of the two separate timelines have been merged into a single one. What’s interesting is that even though the experimental branch was merged with the main branch, the last commit of the experimental branch remains intact, allowing you to continue working on the experimental branch separately if you wish.\n\n\nB.4.2 Resolving Conflicts When Merging\nMerging branches where there are no code changes at the same place in both branches is a straightforward process. It’s also a rare process. In most cases, there will be some form of conflict between the branches – the same code or the same code area has been modified differently in the different branches. Merging two branches with such conflicts will not work, at least not automatically.\nIn this case, Git doesn’t know how to merge this code. So, when such a situation occurs, it’s marked as a conflict, and the merging process is halted. This might sound more dramatic than it is. When you get a conflict warning, Git is saying there are two different versions here, and Git needs to know which one you want to keep. To help you figure out the conflict, Git combines all the code into a single file and automatically marks the conflicting code as the current change, which is the original code from the branch you’re working on, or as the incoming change, which is the code from the file you’re trying to merge.\nTo resolve this conflict, you’ll edit the file to literally resolve the code conflict. This might mean accepting either the current or incoming change and discarding the other. It could mean combining both changes or something else entirely. It’s up to you. So, you edit the code to resolve the conflict. Once you’ve resolved the conflict by editing the code, you add the new conflict-free version to the staging area with git add and then commit the merged code with git commit. That’s how the conflict is resolved.\nA merge conflict occurs when Git struggles to automatically merge changes from two different branches. This usually happens when changes were made to the same line in the same file in both branches. To resolve a merge conflict, you must manually edit the affected files and choose the desired changes. Git marks the conflict areas in the file with special markings like &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. You can search for these markings and manually select the desired changes. After resolving the conflicts, you can add the changes with git add and create a new commit with git commit to complete the merge.\nHere’s an example:\n# Perform the merge (this will cause a conflict)\ngit merge experimenteller-branch\n\n# Open the affected file in an editor and manually resolve the conflicts\n# ...\n\n# Add the modified file\ngit add &lt;filename&gt;\n\n# Create a new commit\ngit commit -m \"Resolved conflicts\"\n\n\nB.4.3 git revert: Undoing Something\nOne of the most powerful features of any software tool is the “Undo” button. Make a mistake, press “Undo,” and it’s as if it never happened. However, that’s not quite as simple when an all-powerful, passive observer is watching and recording your project’s history. How do you undo something that you’ve added to the history without rewriting the history?\nThe answer is that you can overwrite the history with the git reset command, but that’s quite risky and not a good practice.\nA better solution is to work with the historical timeline and simply place an older version of your code at the top of the branch. This is done with git revert. To make this work, you need to know the commit ID of the commit you want to go back to.\nThe commit ID is a machine-generated set of random numbers and letters, also known as a hash. To get a list of all the commits in the repository, including the commit ID and commit message, you can run git log.\n# Show the list of all operations in the repository\ngit log\nBy the way, it’s a good idea to leave clear and informative commit messages for this reason. This way, you know what happened in your previous commits. Once you’ve found the commit you want to revert to, call that commit ID with git revert, and then the ID. This will create a new commit at the top of the branch with the code from the reference commit. To transfer the code to the branch, add a commit message and save it. Now, the last commit in your branch matches the commit you’re reverting to, and your project’s history remains intact.\n\n\n\n\n\n\nAn example with git revert\n\n\n\n# Initialize a new repository\ngit init\n\n# Create a new file\necho \"Hello, World\" &gt; file.txt\n\n# Add the file to the repository\ngit add file.txt\n\n# Create a new commit\ngit commit -m \"First commit\"\n\n# Modify the file\necho \"Goodbye, World\" &gt; file.txt\n\n# Add the modified file\ngit add file.txt\n\n# Create a new commit\ngit commit -m \"Second commit\"\n\n# Use git log to find the commit ID of the second commit\ngit log\n\n# Use git revert to undo the changes from the second commit\ngit revert &lt;commit-id&gt;\n\n\nTo download the students branch from the repository git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git to your local machine, add a file, and upload the changes, you can follow these steps:\n\n\n\n\n\n\nAn example with git clone, git checkout, git add, git commit, git push\n\n\n\n# Clone the repository to your local machine:\ngit clone git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git\n\n# Change to the cloned repository:\ncd numerische-mathematik-sommersemester2023\n\n# Switch to the students branch:\ngit checkout students\n\n# Create the Test folder if it doesn't exist:\nmkdir Test\n\n# Create the Testdatei.txt file in the Test folder:\ntouch Test/Testdatei.txt\n\n# Add the file with git add:\ngit add Test/Testdatei.txt\n\n# Commit the changes with git commit:\ngit commit -m \"Added Testdatei.txt\"\n\n# Push the changes with git push:\ngit push origin students\nThis will upload the changes to the server and update the students branch in the repository.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#downloading-from-gitlab",
    "href": "a_02_git_intro_en.html#downloading-from-gitlab",
    "title": "Appendix B — Git Introduction",
    "section": "B.5 Downloading from GitLab",
    "text": "B.5 Downloading from GitLab\nTo download changes from a GitLab repository to your local machine, you can use the git pull command. This command downloads the latest changes from the specified remote repository and merges them with your local repository.\nHere is an example:\n\n\n\n\n\n\nAn example with git pull\n\n\n\n\n# Navigate to the local repository\n# linked to the GitHub repository:\ncd my-local-repository\n\n# Make sure you are in the correct branch:\ngit checkout main\n\n# Download the latest changes from GitHub:\ngit pull origin main\nThis downloads the latest changes from the main branch of the remote repository named “origin” and merges them with your local repository.\n\n\n\nIf there are conflicts between the downloaded changes and your local changes, you will need to resolve them manually before proceeding.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#advanced",
    "href": "a_02_git_intro_en.html#advanced",
    "title": "Appendix B — Git Introduction",
    "section": "B.6 Advanced",
    "text": "B.6 Advanced\n\nB.6.1 git rebase: Moving the Base of a Branch\nIn some cases, you may need to “rewrite history.” A common scenario is that you’ve been working on a new feature in a feature branch, and you realize that the work should have actually happened in the main branch.\nTo resolve this issue and make it appear as if the work occurred in the main branch, you can reset the experimental branch. “Rebase” literally means detaching the base of the experimental branch and moving it to the beginning of another branch, giving the branch a new base, thus “rebasing.”\nThis operation is performed from the branch you want to “rebase.” You use git rebase and specify the branch you want to use as the new base. If there are no conflicts between the experimental branch and the branch you want to rebase onto, this process happens automatically.\nIf there are conflicts, Git will guide you through the conflict resolution process for each commit from the rebase branch.\nThis may sound like a lot, but there’s a good reason for it. You are literally rewriting history by transferring commits from one branch to another. To maintain the coherence of the new version history, there should be no conflicts within the commits. So, you need to resolve them one by one until the history is clean. It goes without saying that this can be a fairly labor-intensive process. Therefore, you should not use git rebase frequently.\n\n\n\n\n\n\nAn example with git rebase\n\n\n\ngit rebase is a command used to change the base of a branch. This means that commits from the branch are applied to a new base, which is usually another branch. It can be used to clean up the repository history and avoid merge conflicts.\nHere is an example showing how to use git rebase:\n\nIn this example, we initialize a new Git repository and create a new file. We add the file to the repository and make an initial commit. Then, we create a new branch called “feature” and switch to that branch. We make changes to the file in the feature branch and create a new commit.\nThen, we switch back to the main branch and make changes to the file again. We add the modified file and make another commit.\nTo rebase the feature branch onto the main branch, we first switch to the feature branch and then use the git rebase command with the name of the main branch as an argument. This applies the commits from the feature branch to the main branch and changes the base of the feature branch.\n\n# Initialize a new repository\ngit init\n# Create a new file\necho \"Hello World\" &gt; file.txt\n# Add the file to the repository\ngit add file.txt\n# Create an initial commit\ngit commit -m \"Initial commit\"\n# Create a new branch called \"feature\"\ngit branch feature\n# Switch to the \"feature\" branch\ngit checkout feature\n# Make changes to the file in the \"feature\" branch\necho \"Hello Feature World\" &gt; file.txt\n# Add the modified file\ngit add file.txt\n# Create a new commit in the \"feature\" branch\ngit commit -m \"Feature commit\"\n# Switch back to the \"main\" branch\ngit checkout main\n# Make changes to the file in the \"main\" branch\necho \"Hello Main World\" &gt; file.txt\n# Add the modified file\ngit add file.txt\n# Create a new commit in the \"main\" branch\ngit commit -m \"Main commit\"\n# Use git rebase to rebase the \"feature\" branch\n# onto the \"main\" branch\ngit checkout feature\ngit rebase main",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#exercises",
    "href": "a_02_git_intro_en.html#exercises",
    "title": "Appendix B — Git Introduction",
    "section": "B.7 Exercises",
    "text": "B.7 Exercises\nIn order to be able to carry out this exercise, we provide you with a functional working environment. This can be accessed here. You can log in using your GMID. If you do not have one, you can generate one here. Once you have successfully logged in to the server, you must open a terminal instance. You are now in a position to carry out the exercise.\nAlternatively, you can also carry out the exercise locally on your computer, but then you will need to install git.\n\nB.7.1 Create project folder\nFirst create the test-repo folder via the command line and then navigate to this folder using the corresponding command.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_02_git_intro_en.html#initialize-repo",
    "href": "a_02_git_intro_en.html#initialize-repo",
    "title": "Appendix B — Git Introduction",
    "section": "B.8 Initialize repo",
    "text": "B.8 Initialize repo\nNow initialize the repository so that the future project, which will be saved in the test-repo folder, and all associated files are versioned.\n\nB.8.1 Do not upload / ignore certain file types\nIn order to carry out this exercise, you must first download a file which you then have git ignore. To do this, download the current examination regulations for the Bachelor’s degree program in Electrical Engineering using the following command curl -o pruefungsordnung.pdf https://www.th-koeln.de/mam/downloads/deutsch/studium/studiengaenge/f07/ordnungen_plaene/f07_bpo_ba_ekb_2021_01_04.pdf.\nThe PDF file has been stored in the root directory of your repo and you must now exclude it from being uploaded so that no changes to this file are tracked. Please note that not only this one PDF file should be ignored, but all PDF files in the repo.\n\n\nB.8.2 Create file and stage it\nIn order to be able to commit a change later and thus make it traceable, it must first be staged. However, as we only have a PDF file so far, which is to be ignored by git, we cannot stage anything. Therefore, in this task, a file test.txt with some string as content is to be created and then staged.\n\n\nB.8.3 Create another file and check status\nTo understand the status function, you should create the file test2.txt and then call the status function of git.\n\n\nB.8.4 Commit changes\nAfter the changes to the test.txt file have been staged and these are now to be transferred to the project process, they must be committed. Therefore, in this step you should perform a corresponding commit in the current branch with the message test-commit. Finally, you should also display the history of the commits.\n\n\nB.8.5 Create a new branch and switch to it\nIn this task, you are to create a new branch with the name change-text in which you will later make changes. You should then switch to this branch.\n\n\nB.8.6 Commit changes in the new branch\nTo be able to merge the new branch into the main branch later, you must first make changes to the test.txt file. To do this, open the file and simply change the character string in this file before saving the changes and closing the file. Before you now commit the file, you should reset the file to the status of the last commit for practice purposes and thus undo the change. After you have done this, open the file test.txt again and change the character string again before saving and closing the file. This time you should commit the file test.txt and then commit it with the message test-commit2.\n\n\nB.8.7 Merge branch into main\nAfter you have committed the change to the test.txt file, you should merge the change-text branch including the change into the main branch so that it is also available there.\n\n\nB.8.8 Resolve merge conflict\nTo simulate a merge conflict, you must first change the content of the test.txt file before you commit the change. Then switch to the branch change-text and change the file test.txt there as well before you commit the change. Now you should try to merge the branch change-text into the main branch and solve the problems that occur in order to be able to perform the merge successfully.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git Introduction</span>"
    ]
  },
  {
    "objectID": "a_03_python_intro_en.html",
    "href": "a_03_python_intro_en.html",
    "title": "Appendix C — Python Introduction",
    "section": "",
    "text": "C.1 Recommendations\nBeginner’s Guide to Python",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Python Introduction</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html",
    "href": "a_04_spot_doc.html",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "",
    "text": "D.1 An Initial Example\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[\nf(x) = x^2.\n\\]\nfun = analytical().fun_sphere\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\nfrom spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\nspot_1 = spot.Spot(fun=fun,\n                   fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                   design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                   surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_1.run()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x36706fd50&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#organization",
    "href": "a_04_spot_doc.html#organization",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.2 Organization",
    "text": "D.2 Organization\nSpot organizes the surrogate based optimization process in four steps:\n\nSelection of the objective function: fun.\nSelection of the initial design: design.\nSelection of the optimization algorithm: optimizer.\nSelection of the surrogate model: surrogate.\n\nFor each of these steps, the user can specify an object:\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_sphere\nfrom spotPython.design.spacefilling import spacefilling\ndesign = spacefilling(2)\nfrom scipy.optimize import differential_evolution\noptimizer = differential_evolution\nfrom spotPython.build.kriging import Kriging\nsurrogate = Kriging()\n\nFor each of these steps, the user can specify a dictionary of control parameters.\n\nfun_control\ndesign_control\noptimizer_control\nsurrogate_control\n\nEach of these dictionaries has an initialzaion method, e.g., fun_control_init(). The initialization methods set the default values for the control parameters.\n\n\n\n\n\n\nImportant:\n\n\n\n\nThe specification of an lower bound in fun_control is mandatory.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\nfun_control=fun_control_init(lower=np.array([-1, -1]),\n                            upper=np.array([1, 1]))\ndesign_control=design_control_init()\noptimizer_control=optimizer_control_init()\nsurrogate_control=surrogate_control_init()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#the-spot-object",
    "href": "a_04_spot_doc.html#the-spot-object",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.3 The Spot Object",
    "text": "D.3 The Spot Object\nBased on the definition of the fun, design, optimizer, and surrogate objects, and their corresponding control parameter dictionaries, fun_control, design_control, optimizer_control, and surrogate_control, the spot object can be build as follows:\n\nfrom spotPython.spot import spot\nspot_tuner = spot.Spot(fun=fun,\n                       fun_control=fun_control,\n                       design_control=design_control,\n                       optimizer_control=optimizer_control,\n                       surrogate_control=surrogate_control)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run",
    "href": "a_04_spot_doc.html#run",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.4 Run",
    "text": "D.4 Run\n\nspot_tuner.run()\n\nspotPython tuning: 1.801603872454505e-05 [#######---] 73.33% \nspotPython tuning: 1.801603872454505e-05 [########--] 80.00% \nspotPython tuning: 1.801603872454505e-05 [#########-] 86.67% \nspotPython tuning: 1.801603872454505e-05 [#########-] 93.33% \nspotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x3606fd4d0&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#print-the-results",
    "href": "a_04_spot_doc.html#print-the-results",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.5 Print the Results",
    "text": "D.5 Print the Results\n\nspot_tuner.print_results()\n\nmin y: 1.801603872454505e-05\nx0: 0.0019077911677074135\nx1: 0.003791618596979743\n\n\n[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#show-the-progress",
    "href": "a_04_spot_doc.html#show-the-progress",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.6 Show the Progress",
    "text": "D.6 Show the Progress\n\nspot_tuner.plot_progress()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#visualize-the-surrogate",
    "href": "a_04_spot_doc.html#visualize-the-surrogate",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.7 Visualize the Surrogate",
    "text": "D.7 Visualize the Surrogate\n\nThe plot method of the kriging surrogate is used.\nNote: the plot uses the interval defined by the ranges of the natural variables.\n\n\nspot_tuner.surrogate.plot()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "href": "a_04_spot_doc.html#run-with-a-specific-start-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.8 Run With a Specific Start Design",
    "text": "D.8 Run With a Specific Start Design\nTo pass a specific start design, use the X_start argument of the run method.\n\nspot_x0 = spot.Spot(fun=fun,\n                    fun_control=fun_control_init(\n                        lower = np.array([-10]),\n                        upper = np.array([100]),\n                        fun_evals = 7,\n                        fun_repeats = 1,\n                        max_time = inf,\n                        noise = False,\n                        tolerance_x = np.sqrt(np.spacing(1)),\n                        var_type=[\"num\"],\n                        infill_criterion = \"y\",\n                        n_points = 1,\n                        seed=123,\n                        log_level = 50),\n                    design_control=design_control_init(\n                        init_size=5,\n                        repeats=1),\n                    surrogate_control=surrogate_control_init(\n                        noise=False,\n                        min_theta=-4,\n                        max_theta=3,\n                        n_theta=1,\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000))\nspot_x0.run(X_start=np.array([0.5, -0.5]))\nspot_x0.plot_progress()\n\nspotPython tuning: 2.0106521524877827 [#########-] 85.71% \nspotPython tuning: 0.01033163973935242 [##########] 100.00% Done...",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#init-build-initial-design",
    "href": "a_04_spot_doc.html#init-build-initial-design",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.9 Init: Build Initial Design",
    "text": "D.9 Init: Build Initial Design\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\ngen = spacefilling(2)\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\n\n[[ 8.97647221 13.41926847]\n [ 0.66946019  1.22344228]\n [ 5.23614115 13.78185824]\n [ 5.6149825  11.5851384 ]\n [-1.72963184  1.66516096]\n [-4.26945568  7.1325531 ]\n [ 1.26363761 10.17935555]\n [ 2.88779942  8.05508969]\n [-3.39111089  4.15213772]\n [ 7.30131231  5.22275244]]\n[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975\n  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#replicability",
    "href": "a_04_spot_doc.html#replicability",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.10 Replicability",
    "text": "D.10 Replicability\nSeed\n\ngen = spacefilling(2, seed=123)\nX0 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=345)\nX1 = gen.scipy_lhd(3)\nX2 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=123)\nX3 = gen.scipy_lhd(3)\nX0, X1, X2, X3\n\n(array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]),\n array([[0.78373509, 0.86811887],\n        [0.06692621, 0.6058029 ],\n        [0.41374778, 0.00525456]]),\n array([[0.121357  , 0.69043832],\n        [0.41906219, 0.32838498],\n        [0.86742658, 0.52910374]]),\n array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#surrogates",
    "href": "a_04_spot_doc.html#surrogates",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.11 Surrogates",
    "text": "D.11 Surrogates\n\nD.11.1 A Simple Predictor\nThe code below shows how to use a simple model for prediction. Assume that only two (very costly) measurements are available:\n\nf(0) = 0.5\nf(2) = 2.5\n\nWe are interested in the value at \\(x_0 = 1\\), i.e., \\(f(x_0 = 1)\\), but cannot run an additional, third experiment.\n\nfrom sklearn import linear_model\nX = np.array([[0], [2]])\ny = np.array([0.5, 2.5])\nS_lm = linear_model.LinearRegression()\nS_lm = S_lm.fit(X, y)\nX0 = np.array([[1]])\ny0 = S_lm.predict(X0)\nprint(y0)\n\n[1.5]\n\n\nCentral Idea: Evaluation of the surrogate model S_lm is much cheaper (or / and much faster) than running the real-world experiment \\(f\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "a_04_spot_doc.html#demotest-objective-function-fails",
    "href": "a_04_spot_doc.html#demotest-objective-function-fails",
    "title": "Appendix D — Documentation of the Sequential Parameter Optimization",
    "section": "D.12 Demo/Test: Objective Function Fails",
    "text": "D.12 Demo/Test: Objective Function Fails\nSPOT expects np.nan values from failed objective function values. These are handled. Note: SPOT’s counter considers only successful executions of the objective function.\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nimport numpy as np\nfrom math import inf\n# number of initial points:\nni = 20\n# number of points\nn = 30\n\nfun = analytical().fun_random_error\nfun_control=fun_control_init(\n    lower = np.array([-1]),\n    upper= np.array([1]),\n    fun_evals = n,\n    show_progress=False)\ndesign_control=design_control_init(init_size=ni)\n\nspot_1 = spot.Spot(fun=fun,\n                     fun_control=fun_control,\n                     design_control=design_control)\nspot_1.run()\n# To check whether the run was successfully completed,\n# we compare the number of evaluated points to the specified\n# number of points.\nassert spot_1.y.shape[0] == n\n\n[        nan         nan -0.02203599 -0.21843718  0.78240941         nan\n -0.3923345   0.67234256  0.31802454 -0.68898927 -0.75129705  0.97550354\n  0.41757584         nan  0.82585329         nan -0.49274073         nan\n -0.17991251  0.1481835 ]\n[-1.]\n[nan]\n[-0.14624037]\n[0.166475]\n[nan]\n[-0.3352401]\n[-0.47259301]\n[0.95541987]\n[0.17335968]\n[-0.58552368]\n[-0.20126111]\n[-0.60100809]\n[-0.97897336]\n[-0.2748985]\n[0.8359486]\n[0.99035591]\n[0.01641232]\n[0.5629346]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Documentation of the Sequential Parameter Optimization</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadi, Martin, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\nCraig Citro, Greg S. Corrado, et al. 2016. “TensorFlow: Large-Scale Machine Learning on Heterogeneous\nDistributed Systems.” arXiv e-Prints, March,\narXiv:1603.04467.\n\n\nAggarwal, Charu, ed. 2007. Data Streams – Models and\nAlgorithms. Springer-Verlag.\n\n\nBartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf\nMersmann, eds. 2022. Hyperparameter Tuning for\nMachine and Deep Learning with R - A Practical Guide.\nSpringer.\n\n\nBartz-Beielstein, Thomas. 2023. “PyTorch\nHyperparameter Tuning with SPOT: Comparison with Ray\nTuner and Default Hyperparameters on\nCIFAR10.” https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.\n\n\n———. 2024a. “Evaluation and Performance Measurement.” In,\nedited by Eva Bartz and Thomas Bartz-Beielstein, 47–62. Singapore:\nSpringer Nature Singapore.\n\n\n———. 2024b. “Hyperparameter Tuning.” In, edited by Eva\nBartz and Thomas Bartz-Beielstein, 125–40. Singapore: Springer Nature\nSingapore.\n\n\n———. 2024c. “Introduction: From Batch to Online Machine\nLearning.” In Online Machine Learning: A Practical Guide with\nExamples in Python, edited by Eva Bartz and Thomas\nBartz-Beielstein, 1–11. Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-99-7007-0_1.\n\n\nBartz-Beielstein, Thomas, and Lukas Hans. 2024. “Drift Detection\nand Handling.” In Online Machine Learning: A Practical Guide\nwith Examples in Python, edited by Eva Bartz and Thomas\nBartz-Beielstein, 23–39. Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-99-7007-0_3.\n\n\nBartz-Beielstein, Thomas, and Martin Zaefferer. 2022.\n“Hyperparameter Tuning Approaches.” In Hyperparameter Tuning for Machine and Deep Learning with\nR - A Practical Guide, edited by Eva Bartz, Thomas\nBartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, 67–114. Springer.\n\n\nBifet, Albert. 2010. Adaptive Stream Mining: Pattern Learning and\nMining from Evolving Data Streams. Vol. 207. Frontiers in\nArtificial Intelligence and Applications. IOS Press.\n\n\nBifet, Albert, and Ricard Gavaldà. 2007. “Learning from\nTime-Changing Data with Adaptive Windowing.” In Proceedings\nof the 2007 SIAM International Conference on Data Mining (SDM),\n443–48.\n\n\n———. 2009. “Adaptive Learning from Evolving Data Streams.”\nIn Proceedings of the 8th International Symposium on Intelligent\nData Analysis: Advances in Intelligent Data Analysis VIII, 249–60.\nIDA ’09. Berlin, Heidelberg: Springer-Verlag.\n\n\nBifet, Albert, Geoff Holmes, Richard Kirkby, and Bernhard Pfahringer.\n2010a. “MOA: Massive Online\nAnalysis.” Journal of Machine Learning Research 99:\n1601–4.\n\n\n———. 2010b. “MOA: Massive Online Analysis.” Journal of\nMachine Learning Research 11: 1601–4.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\n“BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding.” arXiv\ne-Prints, October, arXiv:1810.04805.\n\n\nDomingos, Pedro M., and Geoff Hulten. 2000. “Mining High-Speed\nData Streams.” In Proceedings of the Sixth ACM\nSIGKDD International Conference on Knowledge Discovery and\nData Mining, Boston, MA, USA, August 20-23, 2000, edited by Raghu\nRamakrishnan, Salvatore J. Stolfo, Roberto J. Bayardo, and Ismail Parsa,\n71–80. ACM.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.\n2020. “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale.” arXiv\ne-Prints, October, arXiv:2010.11929.\n\n\nDredze, Mark, Tim Oates, and Christine Piatko. 2010. “We’re Not in\nKansas Anymore: Detecting Domain Changes in Streams.” In\nProceedings of the 2010 Conference on Empirical Methods in Natural\nLanguage Processing, 585–95.\n\n\nForrester, Alexander, András Sóbester, and Andy Keane. 2008. Engineering Design via Surrogate Modelling.\nWiley.\n\n\nGaber, Mohamed Medhat, Arkady Zaslavsky, and Shonali Krishnaswamy. 2005.\n“Mining Data Streams: A Review.” SIGMOD\nRec. 34: 18–26.\n\n\nGama, João, Pedro Medas, Gladys Castillo, and Pedro Rodrigues. 2004.\n“Learning with Drift Detection.” In Advances in\nArtificial Intelligence – SBIA 2004, edited by Ana L. C. Bazzan and\nSofiane Labidi, 286–95. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nGama, João, Raquel Sebastião, and Pedro Pereira Rodrigues. 2013.\n“On Evaluating Stream Learning Algorithms.” Machine\nLearning 90 (3): 317–46.\n\n\nGramacy, Robert B. 2020. Surrogates. CRC press.\n\n\nHoeglinger, Stefan, and Russel Pears. 2007. “Use of Hoeffding\nTrees in Concept Based Data Stream Mining.” 2007 Third\nInternational Conference on Information and Automation for\nSustainability, 57–62.\n\n\nIkonomovska, Elena. 2012. “Algorithms for Learning Regression\nTrees and Ensembles on Evolving Data Streams.” PhD thesis, Jozef\nStefan International Postgraduate School.\n\n\nJain, Sarthak, and Byron C. Wallace. 2019. “Attention is not Explanation.” arXiv\ne-Prints, February, arXiv:1902.10186.\n\n\nKeller-McNulty, Sallie, ed. 2004. Statistical Analysis of Massive\nData Streams: Proceedings of a Workshop. Washington,\nDC: Committee on Applied; Theoretical Statistics, National Research\nCouncil; National Academies Press.\n\n\nLippe, Phillip. 2022. “UvA Deep Learning\nTutorials.”\n\n\nLiu, Liyuan, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu,\nJianfeng Gao, and Jiawei Han. 2019. “On the\nVariance of the Adaptive Learning Rate and Beyond.”\narXiv e-Prints, August, arXiv:1908.03265.\n\n\nManapragada, Chaitanya, Geoffrey I. Webb, and Mahsa Salehi. 2018.\n“Extremely Fast Decision Tree.” In KDD’ 2018 -\nProceedings of the 24th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, edited by Chih-Jen Lin and Hui Xiong,\n1953–62. United States of America: Association for Computing Machinery\n(ACM). https://doi.org/10.1145/3219819.3220005.\n\n\nMasud, Mohammad, Jing Gao, Latifur Khan, Jiawei Han, and Bhavani M\nThuraisingham. 2011. “Classification and Novel Class Detection in\nConcept-Drifting Data Streams Under Time Constraints.” IEEE\nTransactions on Knowledge and Data Engineering 23 (6): 859–74.\n\n\nMontiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey\nBolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021.\n“River: Machine Learning for Streaming Data in Python.”\n\n\nMourtada, Jaouad, Stephane Gaiffas, and Erwan Scornet. 2019.\n“AMF: Aggregated Mondrian Forests for Online\nLearning.” arXiv e-Prints, June,\narXiv:1906.10529. https://doi.org/10.48550/arXiv.1906.10529.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.\nGrisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning\nin Python.” Journal of Machine Learning\nResearch 12: 2825–30.\n\n\nPutatunda, Sayan. 2021. Practical Machine Learning for Streaming\nData with Python. Springer.\n\n\nSantner, T J, B J Williams, and W I Notz. 2003. The Design and Analysis of Computer\nExperiments. Berlin, Heidelberg, New York: Springer.\n\n\nStreet, W. Nick, and YongSeog Kim. 2001. “A Streaming Ensemble\nAlgorithm (SEA) for Large-Scale Classification.” In\nProceedings of the Seventh ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, 377–82. KDD ’01. New York, NY,\nUSA: Association for Computing Machinery.\n\n\nTay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020.\n“Efficient Transformers: A Survey.” arXiv\ne-Prints, September, arXiv:2009.06732.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” arXiv\ne-Prints, June, 1–15.\n\n\nWiegreffe, Sarah, and Yuval Pinter. 2019. “Attention is not not Explanation.”\narXiv e-Prints, August, arXiv:1908.04626.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]