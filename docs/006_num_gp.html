<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Kriging (Gaussian Process Regression) – Hyperparameter Tuning Cookbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./007_num_spot_intro.html" rel="next">
<link href="./005_num_rsm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c3f85a7a76990882806989dd406db056.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="6&nbsp; Kriging (Gaussian Process Regression) – Hyperparameter Tuning Cookbook">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="[6]{.chapter-number}&nbsp; [Kriging (Gaussian Process Regression)]{.chapter-title}">
<meta name="citation_fulltext_html_url" content="https://arxiv.org/abs/2307.10262">
<meta name="citation_doi" content="10.48550/arXiv.2307.10262">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="arXiv">
<meta name="citation_reference" content="citation_title=Benchmarking in optimization: Best practice and open issues;,citation_author=Thomas Bartz-Beielstein;,citation_author=Carola Doerr;,citation_author=Jakob Bossek;,citation_author=Sowmya Chandrasekaran;,citation_author=Tome Eftimov;,citation_author=Andreas Fischbach;,citation_author=Pascal Kerschke;,citation_author=Manuel Lopez-Ibanez;,citation_author=Katherine M. Malan;,citation_author=Jason H. Moore;,citation_author=Boris Naujoks;,citation_author=Patryk Orzechowski;,citation_author=Vanessa Volz;,citation_author=Markus Wagner;,citation_author=Thomas Weise;,citation_publication_date=2020-07;,citation_cover_date=2020-07;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2007.03488;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Hyperparameter tuning with ray tune;,citation_author=undefined PyTorch;,citation_publication_date=2023-05;,citation_cover_date=2023-05;,citation_year=2023;,citation_fulltext_html_url=https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html;">
<meta name="citation_reference" content="citation_title=Training a classifier;,citation_author=undefined PyTorch;,citation_publication_date=2023-05;,citation_cover_date=2023-05;,citation_year=2023;,citation_fulltext_html_url=https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html;">
<meta name="citation_reference" content="citation_title=PyTorch hyperparameter tuning with SPOT: Comparison with Ray Tuner and default hyperparameters on CIFAR10;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2023-04;,citation_cover_date=2023-04;,citation_year=2023;,citation_fulltext_html_url=https://github.com/sequential-parameter-optimization/spotpython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb;">
<meta name="citation_reference" content="citation_title=Machine learning in official statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.48550/arXiv.1812.10422;">
<meta name="citation_reference" content="citation_title=Qualitätshandbuch der Statistischen Ämter des Bundes und der Länder;,citation_publication_date=2021-03;,citation_cover_date=2021-03;,citation_year=2021;,citation_fulltext_html_url=https://www.destatis.de/DE/Methoden/Qualitaet/qualitaetshandbuch.pdf;,citation_language=de;">
<meta name="citation_reference" content="citation_title=Quality Assurance Framework of the European Statistical System;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://ec.europa.eu/eurostat/documents/64157/4392716/ESS-QAF-V2.0-final.pdf;,citation_language=en;">
<meta name="citation_reference" content="citation_title=Standardisierung der prozesse: 14 jahre AG SteP;,citation_author=T. Blumöhr;,citation_author=C. Teichmann;,citation_author=A. Noack;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://www.destatis.de/DE/Methoden/
               WISTA-Wirtschaft-und-Statistik/2017/05/standardisierung-prozesse-052017.html;,citation_volume=5;,citation_journal_title=WISTA - Wirtschaft und Statistik;,citation_publisher=Wiesbaden: Statistisches Bundesamt (Destatis);">
<meta name="citation_reference" content="citation_title=Generic statistical business process model - GSBPM;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://statswiki.unece.org/display/
               GSBPM/GSBPM+v5.1;,citation_publisher=United Nations Economic Commission for Europe (UNECE);">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Engineering Design via Surrogate Modelling;,citation_author=Alexander Forrester;,citation_author=András Sóbester;,citation_author=Andy Keane;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;">
<meta name="citation_reference" content="citation_title=No Free Lunch Theorems for Optimization;,citation_author=David H Wolpert;,citation_author=William G Macready;,citation_publication_date=1997-04;,citation_cover_date=1997-04;,citation_year=1997;,citation_issue=1;,citation_volume=1;,citation_journal_title=IEEE Transactions on Evolutionary Computation;">
<meta name="citation_reference" content="citation_title=An Introduction to Statistical Learning with Applications in R;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Requirements for papers focusing on new or improved global optimization algorithms;,citation_author=Raphael T. Haftka;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=54;,citation_journal_title=Structural and Multidisciplinary Optimization;">
<meta name="citation_reference" content="citation_title=LightGBM: A highly efficient gradient boosting decision tree;,citation_author=Guolin Ke;,citation_author=Qi Meng;,citation_author=Thomas Finley;,citation_author=Taifeng Wang;,citation_author=Wei Chen;,citation_author=Weidong Ma;,citation_author=Qiwei Ye;,citation_author=Tie-Yan Liu;,citation_editor=I. Guyon;,citation_editor=U. Von Luxburg;,citation_editor=S. Bengio;,citation_editor=H. Wallach;,citation_editor=R. Fergus;,citation_editor=S. Vishwanathan;,citation_editor=R. Garnett;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_conference_title=Advances in neural information processing systems;,citation_conference=Curran Associates, Inc.;">
<meta name="citation_reference" content="citation_title=Greedy function approximation: A gradient boosting machine;,citation_author=Jerome H. Friedman;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_issue=5;,citation_volume=29;,citation_journal_title=The Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Machine learning in official statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.48550/arXiv.1812.10422;">
<meta name="citation_reference" content="citation_title=Official statistics in the era of big data opportunities and threats;,citation_author=Walter J. Radermacher;,citation_publication_date=2018-11-01;,citation_cover_date=2018-11-01;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1007/s41060-018-0124-z;,citation_issue=3;,citation_doi=10.1007/s41060-018-0124-z;,citation_volume=6;,citation_language=en;,citation_journal_title=International Journal of Data Science and Analytics;">
<meta name="citation_reference" content="citation_title=Machine learning in official statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;">
<meta name="citation_reference" content="citation_title=A quality framework for statistical algorithms;,citation_author=Wesley Yung;,citation_author=Siu-Ming Tam;,citation_author=Bart Buelens;,citation_author=Hugh Chipman;,citation_author=Florian Dumpert;,citation_author=Gabriele Ascari;,citation_author=Fabiana Rocci;,citation_author=Joep Burger;,citation_author=InKyung Choi;,citation_publication_date=2022-01-01;,citation_cover_date=2022-01-01;,citation_year=2022;,citation_fulltext_html_url=https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji210875;,citation_issue=1;,citation_doi=10.3233/SJI-210875;,citation_volume=38;,citation_language=en;,citation_journal_title=Statistical Journal of the IAOS;">
<meta name="citation_reference" content="citation_title=A quality framework for statistical algorithms;,citation_author=Wesley Yung;,citation_author=Siu-Ming Tam;,citation_author=Bart Buelens;,citation_author=Hugh Chipman;,citation_author=Florian Dumpert;,citation_author=Gabriele Ascari;,citation_author=Fabiana Rocci;,citation_author=Joep Burger;,citation_author=InKyung Choi;,citation_publication_date=2022-01;,citation_cover_date=2022-01;,citation_year=2022;,citation_issue=1;,citation_volume=38;,citation_journal_title=Statistical Journal of the IAOS;">
<meta name="citation_reference" content="citation_title=Qualitätshandbuch der statistischen Ämter des bundes und der länder;,citation_author=Michael Reichelt;">
<meta name="citation_reference" content="citation_title=Data Science and Official Statistics: Toward a New Data Culture;,citation_author=Stefan Schweinfest;,citation_author=Ronald Jansen;,citation_publication_date=2021-10-28;,citation_cover_date=2021-10-28;,citation_year=2021;,citation_fulltext_html_url=https://hdsr.mitpress.mit.edu/pub/1g514ljw/release/4;,citation_issue=4;,citation_doi=10.1162/99608f92.c1237762;,citation_volume=3;,citation_language=en;,citation_journal_title=Harvard Data Science Review;">
<meta name="citation_reference" content="citation_title=Official statistics 4.0: Verified facts for people in the 21st century;,citation_author=Walter J. Radermacher;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=Detecting covariate drift with explanations;,citation_author=Steffen Castle;,citation_author=Robert Schwarzenberg;,citation_author=Mohsen Pourvali;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Natural language processing and chinese computing: 10th CCF international conference, NLPCC 2021, qingdao, china, october 13–17, 2021, proceedings, part II;,citation_conference=Springer-Verlag;">
<meta name="citation_reference" content="citation_title=Keras;,citation_author=Francois Chollet;,citation_author=others;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_publisher=https:://keras.io;">
<meta name="citation_reference" content="citation_title=TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems;,citation_author=Martin Abadi;,citation_author=Ashish Agarwal;,citation_author=Paul Barham;,citation_author=Eugene Brevdo;,citation_author=Zhifeng Chen;,citation_author=Craig Citro;,citation_author=Greg S. Corrado;,citation_author=Andy Davis;,citation_author=Jeffrey Dean;,citation_author=Matthieu Devin;,citation_author=Sanjay Ghemawat;,citation_author=Ian Goodfellow;,citation_author=Andrew Harp;,citation_author=Geoffrey Irving;,citation_author=Michael Isard;,citation_author=Yangqing Jia;,citation_author=Rafal Jozefowicz;,citation_author=Lukasz Kaiser;,citation_author=Manjunath Kudlur;,citation_author=Josh Levenberg;,citation_author=Dan Mane;,citation_author=Rajat Monga;,citation_author=Sherry Moore;,citation_author=Derek Murray;,citation_author=Chris Olah;,citation_author=Mike Schuster;,citation_author=Jonathon Shlens;,citation_author=Benoit Steiner;,citation_author=Ilya Sutskever;,citation_author=Kunal Talwar;,citation_author=Paul Tucker;,citation_author=Vincent Vanhoucke;,citation_author=Vijay Vasudevan;,citation_author=Fernanda Viegas;,citation_author=Oriol Vinyals;,citation_author=Pete Warden;,citation_author=Martin Wattenberg;,citation_author=Martin Wicke;,citation_author=Yuan Yu;,citation_author=Xiaoqiang Zheng;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Statsmodels: Econometric and statistical modeling with python;,citation_author=Skipper Seabold;,citation_author=Josef Perktold;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=9th python in science conference;">
<meta name="citation_reference" content="citation_title=Scikit-learn: Machine learning in Python;,citation_author=F. Pedregosa;,citation_author=G. Varoquaux;,citation_author=A. Gramfort;,citation_author=V. Michel;,citation_author=B. Thirion;,citation_author=O. Grisel;,citation_author=M. Blondel;,citation_author=P. Prettenhofer;,citation_author=R. Weiss;,citation_author=V. Dubourg;,citation_author=J. Vanderplas;,citation_author=A. Passos;,citation_author=D. Cournapeau;,citation_author=M. Brucher;,citation_author=M. Perrot;,citation_author=E. Duchesnay;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=12;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Array programming with NumPy;,citation_author=Charles R. Harris;,citation_author=K. Jarrod Millman;,citation_author=Stéfan J. Walt;,citation_author=Ralf Gommers;,citation_author=Pauli Virtanen;,citation_author=David Cournapeau;,citation_author=Eric Wieser;,citation_author=Julian Taylor;,citation_author=Sebastian Berg;,citation_author=Nathaniel J. Smith;,citation_author=Robert Kern;,citation_author=Matti Picus;,citation_author=Stephan Hoyer;,citation_author=Marten H. Kerkwijk;,citation_author=Matthew Brett;,citation_author=Allan Haldane;,citation_author=Jaime Fernández Río;,citation_author=Mark Wiebe;,citation_author=Pearu Peterson;,citation_author=Pierre Gérard-Marchant;,citation_author=Kevin Sheppard;,citation_author=Tyler Reddy;,citation_author=Warren Weckesser;,citation_author=Hameer Abbasi;,citation_author=Christoph Gohlke;,citation_author=Travis E. Oliphant;,citation_publication_date=2020-09;,citation_cover_date=2020-09;,citation_year=2020;,citation_issue=7825;,citation_volume=585;,citation_journal_title=Nature;">
<meta name="citation_reference" content="citation_title=Algorithms for learning regression trees and ensembles on evolving data streams;,citation_author=Elena Ikonomovska;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_dissertation_institution=Jozef Stefan International Postgraduate School;">
<meta name="citation_reference" content="citation_title=Online Bagging and Boosting;,citation_author=N C Oza;,citation_conference_title=2005 IEEE international conference on systems, man and cybernetics;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Online bagging and boosting;,citation_author=Nikunj C Oza;,citation_author=Stuart Russell;,citation_editor=T Jaakola;,citation_editor=T Richardson;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=8th insternational workshop on artificial intelligence and statistics;">
<meta name="citation_reference" content="citation_title=Event labeling combining ensemble detectors and background knowledge;,citation_author=Hadi Fanaee-T;,citation_author=Joao Gama;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=2;,citation_volume=2;,citation_journal_title=Progress in Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Adaptive random forests for evolving data stream classification;,citation_author=Heitor M. Gomes;,citation_author=Albert Bifet;,citation_author=Jesse Read;,citation_author=Jean Paul Barddal;,citation_author=Fabricio Enembreck;,citation_author=Bernhard Pfharinger;,citation_author=Geoff Holmes;,citation_author=Talel Abdessalem;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=9;,citation_volume=106;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Literate programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
<meta name="citation_reference" content="citation_title=A Review and Taxonomy of Interactive Optimization Methods in Operations Research;,citation_author=David Meignan;,citation_author=Sigrid Knust;,citation_author=Jean-Marc Frayet;,citation_author=Gilles Pesant;,citation_author=Nicolas Gaud;,citation_publication_date=2015-09;,citation_cover_date=2015-09;,citation_year=2015;,citation_journal_title=ACM Transactions on Interactive Intelligent Systems;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=In a Nutshell – The Sequential Parameter Optimization Toolbox;,citation_author=Thomas Bartz-Beielstein;,citation_author=Martin Zaefferer;,citation_author=Frederik Rehbach;,citation_publication_date=2021-12;,citation_cover_date=2021-12;,citation_year=2021;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Direct search methods: Then and now;,citation_author=R M Lewis;,citation_author=V Torczon;,citation_author=M W Trosset;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_issue=1–2;,citation_volume=124;,citation_journal_title=Journal of Computational and Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization;,citation_author=Lisha Li;,citation_author=Kevin Jamieson;,citation_author=Giulia DeSalvo;,citation_author=Afshin Rostamizadeh;,citation_author=Ameet Talwalkar;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Sequential Parameter Optimization;,citation_author=Thomas Bartz-Beielstein;,citation_author=Christian Lasarczyk;,citation_author=Mike Preuss;,citation_editor=B McKay;,citation_editor=others;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_conference_title=Proceedings 2005 Congress on Evolutionary Computation (CEC’05), Edinburgh, Scotland;,citation_conference=IEEE Press;">
<meta name="citation_reference" content="citation_title=Evolutionary algorithms;,citation_author=Thomas Bartz-Beielstein;,citation_author=Jürgen Branke;,citation_author=Jörn Mehnen;,citation_author=Olaf Mersmann;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=3;,citation_volume=4;,citation_journal_title=Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Classification and Regression Trees;,citation_author=L Breiman;,citation_author=J H Friedman;,citation_author=R A Olshen;,citation_author=C J Stone;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;">
<meta name="citation_reference" content="citation_title=Continuous inspection schemes;,citation_author=E. S. Page;,citation_publication_date=1954-06;,citation_cover_date=1954-06;,citation_year=1954;,citation_issue=1-2;,citation_volume=41;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=Counting large numbers of events in small registers;,citation_author=Robert Morris;,citation_publication_date=1978-10;,citation_cover_date=1978-10;,citation_year=1978;,citation_issue=10;,citation_volume=21;,citation_journal_title=Commun. ACM;">
<meta name="citation_reference" content="citation_title=Approximate counting: A detailed analysis;,citation_author=Philippe Flajolet;,citation_publication_date=1985-03;,citation_cover_date=1985-03;,citation_year=1985;,citation_issue=1;,citation_volume=25;,citation_journal_title=BIT;">
<meta name="citation_reference" content="citation_title=Random sampling with a reservoir;,citation_author=Jeffrey S. Vitter;,citation_publication_date=1985-03;,citation_cover_date=1985-03;,citation_year=1985;,citation_issue=1;,citation_volume=11;,citation_journal_title=ACM Trans. Math. Softw.;">
<meta name="citation_reference" content="citation_title=Catastrophic interference in connectionist networks: The sequential learning problem;,citation_author=Michael McCloskey;,citation_author=Neal J. Cohen;,citation_publication_date=1989-01;,citation_cover_date=1989-01;,citation_year=1989;,citation_issue=C;,citation_volume=24;,citation_journal_title=Psychology of Learning and Motivation - Advances in Research and Theory;">
<meta name="citation_reference" content="citation_title=Detection of Abrupt Changes - Theory and Application;,citation_author=Michèle Basseville;,citation_author=Igor V. Nikiforov;,citation_publication_date=1993;,citation_cover_date=1993;,citation_year=1993;">
<meta name="citation_reference" content="citation_title=An introduction to computational learning theory;,citation_author=Michael J. Kearns;,citation_author=Umesh V. Vazirani;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;">
<meta name="citation_reference" content="citation_title=An introduction to the kalman filter;,citation_author=Greg Welch;,citation_author=Gary Bishop;,citation_publication_date=1995;,citation_cover_date=1995;,citation_year=1995;">
<meta name="citation_reference" content="citation_title=The space complexity of approximating the frequency moments;,citation_author=Noga Alon;,citation_author=Yossi Matias;,citation_author=Mario Szegedy;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_conference_title=Proceedings of the twenty-eighth annual ACM symposium on theory of computing;,citation_conference=Association for Computing Machinery;,citation_series_title=STOC ’96;">
<meta name="citation_reference" content="citation_title=SPLICE-2 comparative evaluation: Electricity pricing;,citation_author=Michael Harries;,citation_author=U Nsw-cse-tr;,citation_author=New South Wales;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;">
<meta name="citation_reference" content="citation_title=Mining high-speed data streams;,citation_author=Pedro M. Domingos;,citation_author=Geoff Hulten;,citation_editor=Raghu Ramakrishnan;,citation_editor=Salvatore J. Stolfo;,citation_editor=Roberto J. Bayardo;,citation_editor=Ismail Parsa;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_conference_title=Proceedings of the sixth ACM SIGKDD international conference on knowledge discovery and data mining, boston, MA, USA, august 20-23, 2000;,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=Mining time-changing data streams;,citation_author=Geoff Hulten;,citation_author=Laurie Spencer;,citation_author=Pedro Domingos;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=Proceedings of the seventh ACM SIGKDD international conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’01;">
<meta name="citation_reference" content="citation_title=A streaming ensemble algorithm (SEA) for large-scale classification;,citation_author=W. Nick Street;,citation_author=YongSeog Kim;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=Proceedings of the seventh ACM SIGKDD international conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’01;">
<meta name="citation_reference" content="citation_title=3D data management: Controlling data volume, velocity, and variety;,citation_author=Douglas Laney;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_technical_report_institution=META Group;">
<meta name="citation_reference" content="citation_title=Models and issues in data stream systems;,citation_author=Brian Babcock;,citation_author=Shivnath Babu;,citation_author=Mayur Datar;,citation_author=Rajeev Motwani;,citation_author=Jennifer Widom;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_conference_title=Proceedings of the 21st ACM SIGMOD-SIGACT-SIGART symposium on principles of database systems;,citation_conference=ACM;,citation_series_title=PODS ’02;">
<meta name="citation_reference" content="citation_title=A new approximate maximal margin classification algorithm;,citation_author=Claudio Gentile;,citation_publication_date=2002-03;,citation_cover_date=2002-03;,citation_year=2002;,citation_volume=2;,citation_journal_title=J. Mach. Learn. Res.;">
<meta name="citation_reference" content="citation_title=The Design and Analysis of Computer Experiments;,citation_author=T J Santner;,citation_author=B J Williams;,citation_author=W I Notz;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;">
<meta name="citation_reference" content="citation_title=Learning with drift detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_conference_title=In SBIA brazilian symposium on artificial intelligence;,citation_conference=Springer Verlag;">
<meta name="citation_reference" content="citation_title=Learning with Drift Detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_inbook_title=Parallel problem solving from nature - PPSN XIII - 13th international conference, ljubljana, slovenia, september 13-17, 2014. proceedings;">
<meta name="citation_reference" content="citation_title=Learning with drift detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_editor=Ana L. C. Bazzan;,citation_editor=Sofiane Labidi;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_conference_title=Advances in artificial intelligence – SBIA 2004;,citation_conference=Springer Berlin Heidelberg;">
<meta name="citation_reference" content="citation_title=Statistical analysis of massive data streams: Proceedings of a workshop;,citation_editor=Sallie Keller-McNulty;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;">
<meta name="citation_reference" content="citation_title=Data mining: Practical machine learning tools and techniques;,citation_author=Ian H. Witten;,citation_author=Eibe Frank;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_series_title=The morgan kaufmann series in data management systems;">
<meta name="citation_reference" content="citation_title=Towards generic pattern mining;,citation_author=Mohammed Javeed Zaki;,citation_author=Nagender Parimi;,citation_author=Nilanjana De;,citation_author=Feng Gao;,citation_author=Benjarath Phoophakdee;,citation_author=Joe Urban;,citation_author=Vineet Chaoji;,citation_author=Mohammad Al Hasan;,citation_author=Saeed Salem;,citation_editor=Bernhard Ganter;,citation_editor=Robert Godin;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=3403;,citation_conference_title=Formal concept analysis, third international conference, ICFCA 2005, lens, france, february 14-18, 2005, proceedings;,citation_conference=Springer;,citation_series_title=Lecture notes in computer science;">
<meta name="citation_reference" content="citation_title=Mining data streams: A review;,citation_author=Mohamed Medhat Gaber;,citation_author=Arkady Zaslavsky;,citation_author=Shonali Krishnaswamy;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=34;,citation_journal_title=SIGMOD Rec.;">
<meta name="citation_reference" content="citation_title=Early drift detection method;,citation_author=Manuel Baena-Garcıa;,citation_author=José Campo-Ávila;,citation_author=Raúl Fidalgo;,citation_author=Albert Bifet;,citation_author=R Gavalda;,citation_author=Rafael Morales-Bueno;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_volume=6;,citation_conference_title=Fourth international workshop on knowledge discovery from data streams;">
<meta name="citation_reference" content="citation_title=Online passive-aggressive algorithms;,citation_author=Koby Crammer;,citation_author=Ofer Dekel;,citation_author=Joseph Keshet;,citation_author=Shai Shalev-Shwartz;,citation_author=Yoram Singer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=19;,citation_volume=7;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Data streams – models and algorithms;,citation_editor=Charu Aggarwal;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;">
<meta name="citation_reference" content="citation_title=Learning from time-changing data with adaptive windowing;,citation_author=Albert Bifet;,citation_author=Ricard Gavaldà;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=Proceedings of the 2007 SIAM international conference on data mining (SDM);">
<meta name="citation_reference" content="citation_title=Learning from time-changing data with adaptive windowing;,citation_author=Albert Bifet;,citation_author=Ricard Gavalda;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_volume=7;,citation_conference_title=Proceedings of the 2007 SIAM international conference on data mining;,citation_conference=SIAM;">
<meta name="citation_reference" content="citation_title=A survey of classification methods in data streams;,citation_author=Mohamed Gaber;,citation_author=Arkady Zaslavsky;,citation_author=Shonali Krishnaswamy;,citation_editor=Charu Aggarwal;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_inbook_title=Data streams – models and algorithms;">
<meta name="citation_reference" content="citation_title=Use of hoeffding trees in concept based data stream mining;,citation_author=Stefan Hoeglinger;,citation_author=Russel Pears;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_journal_title=2007 Third International Conference on Information and Automation for Sustainability;">
<meta name="citation_reference" content="citation_title=Detecting concept drift using statistical testing;,citation_author=Kyosuke Nishida;,citation_author=Koichiro Yamauchi;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=International conference on discovery science;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Olindda: A cluster-based approach for detecting novelty and concept drift in data streams;,citation_author=Eduardo J Spinosa;,citation_author=André Ponce Leon F. de Carvalho;,citation_author=Joao Gama;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=Proceedings of the 2007 ACM symposium on applied computing;">
<meta name="citation_reference" content="citation_title=Statistical Quality Control;,citation_author=Douglas C Montgomery;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;">
<meta name="citation_reference" content="citation_title=Adaptive learning from evolving data streams;,citation_author=Albert Bifet;,citation_author=Ricard Gavaldà;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 8th international symposium on intelligent data analysis: Advances in intelligent data analysis VIII;,citation_conference=Springer-Verlag;,citation_series_title=IDA ’09;">
<meta name="citation_reference" content="citation_title=New ensemble methods for evolving data streams;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_author=Richard Kirkby;,citation_author=Ricard Gavaldà;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 15th ACM SIGKDD international conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’09;">
<meta name="citation_reference" content="citation_title=Adaptive concept drift detection;,citation_author=Anton Dries;,citation_author=Ulrich Rückert;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=5-6;,citation_volume=2;,citation_journal_title=Stat. Anal. Data Min.;">
<meta name="citation_reference" content="citation_title=Issues in evaluation of stream learning algorithms;,citation_author=João Gama;,citation_author=Raquel Sebastião;,citation_author=Pedro Pereira Rodrigues;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 15th ACM SIGKDD international conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’09;">
<meta name="citation_reference" content="citation_title=Stream data processing: A quality of service perspective;,citation_author=Qingchun Jiang;,citation_author=Sharma Chakravarthy;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=Probabilistic counting with randomized storage;,citation_author=Benjamin Van Durme;,citation_author=Ashwin Lall;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 21st international joint conference on artificial intelligence;,citation_conference=Morgan Kaufmann Publishers Inc.;,citation_series_title=IJCAI’09;">
<meta name="citation_reference" content="citation_title=Adaptive stream mining: Pattern learning and mining from evolving data streams;,citation_author=Albert Bifet;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=207;,citation_series_title=Frontiers in artificial intelligence and applications;">
<meta name="citation_reference" content="citation_title=We’re not in kansas anymore: Detecting domain changes in streams;,citation_author=Mark Dredze;,citation_author=Tim Oates;,citation_author=Christine Piatko;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=Proceedings of the 2010 conference on empirical methods in natural language processing;">
<meta name="citation_reference" content="citation_title=Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction (Institute of Mathematical Statistics Monographs);,citation_author=Bradley Efron;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;">
<meta name="citation_reference" content="citation_title=Knowledge discovery from data streams;,citation_author=João Gama;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_series_title=Chapman and hall / CRC data mining and knowledge discovery series;">
<meta name="citation_reference" content="citation_title=A DCT based approach for detecting novelty and concept drift in data streams;,citation_author=Morteza Zi Hayat;,citation_author=Mahmoud Reza Hashemi;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=2010 international conference of soft computing and pattern recognition;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Benchmarking stream clustering algorithms within the MOA framework;,citation_author=Philipp Kranen;,citation_author=Hardy Kremer;,citation_author=Timm Jansen;,citation_author=Thomas Seidl;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=16th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2010), washington, DC, USA;">
<meta name="citation_reference" content="citation_title=MOA: Massive online analysis;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Richard Kirkby;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=99;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Hellinger distance based drift detection for nonstationary environments;,citation_author=Gregory Ditzler;,citation_author=Robi Polikar;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_conference_title=2011 IEEE symposium on computational intelligence in dynamic and uncertain environments (CIDUE);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Classification and novel class detection in concept-drifting data streams under time constraints;,citation_author=Mohammad Masud;,citation_author=Jing Gao;,citation_author=Latifur Khan;,citation_author=Jiawei Han;,citation_author=Bhavani M Thuraisingham;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=6;,citation_volume=23;,citation_journal_title=IEEE Transactions on Knowledge and Data Engineering;">
<meta name="citation_reference" content="citation_title=New drift detection method for data streams;,citation_author=Parinaz Sobhani;,citation_author=Hamid Beigy;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_conference_title=International conference on adaptive and intelligent systems;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Birch: Dealing with very large datasets using BIRCH;,citation_author=Lysiane Charest;,citation_author=Justin Harrington;,citation_author=Matias Salibian-Barrera;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Synopses for massive data: Samples, histograms, wavelets, sketches;,citation_author=Graham Cormode;,citation_author=Minos Garofalakis;,citation_author=Peter J. Haas;,citation_author=Chris Jermaine;,citation_publication_date=2012-01;,citation_cover_date=2012-01;,citation_year=2012;,citation_issue=1–3;,citation_volume=4;,citation_journal_title=Found. Trends Databases;">
<meta name="citation_reference" content="citation_title=Detection of concept drift for learning from stream data;,citation_author=Jeonghoon Lee;,citation_author=Frederic Magoules;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=2012 IEEE 14th international conference on high performance computing and communication &amp;amp;amp; 2012 IEEE 9th international conference on embedded software and systems;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Mlbench: Machine learning benchmark problems;,citation_author=Friedrich Leisch;,citation_author=Evgenia Dimitriadou;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=A unifying view on dataset shift in classification;,citation_author=Jose G. Moreno-Torres;,citation_author=Troy Raeder;,citation_author=Rocı́o Alaı́z-Rodrı́guez;,citation_author=Nitesh V. Chawla;,citation_author=Francisco Herrera;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_volume=45;,citation_journal_title=Pattern Recognit.;">
<meta name="citation_reference" content="citation_title=HadoopStreaming: Utilities for using R scripts in Hadoop streaming;,citation_author=David S. Rosenberg;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Exponentially weighted moving average charts for detecting concept drift;,citation_author=Gordon J Ross;,citation_author=Niall M Adams;,citation_author=Dimitris K Tasoulis;,citation_author=David J Hand;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=2;,citation_volume=33;,citation_journal_title=Pattern recognition letters;">
<meta name="citation_reference" content="citation_title=An efficient method of building an ensemble of classifiers in streaming data;,citation_author=Joung Woo Ryu;,citation_author=Mehmed M Kantardzic;,citation_author=Myung-Won Kim;,citation_author=A Ra Khil;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=International conference on big data analytics;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Rlecuyer: R interface to RNG with multiple streams;,citation_author=Hana Sevcikova;,citation_author=Tony Rossini;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Machine learning that matters;,citation_author=Kiri Wagstaff;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=CD-MOA: Change detection framework for massive online analysis;,citation_author=Albert Bifet;,citation_author=Jesse Read;,citation_author=Bernhard Pfahringer;,citation_author=Geoff Holmes;,citation_author=Indrė Žliobaitė;,citation_editor=Allan Tucker;,citation_editor=Frank Höppner;,citation_editor=Arno Siebes;,citation_editor=Stephen Swift;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Advances in intelligent data analysis XII;,citation_conference=Springer Berlin Heidelberg;">
<meta name="citation_reference" content="citation_title=Novelty detection algorithm for data streams multi-class problems;,citation_author=Elaine R Faria;,citation_author=João Gama;,citation_author=André CPLF Carvalho;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 28th annual ACM symposium on applied computing;">
<meta name="citation_reference" content="citation_title=Turning big data into tiny data: Constant-size coresets for k-means, PCA and projective clustering;,citation_author=Dan Feldman;,citation_author=Melanie Schmidt;,citation_author=Christian Sohler;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the twenty-fourth annual ACM-SIAM symposium on discrete algorithms;,citation_conference=Society for Industrial; Applied Mathematics;,citation_series_title=SODA ’13;">
<meta name="citation_reference" content="citation_title=On evaluating stream learning algorithms;,citation_author=João Gama;,citation_author=Raquel Sebastião;,citation_author=Pedro Pereira Rodrigues;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=3;,citation_volume=90;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Scalable strategies for computing with massive data;,citation_author=Michael J. Kane;,citation_author=John Emerson;,citation_author=Stephen Weston;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=14;,citation_volume=55;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=RStorm: Simulate and develop streaming processing in R;,citation_author=Maurits Kaptein;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;">
<meta name="citation_reference" content="citation_title=Drift detection using uncertainty distribution divergence;,citation_author=Patrick Lindstrom;,citation_author=Brian Mac Namee;,citation_author=Sarah Jane Delany;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=1;,citation_volume=4;,citation_journal_title=Evolving Systems;">
<meta name="citation_reference" content="citation_title=Ad click prediction: A view from the trenches;,citation_author=H. Brendan McMahan;,citation_author=Gary Holt;,citation_author=D. Sculley;,citation_author=Michael Young;,citation_author=Dietmar Ebner;,citation_author=Julian Grady;,citation_author=Lan Nie;,citation_author=Todd Phillips;,citation_author=Eugene Davydov;,citation_author=Daniel Golovin;,citation_author=Sharat Chikkerur;,citation_author=Dan Liu;,citation_author=Martin Wattenberg;,citation_author=Arnar Mar Hrafnkelsson;,citation_author=Tom Boulos;,citation_author=Jeremy Kubica;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 19th ACM SIGKDD international conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’13;">
<meta name="citation_reference" content="citation_title=Data stream clustering: A survey.;,citation_author=Jonathan A. Silva;,citation_author=Elaine R. Faria;,citation_author=Rodrigo C. Barros;,citation_author=Eduardo R. Hruschka;,citation_author=Andre Carvalho;,citation_author=Joao Gama;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=1;,citation_volume=46;,citation_journal_title=ACM Computer Surveys;">
<meta name="citation_reference" content="citation_title=ff: Memory-efficient storage of large data on disk and fast access functions;,citation_author=Daniel Adler;,citation_author=Christian Gläser;,citation_author=Oleg Nenadic;,citation_author=Jens Oehlschlägel;,citation_author=Walter Zucchini;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Factas: Data mining methods for data streams;,citation_author=Romain Bar;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=streamR: Access to Twitter streaming API via R;,citation_author=Pablo Barbera;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=DBI: R database interface;,citation_author=R Special Interest Group on Databases;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Concept drift detection through resampling;,citation_author=Maayan Harel;,citation_author=Shie Mannor;,citation_author=Ran El-Yaniv;,citation_author=Koby Crammer;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=PCA feature extraction for change detection in multidimensional unlabeled data;,citation_author=Ludmila I Kuncheva;,citation_author=William J Faithfull;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=1;,citation_volume=25;,citation_journal_title=IEEE transactions on neural networks and learning systems;">
<meta name="citation_reference" content="citation_title=Mining of massive datasets;,citation_author=Jure Leskovec;,citation_author=Anand Rajaraman;,citation_author=Jeffery D. Ullman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RMOA: Connect r with MOA to perform streaming classifications;,citation_author=Jan Wijffels;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A survey on concept drift adaptation;,citation_author=João Gama;,citation_author=Indrundefined Žliobaitundefined;,citation_author=Albert Bifet;,citation_author=Mykola Pechenizkiy;,citation_author=Abdelhamid Bouchachia;,citation_publication_date=2014-03;,citation_cover_date=2014-03;,citation_year=2014;,citation_issue=4;,citation_volume=46;,citation_journal_title=ACM Comput. Surv.;">
<meta name="citation_reference" content="citation_title=Graph stream algorithms: A survey;,citation_author=Andrew McGregor;,citation_publication_date=2014-05;,citation_cover_date=2014-05;,citation_year=2014;,citation_issue=1;,citation_volume=43;,citation_journal_title=SIGMOD Rec.;">
<meta name="citation_reference" content="citation_title=RStorm: Developing and testing streaming algorithms in r;,citation_author=Maurits Kaptein;,citation_publication_date=2014-06;,citation_cover_date=2014-06;,citation_year=2014;,citation_issue=1;,citation_volume=6;,citation_journal_title=The R Journal;">
<meta name="citation_reference" content="citation_title=SNAP Datasets: Stanford large network dataset collection;,citation_author=Jure Leskovec;,citation_author=Andrej Krevl;,citation_publication_date=2014-06;,citation_cover_date=2014-06;,citation_year=2014;,citation_publisher=http://snap.stanford.edu/data;">
<meta name="citation_reference" content="citation_title=Questioning the lambda architecture;,citation_author=Jay Kreps;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Sketching as a tool for numerical linear algebra;,citation_author=David P. Woodruff;,citation_publication_date=2014-10;,citation_cover_date=2014-10;,citation_year=2014;,citation_issue=1–2;,citation_volume=10;,citation_journal_title=Found. Trends Theor. Comput. Sci.;">
<meta name="citation_reference" content="citation_title=Modeling concept drift: A probabilistic graphical model based approach;,citation_author=Hanen Borchani;,citation_author=Ana Maria Martinez;,citation_author=Andrés R. Masegosa;,citation_author=Helge Langseth;,citation_author=Thomas Dyhre Nielsen;,citation_author=Antonio Salmerón;,citation_author=Antonio Fernández;,citation_author=Anders Læsø Madsen;,citation_author=Ramón Sáez;,citation_editor=Elisa Fromont;,citation_editor=Tijl De Bie;,citation_editor=Matthijs Leeuwen;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=Advances in intelligent data analysis XIV;,citation_conference=Springer;,citation_series_title=Lecture notes in computer science;">
<meta name="citation_reference" content="citation_title=twitteR: R based Twitter client;,citation_author=Jeff Gentry;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=rEMM: Extensible markov model for data stream clustering in R;,citation_author=Michael Hahsler;,citation_author=Margaret H. Dunham;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=streamMOA: Interface for MOA stream clustering algorithms;,citation_author=Michael Hahsler;,citation_author=Matthew Bolanos;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Rstream: Streams of random numbers;,citation_author=Josef Leydold;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Big data: Principles and best practices of scalable realtime data systems;,citation_author=Nathan Marz;,citation_author=James Warren;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=A pca-based change detection framework for multidimensional data streams: Change detection in multidimensional data streams;,citation_author=Abdulhakim A Qahtan;,citation_author=Basma Alharbi;,citation_author=Suojin Wang;,citation_author=Xiangliang Zhang;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining;">
<meta name="citation_reference" content="citation_title=Concept drift detection for streaming data;,citation_author=Heng Wang;,citation_author=Zubin Abraham;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=International joint conference on neural networks (IJCNN);">
<meta name="citation_reference" content="citation_title=Koaning.io: Linear models solving non-linear problems;,citation_author=Vincent Warmerdam;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Experimental Algorithmics Applied to On-line Machine Learning;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Gregor Papa;,citation_editor=Marjan Mernik;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=Bioinspired optimization methods and their applications;">
<meta name="citation_reference" content="citation_title=Quantmod: Quantitative financial modelling framework;,citation_author=Jeffrey A. Ryan;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=A grid density based framework for classifying streaming data in the presence of concept drift;,citation_author=Tegjyot Singh Sethi;,citation_author=Mehmed Kantardzic;,citation_author=Hanquing Hu;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=46;,citation_journal_title=Journal of Intelligent Information Systems;">
<meta name="citation_reference" content="citation_title=rJava: Low-level R to Java interface;,citation_author=Simon Urbanek;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=SparkR: Scaling r programs with spark;,citation_author=Shivaram Venkataraman;,citation_author=Zongheng Yang;,citation_author=Davies Liu;,citation_author=Eric Liang;,citation_author=Hossein Falaki;,citation_author=Xiangrui Meng;,citation_author=Reynold Xin;,citation_author=Ali Ghodsi;,citation_author=Michael Franklin;,citation_author=Ion Stoica;,citation_author=Matei Zaharia;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=Proceedings of the 2016 international conference on management of data;,citation_conference=Association for Computing Machinery;,citation_series_title=SIGMOD ’16;">
<meta name="citation_reference" content="citation_title=Koaning.io: Bayesian/streaming algorithms;,citation_author=Vincent Warmerdam;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=Outlier analysis;,citation_author=Charu C Aggarwal;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Video popularity prediction in data streams based on context-independent features;,citation_author=Vitor Silva;,citation_author=Ana Trindade Winck;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_conference_title=Proceedings of the symposium on applied computing;,citation_conference=Association for Computing Machinery;,citation_series_title=SAC ’17;">
<meta name="citation_reference" content="citation_title=Einsatz von machine-learning-verfahren in amtlichen unternehmensstatistiken;,citation_author=Florian Dumpert;,citation_author=Martin Beck;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=2;,citation_volume=11;,citation_journal_title=AStA Wirtschafts- und Sozialstatistisches Archiv;">
<meta name="citation_reference" content="citation_title=Introduction to stream: An extensible framework for data stream clustering research with r;,citation_author=Michael Hahsler;,citation_author=Matthew Bolaños;,citation_author=John Forrest;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=14;,citation_volume=76;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=Stream: Infrastructure for data stream mining;,citation_author=Michael Hahsler;,citation_author=Matthew Bolaños;,citation_author=John Forrest;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Design and Analysis of Experiments;,citation_author=D C Montgomery;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Time series forecasting in the presence of concept drift: A PSO-based approach;,citation_author=Gustavo H. F. M. Oliveira;,citation_author=Rodolfo C. Cavalcante;,citation_author=George G. Cabral;,citation_author=Leandro L. Minku;,citation_author=Adriano L. I. Oliveira;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_conference_title=2017 IEEE 29th international conference on tools with artificial intelligence (ICTAI);">
<meta name="citation_reference" content="citation_title=United nations global pulse. Harnessing big data for development and humanitarian action.;,citation_author=United Nations;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Koaning.io: Passive agressive algorithms;,citation_author=Vincent Warmerdam;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=On the Reliable Detection of Concept Drift from Streaming Unlabeled Data;,citation_author=Tegjyot Singh Sethi;,citation_author=Mehmed Kantardzic;,citation_publication_date=2017-03;,citation_cover_date=2017-03;,citation_year=2017;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Proof of concept machine learning - Abschlussbericht;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Jörg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_technical_report_institution=Statistisches Bundesamt (Destatis);">
<meta name="citation_reference" content="citation_title=Machine learning for data streams with practical examples in MOA;,citation_author=Albert Bifet;,citation_author=Ricard Gavalda;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Lifelong machine learning;,citation_author=Zhiyuan Chen;,citation_author=Bing Liu;,citation_author=Ronald Brachman;,citation_author=Peter Stone;,citation_author=Francesca Rossi;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Incremental on-line learning: A review and comparison of state of the art algorithms;,citation_author=Viktor Losing;,citation_author=Barbara Hammer;,citation_author=Heiko Wersing;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=275;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Extremely fast decision tree;,citation_author=Chaitanya Manapragada;,citation_author=Geoffrey I. Webb;,citation_author=Mahsa Salehi;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’18;">
<meta name="citation_reference" content="citation_title=An evaluation of data stream clustering algorithms;,citation_author=Stratos Mansalis;,citation_author=Eirini Ntoutsi;,citation_author=Nikos Pelekis;,citation_author=Yannis Theodoridis;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_volume=11;,citation_journal_title=Statistical Analysis and Data Mining: The ASA Data Science Journal;">
<meta name="citation_reference" content="citation_title=Koaning.io: How to win with simple, even linear, models;,citation_author=Vincent Warmerdam;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Anomaly detection in manufacturing systems using structured neural networks;,citation_author=J. Liu;,citation_author=J. Guo;,citation_author=P. V. Orlik;,citation_author=M. Shibata;,citation_author=D. Nakahara;,citation_author=S. Mii;,citation_author=M. Takac;,citation_publication_date=2018-07;,citation_cover_date=2018-07;,citation_year=2018;,citation_technical_report_institution=MITSUBISHI ELECTRIC RESEARCH LABORATORIES;">
<meta name="citation_reference" content="citation_title=Industrial internet of things based ransomware detection using stacked variational neural network;,citation_author=Muna Al-Hawawreh;,citation_author=Elena Sitnikova;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=Proceedings of the 3rd international conference on big data and internet of things;,citation_conference=Association for Computing Machinery;,citation_series_title=BDIOT 2019;">
<meta name="citation_reference" content="citation_title=Evaluation of cognitive architectures for cyber-physical production systems;,citation_author=Andreas Bunte;,citation_author=Andreas Fischbach;,citation_author=Jan Strohschein;,citation_author=Thomas Bartz-Beielstein;,citation_author=Heide Faeskorn-Woyke;,citation_author=Oliver Niggemann;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=24th IEEE international conference on emerging technologies and factory automation, ETFA 2019, zaragoza, spain, september 10-13, 2019;">
<meta name="citation_reference" content="citation_title=Nowcasting: Ein echtzeit-indikator für die konjunkturanalyse;,citation_author=Charlotte Senftleben;,citation_author=Till Strohsal;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Koaning.io: The future of data science is past;,citation_author=Vincent Warmerdam;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Forecasting inflation with online prices;,citation_author=Diego Aparicio;,citation_author=Manuel I. Bertolotto;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_volume=36;,citation_journal_title=International Journal of Forecasting;">
<meta name="citation_reference" content="citation_title=CAAI—a cognitive architecture to introduce artificial intelligence in cyber-physical production systems;,citation_author=Andreas Fischbach;,citation_author=Jan Strohschein;,citation_author=Andreas Bunte;,citation_author=Jörg Stork;,citation_author=Heide Faeskorn-Woyke;,citation_author=Natalia Moriz;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=1;,citation_volume=111;,citation_journal_title=The International Journal of Advanced Manufacturing Technology;">
<meta name="citation_reference" content="citation_title=Delayed labelling evaluation for data streams;,citation_author=Maciej Grzenda;,citation_author=Heitor Murilo Gomes;,citation_author=Albert Bifet;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=5;,citation_volume=34;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Stream data mining: Algorithms and their probabilistic properties;,citation_editor=Leszek Rutkowski;,citation_editor=Maciej Jaworski;,citation_editor=Piotr Duda;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_series_title=Studies in big data;">
<meta name="citation_reference" content="citation_title=Loghub: A Large Collection of System Log Datasets towards Automated Log Analytics;,citation_author=Shilin He;,citation_author=Jieming Zhu;,citation_author=Pinjia He;,citation_author=Michael R. Lyu;,citation_publication_date=2020-08;,citation_cover_date=2020-08;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Nowcasting german GDP: Foreign factors, financial markets, and model averaging;,citation_author=Paolo Andreini;,citation_author=Thomas Hasenzagl;,citation_author=Lucrezia Reichlin;,citation_author=Charlotte Senftleben-König;,citation_author=Till Strohsal;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=International Journal of Forecasting;">
<meta name="citation_reference" content="citation_title=Modelling the COVID-19 virus evolution with incremental machine learning;,citation_author=Andrés L. Suárez-Cetrulo;,citation_author=Ankit Kumar;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Machine learning for time-series with python: Forecast, predict, and detect;,citation_author=Den Auffarth;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Machine learning in der amtlichen statistik - ergebnisse und bewertung eines internationalen projekts;,citation_author=Florian Dumpert;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=Statistisches Bundesamt (Destatis);,citation_technical_report_number=4;">
<meta name="citation_reference" content="citation_title=Recurring concept memory management in data streams: Exploiting data stream concept evolution to improve performance and transparency;,citation_author=Ben Halstead;,citation_author=Yun Sing Koh;,citation_author=Patricia Riddle;,citation_author=Russel Pears;,citation_author=Mykola Pechenizkiy;,citation_author=Albert Bifet;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=3;,citation_volume=35;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Soziale marktwirtschaft in der digitalen zukunft: Foresight-bericht strategischer vorausschauprozess des BMWi;,citation_author=Dirk Holtmannspötter;,citation_author=Ulrich Heimeshoff;,citation_author=Justus Haucap;,citation_author=Ina Loebert;,citation_author=Christoph Busch;,citation_author=Andreas Hoffknecht;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=VDI Technologiezentrum GmbH im Auftrag des Bundesministerium für Wirtschaft und Energie;">
<meta name="citation_reference" content="citation_title=River: Machine learning for streaming data in python;,citation_author=Jacob Montiel;,citation_author=Max Halford;,citation_author=Saulo Martiello Mastelini;,citation_author=Geoffrey Bolmier;,citation_author=Raphael Sourty;,citation_author=Robin Vaysse;,citation_author=Adil Zouitine;,citation_author=Heitor Murilo Gomes;,citation_author=Jesse Read;,citation_author=Talel Abdessalem;,citation_author=others;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Practical machine learning for streaming data with python;,citation_author=Sayan Putatunda;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Infrerring concept drift without labeled data;,citation_author=Andrew Reed;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=Cloudera Fast Forward Labs;,citation_technical_report_number=FF21;">
<meta name="citation_reference" content="citation_title=Cognitive capabilities for the CAAI in cyber-physical production systems;,citation_author=Jan Strohschein;,citation_author=Andreas Fischbach;,citation_author=Andreas Bunte;,citation_author=Heide Faeskorn-Woyke;,citation_author=Natalia Moriz;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=The International Journal of Advanced Manufacturing Technology;">
<meta name="citation_reference" content="citation_title=FARF: A fair and adaptive random forests classifier;,citation_author=Wenbin Zhang;,citation_author=Albert Bifet;,citation_author=Xiangliang Zhang;,citation_author=Jeremy C. Weiss;,citation_author=Wolfgang Nejdl;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Advances in knowledge discovery and data mining: 25th pacific-asia conference, PAKDD 2021, virtual event, may 11–14, 2021, proceedings, part II;,citation_conference=Springer-Verlag;">
<meta name="citation_reference" content="citation_title=Modelling the COVID-19 virus evolution with Incremental Machine Learning;,citation_author=Andrés L. Suárez-Cetrulo;,citation_author=Ankit Kumar;,citation_author=Luis Miralles-Pechuán;,citation_publication_date=2021-04;,citation_cover_date=2021-04;,citation_year=2021;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Online learning and active learning: A comparative study of passive-aggressive algorithm with support vector machine (SVM);,citation_author=K. I Ezukwoke;,citation_author=S. J Zareian;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=3;,citation_volume=21;,citation_journal_title=Journal of Higher Education Theory and Practice;">
<meta name="citation_reference" content="citation_title=Digitale ordnungspolitik – wirtschaftspolitik daten- und evidenzbasiert weiterentwickeln;,citation_author=Philipp Steinberg;,citation_author=Nils Börnsen;,citation_author=Dirk Neumann;,citation_publication_date=2021-09;,citation_cover_date=2021-09;,citation_year=2021;,citation_publisher=Wirtschaftsdienst;">
<meta name="citation_reference" content="citation_title=Incremental unsupervised domain-adversarial training of neural networks;,citation_author=Antonio-Javier Gallego;,citation_author=Jorge Calvo-Zaragoza;,citation_author=Robert B. Fisher;,citation_publication_date=2021-11;,citation_cover_date=2021-11;,citation_year=2021;,citation_issue=11;,citation_volume=32;,citation_journal_title=IEEE Transactions on Neural Networks and Learning Systems;">
<meta name="citation_reference" content="citation_title=Incremental learning for property price estimation using location-based services and open data;,citation_author=Francisco Alvarez;,citation_author=Edgar Roman-Rangel;,citation_author=Luis V. Montiel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_volume=107;,citation_journal_title=Engineering Applications of Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Interpretable machine learning: Moving from mythos to diagnostics;,citation_author=Valerie Chen;,citation_author=Jeffrey Li;,citation_author=Joon Sik Kim;,citation_author=Gregory Plumb;,citation_author=Ameet Talwalkar;,citation_publication_date=2022-01;,citation_cover_date=2022-01;,citation_year=2022;,citation_issue=6;,citation_volume=19;,citation_journal_title=Queue;">
<meta name="citation_reference" content="citation_title=Online time-series anomaly detection: A survey of modern model-based approaches;,citation_author=Lucas Correia;,citation_author=Jan-Christoph Goos;,citation_author=Anna V. Kononova;,citation_author=Thomas Bäck;,citation_author=Philipp Klein;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_technical_report_institution=Mercedes-Benz, Germany;">
<meta name="citation_reference" content="citation_title=Green accelerated hoeffding tree;,citation_author=Eva Garcia-Martin;,citation_author=Albert Bifet;,citation_author=Niklas Lavesson;,citation_author=Rikard König;,citation_author=Henrik Linusson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Monitoring the economy in real time: Trends and gaps in real activity and prices;,citation_author=Thomas Hasenzagl;,citation_author=Filippo Pellegrino;,citation_author=Lucrezia Reichlin;,citation_author=Giovanni Ricco;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Efficiently correcting machine learning: Considering the role of example ordering in human-in-the-loop training of image classification models;,citation_author=Geoff Holmes;,citation_author=Eibe Frank;,citation_author=Dale Fletcher;,citation_author=Corey Sterling;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=27th international conference on intelligent user interfaces;,citation_conference=Association for Computing Machinery;,citation_series_title=IUI ’22;">
<meta name="citation_reference" content="citation_title=TemporalWiki: A lifelong benchmark for training and evaluating ever-evolving language models;,citation_author=Joel Jang;,citation_author=Seonghyeon Ye;,citation_author=Changho Lee;,citation_author=Sohee Yang;,citation_author=Joongbo Shin;,citation_author=Janghoon Han;,citation_author=Gyeonghun Kim;,citation_author=Minjoon Seo;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Fast mining and forecasting of co-evolving epidemiological data streams;,citation_author=Tasuku Kimura;,citation_author=Yasuko Matsubara;,citation_author=Koki Kawabata;,citation_author=Yasushi Sakurai;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’22;">
<meta name="citation_reference" content="citation_title=Maschine learning for streaming data with python;,citation_author=Jan Korstanje;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=The disagreement problem in explainable machine learning: A practitioner’s perspective;,citation_author=Satyapriya Krishna;,citation_author=Tessa Han;,citation_author=Alex Gu;,citation_author=Javin Pombra;,citation_author=Shahin Jabbari;,citation_author=Steven Wu;,citation_author=Himabindu Lakkaraju;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=SparkR: R front end for ’apache spark’;,citation_author=The Apache Software Foundation;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Short-term local predictions of COVID-19 in the united kingdom using dynamic supervised machine learning algorithms;,citation_author=Xin Wang;,citation_author=Yijia Dong;,citation_author=William David Thompson;,citation_author=Harish Nair;,citation_author=You Li;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=1;,citation_volume=2;,citation_journal_title=Communications Medicine;">
<meta name="citation_reference" content="citation_title=Log-based Anomaly Detection with Deep Learning: How Far Are We?;,citation_author=Van-Hoang Le;,citation_author=Hongyu Zhang;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Reliance on metrics is a fundamental challenge for AI.;,citation_author=Rachel L Thomas;,citation_author=David Uminsky;,citation_publication_date=2022-05;,citation_cover_date=2022-05;,citation_year=2022;,citation_issue=5;,citation_volume=3;,citation_journal_title=Patterns (N Y);">
<meta name="citation_reference" content="citation_title=SMRD.de Benutzerhandbuch;,citation_author=Niyaz Valitov;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_technical_report_institution=Bundesnetzagentur für Elektrizität, Gas, Telekommunikation, Post und Eisenbahnen;">
<meta name="citation_reference" content="citation_title=Use of web scraping and text mining techniques in the istat survey on “information and communication technology in enterprises”;,citation_author=G. Barcaroli;,citation_author=A. Nurra;,citation_author=M. Scarnò;,citation_author=D. Summa;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Who makes mistakes? Using data mining techniques to analyze reporting errors in total acres operated;,citation_author=Jaki S. McCarthy;,citation_author=Morgan S. Earp;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_fulltext_html_url=https://ideas.repec.org/p/ags/unasrr/234367.html;,citation_doi=10.22004/AG.ECON.234367;,citation_journal_title=NASS Research Reports;,citation_publisher=United States Department of Agriculture, National Agricultural Statistics Service;">
<meta name="citation_reference" content="citation_title=Modeling non-response in national agricultural statistics service (NASS) surveys using classification trees;,citation_author=Jaki S Mccarthy;,citation_author=Thomas Jacob;,citation_author=Amanda Mccracken;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;">
<meta name="citation_reference" content="citation_title=2007 census of agriculture non-response methodology;,citation_author=Will Cecere;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=Exploring quarterly agricultural survey questionnaire version reduction scenarios;,citation_author=Morgan Earp;,citation_author=Scott Cox;,citation_author=Jody Mcdaniel;,citation_author=Chadd Crouse;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=USE OF MACHINE LEARNING METHODS TO IMPUTE CATEGORICAL DATA;,citation_author=P. Rey;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Innovative uses of data mining techniques in the production of official statistics;,citation_author=Jaki Mccarthy;,citation_author=Thomas Jacob;,citation_author=Dale Atkinson;">
<meta name="citation_reference" content="citation_title=Automatic coding of occupations. Using machine learning algorithms for occupation coding in several german panel surveys;,citation_fulltext_html_url=https://www.researchgate.net/publication/266259591_Automatic_Coding_of_Occupations_Using_Machine_Learning_Algorithms_for_Occupation_Coding_in_Several_German_Panel_Surveys;">
<meta name="citation_reference" content="citation_title=Evaluating hourly air quality forecasting in canada with nonlinear updatable machine learning methods;,citation_author=Huiping Peng;,citation_author=Aranildo R. Lima;,citation_author=Andrew Teakles;,citation_author=Jian Jin;,citation_author=Alex J. Cannon;,citation_author=William W. Hsieh;,citation_publication_date=2017-03-01;,citation_cover_date=2017-03-01;,citation_year=2017;,citation_fulltext_html_url=https://doi.org/10.1007/s11869-016-0414-3;,citation_issue=2;,citation_doi=10.1007/s11869-016-0414-3;,citation_issn=1873-9326;,citation_volume=10;,citation_journal_title=Air Quality, Atmosphere &amp;amp;amp; Health;">
<meta name="citation_reference" content="citation_title=Online machine learning in big data streams;,citation_author=András A. Benczúr;,citation_author=Levente Kocsis;,citation_author=Róbert Pálovics;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://arxiv.org/abs/1802.05872;,citation_volume=abs/1802.05872;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=Learn: A novel incremental learning method for text classification;,citation_author=Guangxu Shan;,citation_author=Shiyao Xu;,citation_author=Li Yang;,citation_author=Shengbin Jia;,citation_author=Yang Xiang;,citation_publication_date=2020-06;,citation_cover_date=2020-06;,citation_year=2020;,citation_doi=10.1016/J.ESWA.2020.113198;,citation_issn=0957-4174;,citation_volume=147;,citation_journal_title=Expert Systems with Applications;,citation_publisher=Pergamon;">
<meta name="citation_reference" content="citation_title=Incremental real-time learning framework for sentiment classification: Indian general election 2019, a case study;,citation_author=Sharmistha Chatterjee;,citation_author=Sushmita Gupta;,citation_publication_date=2021-03;,citation_cover_date=2021-03;,citation_year=2021;,citation_doi=10.1109/ICBDA51983.2021.9402992;,citation_isbn=9780738131672;,citation_journal_title=2021 IEEE 6th International Conference on Big Data Analytics, ICBDA 2021;,citation_publisher=Institute of Electrical; Electronics Engineers Inc.;">
<meta name="citation_reference" content="citation_title=MOA: Massive online analysis;,citation_abstract=Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Na¨ıveNa¨ıve Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis , and is released under the GNU GPL license.;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Richard Kirkby;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=11;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Evaluation and performance measurement;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Hyperparameter tuning;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Hyperparameter tuning approaches;,citation_author=Thomas Bartz-Beielstein;,citation_author=Martin Zaefferer;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_inbook_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;">
<meta name="citation_reference" content="citation_title=Drift detection and&nbsp;handling;,citation_abstract=Structural changes (“drift”) in the data cause problems for many algorithms. Based on the drift definitions given in Chap. 1, methods for drift detection and handling are discussed. For the algorithms presented in Chap. 2, it is clarified to what extent concept drift is reacted to. In turn, the extent to which catastrophic forgetting is an issue is described in Sect. 4.3. Section&nbsp;3.1 describes three architectures for implementing drift detection algorithms. Basic properties of window-based approaches are presented in Sect.&nbsp;3.2. Section&nbsp;3.4 presents commonly used drift detection techniques. Section&nbsp;3.4 describes how the drift detection techniques introduced in Sect.&nbsp;3.3 are used in Online Machine Learning (OML) algorithms and summarizes the tree-based OML techniques implemented in the River package. Section&nbsp;3.5 introduces scaling methods for handling drift.;,citation_author=Thomas Bartz-Beielstein;,citation_author=Lukas Hans;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1007/978-981-99-7007-0_3;,citation_doi=10.1007/978-981-99-7007-0_3;,citation_isbn=978-981-99-7007-0;,citation_inbook_title=Online machine learning: A practical guide with examples in python;">
<meta name="citation_reference" content="citation_title=Introduction: From batch to&nbsp;online machine learning;,citation_abstract=Batch Machine Learning (BML), which is also referred to as “offline machine learning”, reaches its limits when dealing with very large amounts of data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. In this chapter, the basic terms and concepts of OML are introduced and the differences to BML are shown.;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1007/978-981-99-7007-0_1;,citation_doi=10.1007/978-981-99-7007-0_1;,citation_isbn=978-981-99-7007-0;,citation_inbook_title=Online machine learning: A practical guide with examples in python;">
<meta name="citation_reference" content="citation_title=AMF: Aggregated Mondrian Forests for Online Learning;,citation_author=Jaouad Mourtada;,citation_author=Stephane Gaiffas;,citation_author=Erwan Scornet;,citation_publication_date=2019-06;,citation_cover_date=2019-06;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1906.10529;,citation_doi=10.48550/arXiv.1906.10529;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Extremely fast decision tree;,citation_abstract=We introduce a novel incremental decision tree learning algorithm, Hoeffding Anytime Tree, that is statistically more efficient than the current state-of-the-art, Hoeffding Tree. We demonstrate that an implementation of Hoeffding Anytime Tree’“Extremely Fast Decision Tree”, a minor modification to the MOA implementation of Hoeffding Tree’obtains significantly superior prequential accuracy on most of the largest classification datasets from the UCI repository. Hoeffding Anytime Tree produces the asymptotic batch tree in the limit, is naturally resilient to concept drift, and can be used as a higher accuracy replacement for Hoeffding Tree in most scenarios, at a small additional computational cost.;,citation_author=Chaitanya Manapragada;,citation_author=Geoffrey I. Webb;,citation_author=Mahsa Salehi;,citation_editor=Chih-Jen  Lin;,citation_editor=Hui  Xiong;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://www.kdd.org/kdd2018/, https://dl.acm.org/doi/proceedings/10.1145/3219819;,citation_doi=10.1145/3219819.3220005;,citation_conference_title=KDD’ 2018 - proceedings of the 24th ACM SIGKDD international conference on knowledge discovery and data mining;,citation_conference=Association for Computing Machinery (ACM);">
<meta name="citation_reference" content="citation_title=Surrogates;,citation_author=Robert B Gramacy;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=UvA deep learning tutorials;,citation_author=Phillip Lippe;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Attention Is All You Need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017-06;,citation_cover_date=2017-06;,citation_year=2017;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=On the Variance of the Adaptive Learning Rate and Beyond;,citation_author=Liyuan Liu;,citation_author=Haoming Jiang;,citation_author=Pengcheng He;,citation_author=Weizhu Chen;,citation_author=Xiaodong Liu;,citation_author=Jianfeng Gao;,citation_author=Jiawei Han;,citation_publication_date=2019-08;,citation_cover_date=2019-08;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Efficient Transformers: A Survey;,citation_author=Yi Tay;,citation_author=Mostafa Dehghani;,citation_author=Dara Bahri;,citation_author=Donald Metzler;,citation_publication_date=2020-09;,citation_cover_date=2020-09;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding;,citation_author=Jacob Devlin;,citation_author=Ming-Wei Chang;,citation_author=Kenton Lee;,citation_author=Kristina Toutanova;,citation_publication_date=2018-10;,citation_cover_date=2018-10;,citation_year=2018;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale;,citation_author=Alexey Dosovitskiy;,citation_author=Lucas Beyer;,citation_author=Alexander Kolesnikov;,citation_author=Dirk Weissenborn;,citation_author=Xiaohua Zhai;,citation_author=Thomas Unterthiner;,citation_author=Mostafa Dehghani;,citation_author=Matthias Minderer;,citation_author=Georg Heigold;,citation_author=Sylvain Gelly;,citation_author=Jakob Uszkoreit;,citation_author=Neil Houlsby;,citation_publication_date=2020-10;,citation_cover_date=2020-10;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Attention is not Explanation;,citation_author=Sarthak Jain;,citation_author=Byron C. Wallace;,citation_publication_date=2019-02;,citation_cover_date=2019-02;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Attention is not not Explanation;,citation_author=Sarah Wiegreffe;,citation_author=Yuval Pinter;,citation_publication_date=2019-08;,citation_cover_date=2019-08;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Multivariate adaptive regression splines;,citation_author=Jerome H. Friedman;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_issue=1;,citation_volume=19;,citation_journal_title=The annals of statistics;">
<meta name="citation_reference" content="citation_title=Learning model trees from evolving data streams;,citation_author=Elena Ikonomovska;,citation_author=João Gama;,citation_author=Sašo Džeroski;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=1;,citation_volume=23;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=An Introduction to Statistical Learning with Applications in R;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Deep learning with python;,citation_author=Francoise Chollet;,citation_author=J. J. Allaire;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=A survey of cross-validation procedures for model selection;,citation_author=Sylvain Arlot;,citation_author=Alain Celisse;,citation_author=others;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=4;,citation_journal_title=Statistics surveys;">
<meta name="citation_reference" content="citation_title=A cross-validatory method for dependent data;,citation_author=PRABIR BURMAN;,citation_author=EDMOND CHOW;,citation_author=DEBORAH NOLAN;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=2;,citation_volume=81;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=A study of cross-validation and bootstrap for accuracy estimation and model selection;,citation_author=Ron Kohavi;,citation_publication_date=1995;,citation_cover_date=1995;,citation_year=1995;,citation_conference_title=Proceedings of the 14th international joint conference on artificial intelligence - volume 2;,citation_conference=Morgan Kaufmann Publishers Inc.;,citation_series_title=IJCAI’95;">
<meta name="citation_reference" content="citation_title=Kriging-based sequential design strategies using fast cross-validation techniques with extensions to multi-fidelity computer codes ;,citation_author=Loic Le Gratiet;,citation_author=Claire Cannamela;,citation_publication_date=2012-10;,citation_cover_date=2012-10;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Cross-validation of regression models;,citation_author=Richard R. Picard;,citation_author=R. Dennis Cook;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_issue=387;,citation_volume=79;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=The elements of statistical learning;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_author=Jerome Friedman;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./005_num_rsm.html">Numerical Methods</a></li><li class="breadcrumb-item"><a href="./006_num_gp.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hyperparameter Tuning Cookbook</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sequential-parameter-optimization/spotpython" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Hyperparameter-Tuning-Cookbook.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_optimization_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002_awwe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Aircraft Wing Weight Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003_scipy_optimize_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to <code>scipy.optimize</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004_spot_sklearn_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Using <code>scipy</code> Optimizers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Numerical Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_num_rsm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction: Numerical Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_gp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_num_spot_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to spotpython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008_num_spot_multidim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multi-dimensional Functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009_num_spot_anisotropic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Isotropic and Anisotropic Kriging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010_num_spot_sklearn_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Using <code>sklearn</code> Surrogates in <code>spotpython</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_num_spot_sklearn_gaussian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Gaussian Process Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_num_spot_ei.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Expected Improvement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_num_spot_noisy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Handling Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014_num_spot_ocba.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Optimal Computational Budget Allocation in <code>Spot</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_num_spot_correlation_p.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Kriging with Varying Correlation-p</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Data-Driven Modeling and Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_ddmo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Data-Driven Modeling and Optimization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning and AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./200_mlai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Machine Learning and Artificial Intelligence</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Hyperparameter Tuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./300_hpt_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with Sklearn</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./400_spot_hpt_sklearn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">HPT: sklearn</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./401_spot_hpt_sklearn_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">HPT: sklearn SVC on Moons Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with River</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./500_spot_hpt_river.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">HPT: River</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./501_spot_river_gui.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Simplifying Hyperparameter Tuning in Online Machine Learning—The spotRiverGUI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./502_spot_hpt_river_friedman_htr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title"><code>river</code> Hyperparameter Tuning: Hoeffding Tree Regressor with Friedman Drift Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./503_spot_hpt_river_friedman_amfr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">The Friedman Drift Data Set</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with PyTorch Lightning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./600_spot_lightning_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_diabetes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_user_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning and User Data Sets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_user_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning and User Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_pinn_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning: Physics Informed Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./602_spot_lightning_xai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./603_spot_lightning_transformer_introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning Transformer: Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./603_spot_lightning_transformer_hpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning of a Transformer Network with PyTorch Lightning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./604_spot_lightning_save_load_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Saving and Loading</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./605_spot_hpt_light_diabetes_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set Using a ResNet Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./606_spot_hpt_light_diabetes_user_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set Using a User Specified ResNet Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./608_spot_hpt_light_condnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning Using a CondNet Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_01_intro_to_notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Introduction to Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_02_git_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Git Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_03_python_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Python Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_04_spot_doc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Documentation of the Sequential Parameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_05_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_06_slurm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Using Slurm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_99_solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Solutions to Selected Exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#dace-and-rsm" id="toc-dace-and-rsm" class="nav-link active" data-scroll-target="#dace-and-rsm"><span class="header-section-number">6.1</span> DACE and RSM</a>
  <ul class="collapse">
  <li><a href="#noise-handling-in-rsm-and-dace" id="toc-noise-handling-in-rsm-and-dace" class="nav-link" data-scroll-target="#noise-handling-in-rsm-and-dace"><span class="header-section-number">6.1.1</span> Noise Handling in RSM and DACE</a></li>
  </ul></li>
  <li><a href="#background-expectation-mean-standard-deviation" id="toc-background-expectation-mean-standard-deviation" class="nav-link" data-scroll-target="#background-expectation-mean-standard-deviation"><span class="header-section-number">6.2</span> Background: Expectation, Mean, Standard Deviation</a>
  <ul class="collapse">
  <li><a href="#calculation-of-the-standard-deviation-with-python" id="toc-calculation-of-the-standard-deviation-with-python" class="nav-link" data-scroll-target="#calculation-of-the-standard-deviation-with-python"><span class="header-section-number">6.2.1</span> Calculation of the Standard Deviation with Python</a></li>
  <li><a href="#the-argument-axis" id="toc-the-argument-axis" class="nav-link" data-scroll-target="#the-argument-axis"><span class="header-section-number">6.2.2</span> The Argument “axis”</a></li>
  </ul></li>
  <li><a href="#data-types-and-precision-in-python" id="toc-data-types-and-precision-in-python" class="nav-link" data-scroll-target="#data-types-and-precision-in-python"><span class="header-section-number">6.3</span> Data Types and Precision in Python</a></li>
  <li><a href="#distributions-and-random-numbers-in-python" id="toc-distributions-and-random-numbers-in-python" class="nav-link" data-scroll-target="#distributions-and-random-numbers-in-python"><span class="header-section-number">6.4</span> Distributions and Random Numbers in Python</a>
  <ul class="collapse">
  <li><a href="#the-uniform-distribution" id="toc-the-uniform-distribution" class="nav-link" data-scroll-target="#the-uniform-distribution"><span class="header-section-number">6.4.1</span> The Uniform Distribution</a></li>
  <li><a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution"><span class="header-section-number">6.4.2</span> The Normal Distribution</a></li>
  <li><a href="#visualization-of-the-standard-deviation" id="toc-visualization-of-the-standard-deviation" class="nav-link" data-scroll-target="#visualization-of-the-standard-deviation"><span class="header-section-number">6.4.3</span> Visualization of the Standard Deviation</a></li>
  <li><a href="#standardization-of-random-variables" id="toc-standardization-of-random-variables" class="nav-link" data-scroll-target="#standardization-of-random-variables"><span class="header-section-number">6.4.4</span> Standardization of Random Variables</a></li>
  <li><a href="#realizations-of-a-normal-distribution" id="toc-realizations-of-a-normal-distribution" class="nav-link" data-scroll-target="#realizations-of-a-normal-distribution"><span class="header-section-number">6.4.5</span> Realizations of a Normal Distribution</a></li>
  <li><a href="#the-multivariate-normal-distribution" id="toc-the-multivariate-normal-distribution" class="nav-link" data-scroll-target="#the-multivariate-normal-distribution"><span class="header-section-number">6.4.6</span> The Multivariate Normal Distribution</a></li>
  </ul></li>
  <li><a href="#covariance" id="toc-covariance" class="nav-link" data-scroll-target="#covariance"><span class="header-section-number">6.5</span> Covariance</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation"><span class="header-section-number">6.6</span> Correlation</a>
  <ul class="collapse">
  <li><a href="#r-squared-in-simple-linear-regression" id="toc-r-squared-in-simple-linear-regression" class="nav-link" data-scroll-target="#r-squared-in-simple-linear-regression"><span class="header-section-number">6.6.1</span> R-Squared in Simple Linear Regression</a></li>
  </ul></li>
  <li><a href="#cholesky-decomposition-and-positive-definite-matrices" id="toc-cholesky-decomposition-and-positive-definite-matrices" class="nav-link" data-scroll-target="#cholesky-decomposition-and-positive-definite-matrices"><span class="header-section-number">6.7</span> Cholesky Decomposition and Positive Definite Matrices</a></li>
  <li><a href="#maximum-likelihood-estimation-multivariate-normal-distribution" id="toc-maximum-likelihood-estimation-multivariate-normal-distribution" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-multivariate-normal-distribution"><span class="header-section-number">6.8</span> Maximum Likelihood Estimation: Multivariate Normal Distribution</a>
  <ul class="collapse">
  <li><a href="#the-joint-probability-density-function-of-the-multivariate-normal-distribution" id="toc-the-joint-probability-density-function-of-the-multivariate-normal-distribution" class="nav-link" data-scroll-target="#the-joint-probability-density-function-of-the-multivariate-normal-distribution"><span class="header-section-number">6.8.1</span> The Joint Probability Density Function of the Multivariate Normal Distribution</a></li>
  <li><a href="#the-log-likelihood-function" id="toc-the-log-likelihood-function" class="nav-link" data-scroll-target="#the-log-likelihood-function"><span class="header-section-number">6.8.2</span> The Log-Likelihood Function</a></li>
  </ul></li>
  <li><a href="#constructing-a-surrogate" id="toc-constructing-a-surrogate" class="nav-link" data-scroll-target="#constructing-a-surrogate"><span class="header-section-number">6.9</span> Constructing a Surrogate</a>
  <ul class="collapse">
  <li><a href="#stage-one-preparing-the-data-and-choosing-a-modelling-approach" id="toc-stage-one-preparing-the-data-and-choosing-a-modelling-approach" class="nav-link" data-scroll-target="#stage-one-preparing-the-data-and-choosing-a-modelling-approach"><span class="header-section-number">6.9.1</span> Stage One: Preparing the Data and Choosing a Modelling Approach</a></li>
  <li><a href="#sec-stage-two" id="toc-sec-stage-two" class="nav-link" data-scroll-target="#sec-stage-two"><span class="header-section-number">6.9.2</span> Stage Two: Parameter Estimation and Training</a></li>
  <li><a href="#stage-three-model-testing" id="toc-stage-three-model-testing" class="nav-link" data-scroll-target="#stage-three-model-testing"><span class="header-section-number">6.9.3</span> Stage Three: Model Testing</a></li>
  </ul></li>
  <li><a href="#sampling-plans" id="toc-sampling-plans" class="nav-link" data-scroll-target="#sampling-plans"><span class="header-section-number">6.10</span> Sampling Plans</a></li>
  <li><a href="#kriging" id="toc-kriging" class="nav-link" data-scroll-target="#kriging"><span class="header-section-number">6.11</span> Kriging</a>
  <ul class="collapse">
  <li><a href="#the-kriging-idea-in-a-nutshell" id="toc-the-kriging-idea-in-a-nutshell" class="nav-link" data-scroll-target="#the-kriging-idea-in-a-nutshell"><span class="header-section-number">6.11.1</span> The Kriging Idea in a Nutshell</a></li>
  <li><a href="#the-kriging-model" id="toc-the-kriging-model" class="nav-link" data-scroll-target="#the-kriging-model"><span class="header-section-number">6.11.2</span> The Kriging Model</a></li>
  <li><a href="#the-condition-number" id="toc-the-condition-number" class="nav-link" data-scroll-target="#the-condition-number"><span class="header-section-number">6.11.3</span> The Condition Number</a></li>
  <li><a href="#mle-to-estimate-theta-and-p" id="toc-mle-to-estimate-theta-and-p" class="nav-link" data-scroll-target="#mle-to-estimate-theta-and-p"><span class="header-section-number">6.11.4</span> MLE to estimate <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></a></li>
  <li><a href="#implementing-an-mle-of-the-model-parameters" id="toc-implementing-an-mle-of-the-model-parameters" class="nav-link" data-scroll-target="#implementing-an-mle-of-the-model-parameters"><span class="header-section-number">6.11.5</span> Implementing an MLE of the Model Parameters</a></li>
  <li><a href="#kriging-prediction" id="toc-kriging-prediction" class="nav-link" data-scroll-target="#kriging-prediction"><span class="header-section-number">6.11.6</span> Kriging Prediction</a></li>
  </ul></li>
  <li><a href="#kriging-example-sinusoid-function" id="toc-kriging-example-sinusoid-function" class="nav-link" data-scroll-target="#kriging-example-sinusoid-function"><span class="header-section-number">6.12</span> Kriging Example: Sinusoid Function</a>
  <ul class="collapse">
  <li><a href="#calculating-the-correlation-matrix-psi" id="toc-calculating-the-correlation-matrix-psi" class="nav-link" data-scroll-target="#calculating-the-correlation-matrix-psi"><span class="header-section-number">6.12.1</span> Calculating the Correlation Matrix <span class="math inline">\(\Psi\)</span></a></li>
  <li><a href="#computing-the-psi-vector" id="toc-computing-the-psi-vector" class="nav-link" data-scroll-target="#computing-the-psi-vector"><span class="header-section-number">6.12.2</span> Computing the <span class="math inline">\(\psi\)</span> Vector</a></li>
  <li><a href="#predicting-at-new-locations" id="toc-predicting-at-new-locations" class="nav-link" data-scroll-target="#predicting-at-new-locations"><span class="header-section-number">6.12.3</span> Predicting at New Locations</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization"><span class="header-section-number">6.12.4</span> Visualization</a></li>
  </ul></li>
  <li><a href="#cholesky-decomposition-2" id="toc-cholesky-decomposition-2" class="nav-link" data-scroll-target="#cholesky-decomposition-2"><span class="header-section-number">6.13</span> Cholesky Decomposition</a>
  <ul class="collapse">
  <li><a href="#example-of-cholesky-decomposition" id="toc-example-of-cholesky-decomposition" class="nav-link" data-scroll-target="#example-of-cholesky-decomposition"><span class="header-section-number">6.13.1</span> Example of Cholesky Decomposition</a></li>
  <li><a href="#inverse-matrix-using-cholesky-decomposition" id="toc-inverse-matrix-using-cholesky-decomposition" class="nav-link" data-scroll-target="#inverse-matrix-using-cholesky-decomposition"><span class="header-section-number">6.13.2</span> Inverse Matrix Using Cholesky Decomposition</a></li>
  </ul></li>
  <li><a href="#gaussian-processessome-background-information" id="toc-gaussian-processessome-background-information" class="nav-link" data-scroll-target="#gaussian-processessome-background-information"><span class="header-section-number">6.14</span> Gaussian Processes—Some Background Information</a>
  <ul class="collapse">
  <li><a href="#gaussian-process-prior" id="toc-gaussian-process-prior" class="nav-link" data-scroll-target="#gaussian-process-prior"><span class="header-section-number">6.14.1</span> Gaussian Process Prior</a></li>
  <li><a href="#covariance-function" id="toc-covariance-function" class="nav-link" data-scroll-target="#covariance-function"><span class="header-section-number">6.14.2</span> Covariance Function</a></li>
  <li><a href="#construction-of-the-covariance-matrix" id="toc-construction-of-the-covariance-matrix" class="nav-link" data-scroll-target="#construction-of-the-covariance-matrix"><span class="header-section-number">6.14.3</span> Construction of the Covariance Matrix</a></li>
  <li><a href="#generation-of-random-samples-and-plotting-the-realizations-of-the-random-function" id="toc-generation-of-random-samples-and-plotting-the-realizations-of-the-random-function" class="nav-link" data-scroll-target="#generation-of-random-samples-and-plotting-the-realizations-of-the-random-function"><span class="header-section-number">6.14.4</span> Generation of Random Samples and Plotting the Realizations of the Random Function</a></li>
  <li><a href="#properties-of-the-1d-example" id="toc-properties-of-the-1d-example" class="nav-link" data-scroll-target="#properties-of-the-1d-example"><span class="header-section-number">6.14.5</span> Properties of the 1d Example</a></li>
  </ul></li>
  <li><a href="#jupyter-notebook" id="toc-jupyter-notebook" class="nav-link" data-scroll-target="#jupyter-notebook"><span class="header-section-number">6.15</span> Jupyter Notebook</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./005_num_rsm.html">Numerical Methods</a></li><li class="breadcrumb-item"><a href="./006_num_gp.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- bart21mSlides2022Lec-05 -->
<section id="dace-and-rsm" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="dace-and-rsm"><span class="header-section-number">6.1</span> DACE and RSM</h2>
<p>Mathematical models implemented in computer codes are used to circumvent the need for expensive field data collection. These models are particularly useful when dealing with highly nonlinear response surfaces, high signal-to-noise ratios (which often involve deterministic evaluations), and a global scope. As a result, a new approach is required in comparison to Response Surface Methodology (RSM), which was discussed in <a href="005_num_rsm.html#sec-rsm-intro" class="quarto-xref"><span>Section 5.1</span></a>.</p>
<p>With the improvement in computing power and simulation fidelity, researchers gain higher confidence and a better understanding of the dynamics in physical, biological, and social systems. However, the expansion of configuration spaces and increasing input dimensions necessitates more extensive designs. High-performance computing (HPC) allows for thousands of runs, whereas previously only tens were possible. This shift towards larger models and training data presents new computational challenges.</p>
<p>Research questions for DACE (Design and Analysis of Computer Experiments) include how to design computer experiments that make efficient use of computation and how to meta-model computer codes to save on simulation effort. The choice of surrogate model for computer codes significantly impacts the optimal experiment design, and the preferred model-design pairs can vary depending on the specific goal.</p>
<p>The combination of computer simulation, design, and modeling with field data from similar real-world experiments introduces a new category of computer model tuning problems. The ultimate goal is to automate these processes to the greatest extent possible, allowing for the deployment of HPC with minimal human intervention.</p>
<p>One of the remaining differences between RSM and DACE lies in how they handle noise. DACE employs replication, a technique that would not be used in a deterministic setting, to separate signal from noise. Traditional RSM is best suited for situations where a substantial proportion of the variability in the data is due to noise, and where the acquisition of data values can be severely limited. Consequently, RSM is better suited for a different class of problems, aligning with its intended purposes.</p>
<p>Two very good texts on computer experiments and surrogate modeling are <span class="citation" data-cites="Sant03a">Santner, Williams, and Notz (<a href="references.html#ref-Sant03a" role="doc-biblioref">2003</a>)</span> and <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>. The former is the canonical reference in the statistics literature and the latter is perhaps more popular in engineering.</p>
<div id="exm-dace-rsm" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.1 (Example: DACE and RSM)</strong></span> Imagine you are a chemical engineer tasked with optimizing a chemical process to maximize yield. You can control temperature and pressure, but repeated experiments show variability in yield due to inconsistencies in raw materials.</p>
<ul>
<li><p>Using RSM: You would use RSM to design a series of experiments varying temperature and pressure. You would then fit a response surface (a mathematical model) to the data, helping you understand how changes in temperature and pressure affect yield. Using this model, you can identify optimal conditions for maximizing yield despite the noise.</p></li>
<li><p>Using DACE: If instead you use a computational model to simulate the chemical process and want to account for numerical noise or uncertainty in model parameters, you might use DACE. You would run simulations at different conditions, possibly repeating them to assess variability and build a surrogate model that accurately predicts yields, which can be optimized to find the best conditions.</p></li>
</ul>
</div>
<section id="noise-handling-in-rsm-and-dace" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="noise-handling-in-rsm-and-dace"><span class="header-section-number">6.1.1</span> Noise Handling in RSM and DACE</h3>
<p>Noise in RSM: In experimental settings, noise often arises due to variability in experimental conditions, measurement errors, or other uncontrollable factors. This noise can significantly affect the response variable, <span class="math inline">\(Y\)</span>. Replication is a standard procedure for handling noise in RSM. In the context of computer experiments, noise might not be present in the traditional sense since simulations can be deterministic. However, variability can arise from uncertainty in input parameters or model inaccuracies. DACE predominantly utilizes advanced interpolation to construct accurate models of deterministic data, sometimes considering statistical noise modeling if needed.</p>
</section>
</section>
<section id="background-expectation-mean-standard-deviation" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="background-expectation-mean-standard-deviation"><span class="header-section-number">6.2</span> Background: Expectation, Mean, Standard Deviation</h2>
<p>The distribution of a random vector is characterized by some indexes. These are the expectation, the mean, and the standard deviation. The expectation is a measure of the central tendency of a random variable, while the standard deviation quantifies the spread of the distribution. These indexes are essential for understanding the behavior of random variables and making predictions based on them.</p>
<div id="def-random-variable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1 (Random Variable)</strong></span> A random variable <span class="math inline">\(X\)</span> is a mapping from the sample space of a random experiment to the real numbers. It assigns a numerical value to each outcome of the experiment. Random variables can be either:</p>
<ul>
<li>Discrete: If <span class="math inline">\(X\)</span> takes on a countable number of distinct values.</li>
<li>Continuous: If <span class="math inline">\(X\)</span> takes on an uncountable number of values.</li>
</ul>
<p>Mathematically, a random variable is a function <span class="math inline">\(X: \Omega \rightarrow \mathbb{R}\)</span>, where <span class="math inline">\(\Omega\)</span> is the sample space.</p>
</div>
<div id="def-probability-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.2 (Probability Distribution)</strong></span> A probability distribution describes how the values of a random variable are distributed. It is characterized for a discrete random variable <span class="math inline">\(X\)</span> by the probability mass function (PMF) <span class="math inline">\(p_X(x)\)</span> and for a continuous random variable <span class="math inline">\(X\)</span> by the probability density function (PDF) <span class="math inline">\(f_X(x)\)</span>.</p>
</div>
<div id="def-probability-mass-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3 (Probability Mass Function (PMF))</strong></span> <span class="math inline">\(p_X(x) = P(X = x)\)</span> gives the probability that <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(x\)</span>.</p>
</div>
<div id="def-probability-density-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.4 (Probability Density Function (PDF):)</strong></span> <span class="math inline">\(f_X(x)\)</span> is a function such that for any interval <span class="math inline">\([a, b]\)</span>, the probability that <span class="math inline">\(X\)</span> falls within this interval is given by the integral <span class="math inline">\(\int_a^b f_X(x) \mathrm{d}x\)</span>.</p>
</div>
<p>The distribution function must satisfy: <span class="math display">\[
\sum_{x \in D_X} p_X(x) = 1
\]</span> for discrete random variables, where <span class="math inline">\(D_X\)</span> is the domain of <span class="math inline">\(X\)</span> and <span class="math display">\[
\int_{-\infty}^{\infty} f_X(x) \mathrm{d}x = 1
\]</span> for continuous random variables.</p>
<p>With these definitions in place, we can now introduce the definition of the expectation, which is a fundamental measure of the central tendency of a random variable.</p>
<div id="def-expectation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.5 (Expectation)</strong></span> The expectation or expected value of a random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(E[X]\)</span>, is defined as follows:</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>: <span class="math display">\[
E[X] = \sum_{x \in D_X} x p_X(x) \quad \text{if $X$ is discrete}.
\]</span></p>
<p>For a continuous random variable <span class="math inline">\(X\)</span>: <span class="math display">\[
E[X] = \int_{x \in D_X} x f_X(x) \mathrm{d}x \quad \text{if $X$ is continuous.}
\]</span></p>
</div>
<p>The mean, <span class="math inline">\(\mu\)</span>, of a probability distribution is a measure of its central tendency or location. That is, <span class="math inline">\(E(X)\)</span> is defined as the average of all possible values of <span class="math inline">\(X\)</span>, weighted by their probabilities.</p>
<div id="exm-expectation" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.2 (Expectation)</strong></span> Let <span class="math inline">\(X\)</span> denote the number produced by rolling a fair die. Then <span class="math display">\[
E(X) = 1 \times 1/6 + 2 \times 1/6 + 3 \times 1/6 + 4 \times 1/6 + 5 \times 1/6 + 6\times 1/6 = 3.5.
\]</span></p>
</div>
<div id="def-sample-mean" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.6 (Sample Mean)</strong></span> The sample mean is an important estimate of the population mean. The sample mean of a sample <span class="math inline">\(\{x_i\}\)</span> (<span class="math inline">\(i=1,2,\ldots,n\)</span>) is defined as <span class="math display">\[
\overline{x}  = \frac{1}{n} \sum_i x_i.
\]</span></p>
</div>
<p>While both the expectation of a random variable and the sample mean provide measures of central tendency, they differ in their context, calculation, and interpretation.</p>
<ul>
<li>The expectation is a theoretical measure that characterizes the average value of a random variable over an infinite number of repetitions of an experiment. The expectation is calculated using a probability distribution and provides a parameter of the entire population or distribution. It reflects the long-term average or central value of the outcomes generated by the random process.</li>
<li>The sample mean is a statistic. It provides an estimate of the population mean based on a finite sample of data. It is computed directly from the data sample, and its value can vary between different samples from the same population. It serves as an approximation or estimate of the population mean. It is used in statistical inference to make conclusions about the population mean based on sample data.</li>
</ul>
<p>If we are trying to predict the value of a random variable <span class="math inline">\(X\)</span> by its mean <span class="math inline">\(\mu = E(X)\)</span>, the error will be <span class="math inline">\(X-\mu\)</span>. In many situations it is useful to have an idea how large this deviation or error is. Since <span class="math inline">\(E(X-\mu) = E(X) -\mu = 0\)</span>, it is necessary to use the absolute value or the square of (<span class="math inline">\(X-\mu\)</span>). The squared error is the first choice, because the derivatives are easier to calculate. These considerations motivate the definition of the variance:</p>
<div id="def-variance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.7 (Variance)</strong></span> The variance of a random variable <span class="math inline">\(X\)</span> is the mean squared deviation of <span class="math inline">\(X\)</span> from its expected value <span class="math inline">\(\mu = E(X)\)</span>. <span class="math display">\[\begin{equation}
Var(X) = E[ (X-\mu)^2].
\end{equation}\]</span></p>
</div>
<p>The variance is a measure of the spread of a distribution. It quantifies how much the values of a random variable differ from the mean. A high variance indicates that the values are spread out over a wide range, while a low variance indicates that the values are clustered closely around the mean.</p>
<div id="def-standard-deviation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.8 (Standard Deviation)</strong></span> Taking the square root of the variance to get back to the same scale of units as <span class="math inline">\(X\)</span> gives the standard deviation. The standard deviation of <span class="math inline">\(X\)</span> is the square root of the variance of <span class="math inline">\(X\)</span>. <span class="math display">\[\begin{equation}
sd(X) = \sqrt{Var(X)}.
\end{equation}\]</span></p>
</div>
<section id="calculation-of-the-standard-deviation-with-python" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="calculation-of-the-standard-deviation-with-python"><span class="header-section-number">6.2.1</span> Calculation of the Standard Deviation with Python</h3>
<p>The function <code>numpy.std</code> returns the standard deviation, a measure of the spread of a distribution, of the array elements. The argument <code>ddof</code> specifies the Delta Degrees of Freedom. The divisor used in calculations is <code>N - ddof</code>, where <code>N</code> represents the number of elements. By default <code>ddof</code> is zero, i.e., <code>std</code> uses the formula <span class="math display">\[
\sqrt{  \frac{1}{N} \sum_i \left( x_i - \bar{x} \right)^2  } \qquad \text{with } \quad \bar{x} = \sum_{i=1}^N x_i /N.
\]</span></p>
<div id="exm-std-python" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.3 (Standard Deviation with Python)</strong></span> Consider the array <span class="math inline">\([1,2,3]\)</span>: Since <span class="math inline">\(\bar{x} = 2\)</span>, the following value is computed: <span class="math display">\[ \sqrt{1/3 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/3}.\]</span></p>
<div id="1fb4e78e" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]])</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>np.std(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>0.816496580927726</code></pre>
</div>
</div>
</div>
<p>The empirical standard deviation (which uses <span class="math inline">\(N-1\)</span>), <span class="math inline">\(\sqrt{1/2 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/2}\)</span>, can be calculated in Python as follows:</p>
<div id="58902bf2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>np.std(a, ddof<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>1.0</code></pre>
</div>
</div>
</section>
<section id="the-argument-axis" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="the-argument-axis"><span class="header-section-number">6.2.2</span> The Argument “axis”</h3>
<p>When you compute <code>np.std</code> with <code>axis=0</code>, it calculates the standard deviation along the vertical axis, meaning it computes the standard deviation for each column of the array. On the other hand, when you compute <code>np.std</code> with <code>axis=1</code>, it calculates the standard deviation along the horizontal axis, meaning it computes the standard deviation for each row of the array. If the axis parameter is not specified, <code>np.std</code> computes the standard deviation of the flattened array, i.e., it calculates the standard deviation of all the elements in the array.</p>
<div id="exm-std-axis" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.4 (Axes along which the standard deviation is computed)</strong></span> &nbsp;</p>
<div id="7876f069" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>array([[1, 2],
       [3, 4]])</code></pre>
</div>
</div>
<p>First, we calculate the standard deviation of all elements in the array:</p>
<div id="80e80a4f" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>np.std(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>1.118033988749895</code></pre>
</div>
</div>
<p>Setting <code>axis=0</code> calculates the standard deviation along the vertical axis (column-wise):</p>
<div id="5fd68544" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>np.std(A, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([1., 1.])</code></pre>
</div>
</div>
<p>Finally, setting <code>axis=1</code> calculates the standard deviation along the horizontal axis (row-wise):</p>
<div id="4e3a5312" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>np.std(A, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([0.5, 0.5])</code></pre>
</div>
</div>
</div>
</section>
</section>
<section id="data-types-and-precision-in-python" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="data-types-and-precision-in-python"><span class="header-section-number">6.3</span> Data Types and Precision in Python</h2>
<p>The float16 data type in numpy represents a half-precision floating point number. It uses 16 bits of memory, which gives it a precision of about 3 decimal digits.</p>
<p>The float32 data type in numpy represents a single-precision floating point number. It uses 32 bits of memory, which gives it a precision of about 7 decimal digits. On the other hand, float64 represents a double-precision floating point number. It uses 64 bits of memory, which gives it a precision of about 15 decimal digits.</p>
<p>The reason float16 and float32 show fewer digits is because it has less precision due to using less memory. The bits of memory are used to store the sign, exponent, and fraction parts of the floating point number, and with fewer bits, you can represent fewer digits accurately.</p>
<div id="exm-float" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.5 (16 versus 32 versus 64 bit)</strong></span> &nbsp;</p>
<div id="747da7ac" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a number</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> <span class="fl">0.123456789123456789</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>num_float16 <span class="op">=</span> np.float16(num)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>num_float32 <span class="op">=</span> np.float32(num)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>num_float64 <span class="op">=</span> np.float64(num)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"float16: "</span>, num_float16) </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"float32: "</span>, num_float32)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"float64: "</span>, num_float64)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>float16:  0.1235
float32:  0.12345679
float64:  0.12345678912345678</code></pre>
</div>
</div>
</div>
</section>
<section id="distributions-and-random-numbers-in-python" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="distributions-and-random-numbers-in-python"><span class="header-section-number">6.4</span> Distributions and Random Numbers in Python</h2>
<p>Results from computers are deterministic, so it sounds like a contradiction in terms to generate random numbers on a computer. Standard computers generate pseudo-randomnumbers, i.e., numbers that behave as if they were drawn randomly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deterministic Random Numbers
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Idea: Generate deterministically numbers that <strong>look</strong> (behave) as if they were drawn randomly.</li>
</ul>
</div>
</div>
<section id="the-uniform-distribution" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="the-uniform-distribution"><span class="header-section-number">6.4.1</span> The Uniform Distribution</h3>
<div id="def-uniform-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.9 (The Uniform Distribution)</strong></span> The probability density function of the uniform distribution is defined as: <span class="math display">\[
f_X(x) = \frac{1}{b-a} \qquad \text{for $x \in [a,b]$}.
\]</span></p>
</div>
<p>Generate 10 random numbers from a uniform distribution between <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=1\)</span>:</p>
<div id="e681ab17" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the random number generator</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">123456789</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.uniform(low<span class="op">=</span><span class="fl">0.0</span>, high<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>n)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([0.02771274, 0.90670006, 0.88139355, 0.62489728, 0.79071481,
       0.82590801, 0.84170584, 0.47172795, 0.95722878, 0.94659153])</code></pre>
</div>
</div>
<p>Generate 10,000 random numbers from a uniform distribution between 0 and 10 and plot a histogram of the numbers:</p>
<div id="98fe0141" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the random number generator</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">123456789</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers from a uniform distribution</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.uniform(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a histogram of the numbers</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>plt.hist(x, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Uniform Distribution [0,10]'</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-10-output-1.png" width="597" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-normal-distribution" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="the-normal-distribution"><span class="header-section-number">6.4.2</span> The Normal Distribution</h3>
<p>A normally distributed random variable is a random variable whose associated probability distribution is the normal (or Gaussian) distribution. The normal distribution is a continuous probability distribution characterized by a symmetric bell-shaped curve.</p>
<p>The distribution is defined by two parameters: the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. The mean indicates the center of the distribution, while the standard deviation measures the spread or dispersion of the distribution.</p>
<p>This distribution is widely used in statistics and the natural and social sciences as a simple model for random variables with unknown distributions.</p>
<div id="def-normal-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.10 (The Normal Distribution)</strong></span> The probability density function of the normal distribution is defined as: <span id="eq-normal-one"><span class="math display">\[
f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2\right),
\tag{6.1}\]</span></span> where: <span class="math inline">\(\mu\)</span> is the mean; <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
</div>
<p>To generate ten random numbers from a normal distribution, the following command can be used.</p>
<div id="cell-gen-normal-10" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">2</span>, <span class="fl">0.1</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.normal(mu, sigma, n)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="gen-normal-10" class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([2.06408928, 1.93760305, 1.91661533, 1.90237881, 1.94626796,
       2.12266017, 1.91432453, 1.73431068, 1.95289692, 1.93974663])</code></pre>
</div>
</div>
<p>Verify the mean:</p>
<div id="c5ad612f" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(mu <span class="op">-</span> np.mean(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>0.056910663281092067</code></pre>
</div>
</div>
<p>Note: To verify the standard deviation, we use <code>ddof = 1</code> (empirical standard deviation):</p>
<div id="5a24be7c" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(sigma <span class="op">-</span> np.std(x, ddof<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>0.0020938554077270105</code></pre>
</div>
</div>
<div id="8cde083b" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plot_normal_distribution(mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">1</span>, num_samples<span class="op">=</span><span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-15-output-1.png" width="579" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualization-of-the-standard-deviation" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="visualization-of-the-standard-deviation"><span class="header-section-number">6.4.3</span> Visualization of the Standard Deviation</h3>
<p>The standard deviation of normal distributed can be visualized in terms of the histogram of <span class="math inline">\(X\)</span>:</p>
<ul>
<li>about 68% of the values will lie in the interval within one standard deviation of the mean</li>
<li>95% lie within two standard deviation of the mean</li>
<li>and 99.9% lie within 3 standard deviations of the mean.</li>
</ul>
<div id="21a3c8c6" class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-16-output-1.png" width="662" height="469" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="standardization-of-random-variables" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="standardization-of-random-variables"><span class="header-section-number">6.4.4</span> Standardization of Random Variables</h3>
<p>To compare statistical properties of random variables which use different units, it is a common practice to transform these random variables into standardized variables.</p>
<div id="def-standard-units" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.11 (Standard Units)</strong></span> If a random variable <span class="math inline">\(X\)</span> has expectation <span class="math inline">\(E(X) = \mu\)</span> and standard deviation <span class="math inline">\(sd(X) = \sigma &gt;0\)</span>, the random variable <span class="math display">\[
X^{\ast} = (X-\mu)/\sigma
\]</span> is called <span class="math inline">\(X\)</span> in standard units. It has <span class="math inline">\(E(X^{\ast}) = 0\)</span> and <span class="math inline">\(sd(X^{\ast}) =1\)</span>.</p>
</div>
</section>
<section id="realizations-of-a-normal-distribution" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="realizations-of-a-normal-distribution"><span class="header-section-number">6.4.5</span> Realizations of a Normal Distribution</h3>
<p>Realizations of a normal distribution refers to the actual values that you get when you draw samples from a normal distribution. Each sample drawn from the distribution is a realization of that distribution.</p>
<div id="exm-realizations" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.6 (Realizations of a Normal Distribution)</strong></span> If you have a normal distribution with a mean of 0 and a standard deviation of 1, each number you draw from that distribution is a realization. Here is a Python example that generates 10 realizations of a normal distribution with a mean of 0 and a standard deviation of 1:</p>
<div id="f7632f4c" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>realizations <span class="op">=</span> np.random.normal(mu, sigma, <span class="dv">10</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(realizations)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 0.48951662  0.23879586 -0.44811181 -0.610795   -2.02994507  0.60794659
 -0.35410888  0.15258149  0.50127485 -0.78640277]</code></pre>
</div>
</div>
<p>In this code, <code>np.random.normal</code> generates ten realizations of a normal distribution with a mean of 0 and a standard deviation of 1. The realizations array contains the actual values drawn from the distribution.</p>
</div>
</section>
<section id="the-multivariate-normal-distribution" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="the-multivariate-normal-distribution"><span class="header-section-number">6.4.6</span> The Multivariate Normal Distribution</h3>
<p>The multivariate normal, multinormal, or Gaussian distribution serves as a generalization of the one-dimensional normal distribution to higher dimensions. We will consider <span class="math inline">\(k\)</span>-dimensional random vectors <span class="math inline">\(X = (X_1, X_2, \ldots, X_k)\)</span>. When drawing samples from this distribution, it results in a set of values represented as <span class="math inline">\(\{x_1, x_2, \ldots, x_k\}\)</span>. To fully define this distribution, it is necessary to specify its mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. These parameters are analogous to the mean, which represents the central location, and the variance (squared standard deviation) of the one-dimensional normal distribution introduced in <a href="#eq-normal-one" class="quarto-xref">Equation&nbsp;<span>6.1</span></a>.</p>
<div id="def-multivariate-normal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.12 (The Multivariate Normal Distribution)</strong></span> The probability density function (PDF) of the multivariate normal distribution is defined as: <span class="math display">\[
f_X(x) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp\left(-\frac{1}{2} (x-\mu)^T\Sigma^{-1} (x-\mu)\right),
\]</span> where: <span class="math inline">\(\mu\)</span> is the <span class="math inline">\(k \times 1\)</span> mean vector; <span class="math inline">\(\Sigma\)</span> is the <span class="math inline">\(k \times k\)</span> covariance matrix. The covariance matrix <span class="math inline">\(\Sigma\)</span> is assumed to be positive definite, so that its determinant is strictly positive.</p>
</div>
<p>In the context of the multivariate normal distribution, the mean takes the form of a coordinate within an <span class="math inline">\(k\)</span>-dimensional space. This coordinate represents the location where samples are most likely to be generated, akin to the peak of the bell curve in a one-dimensional or univariate normal distribution.</p>
<div id="def-covariance-2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.13 (Covariance of two random variables)</strong></span> For two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values: <span class="math display">\[
\operatorname{cov}(X, Y) = \operatorname{E}{\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]}
\]</span></p>
<p>For discrete random variables, covariance can be written as: <span class="math display">\[
\operatorname{cov} (X,Y) = \frac{1}{n}\sum_{i=1}^n (x_i-E(X)) (y_i-E(Y)).
\]</span></p>
</div>
<p>The covariance within the multivariate normal distribution denotes the extent to which two variables vary together. The elements of the covariance matrix, such as <span class="math inline">\(\Sigma_{ij}\)</span>, represent the covariances between the variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>. These covariances describe how the different variables in the distribution are related to each other in terms of their variability.</p>
<div id="exm-bivariate-normal-cov-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.7 (The Bivariate Normal Distribution with Positive Covariances)</strong></span> <a href="#fig-bi9040" class="quarto-xref">Figure&nbsp;<span>6.1</span></a> shows draws from a bivariate normal distribution with <span class="math inline">\(\mu = \begin{pmatrix}0 \\ 0\end{pmatrix}\)</span> and <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9 \end{pmatrix}\)</span>.</p>
<div id="cell-fig-bi9040" class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<div id="fig-bi9040" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9040-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi9040-output-1.png" width="588" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9040-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The covariance matrix of a bivariate normal distribution determines the shape, orientation, and spread of the distribution in the two-dimensional space.</p>
<p>The diagonal elements of the covariance matrix (<span class="math inline">\(\sigma_1^2\)</span>, <span class="math inline">\(\sigma_2^2\)</span>) are the variances of the individual variables. They determine the spread of the distribution along each axis. A larger variance corresponds to a greater spread along that axis.</p>
<p>The off-diagonal elements of the covariance matrix (<span class="math inline">\(\sigma_{12}, \sigma_{21}\)</span>) are the covariances between the variables. They determine the orientation and shape of the distribution. If the covariance is positive, the distribution is stretched along the line <span class="math inline">\(y=x\)</span>, indicating that the variables tend to increase together. If the covariance is negative, the distribution is stretched along the line <span class="math inline">\(y=-x\)</span>, indicating that one variable tends to decrease as the other increases. If the covariance is zero, the variables are uncorrelated and the distribution is axis-aligned.</p>
<p>In <a href="#fig-bi9040" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>, the variances are identical and the variables are correlated (covariance is 4), so the distribution is stretched along the line <span class="math inline">\(y=x\)</span>.</p>
<div id="cell-fig-bi90403d" class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div id="fig-bi90403d" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi90403d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi90403d-output-1.png" width="481" height="416" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi90403d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9\end{pmatrix}\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-bivariate-normal-zero" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.8 (The Bivariate Normal Distribution with Mean Zero and Zero Covariances)</strong></span> The Bivariate Normal Distribution with Mean Zero and Zero Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = 0\)</span>.</p>
<p><span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 0 \\ 0 &amp; 9\end{pmatrix}\)</span></p>
<div id="cell-fig-bi9000" class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<div id="fig-bi9000" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi9000-output-1.png" width="582" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 0 \\ 0 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-bivariate-normal-zero-neg" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.9 (The Bivariate Normal Distribution with Mean Zero and Negative Covariances)</strong></span> The Bivariate Normal Distribution with Mean Zero and Negative Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = -4\)</span>.</p>
<p><span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; -4 \\ -4 &amp; 9\end{pmatrix}\)</span></p>
<div id="cell-fig-bi9449" class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<div id="fig-bi9449" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9449-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi9449-output-1.png" width="577" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9449-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; -4 \\ -4 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="covariance" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="covariance"><span class="header-section-number">6.5</span> Covariance</h2>
<p>In statistics, understanding the relationship between random variables is crucial for making inferences and predictions. Two common measures of such relationships are covariance and correlation. Covariance is a measure of how much two random variables change together. If the variables tend to show similar behavior (i.e., when one increases, the other tends to increase), the covariance is positive. Conversely, if they tend to move in opposite directions, the covariance is negative.</p>
<div id="def-covariance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.14 (Covariance)</strong></span> Covariance is calculated as:</p>
<p><span class="math display">\[
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
\]</span></p>
<p>Here, <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[Y]\)</span> are the expected values (means) of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively. Covariance has units that are the product of the units of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<p>For a vector of random variables <span class="math inline">\(\mathbf{Y} = \begin{pmatrix} Y^{(1)}, \ldots, Y^{(n)} \end{pmatrix}^T\)</span>, the covariance matrix <span class="math inline">\(\Sigma\)</span> encapsulates the covariances between each pair of variables:</p>
<p><span class="math display">\[
\Sigma = \text{Cov}(\mathbf{Y}, \mathbf{Y}) =
\begin{pmatrix}
\text{Var}(Y^{(1)}) &amp; \text{Cov}(Y^{(1)}, Y^{(2)}) &amp; \ldots \\
\text{Cov}(Y^{(2)}, Y^{(1)}) &amp; \text{Var}(Y^{(2)}) &amp; \ldots \\
\vdots &amp; \vdots &amp; \ddots
\end{pmatrix}
\]</span></p>
<p>The diagonal elements represent the variances, while the off-diagonal elements are the covariances.</p>
</section>
<section id="correlation" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="correlation"><span class="header-section-number">6.6</span> Correlation</h2>
<div id="def-correlation-coefficient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.15 ((Pearson) Correlation Coefficient)</strong></span> The Pearson correlation coefficient, often denoted by <span class="math inline">\(\rho\)</span> for the population or <span class="math inline">\(r\)</span> for a sample, is calculated by dividing the covariance of two variables by the product of their standard deviations.</p>
<p><span class="math display">\[
\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y},
\]</span></p>
<p>where <span class="math inline">\(\text{Cov}(X, Y)\)</span> is the covariance between variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively.</p>
</div>
<p>Correlation, specifically the correlation coefficient, is a normalized measure of the linear relationship between two variables. It provides a value ranging from <span class="math inline">\(-1\)</span> to <span class="math inline">\(1\)</span>, which is scale-free, making it easier to interpret:</p>
<ul>
<li><span class="math inline">\(-1\)</span>: Perfect negative correlation, indicating that as one variable increases, the other decreases.</li>
<li><span class="math inline">\(0\)</span>: No correlation, indicating no linear relationship between the variables.</li>
<li><span class="math inline">\(1\)</span>: Perfect positive correlation, indicating that both variables increase together.</li>
</ul>
<div id="def-correlation-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.16 (The Correlation Matrix <span class="math inline">\(\Psi\)</span>)</strong></span> The correlation matrix <span class="math inline">\(\Psi\)</span> is an extension of the concept of the correlation coefficient to multiple variables. It is derived from the covariance matrix <span class="math inline">\(\Sigma\)</span> and standardizes each covariance element by dividing it by the product of the standard deviations of the corresponding variables. The correlation matrix <span class="math inline">\(\Psi\)</span> is defined as:</p>
<p><span class="math display">\[
\Psi = \begin{pmatrix} \rho_{ij} \end{pmatrix} = \begin{pmatrix} \frac{\sigma_{ij}}{\sigma_i \sigma_j} \end{pmatrix}
\]</span></p>
<p>where <span class="math inline">\(\rho_{ij}\)</span> denotes the correlation coefficient between the <span class="math inline">\(i^{\text{th}}\)</span> and <span class="math inline">\(j^{\text{th}}\)</span> variables. Hence, <span class="math inline">\(\Psi\)</span> is a symmetric matrix, and its diagonal elements are always 1, because the correlation of any variable with itself is always 1.</p>
</div>
<div id="exm-covariance-correlation-matrix" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.10 (Computing a Correlation Matrix)</strong></span> Suppose you have a dataset consisting of three variables: <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Z\)</span>. You can compute the correlation matrix as follows:</p>
<ol type="1">
<li>Calculate the covariance matrix <span class="math inline">\(\Sigma\)</span>, which contains covariances between all pairs of variables.</li>
<li>Extract the standard deviations for each variable from the diagonal elements of <span class="math inline">\(\Sigma\)</span>.</li>
<li>Use the standard deviations to compute the correlation matrix <span class="math inline">\(\Psi\)</span>.</li>
</ol>
<p>Suppose we have two sets of data points:</p>
<ul>
<li><span class="math inline">\(X = [1, 2, 3]\)</span></li>
<li><span class="math inline">\(Y = [4, 5, 6]\)</span></li>
</ul>
<p>We want to compute the correlation matrix <span class="math inline">\(\Psi\)</span> for these variables. First, calculate the mean of each variable.</p>
<p><span class="math display">\[
\bar{X} = \frac{1 + 2 + 3}{3} = 2
\]</span></p>
<p><span class="math display">\[
\bar{Y} = \frac{4 + 5 + 6}{3} = 5
\]</span></p>
<p>Second, compute the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The covariance is calculated as:</p>
<p><span class="math display">\[
\text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})
\]</span></p>
<p>For our data:</p>
<p><span class="math display">\[
\text{Cov}(X, Y) = \frac{1}{3-1} \left[(1 - 2)(4 - 5) + (2 - 2)(5 - 5) + (3 - 2)(6 - 5)\right]
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2} \left[1 + 0 + 1\right] = 1
\]</span></p>
<p>Third, calculate the variances of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Variance is calculated as:</p>
<p><span class="math display">\[
\text{Var}(X) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2} \left[(1-2)^2 + (2-2)^2 + (3-2)^2\right] = \frac{1}{2} (1 + 0 + 1) = 1
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
\text{Var}(Y) = \frac{1}{2} \left[(4-5)^2 + (5-5)^2 + (6-5)^2\right] = \frac{1}{2} (1 + 0 + 1) = 1
\]</span></p>
<p>Then, compute the correlation coefficient. The correlation coefficient <span class="math inline">\(\rho_{XY}\)</span> is:</p>
<p><span class="math display">\[
\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \cdot \sqrt{\text{Var}(Y)}}
\]</span></p>
<p><span class="math display">\[
= \frac{1}{\sqrt{1} \cdot \sqrt{1}} = 1
\]</span></p>
<p>Finally, construct the correlation matrix. The correlation matrix <span class="math inline">\(\Psi\)</span> is given as:</p>
<p><span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; \rho_{XY} \\
\rho_{XY} &amp; 1
\end{pmatrix}
= \begin{pmatrix}
1 &amp; 1 \\
1 &amp; 1
\end{pmatrix}
\]</span></p>
<p>Thus, for these two variables, the correlation matrix indicates a perfect positive linear relationship (correlation coefficient of 1) between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="exm-covariance-correlation" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.11 (Computing the Correlation Matrix from a Covariance Matrix)</strong></span> The provided Python code computes the correlation matrix from a covariance matrix using the NumPy library. The steps involved are as follows:</p>
<ol type="1">
<li>Extract Standard Deviations:
<ul>
<li><code>np.sqrt(np.diag(cov))</code> extracts the diagonal elements of the covariance matrix (variances) and computes their square root to get the standard deviations.</li>
</ul></li>
<li>Compute Correlation Matrix:
<ul>
<li>The correlation matrix is obtained by dividing each element of the covariance matrix by the product of the corresponding variables’ standard deviations: <code>cov / np.outer(std_devs, std_devs)</code>.</li>
</ul></li>
<li>Function Usage:
<ul>
<li>The function <code>covariance_to_correlation</code> takes a covariance matrix as input and returns the corresponding correlation matrix.</li>
</ul></li>
</ol>
<div id="127d9d96" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> covariance_to_correlation(cov):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    std_devs <span class="op">=</span> np.sqrt(np.diag(cov))</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    corr <span class="op">=</span> cov <span class="op">/</span> np.outer(std_devs, std_devs)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> corr</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Example covariance matrix</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="op">-</span><span class="dv">4</span>], [<span class="op">-</span><span class="dv">4</span>, <span class="dv">9</span>]])</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and display the correlation matrix</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>correlation_matrix <span class="op">=</span> covariance_to_correlation(cov)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(correlation_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[ 1.         -0.44444444]
 [-0.44444444  1.        ]]</code></pre>
</div>
</div>
</div>
<div id="exm-cov-independent" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.12 (Covariance of Independent Variables)</strong></span> Consider a covariance matrix where variables are independent:</p>
<div id="3fbece08" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>cov_independent <span class="op">=</span> np.array([[<span class="dv">4</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">9</span>]])</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>correlation_independent <span class="op">=</span> covariance_to_correlation(cov_independent)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(correlation_independent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1. 0.]
 [0. 1.]]</code></pre>
</div>
</div>
<p>Here, since the off-diagonal elements are 0, the variables are uncorrelated.</p>
</div>
<div id="exm-cov-strong" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.13 (Strong Correlation)</strong></span> For a covariance matrix with strong positive correlation:</p>
<div id="0db75932" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>cov_strong <span class="op">=</span> np.array([[<span class="dv">16</span>, <span class="dv">12</span>], [<span class="dv">12</span>, <span class="dv">16</span>]])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>correlation_strong <span class="op">=</span> covariance_to_correlation(cov_strong)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(correlation_strong)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1.   0.75]
 [0.75 1.  ]]</code></pre>
</div>
</div>
<p>A value close to 1 suggests a strong positive relationship between the variables.</p>
</div>
<div id="exm-cov-negative" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.14 (Strong Negative Correlation)</strong></span> &nbsp;</p>
<div id="eb27308a" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>cov_negative <span class="op">=</span> np.array([[<span class="dv">25</span>, <span class="op">-</span><span class="dv">25</span>], [<span class="op">-</span><span class="dv">25</span>, <span class="dv">25</span>]])</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>correlation_negative <span class="op">=</span> covariance_to_correlation(cov_negative)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(correlation_negative)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[ 1. -1.]
 [-1.  1.]]</code></pre>
</div>
</div>
<p>This matrix indicates a perfect negative correlation where one variable increases as the other decreases.</p>
</div>
<section id="r-squared-in-simple-linear-regression" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="r-squared-in-simple-linear-regression"><span class="header-section-number">6.6.1</span> R-Squared in Simple Linear Regression</h3>
<p>In simple linear regression, where there is one independent variable <span class="math inline">\(X\)</span> and one dependent variable <span class="math inline">\(Y\)</span>, the R-squared (<span class="math inline">\(R^2\)</span>) is the square of the Pearson correlation coefficient (<span class="math inline">\(r\)</span>) between the observed values of the dependent variable and the values predicted by the regression model.</p>
<p>In simple linear regression, the relationship between the independent variable <span class="math inline">\(X\)</span> and the dependent variable <span class="math inline">\(Y\)</span> is modeled using the equation:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X + \epsilon
\]</span></p>
<p>Here, <span class="math inline">\(\beta_0\)</span> is the intercept, <span class="math inline">\(\beta_1\)</span> is the slope or regression coefficient, and <span class="math inline">\(\epsilon\)</span> is the error term.</p>
<div id="def-r-squared" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.17 (R-Squared (<span class="math inline">\(R^2\)</span>))</strong></span> <span class="math inline">\(R^2\)</span> is a measure of how well the regression model explains the variance in the dependent variable. It is calculated as the square of the correlation coefficient (<span class="math inline">\(r\)</span>) between the actual values <span class="math inline">\(Y\)</span> and the predicted values <span class="math inline">\(\hat{Y}\)</span> from the regression model. It ranges from 0 to 1, where:</p>
<ul>
<li>1 indicates that the regression predictions perfectly fit the data.</li>
<li>0 indicates that the model does not explain any of the variability in the target data around its mean.</li>
</ul>
</div>
<p>In simple linear regression, we have<br>
<span class="math display">\[
R^2 = r^2.
\]</span></p>
<p>The correlation coefficient <span class="math inline">\(r\)</span> measures the strength and direction of the linear relationship between two variables. So, in simple linear regression, the R-squared is equal to the square of the correlation coefficient between the observed values and the predicted values:</p>
<p><span class="math display">\[
R^2 = ( \text{correlation coefficient of } Y \text{ and } \hat{Y} )^2
\]</span></p>
<p>This equivalence holds specifically for simple linear regression due to the direct relationship between the linear fit and the correlation of two variables. In multiple linear regression, while <span class="math inline">\(R^2\)</span> still represents the proportion of variance explained by the model, it is not simply the square of a single correlation coefficient as it involves multiple predictors.</p>
</section>
</section>
<section id="cholesky-decomposition-and-positive-definite-matrices" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="cholesky-decomposition-and-positive-definite-matrices"><span class="header-section-number">6.7</span> Cholesky Decomposition and Positive Definite Matrices</h2>
<p>The covariance matrix must be positive definite for a multivariate normal distribution for a couple of reasons:</p>
<ul>
<li>Semidefinite vs Definite: A covariance matrix is always symmetric and positive semidefinite. However, for a multivariate normal distribution, it must be positive definite, not just semidefinite. This is because a positive semidefinite matrix can have zero eigenvalues, which would imply that some dimensions in the distribution have zero variance, collapsing the distribution in those dimensions. A positive definite matrix has all positive eigenvalues, ensuring that the distribution has positive variance in all dimensions.</li>
<li>Invertibility: The multivariate normal distribution’s probability density function involves the inverse of the covariance matrix. If the covariance matrix is not positive definite, it may not be invertible, and the density function would be undefined.</li>
</ul>
<p>In summary, the covariance matrix being positive definite ensures that the multivariate normal distribution is well-defined and has positive variance in all dimensions.</p>
<p>The definiteness of a matrix can be checked by examining the eigenvalues of the matrix. If all eigenvalues are positive, the matrix is positive definite.</p>
<div id="f6e1b3a9" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_positive_definite(matrix):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">all</span>(np.linalg.eigvals(matrix) <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]])</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(is_positive_definite(matrix))  <span class="co"># Outputs: True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True</code></pre>
</div>
</div>
<p>However, a more efficient way to check the definiteness of a matrix is through the Cholesky decomposition.</p>
<div id="def-cholesky-decomposition" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.18 (Cholesky Decomposition)</strong></span> For a given symmetric positive-definite matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, there exists a unique lower triangular matrix <span class="math inline">\(L \in \mathbb{R}^{n \times n}\)</span> with positive diagonal elements such that:</p>
<p><span class="math display">\[
A = L L^T.
\]</span></p>
<p>Here, <span class="math inline">\(L^T\)</span> denotes the transpose of <span class="math inline">\(L\)</span>.</p>
</div>
<div id="exm-cholesky-decomposition" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.15 (Cholesky Decomposition)</strong></span> Given a symmetric positive-definite matrix <span class="math inline">\(A = \begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9 \end{pmatrix}\)</span>, the Cholesky decomposition computes the lower triangular matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(A = L L^T\)</span>. The matrix <span class="math inline">\(L\)</span> is computed as: <span class="math display">\[
L = \begin{pmatrix} 3 &amp; 0 \\ 4/3 &amp; 2 \end{pmatrix},
\]</span> so that <span class="math display">\[
L L^T = \begin{pmatrix} 3 &amp; 0 \\ 4/3 &amp; \sqrt{65}/3 \end{pmatrix} \begin{pmatrix} 3 &amp; 4/3 \\ 0 &amp; \sqrt{65}/3 \end{pmatrix} = \begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9 \end{pmatrix} = A.
\]</span></p>
</div>
<p>An efficient implementation of the definiteness-check based on Cholesky is already available in the <code>numpy</code> library. It provides the <code>np.linalg.cholesky</code> function to compute the Cholesky decomposition of a matrix. This more efficient <code>numpy</code>-approach can be used as follows:</p>
<div id="9a13a4ea" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_pd(K):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>        np.linalg.cholesky(K)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> np.linalg.linalg.LinAlgError <span class="im">as</span> err:</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'Matrix is not positive definite'</span> <span class="kw">in</span> err.message:</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]])</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(is_pd(matrix))  <span class="co"># Outputs: True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True</code></pre>
</div>
</div>
<div id="exm-cholesky-decomposition" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.16 (Cholesky decomposition using <code>numpy</code>)</strong></span> <code>linalg.cholesky</code> computes the Cholesky decomposition of a matrix, i.e., it computes a lower triangular matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(LL^T = A\)</span>. If the matrix is not positive definite, an error (<code>LinAlgError</code>) is raised.</p>
<div id="bf7d1bb3" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a Hermitian, positive-definite matrix</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]]) </span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the Cholesky decomposition</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(A)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L = </span><span class="ch">\n</span><span class="st">"</span>, L)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L*LT = </span><span class="ch">\n</span><span class="st">"</span>, np.dot(L, L.T))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>L = 
 [[3.         0.        ]
 [1.33333333 2.68741925]]
L*LT = 
 [[9. 4.]
 [4. 9.]]</code></pre>
</div>
</div>
</div>
</section>
<section id="maximum-likelihood-estimation-multivariate-normal-distribution" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="maximum-likelihood-estimation-multivariate-normal-distribution"><span class="header-section-number">6.8</span> Maximum Likelihood Estimation: Multivariate Normal Distribution</h2>
<section id="the-joint-probability-density-function-of-the-multivariate-normal-distribution" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="the-joint-probability-density-function-of-the-multivariate-normal-distribution"><span class="header-section-number">6.8.1</span> The Joint Probability Density Function of the Multivariate Normal Distribution</h3>
<p>Consider the first <span class="math inline">\(n\)</span> terms of an identically and independently distributed (i.i..d.) sequence <span class="math inline">\({X^{(j)}}\)</span> of <span class="math inline">\(k\)</span>-dimensional multivariate normal random vectors, i.e., <span id="eq-mvn"><span class="math display">\[
X^{(j)} \sim N(\mu, \Sigma), j=1,2,\ldots.
\tag{6.2}\]</span></span></p>
<p>The joint probability density function of the <span class="math inline">\(j\)</span>-th term of the sequence is <span class="math display">\[
f_X(x_j) = \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right),
\]</span></p>
<p>where: <span class="math inline">\(\mu\)</span> is the <span class="math inline">\(k \times 1\)</span> mean vector; <span class="math inline">\(\Sigma\)</span> is the <span class="math inline">\(k \times k\)</span> covariance matrix. The covariance matrix <span class="math inline">\(\Sigma\)</span> is assumed to be positive definite, so that its determinant is strictly positive. We use <span class="math inline">\(x_1, \ldots x_n\)</span>, i.e., the realizations of the first <span class="math inline">\(n\)</span> random vectors in the sequence, to estimate the two unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>.</p>
</section>
<section id="the-log-likelihood-function" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="the-log-likelihood-function"><span class="header-section-number">6.8.2</span> The Log-Likelihood Function</h3>
<div id="def-likelihood" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.19 (Likelihood Function)</strong></span> The likelihood function is defined as the joint probability density function of the observed data, viewed as a function of the unknown parameters.</p>
</div>
<p>Since the terms in the sequence <a href="#eq-mvn" class="quarto-xref">Equation&nbsp;<span>6.2</span></a> are independent, their joint density is equal to the product of their marginal densities. As a consequence, the likelihood function can be written as the product of the individual densities:</p>
<p><span class="math display">\[
L(\mu, \Sigma) = \prod_{j=1}^n f_X(x_j) = \prod_{j=1}^n \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right)
\]</span> <span id="eq-likelihood-mvn"><span class="math display">\[
= \frac{1}{(2\pi)^{nk/2} \det(\Sigma)^{n/2}} \exp\left(-\frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right).
\tag{6.3}\]</span></span></p>
<p>Taking the natural logarithm of the likelihood function, we obtain the log-likelihood function:</p>
<div id="exm-log-likelihood" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.17 (Log-Likelihood Function of the Multivariate Normal Distribution)</strong></span> The log-likelihood function of the multivariate normal distribution is given by <span class="math display">\[
\ell(\mu, \Sigma) = -\frac{nk}{2} \ln(2\pi) - \frac{n}{2} \ln(\det(\Sigma)) - \frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu).
\]</span></p>
</div>
<p>The likelihood function is well-defined only if <span class="math inline">\(\det(\Sigma)&gt;0\)</span>.</p>
</section>
</section>
<section id="constructing-a-surrogate" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="constructing-a-surrogate"><span class="header-section-number">6.9</span> Constructing a Surrogate</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section is based on chapter 2 in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>.</p>
</div>
</div>
<div id="def-black-box" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.20 (Black Box Problem)</strong></span> We are trying to learn a mapping that converts the vector <span class="math inline">\(\mathbf{x}\)</span> into a scalar output <span class="math inline">\(y\)</span>, i.e., we are trying to learn a function <span class="math display">\[
y = f(x).
\]</span> If function is hidden (“lives in a black box”), so that the physics of the problem is not known, the problem is called a black box problem.</p>
</div>
<p>This black box could take the form of either a physical or computer experiment, for example, a finite element code, which calculates the maximum stress (<span class="math inline">\(\sigma\)</span>) for given product dimensions (<span class="math inline">\(\mathbf{x}\)</span>).</p>
<div id="def-generic-solution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.21 (Generic Solution)</strong></span> The generic solution method is to collect the output values <span class="math inline">\(y^{(1)}\)</span>, <span class="math inline">\(y^{(2)}\)</span>, , <span class="math inline">\(y^{(n)}\)</span> that result from a set of inputs <span class="math inline">\(\mathbf{x}^{(1)}\)</span>, <span class="math inline">\(\mathbf{x}^{(2)}\)</span>, , <span class="math inline">\(\mathbf{x}^{(n)}\)</span> and find a best guess <span class="math inline">\(\hat{f}(\mathbf{x})\)</span> for the black box mapping <span class="math inline">\(f\)</span>, based on these known observations.</p>
</div>
<section id="stage-one-preparing-the-data-and-choosing-a-modelling-approach" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="stage-one-preparing-the-data-and-choosing-a-modelling-approach"><span class="header-section-number">6.9.1</span> Stage One: Preparing the Data and Choosing a Modelling Approach</h3>
<p>The first step is the identification, through a small number of observations, of the inputs that have a significant impact on <span class="math inline">\(f\)</span>; that is the determination of the shortest design variable vector <span class="math inline">\(\mathbf{x} = \{x_1, x_2, \ldots, x_k\}^T\)</span> that, by sweeping the ranges of all of its variables, can still elicit most of the behavior the black box is capable of. The ranges of the various design variables also have to be established at this stage.</p>
<p>The second step is to recruit <span class="math inline">\(n\)</span> of these <span class="math inline">\(k\)</span>-vectors into a list <span class="math display">\[
\mathbf{X} = \{ \mathbf{x}^{(1)},\mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(n)} \}^T,
\]</span> where each <span class="math inline">\(\mathbf{x}^{(i)}\)</span> is a <span class="math inline">\(k\)</span>-vector. The corresponding responses are collected in a vector such that this represents the design space as thoroughly as possible.</p>
<p>In the surrogate modeling process, the number of samples <span class="math inline">\(n\)</span> is often limited, as it is constrained by the computational cost (money and/or time) associated with obtaining each observation.</p>
<p>It is advisable to scale <span class="math inline">\(\mathbf{x}\)</span> at this stage into the unit cube <span class="math inline">\([0, 1]^k\)</span>, a step that can simplify the subsequent mathematics and prevent multidimensional scaling issues.</p>
<p>We now focus on the attempt to learn <span class="math inline">\(f\)</span> through data pairs <span class="math display">\[
\{ (\mathbf{x}^{(1)}, y^{(1)}), (\mathbf{x}^{(2)}, y^{(2)}), \ldots, (\mathbf{x}^{(n)}, y^{(n)}) \}.
\]</span></p>
<p>This supervised learning process essentially involves searching across the space of possible functions <span class="math inline">\(\hat{f}\)</span> that would replicate observations of <span class="math inline">\(f\)</span>. This space of functions is infinite. Any number of hypersurfaces could be drawn to pass through or near the known observations, accounting for experimental error. However, most of these would generalize poorly; they would be practically useless at predicting responses at new sites, which is the ultimate goal.</p>
<div id="exm-needle-haystack" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.18 (The Needle(s) in the Haystack Function)</strong></span> An extreme example is the ‘needle(s) in the haystack’ function:</p>
<p><span class="math display">\[
f(x) = \begin{cases}
y^{(1)}, &amp; \text{if } x = \mathbf{x}^{(1)} \\
y^{(2)}, &amp; \text{if } x = \mathbf{x}^{(2)} \\
\vdots &amp; \\
y^{(n)}, &amp; \text{if } x = \mathbf{x}^{(n)} \\
0, &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p>While this predictor reproduces all training data, it seems counter-intuitive and unsettling to predict 0 everywhere else for most engineering functions. Although there is a small chance that the function genuinely resembles the equation above and we sampled exactly where the needles are, it is highly unlikely.</p>
</div>
<p>There are countless other configurations, perhaps less contrived, that still generalize poorly. This suggests a need for systematic means to filter out nonsensical predictors. In our approach, we embed the structure of <span class="math inline">\(f\)</span> into the model selection algorithm and search over its parameters to fine-tune the approximation to observations. For instance, consider one of the simplest models, <span id="eq-linear-model-simple"><span class="math display">\[
f(x, \mathbf{w}) = \mathbf{w}^T\mathbf{x} + v.
\tag{6.4}\]</span></span> Learning <span class="math inline">\(f\)</span> with this model implies that its structure—a hyperplane—is predetermined, and the fitting process involves finding the <span class="math inline">\(k + 1\)</span> parameters (the slope vector <span class="math inline">\(\mathbf{w}\)</span> and the intercept <span class="math inline">\(v\)</span>) that best fit the data. This will be accomplished in Stage Two.</p>
<p>Complicating this further is the noise present in observed responses (we assume design vectors <span class="math inline">\(\mathbf{x}\)</span> are not corrupted). Here, we focus on learning from such data, which sometimes risks overfitting.</p>
<div id="def-overfitting" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.22 (Overfitting)</strong></span> Overfitting occurs when the model becomes too flexible and captures not only the underlying trend but also the noise in the data.</p>
</div>
<p>In the surrogate modeling process, the second stage as described in <a href="#sec-stage-two" class="quarto-xref"><span>Section 6.9.2</span></a>, addresses this issue of complexity control by estimating the parameters of the fixed structure model. However, foresight is necessary even at the model type selection stage.</p>
<p>Model selection often involves physics-based considerations, where the modeling technique is chosen based on expected underlying responses.</p>
<div id="exm-model-selection" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.19 (Model Selection)</strong></span> Modeling stress in an elastically deformed solid due to small strains may justify using a simple linear approximation. Without insights into the physics, and if one fails to account for the simplicity of the data, a more complex and excessively flexible model may be incorrectly chosen. Although parameter estimation might still adjust the approximation to become linear, an opportunity to develop a simpler and robust model may be lost.</p>
<ul>
<li>Simple linear (or polynomial) models, despite their lack of flexibility, have advantages like applicability in further symbolic computations.</li>
<li>Conversely, if we incorrectly assume a quadratic process when multiple peaks and troughs exist, the parameter estimation stage will not compensate for an unsuitable model choice. A quadratic model is too rigid to fit a multimodal function, regardless of parameter adjustments.</li>
</ul>
</div>
</section>
<section id="sec-stage-two" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="sec-stage-two"><span class="header-section-number">6.9.2</span> Stage Two: Parameter Estimation and Training</h3>
<p>Assuming that Stage One helped identify the <span class="math inline">\(k\)</span> critical design variables, acquire the learning data set, and select a generic model structure <span class="math inline">\(f(\mathbf{x}, \mathbf{w})\)</span>, the task now is to estimate parameters <span class="math inline">\(\mathbf{w}\)</span> to ensure the model fits the data optimally. Among several estimation criteria, we will discuss two methods here.</p>
<div id="def-mle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.23 (Maximum Likelihood Estimation)</strong></span> Given a set of parameters <span class="math inline">\(\mathbf{w}\)</span>, the model <span class="math inline">\(f(\mathbf{x}, \mathbf{w})\)</span> allows computation of the probability of the data set <span class="math display">\[
\{(\mathbf{x}^{(1)}, y^{(1)} \pm \epsilon), (\mathbf{x}^{(2)}, y^{(2)} \pm \epsilon), \ldots, (\mathbf{x}^{(n)}, y^{(n)} \pm \epsilon)\}
\]</span> resulting from <span class="math inline">\(f\)</span> (where <span class="math inline">\(\epsilon\)</span> is a small error margin around each data point).</p>
<p>Taking <a href="#eq-likelihood-mvn" class="quarto-xref">Equation&nbsp;<span>6.3</span></a> and assuming errors <span class="math inline">\(\epsilon\)</span> are independently and normally distributed with standard deviation <span class="math inline">\(\sigma\)</span>, the probability of the data set is given by:</p>
<p><span class="math display">\[
P = \frac{1}{(2\pi \sigma^2)^{n/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^{n} \left( y^{(i)} - f(\mathbf{x}^{(i)}, \mathbf{w}) \right)^2 \epsilon \right].
\]</span></p>
<p>Intuitively, this is equivalent to the likelihood of the parameters given the data. Accepting this intuitive relationship as a mathematical one aids in model parameter estimation. This is achieved by maximizing the likelihood or, more conveniently, minimizing the negative of its natural logarithm:</p>
<p><span id="eq-forr23"><span class="math display">\[
\min_{\mathbf{w}} \sum_{i=1}^{n} \frac{[y^{(i)} - f(\mathbf{x}^{(i)}, \mathbf{w})]^2}{2\sigma^2} + \frac{n}{2} \ln \epsilon .
\tag{6.5}\]</span></span></p>
</div>
<p>If we assume <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\epsilon\)</span> are constants, <a href="#eq-forr23" class="quarto-xref">Equation&nbsp;<span>6.5</span></a> simplifies to the well-known least squares criterion:</p>
<p><span class="math display">\[
\min_{\mathbf{w}} \sum_{i=1}^{n} [y^{(i)} - f(\mathbf{x}^{(i)}, \mathbf{w})]^2 .
\]</span></p>
<p>Cross-validation is another method used to estimate model performance.</p>
<div id="def-cross-validation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.24 (Cross-Validation)</strong></span> Cross-validation splits the data randomly into <span class="math inline">\(q\)</span> roughly equal subsets, and then cyclically removing each subset and fitting the model to the remaining <span class="math inline">\(q - 1\)</span> subsets. A loss function <span class="math inline">\(L\)</span> is then computed to measure the error between the predictor and the withheld subset for each iteration, with contributions summed over all <span class="math inline">\(q\)</span> iterations. More formally, if a mapping <span class="math inline">\(\theta: \{1, \ldots, n\} \to \{1, \ldots, q\}\)</span> describes the allocation of the <span class="math inline">\(n\)</span> training points to one of the <span class="math inline">\(q\)</span> subsets and <span class="math inline">\(f^{(-\theta(i))}(\mathbf{x})\)</span> is the predicted value by removing the subset <span class="math inline">\(\theta(i)\)</span> (i.e., the subset where observation <span class="math inline">\(i\)</span> belongs), the cross-validation measure, used as an estimate of prediction error, is:</p>
<p><span id="eq-cv-basis"><span class="math display">\[
CV = \frac{1}{n} \sum_{i=1}^{n} L(y^{(i)}, f^{(-\theta(i))}(\mathbf{x}^{(i)})) .
\tag{6.6}\]</span></span></p>
</div>
<p>Introducing the squared error as the loss function and considering our generic model <span class="math inline">\(f\)</span> still dependent on undetermined parameters, we write <a href="#eq-cv-basis" class="quarto-xref">Equation&nbsp;<span>6.6</span></a> as:</p>
<p><span id="eq-cv-sse"><span class="math display">\[
CV = \frac{1}{n} \sum_{i=1}^{n} [y^{(i)} - f^{(-\theta(i))}(\mathbf{x}^{(i)})]^2 .
\tag{6.7}\]</span></span></p>
<p>The extent to which <a href="#eq-cv-sse" class="quarto-xref">Equation&nbsp;<span>6.7</span></a> is an unbiased estimator of true risk depends on <span class="math inline">\(q\)</span>. It is shown that if <span class="math inline">\(q = n\)</span>, the leave-one-out cross-validation (LOOCV) measure is almost unbiased. However, LOOCV can have high variance because subsets are very similar. <span class="citation" data-cites="Hast17a">Hastie, Tibshirani, and Friedman (<a href="references.html#ref-Hast17a" role="doc-biblioref">2017</a>)</span>) suggest using compromise values like <span class="math inline">\(q = 5\)</span> or <span class="math inline">\(q = 10\)</span>. Using fewer subsets also reduces the computational cost of the cross-validation process, see also <span class="citation" data-cites="arlot2010">Arlot, Celisse, et al. (<a href="references.html#ref-arlot2010" role="doc-biblioref">2010</a>)</span> and <span class="citation" data-cites="Koha95a">Kohavi (<a href="references.html#ref-Koha95a" role="doc-biblioref">1995</a>)</span>.</p>
</section>
<section id="stage-three-model-testing" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="stage-three-model-testing"><span class="header-section-number">6.9.3</span> Stage Three: Model Testing</h3>
<p>If there is a sufficient amount of observational data, a random subset should be set aside initially for model testing. <span class="citation" data-cites="Hast17a">Hastie, Tibshirani, and Friedman (<a href="references.html#ref-Hast17a" role="doc-biblioref">2017</a>)</span> recommend setting aside approximately <span class="math inline">\(0.25n\)</span> of <span class="math inline">\(\mathbf{x} \rightarrow y\)</span> pairs for testing purposes. These observations must remain untouched during Stages One and Two, as their sole purpose is to evaluate the testing error—the difference between true and approximated function values at the test sites—once the model has been built. Interestingly, if the main goal is to construct an initial surrogate for seeding a global refinement criterion-based strategy (as discussed in Section 3.2 in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>), the model testing phase might be skipped.</p>
<p>It is noted that, ideally, parameter estimation (Stage Two) should also rely on a separate subset. However, observational data is rarely abundant enough to afford this luxury (if the function is cheap to evaluate and evaluation sites are selectable, a surrogate model might not be necessary).</p>
<p>When data are available for model testing and the primary objective is a globally accurate model, using either a root mean square error (RMSE) metric or the correlation coefficient (<span class="math inline">\(r^2\)</span>) is recommended. To test the model, a test data set of size <span class="math inline">\(n_t\)</span> is used alongside predictions at the corresponding locations to calculate these metrics, which are defined as follows:</p>
<div id="def-rmse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.25 (Root Mean Square Error (RMSE))</strong></span> <span class="math display">\[
\text{RMSE} = \sqrt{\frac{1}{n_t} \sum_{i=1}^{n_t} (y^{(i)} - \hat{y}^{(i)})^2},
\]</span></p>
</div>
<p>and</p>
<div id="def-r2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.26 (Correlation Coefficient (<span class="math inline">\(r^2\)</span>))</strong></span> <span id="eq-r2"><span class="math display">\[
r^2 = \frac{\text{cov}(y, \hat{y})}{\sqrt{\text{var}(y)\text{var}(\hat{y})}},
\tag{6.8}\]</span></span></p>
</div>
<p><a href="#eq-r2" class="quarto-xref">Equation&nbsp;<span>6.8</span></a> is the correlation coefficient between the observed <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat{y}\)</span> values and can be expanded as:</p>
<p><span class="math display">\[
r^2 = \frac{n_t \sum_{i=1}^{n_t} y^{(i)} \hat{y}^{(i)} - \left(\sum_{i=1}^{n_t} y^{(i)}\right) \left(\sum_{i=1}^{n_t} \hat{y}^{(i)}\right)}{\left( n_t \sum_{i=1}^{n_t} (y^{(i)})^2 - \left(\sum_{i=1}^{n_t} y^{(i)}\right)^2 \right) \left( n_t \sum_{i=1}^{n_t} (\hat{y}^{(i)})^2 - \left(\sum_{i=1}^{n_t} \hat{y}^{(i)}\right)^2 \right)}.
\]</span></p>
<p>Ideally, the RMSE should be minimized, acknowledging its limitation by errors in the objective function <span class="math inline">\(f\)</span> calculation. If the error level is known, like a standard deviation, the aim might be to achieve an RMSE within this value. Often, the target is an RMSE within a specific percentage of the observed data’s objective value range.</p>
<p>The correlation coefficient <span class="math inline">\(r^2\)</span> does not require scaling the data sets and only compares landscape shapes, not values. An <span class="math inline">\(r^2 &gt; 0.8\)</span> typically indicates a surrogate with good predictive capability.</p>
<p>The methods outlined provide quantitative assessments of model accuracy, yet visual evaluations can also be insightful. In general, the RMSE won’t reach zero but will stabilize around a low value. At this point, the surrogate model is saturated with data, and further additions do not enhance the model globally (though local improvements can occur at newly added points if using an interpolating model).</p>
<div id="exm-tea-sugar" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.20 (The Tea and Sugar Analogy)</strong></span> <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span> illustrates this saturation point using a comparision with a cup of tea and sugar. The tea represents the surrogate model, and sugar represents data. Initially, the tea is unsweetened, and adding sugar increases its sweetness. Eventually, a saturation point is reached where no more sugar dissolves, and the tea cannot get any sweeter. Similarly, a more flexible model, like one with additional parameters or employing interpolation rather than regression, can increase the saturation point—akin to making a hotter cup of tea for dissolving more sugar.</p>
</div>
</section>
</section>
<section id="sampling-plans" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="sampling-plans"><span class="header-section-number">6.10</span> Sampling Plans</h2>
<div id="def-sampling-plan" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.27 (Sampling Plan)</strong></span> In the context of computer experiments, the term “sampling plan” refers to the set of input values at which the computer code is evaluated.</p>
</div>
<p>The goal of a sampling plan is to efficiently explore the input space to understand the behavior of the computer code and build a surrogate model that accurately represents the code’s behavior. Traditionally, Response Surface Methodology (RSM) has been used to design sampling plans for computer experiments. These sampling plans are based on procedures that generate points by means of a rectangiular grid or a factorial design. However, more recently, Design and Analysis of Computer Experiments (DACE) has emerged as a more flexible and powerful approach for designing sampling plans. <code>spotpython</code> uses a class for generating space-filling designs using Latin Hypercube Sampling (LHS) and maximin distance criteria. It is based on <code>scipy</code>’s <code>LatinHypercube</code> class. The following example demonstrates how to generate a Latin Hypercube Sampling design using <code>spotpython</code>. The result is shown in <a href="#fig-lhs-spotpython" class="quarto-xref">Figure&nbsp;<span>6.5</span></a>. As can seen in the figure, a Latin hypercube sample generates <span class="math inline">\(n\)</span> points in <span class="math inline">\([0,1)^{d}\)</span>. Each univariate marginal distribution is stratified, placing exactly one point in <span class="math inline">\([j/n, (j+1)/n)\)</span> for <span class="math inline">\(j=0,1,...,n-1\)</span>.</p>
<div id="cell-fig-lhs-spotpython" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.design.spacefilling <span class="im">import</span> SpaceFilling</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>lhd <span class="op">=</span> SpaceFilling(k<span class="op">=</span><span class="dv">2</span>, seed<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> lhd.scipy_lhd(n<span class="op">=</span><span class="dv">10</span>, repeats<span class="op">=</span><span class="dv">1</span>, lower<span class="op">=</span>np.array([<span class="dv">0</span>, <span class="dv">0</span>]), upper<span class="op">=</span>np.array([<span class="dv">10</span>, <span class="dv">10</span>]))</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>])</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x1'</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'x2'</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>plt.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-lhs-spotpython" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lhs-spotpython-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-lhs-spotpython-output-1.png" width="576" height="429" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lhs-spotpython-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: Latin Hypercube Sampling design (sampling plan)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="kriging" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="kriging"><span class="header-section-number">6.11</span> Kriging</h2>
<section id="the-kriging-idea-in-a-nutshell" class="level3" data-number="6.11.1">
<h3 data-number="6.11.1" class="anchored" data-anchor-id="the-kriging-idea-in-a-nutshell"><span class="header-section-number">6.11.1</span> The Kriging Idea in a Nutshell</h3>
<p>Kriging can be applied to planned experiments, where the design is based on a sampling plan as shown in <a href="#fig-lhs-spotpython" class="quarto-xref">Figure&nbsp;<span>6.5</span></a>, as well as to computer experiments, where the design is based on the computer code’s input space, as shown in <a href="#fig-unknownf" class="quarto-xref">Figure&nbsp;<span>6.6</span></a>.</p>
<div id="cell-fig-unknownf" class="cell" data-execution_count="29">
<div class="cell-output cell-output-display">
<div id="fig-unknownf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unknownf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-unknownf-output-1.png" width="600" height="429" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unknownf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: Eight measurements of an unknown function. No sampling plan was used.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In general, we onsider observed data of an unknown function <span class="math inline">\(f\)</span> at <span class="math inline">\(n\)</span> points <span class="math inline">\(x_1, \ldots, x_n\)</span>. These measurements a considered as realizations of MVN random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span> with mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma_n\)</span> as shown in <a href="#fig-mvn1-3" class="quarto-xref">Figure&nbsp;<span>6.11</span></a>, <a href="#fig-mvn2" class="quarto-xref">Figure&nbsp;<span>6.12</span></a> or <a href="#fig-mvn5" class="quarto-xref">Figure&nbsp;<span>6.13</span></a>.</p>
<p>In Kriging, a more general covariance matrix (or equivalently, a correlation matrix <span class="math inline">\(\Psi\)</span>) is used, see <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>6.9</span></a>. Using a maximum likelihood approach, we can estimate the unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma_n\)</span> from the data so that the likelihood function is maximized.</p>
<div id="def-kriging-basis-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.28 (The Kriging Basis Functions)</strong></span> Kriging uses <span class="math inline">\(k\)</span>-dimensional basis functions of the form <span id="eq-krigingbase"><span class="math display">\[
\psi(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp \left( - \sum_{l=1}^k \theta_l | x_{l}^{(i)} - x_{l}^{(j)} | ^{p_l} \right),
\tag{6.9}\]</span></span> where <span class="math inline">\(\vec{x}^{(i)}\)</span> denotes the <span class="math inline">\(k\)</span>-dim vector <span class="math inline">\(\vec{x}^{(i)}= (x_1^{(i)}, \ldots, x_k^{(i)})^T\)</span>.</p>
</div>
</section>
<section id="the-kriging-model" class="level3" data-number="6.11.2">
<h3 data-number="6.11.2" class="anchored" data-anchor-id="the-kriging-model"><span class="header-section-number">6.11.2</span> The Kriging Model</h3>
<p>Consider sample data <span class="math inline">\(\vec{X}\)</span> and <span class="math inline">\(\vec{y}\)</span> from <span class="math inline">\(n\)</span> locations that are available in matrix form: <span class="math inline">\(\vec{X}\)</span> is a <span class="math inline">\((n \times k)\)</span> matrix, where <span class="math inline">\(k\)</span> denotes the problem dimension and <span class="math inline">\(\vec{y}\)</span> is a <span class="math inline">\((n\times 1)\)</span> vector. We want to find an expression for a predicted values at a new point <span class="math inline">\(\vec{x}\)</span>, denoted as <span class="math inline">\(\hat{y}\)</span>.</p>
<p>We start with an abstract, not really intuitive concept: The observed responses <span class="math inline">\(\vec{y}\)</span> are considered as if they are from a stochastic process, which will be denoted as <span id="eq-yvec-51"><span class="math display">\[
\begin{pmatrix}
\vec{Y}(\vec{x}^{(1)})\\
\vdots\\
\vec{Y}(\vec{x}^{(n)})\\
\end{pmatrix}.
\tag{6.10}\]</span></span></p>
<p>The set of random vectors from <a href="#eq-yvec-51" class="quarto-xref">Equation&nbsp;<span>6.10</span></a> (also referred to as a <em>random field</em>) has a mean of <span class="math inline">\(\vec{1} \mu\)</span>, which is a <span class="math inline">\((n\times 1)\)</span> vector. The random vectors are correlated with each other using the basis function expression from <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>6.9</span></a>: <span id="eq-corr-kriging-51"><span class="math display">\[
\text{cor} \left(\vec{Y}(\vec{x}^{(i)}),\vec{Y}(\vec{x}^{(l)}) \right) = \exp\left\{ - \sum_{j=1}^k \theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\right\}.
\tag{6.11}\]</span></span> Using <a href="#eq-corr-kriging-51" class="quarto-xref">Equation&nbsp;<span>6.11</span></a>, we can compute the <span class="math inline">\((n \times n)\)</span> correlation matrix <span class="math inline">\(\vec{\Psi}\)</span> of the observed sample data as shown in <a href="#eq-corr-matrix-kriging-51" class="quarto-xref">Equation&nbsp;<span>6.12</span></a>,</p>
<p><span id="eq-corr-matrix-kriging-51"><span class="math display">\[
\vec{\Psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right) &amp; \ldots &amp;
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right)\\
\vdots  &amp; \vdots &amp;  \vdots\\
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right)&amp;
\ldots &amp;
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right)
\end{pmatrix},
\tag{6.12}\]</span></span></p>
<p>and a covariance matrix as shown in <a href="#eq-cov-matrix-kriging-52" class="quarto-xref">Equation&nbsp;<span>6.13</span></a>,</p>
<p><span id="eq-cov-matrix-kriging-52"><span class="math display">\[
\text{Cov}(\mathbf{Y}, \mathbf{Y} ) = \sigma^2 \mathbf{\Psi}.
\tag{6.13}\]</span></span></p>
<p>This assumed correlation between the sample data reflects our expectation that an engineering function will behave in a certain way and it will be smoothly and continuous.</p>
<p>We now have a set of <span class="math inline">\(n\)</span> random variables (<span class="math inline">\(\mathbf{Y}\)</span>) that are correlated with each other as described in the <span class="math inline">\((n \times n)\)</span> correlation matrix <span class="math inline">\(\mathbf{\Psi}\)</span>, see <a href="#eq-corr-matrix-kriging-51" class="quarto-xref">Equation&nbsp;<span>6.12</span></a>. The correlations depend on the absolute distances between sample points <span class="math inline">\(|x_j^{(n)} - x_j^{(n)}|\)</span> and the parameters <span class="math inline">\(p_j\)</span> and <span class="math inline">\(\theta_j\)</span>. The correlation is intuitive, because when two points move close together, then <span class="math inline">\(|x_l^{(i)} - x_l| \to 0\)</span> and <span class="math inline">\(\exp(-|x_l^{(i)} - x_l| \to 1\)</span>, points show very close correlation and <span class="math inline">\(Y(x_l^{(i)}) = Y(x_l)\)</span>.</p>
<p>Three different correlations are shown in <a href="#fig-pval12" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>: <span class="math inline">\(p_j= 0.1, 1, 2\)</span>. The smoothness parameter <span class="math inline">\(p_j\)</span> affects the correlation:</p>
<ul>
<li>With <span class="math inline">\(p_j=0.1\)</span>, there is basicaly no immediate correlation between the points and there is a near discontinuity between the points <span class="math inline">\(Y(\vec{x}_j^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}_j)\)</span>.</li>
<li>With <span class="math inline">\(p_j=2\)</span>, the correlation is more smooth and we have a continuous gradient through <span class="math inline">\(x_j^{(i)} - x_j\)</span>.</li>
</ul>
<p>Reducing <span class="math inline">\(p_j\)</span> increases the rate at which the correlation initially drops with distance. This is shown in <a href="#fig-pval12" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>.</p>
<div id="cell-fig-pval12" class="cell" data-execution_count="31">
<div class="cell-output cell-output-display">
<div id="fig-pval12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pval12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-pval12-output-1.png" width="571" height="411" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pval12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: Correlations with varying <span class="math inline">\(\theta\)</span>. <span class="math inline">\(\theta\)</span> set to 1/10, 1, and 10.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-theta12" class="quarto-xref">Figure&nbsp;<span>6.8</span></a> visualizes the correlation between two points <span class="math inline">\(Y(\vec{x}_j^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}_j)\)</span> for different values of <span class="math inline">\(\theta\)</span>. The parameter <span class="math inline">\(\theta\)</span> can be seen as a width parameter:</p>
<ul>
<li>low <span class="math inline">\(\theta_j\)</span> means that all points will have a high correlation, with <span class="math inline">\(Y(x_j)\)</span> being similar across the sample.</li>
<li>high <span class="math inline">\(\theta_j\)</span> means that there is a significant difference between the <span class="math inline">\(Y(x_j)\)</span>’s.</li>
<li><span class="math inline">\(\theta_j\)</span> is a measure of how active the function we are approximating is.</li>
<li>High <span class="math inline">\(\theta_j\)</span> indicate important parameters, see <a href="#fig-theta12" class="quarto-xref">Figure&nbsp;<span>6.8</span></a>.</li>
</ul>
<div id="cell-fig-theta12" class="cell" data-execution_count="32">
<div class="cell-output cell-output-display">
<div id="fig-theta12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-theta12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-theta12-output-1.png" width="571" height="411" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-theta12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: Correlations with varying <span class="math inline">\(\theta\)</span>. <span class="math inline">\(\theta\)</span> set to 1/10, 1, and 10.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Considering the activity parameter <span class="math inline">\(\theta\)</span> is useful in high-dimensional problems where it is difficult to visualize the design landscape and the effect of the variable is unknown. By examining the elements of the vector <span class="math inline">\(\vec{\theta}\)</span>, we can identify the most important variables and focus on them. This is a crucial step in the optimization process, as it allows us to reduce the dimensionality of the problem and focus on the most important variables.</p>
<div id="exm-corr-matrix-detailed" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.21 (Example: The Correlation Matrix (Detailed Computation))</strong></span> Let <span class="math inline">\(n=4\)</span> and <span class="math inline">\(k=3\)</span>. The sample plan is represented by the following matrix <span class="math inline">\(X\)</span>: <span class="math display">\[
X = \begin{pmatrix} x_{11} &amp; x_{12} &amp; x_{13}\\
x_{21} &amp; x_{22} &amp; x_{23}\\
x_{31} &amp; x_{32} &amp; x_{33}\\
x_{41} &amp; x_{42} &amp; x_{43}\\
\end{pmatrix}
\]</span></p>
<p>To compute the elements of the matrix <span class="math inline">\(\Psi\)</span>, the following <span class="math inline">\(k\)</span> (one for each of the <span class="math inline">\(k\)</span> dimensions) <span class="math inline">\((n,n)\)</span>-matrices have to be computed: <span class="math display">\[
D_1 = \begin{pmatrix} x_{11} - x_{11} &amp; x_{11} - x_{21} &amp; x_{11} -x_{31} &amp; x_{11} - x_{41} \\  x_{21} - x_{11} &amp; x_{21} - x_{21} &amp; x_{21} -x_{31} &amp; x_{21} - x_{41} \\ x_{31} - x_{11} &amp; x_{31} - x_{21} &amp; x_{31} -x_{31} &amp; x_{31} - x_{41} \\ x_{41} - x_{11} &amp; x_{41} - x_{21} &amp; x_{41} -x_{31} &amp; x_{41} - x_{41} \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_2 = \begin{pmatrix} x_{12} - x_{12} &amp; x_{12} - x_{22} &amp; x_{12} -x_{32} &amp; x_{12} - x_{42} \\  x_{22} - x_{12} &amp; x_{22} - x_{22} &amp; x_{22} -x_{32} &amp; x_{22} - x_{42} \\ x_{32} - x_{12} &amp; x_{32} - x_{22} &amp; x_{32} -x_{32} &amp; x_{32} - x_{42} \\ x_{42} - x_{12} &amp; x_{42} - x_{22} &amp; x_{42} -x_{32} &amp; x_{42} - x_{42} \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \begin{pmatrix} x_{13} - x_{13} &amp; x_{13} - x_{23} &amp; x_{13} -x_{33} &amp; x_{13} - x_{43} \\  x_{23} - x_{13} &amp; x_{23} - x_{23} &amp; x_{23} -x_{33} &amp; x_{23} - x_{43} \\ x_{33} - x_{13} &amp; x_{33} - x_{23} &amp; x_{33} -x_{33} &amp; x_{33} - x_{43} \\ x_{43} - x_{13} &amp; x_{43} - x_{23} &amp; x_{43} -x_{33} &amp; x_{43} - x_{43} \\\end{pmatrix}
\]</span></p>
<p>Since the matrices are symmetric and the main diagonals are zero, it is sufficient to compute the following matrices: <span class="math display">\[
D_1 = \begin{pmatrix} 0 &amp; x_{11} - x_{21} &amp; x_{11} -x_{31} &amp; x_{11} - x_{41} \\  0 &amp;  0 &amp; x_{21} -x_{31} &amp; x_{21} - x_{41} \\ 0 &amp; 0 &amp; 0 &amp; x_{31} - x_{41} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span> <span class="math display">\[
D_2 = \begin{pmatrix} 0 &amp; x_{12} - x_{22} &amp; x_{12} -x_{32} &amp; x_{12} - x_{42} \\  0 &amp; 0 &amp; x_{22} -x_{32} &amp; x_{22} - x_{42} \\ 0 &amp; 0 &amp; 0 &amp; x_{32} - x_{42} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \begin{pmatrix} 0 &amp; x_{13} - x_{23} &amp; x_{13} -x_{33} &amp; x_{13} - x_{43} \\  0 &amp; 0 &amp; x_{23} -x_{33} &amp; x_{23} - x_{43} \\ 0 &amp; 0 &amp; 0 &amp; x_{33} - x_{43} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>We will consider <span class="math inline">\(p_l=2\)</span>. The differences will be squared and multiplied by <span class="math inline">\(\theta_i\)</span>, i.e.:</p>
<p><span class="math display">\[
D_1 = \theta_1 \begin{pmatrix} 0 &amp; (x_{11} - x_{21})^2 &amp; (x_{11} -x_{31})^2 &amp; (x_{11} - x_{41})^2 \\  0 &amp;  0 &amp; (x_{21} -x_{31})^2 &amp; (x_{21} - x_{41})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{31} - x_{41})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_2 = \theta_2 \begin{pmatrix} 0 &amp; (x_{12} - x_{22})^2 &amp; (x_{12} -x_{32})^2 &amp; (x_{12} - x_{42})^2 \\  0 &amp; 0 &amp; (x_{22} -x_{32})^2 &amp; (x_{22} - x_{42})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{32} - x_{42})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \theta_3 \begin{pmatrix} 0 &amp; (x_{13} - x_{23})^2 &amp; (x_{13} -x_{33})^2 &amp; (x_{13} - x_{43})^2 \\  0 &amp; 0 &amp; (x_{23} -x_{33})^2 &amp; (x_{23} - x_{43})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{33} - x_{43})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>The sum of the three matrices <span class="math inline">\(D=D_1+ D_2 + D_3\)</span> will be calculated next:</p>
<p><span class="math display">\[
\begin{pmatrix} 0 &amp;
\theta_1  (x_{11} - x_{21})^2 + \theta_2 (x_{12} - x_{22})^2 + \theta_3  (x_{13} - x_{23})^2  &amp;
\theta_1 (x_{11} -x_{31})^2 + \theta_2  (x_{12} -x_{32})^2 + \theta_3  (x_{13} -x_{33})^2 &amp;
\theta_1  (x_{11} - x_{41})^2 + \theta_2  (x_{12} - x_{42})^2 + \theta_3 (x_{13} - x_{43})^2
\\  0 &amp;  0 &amp;
\theta_1  (x_{21} -x_{31})^2 + \theta_2 (x_{22} -x_{32})^2 + \theta_3  (x_{23} -x_{33})^2 &amp;
\theta_1  x_{21} - x_{41})^2 + \theta_2  (x_{22} - x_{42})^2 + \theta_3 (x_{23} - x_{43})^2
\\ 0 &amp; 0 &amp; 0 &amp;
\theta_1 (x_{31} - x_{41})^2 + \theta_2 (x_{32} - x_{42})^2 + \theta_3 (x_{33} - x_{43})^2
\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>Finally, <span class="math display">\[ \Psi = \exp(-D)\]</span> is computed.</p>
<p>Next, we will demonstrate how this computation can be implemented in Python.</p>
<div id="41918ed0" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> (array, zeros, power, ones, exp, multiply,</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>                    eye, linspace, mat, spacing, sqrt, arange,</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>                    append, ravel)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> cholesky, solve</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>], [<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>], [<span class="dv">101</span>, <span class="dv">100</span>, <span class="dv">100</span>]])</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>array([[  1,   0,   0],
       [  0,   1,   0],
       [100, 100, 100],
       [101, 100, 100]])</code></pre>
</div>
</div>
<div id="141d7a61" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Psi(X, theta):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((k, n, n))</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, n):</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>                D[l, i, j] <span class="op">=</span> theta[l]<span class="op">*</span>(X[i,l] <span class="op">-</span> X[j,l])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="bu">sum</span>(D)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">+</span> D.T</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span>D)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="23b79d94" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>Psi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])</code></pre>
</div>
</div>
</div>
<div id="exm-corr-matrix-existing" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.22 (Example: The Correlation Matrix (Using Existing Functions))</strong></span> The same result as computed in the previous example can be obtained with existing python functions, e.g., from the package <code>scipy</code>.</p>
<div id="eb38a3ce" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> squareform</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> pdist</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Psi(X, theta, eps<span class="op">=</span>sqrt(spacing(<span class="dv">1</span>))):</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span> squareform(pdist(X,</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>                            metric<span class="op">=</span><span class="st">'sqeuclidean'</span>,</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>                            out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>                            w<span class="op">=</span>theta))) <span class="op">+</span>  multiply(eye(X.shape[<span class="dv">0</span>]),</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>                                                   eps)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta, eps<span class="op">=</span><span class="fl">.0</span>)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>Psi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])</code></pre>
</div>
</div>
</div>
</section>
<section id="the-condition-number" class="level3" data-number="6.11.3">
<h3 data-number="6.11.3" class="anchored" data-anchor-id="the-condition-number"><span class="header-section-number">6.11.3</span> The Condition Number</h3>
<p>A small value, <code>eps</code>, can be passed to the function <code>build_Psi</code> to improve the condition number. For example, <code>eps=sqrt(spacing(1))</code> can be used. The numpy function <code>spacing()</code> returns the distance between a number and its nearest adjacent number.</p>
<p>The condition number of a matrix is a measure of its sensitivity to small changes in its elements. It is used to estimate how much the output of a function will change if the input is slightly altered.</p>
<p>A matrix with a low condition number is well-conditioned, which means its behavior is relatively stable, while a matrix with a high condition number is ill-conditioned, meaning its behavior is unstable with respect to numerical precision.</p>
<div id="dfc1dc2b" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a well-conditioned matrix (low condition number)</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="fl">0.1</span>], [<span class="fl">0.1</span>, <span class="dv">1</span>]])</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Condition number of A: "</span>, np.linalg.cond(A))</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define an ill-conditioned matrix (high condition number)</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="fl">0.99999999</span>], [<span class="fl">0.99999999</span>, <span class="dv">1</span>]])</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Condition number of B: "</span>, np.linalg.cond(B))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Condition number of A:  1.2222222222222225
Condition number of B:  200000000.53159264</code></pre>
</div>
</div>
<div id="93c64b7f" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>np.linalg.cond(Psi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>2.163953413738652</code></pre>
</div>
</div>
</section>
<section id="mle-to-estimate-theta-and-p" class="level3" data-number="6.11.4">
<h3 data-number="6.11.4" class="anchored" data-anchor-id="mle-to-estimate-theta-and-p"><span class="header-section-number">6.11.4</span> MLE to estimate <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></h3>
<p>Until now, the observed data <span class="math inline">\(\vec{y}\)</span> was not used. We know what the correlations mean, but how do we estimate the values of <span class="math inline">\(\theta_j\)</span> and where does our observed data <span class="math inline">\(y\)</span> come in? To estimate the values of <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span>, they are chosen to maximize the likelihood of <span class="math inline">\(\vec{y}\)</span>, which can be expressed in terms of the sample data <span class="math display">\[L\left(\vec{Y}(\vec{x}^{(1)}), \ldots, \vec{Y}(\vec{x}^{(n)}) | \mu, \sigma \right) = \frac{1}{(2\pi \sigma)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left\{ \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}\right\},\]</span> and formulated as the log-likelihood: <span id="eq-loglikelihood-55"><span class="math display">\[
\ln(L) = - \frac{n}{2} \ln(2\pi \sigma) - \frac{1}{2} \ln |\vec{\Psi}| \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}.
\tag{6.14}\]</span></span></p>
<p>Optimization of the log-likelihood by taking derivatives with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> results in <span id="eq-muhat-55"><span class="math display">\[
\hat{\mu} = \frac{\vec{1}^T \vec{\Psi}^{-1} \vec{y}^T}{\vec{1}^T \vec{\Psi}^{-1} \vec{1}^T}
\tag{6.15}\]</span></span> and <span id="eq-sigmahat-55"><span class="math display">\[
\hat{\sigma}^2 = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{n}.
\tag{6.16}\]</span></span></p>
<p>Combining the equations, i.e., substituting <a href="#eq-muhat-55" class="quarto-xref">Equation&nbsp;<span>6.15</span></a> and <a href="#eq-sigmahat-55" class="quarto-xref">Equation&nbsp;<span>6.16</span></a> inti <a href="#eq-loglikelihood-55" class="quarto-xref">Equation&nbsp;<span>6.14</span></a> leads to the concentrated log-likelihood function: <span id="eq-concentrated-loglikelihood"><span class="math display">\[
\ln(L) = - \frac{n}{2} \ln(\hat{\sigma}) - \frac{1}{2} \ln |\vec{\Psi}|.
\tag{6.17}\]</span></span></p>
<div id="rem-concentrated-loglikelihood" class="proof remark">
<p><span class="proof-title"><em>Remark 6.1</em> (The Concentrated Log-Likelihood). </span></p>
<ul>
<li>The first term in <a href="#eq-concentrated-loglikelihood" class="quarto-xref">Equation&nbsp;<span>6.17</span></a> requires information about the measured point (observations) <span class="math inline">\(y_i\)</span>.</li>
<li>To maximize <span class="math inline">\(\ln(L)\)</span>, optimal values of <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span> are determined numerically, because the function (<a href="#eq-concentrated-loglikelihood" class="quarto-xref">Equation&nbsp;<span>6.17</span></a>) is not differentiable.</li>
</ul>
</div>
<p>The concentrated log-likelihood function is very quick to compute. We do not need a statistical model, because we are only interested in the maximum likelihood estimate (MLE) of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span>. Optimizers such as Nelder-Mead, Conjugate Gradient, or Simulated Annealing can be used to determine optimal values for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span>. After the optimization, the correlation matrix <span class="math inline">\(\Psi\)</span> is build with the optimized <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span> values. This is best (most likely) Kriging model for the given data <span class="math inline">\(y\)</span>.</p>
<p>Observing <a href="#fig-theta12" class="quarto-xref">Figure&nbsp;<span>6.8</span></a>, there’s significant change between <span class="math inline">\(\theta = 0.1\)</span> and <span class="math inline">\(\theta = 1\)</span>, just as there is between <span class="math inline">\(\theta = 1\)</span> and <span class="math inline">\(\theta = 10\)</span>. Hence, it is sensible to search for <span class="math inline">\(\theta\)</span> on a logarithmic scale. Suitable search bounds typically range from <span class="math inline">\(10^{-3}\)</span> to <span class="math inline">\(10^2\)</span>, although this is not a stringent requirement. Importantly, the scaling of the observed data does not affect the values of <span class="math inline">\(\hat{\theta}\)</span>, but the scaling of the design space does. Therefore, it is advisable to consistently scale variable ranges between zero and one to ensure consistency in the degree of activity <span class="math inline">\(\hat{\theta}_j\)</span> represents across different problems.</p>
<p>Optimizing <span class="math inline">\(\hat{\phi}\)</span> can enhance prediction accuracy across various problems.</p>
</section>
<section id="implementing-an-mle-of-the-model-parameters" class="level3" data-number="6.11.5">
<h3 data-number="6.11.5" class="anchored" data-anchor-id="implementing-an-mle-of-the-model-parameters"><span class="header-section-number">6.11.5</span> Implementing an MLE of the Model Parameters</h3>
<p>The matrix algebra necessary for calculating the likelihood is the most computationally intensive aspect of the Kriging process. It’s crucial to ensure that the code implementation is as efficient as possible.</p>
<p>Given that <span class="math inline">\(\Psi\)</span> (our correlation matrix) is symmetric, only half of the matrix needs to be computed before adding it to its transpose. When calculating the log-likelihood, several matrix inversions are required. The fastest approach is to conduct one Cholesky factorization and then apply backward and forward substitution for each inverse.</p>
<p>The Cholesky factorization is applicable only to positive-definite matrices, which <span class="math inline">\(\Psi\)</span> generally is. However, if <span class="math inline">\(\Psi\)</span> becomes nearly singular, such as when the <span class="math inline">\(\mathbf{x}^{(i)}\)</span>’s are densely packed, the Cholesky factorization might fail. In these cases, one could employ an LU-decomposition, though the result might be unreliable. When <span class="math inline">\(\Psi\)</span> is near singular, the best course of action is to either use regression techniques or, as we do here, assign a poor likelihood value to parameters generating the near singular matrix, thus diverting the MLE search towards better-conditioned <span class="math inline">\(\Psi\)</span> matrices.</p>
<p>Another consideration in calculating the concentrated log-likelihood is that <span class="math inline">\(\det(\Psi) \rightarrow 0\)</span> for poorly conditioned matrices, so it is advisable to use twice the sum of the logarithms of the diagonal of the Cholesky factorization when calculating <span class="math inline">\(\ln(\lvert\Psi\rvert)\)</span> in <a href="#eq-concentrated-loglikelihood" class="quarto-xref">Equation&nbsp;<span>6.17</span></a>.</p>
</section>
<section id="kriging-prediction" class="level3" data-number="6.11.6">
<h3 data-number="6.11.6" class="anchored" data-anchor-id="kriging-prediction"><span class="header-section-number">6.11.6</span> Kriging Prediction</h3>
<p>We will use the Kriging correlation <span class="math inline">\(\Psi\)</span> to predict new values based on the observed data. The matrix algebra involved for calculating the likelihood is the most computationally intensive part of the Kriging process. Care must be taken that the computer code is as efficient as possible.</p>
<p>Basic elements of the Kriging based surrogate optimization such as interpolation, expected improvement, and regression are presented. The presentation follows the approach described in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span> and <span class="citation" data-cites="bart21i">Bartz et al. (<a href="references.html#ref-bart21i" role="doc-biblioref">2022</a>)</span>.</p>
<p>Main idea for prediction is that the new <span class="math inline">\(Y(\vec{x})\)</span> should be consistent with the old sample data <span class="math inline">\(X\)</span>. For a new prediction <span class="math inline">\(\hat{y}\)</span> at <span class="math inline">\(\vec{x}\)</span>, the value of <span class="math inline">\(\hat{y}\)</span> is chosen so that it maximizes the likelihood of the sample data <span class="math inline">\(\vec{X}\)</span> and the prediction, given the (optimized) correlation parameter <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span> from above. The observed data <span class="math inline">\(\vec{y}\)</span> is augmented with the new prediction <span class="math inline">\(\hat{y}\)</span> which results in the augmented vector <span class="math inline">\(\vec{\tilde{y}} = ( \vec{y}^T, \hat{y})^T\)</span>. A vector of correlations between the observed data and the new prediction is defined as</p>
<p><span class="math display">\[ \vec{\psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(1)}),
\vec{Y}(\vec{x})
\right) \\
\vdots  \\
\text{cor}\left(
\vec{Y}(\vec{x}^{(n)}),
\vec{Y}(\vec{x})
\right)
\end{pmatrix}
=
\begin{pmatrix}
\vec{\psi}^{(1)}\\
\vdots\\
\vec{\psi}^{(n)}
\end{pmatrix}.
\]</span></p>
<div id="def-augmented-correlation-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.29 (The Augmented Correlation Matrix)</strong></span> The augmented correlation matrix is constructed as <span class="math display">\[ \tilde{\vec{\Psi}} =
\begin{pmatrix}
\vec{\Psi} &amp; \vec{\psi} \\
\vec{\psi}^T &amp; 1
\end{pmatrix}.
\]</span></p>
</div>
<p>The log-likelihood of the augmented data is <span id="eq-loglikelihood-augmented"><span class="math display">\[
\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\hat{\sigma}^2) - \frac{1}{2} \ln |\vec{\hat{\Psi}}| -  \frac{(\vec{\tilde{y}} - \vec{1}\hat{\mu})^T \vec{\tilde{\Psi}}^{-1}(\vec{\tilde{y}} - \vec{1}\hat{\mu})}{2 \hat{\sigma}^2},
\tag{6.18}\]</span></span></p>
<p>where <span class="math inline">\(\vec{1}\)</span> is a vector of ones and <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span> are the MLEs from <a href="#eq-muhat-55" class="quarto-xref">Equation&nbsp;<span>6.15</span></a> and <a href="#eq-sigmahat-55" class="quarto-xref">Equation&nbsp;<span>6.16</span></a>. Only the last term in <a href="#eq-loglikelihood-augmented" class="quarto-xref">Equation&nbsp;<span>6.18</span></a> depends on <span class="math inline">\(\hat{y}\)</span>, so we need only consider this term in the maximization. Details cen be found in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>. Finally, the MLE for <span class="math inline">\(\hat{y}\)</span> can be calculated as <span id="eq-mle-yhat"><span class="math display">\[
\hat{y}(\vec{x}) = \hat{\mu} + \vec{\psi}^T \vec{\tilde{\Psi}}^{-1} (\vec{y} - \vec{1}\hat{\mu}).
\tag{6.19}\]</span></span></p>
<div id="lem-mle-yhat" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 6.1 (Properties of the Predictor)</strong></span> <a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>6.19</span></a> reveals two important properties of the Kriging predictor:</p>
<ol type="1">
<li>Basis functions: The basis function impacts the vector <span class="math inline">\(\vec{\psi}\)</span>, which contains the <span class="math inline">\(n\)</span> correlations between the new point <span class="math inline">\(\vec{x}\)</span> and the observed locations. Values from the <span class="math inline">\(n\)</span> basis functions are added to a mean base term <span class="math inline">\(\mu\)</span> with weightings <span class="math inline">\(\vec{w} = \vec{\tilde{\Psi}}^{(-1)} (\vec{y} - \vec{1}\hat{\mu})\)</span>.</li>
<li>Interpolation: The predictions interpolate the sample data. When calculating the prediction at the <span class="math inline">\(i\)</span>th sample point, <span class="math inline">\(\vec{x}^{(i)}\)</span>, the <span class="math inline">\(i\)</span>th column of <span class="math inline">\(\vec{\Psi}^{-1}\)</span> is <span class="math inline">\(\vec{\psi}\)</span>, and <span class="math inline">\(\vec{\psi}  \vec{\Psi}^{-1}\)</span> is the <span class="math inline">\(i\)</span>th unit vector. Hence, <span class="math inline">\(\hat{y}(\vec{x}^{(i)}) = y^{(i)}\)</span>.</li>
</ol>
</div>
</section>
</section>
<section id="kriging-example-sinusoid-function" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="kriging-example-sinusoid-function"><span class="header-section-number">6.12</span> Kriging Example: Sinusoid Function</h2>
<p>Toy example in 1d where the response is a simple sinusoid measured at eight equally spaced <span class="math inline">\(x\)</span>-locations in the span of a single period of oscillation.</p>
<section id="calculating-the-correlation-matrix-psi" class="level3" data-number="6.12.1">
<h3 data-number="6.12.1" class="anchored" data-anchor-id="calculating-the-correlation-matrix-psi"><span class="header-section-number">6.12.1</span> Calculating the Correlation Matrix <span class="math inline">\(\Psi\)</span></h3>
<p>The correlation matrix <span class="math inline">\(\Psi\)</span> is based on the pairwise squared distances between the input locations. Here we will use <span class="math inline">\(n=8\)</span> sample locations and <span class="math inline">\(\theta\)</span> is set to 1.0.</p>
<div id="c30675c2" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># theta should be an array (of one value, for the moment, will be changed later)</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="fl">1.0</span>])</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Evaluate at sample points</p>
<div id="73e9292a" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="6c4ce281" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"bo"</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Sin(x) evaluated at </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> points"</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-42-output-1.png" width="590" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="computing-the-psi-vector" class="level3" data-number="6.12.2">
<h3 data-number="6.12.2" class="anchored" data-anchor-id="computing-the-psi-vector"><span class="header-section-number">6.12.2</span> Computing the <span class="math inline">\(\psi\)</span> Vector</h3>
<p>Distances between testing locations <span class="math inline">\(x\)</span> and training data locations <span class="math inline">\(X\)</span>.</p>
<div id="23f433d8" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_psi(X, x, theta, eps<span class="op">=</span>sqrt(spacing(<span class="dv">1</span>))):</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> zeros((n, m))</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">*</span> ones(k)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((n, m))</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> cdist(x.reshape(<span class="op">-</span><span class="dv">1</span>, k),</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>              X.reshape(<span class="op">-</span><span class="dv">1</span>, k),</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>              metric<span class="op">=</span><span class="st">'sqeuclidean'</span>,</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>              out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>              w<span class="op">=</span>theta)</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(D.shape)</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> exp(<span class="op">-</span>D)</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return psi transpose to be consistent with the literature</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(psi.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="predicting-at-new-locations" class="level3" data-number="6.12.3">
<h3 data-number="6.12.3" class="anchored" data-anchor-id="predicting-at-new-locations"><span class="header-section-number">6.12.3</span> Predicting at New Locations</h3>
<p>We would like to predict at <span class="math inline">\(m = 100\)</span> new locations in the interval <span class="math inline">\([0, 2\pi]\)</span>. The new locations are stored in the variable <code>x</code>.</p>
<div id="f7426bf3" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, m, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>psi <span class="op">=</span> build_psi(X, x, theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(100, 8)</code></pre>
</div>
</div>
<p>Computation of the predictive equations. CV</p>
<div id="0d881f8a" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> cholesky(Psi).T</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>one <span class="op">=</span> np.ones(n).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> (one.T.dot(solve(U, solve(U.T, y)))) <span class="op">/</span> one.T.dot(solve(U, solve(U.T, one)))</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> mu <span class="op">*</span> ones(m).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="op">+</span> psi.T.dot(solve(U, solve(U.T, y <span class="op">-</span> one <span class="op">*</span> mu)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To compute <span class="math inline">\(f\)</span>, <a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>6.19</span></a> is used.</p>
</section>
<section id="visualization" class="level3" data-number="6.12.4">
<h3 data-number="6.12.4" class="anchored" data-anchor-id="visualization"><span class="header-section-number">6.12.4</span> Visualization</h3>
<div id="24b7dc29" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x, f, color <span class="op">=</span> <span class="st">"orange"</span>, label<span class="op">=</span><span class="st">"Fitted"</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, np.sin(x), color <span class="op">=</span> <span class="st">"grey"</span>, label<span class="op">=</span><span class="st">"Original"</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"bo"</span>, label<span class="op">=</span><span class="st">"Measurements"</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Kriging prediction of sin(x) with </span><span class="sc">{}</span><span class="st"> points.</span><span class="ch">\n</span><span class="st"> theta: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(n, theta[<span class="dv">0</span>]))</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-46-output-1.png" width="590" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="cholesky-decomposition-2" class="level2" data-number="6.13">
<h2 data-number="6.13" class="anchored" data-anchor-id="cholesky-decomposition-2"><span class="header-section-number">6.13</span> Cholesky Decomposition</h2>
<section id="example-of-cholesky-decomposition" class="level3" data-number="6.13.1">
<h3 data-number="6.13.1" class="anchored" data-anchor-id="example-of-cholesky-decomposition"><span class="header-section-number">6.13.1</span> Example of Cholesky Decomposition</h3>
<p>We consider dimension <span class="math inline">\(k=1\)</span> and <span class="math inline">\(n=2\)</span> sample points. The sample points are located at <span class="math inline">\(x_1=1\)</span> and <span class="math inline">\(x_2=5\)</span>. The response values are <span class="math inline">\(y_1=2\)</span> and <span class="math inline">\(y_2=10\)</span>. The correlation parameter is <span class="math inline">\(\theta=1\)</span> and <span class="math inline">\(p\)</span> is set to <span class="math inline">\(1\)</span>. Using <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>6.9</span></a>, we can compute the correlation matrix <span class="math inline">\(\Psi\)</span>:</p>
<p><span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; e^{-1}\\
e^{-1} &amp; 1
\end{pmatrix}.
\]</span></p>
<p>To determine MLE as in <a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>6.19</span></a>, we need to compute <span class="math inline">\(\Psi^{-1}\)</span>:</p>
<p><span class="math display">\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e &amp; -1\\
-1 &amp; e
\end{pmatrix}.
\]</span></p>
<p>Cholesky-decomposition of <span class="math inline">\(\Psi\)</span> is recommended to compute <span class="math inline">\(\Psi^{-1}\)</span>. Cholesky decomposition is a decomposition of a positive definite symmetric matrix into the product of a lower triangular matrix <span class="math inline">\(L\)</span>, a diagonal matrix <span class="math inline">\(D\)</span> and the transpose of <span class="math inline">\(L\)</span>, which is denoted as <span class="math inline">\(L^T\)</span>. Consider the following example:</p>
<p><span class="math display">\[
LDL^T=
\begin{pmatrix}
1 &amp; 0 \\
l_{21} &amp; 1
\end{pmatrix}
\begin{pmatrix}
d_{11} &amp; 0 \\
0 &amp; d_{22}
\end{pmatrix}
\begin{pmatrix}
1 &amp; l_{21} \\
0 &amp; 1
\end{pmatrix}=
\]</span></p>
<p><span id="eq-cholex"><span class="math display">\[
\begin{pmatrix}
d_{11} &amp; 0 \\
d_{11} l_{21} &amp; d_{22}
\end{pmatrix}
\begin{pmatrix}
1 &amp; l_{21} \\
0 &amp; 1
\end{pmatrix}
=
\begin{pmatrix}
d_{11} &amp; d_{11} l_{21} \\
d_{11} l_{21} &amp; d_{11} l_{21}^2 + d_{22}
\end{pmatrix}.
\tag{6.20}\]</span></span></p>
<p>Using <a href="#eq-cholex" class="quarto-xref">Equation&nbsp;<span>6.20</span></a>, we can compute the Cholesky decomposition of <span class="math inline">\(\Psi\)</span>:</p>
<ol type="1">
<li><span class="math inline">\(d_{11} = 1\)</span>,</li>
<li><span class="math inline">\(l_{21}d_{11} = e^{-1} \Rightarrow l_{21} = e^{-1}\)</span>, and</li>
<li><span class="math inline">\(d_{11} l_{21}^2 + d_{22} = 1 \Rightarrow d_{22} = 1 - e^{-2}\)</span>.</li>
</ol>
<p>The Cholesky decomposition of <span class="math inline">\(\Psi\)</span> is <span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; 0\\
e^{-1} &amp; 1\\
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0\\
0 &amp; 1 - e^{-2}\\
\end{pmatrix}
\begin{pmatrix}
1 &amp; e^{-1}\\
0 &amp; 1\\
\end{pmatrix}
= LDL^T\]</span></p>
<p>Some programs use <span class="math inline">\(U\)</span> instead of <span class="math inline">\(L\)</span>. The Cholesky decomposition of <span class="math inline">\(\Psi\)</span> is <span class="math display">\[
\Psi = LDL^T = U^TDU.
\]</span></p>
<p>Using <span class="math display">\[
\sqrt{D} =\begin{pmatrix}
1 &amp; 0\\
0 &amp; \sqrt{1 - e^{-2}}\\
\end{pmatrix},
\]</span> we can write the Cholesky decomposition of <span class="math inline">\(\Psi\)</span> without a diagonal matrix <span class="math inline">\(D\)</span> as <span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; 0\\
e^{-1} &amp; \sqrt{1 - e^{-2}}\\
\end{pmatrix}
\begin{pmatrix}
1 &amp; e^{-1}\\
0 &amp; \sqrt{1 - e^{-2}}\\
\end{pmatrix}
= U^TU.
\]</span></p>
</section>
<section id="inverse-matrix-using-cholesky-decomposition" class="level3" data-number="6.13.2">
<h3 data-number="6.13.2" class="anchored" data-anchor-id="inverse-matrix-using-cholesky-decomposition"><span class="header-section-number">6.13.2</span> Inverse Matrix Using Cholesky Decomposition</h3>
<p>To compute the inverse of a matrix using the Cholesky decomposition, you can follow these steps:</p>
<ol type="1">
<li>Decompose the matrix <span class="math inline">\(A\)</span> into <span class="math inline">\(L\)</span> and <span class="math inline">\(L^T\)</span>, where <span class="math inline">\(L\)</span> is a lower triangular matrix and <span class="math inline">\(L^T\)</span> is the transpose of <span class="math inline">\(L\)</span>.</li>
<li>Compute <span class="math inline">\(L^{-1}\)</span>, the inverse of <span class="math inline">\(L\)</span>.</li>
<li>The inverse of <span class="math inline">\(A\)</span> is then <span class="math inline">\((L^{-1})^T  L^-1\)</span>.</li>
</ol>
<p>Please note that this method only applies to symmetric, positive-definite matrices.</p>
<p>The inverse of the matrix <span class="math inline">\(\Psi\)</span> from above is:</p>
<p><span class="math display">\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e &amp; -1\\
-1 &amp; e
\end{pmatrix}.
\]</span></p>
<p>Here’s an example of how to compute the inverse of a matrix using Cholesky decomposition in Python:</p>
<div id="bcd12cae" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> cholesky, inv</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> np.exp(<span class="dv">1</span>)</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Psi is a symmetric, positive-definite matrix </span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span><span class="op">/</span>E], [<span class="dv">1</span><span class="op">/</span>E, <span class="dv">1</span>]])</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> cholesky(Psi, lower<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>L_inv <span class="op">=</span> inv(L)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="co"># The inverse of A is (L^-1)^T * L^-1</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>Psi_inv <span class="op">=</span> np.dot(L_inv.T, L_inv)</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Psi:</span><span class="ch">\n</span><span class="st">"</span>, Psi)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Psi Inverse:</span><span class="ch">\n</span><span class="st">"</span>, Psi_inv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Psi:
 [[1.         0.36787944]
 [0.36787944 1.        ]]
Psi Inverse:
 [[ 1.15651764 -0.42545906]
 [-0.42545906  1.15651764]]</code></pre>
</div>
</div>
</section>
</section>
<section id="gaussian-processessome-background-information" class="level2" data-number="6.14">
<h2 data-number="6.14" class="anchored" data-anchor-id="gaussian-processessome-background-information"><span class="header-section-number">6.14</span> Gaussian Processes—Some Background Information</h2>
<p>The concept of GP (Gaussian Process) regression can be understood as a simple extension of linear modeling. It is worth noting that this approach goes by various names and acronyms, including “kriging,” a term derived from geostatistics, as introduced by Matheron in 1963. Additionally, it is referred to as Gaussian spatial modeling or a Gaussian stochastic process, and machine learning (ML) researchers often use the term Gaussian process regression (GPR). In all of these instances, the central focus is on regression. This involves training on both inputs and outputs, with the ultimate objective of making predictions and quantifying uncertainty (referred to as uncertainty quantification or UQ).</p>
<p>However, it’s important to emphasize that GPs are not a universal solution for every problem. Specialized tools may outperform GPs in specific, non-generic contexts, and GPs have their own set of limitations that need to be considered.</p>
<section id="gaussian-process-prior" class="level3" data-number="6.14.1">
<h3 data-number="6.14.1" class="anchored" data-anchor-id="gaussian-process-prior"><span class="header-section-number">6.14.1</span> Gaussian Process Prior</h3>
<p>In the context of GP, any finite collection of realizations, which is represented by <span class="math inline">\(n\)</span> observations, is modeled as having a multivariate normal (MVN) distribution. The characteristics of these realizations can be fully described by two key parameters:</p>
<ol type="1">
<li>Their mean, denoted as an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mu\)</span>.</li>
<li>The covariance matrix, denoted as an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\Sigma\)</span>. This covariance matrix encapsulates the relationships and variability between the individual realizations within the collection.</li>
</ol>
</section>
<section id="covariance-function" class="level3" data-number="6.14.2">
<h3 data-number="6.14.2" class="anchored" data-anchor-id="covariance-function"><span class="header-section-number">6.14.2</span> Covariance Function</h3>
<p>The covariance function is defined by inverse exponentiated squared Euclidean distance: <span class="math display">\[
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 \},
\]</span> where <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{x}'\)</span> are two points in the <span class="math inline">\(k\)</span>-dimensional input space and <span class="math inline">\(\| \cdot \|\)</span> denotes the Euclidean distance, i.e., <span class="math display">\[
|| \vec{x} - \vec{x}'||^2 = \sum_{i=1}^k (x_i - x_i')^2.
\]</span></p>
<p>An 1-d example is shown in <a href="#fig-exp2euclid" class="quarto-xref">Figure&nbsp;<span>6.9</span></a>.</p>
<div id="cell-fig-exp2euclid" class="cell" data-execution_count="48">
<div class="cell-output cell-output-display">
<div id="fig-exp2euclid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-exp2euclid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-exp2euclid-output-1.png" width="571" height="411" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exp2euclid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: One-dim inverse exponentiated squared Euclidean distance
</figcaption>
</figure>
</div>
</div>
</div>
<p>The covariance function is also referred to as the kernel function. The <em>Gaussian</em> kernel uses an additional parameter, <span class="math inline">\(\sigma^2\)</span>, to control the rate of decay. This parameter is referred to as the length scale or the characteristic length scale. The covariance function is then defined as</p>
<p><span id="eq-Sigma"><span class="math display">\[
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 / (2 \sigma^2) \}.
\tag{6.21}\]</span></span></p>
<p>The covariance decays exponentially fast as <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{x}'\)</span> become farther apart. Observe that</p>
<p><span class="math display">\[
\Sigma(\vec{x},\vec{x}) = 1
\]</span> and</p>
<p><span class="math display">\[
\Sigma(\vec{x}, \vec{x}') &lt; 1
\]</span> for <span class="math inline">\(\vec{x} \neq \vec{x}'\)</span>. The function <span class="math inline">\(\Sigma(\vec{x},\vec{x}')\)</span> must be positive definite.</p>
<div id="rem-krigingbase-gauss" class="proof remark">
<p><span class="proof-title"><em>Remark 6.2</em> (Kriging and Gaussian Basis Functions). </span>The Kriging basis function (<a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>6.9</span></a>) is related to the 1-dim Gaussian basis function (<a href="#eq-Sigma" class="quarto-xref">Equation&nbsp;<span>6.21</span></a>), which is defined as <span id="eq-Sigma2"><span class="math display">\[
\Sigma(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp\{ - || \vec{x}^{(i)} - \vec{x}^{(j)}||^2 / (2\sigma^2) \}.
\tag{6.22}\]</span></span></p>
<p>There are some differences between Gaussian basis functions and Kriging basis functions:</p>
<ul>
<li>Where the Gaussian basis function has <span class="math inline">\(1/(2\sigma^2)\)</span>, the Kriging basis has a vector <span class="math inline">\(\theta = [\theta_1, \theta_2, \ldots, \theta_k]^T\)</span>.</li>
<li>The <span class="math inline">\(\theta\)</span> vector allows the width of the basis function to vary from dimension to dimension.</li>
<li>In the Gaussian basis function, the exponent is fixed at 2, Kriging allows this exponent <span class="math inline">\(p_l\)</span> to vary (typically from 1 to 2).</li>
</ul>
</div>
<section id="positive-definiteness" class="level4" data-number="6.14.2.1">
<h4 data-number="6.14.2.1" class="anchored" data-anchor-id="positive-definiteness"><span class="header-section-number">6.14.2.1</span> Positive Definiteness</h4>
<p>Positive definiteness in the context of the covariance matrix <span class="math inline">\(\Sigma_n\)</span> is a fundamental requirement. It is determined by evaluating <span class="math inline">\(\Sigma(x_i, x_j)\)</span> at pairs of <span class="math inline">\(n\)</span> <span class="math inline">\(\vec{x}\)</span>-values, denoted as <span class="math inline">\(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\)</span>. The condition for positive definiteness is that for all <span class="math inline">\(\vec{x}\)</span> vectors that are not equal to zero, the expression <span class="math inline">\(\vec{x}^\top \Sigma_n \vec{x}\)</span> must be greater than zero. This property is essential when intending to use <span class="math inline">\(\Sigma_n\)</span> as a covariance matrix in multivariate normal (MVN) analysis. It is analogous to the requirement in univariate Gaussian distributions where the variance parameter, <span class="math inline">\(\sigma^2\)</span>, must be positive.</p>
<p>Gaussian Processes (GPs) can be effectively utilized to generate random data that follows a smooth functional relationship. The process involves the following steps:</p>
<ol type="1">
<li>Select a set of <span class="math inline">\(\vec{x}\)</span>-values, denoted as <span class="math inline">\(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\)</span>.</li>
<li>Define the covariance matrix <span class="math inline">\(\Sigma_n\)</span> by evaluating <span class="math inline">\(\Sigma_n^{ij} = \Sigma(\vec{x}_i, \vec{x}_j)\)</span> for <span class="math inline">\(i, j = 1, 2, \ldots, n\)</span>.</li>
<li>Generate an <span class="math inline">\(n\)</span>-variate realization <span class="math inline">\(Y\)</span> that follows a multivariate normal distribution with a mean of zero and a covariance matrix <span class="math inline">\(\Sigma_n\)</span>, expressed as <span class="math inline">\(Y \sim \mathcal{N}_n(0, \Sigma_n)\)</span>.</li>
<li>Visualize the result by plotting it in the <span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span> plane.</li>
</ol>
</section>
</section>
<section id="construction-of-the-covariance-matrix" class="level3" data-number="6.14.3">
<h3 data-number="6.14.3" class="anchored" data-anchor-id="construction-of-the-covariance-matrix"><span class="header-section-number">6.14.3</span> Construction of the Covariance Matrix</h3>
<p>Here is an one-dimensional example. The process begins by creating an input grid using <span class="math inline">\(\vec{x}\)</span>-values. This grid consists of 100 elements, providing the basis for further analysis and visualization.</p>
<div id="674235ba" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the context of this discussion, the construction of the covariance matrix, denoted as <span class="math inline">\(\Sigma_n\)</span>, relies on the concept of inverse exponentiated squared Euclidean distances. However, it’s important to note that a modification is introduced later in the process. Specifically, the diagonal of the covariance matrix is augmented with a small value, represented as “eps” or <span class="math inline">\(\epsilon\)</span>.</p>
<p>The reason for this augmentation is that while inverse exponentiated distances theoretically ensure the covariance matrix’s positive definiteness, in practical applications, the matrix can sometimes become numerically ill-conditioned. By adding a small value to the diagonal, such as <span class="math inline">\(\epsilon\)</span>, this ill-conditioning issue is mitigated. In this context, <span class="math inline">\(\epsilon\)</span> is often referred to as “jitter.”</p>
<div id="1534cee8" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> cholesky, solve</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> multivariate_normal</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Sigma(X, sigma2):</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((k, n, n))</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, n):</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>                D[l, i, j] <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma2[l])<span class="op">*</span>(X[i,l] <span class="op">-</span> X[j,l])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="bu">sum</span>(D)</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">+</span> D.T</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span>D)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="98c6c34f" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> np.array([<span class="fl">1.0</span>])</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> build_Sigma(X, sigma2)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(Sigma[:<span class="dv">3</span>,:], <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>array([[1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783, 0.726,
        0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278, 0.236,
        0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044, 0.034,
        0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003, 0.002,
        0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783,
        0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278,
        0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044,
        0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003,
        0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.98 , 0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835,
        0.783, 0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325,
        0.278, 0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056,
        0.044, 0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004,
        0.003, 0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ]])</code></pre>
</div>
</div>
<div id="b9b6a786" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(Sigma, cmap<span class="op">=</span><span class="st">'hot'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-53-output-1.png" width="490" height="416" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="generation-of-random-samples-and-plotting-the-realizations-of-the-random-function" class="level3" data-number="6.14.4">
<h3 data-number="6.14.4" class="anchored" data-anchor-id="generation-of-random-samples-and-plotting-the-realizations-of-the-random-function"><span class="header-section-number">6.14.4</span> Generation of Random Samples and Plotting the Realizations of the Random Function</h3>
<p>In the context of the multivariate normal distribution, the next step is to utilize the previously constructed covariance matrix denoted as <code>Sigma</code>. It is used as an essential component in generating random samples from the multivariate normal distribution.</p>
<p>The function <code>multivariate_normal</code> is employed for this purpose. It serves as a random number generator specifically designed for the multivariate normal distribution. In this case, the mean of the distribution is set equal to <code>mean</code>, and the covariance matrix is provided as <code>Psi</code>. The argument <code>size</code> specifies the number of realizations, which, in this specific scenario, is set to one.</p>
<p>By default, the mean vector is initialized to zero. To match the number of samples, which is equivalent to the number of rows in the <code>X</code> and <code>Sigma</code> matrices, the argument <code>zeros(n)</code> is used, where <code>n</code> represents the number of samples (here taken from the size of the matrix, e.g.,: <code>Sigma.shape[0]</code>).</p>
<div id="68ade195" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">12345</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> rng.multivariate_normal(zeros(Sigma.shape[<span class="dv">0</span>]), Sigma, size <span class="op">=</span> <span class="dv">1</span>, check_valid<span class="op">=</span><span class="st">"raise"</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>Y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>(100, 1)</code></pre>
</div>
</div>
<p>Now we can plot the results, i.e., a finite realization of the random function <span class="math inline">\(Y()\)</span> under a GP prior with a particular covariance structure. We will plot those <code>X</code> and <code>Y</code> pairs as connected points on an <span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span> plane.</p>
<div id="cell-fig-mvn1-1" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, Y)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Realization of Random Functions under a GP prior.</span><span class="ch">\n</span><span class="st"> sigma2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(sigma2[<span class="dv">0</span>]))</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn1-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn1-1-output-1.png" width="582" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: Realization of one random function under a GP prior. sigma2: 1.0
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-mvn1-3" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">12345</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> rng.multivariate_normal(zeros(Sigma.shape[<span class="dv">0</span>]), Sigma, size <span class="op">=</span> <span class="dv">3</span>, check_valid<span class="op">=</span><span class="st">"raise"</span>)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>plt.plot(X, Y.T)</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Realization of Three Random Functions under a GP prior.</span><span class="ch">\n</span><span class="st"> sigma2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(sigma2[<span class="dv">0</span>]))</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn1-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn1-3-output-1.png" width="569" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.11: Realization of three random functions under a GP prior. sigma2: 1.0
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="properties-of-the-1d-example" class="level3" data-number="6.14.5">
<h3 data-number="6.14.5" class="anchored" data-anchor-id="properties-of-the-1d-example"><span class="header-section-number">6.14.5</span> Properties of the 1d Example</h3>
<section id="several-bumps" class="level4" data-number="6.14.5.1">
<h4 data-number="6.14.5.1" class="anchored" data-anchor-id="several-bumps"><span class="header-section-number">6.14.5.1</span> Several Bumps:</h4>
<p>In this analysis, we observe several bumps in the <span class="math inline">\(x\)</span>-range of <span class="math inline">\([0,10]\)</span>. These bumps in the function occur because shorter distances exhibit high correlation, while longer distances tend to be essentially uncorrelated. This leads to variations in the function’s behavior:</p>
<ul>
<li>When <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> are one <span class="math inline">\(\sigma\)</span> unit apart, the correlation is <span class="math inline">\(\exp\left(-\sigma^2 / (2\sigma^2)\right) = \exp(-1/2) \approx 0.61\)</span>, i.e., a relative high correlation.</li>
<li><span class="math inline">\(2\sigma\)</span> apart means correlation <span class="math inline">\(\exp(− 2^2 /2) \approx 0.14\)</span>, i.e., only small correlation.</li>
<li><span class="math inline">\(4\sigma\)</span> apart means correlation <span class="math inline">\(\exp(− 4^2 /2) \approx 0.0003\)</span>, i.e., nearly no correlation—variables are considered independent for almost all practical application.</li>
</ul>
</section>
<section id="smoothness" class="level4" data-number="6.14.5.2">
<h4 data-number="6.14.5.2" class="anchored" data-anchor-id="smoothness"><span class="header-section-number">6.14.5.2</span> Smoothness:</h4>
<p>The function plotted in <a href="#fig-mvn1-1" class="quarto-xref">Figure&nbsp;<span>6.10</span></a> represents only a finite realization, which means that we have data for a limited number of pairs, specifically 100 points. These points appear smooth in a tactile sense because they are closely spaced, and the plot function connects the dots with lines to create the appearance of smoothness. The complete surface, which can be conceptually extended to an infinite realization over a compact domain, is exceptionally smooth in a calculus sense due to the covariance function’s property of being infinitely differentiable.</p>
</section>
<section id="scale-of-two" class="level4" data-number="6.14.5.3">
<h4 data-number="6.14.5.3" class="anchored" data-anchor-id="scale-of-two"><span class="header-section-number">6.14.5.3</span> Scale of Two:</h4>
<p>Regarding the scale of the <span class="math inline">\(Y\)</span> values, they have a range of approximately <span class="math inline">\([-2,2]\)</span>, with a 95% probability of falling within this range. In standard statistical terms, 95% of the data points typically fall within two standard deviations of the mean, which is a common measure of the spread or range of data.</p>
<div id="dde49d33" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> multivariate_normal</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Sigma(X, sigma2):</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((k, n, n))</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, n):</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>                D[l, i, j] <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma2[l])<span class="op">*</span>(X[i,l] <span class="op">-</span> X[j,l])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="bu">sum</span>(D)</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">+</span> D.T</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span>D)</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_mvn( a<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">10</span>, sigma2<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span><span class="dv">1</span>, n<span class="op">=</span><span class="dv">100</span>, show<span class="op">=</span><span class="va">True</span>):    </span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.linspace(a, b, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>    sigma2 <span class="op">=</span> np.array([sigma2])</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>    Sigma <span class="op">=</span> build_Sigma(X, sigma2)</span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">12345</span>)</span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> rng.multivariate_normal(zeros(Sigma.shape[<span class="dv">0</span>]), Sigma, size <span class="op">=</span> size, check_valid<span class="op">=</span><span class="st">"raise"</span>)</span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, Y.T)</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Realization of Random Functions under a GP prior.</span><span class="ch">\n</span><span class="st"> sigma2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(sigma2[<span class="dv">0</span>]))</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> show:</span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a>        plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-fig-mvn2" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>plot_mvn(a<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">10</span>, sigma2<span class="op">=</span><span class="fl">10.0</span>, size<span class="op">=</span><span class="dv">3</span>, n<span class="op">=</span><span class="dv">250</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn2-output-1.png" width="569" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.12: Realization of Random Functions under a GP prior. sigma2: 10
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-mvn5" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>plot_mvn(a<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">10</span>, sigma2<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">3</span>, n<span class="op">=</span><span class="dv">250</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn5-output-1.png" width="569" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.13: Realization of Random Functions under a GP prior. sigma2: 0.1
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="jupyter-notebook" class="level2" data-number="6.15">
<h2 data-number="6.15" class="anchored" data-anchor-id="jupyter-notebook"><span class="header-section-number">6.15</span> Jupyter Notebook</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The Jupyter-Notebook of this lecture is available on GitHub in the <a href="https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_num_gp.ipynb">Hyperparameter-Tuning-Cookbook Repository</a></li>
</ul>
</div>
</div>
<!-- 

::: {#361a581b .cell execution_count=59}
``` {.python .cell-code}
from spotpython.build.kriging import Kriging
import numpy as np
nat_X = np.array([[1], [2]])
nat_y = np.array([5, 10])
n=2
p=1
S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)
S.initialize_variables(nat_X, nat_y)
S.set_variable_types()
S.set_theta_values()
S.initialize_matrices()
S.build_Psi()
S.build_U()
S.likelihood()
# assert S.mu is close to 7.5 with a tolerance of 1e-6
assert np.allclose(S.mu, 7.5, atol=1e-6)
E = np.exp(1)
sigma2 = E/(E**2 -1) * (25/4 + 25/4*E)
# asssert S.SigmaSqr is close to sigma2 with a tolerance of 1e-6
assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)
```
:::


-->
<!-- 

## Exercises

### 1 Number of Sample Points

* The example uses $n=8$ sample points to fit the sin function.
  * What happens, if less than 8 samples are available?

### 2 Modified $\theta$ values

* The example uses a $\theta$ value of $1.0$.
  * What happens if $\theta$ is modified?
  * Can get better predictions with smaller or larger $\theta$ values?

### 3 Prediction Interval

* The prediction interval was identical to the measurement interval, i.e., in the range from $0$ to $2\pi$. This is referred to as "interpolation".
  * What happens if this interval is increased (which is referred to as "extrapolation")?



### Exercise RBF


#### Package Loading

::: {#eb1e1fff .cell execution_count=60}
``` {.python .cell-code}
%matplotlib inline
import numpy as np
from numpy.matlib import eye
import scipy.linalg
from numpy import linalg as LA
from spotpython.design.spacefilling import SpaceFilling
from spotpython.fun.objectivefunctions import analytical
import matplotlib.pyplot as plt
```
:::


#### Define a small number

::: {#63bda535 .cell execution_count=61}
``` {.python .cell-code}
eps = np.sqrt(np.spacing(1))
```
:::


#### The Sampling Plan (X)

* We will use 256 points.
* The first 10 points are shown below.

::: {#defadda6 .cell execution_count=62}
``` {.python .cell-code}
gen = SpaceFilling(2)
rng = np.random.RandomState(1)
lower = np.array([-1,-1])
upper = np.array([2,2])
X = gen.scipy_lhd(256, lower=lower, upper = upper)
X[1:10]
```

::: {.cell-output .cell-output-display execution_count=62}
```
array([[ 0.67319891, -0.11153561],
       [ 0.4979386 , -0.10717298],
       [-0.9991017 ,  1.66863389],
       [-0.423669  , -0.97527218],
       [-0.57241762,  1.77447307],
       [ 1.34580967,  0.62640122],
       [ 0.18662343,  0.18793039],
       [ 1.7664757 ,  1.65743858],
       [ 0.98282275,  0.42361525]])
```
:::
:::


#### The Objective Function

* Here we use $\sum_{i=1}^n (x_i-1)^2$.
* `f_map()` is a helper function that maps $f$ to the entries (points) in the matrix $X$.

::: {#7f96709e .cell execution_count=63}
``` {.python .cell-code}
def f(x):
    return np.sum((x-1.0)**2)

def f_map(x):
    return np.array(list(map(f, x)))
```
:::


::: {#6529fabf .cell execution_count=64}
``` {.python .cell-code}
y = f_map(X)
y[1:10]
```

::: {.cell-output .cell-output-display execution_count=64}
```
array([1.34231036, 1.47789766, 4.44347889, 5.9285336 , 3.07230572,
       0.25916038, 1.32103849, 1.01971047, 0.33251443])
```
:::
:::


* Alternatively, we can use pre-defined functions from the `pyspot` package:

::: {#dfdd4790 .cell execution_count=65}
``` {.python .cell-code}
# fun = analytical(sigma=0).fun_branin
# fun = analytical(sigma=0).fun_sphere
```
:::


::: {#5c150353 .cell execution_count=66}
``` {.python .cell-code}
XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))
zz = np.array([f_map(np.array([xi, yi]).reshape(-1,2)) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)
```
:::


::: {#39146ca2 .cell execution_count=67}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-68-output-1.png){width=491 height=270}
:::
:::


::: {#0a66f3ea .cell execution_count=68}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 10))
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-69-output-1.png){width=491 height=270}
:::
:::


#### The Gram Matrix

::: {#61f8ed44 .cell execution_count=69}
``` {.python .cell-code}
def build_Gram(X):
        """
        Construction of the Gram matrix.
        """
        n = X.shape[0]
        G = np.zeros((n, n))
        for i in range(n):
            for j in range(i, n):
                G[i, j] = np.linalg.norm(X[i] - X[j])
        G = G + G.T    
        return G
```
:::


::: {#480d035a .cell execution_count=70}
``` {.python .cell-code}
G = build_Gram(X)
np.round(G,2)
```

::: {.cell-output .cell-output-display execution_count=70}
```
array([[0.  , 0.51, 0.57, ..., 0.99, 0.62, 1.39],
       [0.51, 0.  , 0.18, ..., 0.91, 0.94, 1.41],
       [0.57, 0.18, 0.  , ..., 0.75, 0.87, 1.26],
       ...,
       [0.99, 0.91, 0.75, ..., 0.  , 0.71, 0.52],
       [0.62, 0.94, 0.87, ..., 0.71, 0.  , 0.91],
       [1.39, 1.41, 1.26, ..., 0.52, 0.91, 0.  ]])
```
:::
:::


#### The Radial Basis Functions

::: {#a8ee9a74 .cell execution_count=71}
``` {.python .cell-code}
def basis_linear(r):
    return r*r*r
```
:::


::: {#5a0d2eb9 .cell execution_count=72}
``` {.python .cell-code}
def basis_gauss(r, sigma = 1e-1):
    return np.exp(-r**2/sigma)
```
:::


+ We select the Gaussian basis function for the following examples:

::: {#4e40ca1e .cell execution_count=73}
``` {.python .cell-code}
basis = basis_gauss
```
:::


#### The $\Psi$ Matrix

::: {#3459895b .cell execution_count=74}
``` {.python .cell-code}
def build_Phi(G, basis, eps=np.sqrt(np.spacing(1))):
    n = G.shape[0]
    Phi = np.zeros((n,n))
    for i in range(n):
        for j in range(n):
            Phi[i,j] = basis(G[i,j])
    Phi = Phi +  np.multiply(np.mat(eye(n)), eps)
    return Phi
```
:::


::: {#5838e14e .cell execution_count=75}
``` {.python .cell-code}
Phi = build_Phi(G, basis=basis)
Phi[0:3,0:3]
```

::: {.cell-output .cell-output-display execution_count=75}
```
matrix([[1.00000001, 0.07413611, 0.03828587],
        [0.07413611, 1.00000001, 0.73539165],
        [0.03828587, 0.73539165, 1.00000001]])
```
:::
:::


#### Inverting $\Psi$ via Cholesky Factorization

* There a two different implementations of the Cholesky factorization oin Python:
  * `numpy`'s  `linalg.cholesky()` and
  * `scipy`'s  `linalg.cholesky()`
* We will use `numpy`'s version.

::: {#8207074f .cell execution_count=76}
``` {.python .cell-code}
def get_rbf_weights(Phi, y):
    """ 
    Calculating the weights of the radial basis function surrogate.
    Cholesky factorization used.
    LU decomposition otherwise (not implemented yet).
    """
    # U = scipy.linalg.cholesky(Phi, lower=True)
    U = np.linalg.cholesky(Phi)
    U = U.T
    # w = U$U'\ModelInfo.y)
    w = np.linalg.solve(U, np.linalg.solve(U.T, y))
    return w
```
:::


::: {#5bb5fb42 .cell execution_count=77}
``` {.python .cell-code}
w = get_rbf_weights(Phi, y)
w[0:3]
```

::: {.cell-output .cell-output-display execution_count=77}
```
array([-6.98432952,  3.06393352, -5.13738971])
```
:::
:::


#### Predictions

##### The Predictor

::: {#db55cccb .cell execution_count=78}
``` {.python .cell-code}
def pred_rbf(x, X, basis, w):
    n = X.shape[0]
    d = np.zeros((n))
    phi = np.zeros((n))
    for i in range(n):
        d[i] = np.linalg.norm(x - X[i])
    for i in range(n):
        phi[i] = basis(d[i])
    return w @ phi    
```
:::


##### Testing some Example Points

::: {#2bd599d6 .cell execution_count=79}
``` {.python .cell-code}
x = X[0]
x
```

::: {.cell-output .cell-output-display execution_count=79}
```
array([ 0.76153494, -0.61391197])
```
:::
:::


##### The RBF Prediction $\hat{f}$

::: {#e6254d5d .cell execution_count=80}
``` {.python .cell-code}
pred_rbf(x=x, X=X, basis=basis, w=w)
```

::: {.cell-output .cell-output-display execution_count=80}
```
2.6615775203204652
```
:::
:::


##### The Original (True) Value $f$

::: {#c45fa8ee .cell execution_count=81}
``` {.python .cell-code}
f_map(np.array(x).reshape(1,-1))
```

::: {.cell-output .cell-output-display execution_count=81}
```
array([2.66157742])
```
:::
:::


##### Visualizations

::: {#b7b49b8c .cell execution_count=82}
``` {.python .cell-code}
XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))
zz = np.array([pred_rbf(x=np.array([xi, yi]), X=X, basis=basis,w=w) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)
```
:::


::: {#ce5dc48e .cell execution_count=83}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-84-output-1.png){width=491 height=270}
:::
:::


::: {#27bc2ac5 .cell execution_count=84}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 5))
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-85-output-1.png){width=491 height=270}
:::
:::


##### Note

The original function $f$ is cheaper than the surrogate $\hat{f}$ in this example, because we have chosen a simple analytical function as the ground truth. This is not the case in real-world settings.

#### Cholesky Factorization

##### $A = U^T U$

* $U$ is an upper triangular matrix

::: {#bbb78a48 .cell execution_count=85}
``` {.python .cell-code}
def cholesky_U(A):
    N = A.shape[0]
    U = np.zeros((N,N))
    for k in range(0,N):
         # compute diagonal entry
         U[k,k] = A[k,k]
         for j in range(0,k):
             U[k,k] = U[k,k] - U[j,k]*U[j,k]
         U[k,k] = np.sqrt(U[k,k])
         # compute remaining column
         for i in range(k+1,N):
             U[k,i] = A[k,i]
             for j in range(0,k):
                 U[k,i] = U[k,i] - U[j,i]*U[j,k]
             U[k,i] = U[k,i] / U[k,k]
    return U
```
:::


#### $A = L L^T$

$L$ is a lower triangular matrix

::: {#1bf39435 .cell execution_count=86}
``` {.python .cell-code}
def cholesky_L(A):
    N = A.shape[0]
    L = np.zeros((N,N))
    for k in range(0,N):
         # compute diagonal entry
         L[k,k] = A[k,k]
         for j in range(0,k):
             L[k,k] = L[k,k] - L[k,j]*L[k,j]
         L[k,k] = np.sqrt(L[k,k])
         # compute remaining column
         for i in range(k+1,N):
             L[i,k] = A[i,k]
             for j in range(0,k):
                 L[i,k] = L[i,k] - L[i,j]*L[k,j]
             L[i,k] = L[i,k] / L[k,k]
    return L
```
:::


#### Example

::: {#fd32d1c9 .cell execution_count=87}
``` {.python .cell-code}
A = np.array([[4, 2, 4, 4], [2, 10, 5, 2], [4, 5, 9, 6], [4, 2, 6, 9]])
A
```

::: {.cell-output .cell-output-display execution_count=87}
```
array([[ 4,  2,  4,  4],
       [ 2, 10,  5,  2],
       [ 4,  5,  9,  6],
       [ 4,  2,  6,  9]])
```
:::
:::


#### Check: Is $A$ positive definite?

::: {#56ae9541 .cell execution_count=88}
``` {.python .cell-code}
assert(np.all(np.linalg.eigvals(A) > 0))
```
:::


####  $A = U^T U$

Perform Cholesky Factorization

::: {#c6755e90 .cell execution_count=89}
``` {.python .cell-code}
U = cholesky_U(A)
U
```

::: {.cell-output .cell-output-display execution_count=89}
```
array([[2., 1., 2., 2.],
       [0., 3., 1., 0.],
       [0., 0., 2., 1.],
       [0., 0., 0., 2.]])
```
:::
:::


Test Result

::: {#0f563eaa .cell execution_count=90}
``` {.python .cell-code}
U.T @ U
```

::: {.cell-output .cell-output-display execution_count=90}
```
array([[ 4.,  2.,  4.,  4.],
       [ 2., 10.,  5.,  2.],
       [ 4.,  5.,  9.,  6.],
       [ 4.,  2.,  6.,  9.]])
```
:::
:::


####  $A = L L^T$

::: {#bc4984aa .cell execution_count=91}
``` {.python .cell-code}
L = cholesky_L(A)
L
```

::: {.cell-output .cell-output-display execution_count=91}
```
array([[2., 0., 0., 0.],
       [1., 3., 0., 0.],
       [2., 1., 2., 0.],
       [2., 0., 1., 2.]])
```
:::
:::


Test Result

::: {#24309cec .cell execution_count=92}
``` {.python .cell-code}
L @ L.T
```

::: {.cell-output .cell-output-display execution_count=92}
```
array([[ 4.,  2.,  4.,  4.],
       [ 2., 10.,  5.,  2.],
       [ 4.,  5.,  9.,  6.],
       [ 4.,  2.,  6.,  9.]])
```
:::
:::


## Exercises

### Gaussian Basis Function

* Plot the Gaussian Basis Function `basis_gauss` in the range from -2 to 2 using `matplotlib.pyplot`
  * Hint: Check the [matplotlib documentation](https://matplotlib.org/stable/tutorials/introductory/pyplot.html) for examples.
  * Generate a plot with several `sigma` values, e.g., 0.1, 1.0, and 10.
* What is the meaning of the `sigma` parameter: Can you explain its influence / effect on the model quality?
  * Is the `sigma` value important?

### Linear Basis Function

* Select the linear basis function?
* What errors occur?
* Do you have any ideas how to fix this error? -->


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-arlot2010" class="csl-entry" role="listitem">
Arlot, Sylvain, Alain Celisse, et al. 2010. <span>“A Survey of Cross-Validation Procedures for Model Selection.”</span> <em>Statistics Surveys</em> 4: 40–79.
</div>
<div id="ref-bart21i" class="csl-entry" role="listitem">
Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. <em><span class="nocase">Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide</span></em>. Springer.
</div>
<div id="ref-Forr08a" class="csl-entry" role="listitem">
Forrester, Alexander, András Sóbester, and Andy Keane. 2008. <em><span class="nocase">Engineering Design via Surrogate Modelling</span></em>. Wiley.
</div>
<div id="ref-Hast17a" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. <em>The Elements of Statistical Learning</em>. Second. Springer.
</div>
<div id="ref-Koha95a" class="csl-entry" role="listitem">
Kohavi, Ron. 1995. <span>“A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.”</span> In <em>Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2</em>, 1137–43. IJCAI’95. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
</div>
<div id="ref-Sant03a" class="csl-entry" role="listitem">
Santner, T J, B J Williams, and W I Notz. 2003. <em><span class="nocase">The Design and Analysis of Computer Experiments</span></em>. Berlin, Heidelberg, New York: Springer.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./005_num_rsm.html" class="pagination-link" aria-label="Introduction: Numerical Methods">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction: Numerical Methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./007_num_spot_intro.html" class="pagination-link" aria-label="Introduction to spotpython">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to spotpython</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, T. Bartz-Beielstein</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/bartzbeielstein">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>