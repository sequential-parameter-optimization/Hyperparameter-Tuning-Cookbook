<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.548">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hyperparameter Tuning Cookbook - 6&nbsp; Kriging (Gaussian Process Regression)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./007_num_spot_intro.html" rel="next">
<link href="./005_num_rsm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="Hyperparameter Tuning Cookbook - 6&nbsp; Kriging (Gaussian Process Regression)">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./005_num_rsm.html">Numerical Methods</a></li><li class="breadcrumb-item"><a href="./006_num_gp.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hyperparameter Tuning Cookbook</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sequential-parameter-optimization/spotPython" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Hyperparameter-Tuning-Cookbook.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_optimization_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002_awwe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Aircraft Wing Weight Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003_scipy_optimize_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to <code>scipy.optimize</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004_spot_sklearn_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Using <code>scipy</code> Optimizers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Numerical Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_num_rsm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction: Numerical Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_gp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_num_spot_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to spotPython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008_num_spot_multidim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multi-dimensional Functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009_num_spot_anisotropic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Isotropic and Anisotropic Kriging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010_num_spot_sklearn_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Using <code>sklearn</code> Surrogates in <code>spotPython</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_num_spot_sklearn_gaussian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Gaussian Process Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_num_spot_ei.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Expected Improvement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_num_spot_noisy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Handling Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014_num_spot_ocba.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Optimal Computational Budget Allocation in <code>Spot</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_num_spot_correlation_p.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Kriging with Varying Correlation-p</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with Sklearn</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016_spot_hpt_sklearn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">HPT: sklearn</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017_spot_hpt_sklearn_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">HPT: sklearn SVC on Moons Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with River</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./022_spot_hpt_river.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">HPT: River</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./023_spot_river_gui.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Simplifying Hyperparameter Tuning in Online Machine Learning—The spotRiverGUI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./024_spot_hpt_river_friedman_hatr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title"><code>river</code> Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./025_spot_hpt_river_friedman_amfr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title"><code>river</code> Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with PyTorch Lightning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./030_spot_lightning_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./031_spot_lightning_linear_diabetes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: Diabetes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./032_spot_lightning_rnn_diabetes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: Diabetes Using a Recurrent Neural Network</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./033_spot_lightning_linear_sensitive.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: User Specified Data Set and Regression Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./034_spot_lightning_xai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./035_spot_lightning_transformer_introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning Transformer: Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./036_spot_lightning_transformer_diabetes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning Transformer: Diabetes</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_01_intro_to_notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Introduction to Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_02_git_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Git Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_03_python_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Python Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_04_spot_doc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Documentation of the Sequential Parameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#dace-and-rsm" id="toc-dace-and-rsm" class="nav-link active" data-scroll-target="#dace-and-rsm"><span class="header-section-number">6.1</span> DACE and RSM</a></li>
  <li><a href="#background-expectation-mean-standard-deviation" id="toc-background-expectation-mean-standard-deviation" class="nav-link" data-scroll-target="#background-expectation-mean-standard-deviation"><span class="header-section-number">6.2</span> Background: Expectation, Mean, Standard Deviation</a>
  <ul class="collapse">
  <li><a href="#sample-mean" id="toc-sample-mean" class="nav-link" data-scroll-target="#sample-mean"><span class="header-section-number">6.2.1</span> Sample Mean</a></li>
  <li><a href="#variance-and-standard-deviation" id="toc-variance-and-standard-deviation" class="nav-link" data-scroll-target="#variance-and-standard-deviation"><span class="header-section-number">6.2.2</span> Variance and Standard Deviation</a></li>
  <li><a href="#standard-deviation" id="toc-standard-deviation" class="nav-link" data-scroll-target="#standard-deviation"><span class="header-section-number">6.2.3</span> Standard Deviation</a></li>
  <li><a href="#calculation-of-the-standard-deviation-with-python" id="toc-calculation-of-the-standard-deviation-with-python" class="nav-link" data-scroll-target="#calculation-of-the-standard-deviation-with-python"><span class="header-section-number">6.2.4</span> Calculation of the Standard Deviation with Python</a></li>
  <li><a href="#the-empirical-standard-deviation" id="toc-the-empirical-standard-deviation" class="nav-link" data-scroll-target="#the-empirical-standard-deviation"><span class="header-section-number">6.2.5</span> The Empirical Standard Deviation</a></li>
  <li><a href="#the-argument-axis" id="toc-the-argument-axis" class="nav-link" data-scroll-target="#the-argument-axis"><span class="header-section-number">6.2.6</span> The Argument “axis”</a></li>
  </ul></li>
  <li><a href="#data-types-and-precision-in-python" id="toc-data-types-and-precision-in-python" class="nav-link" data-scroll-target="#data-types-and-precision-in-python"><span class="header-section-number">6.3</span> Data Types and Precision in Python</a></li>
  <li><a href="#distributions-and-random-numbers-in-python" id="toc-distributions-and-random-numbers-in-python" class="nav-link" data-scroll-target="#distributions-and-random-numbers-in-python"><span class="header-section-number">6.4</span> Distributions and Random Numbers in Python</a>
  <ul class="collapse">
  <li><a href="#the-uniform-distribution" id="toc-the-uniform-distribution" class="nav-link" data-scroll-target="#the-uniform-distribution"><span class="header-section-number">6.4.1</span> The Uniform Distribution</a></li>
  <li><a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution"><span class="header-section-number">6.4.2</span> The Normal Distribution</a></li>
  <li><a href="#visualization-of-the-standard-deviation" id="toc-visualization-of-the-standard-deviation" class="nav-link" data-scroll-target="#visualization-of-the-standard-deviation"><span class="header-section-number">6.4.3</span> Visualization of the Standard Deviation</a></li>
  <li><a href="#standardization-of-random-variables" id="toc-standardization-of-random-variables" class="nav-link" data-scroll-target="#standardization-of-random-variables"><span class="header-section-number">6.4.4</span> Standardization of Random Variables</a></li>
  <li><a href="#realizations-of-a-normal-distribution" id="toc-realizations-of-a-normal-distribution" class="nav-link" data-scroll-target="#realizations-of-a-normal-distribution"><span class="header-section-number">6.4.5</span> Realizations of a Normal Distribution</a></li>
  <li><a href="#the-multivariate-normal-distribution" id="toc-the-multivariate-normal-distribution" class="nav-link" data-scroll-target="#the-multivariate-normal-distribution"><span class="header-section-number">6.4.6</span> The Multivariate Normal Distribution</a></li>
  <li><a href="#the-bivariate-normal-distribution-with-mean-zero-and-zero-covariances-sigma_12-sigma_21-0" id="toc-the-bivariate-normal-distribution-with-mean-zero-and-zero-covariances-sigma_12-sigma_21-0" class="nav-link" data-scroll-target="#the-bivariate-normal-distribution-with-mean-zero-and-zero-covariances-sigma_12-sigma_21-0"><span class="header-section-number">6.4.7</span> The Bivariate Normal Distribution with Mean Zero and Zero Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = 0\)</span></a></li>
  <li><a href="#the-bivariate-normal-distribution-with-mean-zero-and-negative-covariances-sigma_12-sigma_21--4" id="toc-the-bivariate-normal-distribution-with-mean-zero-and-negative-covariances-sigma_12-sigma_21--4" class="nav-link" data-scroll-target="#the-bivariate-normal-distribution-with-mean-zero-and-negative-covariances-sigma_12-sigma_21--4"><span class="header-section-number">6.4.8</span> The Bivariate Normal Distribution with Mean Zero and Negative Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = -4\)</span></a></li>
  </ul></li>
  <li><a href="#cholesky-decomposition-and-positive-definite-matrices" id="toc-cholesky-decomposition-and-positive-definite-matrices" class="nav-link" data-scroll-target="#cholesky-decomposition-and-positive-definite-matrices"><span class="header-section-number">6.5</span> Cholesky Decomposition and Positive Definite Matrices</a></li>
  <li><a href="#maximum-likelihood-estimation-multivariate-normal-distribution" id="toc-maximum-likelihood-estimation-multivariate-normal-distribution" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-multivariate-normal-distribution"><span class="header-section-number">6.6</span> Maximum Likelihood Estimation: Multivariate Normal Distribution</a></li>
  <li><a href="#introduction-to-gaussian-processes" id="toc-introduction-to-gaussian-processes" class="nav-link" data-scroll-target="#introduction-to-gaussian-processes"><span class="header-section-number">6.7</span> Introduction to Gaussian Processes</a>
  <ul class="collapse">
  <li><a href="#gaussian-process-prior" id="toc-gaussian-process-prior" class="nav-link" data-scroll-target="#gaussian-process-prior"><span class="header-section-number">6.7.1</span> Gaussian Process Prior</a></li>
  <li><a href="#covariance-function" id="toc-covariance-function" class="nav-link" data-scroll-target="#covariance-function"><span class="header-section-number">6.7.2</span> Covariance Function</a></li>
  <li><a href="#construction-of-the-covariance-matrix" id="toc-construction-of-the-covariance-matrix" class="nav-link" data-scroll-target="#construction-of-the-covariance-matrix"><span class="header-section-number">6.7.3</span> Construction of the Covariance Matrix</a></li>
  <li><a href="#generation-of-random-samples-and-plotting-the-realizations-of-the-random-function" id="toc-generation-of-random-samples-and-plotting-the-realizations-of-the-random-function" class="nav-link" data-scroll-target="#generation-of-random-samples-and-plotting-the-realizations-of-the-random-function"><span class="header-section-number">6.7.4</span> Generation of Random Samples and Plotting the Realizations of the Random Function</a></li>
  <li><a href="#properties-of-the-1d-example" id="toc-properties-of-the-1d-example" class="nav-link" data-scroll-target="#properties-of-the-1d-example"><span class="header-section-number">6.7.5</span> Properties of the 1d Example</a></li>
  </ul></li>
  <li><a href="#kriging-modeling-basics" id="toc-kriging-modeling-basics" class="nav-link" data-scroll-target="#kriging-modeling-basics"><span class="header-section-number">6.8</span> Kriging: Modeling Basics</a>
  <ul class="collapse">
  <li><a href="#the-kriging-idea-in-a-nutshell" id="toc-the-kriging-idea-in-a-nutshell" class="nav-link" data-scroll-target="#the-kriging-idea-in-a-nutshell"><span class="header-section-number">6.8.1</span> The Kriging Idea in a Nutshell</a></li>
  <li><a href="#the-kriging-basis-function" id="toc-the-kriging-basis-function" class="nav-link" data-scroll-target="#the-kriging-basis-function"><span class="header-section-number">6.8.2</span> The Kriging Basis Function</a></li>
  <li><a href="#the-correlation-coefficient" id="toc-the-correlation-coefficient" class="nav-link" data-scroll-target="#the-correlation-coefficient"><span class="header-section-number">6.8.3</span> The Correlation Coefficient</a></li>
  <li><a href="#covariance-matrix-and-correlation-matrix" id="toc-covariance-matrix-and-correlation-matrix" class="nav-link" data-scroll-target="#covariance-matrix-and-correlation-matrix"><span class="header-section-number">6.8.4</span> Covariance Matrix and Correlation Matrix</a></li>
  <li><a href="#the-kriging-model" id="toc-the-kriging-model" class="nav-link" data-scroll-target="#the-kriging-model"><span class="header-section-number">6.8.5</span> The Kriging Model</a></li>
  <li><a href="#correlations" id="toc-correlations" class="nav-link" data-scroll-target="#correlations"><span class="header-section-number">6.8.6</span> Correlations</a></li>
  <li><a href="#the-condition-number" id="toc-the-condition-number" class="nav-link" data-scroll-target="#the-condition-number"><span class="header-section-number">6.8.7</span> The Condition Number</a></li>
  <li><a href="#mle-to-estimate-theta-and-p" id="toc-mle-to-estimate-theta-and-p" class="nav-link" data-scroll-target="#mle-to-estimate-theta-and-p"><span class="header-section-number">6.8.8</span> MLE to estimate <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></a></li>
  <li><a href="#tuning-theta-and-p" id="toc-tuning-theta-and-p" class="nav-link" data-scroll-target="#tuning-theta-and-p"><span class="header-section-number">6.8.9</span> Tuning <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></a></li>
  </ul></li>
  <li><a href="#kriging-prediction" id="toc-kriging-prediction" class="nav-link" data-scroll-target="#kriging-prediction"><span class="header-section-number">6.9</span> Kriging Prediction</a>
  <ul class="collapse">
  <li><a href="#the-augmented-correlation-matrix" id="toc-the-augmented-correlation-matrix" class="nav-link" data-scroll-target="#the-augmented-correlation-matrix"><span class="header-section-number">6.9.1</span> The Augmented Correlation Matrix</a></li>
  <li><a href="#properties-of-the-predictor" id="toc-properties-of-the-predictor" class="nav-link" data-scroll-target="#properties-of-the-predictor"><span class="header-section-number">6.9.2</span> Properties of the Predictor</a></li>
  </ul></li>
  <li><a href="#kriging-example-sinusoid-function" id="toc-kriging-example-sinusoid-function" class="nav-link" data-scroll-target="#kriging-example-sinusoid-function"><span class="header-section-number">6.10</span> Kriging Example: Sinusoid Function</a>
  <ul class="collapse">
  <li><a href="#calculating-the-correlation-matrix-psi" id="toc-calculating-the-correlation-matrix-psi" class="nav-link" data-scroll-target="#calculating-the-correlation-matrix-psi"><span class="header-section-number">6.10.1</span> Calculating the Correlation Matrix <span class="math inline">\(\Psi\)</span></a></li>
  <li><a href="#computing-the-psi-vector" id="toc-computing-the-psi-vector" class="nav-link" data-scroll-target="#computing-the-psi-vector"><span class="header-section-number">6.10.2</span> Computing the <span class="math inline">\(\psi\)</span> Vector</a></li>
  <li><a href="#predicting-at-new-locations" id="toc-predicting-at-new-locations" class="nav-link" data-scroll-target="#predicting-at-new-locations"><span class="header-section-number">6.10.3</span> Predicting at New Locations</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization"><span class="header-section-number">6.10.4</span> Visualization</a></li>
  </ul></li>
  <li><a href="#cholesky-example-with-two-points" id="toc-cholesky-example-with-two-points" class="nav-link" data-scroll-target="#cholesky-example-with-two-points"><span class="header-section-number">6.11</span> Cholesky Example With Two Points</a>
  <ul class="collapse">
  <li><a href="#cholesky-decomposition" id="toc-cholesky-decomposition" class="nav-link" data-scroll-target="#cholesky-decomposition"><span class="header-section-number">6.11.1</span> Cholesky Decomposition</a></li>
  <li><a href="#computation-of-the-inverse-matrix" id="toc-computation-of-the-inverse-matrix" class="nav-link" data-scroll-target="#computation-of-the-inverse-matrix"><span class="header-section-number">6.11.2</span> Computation of the Inverse Matrix</a></li>
  </ul></li>
  <li><a href="#jupyter-notebook" id="toc-jupyter-notebook" class="nav-link" data-scroll-target="#jupyter-notebook"><span class="header-section-number">6.12</span> Jupyter Notebook</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./005_num_rsm.html">Numerical Methods</a></li><li class="breadcrumb-item"><a href="./006_num_gp.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- bart21mSlides2022Lec-05 -->
<section id="dace-and-rsm" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="dace-and-rsm"><span class="header-section-number">6.1</span> DACE and RSM</h2>
<p>Mathematical models implemented in computer codes are used to circumvent the need for expensive field data collection. These models are particularly useful when dealing with highly nonlinear response surfaces, high signal-to-noise ratios (which often involve deterministic evaluations), and a global scope. As a result, a new approach is required in comparison to Response Surface Methodology (RSM).</p>
<p>With the improvement in computing power and simulation fidelity, researchers gain higher confidence and a better understanding of the dynamics in physical, biological, and social systems. However, the expansion of configuration spaces and increasing input dimensions necessitates more extensive designs. High-performance computing (HPC) allows for thousands of runs, whereas previously only tens were possible. This shift towards larger models and training data presents new computational challenges.</p>
<p>Research questions for DACE (Design and Analysis of Computer Experiments) include how to design computer experiments that make efficient use of computation and how to meta-model computer codes to save on simulation effort. The choice of surrogate model for computer codes significantly impacts the optimal experiment design, and the preferred model-design pairs can vary depending on the specific goal.</p>
<p>The combination of computer simulation, design, and modeling with field data from similar real-world experiments introduces a new category of computer model tuning problems. The ultimate goal is to automate these processes to the greatest extent possible, allowing for the deployment of HPC with minimal human intervention.</p>
<p>One of the remaining differences between RSM and DACE lies in how they handle noise. DACE employs replication, a technique that would not be used in a deterministic setting, to separate signal from noise. Traditional RSM is best suited for situations where a substantial proportion of the variability in the data is due to noise, and where the acquisition of data values can be severely limited. Consequently, RSM is better suited for a different class of problems, aligning with its intended purposes.</p>
<p>Two very good texts on computer experiments and surrogate modeling are <span class="citation" data-cites="Sant03a">Santner, Williams, and Notz (<a href="references.html#ref-Sant03a" role="doc-biblioref">2003</a>)</span> and <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>. The former is the canonical reference in the statistics literature and the latter is perhaps more popular in engineering.</p>
</section>
<section id="background-expectation-mean-standard-deviation" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="background-expectation-mean-standard-deviation"><span class="header-section-number">6.2</span> Background: Expectation, Mean, Standard Deviation</h2>
<p>The distribution of a random vector is characterized by some indexes. One of them is the expected value, which is defined as <span class="math display">\[
E[X] = \sum_{x \in D_X} xp_X(x)  \qquad \text{if $X$ is discrete}
\]</span> <span class="math display">\[
E[X] = \int\limits_{x \in D_X} xf_X(x)\mathrm{d}x  \quad  \text{if $X$ is continuous.}
\]</span></p>
<p>The mean, <span class="math inline">\(\mu\)</span>, of a probability distribution is a measure of its central tendency or location. That is, <span class="math inline">\(E(X)\)</span> is defined as the average of all possible values of <span class="math inline">\(X\)</span>, weighted by their probabilities.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Expectation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(X\)</span> denote the number produced by rolling a fair die. Then <span class="math display">\[
E(X) = 1 \times 1/6 + 2 \times 1/6 + 3 \times 1/6 + 4 \times 1/6 + 5 \times 1/6 + 6\times 1/6 = 3.5.
\]</span></p>
</div>
</div>
<section id="sample-mean" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="sample-mean"><span class="header-section-number">6.2.1</span> Sample Mean</h3>
<p>The sample mean is an important estimate of the population mean. The sample mean of a sample <span class="math inline">\(\{x_i\}\)</span> (<span class="math inline">\(i=1,2,\ldots,n\)</span>) is defined as <span class="math display">\[\overline{x}  = \frac{1}{n} \sum_i x_i.\]</span></p>
</section>
<section id="variance-and-standard-deviation" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="variance-and-standard-deviation"><span class="header-section-number">6.2.2</span> Variance and Standard Deviation</h3>
<p>If we are trying to predict the value of a random variable <span class="math inline">\(X\)</span> by its mean <span class="math inline">\(\mu = E(X)\)</span>, the error will be <span class="math inline">\(X-\mu\)</span>. In many situations it is useful to have an idea how large this deviation or error is. Since <span class="math inline">\(E(X-\mu) = E(X) -\mu = 0\)</span>, it is necessary to use the absolute value or the square of (<span class="math inline">\(X-\mu\)</span>). The squared error is the first choice, because the derivatives are easier to calculate. These considerations motivate the definition of the variance:</p>
<p>The variance of a random variable <span class="math inline">\(X\)</span> is the mean squared deviation of <span class="math inline">\(X\)</span> from its expected value <span class="math inline">\(\mu = E(X)\)</span>. <span class="math display">\[\begin{equation}
Var(X) = E[ (X-\mu)^2].
\end{equation}\]</span></p>
</section>
<section id="standard-deviation" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="standard-deviation"><span class="header-section-number">6.2.3</span> Standard Deviation</h3>
<p>Taking the square root of the variance to get back to the same scale of units as <span class="math inline">\(X\)</span> gives the standard deviation. The standard deviation of <span class="math inline">\(X\)</span> is the square root of the variance of <span class="math inline">\(X\)</span>. <span class="math display">\[\begin{equation}
sd(X) = \sqrt{Var(X)}.
\end{equation}\]</span></p>
</section>
<section id="calculation-of-the-standard-deviation-with-python" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="calculation-of-the-standard-deviation-with-python"><span class="header-section-number">6.2.4</span> Calculation of the Standard Deviation with Python</h3>
<p>The function <code>numpy.std</code> returns the standard deviation, a measure of the spread of a distribution, of the array elements. The argument <code>ddof</code> specifies the Delta Degrees of Freedom. The divisor used in calculations is <code>N - ddof</code>, where <code>N</code> represents the number of elements. By default <code>ddof</code> is zero, i.e., <code>std</code> uses the formula <span class="math display">\[\begin{equation}  \sqrt{  \frac{1}{N} \sum_i \left( x_i - \bar{x} \right)^2  } \qquad \text{with } \quad \bar{x} = \sum_{i=1}^N x_i /N. \end{equation}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Standard Deviation with Python
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider the array <span class="math inline">\([1,2,3]\)</span>: Since <span class="math inline">\(\bar{x} = 2\)</span>, the following value is computed: <span class="math display">\[ \sqrt{1/3 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/3}.\]</span></p>
<div id="d644a698" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]])</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>np.std(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>0.816496580927726</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="the-empirical-standard-deviation" class="level3" data-number="6.2.5">
<h3 data-number="6.2.5" class="anchored" data-anchor-id="the-empirical-standard-deviation"><span class="header-section-number">6.2.5</span> The Empirical Standard Deviation</h3>
<p>The empirical standard deviation (which uses <span class="math inline">\(N-1\)</span>), <span class="math inline">\(\sqrt{1/2 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/2}\)</span>, can be calculated as follows:</p>
<div id="58e60815" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>np.std(a, ddof<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>1.0</code></pre>
</div>
</div>
</section>
<section id="the-argument-axis" class="level3" data-number="6.2.6">
<h3 data-number="6.2.6" class="anchored" data-anchor-id="the-argument-axis"><span class="header-section-number">6.2.6</span> The Argument “axis”</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Axes along which the standard deviation is computed
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>When you compute np.std with axis=0, it calculates the standard deviation along the vertical axis, meaning it computes the standard deviation for each column of the array.</li>
<li>On the other hand, when you compute np.std with axis=1, it calculates the standard deviation along the horizontal axis, meaning it computes the standard deviation for each row of the array.</li>
<li>If the axis parameter is not specified, np.std computes the standard deviation of the flattened array.</li>
</ul>
</div>
</div>
<div id="0acc030e" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>array([[1, 2],
       [3, 4]])</code></pre>
</div>
</div>
<div id="dfc8d842" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>np.std(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>1.118033988749895</code></pre>
</div>
</div>
<div id="19d442f9" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>np.std(A, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([1., 1.])</code></pre>
</div>
</div>
<div id="b1b612d0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>np.std(A, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([0.5, 0.5])</code></pre>
</div>
</div>
</section>
</section>
<section id="data-types-and-precision-in-python" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="data-types-and-precision-in-python"><span class="header-section-number">6.3</span> Data Types and Precision in Python</h2>
<p>We consider single versus double precision in Python. In single precision, <code>std()</code> can be inaccurate:</p>
<div id="0dee4c92" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.zeros((<span class="dv">2</span>, <span class="dv">4</span><span class="op">*</span><span class="dv">4</span>), dtype<span class="op">=</span>np.float32)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">0</span>, :] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">1</span>, :] <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>a </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([[1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,
        1. , 1. , 1. ],
       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
        0.1, 0.1, 0.1]], dtype=float32)</code></pre>
</div>
</div>
<div id="b4b194fe" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>np.std(a, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
       0.45, 0.45, 0.45, 0.45, 0.45], dtype=float32)</code></pre>
</div>
</div>
<div id="934ec1dd" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>np.std(a, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([0., 0.], dtype=float32)</code></pre>
</div>
</div>
<div id="5b3e0ee1" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(<span class="fl">0.45</span> <span class="op">-</span> np.std(a))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>1.7881393421514957e-08</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Float data types
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>float32 and float64 are data types in numpy that specify the precision of floating point numbers.</li>
<li>float32 is a single-precision floating point number that occupies 32 bits of memory. It has a precision of about 7 decimal digits.</li>
<li>float64 is a double-precision floating point number that occupies 64 bits of memory. It has a precision of about 15 decimal digits.</li>
<li>The main difference between float32 and float64 is the precision and memory usage. float64 provides a higher precision but uses more memory, while float32 uses less memory but has a lower precision.</li>
</ul>
</div>
</div>
<p>Computing the standard deviation in float64 is more accurate (result may vary), see <a href="https://numpy.org/devdocs/reference/generated/numpy.std.html">https://numpy.org/devdocs/reference/generated/numpy.std.html</a>.</p>
<div id="4f3e233f" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(<span class="fl">0.45</span> <span class="op">-</span> np.std(a, dtype<span class="op">=</span>np.float64))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>7.450580707946131e-10</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: 32 versus 64 bit
</div>
</div>
<div class="callout-body-container callout-body">
<div id="12131199" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a number</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> <span class="fl">0.123456789123456789</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to float32 and float64</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>num_float32 <span class="op">=</span> np.float32(num)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>num_float64 <span class="op">=</span> np.float64(num)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number in both formats</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"float32: "</span>, num_float32)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"float64: "</span>, num_float64)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>float32:  0.12345679
float64:  0.12345678912345678</code></pre>
</div>
</div>
</div>
</div>
<p>The float32 data type in numpy represents a single-precision floating point number. It uses 32 bits of memory, which gives it a precision of about 7 decimal digits. On the other hand, float64 represents a double-precision floating point number. It uses 64 bits of memory, which gives it a precision of about 15 decimal digits.</p>
<p>The reason float32 shows fewer digits is because it has less precision due to using less memory. The bits of memory are used to store the sign, exponent, and fraction parts of the floating point number, and with fewer bits, you can represent fewer digits accurately.</p>
</section>
<section id="distributions-and-random-numbers-in-python" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="distributions-and-random-numbers-in-python"><span class="header-section-number">6.4</span> Distributions and Random Numbers in Python</h2>
<p>Results from computers are deterministic, so it sounds like a contradiction in terms to generate random numbers on a computer. Standard computers generate pseudo-randomnumbers, i.e., numbers that behave as if they were drawn randomly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deterministic Random Numbers
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Idea: Generate deterministically numbers that <strong>look</strong> (behave) as if they were drawn randomly.</li>
</ul>
</div>
</div>
<section id="the-uniform-distribution" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="the-uniform-distribution"><span class="header-section-number">6.4.1</span> The Uniform Distribution</h3>
<p>The probability density function of the uniform distribution is defined as: <span class="math display">\[
f_X(x) = \frac{1}{b-a} \qquad \text{for $x \in [a,b]$}.
\]</span></p>
<p>Generate 10 random numbers from a uniform distribution between <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=1\)</span>:</p>
<div id="49f85f24" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the random number generator</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">123456789</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.uniform(low<span class="op">=</span><span class="fl">0.0</span>, high<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>n)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>array([0.02771274, 0.90670006, 0.88139355, 0.62489728, 0.79071481,
       0.82590801, 0.84170584, 0.47172795, 0.95722878, 0.94659153])</code></pre>
</div>
</div>
<p>Generate 10,000 random numbers from a uniform distribution between 0 and 10 and plot a histogram of the numbers:</p>
<div id="c1f4a617" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the random number generator</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">123456789</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers from a uniform distribution</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.uniform(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a histogram of the numbers</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>plt.hist(x, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Uniform Distribution [0,10]'</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-15-output-1.png" width="597" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-normal-distribution" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="the-normal-distribution"><span class="header-section-number">6.4.2</span> The Normal Distribution</h3>
<p>The probability density function of the normal distribution is defined as: <span id="eq-normal-one"><span class="math display">\[
f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2\right),
\tag{6.1}\]</span></span> where: <span class="math inline">\(\mu\)</span> is the mean; <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
<p>To generate ten random numbers from a normal distribution, the following command can be used.</p>
<div id="013a8f51" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate 10 random numbers between from a normal distribution</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng()</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">2</span>, <span class="fl">0.1</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.normal(mu, sigma, n)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>array([2.14714386, 1.97549869, 2.08131275, 2.03918561, 1.97485864,
       1.90273969, 1.84683475, 2.20978642, 1.96055897, 2.24483493])</code></pre>
</div>
</div>
<p>Verify the mean:</p>
<div id="ee503686" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(mu <span class="op">-</span> np.mean(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>0.03827543224027874</code></pre>
</div>
</div>
<p>Note: To verify the standard deviation, we use <code>ddof = 1</code> (empirical standard deviation):</p>
<div id="05ebc31e" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(sigma <span class="op">-</span> np.std(x, ddof<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>0.031153069860520632</code></pre>
</div>
</div>
<p>A normally distributed random variable is a random variable whose associated probability distribution is the normal (or Gaussian) distribution. The normal distribution is a continuous probability distribution characterized by a symmetric bell-shaped curve.</p>
<p>The distribution is defined by two parameters: the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. The mean indicates the center of the distribution, while the standard deviation measures the spread or dispersion of the distribution.</p>
<p>This distribution is widely used in statistics and the natural and social sciences as a simple model for random variables with unknown distributions.</p>
<div id="e6026ee6" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plot_normal_distribution(mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">1</span>, num_samples<span class="op">=</span><span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-20-output-1.png" width="579" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualization-of-the-standard-deviation" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="visualization-of-the-standard-deviation"><span class="header-section-number">6.4.3</span> Visualization of the Standard Deviation</h3>
<p>The standard deviation of normal distributed can be visualized in terms of the histogram of <span class="math inline">\(X\)</span>:</p>
<ul>
<li>about 68% of the values will lie in the interval within one standard deviation of the mean</li>
<li>95% lie within two standard deviation of the mean</li>
<li>and 99.9% lie within 3 standard deviations of the mean.</li>
</ul>
<div id="a23400cc" class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-21-output-1.png" width="662" height="469" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="standardization-of-random-variables" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="standardization-of-random-variables"><span class="header-section-number">6.4.4</span> Standardization of Random Variables</h3>
<p>To compare statistical properties of random variables which use different units, it is a common practice to transform these random variables into standardized variables. If a random variable <span class="math inline">\(X\)</span> has expectation <span class="math inline">\(E(X) = \mu\)</span> and standard deviation <span class="math inline">\(sd(X) = \sigma &gt;0\)</span>, the random variable <span class="math display">\[
X^{\ast} = (X-\mu)/\sigma
\]</span> is called <span class="math inline">\(X\)</span> in standard units. It has <span class="math inline">\(E(X^{\ast}) = 0\)</span> and <span class="math inline">\(sd(X^{\ast}) =1\)</span>.</p>
</section>
<section id="realizations-of-a-normal-distribution" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="realizations-of-a-normal-distribution"><span class="header-section-number">6.4.5</span> Realizations of a Normal Distribution</h3>
<p>Realizations of a normal distribution refers to the actual values that you get when you draw samples from a normal distribution. Each sample drawn from the distribution is a realization of that distribution.</p>
<p>For example, if you have a normal distribution with a mean of 0 and a standard deviation of 1, each number you draw from that distribution is a realization.</p>
<p>Here’s a Python example:</p>
<div id="bddf8cc4" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameters of the normal distribution</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw 10 samples (realizations) from the normal distribution</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>realizations <span class="op">=</span> np.random.normal(mu, sigma, <span class="dv">10</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(realizations)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 0.48951662  0.23879586 -0.44811181 -0.610795   -2.02994507  0.60794659
 -0.35410888  0.15258149  0.50127485 -0.78640277]</code></pre>
</div>
</div>
<p>In this code, np.random.normal generates 10 realizations of a normal distribution with a mean of 0 and a standard deviation of 1. The realizations array contains the actual values drawn from the distribution.</p>
</section>
<section id="the-multivariate-normal-distribution" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="the-multivariate-normal-distribution"><span class="header-section-number">6.4.6</span> The Multivariate Normal Distribution</h3>
<p>The multivariate normal, multinormal, or Gaussian distribution serves as a generalization of the one-dimensional normal distribution to higher dimensions. We will consider <span class="math inline">\(k\)</span>-dimensional random vectors <span class="math inline">\(X = (X_1, X_2, \ldots, X_k)\)</span>. When drawing samples from this distribution, it results in a set of values represented as <span class="math inline">\(\{x_1, x_2, \ldots, x_k\}\)</span>. To fully define this distribution, it is necessary to specify its mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. These parameters are analogous to the mean, which represents the central location, and the variance (squared standard deviation) of the one-dimensional normal distribution introduced in <a href="#eq-normal-one" class="quarto-xref">Equation&nbsp;<span>6.1</span></a>.</p>
<p>In the context of the multivariate normal distribution, the mean takes the form of a coordinate within an <span class="math inline">\(k\)</span>-dimensional space. This coordinate represents the location where samples are most likely to be generated, akin to the peak of the bell curve in a one-dimensional or univariate normal distribution.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Covariance of two random variables
</div>
</div>
<div class="callout-body-container callout-body">
<p>For two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values: <span class="math display">\[
\operatorname{cov}(X, Y) = \operatorname{E}{\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]}
\]</span></p>
<p>The covariance within the multivariate normal distribution denotes the extent to which two variables vary together. The elements of the covariance matrix, such as <span class="math inline">\(\Sigma_{ij}\)</span>, represent the covariances between the variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>. These covariances describe how the different variables in the distribution are related to each other in terms of their variability.</p>
<p>The probability density function (PDF) of the multivariate normal distribution is defined as: <span class="math display">\[
f_X(x) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp\left(-\frac{1}{2} (x-\mu)^T\Sigma^{-1} (x-\mu)\right),
\]</span> where: <span class="math inline">\(\mu\)</span> is the <span class="math inline">\(k \times 1\)</span> mean vector; <span class="math inline">\(\Sigma\)</span> is the <span class="math inline">\(k \times k\)</span> covariance matrix. The covariance matrix <span class="math inline">\(\Sigma\)</span> is assumed to be positive definite, so that its determinant is strictly positive.</p>
<p>For discrete random variables, covariance can be written as: <span class="math display">\[
\operatorname{cov} (X,Y) = \frac{1}{n}\sum_{i=1}^n (x_i-E(X)) (y_i-E(Y)).
\]</span></p>
</div>
</div>
<p><a href="#fig-bi9040" class="quarto-xref">Figure&nbsp;<span>6.1</span></a> shows draws from a bivariate normal distribution with <span class="math inline">\(\mu = \begin{pmatrix}0 \\ 0\end{pmatrix}\)</span> and <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9 \end{pmatrix}\)</span>.</p>
<div id="cell-fig-bi9040" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng()</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]]  <span class="co"># diagonal covariance</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> rng.multivariate_normal(mean, cov, <span class="dv">1000</span>).T</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot of the numbers</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y, s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'equal'</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Bivariate Normal. Mean zero and positive covariance: </span><span class="sc">{</span>cov<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-bi9040" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9040-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi9040-output-1.png" width="592" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9040-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>The covariance matrix of a bivariate normal distribution determines the shape, orientation, and spread of the distribution in the two-dimensional space.</p>
<p>The diagonal elements of the covariance matrix (<span class="math inline">\(\sigma_1^2\)</span>, <span class="math inline">\(\sigma_2^2\)</span>) are the variances of the individual variables. They determine the spread of the distribution along each axis. A larger variance corresponds to a greater spread along that axis.</p>
<p>The off-diagonal elements of the covariance matrix (<span class="math inline">\(\sigma_{12}, \sigma_{21}\)</span>) are the covariances between the variables. They determine the orientation and shape of the distribution. If the covariance is positive, the distribution is stretched along the line <span class="math inline">\(y=x\)</span>, indicating that the variables tend to increase together. If the covariance is negative, the distribution is stretched along the line <span class="math inline">\(y=-x\)</span>, indicating that one variable tends to decrease as the other increases. If the covariance is zero, the variables are uncorrelated and the distribution is axis-aligned.</p>
<p>In <a href="#fig-bi9040" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>, the variances are identical and the variables are correlated (covariance is 4), so the distribution is stretched along the line <span class="math inline">\(y=x\)</span>.</p>
<div id="cell-fig-bi90403d" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]])</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid and multivariate normal</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">100</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">100</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x,y)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> np.empty(X.shape <span class="op">+</span> (<span class="dv">2</span>,))</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>pos[:, :, <span class="dv">0</span>] <span class="op">=</span> X<span class="op">;</span> pos[:, :, <span class="dv">1</span>] <span class="op">=</span> Y</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> multivariate_normal(mu, cov)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.axes(projection<span class="op">=</span><span class="st">'3d'</span>)  </span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>surf<span class="op">=</span>ax.plot_surface(X, Y, rv.pdf(pos),cmap<span class="op">=</span><span class="st">'viridis'</span>,linewidth<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'X axis'</span>)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Y axis'</span>)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">'Z axis'</span>)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Bivariate Normal Distribution'</span>)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>fig.colorbar(surf, shrink<span class="op">=</span><span class="fl">0.5</span>, aspect<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-bi90403d" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi90403d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi90403d-output-1.png" width="481" height="416" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi90403d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-bivariate-normal-distribution-with-mean-zero-and-zero-covariances-sigma_12-sigma_21-0" class="level3" data-number="6.4.7">
<h3 data-number="6.4.7" class="anchored" data-anchor-id="the-bivariate-normal-distribution-with-mean-zero-and-zero-covariances-sigma_12-sigma_21-0"><span class="header-section-number">6.4.7</span> The Bivariate Normal Distribution with Mean Zero and Zero Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = 0\)</span></h3>
<p><span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 0 \\ 0 &amp; 9\end{pmatrix}\)</span></p>
<div id="cell-fig-bi9000" class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<div id="fig-bi9000" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi9000-output-1.png" width="577" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 0 \\ 0 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-bivariate-normal-distribution-with-mean-zero-and-negative-covariances-sigma_12-sigma_21--4" class="level3" data-number="6.4.8">
<h3 data-number="6.4.8" class="anchored" data-anchor-id="the-bivariate-normal-distribution-with-mean-zero-and-negative-covariances-sigma_12-sigma_21--4"><span class="header-section-number">6.4.8</span> The Bivariate Normal Distribution with Mean Zero and Negative Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = -4\)</span></h3>
<p><span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; -4 \\ -4 &amp; 9\end{pmatrix}\)</span></p>
<div id="cell-fig-bi9449" class="cell" data-execution_count="25">
<div class="cell-output cell-output-display">
<div id="fig-bi9449" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9449-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-bi9449-output-1.png" width="582" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9449-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; -4 \\ -4 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="cholesky-decomposition-and-positive-definite-matrices" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="cholesky-decomposition-and-positive-definite-matrices"><span class="header-section-number">6.5</span> Cholesky Decomposition and Positive Definite Matrices</h2>
<p>The covariance matrix must be positive definite for a multivariate normal distribution for a couple of reasons:</p>
<ul>
<li>Semidefinite vs Definite: A covariance matrix is always symmetric and positive semidefinite. However, for a multivariate normal distribution, it must be positive definite, not just semidefinite. This is because a positive semidefinite matrix can have zero eigenvalues, which would imply that some dimensions in the distribution have zero variance, collapsing the distribution in those dimensions. A positive definite matrix has all positive eigenvalues, ensuring that the distribution has positive variance in all dimensions.</li>
<li>Invertibility: The multivariate normal distribution’s probability density function involves the inverse of the covariance matrix. If the covariance matrix is not positive definite, it may not be invertible, and the density function would be undefined.</li>
</ul>
<p>In summary, the covariance matrix being positive definite ensures that the multivariate normal distribution is well-defined and has positive variance in all dimensions.</p>
<div id="91739ab6" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_positive_definite(matrix):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">all</span>(np.linalg.eigvals(matrix) <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]])</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(is_positive_definite(matrix))  <span class="co"># Outputs: True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True</code></pre>
</div>
</div>
<p>More effficent (and check if symmetric) is based on Cholesky decomposition.</p>
<div id="3d47844d" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_pd(K):</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        np.linalg.cholesky(K)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> np.linalg.linalg.LinAlgError <span class="im">as</span> err:</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'Matrix is not positive definite'</span> <span class="kw">in</span> err.message:</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]])</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(is_pd(matrix))  <span class="co"># Outputs: True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: Cholesky decomposition.
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>linalg.cholesky</code> computes the Cholesky decomposition of a matrix, i.e., it computes a lower triangular matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(LL^T = A\)</span>. If the matrix is not positive definite, an error (<code>LinAlgError</code>) is raised.</p>
<div id="e11fac38" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a Hermitian, positive-definite matrix</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">4</span>], [<span class="dv">4</span>, <span class="dv">9</span>]]) </span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the Cholesky decomposition</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(A)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L = </span><span class="ch">\n</span><span class="st">"</span>, L)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L*LT = </span><span class="ch">\n</span><span class="st">"</span>, np.dot(L, L.T))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>L = 
 [[3.         0.        ]
 [1.33333333 2.68741925]]
L*LT = 
 [[9. 4.]
 [4. 9.]]</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="maximum-likelihood-estimation-multivariate-normal-distribution" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="maximum-likelihood-estimation-multivariate-normal-distribution"><span class="header-section-number">6.6</span> Maximum Likelihood Estimation: Multivariate Normal Distribution</h2>
<p>Consider the first <span class="math inline">\(n\)</span> terms of an identically and independently distributed (i.i..d.) sequence <span class="math inline">\({X^{(j)}}\)</span> of <span class="math inline">\(k\)</span>-dimensional multivariate normal random vectors, i.e., <span class="math inline">\(X^{(j)} \sim N(\mu, \Sigma)\)</span>, <span class="math inline">\(j=1,2,\ldots\)</span>. The joint probability density function of the <span class="math inline">\(j\)</span>-th term of the sequence is <span class="math display">\[
f_X(x_j) = \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right),
\]</span></p>
<p>where: <span class="math inline">\(\mu\)</span> is the <span class="math inline">\(k \times 1\)</span> mean vector; <span class="math inline">\(\Sigma\)</span> is the <span class="math inline">\(k \times k\)</span> covariance matrix. The covariance matrix <span class="math inline">\(\Sigma\)</span> is assumed to be positive definite, so that its determinant is strictly positive. We use <span class="math inline">\(x_1, \ldots x_n\)</span>, i.e., the realizations of the first <span class="math inline">\(n\)</span> random vectors in the sequence, to estimate the two unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>.</p>
<p>The likelihood function is defined as the joint probability density function of the observed data, viewed as a function of the unknown parameters. Since the terms in the sequence are independent, their joint density is equal to the product of their marginal densities. As a consequence, the likelihood function can be written as the product of the individual densities:</p>
<p><span class="math display">\[
L(\mu, \Sigma) = \prod_{j=1}^n f_X(x_j) = \prod_{j=1}^n \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right)
\]</span> <span class="math display">\[
= \frac{1}{(2\pi)^{nk/2} \det(\Sigma)^{n/2}} \exp\left(-\frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right).
\]</span> The log-likelihood function is <span class="math display">\[
\ell(\mu, \Sigma) = -\frac{nk}{2} \ln(2\pi) - \frac{n}{2} \ln(\det(\Sigma)) - \frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu).
\]</span> The likelihood function is well-defined only if <span class="math inline">\(\det(\Sigma)&gt;0\)</span>.</p>
</section>
<section id="introduction-to-gaussian-processes" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="introduction-to-gaussian-processes"><span class="header-section-number">6.7</span> Introduction to Gaussian Processes</h2>
<p>The concept of GP (Gaussian Process) regression can be understood as a simple extension of linear modeling. It is worth noting that this approach goes by various names and acronyms, including “kriging,” a term derived from geostatistics, as introduced by Matheron in 1963. Additionally, it is referred to as Gaussian spatial modeling or a Gaussian stochastic process, and machine learning (ML) researchers often use the term Gaussian process regression (GPR). In all of these instances, the central focus is on regression. This involves training on both inputs and outputs, with the ultimate objective of making predictions and quantifying uncertainty (referred to as uncertainty quantification or UQ).</p>
<p>However, it’s important to emphasize that GPs are not a universal solution for every problem. Specialized tools may outperform GPs in specific, non-generic contexts, and GPs have their own set of limitations that need to be considered.</p>
<section id="gaussian-process-prior" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="gaussian-process-prior"><span class="header-section-number">6.7.1</span> Gaussian Process Prior</h3>
<p>In the context of GP, any finite collection of realizations, which is represented by <span class="math inline">\(n\)</span> observations, is modeled as having a multivariate normal (MVN) distribution. The characteristics of these realizations can be fully described by two key parameters:</p>
<ol type="1">
<li>Their mean, denoted as an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mu\)</span>.</li>
<li>The covariance matrix, denoted as an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\Sigma\)</span>. This covariance matrix encapsulates the relationships and variability between the individual realizations within the collection.</li>
</ol>
</section>
<section id="covariance-function" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="covariance-function"><span class="header-section-number">6.7.2</span> Covariance Function</h3>
<p>The covariance function is defined by inverse exponentiated squared Euclidean distance: <span class="math display">\[
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 \},
\]</span> where <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{x}'\)</span> are two points in the <span class="math inline">\(k\)</span>-dimensional input space and <span class="math inline">\(\| \cdot \|\)</span> denotes the Euclidean distance, i.e., <span class="math display">\[
|| \vec{x} - \vec{x}'||^2 = \sum_{i=1}^k (x_i - x_i')^2.
\]</span></p>
<p>An 1-d example is shown in <a href="#fig-exp2euclid" class="quarto-xref">Figure&nbsp;<span>6.5</span></a>.</p>
<div id="cell-fig-exp2euclid" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>visualize_inverse_exp_squared_distance(<span class="dv">5</span>, <span class="fl">0.0</span>, [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="fl">2.0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-exp2euclid" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-exp2euclid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-exp2euclid-output-1.png" width="571" height="411" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exp2euclid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: One-dim inverse exponentiated squared Euclidean distance
</figcaption>
</figure>
</div>
</div>
</div>
<p>The covariance function is also referred to as the kernel function. The <em>Gaussian</em> kernel uses an additional parameter, <span class="math inline">\(\sigma^2\)</span>, to control the rate of decay. This parameter is referred to as the length scale or the characteristic length scale. The covariance function is then defined as</p>
<p><span id="eq-Sigma"><span class="math display">\[
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 / (2 \sigma^2) \}.
\tag{6.2}\]</span></span></p>
<p>The covariance decays exponentially fast as <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{x}'\)</span> become farther apart. Observe that</p>
<p><span class="math display">\[
\Sigma(\vec{x},\vec{x}) = 1
\]</span> and</p>
<p><span class="math display">\[
\Sigma(\vec{x}, \vec{x}') &lt; 1
\]</span> for <span class="math inline">\(\vec{x} \neq \vec{x}'\)</span>. The function <span class="math inline">\(\Sigma(\vec{x},\vec{x}')\)</span> must be positive definite.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Positive Definiteness
</div>
</div>
<div class="callout-body-container callout-body">
<p>Positive definiteness in the context of the covariance matrix <span class="math inline">\(\Sigma_n\)</span> is a fundamental requirement. It is determined by evaluating <span class="math inline">\(\Sigma(x_i, x_j)\)</span> at pairs of <span class="math inline">\(n\)</span> <span class="math inline">\(\vec{x}\)</span>-values, denoted as <span class="math inline">\(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\)</span>. The condition for positive definiteness is that for all <span class="math inline">\(\vec{x}\)</span> vectors that are not equal to zero, the expression <span class="math inline">\(\vec{x}^\top \Sigma_n \vec{x}\)</span> must be greater than zero. This property is essential when intending to use <span class="math inline">\(\Sigma_n\)</span> as a covariance matrix in multivariate normal (MVN) analysis. It is analogous to the requirement in univariate Gaussian distributions where the variance parameter, <span class="math inline">\(\sigma^2\)</span>, must be positive.</p>
</div>
</div>
<p>Gaussian Processes (GPs) can be effectively utilized to generate random data that follows a smooth functional relationship. The process involves the following steps:</p>
<ol type="1">
<li>Select a set of <span class="math inline">\(\vec{x}\)</span>-values, denoted as <span class="math inline">\(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\)</span>.</li>
<li>Define the covariance matrix <span class="math inline">\(\Sigma_n\)</span> by evaluating <span class="math inline">\(\Sigma_n^{ij} = \Sigma(\vec{x}_i, \vec{x}_j)\)</span> for <span class="math inline">\(i, j = 1, 2, \ldots, n\)</span>.</li>
<li>Generate an <span class="math inline">\(n\)</span>-variate realization <span class="math inline">\(Y\)</span> that follows a multivariate normal distribution with a mean of zero and a covariance matrix <span class="math inline">\(\Sigma_n\)</span>, expressed as <span class="math inline">\(Y \sim \mathcal{N}_n(0, \Sigma_n)\)</span>.</li>
<li>Visualize the result by plotting it in the <span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span> plane.</li>
</ol>
</section>
<section id="construction-of-the-covariance-matrix" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="construction-of-the-covariance-matrix"><span class="header-section-number">6.7.3</span> Construction of the Covariance Matrix</h3>
<p>Here is an one-dimensional example. The process begins by creating an input grid using <span class="math inline">\(\vec{x}\)</span>-values. This grid consists of 100 elements, providing the basis for further analysis and visualization.</p>
<div id="69b6213a" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the context of this discussion, the construction of the covariance matrix, denoted as <span class="math inline">\(\Sigma_n\)</span>, relies on the concept of inverse exponentiated squared Euclidean distances. However, it’s important to note that a modification is introduced later in the process. Specifically, the diagonal of the covariance matrix is augmented with a small value, represented as “eps” or <span class="math inline">\(\epsilon\)</span>.</p>
<p>The reason for this augmentation is that while inverse exponentiated distances theoretically ensure the covariance matrix’s positive definiteness, in practical applications, the matrix can sometimes become numerically ill-conditioned. By adding a small value to the diagonal, such as <span class="math inline">\(\epsilon\)</span>, this ill-conditioning issue is mitigated. In this context, <span class="math inline">\(\epsilon\)</span> is often referred to as “jitter.”</p>
<div id="0868408f" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> cholesky, solve</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> multivariate_normal</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Sigma(X, sigma2):</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((k, n, n))</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, n):</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>                D[l, i, j] <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma2[l])<span class="op">*</span>(X[i,l] <span class="op">-</span> X[j,l])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="bu">sum</span>(D)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">+</span> D.T</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span>D)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1094017a" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> np.array([<span class="fl">1.0</span>])</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> build_Sigma(X, sigma2)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(Sigma[:<span class="dv">3</span>,:], <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>array([[1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783, 0.726,
        0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278, 0.236,
        0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044, 0.034,
        0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003, 0.002,
        0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783,
        0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278,
        0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044,
        0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003,
        0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.98 , 0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835,
        0.783, 0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325,
        0.278, 0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056,
        0.044, 0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004,
        0.003, 0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ]])</code></pre>
</div>
</div>
<div id="2a13df50" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(Sigma, cmap<span class="op">=</span><span class="st">'hot'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-35-output-1.png" width="490" height="416" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="generation-of-random-samples-and-plotting-the-realizations-of-the-random-function" class="level3" data-number="6.7.4">
<h3 data-number="6.7.4" class="anchored" data-anchor-id="generation-of-random-samples-and-plotting-the-realizations-of-the-random-function"><span class="header-section-number">6.7.4</span> Generation of Random Samples and Plotting the Realizations of the Random Function</h3>
<p>In the context of the multivariate normal distribution, the next step is to utilize the previously constructed covariance matrix denoted as <code>Sigma</code>. It is used as an essential component in generating random samples from the multivariate normal distribution.</p>
<p>The function <code>multivariate_normal</code> is employed for this purpose. It serves as a random number generator specifically designed for the multivariate normal distribution. In this case, the mean of the distribution is set equal to <code>mean</code>, and the covariance matrix is provided as <code>Psi</code>. The argument <code>size</code> specifies the number of realizations, which, in this specific scenario, is set to one.</p>
<p>By default, the mean vector is initialized to zero. To match the number of samples, which is equivalent to the number of rows in the <code>X</code> and <code>Sigma</code> matrices, the argument <code>zeros(n)</code> is used, where <code>n</code> represents the number of samples (here taken from the size of the matrix, e.g.,: <code>Sigma.shape[0]</code>).</p>
<div id="9a854ff0" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">12345</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> rng.multivariate_normal(zeros(Sigma.shape[<span class="dv">0</span>]), Sigma, size <span class="op">=</span> <span class="dv">1</span>, check_valid<span class="op">=</span><span class="st">"raise"</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>Y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>(100, 1)</code></pre>
</div>
</div>
<p>Now we can plot the results, i.e., a finite realization of the random function <span class="math inline">\(Y()\)</span> under a GP prior with a particular covariance structure. We will plot those <code>X</code> and <code>Y</code> pairs as connected points on an <span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span> plane.</p>
<div id="cell-fig-mvn1-1" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, Y)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Realization of Random Functions under a GP prior.</span><span class="ch">\n</span><span class="st"> sigma2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(sigma2[<span class="dv">0</span>]))</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn1-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn1-1-output-1.png" width="582" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn1-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: Realization of one random function under a GP prior. sigma2: 1.0
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-mvn1-3" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">12345</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> rng.multivariate_normal(zeros(Sigma.shape[<span class="dv">0</span>]), Sigma, size <span class="op">=</span> <span class="dv">3</span>, check_valid<span class="op">=</span><span class="st">"raise"</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>plt.plot(X, Y.T)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Realization of Three Random Functions under a GP prior.</span><span class="ch">\n</span><span class="st"> sigma2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(sigma2[<span class="dv">0</span>]))</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn1-3" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn1-3-output-1.png" width="569" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: Realization of three random functions under a GP prior. sigma2: 1.0
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="properties-of-the-1d-example" class="level3" data-number="6.7.5">
<h3 data-number="6.7.5" class="anchored" data-anchor-id="properties-of-the-1d-example"><span class="header-section-number">6.7.5</span> Properties of the 1d Example</h3>
<section id="several-bumps" class="level4" data-number="6.7.5.1">
<h4 data-number="6.7.5.1" class="anchored" data-anchor-id="several-bumps"><span class="header-section-number">6.7.5.1</span> Several Bumps:</h4>
<p>In this analysis, we observe several bumps in the <span class="math inline">\(x\)</span>-range of <span class="math inline">\([0,10]\)</span>. These bumps in the function occur because shorter distances exhibit high correlation, while longer distances tend to be essentially uncorrelated. This leads to variations in the function’s behavior:</p>
<ul>
<li>When <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> are one <span class="math inline">\(\sigma\)</span> unit apart, the correlation is <span class="math inline">\(\exp\left(-\sigma^2 / (2\sigma^2)\right) = \exp(-1/2) \approx 0.61\)</span>, i.e., a relative high correlation.</li>
<li><span class="math inline">\(2\sigma\)</span> apart means correlation <span class="math inline">\(\exp(− 2^2 /2) \approx 0.14\)</span>, i.e., only small correlation.</li>
<li><span class="math inline">\(4\sigma\)</span> apart means correlation <span class="math inline">\(\exp(− 4^2 /2) \approx 0.0003\)</span>, i.e., nearly no correlation—variables are considered independent for almost all practical application.</li>
</ul>
</section>
<section id="smoothness" class="level4" data-number="6.7.5.2">
<h4 data-number="6.7.5.2" class="anchored" data-anchor-id="smoothness"><span class="header-section-number">6.7.5.2</span> Smoothness:</h4>
<p>The function plotted in <a href="#fig-mvn1-1" class="quarto-xref">Figure&nbsp;<span>6.6</span></a> represents only a finite realization, which means that we have data for a limited number of pairs, specifically 100 points. These points appear smooth in a tactile sense because they are closely spaced, and the plot function connects the dots with lines to create the appearance of smoothness. The complete surface, which can be conceptually extended to an infinite realization over a compact domain, is exceptionally smooth in a calculus sense due to the covariance function’s property of being infinitely differentiable.</p>
</section>
<section id="scale-of-two" class="level4" data-number="6.7.5.3">
<h4 data-number="6.7.5.3" class="anchored" data-anchor-id="scale-of-two"><span class="header-section-number">6.7.5.3</span> Scale of Two:</h4>
<p>Regarding the scale of the <span class="math inline">\(Y\)</span> values, they have a range of approximately <span class="math inline">\([-2,2]\)</span>, with a 95% probability of falling within this range. In standard statistical terms, 95% of the data points typically fall within two standard deviations of the mean, which is a common measure of the spread or range of data.</p>
<div id="7e19be55" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> multivariate_normal</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Sigma(X, sigma2):</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((k, n, n))</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, n):</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>                D[l, i, j] <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma2[l])<span class="op">*</span>(X[i,l] <span class="op">-</span> X[j,l])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="bu">sum</span>(D)</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">+</span> D.T</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span>D)</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_mvn( a<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">10</span>, sigma2<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span><span class="dv">1</span>, n<span class="op">=</span><span class="dv">100</span>, show<span class="op">=</span><span class="va">True</span>):    </span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.linspace(a, b, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>    sigma2 <span class="op">=</span> np.array([sigma2])</span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>    Sigma <span class="op">=</span> build_Sigma(X, sigma2)</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">12345</span>)</span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> rng.multivariate_normal(zeros(Sigma.shape[<span class="dv">0</span>]), Sigma, size <span class="op">=</span> size, check_valid<span class="op">=</span><span class="st">"raise"</span>)</span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, Y.T)</span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Realization of Random Functions under a GP prior.</span><span class="ch">\n</span><span class="st"> sigma2: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(sigma2[<span class="dv">0</span>]))</span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> show:</span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>        plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-fig-mvn2" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>plot_mvn(a<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">10</span>, sigma2<span class="op">=</span><span class="fl">10.0</span>, size<span class="op">=</span><span class="dv">3</span>, n<span class="op">=</span><span class="dv">250</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn2-output-1.png" width="569" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: Realization of Random Functions under a GP prior. sigma2: 10
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-mvn5" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>plot_mvn(a<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">10</span>, sigma2<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">3</span>, n<span class="op">=</span><span class="dv">250</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-mvn5" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mvn5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-mvn5-output-1.png" width="569" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mvn5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: Realization of Random Functions under a GP prior. sigma2: 0.1
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="kriging-modeling-basics" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="kriging-modeling-basics"><span class="header-section-number">6.8</span> Kriging: Modeling Basics</h2>
<section id="the-kriging-idea-in-a-nutshell" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="the-kriging-idea-in-a-nutshell"><span class="header-section-number">6.8.1</span> The Kriging Idea in a Nutshell</h3>
<p>We consider observed data of an unknown function <span class="math inline">\(f\)</span> at <span class="math inline">\(n\)</span> points <span class="math inline">\(x_1, \ldots, x_n\)</span>, see <a href="#fig-unknownf" class="quarto-xref">Figure&nbsp;<span>6.10</span></a>. These measurements a considered as realizations of MVN random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span> with mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma_n\)</span> as shown in <a href="#fig-mvn1-3" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>, <a href="#fig-mvn2" class="quarto-xref">Figure&nbsp;<span>6.8</span></a> or <a href="#fig-mvn5" class="quarto-xref">Figure&nbsp;<span>6.9</span></a>. In Kriging, a more general covariance matrix (or equivalently, a correlation matrix <span class="math inline">\(\Psi\)</span>) is used, see <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>6.3</span></a>. Using a maximum likelihood approach, we can estimate the unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma_n\)</span> from the data so that the likelihood function is maximized.</p>
<div id="cell-fig-unknownf" class="cell" data-execution_count="41">
<div class="cell-output cell-output-display">
<div id="fig-unknownf" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unknownf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-unknownf-output-1.png" width="600" height="429" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unknownf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: Eight measurements of an unknown function
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-kriging-basis-function" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="the-kriging-basis-function"><span class="header-section-number">6.8.2</span> The Kriging Basis Function</h3>
<p><span class="math inline">\(k\)</span>-dimensional basis functions of the form <span id="eq-krigingbase"><span class="math display">\[
\psi(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp \left( - \sum_{l=1}^k \theta_l | x_{l}^{(i)} - x_{l}^{(j)} | ^{p_l} \right)
\tag{6.3}\]</span></span> are used in a method known as Kriging. Note, <span class="math inline">\(\vec{x}^{(i)}\)</span> denotes the <span class="math inline">\(k\)</span>-dim vector <span class="math inline">\(\vec{x}^{(i)}= (x_1^{(i)}, \ldots, x_k^{(i)})^T\)</span>.</p>
<p>The Kriging basis function is related to the 1-dim Gaussian basis function (<a href="#eq-Sigma" class="quarto-xref">Equation&nbsp;<span>6.2</span></a>), which is defined as <span id="eq-Sigma2"><span class="math display">\[
\Sigma(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp\{ - || \vec{x}^{(i)} - \vec{x}^{(j)}||^2 / (2\sigma^2) \}.
\tag{6.4}\]</span></span></p>
<p>There are some differences between Gaussian basis functions and Kriging basis functions:</p>
<ul>
<li>Where the Gaussian basis function has <span class="math inline">\(1/(2\sigma^2)\)</span>, the Kriging basis has a vector <span class="math inline">\(\theta = [\theta_1, \theta_2, \ldots, \theta_k]^T\)</span>.</li>
<li>The <span class="math inline">\(\theta\)</span> vector allows the width of the basis function to vary from dimension to dimension.</li>
<li>In the Gaussian basis function, the exponent is fixed at 2, Kriging allows this exponent <span class="math inline">\(p_l\)</span> to vary (typically from 1 to 2).</li>
</ul>
</section>
<section id="the-correlation-coefficient" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="the-correlation-coefficient"><span class="header-section-number">6.8.3</span> The Correlation Coefficient</h3>
<p>In a bivariate normal distribution, the covariance matrix and the correlation coefficient are closely related. The covariance matrix <span class="math inline">\(\Sigma\)</span> for a bivariate normal distribution is a <span class="math inline">\(2\times 2\)</span> matrix that looks like this:</p>
<p><span class="math display">\[
\Sigma =
\begin{pmatrix}
\sigma_1^2 &amp; \sigma_{12}\\
\sigma_{21} &amp; \sigma_2^2
\end{pmatrix},
\]</span> where <span class="math inline">\(\sigma_1^2\)</span> and <span class="math inline">\(\sigma_2^2\)</span> are the variances of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(\sigma_{12} = \sigma_{21}\)</span> is the covariance between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<p>The correlation coefficient, often denoted as <span class="math inline">\(\rho\)</span>, is a normalized measure of the linear relationship between two variables. It is calculated from the covariance and the standard deviations <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> (or the square roots of the variances) of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> as follows: <span class="math display">\[
\rho = \sigma_{12} / (\sqrt{\sigma_1^2} \times \sqrt{\sigma_2^2}) = \sigma_{12} / (\sigma_1 \times \sigma_2).
\]</span></p>
<p>So we can express the correlation coefficient <span class="math inline">\(\rho\)</span> in terms of the elements of the covariance matrix <span class="math inline">\(\Sigma\)</span>. It can be interpreted as follows: The correlation coefficient ranges from -1 to 1. A value of 1 means that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are perfectly positively correlated, a value of -1 means they are perfectly negatively correlated, and a value of 0 means they are uncorrelated. This gives the same information as the covariance, but on a standardized scale that does not depend on the units of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
</section>
<section id="covariance-matrix-and-correlation-matrix" class="level3" data-number="6.8.4">
<h3 data-number="6.8.4" class="anchored" data-anchor-id="covariance-matrix-and-correlation-matrix"><span class="header-section-number">6.8.4</span> Covariance Matrix and Correlation Matrix</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Covariance and Correlation (taken from <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Covariance is a measure of the correlation between two or more sets of random variables.</p>
<p><span class="math display">\[
\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
\]</span></p>
<p>From the covariance, we can derive the correlation</p>
<p><span id="eq-corrxy"><span class="math display">\[
\text{Corr}(X,Y) = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}} = \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y}.
\tag{6.5}\]</span></span></p>
<p>For a vector of random variables</p>
<p><span class="math display">\[
Y =
\begin{pmatrix}
(Y^{(l)}, \ldots, Y^{(n)})
\end{pmatrix}^T
\]</span></p>
<p>the covariance matrix is a matrix of covariances between the random variables</p>
<p><span class="math display">\[
\Sigma =
\text{Cov}(Y, Y) =
\begin{pmatrix}
\text{Cov}(Y^{(1)}, Y^{(1)}) &amp; \ldots &amp; \text{Cov}(Y^{(1)}, Y^{(n)}) \\
\vdots &amp; \ddots &amp; \vdots \\
\text{Cov}(Y^{(n)}, Y^{(1)}) &amp; \ldots &amp; \text{Cov}(Y^{(n)}, Y^{(n)})
\end{pmatrix},
\]</span></p>
<p>and from <a href="#eq-corrxy" class="quarto-xref">Equation&nbsp;<span>6.5</span></a></p>
<p><span class="math display">\[
\text{Cov}(Y, Y) = \sigma_Y^2 \text{Cor}(Y, Y).
\]</span></p>
</div>
</div>
<p>You can compute the correlation matrix <span class="math inline">\(\Psi\)</span> from a covariance matrix <span class="math inline">\(\Sigma\)</span> in Python using the numpy library. The correlation matrix is computed by dividing each element of the covariance matrix by the product of the standard deviations of the corresponding variables.</p>
<p>The function <code>covariance_to_correlation</code> first computes the standard deviations of the variables with <code>np.sqrt(np.diag(cov))</code>. It then computes the correlation matrix by dividing each element of the covariance matrix by the product of the standard deviations of the corresponding variables with <code>cov / np.outer(std_devs, std_devs)</code>.</p>
<div id="33a62161" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> covariance_to_correlation(cov):</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute standard deviations</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    std_devs <span class="op">=</span> np.sqrt(np.diag(cov))</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute correlation matrix</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    corr <span class="op">=</span> cov <span class="op">/</span> np.outer(std_devs, std_devs)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> corr</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="op">-</span><span class="dv">4</span>], [<span class="op">-</span><span class="dv">4</span>, <span class="dv">9</span>]])</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(covariance_to_correlation(cov))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[ 1.         -0.44444444]
 [-0.44444444  1.        ]]</code></pre>
</div>
</div>
</section>
<section id="the-kriging-model" class="level3" data-number="6.8.5">
<h3 data-number="6.8.5" class="anchored" data-anchor-id="the-kriging-model"><span class="header-section-number">6.8.5</span> The Kriging Model</h3>
<p>Consider sample data <span class="math inline">\(\vec{X}\)</span> and <span class="math inline">\(\vec{y}\)</span> from <span class="math inline">\(n\)</span> locations that are available in matrix form: <span class="math inline">\(\vec{X}\)</span> is a <span class="math inline">\((n \times k)\)</span> matrix, where <span class="math inline">\(k\)</span> denotes the problem dimension and <span class="math inline">\(\vec{y}\)</span> is a <span class="math inline">\((n\times 1)\)</span> vector.</p>
<p>The observed responses <span class="math inline">\(\vec{y}\)</span> are considered as if they are from a stochastic process, which will be denoted as <span class="math display">\[
\begin{pmatrix}
\vec{Y}(\vec{x}^{(1)})\\
\vdots\\
\vec{Y}(\vec{x}^{(n)})\\
\end{pmatrix}.
\]</span></p>
<p>The set of random vectors (also referred to as a <em>random field</em>) has a mean of <span class="math inline">\(\vec{1} \mu\)</span>, which is a <span class="math inline">\((n\times 1)\)</span> vector.</p>
</section>
<section id="correlations" class="level3" data-number="6.8.6">
<h3 data-number="6.8.6" class="anchored" data-anchor-id="correlations"><span class="header-section-number">6.8.6</span> Correlations</h3>
<p>The random vectors are correlated with each other using the basis function expression from <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>6.3</span></a>: <span class="math display">\[
\text{cor} \left(\vec{Y}(\vec{x}^{(i)}),\vec{Y}(\vec{x}^{(l)}) \right) = \exp\left\{ - \sum_{j=1}^k \theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\right\}.
\]</span></p>
<p>The <span class="math inline">\((n \times n)\)</span> correlation matrix of the observed sample data is</p>
<p><span class="math display">\[
\vec{\Psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right) &amp; \ldots &amp;
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right)\\
\vdots  &amp; \vdots &amp;  \vdots\\
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right)&amp;
\ldots &amp;
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)})
\right)
\end{pmatrix}.
\]</span></p>
<p>Note: correlations depend on the absolute distances between sample points <span class="math inline">\(|x_j^{(n)} - x_j^{(n)}|\)</span> and the parameters <span class="math inline">\(p_j\)</span> and <span class="math inline">\(\theta_j\)</span>.</p>
<p>Correlation is intuitive, because when two points move close together, then <span class="math inline">\(|x_l^{(i)} - x_l| \to 0\)</span> and <span class="math inline">\(\exp(-|x_l^{(i)} - x_l| \to 1\)</span>, points show very close correlation and <span class="math inline">\(Y(x_l^{(i)}) = Y(x_l)\)</span>.</p>
<p><span class="math inline">\(\theta\)</span> can be seen as a width parameter:</p>
<ul>
<li>low <span class="math inline">\(\theta_j\)</span> means that all points will have a high correlation, with <span class="math inline">\(Y(x_j)\)</span> being similar across the sample.</li>
<li>high <span class="math inline">\(\theta_j\)</span> means that there is a significant difference between the <span class="math inline">\(Y(x_j)\)</span>’s.</li>
<li><span class="math inline">\(\theta_j\)</span> is a measure of how active the function we are approximating is.</li>
<li>High <span class="math inline">\(\theta_j\)</span> indicate important parameters, see <a href="#fig-theta12" class="quarto-xref">Figure&nbsp;<span>6.11</span></a>.</li>
</ul>
<div id="cell-fig-theta12" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>visualize_inverse_exp_squared_distance(<span class="dv">5</span>, <span class="dv">0</span>, theta_values<span class="op">=</span>[<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="fl">2.0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-theta12" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-theta12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-theta12-output-1.png" width="571" height="411" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-theta12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.11: Theta set to 1/2, 1, and 2
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: The Correlation Matrix (Detailed Computation)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(n=4\)</span> and <span class="math inline">\(k=3\)</span>. The sample plan is represented by the following matrix <span class="math inline">\(X\)</span>: <span class="math display">\[
X = \begin{pmatrix} x_{11} &amp; x_{12} &amp; x_{13}\\
x_{21} &amp; x_{22} &amp; x_{23}\\
x_{31} &amp; x_{32} &amp; x_{33}\\
x_{41} &amp; x_{42} &amp; x_{43}\\
\end{pmatrix}
\]</span></p>
<p>To compute the elements of the matrix <span class="math inline">\(\Psi\)</span>, the following <span class="math inline">\(k\)</span> (one for each of the <span class="math inline">\(k\)</span> dimensions) <span class="math inline">\((n,n)\)</span>-matrices have to be computed: <span class="math display">\[
D_1 = \begin{pmatrix} x_{11} - x_{11} &amp; x_{11} - x_{21} &amp; x_{11} -x_{31} &amp; x_{11} - x_{41} \\  x_{21} - x_{11} &amp; x_{21} - x_{21} &amp; x_{21} -x_{31} &amp; x_{21} - x_{41} \\ x_{31} - x_{11} &amp; x_{31} - x_{21} &amp; x_{31} -x_{31} &amp; x_{31} - x_{41} \\ x_{41} - x_{11} &amp; x_{41} - x_{21} &amp; x_{41} -x_{31} &amp; x_{41} - x_{41} \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_2 = \begin{pmatrix} x_{12} - x_{12} &amp; x_{12} - x_{22} &amp; x_{12} -x_{32} &amp; x_{12} - x_{42} \\  x_{22} - x_{12} &amp; x_{22} - x_{22} &amp; x_{22} -x_{32} &amp; x_{22} - x_{42} \\ x_{32} - x_{12} &amp; x_{32} - x_{22} &amp; x_{32} -x_{32} &amp; x_{32} - x_{42} \\ x_{42} - x_{12} &amp; x_{42} - x_{22} &amp; x_{42} -x_{32} &amp; x_{42} - x_{42} \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \begin{pmatrix} x_{13} - x_{13} &amp; x_{13} - x_{23} &amp; x_{13} -x_{33} &amp; x_{13} - x_{43} \\  x_{23} - x_{13} &amp; x_{23} - x_{23} &amp; x_{23} -x_{33} &amp; x_{23} - x_{43} \\ x_{33} - x_{13} &amp; x_{33} - x_{23} &amp; x_{33} -x_{33} &amp; x_{33} - x_{43} \\ x_{43} - x_{13} &amp; x_{43} - x_{23} &amp; x_{43} -x_{33} &amp; x_{43} - x_{43} \\\end{pmatrix}
\]</span></p>
<p>Since the matrices are symmetric and the main diagonals are zero, it is sufficient to compute the following matrices: <span class="math display">\[
D_1 = \begin{pmatrix} 0 &amp; x_{11} - x_{21} &amp; x_{11} -x_{31} &amp; x_{11} - x_{41} \\  0 &amp;  0 &amp; x_{21} -x_{31} &amp; x_{21} - x_{41} \\ 0 &amp; 0 &amp; 0 &amp; x_{31} - x_{41} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span> <span class="math display">\[
D_2 = \begin{pmatrix} 0 &amp; x_{12} - x_{22} &amp; x_{12} -x_{32} &amp; x_{12} - x_{42} \\  0 &amp; 0 &amp; x_{22} -x_{32} &amp; x_{22} - x_{42} \\ 0 &amp; 0 &amp; 0 &amp; x_{32} - x_{42} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \begin{pmatrix} 0 &amp; x_{13} - x_{23} &amp; x_{13} -x_{33} &amp; x_{13} - x_{43} \\  0 &amp; 0 &amp; x_{23} -x_{33} &amp; x_{23} - x_{43} \\ 0 &amp; 0 &amp; 0 &amp; x_{33} - x_{43} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>We will consider <span class="math inline">\(p_l=2\)</span>. The differences will be squared and multiplied by <span class="math inline">\(\theta_i\)</span>, i.e.:</p>
<p><span class="math display">\[
D_1 = \theta_1 \begin{pmatrix} 0 &amp; (x_{11} - x_{21})^2 &amp; (x_{11} -x_{31})^2 &amp; (x_{11} - x_{41})^2 \\  0 &amp;  0 &amp; (x_{21} -x_{31})^2 &amp; (x_{21} - x_{41})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{31} - x_{41})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_2 = \theta_2 \begin{pmatrix} 0 &amp; (x_{12} - x_{22})^2 &amp; (x_{12} -x_{32})^2 &amp; (x_{12} - x_{42})^2 \\  0 &amp; 0 &amp; (x_{22} -x_{32})^2 &amp; (x_{22} - x_{42})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{32} - x_{42})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \theta_3 \begin{pmatrix} 0 &amp; (x_{13} - x_{23})^2 &amp; (x_{13} -x_{33})^2 &amp; (x_{13} - x_{43})^2 \\  0 &amp; 0 &amp; (x_{23} -x_{33})^2 &amp; (x_{23} - x_{43})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{33} - x_{43})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>The sum of the three matrices <span class="math inline">\(D=D_1+ D_2 + D_3\)</span> will be calculated next:</p>
<p><span class="math display">\[
\begin{pmatrix} 0 &amp;
\theta_1  (x_{11} - x_{21})^2 + \theta_2 (x_{12} - x_{22})^2 + \theta_3  (x_{13} - x_{23})^2  &amp;
\theta_1 (x_{11} -x_{31})^2 + \theta_2  (x_{12} -x_{32})^2 + \theta_3  (x_{13} -x_{33})^2 &amp;
\theta_1  (x_{11} - x_{41})^2 + \theta_2  (x_{12} - x_{42})^2 + \theta_3 (x_{13} - x_{43})^2
\\  0 &amp;  0 &amp;
\theta_1  (x_{21} -x_{31})^2 + \theta_2 (x_{22} -x_{32})^2 + \theta_3  (x_{23} -x_{33})^2 &amp;
\theta_1  x_{21} - x_{41})^2 + \theta_2  (x_{22} - x_{42})^2 + \theta_3 (x_{23} - x_{43})^2
\\ 0 &amp; 0 &amp; 0 &amp;
\theta_1 (x_{31} - x_{41})^2 + \theta_2 (x_{32} - x_{42})^2 + \theta_3 (x_{33} - x_{43})^2
\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>Finally, <span class="math display">\[ \Psi = \exp(-D)\]</span> is computed.</p>
<p>Next, we will demonstrate how this computation can be implemented in Python.</p>
<div id="a014728c" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> (array, zeros, power, ones, exp, multiply,</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>                    eye, linspace, mat, spacing, sqrt, arange,</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>                    append, ravel)</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> cholesky, solve</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>], [<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>], [<span class="dv">101</span>, <span class="dv">100</span>, <span class="dv">100</span>]])</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>array([[  1,   0,   0],
       [  0,   1,   0],
       [100, 100, 100],
       [101, 100, 100]])</code></pre>
</div>
</div>
<div id="2d43991f" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Psi(X, theta):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((k, n, n))</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, n):</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>                D[l, i, j] <span class="op">=</span> theta[l]<span class="op">*</span>(X[i,l] <span class="op">-</span> X[j,l])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="bu">sum</span>(D)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">+</span> D.T</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span>D)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="30aa4e63" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>Psi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])</code></pre>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: The Correlation Matrix (Using Existing Functions)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The same result as computed in the previous example can be obtained with existing python functions, e.g., from the package <code>scipy</code>.</p>
<div id="3a94522e" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> squareform</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> pdist</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Psi(X, theta, eps<span class="op">=</span>sqrt(spacing(<span class="dv">1</span>))):</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span> squareform(pdist(X,</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>                            metric<span class="op">=</span><span class="st">'sqeuclidean'</span>,</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>                            out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>                            w<span class="op">=</span>theta))) <span class="op">+</span>  multiply(eye(X.shape[<span class="dv">0</span>]),</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>                                                   eps)</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta, eps<span class="op">=</span><span class="fl">.0</span>)</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>Psi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="the-condition-number" class="level3" data-number="6.8.7">
<h3 data-number="6.8.7" class="anchored" data-anchor-id="the-condition-number"><span class="header-section-number">6.8.7</span> The Condition Number</h3>
<p>A small value, <code>eps</code>, can be passed to the function <code>build_Psi</code> to improve the condition number. For example, <code>eps=sqrt(spacing(1))</code> can be used. The numpy function <code>spacing()</code> returns the distance between a number and its nearest adjacent number.</p>
<p>The condition number of a matrix is a measure of its sensitivity to small changes in its elements. It is used to estimate how much the output of a function will change if the input is slightly altered.</p>
<p>A matrix with a low condition number is well-conditioned, which means its behavior is relatively stable, while a matrix with a high condition number is ill-conditioned, meaning its behavior is unstable with respect to numerical precision.</p>
<div id="6ab2ff6c" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a well-conditioned matrix (low condition number)</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="fl">0.1</span>], [<span class="fl">0.1</span>, <span class="dv">1</span>]])</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Condition number of A: "</span>, np.linalg.cond(A))</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define an ill-conditioned matrix (high condition number)</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="fl">0.99999999</span>], [<span class="fl">0.99999999</span>, <span class="dv">1</span>]])</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Condition number of B: "</span>, np.linalg.cond(B))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Condition number of A:  1.2222222222222225
Condition number of B:  200000000.53159264</code></pre>
</div>
</div>
<div id="13dc6dbe" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>np.linalg.cond(Psi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>2.163953413738652</code></pre>
</div>
</div>
</section>
<section id="mle-to-estimate-theta-and-p" class="level3" data-number="6.8.8">
<h3 data-number="6.8.8" class="anchored" data-anchor-id="mle-to-estimate-theta-and-p"><span class="header-section-number">6.8.8</span> MLE to estimate <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></h3>
<p>We know what the correlations mean, but how do we estimate the values of <span class="math inline">\(\theta_j\)</span> and where does our observed data <span class="math inline">\(y\)</span> come in? To estimate the values of <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span>, they are chosen to maximize the likelihood of <span class="math inline">\(\vec{y}\)</span>, which can be expressed in terms of the sample data <span class="math display">\[L\left(\vec{Y}(\vec{x}^{(1)}), \ldots, \vec{Y}(\vec{x}^{(n)}) | \mu, \sigma \right) = \frac{1}{(2\pi \sigma)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left\{ \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}\right\},\]</span> and formulated as the log-likelihood: <span class="math display">\[\ln(L) = - \frac{n}{2} \ln(2\pi \sigma) - \frac{1}{2} \ln |\vec{\Psi}| \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}.\]</span></p>
<p>Optimization of the log-likelihood by taking derivatives with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> results in <span class="math display">\[\hat{\mu} = \frac{\vec{1}^T \vec{\Psi}^{-1} \vec{y}^T}{\vec{1}^T \vec{\Psi}^{-1} \vec{1}^T}\]</span> and <span class="math display">\[\hat{\sigma} = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{n}.\]</span></p>
<p>Combining the equations leads to the concentrated log-likelihood: <span id="eq-concentrated-loglikelihood"><span class="math display">\[\ln(L) = - \frac{n}{2} \ln(\hat{\sigma}) - \frac{1}{2} \ln |\vec{\Psi}|. \tag{6.6}\]</span></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note: The Concentrated Log-Likelihood
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The first term in <a href="#eq-concentrated-loglikelihood" class="quarto-xref">Equation&nbsp;<span>6.6</span></a> requires information about the measured point (observations) <span class="math inline">\(y_i\)</span>.</li>
<li>To maximize <span class="math inline">\(\ln(L)\)</span>, optimal values of <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span> are determined numerically, because the equation is not differentiable.</li>
</ul>
</div>
</div>
</section>
<section id="tuning-theta-and-p" class="level3" data-number="6.8.9">
<h3 data-number="6.8.9" class="anchored" data-anchor-id="tuning-theta-and-p"><span class="header-section-number">6.8.9</span> Tuning <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></h3>
<p>Optimizers such as Nelder-Mead, Conjugate Gradient, or Simulated Annealing can be used to determine optimal values for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span>. After the optimization, the correlation matrix <span class="math inline">\(\Psi\)</span> is build with the optimized <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span> values. This is best (most likely) Kriging model for the given data <span class="math inline">\(y\)</span>.</p>
</section>
</section>
<section id="kriging-prediction" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="kriging-prediction"><span class="header-section-number">6.9</span> Kriging Prediction</h2>
<section id="the-augmented-correlation-matrix" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="the-augmented-correlation-matrix"><span class="header-section-number">6.9.1</span> The Augmented Correlation Matrix</h3>
<p>We will use the Kriging correlation <span class="math inline">\(\Psi\)</span> to predict new values based on the observed data. The matrix algebra involved for calculating the likelihood is the most computationally intensive part of the Kriging process. Care must be taken that the computer code is as efficient as possible.</p>
<p>Basic elements of the Kriging based surrogate optimization such as interpolation, expected improvement, and regression are presented. The presentation follows the approach described in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span> and <span class="citation" data-cites="bart21i">Bartz et al. (<a href="references.html#ref-bart21i" role="doc-biblioref">2022</a>)</span>.</p>
<p>Main idea for prediction is that the new <span class="math inline">\(Y(\vec{x})\)</span> should be consistent with the old sample data <span class="math inline">\(X\)</span>. For a new prediction <span class="math inline">\(\hat{y}\)</span> at <span class="math inline">\(\vec{x}\)</span>, the value of <span class="math inline">\(\hat{y}\)</span> is chosen so that it maximizes the likelihood of the sample data <span class="math inline">\(\vec{X}\)</span> and the prediction, given the (optimized) correlation parameter <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span> from above. The observed data <span class="math inline">\(\vec{y}\)</span> is augmented with the new prediction <span class="math inline">\(\hat{y}\)</span> which results in the augmented vector <span class="math inline">\(\vec{\tilde{y}} = ( \vec{y}^T, \hat{y})^T\)</span>. A vector of correlations between the observed data and the new prediction is defined as</p>
<p><span class="math display">\[ \vec{\psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(1)}),
\vec{Y}(\vec{x})
\right) \\
\vdots  \\
\text{cor}\left(
\vec{Y}(\vec{x}^{(n)}),
\vec{Y}(\vec{x})
\right)
\end{pmatrix}
=
\begin{pmatrix}
\vec{\psi}^{(1)}\\
\vdots\\
\vec{\psi}^{(n)}
\end{pmatrix}.
\]</span> The augmented correlation matrix is constructed as <span class="math display">\[ \tilde{\vec{\Psi}} =
\begin{pmatrix}
\vec{\Psi} &amp; \vec{\psi} \\
\vec{\psi}^T &amp; 1
\end{pmatrix}.
\]</span></p>
<p>The log-likelihood of the augmented data is <span class="math display">\[
\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\hat{\sigma}^2) - \frac{1}{2} \ln |\vec{\hat{\Psi}}| -  \frac{(\vec{\tilde{y}} - \vec{1}\hat{\mu})^T \vec{\tilde{\Psi}}^{-1}(\vec{\tilde{y}} - \vec{1}\hat{\mu})}{2 \hat{\sigma}^2}.
\]</span></p>
<p>The MLE for <span class="math inline">\(\hat{y}\)</span> can be calculated as <span id="eq-mle-yhat"><span class="math display">\[
\hat{y}(\vec{x}) = \hat{\mu} + \vec{\psi}^T \vec{\tilde{\Psi}}^{-1} (\vec{y} - \vec{1}\hat{\mu}).
\tag{6.7}\]</span></span></p>
</section>
<section id="properties-of-the-predictor" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="properties-of-the-predictor"><span class="header-section-number">6.9.2</span> Properties of the Predictor</h3>
<p><a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>6.7</span></a> reveals two important properties of the Kriging predictor:</p>
<ol type="1">
<li>Basis functions: The basis function impacts the vector <span class="math inline">\(\vec{\psi}\)</span>, which contains the <span class="math inline">\(n\)</span> correlations between the new point <span class="math inline">\(\vec{x}\)</span> and the observed locations. Values from the <span class="math inline">\(n\)</span> basis functions are added to a mean base term <span class="math inline">\(\mu\)</span> with weightings <span class="math inline">\(\vec{w} = \vec{\tilde{\Psi}}^{(-1)} (\vec{y} - \vec{1}\hat{\mu})\)</span>.</li>
<li>Interpolation: The predictions interpolate the sample data. When calculating the prediction at the <span class="math inline">\(i\)</span>th sample point, <span class="math inline">\(\vec{x}^{(i)}\)</span>, the <span class="math inline">\(i\)</span>th column of <span class="math inline">\(\vec{\Psi}^{-1}\)</span> is <span class="math inline">\(\vec{\psi}\)</span>, and <span class="math inline">\(\vec{\psi}  \vec{\Psi}^{-1}\)</span> is the <span class="math inline">\(i\)</span>th unit vector. Hence, <span class="math inline">\(\hat{y}(\vec{x}^{(i)}) = y^{(i)}\)</span>.</li>
</ol>
</section>
</section>
<section id="kriging-example-sinusoid-function" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="kriging-example-sinusoid-function"><span class="header-section-number">6.10</span> Kriging Example: Sinusoid Function</h2>
<p>Toy example in 1d where the response is a simple sinusoid measured at eight equally spaced <span class="math inline">\(x\)</span>-locations in the span of a single period of oscillation.</p>
<section id="calculating-the-correlation-matrix-psi" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="calculating-the-correlation-matrix-psi"><span class="header-section-number">6.10.1</span> Calculating the Correlation Matrix <span class="math inline">\(\Psi\)</span></h3>
<p>The correlation matrix <span class="math inline">\(\Psi\)</span> is based on the pairwise squared distances between the input locations. Here we will use <span class="math inline">\(n=8\)</span> sample locations and <span class="math inline">\(\theta\)</span> is set to 1.0.</p>
<div id="2bfcc4c5" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="co"># theta should be an array (of one value, for the moment, will be changed later)</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="fl">1.0</span>])</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Evaluate at sample points</p>
<div id="b46c4363" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7080da9a" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"bo"</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Sin(x) evaluated at </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> points"</span>)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-54-output-1.png" width="590" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="computing-the-psi-vector" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="computing-the-psi-vector"><span class="header-section-number">6.10.2</span> Computing the <span class="math inline">\(\psi\)</span> Vector</h3>
<p>Distances between testing locations <span class="math inline">\(x\)</span> and training data locations <span class="math inline">\(X\)</span>.</p>
<div id="92e2c19c" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_psi(X, x, theta, eps<span class="op">=</span>sqrt(spacing(<span class="dv">1</span>))):</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> zeros((n, m))</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">*</span> ones(k)</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((n, m))</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> cdist(x.reshape(<span class="op">-</span><span class="dv">1</span>, k),</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>              X.reshape(<span class="op">-</span><span class="dv">1</span>, k),</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>              metric<span class="op">=</span><span class="st">'sqeuclidean'</span>,</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>              out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>              w<span class="op">=</span>theta)</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(D.shape)</span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> exp(<span class="op">-</span>D)</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return psi transpose to be consistent with the literature</span></span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(psi.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="predicting-at-new-locations" class="level3" data-number="6.10.3">
<h3 data-number="6.10.3" class="anchored" data-anchor-id="predicting-at-new-locations"><span class="header-section-number">6.10.3</span> Predicting at New Locations</h3>
<p>We would like to predict at <span class="math inline">\(m = 100\)</span> new locations in the interval <span class="math inline">\([0, 2\pi]\)</span>. The new locations are stored in the variable <code>x</code>.</p>
<div id="ae663b89" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, m, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>psi <span class="op">=</span> build_psi(X, x, theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(100, 8)</code></pre>
</div>
</div>
<p>Computation of the predictive equations.</p>
<div id="57cc6c0e" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> cholesky(Psi).T</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>one <span class="op">=</span> np.ones(n).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> (one.T.dot(solve(U, solve(U.T, y)))) <span class="op">/</span> one.T.dot(solve(U, solve(U.T, one)))</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> mu <span class="op">*</span> ones(m).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="op">+</span> psi.T.dot(solve(U, solve(U.T, y <span class="op">-</span> one <span class="op">*</span> mu)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To compute <span class="math inline">\(f\)</span>, <a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>6.7</span></a> is used.</p>
</section>
<section id="visualization" class="level3" data-number="6.10.4">
<h3 data-number="6.10.4" class="anchored" data-anchor-id="visualization"><span class="header-section-number">6.10.4</span> Visualization</h3>
<div id="5acee0c1" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x, f, color <span class="op">=</span> <span class="st">"orange"</span>, label<span class="op">=</span><span class="st">"Fitted"</span>)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, np.sin(x), color <span class="op">=</span> <span class="st">"grey"</span>, label<span class="op">=</span><span class="st">"Original"</span>)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"bo"</span>, label<span class="op">=</span><span class="st">"Measurements"</span>)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Kriging prediction of sin(x) with </span><span class="sc">{}</span><span class="st"> points.</span><span class="ch">\n</span><span class="st"> theta: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(n, theta[<span class="dv">0</span>]))</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-58-output-1.png" width="590" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="cholesky-example-with-two-points" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="cholesky-example-with-two-points"><span class="header-section-number">6.11</span> Cholesky Example With Two Points</h2>
<section id="cholesky-decomposition" class="level3" data-number="6.11.1">
<h3 data-number="6.11.1" class="anchored" data-anchor-id="cholesky-decomposition"><span class="header-section-number">6.11.1</span> Cholesky Decomposition</h3>
<p>We consider <span class="math inline">\(k=1\)</span> and <span class="math inline">\(n=2\)</span> sample points. The sample points are located at <span class="math inline">\(x_1=1\)</span> and <span class="math inline">\(x_2=5\)</span>. The response values are <span class="math inline">\(y_1=2\)</span> and <span class="math inline">\(y_2=10\)</span>. The correlation parameter is <span class="math inline">\(\theta=1\)</span> and <span class="math inline">\(p\)</span> is set to <span class="math inline">\(1\)</span>. Using <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>6.3</span></a>, we can compute the correlation matrix <span class="math inline">\(\Psi\)</span>:</p>
<p><span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; e^{-1}\\
e^{-1} &amp; 1
\end{pmatrix}.
\]</span></p>
<p>To determine MLE as in <a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>6.7</span></a>, we need to compute <span class="math inline">\(\Psi^{-1}\)</span>:</p>
<p><span class="math display">\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e &amp; -1\\
-1 &amp; e
\end{pmatrix}.
\]</span></p>
<p>Cholesky-decomposition of <span class="math inline">\(\Psi\)</span> is recommended to compute <span class="math inline">\(\Psi^{-1}\)</span>. Cholesky decomposition is a decomposition of a positive definite symmetric matrix into the product of a lower triangular matrix <span class="math inline">\(L\)</span>, a diagonal matrix <span class="math inline">\(D\)</span> and the transpose of <span class="math inline">\(L\)</span>, which is denoted as <span class="math inline">\(L^T\)</span>. Consider the following example:</p>
<p><span class="math display">\[
LDL^T=
\begin{pmatrix}
1 &amp; 0 \\
l_{21} &amp; 1
\end{pmatrix}
\begin{pmatrix}
d_{11} &amp; 0 \\
0 &amp; d_{22}
\end{pmatrix}
\begin{pmatrix}
1 &amp; l_{21} \\
0 &amp; 1
\end{pmatrix}=
\]</span></p>
<p><span id="eq-cholex"><span class="math display">\[
\begin{pmatrix}
d_{11} &amp; 0 \\
d_{11} l_{21} &amp; d_{22}
\end{pmatrix}
\begin{pmatrix}
1 &amp; l_{21} \\
0 &amp; 1
\end{pmatrix}
=
\begin{pmatrix}
d_{11} &amp; d_{11} l_{21} \\
d_{11} l_{21} &amp; d_{11} l_{21}^2 + d_{22}
\end{pmatrix}.
\tag{6.8}\]</span></span></p>
<p>Using <a href="#eq-cholex" class="quarto-xref">Equation&nbsp;<span>6.8</span></a>, we can compute the Cholesky decomposition of <span class="math inline">\(\Psi\)</span>:</p>
<ol type="1">
<li><span class="math inline">\(d_{11} = 1\)</span>,</li>
<li><span class="math inline">\(l_{21}d_{11} = e^{-1} \Rightarrow l_{21} = e^{-1}\)</span>, and</li>
<li><span class="math inline">\(d_{11} l_{21}^2 + d_{22} = 1 \Rightarrow d_{22} = 1 - e^{-2}\)</span>.</li>
</ol>
<p>The Cholesky decomposition of <span class="math inline">\(\Psi\)</span> is <span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; 0\\
e^{-1} &amp; 1\\
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0\\
0 &amp; 1 - e^{-2}\\
\end{pmatrix}
\begin{pmatrix}
1 &amp; e^{-1}\\
0 &amp; 1\\
\end{pmatrix}
= LDL^T\]</span></p>
<p>Some programs use <span class="math inline">\(U\)</span> instead of <span class="math inline">\(L\)</span>. The Cholesky decomposition of <span class="math inline">\(\Psi\)</span> is <span class="math display">\[
\Psi = LDL^T = U^TDU.
\]</span></p>
<p>Using <span class="math display">\[
\sqrt{D} =\begin{pmatrix}
1 &amp; 0\\
0 &amp; \sqrt{1 - e^{-2}}\\
\end{pmatrix},
\]</span> we can write the Cholesky decomposition of <span class="math inline">\(\Psi\)</span> without a diagonal matrix <span class="math inline">\(D\)</span> as <span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; 0\\
e^{-1} &amp; \sqrt{1 - e^{-2}}\\
\end{pmatrix}
\begin{pmatrix}
1 &amp; e^{-1}\\
0 &amp; \sqrt{1 - e^{-2}}\\
\end{pmatrix}
= U^TU.
\]</span></p>
</section>
<section id="computation-of-the-inverse-matrix" class="level3" data-number="6.11.2">
<h3 data-number="6.11.2" class="anchored" data-anchor-id="computation-of-the-inverse-matrix"><span class="header-section-number">6.11.2</span> Computation of the Inverse Matrix</h3>
<p>To compute the inverse of a matrix using the Cholesky decomposition, you can follow these steps:</p>
<ol type="1">
<li>Decompose the matrix <span class="math inline">\(A\)</span> into <span class="math inline">\(L\)</span> and <span class="math inline">\(L^T\)</span>, where <span class="math inline">\(L\)</span> is a lower triangular matrix and <span class="math inline">\(L^T\)</span> is the transpose of <span class="math inline">\(L\)</span>.</li>
<li>Compute <span class="math inline">\(L^{-1}\)</span>, the inverse of <span class="math inline">\(L\)</span>.</li>
<li>The inverse of <span class="math inline">\(A\)</span> is then <span class="math inline">\((L^{-1})^T  L^-1\)</span>.</li>
</ol>
<p>Please note that this method only applies to symmetric, positive-definite matrices.</p>
<p>The inverse of the matrix <span class="math inline">\(\Psi\)</span> from above is:</p>
<p><span class="math display">\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e &amp; -1\\
-1 &amp; e
\end{pmatrix}.
\]</span></p>
<p>Here’s an example of how to compute the inverse of a matrix using Cholesky decomposition in Python:</p>
<div id="9cb353cb" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> cholesky, inv</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> np.exp(<span class="dv">1</span>)</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Psi is a symmetric, positive-definite matrix </span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span><span class="op">/</span>E], [<span class="dv">1</span><span class="op">/</span>E, <span class="dv">1</span>]])</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> cholesky(Psi, lower<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>L_inv <span class="op">=</span> inv(L)</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="co"># The inverse of A is (L^-1)^T * L^-1</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>Psi_inv <span class="op">=</span> np.dot(L_inv.T, L_inv)</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Psi:</span><span class="ch">\n</span><span class="st">"</span>, Psi)</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Psi Inverse:</span><span class="ch">\n</span><span class="st">"</span>, Psi_inv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Psi:
 [[1.         0.36787944]
 [0.36787944 1.        ]]
Psi Inverse:
 [[ 1.15651764 -0.42545906]
 [-0.42545906  1.15651764]]</code></pre>
</div>
</div>
</section>
</section>
<section id="jupyter-notebook" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="jupyter-notebook"><span class="header-section-number">6.12</span> Jupyter Notebook</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The Jupyter-Notebook of this lecture is available on GitHub in the <a href="https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_num_gp.ipynb">Hyperparameter-Tuning-Cookbook Repository</a></li>
</ul>
</div>
</div>
<!-- 

::: {#5dad51bb .cell execution_count=59}
``` {.python .cell-code}
from spotPython.build.kriging import Kriging
import numpy as np
nat_X = np.array([[1], [2]])
nat_y = np.array([5, 10])
n=2
p=1
S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)
S.initialize_variables(nat_X, nat_y)
S.set_variable_types()
S.set_theta_values()
S.initialize_matrices()
S.build_Psi()
S.build_U()
S.likelihood()
# assert S.mu is close to 7.5 with a tolerance of 1e-6
assert np.allclose(S.mu, 7.5, atol=1e-6)
E = np.exp(1)
sigma2 = E/(E**2 -1) * (25/4 + 25/4*E)
# asssert S.SigmaSqr is close to sigma2 with a tolerance of 1e-6
assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)
```
:::


-->
<!-- 

## Exercises

### 1 Number of Sample Points

* The example uses $n=8$ sample points to fit the sin function.
  * What happens, if less than 8 samples are available?

### 2 Modified $\theta$ values

* The example uses a $\theta$ value of $1.0$.
  * What happens if $\theta$ is modified?
  * Can get better predictions with smaller or larger $\theta$ values?

### 3 Prediction Interval

* The prediction interval was identical to the measurement interval, i.e., in the range from $0$ to $2\pi$. This is referred to as "interpolation".
  * What happens if this interval is increased (which is referred to as "extrapolation")?



### Exercise RBF


#### Package Loading

::: {#e6360eaf .cell execution_count=60}
``` {.python .cell-code}
%matplotlib inline
import numpy as np
from numpy.matlib import eye
import scipy.linalg
from numpy import linalg as LA
from spotPython.design.spacefilling import spacefilling
from spotPython.fun.objectivefunctions import analytical
import matplotlib.pyplot as plt
```
:::


#### Define a small number

::: {#75c6edd8 .cell execution_count=61}
``` {.python .cell-code}
eps = np.sqrt(np.spacing(1))
```
:::


#### The Sampling Plan (X)

* We will use 256 points.
* The first 10 points are shown below.

::: {#13113ab0 .cell execution_count=62}
``` {.python .cell-code}
gen = spacefilling(2)
rng = np.random.RandomState(1)
lower = np.array([-1,-1])
upper = np.array([2,2])
X = gen.scipy_lhd(256, lower=lower, upper = upper)
X[1:10]
```

::: {.cell-output .cell-output-display execution_count=62}
```
array([[ 0.67319891, -0.11153561],
       [ 0.4979386 , -0.10717298],
       [-0.9991017 ,  1.66863389],
       [-0.423669  , -0.97527218],
       [-0.57241762,  1.77447307],
       [ 1.34580967,  0.62640122],
       [ 0.18662343,  0.18793039],
       [ 1.7664757 ,  1.65743858],
       [ 0.98282275,  0.42361525]])
```
:::
:::


#### The Objective Function

* Here we use $\sum_{i=1}^n (x_i-1)^2$.
* `f_map()` is a helper function that maps $f$ to the entries (points) in the matrix $X$.

::: {#c210d87f .cell execution_count=63}
``` {.python .cell-code}
def f(x):
    return np.sum((x-1.0)**2)

def f_map(x):
    return np.array(list(map(f, x)))
```
:::


::: {#304d3c44 .cell execution_count=64}
``` {.python .cell-code}
y = f_map(X)
y[1:10]
```

::: {.cell-output .cell-output-display execution_count=64}
```
array([1.34231036, 1.47789766, 4.44347889, 5.9285336 , 3.07230572,
       0.25916038, 1.32103849, 1.01971047, 0.33251443])
```
:::
:::


* Alternatively, we can use pre-defined functions from the `pyspot` package:

::: {#3b89e6ba .cell execution_count=65}
``` {.python .cell-code}
# fun = analytical(sigma=0).fun_branin
# fun = analytical(sigma=0).fun_sphere
```
:::


::: {#7e34fcc5 .cell execution_count=66}
``` {.python .cell-code}
XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))
zz = np.array([f_map(np.array([xi, yi]).reshape(-1,2)) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)
```
:::


::: {#eab2ac05 .cell execution_count=67}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-68-output-1.png){width=491 height=270}
:::
:::


::: {#3bb81a1b .cell execution_count=68}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 10))
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-69-output-1.png){width=491 height=270}
:::
:::


#### The Gram Matrix

::: {#8be8e5f4 .cell execution_count=69}
``` {.python .cell-code}
def build_Gram(X):
        """
        Construction of the Gram matrix.
        """
        n = X.shape[0]
        G = np.zeros((n, n))
        for i in range(n):
            for j in range(i, n):
                G[i, j] = np.linalg.norm(X[i] - X[j])
        G = G + G.T    
        return G
```
:::


::: {#6719a35c .cell execution_count=70}
``` {.python .cell-code}
G = build_Gram(X)
np.round(G,2)
```

::: {.cell-output .cell-output-display execution_count=70}
```
array([[0.  , 0.51, 0.57, ..., 0.99, 0.62, 1.39],
       [0.51, 0.  , 0.18, ..., 0.91, 0.94, 1.41],
       [0.57, 0.18, 0.  , ..., 0.75, 0.87, 1.26],
       ...,
       [0.99, 0.91, 0.75, ..., 0.  , 0.71, 0.52],
       [0.62, 0.94, 0.87, ..., 0.71, 0.  , 0.91],
       [1.39, 1.41, 1.26, ..., 0.52, 0.91, 0.  ]])
```
:::
:::


#### The Radial Basis Functions

::: {#6e63aa7c .cell execution_count=71}
``` {.python .cell-code}
def basis_linear(r):
    return r*r*r
```
:::


::: {#5e943fd9 .cell execution_count=72}
``` {.python .cell-code}
def basis_gauss(r, sigma = 1e-1):
    return np.exp(-r**2/sigma)
```
:::


+ We select the Gaussian basis function for the following examples:

::: {#3c6a2502 .cell execution_count=73}
``` {.python .cell-code}
basis = basis_gauss
```
:::


#### The $\Psi$ Matrix

::: {#4f3765e5 .cell execution_count=74}
``` {.python .cell-code}
def build_Phi(G, basis, eps=np.sqrt(np.spacing(1))):
    n = G.shape[0]
    Phi = np.zeros((n,n))
    for i in range(n):
        for j in range(n):
            Phi[i,j] = basis(G[i,j])
    Phi = Phi +  np.multiply(np.mat(eye(n)), eps)
    return Phi
```
:::


::: {#857592bf .cell execution_count=75}
``` {.python .cell-code}
Phi = build_Phi(G, basis=basis)
Phi[0:3,0:3]
```

::: {.cell-output .cell-output-display execution_count=75}
```
matrix([[1.00000001, 0.07413611, 0.03828587],
        [0.07413611, 1.00000001, 0.73539165],
        [0.03828587, 0.73539165, 1.00000001]])
```
:::
:::


#### Inverting $\Psi$ via Cholesky Factorization

* There a two different implementations of the Cholesky factorization oin Python:
  * `numpy`'s  `linalg.cholesky()` and
  * `scipy`'s  `linalg.cholesky()`
* We will use `numpy`'s version.

::: {#dbae4b94 .cell execution_count=76}
``` {.python .cell-code}
def get_rbf_weights(Phi, y):
    """ 
    Calculating the weights of the radial basis function surrogate.
    Cholesky factorization used.
    LU decomposition otherwise (not implemented yet).
    """
    # U = scipy.linalg.cholesky(Phi, lower=True)
    U = np.linalg.cholesky(Phi)
    U = U.T
    # w = U\(U'\ModelInfo.y)
    w = np.linalg.solve(U, np.linalg.solve(U.T, y))
    return w
```
:::


::: {#becc3ed5 .cell execution_count=77}
``` {.python .cell-code}
w = get_rbf_weights(Phi, y)
w[0:3]
```

::: {.cell-output .cell-output-display execution_count=77}
```
array([-6.98432952,  3.06393352, -5.13738971])
```
:::
:::


#### Predictions

##### The Predictor

::: {#4e038424 .cell execution_count=78}
``` {.python .cell-code}
def pred_rbf(x, X, basis, w):
    n = X.shape[0]
    d = np.zeros((n))
    phi = np.zeros((n))
    for i in range(n):
        d[i] = np.linalg.norm(x - X[i])
    for i in range(n):
        phi[i] = basis(d[i])
    return w @ phi    
```
:::


##### Testing some Example Points

::: {#cbe074a0 .cell execution_count=79}
``` {.python .cell-code}
x = X[0]
x
```

::: {.cell-output .cell-output-display execution_count=79}
```
array([ 0.76153494, -0.61391197])
```
:::
:::


##### The RBF Prediction $\hat{f}$

::: {#afa8cc36 .cell execution_count=80}
``` {.python .cell-code}
pred_rbf(x=x, X=X, basis=basis, w=w)
```

::: {.cell-output .cell-output-display execution_count=80}
```
2.6615775203204706
```
:::
:::


##### The Original (True) Value $f$

::: {#ce31b5d0 .cell execution_count=81}
``` {.python .cell-code}
f_map(np.array(x).reshape(1,-1))
```

::: {.cell-output .cell-output-display execution_count=81}
```
array([2.66157742])
```
:::
:::


##### Visualizations

::: {#d3670b74 .cell execution_count=82}
``` {.python .cell-code}
XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))
zz = np.array([pred_rbf(x=np.array([xi, yi]), X=X, basis=basis,w=w) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)
```
:::


::: {#1df182f2 .cell execution_count=83}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-84-output-1.png){width=491 height=270}
:::
:::


::: {#7275c8cc .cell execution_count=84}
``` {.python .cell-code}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 5))
```

::: {.cell-output .cell-output-display}
![](006_num_gp_files/figure-html/cell-85-output-1.png){width=491 height=270}
:::
:::


##### Note

The original function $f$ is cheaper than the surrogate $\hat{f}$ in this example, because we have chosen a simple analytical function as the ground truth. This is not the case in real-world settings.

#### Cholesky Factorization

##### $A = U^T U$

* $U$ is an upper triangular matrix

::: {#a41274d9 .cell execution_count=85}
``` {.python .cell-code}
def cholesky_U(A):
    N = A.shape[0]
    U = np.zeros((N,N))
    for k in range(0,N):
         # compute diagonal entry
         U[k,k] = A[k,k]
         for j in range(0,k):
             U[k,k] = U[k,k] - U[j,k]*U[j,k]
         U[k,k] = np.sqrt(U[k,k])
         # compute remaining column
         for i in range(k+1,N):
             U[k,i] = A[k,i]
             for j in range(0,k):
                 U[k,i] = U[k,i] - U[j,i]*U[j,k]
             U[k,i] = U[k,i] / U[k,k]
    return U
```
:::


#### $A = L L^T$

$L$ is a lower triangular matrix

::: {#5ef0e1ed .cell execution_count=86}
``` {.python .cell-code}
def cholesky_L(A):
    N = A.shape[0]
    L = np.zeros((N,N))
    for k in range(0,N):
         # compute diagonal entry
         L[k,k] = A[k,k]
         for j in range(0,k):
             L[k,k] = L[k,k] - L[k,j]*L[k,j]
         L[k,k] = np.sqrt(L[k,k])
         # compute remaining column
         for i in range(k+1,N):
             L[i,k] = A[i,k]
             for j in range(0,k):
                 L[i,k] = L[i,k] - L[i,j]*L[k,j]
             L[i,k] = L[i,k] / L[k,k]
    return L
```
:::


#### Example

::: {#15cfd569 .cell execution_count=87}
``` {.python .cell-code}
A = np.array([[4, 2, 4, 4], [2, 10, 5, 2], [4, 5, 9, 6], [4, 2, 6, 9]])
A
```

::: {.cell-output .cell-output-display execution_count=87}
```
array([[ 4,  2,  4,  4],
       [ 2, 10,  5,  2],
       [ 4,  5,  9,  6],
       [ 4,  2,  6,  9]])
```
:::
:::


#### Check: Is $A$ positive definite?

::: {#a901bc34 .cell execution_count=88}
``` {.python .cell-code}
assert(np.all(np.linalg.eigvals(A) > 0))
```
:::


####  $A = U^T U$

Perform Cholesky Factorization

::: {#8552e55b .cell execution_count=89}
``` {.python .cell-code}
U = cholesky_U(A)
U
```

::: {.cell-output .cell-output-display execution_count=89}
```
array([[2., 1., 2., 2.],
       [0., 3., 1., 0.],
       [0., 0., 2., 1.],
       [0., 0., 0., 2.]])
```
:::
:::


Test Result

::: {#d6098b0a .cell execution_count=90}
``` {.python .cell-code}
U.T @ U
```

::: {.cell-output .cell-output-display execution_count=90}
```
array([[ 4.,  2.,  4.,  4.],
       [ 2., 10.,  5.,  2.],
       [ 4.,  5.,  9.,  6.],
       [ 4.,  2.,  6.,  9.]])
```
:::
:::


####  $A = L L^T$

::: {#d8113045 .cell execution_count=91}
``` {.python .cell-code}
L = cholesky_L(A)
L
```

::: {.cell-output .cell-output-display execution_count=91}
```
array([[2., 0., 0., 0.],
       [1., 3., 0., 0.],
       [2., 1., 2., 0.],
       [2., 0., 1., 2.]])
```
:::
:::


Test Result

::: {#94559178 .cell execution_count=92}
``` {.python .cell-code}
L @ L.T
```

::: {.cell-output .cell-output-display execution_count=92}
```
array([[ 4.,  2.,  4.,  4.],
       [ 2., 10.,  5.,  2.],
       [ 4.,  5.,  9.,  6.],
       [ 4.,  2.,  6.,  9.]])
```
:::
:::


## Exercises

### Gaussian Basis Function

* Plot the Gaussian Basis Function `basis_gauss` in the range from -2 to 2 using `matplotlib.pyplot`
  * Hint: Check the [matplotlib documentation](https://matplotlib.org/stable/tutorials/introductory/pyplot.html) for examples.
  * Generate a plot with several `sigma` values, e.g., 0.1, 1.0, and 10.
* What is the meaning of the `sigma` parameter: Can you explain its influence / effect on the model quality?
  * Is the `sigma` value important?

### Linear Basis Function

* Select the linear basis function?
* What errors occur?
* Do you have any ideas how to fix this error? -->


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bart21i" class="csl-entry" role="listitem">
Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. <em><span class="nocase">Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide</span></em>. Springer.
</div>
<div id="ref-Forr08a" class="csl-entry" role="listitem">
Forrester, Alexander, András Sóbester, and Andy Keane. 2008. <em><span class="nocase">Engineering Design via Surrogate Modelling</span></em>. Wiley.
</div>
<div id="ref-Sant03a" class="csl-entry" role="listitem">
Santner, T J, B J Williams, and W I Notz. 2003. <em><span class="nocase">The Design and Analysis of Computer Experiments</span></em>. Berlin, Heidelberg, New York: Springer.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./005_num_rsm.html" class="pagination-link  aria-label=" &lt;span="" numerical="" methods&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction: Numerical Methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./007_num_spot_intro.html" class="pagination-link" aria-label="<span class='chapter-number'>7</span>&nbsp; <span class='chapter-title'>Introduction to spotPython</span>">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to spotPython</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>