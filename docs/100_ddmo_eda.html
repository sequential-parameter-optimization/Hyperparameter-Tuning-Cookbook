<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>18&nbsp; Basic Statistics and Data Analysis – Hyperparameter Tuning Cookbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./100_ddmo_regression.html" rel="next">
<link href="./017_num_spot_user_function.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-93074729595c0d1bb0917954c6b21507.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="18&nbsp; Basic Statistics and Data Analysis – Hyperparameter Tuning Cookbook">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="100_ddmo_eda_files/figure-html/fig-histogram-output-2.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="[18]{.chapter-number}&nbsp; [Basic Statistics and Data Analysis]{.chapter-title}">
<meta name="citation_fulltext_html_url" content="https://arxiv.org/abs/2307.10262">
<meta name="citation_doi" content="10.48550/arXiv.2307.10262">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="arXiv">
<meta name="citation_reference" content="citation_title=Benchmarking in Optimization: Best Practice and Open Issues;,citation_author=Thomas Bartz-Beielstein;,citation_author=Carola Doerr;,citation_author=Jakob Bossek;,citation_author=Sowmya Chandrasekaran;,citation_author=Tome Eftimov;,citation_author=Andreas Fischbach;,citation_author=Pascal Kerschke;,citation_author=Manuel Lopez-Ibanez;,citation_author=Katherine M. Malan;,citation_author=Jason H. Moore;,citation_author=Boris Naujoks;,citation_author=Patryk Orzechowski;,citation_author=Vanessa Volz;,citation_author=Markus Wagner;,citation_author=Thomas Weise;,citation_publication_date=2020-07;,citation_cover_date=2020-07;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2007.03488;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning With Ray Tune;,citation_author=undefined PyTorch;,citation_publication_date=2023-05;,citation_cover_date=2023-05;,citation_year=2023;,citation_fulltext_html_url=https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html;">
<meta name="citation_reference" content="citation_title=Training a Classifier;,citation_author=undefined PyTorch;,citation_publication_date=2023-05;,citation_cover_date=2023-05;,citation_year=2023;,citation_fulltext_html_url=https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html;">
<meta name="citation_reference" content="citation_title=PyTorch Hyperparameter Tuning with SPOT: Comparison with Ray Tuner and Default Hyperparameters on CIFAR10;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2023-04;,citation_cover_date=2023-04;,citation_year=2023;,citation_fulltext_html_url=https://github.com/sequential-parameter-optimization/spotpython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb;">
<meta name="citation_reference" content="citation_title=Machine Learning in Official Statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.48550/arXiv.1812.10422;">
<meta name="citation_reference" content="citation_title=Qualitätshandbuch der Statistischen Ämter des Bundes und der Länder;,citation_publication_date=2021-03;,citation_cover_date=2021-03;,citation_year=2021;,citation_fulltext_html_url=https://www.destatis.de/DE/Methoden/Qualitaet/qualitaetshandbuch.pdf;,citation_language=de;">
<meta name="citation_reference" content="citation_title=Quality Assurance Framework of the European Statistical System;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://ec.europa.eu/eurostat/documents/64157/4392716/ESS-QAF-V2.0-final.pdf;,citation_language=en;">
<meta name="citation_reference" content="citation_title=Standardisierung der Prozesse: 14 Jahre AG SteP;,citation_author=T. Blumöhr;,citation_author=C. Teichmann;,citation_author=A. Noack;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://www.destatis.de/DE/Methoden/
               WISTA-Wirtschaft-und-Statistik/2017/05/standardisierung-prozesse-052017.html;,citation_volume=5;,citation_journal_title=WISTA - Wirtschaft und Statistik;,citation_publisher=Wiesbaden: Statistisches Bundesamt (Destatis);">
<meta name="citation_reference" content="citation_title=Generic Statistical Business Process Model - GSBPM;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://statswiki.unece.org/display/
               GSBPM/GSBPM+v5.1;,citation_publisher=United Nations Economic Commission for Europe (UNECE);">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Engineering Design via Surrogate Modelling;,citation_author=Alexander Forrester;,citation_author=András Sóbester;,citation_author=Andy Keane;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;">
<meta name="citation_reference" content="citation_title=No Free Lunch Theorems for Optimization;,citation_author=David H Wolpert;,citation_author=William G Macready;,citation_publication_date=1997-04;,citation_cover_date=1997-04;,citation_year=1997;,citation_issue=1;,citation_volume=1;,citation_journal_title=IEEE Transactions on Evolutionary Computation;">
<meta name="citation_reference" content="citation_title=An Introduction to Statistical Learning with Applications in R;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Requirements for papers focusing on new or improved global optimization algorithms;,citation_author=Raphael T. Haftka;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=54;,citation_journal_title=Structural and Multidisciplinary Optimization;">
<meta name="citation_reference" content="citation_title=LightGBM: A Highly Efficient Gradient Boosting Decision Tree;,citation_author=Guolin Ke;,citation_author=Qi Meng;,citation_author=Thomas Finley;,citation_author=Taifeng Wang;,citation_author=Wei Chen;,citation_author=Weidong Ma;,citation_author=Qiwei Ye;,citation_author=Tie-Yan Liu;,citation_editor=I. Guyon;,citation_editor=U. Von Luxburg;,citation_editor=S. Bengio;,citation_editor=H. Wallach;,citation_editor=R. Fergus;,citation_editor=S. Vishwanathan;,citation_editor=R. Garnett;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_conference_title=Advances in Neural Information Processing Systems;,citation_conference=Curran Associates, Inc.;">
<meta name="citation_reference" content="citation_title=Greedy Function Approximation: A Gradient Boosting Machine;,citation_author=Jerome H. Friedman;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_issue=5;,citation_volume=29;,citation_journal_title=The Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Machine Learning in Official Statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.48550/arXiv.1812.10422;">
<meta name="citation_reference" content="citation_title=Official statistics in the era of big data opportunities and threats;,citation_author=Walter J. Radermacher;,citation_publication_date=2018-11-01;,citation_cover_date=2018-11-01;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1007/s41060-018-0124-z;,citation_issue=3;,citation_doi=10.1007/s41060-018-0124-z;,citation_volume=6;,citation_language=en;,citation_journal_title=International Journal of Data Science and Analytics;">
<meta name="citation_reference" content="citation_title=Machine Learning in Official Statistics;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Joerg Feuerhake;">
<meta name="citation_reference" content="citation_title=A quality framework for statistical algorithms;,citation_author=Wesley Yung;,citation_author=Siu-Ming Tam;,citation_author=Bart Buelens;,citation_author=Hugh Chipman;,citation_author=Florian Dumpert;,citation_author=Gabriele Ascari;,citation_author=Fabiana Rocci;,citation_author=Joep Burger;,citation_author=InKyung Choi;,citation_publication_date=2022-01-01;,citation_cover_date=2022-01-01;,citation_year=2022;,citation_fulltext_html_url=https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji210875;,citation_issue=1;,citation_doi=10.3233/SJI-210875;,citation_volume=38;,citation_language=en;,citation_journal_title=Statistical Journal of the IAOS;">
<meta name="citation_reference" content="citation_title=A quality framework for statistical algorithms;,citation_author=Wesley Yung;,citation_author=Siu-Ming Tam;,citation_author=Bart Buelens;,citation_author=Hugh Chipman;,citation_author=Florian Dumpert;,citation_author=Gabriele Ascari;,citation_author=Fabiana Rocci;,citation_author=Joep Burger;,citation_author=InKyung Choi;,citation_publication_date=2022-01;,citation_cover_date=2022-01;,citation_year=2022;,citation_issue=1;,citation_volume=38;,citation_journal_title=Statistical Journal of the IAOS;">
<meta name="citation_reference" content="citation_title=Qualitätshandbuch der Statistischen Ämter des Bundes und der Länder;,citation_author=Michael Reichelt;">
<meta name="citation_reference" content="citation_title=Data Science and Official Statistics: Toward a New Data Culture;,citation_author=Stefan Schweinfest;,citation_author=Ronald Jansen;,citation_publication_date=2021-10-28;,citation_cover_date=2021-10-28;,citation_year=2021;,citation_fulltext_html_url=https://hdsr.mitpress.mit.edu/pub/1g514ljw/release/4;,citation_issue=4;,citation_doi=10.1162/99608f92.c1237762;,citation_volume=3;,citation_language=en;,citation_journal_title=Harvard Data Science Review;">
<meta name="citation_reference" content="citation_title=Official Statistics 4.0: Verified Facts for People in the 21st Century;,citation_author=Walter J. Radermacher;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=Detecting Covariate Drift with Explanations;,citation_author=Steffen Castle;,citation_author=Robert Schwarzenberg;,citation_author=Mohsen Pourvali;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13–17, 2021, Proceedings, Part II;,citation_conference=Springer-Verlag;">
<meta name="citation_reference" content="citation_title=Keras;,citation_author=Francois Chollet;,citation_author=others;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_publisher=https:://keras.io;">
<meta name="citation_reference" content="citation_title=TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems;,citation_author=Martin Abadi;,citation_author=Ashish Agarwal;,citation_author=Paul Barham;,citation_author=Eugene Brevdo;,citation_author=Zhifeng Chen;,citation_author=Craig Citro;,citation_author=Greg S. Corrado;,citation_author=Andy Davis;,citation_author=Jeffrey Dean;,citation_author=Matthieu Devin;,citation_author=Sanjay Ghemawat;,citation_author=Ian Goodfellow;,citation_author=Andrew Harp;,citation_author=Geoffrey Irving;,citation_author=Michael Isard;,citation_author=Yangqing Jia;,citation_author=Rafal Jozefowicz;,citation_author=Lukasz Kaiser;,citation_author=Manjunath Kudlur;,citation_author=Josh Levenberg;,citation_author=Dan Mane;,citation_author=Rajat Monga;,citation_author=Sherry Moore;,citation_author=Derek Murray;,citation_author=Chris Olah;,citation_author=Mike Schuster;,citation_author=Jonathon Shlens;,citation_author=Benoit Steiner;,citation_author=Ilya Sutskever;,citation_author=Kunal Talwar;,citation_author=Paul Tucker;,citation_author=Vincent Vanhoucke;,citation_author=Vijay Vasudevan;,citation_author=Fernanda Viegas;,citation_author=Oriol Vinyals;,citation_author=Pete Warden;,citation_author=Martin Wattenberg;,citation_author=Martin Wicke;,citation_author=Yuan Yu;,citation_author=Xiaoqiang Zheng;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=statsmodels: Econometric and statistical modeling with python;,citation_author=Skipper Seabold;,citation_author=Josef Perktold;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=9th Python in Science Conference;">
<meta name="citation_reference" content="citation_title=Scikit-learn: Machine Learning in Python;,citation_author=F. Pedregosa;,citation_author=G. Varoquaux;,citation_author=A. Gramfort;,citation_author=V. Michel;,citation_author=B. Thirion;,citation_author=O. Grisel;,citation_author=M. Blondel;,citation_author=P. Prettenhofer;,citation_author=R. Weiss;,citation_author=V. Dubourg;,citation_author=J. Vanderplas;,citation_author=A. Passos;,citation_author=D. Cournapeau;,citation_author=M. Brucher;,citation_author=M. Perrot;,citation_author=E. Duchesnay;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=12;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Array programming with NumPy;,citation_author=Charles R. Harris;,citation_author=K. Jarrod Millman;,citation_author=Stéfan J. Walt;,citation_author=Ralf Gommers;,citation_author=Pauli Virtanen;,citation_author=David Cournapeau;,citation_author=Eric Wieser;,citation_author=Julian Taylor;,citation_author=Sebastian Berg;,citation_author=Nathaniel J. Smith;,citation_author=Robert Kern;,citation_author=Matti Picus;,citation_author=Stephan Hoyer;,citation_author=Marten H. Kerkwijk;,citation_author=Matthew Brett;,citation_author=Allan Haldane;,citation_author=Jaime Fernández Río;,citation_author=Mark Wiebe;,citation_author=Pearu Peterson;,citation_author=Pierre Gérard-Marchant;,citation_author=Kevin Sheppard;,citation_author=Tyler Reddy;,citation_author=Warren Weckesser;,citation_author=Hameer Abbasi;,citation_author=Christoph Gohlke;,citation_author=Travis E. Oliphant;,citation_publication_date=2020-09;,citation_cover_date=2020-09;,citation_year=2020;,citation_issue=7825;,citation_volume=585;,citation_journal_title=Nature;">
<meta name="citation_reference" content="citation_title=Algorithms for Learning Regression Trees and Ensembles on Evolving Data Streams;,citation_author=Elena Ikonomovska;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_dissertation_institution=Jozef Stefan International Postgraduate School;">
<meta name="citation_reference" content="citation_title=Online Bagging and Boosting;,citation_author=N C Oza;,citation_conference_title=2005 IEEE International Conference on Systems, Man and Cybernetics;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Online bagging and boosting;,citation_author=Nikunj C Oza;,citation_author=Stuart Russell;,citation_editor=T Jaakola;,citation_editor=T Richardson;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=8th Insternational Workshop on Artificial Intelligence and Statistics;">
<meta name="citation_reference" content="citation_title=Event labeling combining ensemble detectors and background knowledge;,citation_author=Hadi Fanaee-T;,citation_author=Joao Gama;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=2;,citation_volume=2;,citation_journal_title=Progress in Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Adaptive random forests for evolving data stream classification;,citation_author=Heitor M. Gomes;,citation_author=Albert Bifet;,citation_author=Jesse Read;,citation_author=Jean Paul Barddal;,citation_author=Fabricio Enembreck;,citation_author=Bernhard Pfharinger;,citation_author=Geoff Holmes;,citation_author=Talel Abdessalem;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=9;,citation_volume=106;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Literate Programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
<meta name="citation_reference" content="citation_title=A Review and Taxonomy of Interactive Optimization Methods in Operations Research;,citation_author=David Meignan;,citation_author=Sigrid Knust;,citation_author=Jean-Marc Frayet;,citation_author=Gilles Pesant;,citation_author=Nicolas Gaud;,citation_publication_date=2015-09;,citation_cover_date=2015-09;,citation_year=2015;,citation_journal_title=ACM Transactions on Interactive Intelligent Systems;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=In a Nutshell – The Sequential Parameter Optimization Toolbox;,citation_author=Thomas Bartz-Beielstein;,citation_author=Martin Zaefferer;,citation_author=Frederik Rehbach;,citation_publication_date=2021-12;,citation_cover_date=2021-12;,citation_year=2021;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Direct search methods: Then and now;,citation_author=R M Lewis;,citation_author=V Torczon;,citation_author=M W Trosset;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_issue=1–2;,citation_volume=124;,citation_journal_title=Journal of Computational and Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization;,citation_author=Lisha Li;,citation_author=Kevin Jamieson;,citation_author=Giulia DeSalvo;,citation_author=Afshin Rostamizadeh;,citation_author=Ameet Talwalkar;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Sequential Parameter Optimization;,citation_author=Thomas Bartz-Beielstein;,citation_author=Christian Lasarczyk;,citation_author=Mike Preuss;,citation_editor=B McKay;,citation_editor=others;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_conference_title=Proceedings 2005 Congress on Evolutionary Computation (CEC’05), Edinburgh, Scotland;,citation_conference=IEEE Press;">
<meta name="citation_reference" content="citation_title=Evolutionary Algorithms;,citation_author=Thomas Bartz-Beielstein;,citation_author=Jürgen Branke;,citation_author=Jörn Mehnen;,citation_author=Olaf Mersmann;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=3;,citation_volume=4;,citation_journal_title=Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Classification and Regression Trees;,citation_author=L Breiman;,citation_author=J H Friedman;,citation_author=R A Olshen;,citation_author=C J Stone;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;">
<meta name="citation_reference" content="citation_title=Continuous inspection schemes;,citation_author=E. S. Page;,citation_publication_date=1954-06;,citation_cover_date=1954-06;,citation_year=1954;,citation_issue=1-2;,citation_volume=41;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=Counting Large Numbers of Events in Small Registers;,citation_author=Robert Morris;,citation_publication_date=1978-10;,citation_cover_date=1978-10;,citation_year=1978;,citation_issue=10;,citation_volume=21;,citation_journal_title=Commun. ACM;">
<meta name="citation_reference" content="citation_title=Approximate Counting: A Detailed Analysis;,citation_author=Philippe Flajolet;,citation_publication_date=1985-03;,citation_cover_date=1985-03;,citation_year=1985;,citation_issue=1;,citation_volume=25;,citation_journal_title=BIT;">
<meta name="citation_reference" content="citation_title=Random Sampling with a Reservoir;,citation_author=Jeffrey S. Vitter;,citation_publication_date=1985-03;,citation_cover_date=1985-03;,citation_year=1985;,citation_issue=1;,citation_volume=11;,citation_journal_title=ACM Trans. Math. Softw.;">
<meta name="citation_reference" content="citation_title=Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem;,citation_author=Michael McCloskey;,citation_author=Neal J. Cohen;,citation_publication_date=1989-01;,citation_cover_date=1989-01;,citation_year=1989;,citation_issue=C;,citation_volume=24;,citation_journal_title=Psychology of Learning and Motivation - Advances in Research and Theory;">
<meta name="citation_reference" content="citation_title=Detection of Abrupt Changes - Theory and Application;,citation_author=Michèle Basseville;,citation_author=Igor V. Nikiforov;,citation_publication_date=1993;,citation_cover_date=1993;,citation_year=1993;">
<meta name="citation_reference" content="citation_title=An Introduction to Computational Learning Theory;,citation_author=Michael J. Kearns;,citation_author=Umesh V. Vazirani;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;">
<meta name="citation_reference" content="citation_title=An Introduction to the Kalman Filter;,citation_author=Greg Welch;,citation_author=Gary Bishop;,citation_publication_date=1995;,citation_cover_date=1995;,citation_year=1995;">
<meta name="citation_reference" content="citation_title=The Space Complexity of Approximating the Frequency Moments;,citation_author=Noga Alon;,citation_author=Yossi Matias;,citation_author=Mario Szegedy;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_conference_title=Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing;,citation_conference=Association for Computing Machinery;,citation_series_title=STOC ’96;">
<meta name="citation_reference" content="citation_title=SPLICE-2 Comparative Evaluation: Electricity Pricing;,citation_author=Michael Harries;,citation_author=U Nsw-cse-tr;,citation_author=New South Wales;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;">
<meta name="citation_reference" content="citation_title=Mining high-speed data streams;,citation_author=Pedro M. Domingos;,citation_author=Geoff Hulten;,citation_editor=Raghu Ramakrishnan;,citation_editor=Salvatore J. Stolfo;,citation_editor=Roberto J. Bayardo;,citation_editor=Ismail Parsa;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_conference_title=Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, Boston, MA, USA, August 20-23, 2000;,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=Mining Time-Changing Data Streams;,citation_author=Geoff Hulten;,citation_author=Laurie Spencer;,citation_author=Pedro Domingos;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’01;">
<meta name="citation_reference" content="citation_title=A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification;,citation_author=W. Nick Street;,citation_author=YongSeog Kim;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_conference_title=Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’01;">
<meta name="citation_reference" content="citation_title=3D Data Management: Controlling Data Volume, Velocity, and Variety;,citation_author=Douglas Laney;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_technical_report_institution=META Group;">
<meta name="citation_reference" content="citation_title=Models and Issues in Data Stream Systems;,citation_author=Brian Babcock;,citation_author=Shivnath Babu;,citation_author=Mayur Datar;,citation_author=Rajeev Motwani;,citation_author=Jennifer Widom;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_conference_title=Proceedings of the 21st ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems;,citation_conference=ACM;,citation_series_title=PODS ’02;">
<meta name="citation_reference" content="citation_title=A New Approximate Maximal Margin Classification Algorithm;,citation_author=Claudio Gentile;,citation_publication_date=2002-03;,citation_cover_date=2002-03;,citation_year=2002;,citation_volume=2;,citation_journal_title=J. Mach. Learn. Res.;">
<meta name="citation_reference" content="citation_title=The Design and Analysis of Computer Experiments;,citation_author=T J Santner;,citation_author=B J Williams;,citation_author=W I Notz;,citation_publication_date=2003;,citation_cover_date=2003;,citation_year=2003;">
<meta name="citation_reference" content="citation_title=Learning with drift detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_conference_title=In SBIA Brazilian Symposium on Artificial Intelligence;,citation_conference=Springer Verlag;">
<meta name="citation_reference" content="citation_title=Learning with Drift Detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_inbook_title=Parallel Problem Solving from Nature - PPSN XIII - 13th International Conference, Ljubljana, Slovenia, September 13-17, 2014. Proceedings;">
<meta name="citation_reference" content="citation_title=Learning with Drift Detection;,citation_author=João Gama;,citation_author=Pedro Medas;,citation_author=Gladys Castillo;,citation_author=Pedro Rodrigues;,citation_editor=Ana L. C. Bazzan;,citation_editor=Sofiane Labidi;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_conference_title=Advances in Artificial Intelligence – SBIA 2004;,citation_conference=Springer Berlin Heidelberg;">
<meta name="citation_reference" content="citation_title=Statistical Analysis of Massive Data Streams: Proceedings of a Workshop;,citation_editor=Sallie Keller-McNulty;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;">
<meta name="citation_reference" content="citation_title=Data Mining: Practical Machine Learning Tools and Techniques;,citation_author=Ian H. Witten;,citation_author=Eibe Frank;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_series_title=The Morgan Kaufmann Series in Data Management Systems;">
<meta name="citation_reference" content="citation_title=Towards Generic Pattern Mining;,citation_author=Mohammed Javeed Zaki;,citation_author=Nagender Parimi;,citation_author=Nilanjana De;,citation_author=Feng Gao;,citation_author=Benjarath Phoophakdee;,citation_author=Joe Urban;,citation_author=Vineet Chaoji;,citation_author=Mohammad Al Hasan;,citation_author=Saeed Salem;,citation_editor=Bernhard Ganter;,citation_editor=Robert Godin;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=3403;,citation_conference_title=Formal Concept Analysis, Third International Conference, ICFCA 2005, Lens, France, February 14-18, 2005, Proceedings;,citation_conference=Springer;,citation_series_title=Lecture Notes in Computer Science;">
<meta name="citation_reference" content="citation_title=Mining Data Streams: A Review;,citation_author=Mohamed Medhat Gaber;,citation_author=Arkady Zaslavsky;,citation_author=Shonali Krishnaswamy;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=34;,citation_journal_title=SIGMOD Rec.;">
<meta name="citation_reference" content="citation_title=Early drift detection method;,citation_author=Manuel Baena-Garcıa;,citation_author=José Campo-Ávila;,citation_author=Raúl Fidalgo;,citation_author=Albert Bifet;,citation_author=R Gavalda;,citation_author=Rafael Morales-Bueno;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_volume=6;,citation_conference_title=Fourth international workshop on knowledge discovery from data streams;">
<meta name="citation_reference" content="citation_title=Online Passive-Aggressive Algorithms;,citation_author=Koby Crammer;,citation_author=Ofer Dekel;,citation_author=Joseph Keshet;,citation_author=Shai Shalev-Shwartz;,citation_author=Yoram Singer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=19;,citation_volume=7;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Data Streams – Models and Algorithms;,citation_editor=Charu Aggarwal;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;">
<meta name="citation_reference" content="citation_title=Learning from Time-Changing Data with Adaptive Windowing;,citation_author=Albert Bifet;,citation_author=Ricard Gavaldà;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=Proceedings of the 2007 SIAM International Conference on Data Mining (SDM);">
<meta name="citation_reference" content="citation_title=Learning from time-changing data with adaptive windowing;,citation_author=Albert Bifet;,citation_author=Ricard Gavalda;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_volume=7;,citation_conference_title=Proceedings of the 2007 SIAM international conference on data mining;,citation_conference=SIAM;">
<meta name="citation_reference" content="citation_title=A Survey of Classification Methods in Data Streams;,citation_author=Mohamed Gaber;,citation_author=Arkady Zaslavsky;,citation_author=Shonali Krishnaswamy;,citation_editor=Charu Aggarwal;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_inbook_title=Data Streams – Models and Algorithms;">
<meta name="citation_reference" content="citation_title=Use of Hoeffding trees in concept based data stream mining;,citation_author=Stefan Hoeglinger;,citation_author=Russel Pears;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_journal_title=2007 Third International Conference on Information and Automation for Sustainability;">
<meta name="citation_reference" content="citation_title=Detecting concept drift using statistical testing;,citation_author=Kyosuke Nishida;,citation_author=Koichiro Yamauchi;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=International conference on discovery science;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Olindda: A cluster-based approach for detecting novelty and concept drift in data streams;,citation_author=Eduardo J Spinosa;,citation_author=André Ponce Leon F. de Carvalho;,citation_author=Joao Gama;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_conference_title=Proceedings of the 2007 ACM symposium on Applied computing;">
<meta name="citation_reference" content="citation_title=Statistical Quality Control;,citation_author=Douglas C Montgomery;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;">
<meta name="citation_reference" content="citation_title=Adaptive Learning from Evolving Data Streams;,citation_author=Albert Bifet;,citation_author=Ricard Gavaldà;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 8th International Symposium on Intelligent Data Analysis: Advances in Intelligent Data Analysis VIII;,citation_conference=Springer-Verlag;,citation_series_title=IDA ’09;">
<meta name="citation_reference" content="citation_title=New Ensemble Methods for Evolving Data Streams;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_author=Richard Kirkby;,citation_author=Ricard Gavaldà;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’09;">
<meta name="citation_reference" content="citation_title=Adaptive concept drift detection;,citation_author=Anton Dries;,citation_author=Ulrich Rückert;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=5-6;,citation_volume=2;,citation_journal_title=Stat. Anal. Data Min.;">
<meta name="citation_reference" content="citation_title=Issues in Evaluation of Stream Learning Algorithms;,citation_author=João Gama;,citation_author=Raquel Sebastião;,citation_author=Pedro Pereira Rodrigues;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’09;">
<meta name="citation_reference" content="citation_title=Stream Data Processing: A Quality of Service Perspective;,citation_author=Qingchun Jiang;,citation_author=Sharma Chakravarthy;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=Probabilistic Counting with Randomized Storage;,citation_author=Benjamin Van Durme;,citation_author=Ashwin Lall;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the 21st International Joint Conference on Artificial Intelligence;,citation_conference=Morgan Kaufmann Publishers Inc.;,citation_series_title=IJCAI’09;">
<meta name="citation_reference" content="citation_title=Adaptive Stream Mining: Pattern Learning and Mining from Evolving Data Streams;,citation_author=Albert Bifet;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=207;,citation_series_title=Frontiers in Artificial Intelligence and Applications;">
<meta name="citation_reference" content="citation_title=We’re not in kansas anymore: detecting domain changes in streams;,citation_author=Mark Dredze;,citation_author=Tim Oates;,citation_author=Christine Piatko;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing;">
<meta name="citation_reference" content="citation_title=Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction (Institute of Mathematical Statistics Monographs);,citation_author=Bradley Efron;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;">
<meta name="citation_reference" content="citation_title=Knowledge Discovery from Data Streams;,citation_author=João Gama;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_series_title=Chapman and Hall / CRC Data Mining and Knowledge Discovery Series;">
<meta name="citation_reference" content="citation_title=A DCT based approach for detecting novelty and concept drift in data streams;,citation_author=Morteza Zi Hayat;,citation_author=Mahmoud Reza Hashemi;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=2010 International Conference of Soft Computing and Pattern Recognition;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Benchmarking Stream Clustering Algorithms within the MOA Framework;,citation_author=Philipp Kranen;,citation_author=Hardy Kremer;,citation_author=Timm Jansen;,citation_author=Thomas Seidl;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_conference_title=16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2010), Washington, DC, USA;">
<meta name="citation_reference" content="citation_title=MOA: Massive Online Analysis;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Richard Kirkby;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=99;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Hellinger distance based drift detection for nonstationary environments;,citation_author=Gregory Ditzler;,citation_author=Robi Polikar;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_conference_title=2011 IEEE symposium on computational intelligence in dynamic and uncertain environments (CIDUE);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Classification and novel class detection in concept-drifting data streams under time constraints;,citation_author=Mohammad Masud;,citation_author=Jing Gao;,citation_author=Latifur Khan;,citation_author=Jiawei Han;,citation_author=Bhavani M Thuraisingham;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=6;,citation_volume=23;,citation_journal_title=IEEE Transactions on Knowledge and Data Engineering;">
<meta name="citation_reference" content="citation_title=New drift detection method for data streams;,citation_author=Parinaz Sobhani;,citation_author=Hamid Beigy;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_conference_title=International conference on adaptive and intelligent systems;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=birch: Dealing With Very Large Datasets Using BIRCH;,citation_author=Lysiane Charest;,citation_author=Justin Harrington;,citation_author=Matias Salibian-Barrera;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Synopses for Massive Data: Samples, Histograms, Wavelets, Sketches;,citation_author=Graham Cormode;,citation_author=Minos Garofalakis;,citation_author=Peter J. Haas;,citation_author=Chris Jermaine;,citation_publication_date=2012-01;,citation_cover_date=2012-01;,citation_year=2012;,citation_issue=1–3;,citation_volume=4;,citation_journal_title=Found. Trends Databases;">
<meta name="citation_reference" content="citation_title=Detection of concept drift for learning from stream data;,citation_author=Jeonghoon Lee;,citation_author=Frederic Magoules;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=2012 IEEE 14th International Conference on High Performance Computing and Communication &amp;amp;amp; 2012 IEEE 9th International Conference on Embedded Software and Systems;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=mlbench: Machine Learning Benchmark Problems;,citation_author=Friedrich Leisch;,citation_author=Evgenia Dimitriadou;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=A unifying view on dataset shift in classification;,citation_author=Jose G. Moreno-Torres;,citation_author=Troy Raeder;,citation_author=Rocı́o Alaı́z-Rodrı́guez;,citation_author=Nitesh V. Chawla;,citation_author=Francisco Herrera;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_volume=45;,citation_journal_title=Pattern Recognit.;">
<meta name="citation_reference" content="citation_title=HadoopStreaming: Utilities for Using R Scripts in Hadoop Streaming;,citation_author=David S. Rosenberg;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Exponentially weighted moving average charts for detecting concept drift;,citation_author=Gordon J Ross;,citation_author=Niall M Adams;,citation_author=Dimitris K Tasoulis;,citation_author=David J Hand;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=2;,citation_volume=33;,citation_journal_title=Pattern recognition letters;">
<meta name="citation_reference" content="citation_title=An efficient method of building an ensemble of classifiers in streaming data;,citation_author=Joung Woo Ryu;,citation_author=Mehmed M Kantardzic;,citation_author=Myung-Won Kim;,citation_author=A Ra Khil;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=International Conference on Big Data Analytics;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=rlecuyer: R Interface to RNG With Multiple Streams;,citation_author=Hana Sevcikova;,citation_author=Tony Rossini;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Machine Learning that Matters;,citation_author=Kiri Wagstaff;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=CD-MOA: Change Detection Framework for Massive Online Analysis;,citation_author=Albert Bifet;,citation_author=Jesse Read;,citation_author=Bernhard Pfahringer;,citation_author=Geoff Holmes;,citation_author=Indrė Žliobaitė;,citation_editor=Allan Tucker;,citation_editor=Frank Höppner;,citation_editor=Arno Siebes;,citation_editor=Stephen Swift;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Advances in Intelligent Data Analysis XII;,citation_conference=Springer Berlin Heidelberg;">
<meta name="citation_reference" content="citation_title=Novelty detection algorithm for data streams multi-class problems;,citation_author=Elaine R Faria;,citation_author=João Gama;,citation_author=André CPLF Carvalho;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 28th annual ACM symposium on applied computing;">
<meta name="citation_reference" content="citation_title=Turning Big Data into Tiny Data: Constant-Size Coresets for k-Means, PCA and Projective Clustering;,citation_author=Dan Feldman;,citation_author=Melanie Schmidt;,citation_author=Christian Sohler;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms;,citation_conference=Society for Industrial; Applied Mathematics;,citation_series_title=SODA ’13;">
<meta name="citation_reference" content="citation_title=On evaluating stream learning algorithms;,citation_author=João Gama;,citation_author=Raquel Sebastião;,citation_author=Pedro Pereira Rodrigues;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=3;,citation_volume=90;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Scalable Strategies for Computing with Massive Data;,citation_author=Michael J. Kane;,citation_author=John Emerson;,citation_author=Stephen Weston;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=14;,citation_volume=55;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=RStorm: Simulate and Develop Streaming Processing in R;,citation_author=Maurits Kaptein;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;">
<meta name="citation_reference" content="citation_title=Drift detection using uncertainty distribution divergence;,citation_author=Patrick Lindstrom;,citation_author=Brian Mac Namee;,citation_author=Sarah Jane Delany;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=1;,citation_volume=4;,citation_journal_title=Evolving Systems;">
<meta name="citation_reference" content="citation_title=Ad Click Prediction: A View from the Trenches;,citation_author=H. Brendan McMahan;,citation_author=Gary Holt;,citation_author=D. Sculley;,citation_author=Michael Young;,citation_author=Dietmar Ebner;,citation_author=Julian Grady;,citation_author=Lan Nie;,citation_author=Todd Phillips;,citation_author=Eugene Davydov;,citation_author=Daniel Golovin;,citation_author=Sharat Chikkerur;,citation_author=Dan Liu;,citation_author=Martin Wattenberg;,citation_author=Arnar Mar Hrafnkelsson;,citation_author=Tom Boulos;,citation_author=Jeremy Kubica;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_conference_title=Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’13;">
<meta name="citation_reference" content="citation_title=Data Stream Clustering: A Survey.;,citation_author=Jonathan A. Silva;,citation_author=Elaine R. Faria;,citation_author=Rodrigo C. Barros;,citation_author=Eduardo R. Hruschka;,citation_author=Andre Carvalho;,citation_author=Joao Gama;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=1;,citation_volume=46;,citation_journal_title=ACM Computer Surveys;">
<meta name="citation_reference" content="citation_title=ff: Memory-efficient Storage of Large Data on Disk and Fast Access Functions;,citation_author=Daniel Adler;,citation_author=Christian Gläser;,citation_author=Oleg Nenadic;,citation_author=Jens Oehlschlägel;,citation_author=Walter Zucchini;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=factas: Data Mining Methods for Data Streams;,citation_author=Romain Bar;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=streamR: Access to Twitter Streaming API via R;,citation_author=Pablo Barbera;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=DBI: R Database Interface;,citation_author=R Special Interest Group on Databases;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Concept drift detection through resampling;,citation_author=Maayan Harel;,citation_author=Shie Mannor;,citation_author=Ran El-Yaniv;,citation_author=Koby Crammer;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=PCA feature extraction for change detection in multidimensional unlabeled data;,citation_author=Ludmila I Kuncheva;,citation_author=William J Faithfull;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=1;,citation_volume=25;,citation_journal_title=IEEE transactions on neural networks and learning systems;">
<meta name="citation_reference" content="citation_title=Mining of Massive Datasets;,citation_author=Jure Leskovec;,citation_author=Anand Rajaraman;,citation_author=Jeffery D. Ullman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=RMOA: Connect R with MOA to perform streaming classifications;,citation_author=Jan Wijffels;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=A Survey on Concept Drift Adaptation;,citation_author=João Gama;,citation_author=Indrundefined Žliobaitundefined;,citation_author=Albert Bifet;,citation_author=Mykola Pechenizkiy;,citation_author=Abdelhamid Bouchachia;,citation_publication_date=2014-03;,citation_cover_date=2014-03;,citation_year=2014;,citation_issue=4;,citation_volume=46;,citation_journal_title=ACM Comput. Surv.;">
<meta name="citation_reference" content="citation_title=Graph Stream Algorithms: A Survey;,citation_author=Andrew McGregor;,citation_publication_date=2014-05;,citation_cover_date=2014-05;,citation_year=2014;,citation_issue=1;,citation_volume=43;,citation_journal_title=SIGMOD Rec.;">
<meta name="citation_reference" content="citation_title=RStorm: Developing and Testing Streaming Algorithms in R;,citation_author=Maurits Kaptein;,citation_publication_date=2014-06;,citation_cover_date=2014-06;,citation_year=2014;,citation_issue=1;,citation_volume=6;,citation_journal_title=The R Journal;">
<meta name="citation_reference" content="citation_title=SNAP Datasets: Stanford Large Network Dataset Collection;,citation_author=Jure Leskovec;,citation_author=Andrej Krevl;,citation_publication_date=2014-06;,citation_cover_date=2014-06;,citation_year=2014;,citation_publisher=http://snap.stanford.edu/data;">
<meta name="citation_reference" content="citation_title=Questioning the Lambda Architecture;,citation_author=Jay Kreps;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Sketching as a Tool for Numerical Linear Algebra;,citation_author=David P. Woodruff;,citation_publication_date=2014-10;,citation_cover_date=2014-10;,citation_year=2014;,citation_issue=1–2;,citation_volume=10;,citation_journal_title=Found. Trends Theor. Comput. Sci.;">
<meta name="citation_reference" content="citation_title=Modeling concept drift: A probabilistic graphical model based approach;,citation_author=Hanen Borchani;,citation_author=Ana Maria Martinez;,citation_author=Andrés R. Masegosa;,citation_author=Helge Langseth;,citation_author=Thomas Dyhre Nielsen;,citation_author=Antonio Salmerón;,citation_author=Antonio Fernández;,citation_author=Anders Læsø Madsen;,citation_author=Ramón Sáez;,citation_editor=Elisa Fromont;,citation_editor=Tijl De Bie;,citation_editor=Matthijs Leeuwen;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=Advances in Intelligent Data Analysis XIV;,citation_conference=Springer;,citation_series_title=Lecture Notes in Computer Science;">
<meta name="citation_reference" content="citation_title=twitteR: R Based Twitter Client;,citation_author=Jeff Gentry;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=rEMM: Extensible Markov Model for Data Stream Clustering in R;,citation_author=Michael Hahsler;,citation_author=Margaret H. Dunham;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=streamMOA: Interface for MOA Stream Clustering Algorithms;,citation_author=Michael Hahsler;,citation_author=Matthew Bolanos;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=rstream: Streams of Random Numbers;,citation_author=Josef Leydold;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Big Data: Principles and Best Practices of Scalable Realtime Data Systems;,citation_author=Nathan Marz;,citation_author=James Warren;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=A pca-based change detection framework for multidimensional data streams: Change detection in multidimensional data streams;,citation_author=Abdulhakim A Qahtan;,citation_author=Basma Alharbi;,citation_author=Suojin Wang;,citation_author=Xiangliang Zhang;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;">
<meta name="citation_reference" content="citation_title=Concept Drift Detection for Streaming Data;,citation_author=Heng Wang;,citation_author=Zubin Abraham;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=International Joint Conference on Neural Networks (IJCNN);">
<meta name="citation_reference" content="citation_title=koaning.io: Linear Models Solving Non-Linear Problems;,citation_author=Vincent Warmerdam;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Experimental Algorithmics Applied to On-line Machine Learning;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Gregor Papa;,citation_editor=Marjan Mernik;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=Bioinspired Optimization Methods and their Applications;">
<meta name="citation_reference" content="citation_title=quantmod: Quantitative Financial Modelling Framework;,citation_author=Jeffrey A. Ryan;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=A grid density based framework for classifying streaming data in the presence of concept drift;,citation_author=Tegjyot Singh Sethi;,citation_author=Mehmed Kantardzic;,citation_author=Hanquing Hu;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=46;,citation_journal_title=Journal of Intelligent Information Systems;">
<meta name="citation_reference" content="citation_title=rJava: Low-level R to Java interface;,citation_author=Simon Urbanek;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=SparkR: Scaling R Programs with Spark;,citation_author=Shivaram Venkataraman;,citation_author=Zongheng Yang;,citation_author=Davies Liu;,citation_author=Eric Liang;,citation_author=Hossein Falaki;,citation_author=Xiangrui Meng;,citation_author=Reynold Xin;,citation_author=Ali Ghodsi;,citation_author=Michael Franklin;,citation_author=Ion Stoica;,citation_author=Matei Zaharia;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=Proceedings of the 2016 International Conference on Management of Data;,citation_conference=Association for Computing Machinery;,citation_series_title=SIGMOD ’16;">
<meta name="citation_reference" content="citation_title=koaning.io: Bayesian/Streaming Algorithms;,citation_author=Vincent Warmerdam;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=Outlier Analysis;,citation_author=Charu C Aggarwal;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Video Popularity Prediction in Data Streams Based on Context-Independent Features;,citation_author=Vitor Silva;,citation_author=Ana Trindade Winck;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_conference_title=Proceedings of the Symposium on Applied Computing;,citation_conference=Association for Computing Machinery;,citation_series_title=SAC ’17;">
<meta name="citation_reference" content="citation_title=Einsatz von Machine-Learning-Verfahren in amtlichen Unternehmensstatistiken;,citation_author=Florian Dumpert;,citation_author=Martin Beck;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=2;,citation_volume=11;,citation_journal_title=AStA Wirtschafts- und Sozialstatistisches Archiv;">
<meta name="citation_reference" content="citation_title=Introduction to stream: An Extensible Framework for Data Stream Clustering Research with R;,citation_author=Michael Hahsler;,citation_author=Matthew Bolaños;,citation_author=John Forrest;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=14;,citation_volume=76;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=stream: Infrastructure for Data Stream Mining;,citation_author=Michael Hahsler;,citation_author=Matthew Bolaños;,citation_author=John Forrest;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Design and Analysis of Experiments;,citation_author=D C Montgomery;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Time Series Forecasting in the Presence of Concept Drift: A PSO-based Approach;,citation_author=Gustavo H. F. M. Oliveira;,citation_author=Rodolfo C. Cavalcante;,citation_author=George G. Cabral;,citation_author=Leandro L. Minku;,citation_author=Adriano L. I. Oliveira;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_conference_title=2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI);">
<meta name="citation_reference" content="citation_title=United Nations Global Pulse. Harnessing big data for development and humanitarian action.;,citation_author=United Nations;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=koaning.io: Passive Agressive Algorithms;,citation_author=Vincent Warmerdam;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=On the Reliable Detection of Concept Drift from Streaming Unlabeled Data;,citation_author=Tegjyot Singh Sethi;,citation_author=Mehmed Kantardzic;,citation_publication_date=2017-03;,citation_cover_date=2017-03;,citation_year=2017;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Proof of Concept Machine Learning - Abschlussbericht;,citation_author=Martin Beck;,citation_author=Florian Dumpert;,citation_author=Jörg Feuerhake;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_technical_report_institution=Statistisches Bundesamt (Destatis);">
<meta name="citation_reference" content="citation_title=Machine Learning for Data Streams with Practical Examples in MOA;,citation_author=Albert Bifet;,citation_author=Ricard Gavalda;,citation_author=Geoff Holmes;,citation_author=Bernhard Pfahringer;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Lifelong Machine Learning;,citation_author=Zhiyuan Chen;,citation_author=Bing Liu;,citation_author=Ronald Brachman;,citation_author=Peter Stone;,citation_author=Francesca Rossi;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Incremental on-line learning: A review and comparison of state of the art algorithms;,citation_author=Viktor Losing;,citation_author=Barbara Hammer;,citation_author=Heiko Wersing;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_volume=275;,citation_journal_title=Neurocomputing;">
<meta name="citation_reference" content="citation_title=Extremely Fast Decision Tree;,citation_author=Chaitanya Manapragada;,citation_author=Geoffrey I. Webb;,citation_author=Mahsa Salehi;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’18;">
<meta name="citation_reference" content="citation_title=An evaluation of data stream clustering algorithms;,citation_author=Stratos Mansalis;,citation_author=Eirini Ntoutsi;,citation_author=Nikos Pelekis;,citation_author=Yannis Theodoridis;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_volume=11;,citation_journal_title=Statistical Analysis and Data Mining: The ASA Data Science Journal;">
<meta name="citation_reference" content="citation_title=koaning.io: How to win with simple, even linear, models;,citation_author=Vincent Warmerdam;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Anomaly Detection in Manufacturing Systems Using Structured Neural Networks;,citation_author=J. Liu;,citation_author=J. Guo;,citation_author=P. V. Orlik;,citation_author=M. Shibata;,citation_author=D. Nakahara;,citation_author=S. Mii;,citation_author=M. Takac;,citation_publication_date=2018-07;,citation_cover_date=2018-07;,citation_year=2018;,citation_technical_report_institution=MITSUBISHI ELECTRIC RESEARCH LABORATORIES;">
<meta name="citation_reference" content="citation_title=Industrial Internet of Things Based Ransomware Detection Using Stacked Variational Neural Network;,citation_author=Muna Al-Hawawreh;,citation_author=Elena Sitnikova;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=Proceedings of the 3rd International Conference on Big Data and Internet of Things;,citation_conference=Association for Computing Machinery;,citation_series_title=BDIOT 2019;">
<meta name="citation_reference" content="citation_title=Evaluation of Cognitive Architectures for Cyber-Physical Production Systems;,citation_author=Andreas Bunte;,citation_author=Andreas Fischbach;,citation_author=Jan Strohschein;,citation_author=Thomas Bartz-Beielstein;,citation_author=Heide Faeskorn-Woyke;,citation_author=Oliver Niggemann;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=24th IEEE International Conference on Emerging Technologies and Factory Automation, ETFA 2019, Zaragoza, Spain, September 10-13, 2019;">
<meta name="citation_reference" content="citation_title=Nowcasting: Ein Echtzeit-Indikator für die Konjunkturanalyse;,citation_author=Charlotte Senftleben;,citation_author=Till Strohsal;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=koaning.io: The Future of Data Science is Past;,citation_author=Vincent Warmerdam;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;">
<meta name="citation_reference" content="citation_title=Forecasting inflation with online prices;,citation_author=Diego Aparicio;,citation_author=Manuel I. Bertolotto;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_volume=36;,citation_journal_title=International Journal of Forecasting;">
<meta name="citation_reference" content="citation_title=CAAI—a cognitive architecture to introduce artificial intelligence in cyber-physical production systems;,citation_author=Andreas Fischbach;,citation_author=Jan Strohschein;,citation_author=Andreas Bunte;,citation_author=Jörg Stork;,citation_author=Heide Faeskorn-Woyke;,citation_author=Natalia Moriz;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=1;,citation_volume=111;,citation_journal_title=The International Journal of Advanced Manufacturing Technology;">
<meta name="citation_reference" content="citation_title=Delayed labelling evaluation for data streams;,citation_author=Maciej Grzenda;,citation_author=Heitor Murilo Gomes;,citation_author=Albert Bifet;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=5;,citation_volume=34;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Stream Data Mining: Algorithms and Their Probabilistic Properties;,citation_editor=Leszek Rutkowski;,citation_editor=Maciej Jaworski;,citation_editor=Piotr Duda;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_series_title=Studies in Big Data;">
<meta name="citation_reference" content="citation_title=Loghub: A Large Collection of System Log Datasets towards Automated Log Analytics;,citation_author=Shilin He;,citation_author=Jieming Zhu;,citation_author=Pinjia He;,citation_author=Michael R. Lyu;,citation_publication_date=2020-08;,citation_cover_date=2020-08;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Nowcasting German GDP: Foreign factors, financial markets, and model averaging;,citation_author=Paolo Andreini;,citation_author=Thomas Hasenzagl;,citation_author=Lucrezia Reichlin;,citation_author=Charlotte Senftleben-König;,citation_author=Till Strohsal;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=International Journal of Forecasting;">
<meta name="citation_reference" content="citation_title=Modelling the COVID-19 Virus Evolution With Incremental Machine Learning;,citation_author=Andrés L. Suárez-Cetrulo;,citation_author=Ankit Kumar;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Machine Learning for Time-Series with Python: Forecast, predict, and detect;,citation_author=Den Auffarth;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Machine Learning in der amtlichen Statistik - Ergebnisse und Bewertung eines internationalen Projekts;,citation_author=Florian Dumpert;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=Statistisches Bundesamt (Destatis);,citation_technical_report_number=4;">
<meta name="citation_reference" content="citation_title=Recurring concept memory management in data streams: exploiting data stream concept evolution to improve performance and transparency;,citation_author=Ben Halstead;,citation_author=Yun Sing Koh;,citation_author=Patricia Riddle;,citation_author=Russel Pears;,citation_author=Mykola Pechenizkiy;,citation_author=Albert Bifet;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=3;,citation_volume=35;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=Soziale Marktwirtschaft in der digitalen Zukunft: Foresight-Bericht Strategischer Vorausschauprozess des BMWi;,citation_author=Dirk Holtmannspötter;,citation_author=Ulrich Heimeshoff;,citation_author=Justus Haucap;,citation_author=Ina Loebert;,citation_author=Christoph Busch;,citation_author=Andreas Hoffknecht;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=VDI Technologiezentrum GmbH im Auftrag des Bundesministerium für Wirtschaft und Energie;">
<meta name="citation_reference" content="citation_title=River: machine learning for streaming data in Python;,citation_author=Jacob Montiel;,citation_author=Max Halford;,citation_author=Saulo Martiello Mastelini;,citation_author=Geoffrey Bolmier;,citation_author=Raphael Sourty;,citation_author=Robin Vaysse;,citation_author=Adil Zouitine;,citation_author=Heitor Murilo Gomes;,citation_author=Jesse Read;,citation_author=Talel Abdessalem;,citation_author=others;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Practical Machine Learning for Streaming Data with Python;,citation_author=Sayan Putatunda;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Infrerring Concept Drift Without Labeled Data;,citation_author=Andrew Reed;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_technical_report_institution=Cloudera Fast Forward Labs;,citation_technical_report_number=FF21;">
<meta name="citation_reference" content="citation_title=Cognitive capabilities for the CAAI in cyber-physical production systems;,citation_author=Jan Strohschein;,citation_author=Andreas Fischbach;,citation_author=Andreas Bunte;,citation_author=Heide Faeskorn-Woyke;,citation_author=Natalia Moriz;,citation_author=Thomas Bartz-Beielstein;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=The International Journal of Advanced Manufacturing Technology;">
<meta name="citation_reference" content="citation_title=FARF: A Fair and Adaptive Random Forests Classifier;,citation_author=Wenbin Zhang;,citation_author=Albert Bifet;,citation_author=Xiangliang Zhang;,citation_author=Jeremy C. Weiss;,citation_author=Wolfgang Nejdl;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Advances in Knowledge Discovery and Data Mining: 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11–14, 2021, Proceedings, Part II;,citation_conference=Springer-Verlag;">
<meta name="citation_reference" content="citation_title=Modelling the COVID-19 virus evolution with Incremental Machine Learning;,citation_author=Andrés L. Suárez-Cetrulo;,citation_author=Ankit Kumar;,citation_author=Luis Miralles-Pechuán;,citation_publication_date=2021-04;,citation_cover_date=2021-04;,citation_year=2021;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Online Learning and Active Learning: A Comparative Study of Passive-Aggressive Algorithm With Support Vector Machine (SVM);,citation_author=K. I Ezukwoke;,citation_author=S. J Zareian;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=3;,citation_volume=21;,citation_journal_title=Journal of Higher Education Theory and Practice;">
<meta name="citation_reference" content="citation_title=Digitale Ordnungspolitik – Wirtschaftspolitik daten- und evidenzbasiert weiterentwickeln;,citation_author=Philipp Steinberg;,citation_author=Nils Börnsen;,citation_author=Dirk Neumann;,citation_publication_date=2021-09;,citation_cover_date=2021-09;,citation_year=2021;,citation_publisher=Wirtschaftsdienst;">
<meta name="citation_reference" content="citation_title=Incremental Unsupervised Domain-Adversarial Training of Neural Networks;,citation_author=Antonio-Javier Gallego;,citation_author=Jorge Calvo-Zaragoza;,citation_author=Robert B. Fisher;,citation_publication_date=2021-11;,citation_cover_date=2021-11;,citation_year=2021;,citation_issue=11;,citation_volume=32;,citation_journal_title=IEEE Transactions on Neural Networks and Learning Systems;">
<meta name="citation_reference" content="citation_title=Incremental learning for property price estimation using location-based services and open data;,citation_author=Francisco Alvarez;,citation_author=Edgar Roman-Rangel;,citation_author=Luis V. Montiel;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_volume=107;,citation_journal_title=Engineering Applications of Artificial Intelligence;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Interpretable Machine Learning: Moving from Mythos to Diagnostics;,citation_author=Valerie Chen;,citation_author=Jeffrey Li;,citation_author=Joon Sik Kim;,citation_author=Gregory Plumb;,citation_author=Ameet Talwalkar;,citation_publication_date=2022-01;,citation_cover_date=2022-01;,citation_year=2022;,citation_issue=6;,citation_volume=19;,citation_journal_title=Queue;">
<meta name="citation_reference" content="citation_title=Online Time-series Anomaly Detection: A Survey of Modern Model-based Approaches;,citation_author=Lucas Correia;,citation_author=Jan-Christoph Goos;,citation_author=Anna V. Kononova;,citation_author=Thomas Bäck;,citation_author=Philipp Klein;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_technical_report_institution=Mercedes-Benz, Germany;">
<meta name="citation_reference" content="citation_title=Green Accelerated Hoeffding Tree;,citation_author=Eva Garcia-Martin;,citation_author=Albert Bifet;,citation_author=Niklas Lavesson;,citation_author=Rikard König;,citation_author=Henrik Linusson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Monitoring the Economy in Real Time: Trends and Gaps in Real Activity and Prices;,citation_author=Thomas Hasenzagl;,citation_author=Filippo Pellegrino;,citation_author=Lucrezia Reichlin;,citation_author=Giovanni Ricco;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Efficiently Correcting Machine Learning: Considering the Role of Example Ordering in Human-in-the-Loop Training of Image Classification Models;,citation_author=Geoff Holmes;,citation_author=Eibe Frank;,citation_author=Dale Fletcher;,citation_author=Corey Sterling;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=27th International Conference on Intelligent User Interfaces;,citation_conference=Association for Computing Machinery;,citation_series_title=IUI ’22;">
<meta name="citation_reference" content="citation_title=TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models;,citation_author=Joel Jang;,citation_author=Seonghyeon Ye;,citation_author=Changho Lee;,citation_author=Sohee Yang;,citation_author=Joongbo Shin;,citation_author=Janghoon Han;,citation_author=Gyeonghun Kim;,citation_author=Minjoon Seo;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Fast Mining and Forecasting of Co-Evolving Epidemiological Data Streams;,citation_author=Tasuku Kimura;,citation_author=Yasuko Matsubara;,citation_author=Koki Kawabata;,citation_author=Yasushi Sakurai;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery;,citation_series_title=KDD ’22;">
<meta name="citation_reference" content="citation_title=Maschine Learning for Streaming Data with Python;,citation_author=Jan Korstanje;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=The Disagreement Problem in Explainable Machine Learning: A Practitioner’s Perspective;,citation_author=Satyapriya Krishna;,citation_author=Tessa Han;,citation_author=Alex Gu;,citation_author=Javin Pombra;,citation_author=Shahin Jabbari;,citation_author=Steven Wu;,citation_author=Himabindu Lakkaraju;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=SparkR: R Front End for ’Apache Spark’;,citation_author=The Apache Software Foundation;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Short-term local predictions of COVID-19 in the United Kingdom using dynamic supervised machine learning algorithms;,citation_author=Xin Wang;,citation_author=Yijia Dong;,citation_author=William David Thompson;,citation_author=Harish Nair;,citation_author=You Li;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=1;,citation_volume=2;,citation_journal_title=Communications Medicine;">
<meta name="citation_reference" content="citation_title=Log-based Anomaly Detection with Deep Learning: How Far Are We?;,citation_author=Van-Hoang Le;,citation_author=Hongyu Zhang;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Reliance on metrics is a fundamental challenge for AI.;,citation_author=Rachel L Thomas;,citation_author=David Uminsky;,citation_publication_date=2022-05;,citation_cover_date=2022-05;,citation_year=2022;,citation_issue=5;,citation_volume=3;,citation_journal_title=Patterns (N Y);">
<meta name="citation_reference" content="citation_title=SMRD.de Benutzerhandbuch;,citation_author=Niyaz Valitov;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_technical_report_institution=Bundesnetzagentur für Elektrizität, Gas, Telekommunikation, Post und Eisenbahnen;">
<meta name="citation_reference" content="citation_title=Use of web scraping and text mining techniques in the Istat survey on “Information and Communication Technology in enterprises”;,citation_author=G. Barcaroli;,citation_author=A. Nurra;,citation_author=M. Scarnò;,citation_author=D. Summa;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Who Makes Mistakes? Using Data Mining Techniques to Analyze Reporting Errors in Total Acres Operated;,citation_author=Jaki S. McCarthy;,citation_author=Morgan S. Earp;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_fulltext_html_url=https://ideas.repec.org/p/ags/unasrr/234367.html;,citation_doi=10.22004/AG.ECON.234367;,citation_journal_title=NASS Research Reports;,citation_publisher=United States Department of Agriculture, National Agricultural Statistics Service;">
<meta name="citation_reference" content="citation_title=Modeling Non-response in National Agricultural Statistics Service (NASS) Surveys Using Classification Trees;,citation_author=Jaki S Mccarthy;,citation_author=Thomas Jacob;,citation_author=Amanda Mccracken;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;">
<meta name="citation_reference" content="citation_title=2007 Census of Agriculture Non-Response Methodology;,citation_author=Will Cecere;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=Exploring Quarterly Agricultural Survey Questionnaire Version Reduction Scenarios;,citation_author=Morgan Earp;,citation_author=Scott Cox;,citation_author=Jody Mcdaniel;,citation_author=Chadd Crouse;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;">
<meta name="citation_reference" content="citation_title=USE OF MACHINE LEARNING METHODS TO IMPUTE CATEGORICAL DATA;,citation_author=P. Rey;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Innovative Uses of Data Mining Techniques in the Production of Official Statistics;,citation_author=Jaki Mccarthy;,citation_author=Thomas Jacob;,citation_author=Dale Atkinson;">
<meta name="citation_reference" content="citation_title=Automatic Coding of Occupations. Using Machine Learning Algorithms for Occupation Coding in Several German Panel Surveys;,citation_fulltext_html_url=https://www.researchgate.net/publication/266259591_Automatic_Coding_of_Occupations_Using_Machine_Learning_Algorithms_for_Occupation_Coding_in_Several_German_Panel_Surveys;">
<meta name="citation_reference" content="citation_title=Evaluating hourly air quality forecasting in Canada with nonlinear updatable machine learning methods;,citation_author=Huiping Peng;,citation_author=Aranildo R. Lima;,citation_author=Andrew Teakles;,citation_author=Jian Jin;,citation_author=Alex J. Cannon;,citation_author=William W. Hsieh;,citation_publication_date=2017-03-01;,citation_cover_date=2017-03-01;,citation_year=2017;,citation_fulltext_html_url=https://doi.org/10.1007/s11869-016-0414-3;,citation_issue=2;,citation_doi=10.1007/s11869-016-0414-3;,citation_issn=1873-9326;,citation_volume=10;,citation_journal_title=Air Quality, Atmosphere &amp;amp;amp; Health;">
<meta name="citation_reference" content="citation_title=Online Machine Learning in Big Data Streams;,citation_author=András A. Benczúr;,citation_author=Levente Kocsis;,citation_author=Róbert Pálovics;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://arxiv.org/abs/1802.05872;,citation_volume=abs/1802.05872;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=Learn: A Novel incremental learning method for text classification;,citation_author=Guangxu Shan;,citation_author=Shiyao Xu;,citation_author=Li Yang;,citation_author=Shengbin Jia;,citation_author=Yang Xiang;,citation_publication_date=2020-06;,citation_cover_date=2020-06;,citation_year=2020;,citation_doi=10.1016/J.ESWA.2020.113198;,citation_issn=0957-4174;,citation_volume=147;,citation_journal_title=Expert Systems with Applications;,citation_publisher=Pergamon;">
<meta name="citation_reference" content="citation_title=Incremental Real-Time Learning Framework for Sentiment Classification: Indian General Election 2019, A Case Study;,citation_author=Sharmistha Chatterjee;,citation_author=Sushmita Gupta;,citation_publication_date=2021-03;,citation_cover_date=2021-03;,citation_year=2021;,citation_doi=10.1109/ICBDA51983.2021.9402992;,citation_isbn=9780738131672;,citation_journal_title=2021 IEEE 6th International Conference on Big Data Analytics, ICBDA 2021;,citation_publisher=Institute of Electrical; Electronics Engineers Inc.;">
<meta name="citation_reference" content="citation_title=MOA: Massive Online Analysis;,citation_abstract=Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Na¨ıveNa¨ıve Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis , and is released under the GNU GPL license.;,citation_author=Albert Bifet;,citation_author=Geoff Holmes;,citation_author=Richard Kirkby;,citation_author=Bernhard Pfahringer;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=11;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Evaluation and Performance Measurement;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Hyperparameter Tuning Approaches;,citation_author=Thomas Bartz-Beielstein;,citation_author=Martin Zaefferer;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_editor=Martin Zaefferer;,citation_editor=Olaf Mersmann;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_inbook_title=Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide;">
<meta name="citation_reference" content="citation_title=Drift Detection and&nbsp;Handling;,citation_abstract=Structural changes (“drift”) in the data cause problems for many algorithms. Based on the drift definitions given in Chap. 1, methods for drift detection and handling are discussed. For the algorithms presented in Chap. 2, it is clarified to what extent concept drift is reacted to. In turn, the extent to which catastrophic forgetting is an issue is described in Sect. 4.3. Section&nbsp;3.1 describes three architectures for implementing drift detection algorithms. Basic properties of window-based approaches are presented in Sect.&nbsp;3.2. Section&nbsp;3.4 presents commonly used drift detection techniques. Section&nbsp;3.4 describes how the drift detection techniques introduced in Sect.&nbsp;3.3 are used in Online Machine Learning (OML) algorithms and summarizes the tree-based OML techniques implemented in the River package. Section&nbsp;3.5 introduces scaling methods for handling drift.;,citation_author=Thomas Bartz-Beielstein;,citation_author=Lukas Hans;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1007/978-981-99-7007-0_3;,citation_doi=10.1007/978-981-99-7007-0_3;,citation_isbn=978-981-99-7007-0;,citation_inbook_title=Online Machine Learning: A Practical Guide with Examples in Python;">
<meta name="citation_reference" content="citation_title=Introduction: From Batch to&nbsp;Online Machine Learning;,citation_abstract=Batch Machine Learning (BML), which is also referred to as “offline machine learning”, reaches its limits when dealing with very large amounts of data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. In this chapter, the basic terms and concepts of OML are introduced and the differences to BML are shown.;,citation_author=Thomas Bartz-Beielstein;,citation_editor=Eva Bartz;,citation_editor=Thomas Bartz-Beielstein;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1007/978-981-99-7007-0_1;,citation_doi=10.1007/978-981-99-7007-0_1;,citation_isbn=978-981-99-7007-0;,citation_inbook_title=Online Machine Learning: A Practical Guide with Examples in Python;">
<meta name="citation_reference" content="citation_title=AMF: Aggregated Mondrian Forests for Online Learning;,citation_author=Jaouad Mourtada;,citation_author=Stephane Gaiffas;,citation_author=Erwan Scornet;,citation_publication_date=2019-06;,citation_cover_date=2019-06;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1906.10529;,citation_doi=10.48550/arXiv.1906.10529;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Extremely fast decision tree;,citation_abstract=We introduce a novel incremental decision tree learning algorithm, Hoeffding Anytime Tree, that is statistically more efficient than the current state-of-the-art, Hoeffding Tree. We demonstrate that an implementation of Hoeffding Anytime Tree’“Extremely Fast Decision Tree”, a minor modification to the MOA implementation of Hoeffding Tree’obtains significantly superior prequential accuracy on most of the largest classification datasets from the UCI repository. Hoeffding Anytime Tree produces the asymptotic batch tree in the limit, is naturally resilient to concept drift, and can be used as a higher accuracy replacement for Hoeffding Tree in most scenarios, at a small additional computational cost.;,citation_author=Chaitanya Manapragada;,citation_author=Geoffrey I. Webb;,citation_author=Mahsa Salehi;,citation_editor=Chih-Jen  Lin;,citation_editor=Hui  Xiong;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://www.kdd.org/kdd2018/, https://dl.acm.org/doi/proceedings/10.1145/3219819;,citation_doi=10.1145/3219819.3220005;,citation_conference_title=KDD’ 2018 - Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;,citation_conference=Association for Computing Machinery (ACM);">
<meta name="citation_reference" content="citation_title=Surrogates;,citation_author=Robert B Gramacy;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=UvA Deep Learning Tutorials;,citation_author=Phillip Lippe;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://github.com/phlippe/uvadlc_notebooks/tree/master;">
<meta name="citation_reference" content="citation_title=Attention Is All You Need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017-06;,citation_cover_date=2017-06;,citation_year=2017;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=On the Variance of the Adaptive Learning Rate and Beyond;,citation_author=Liyuan Liu;,citation_author=Haoming Jiang;,citation_author=Pengcheng He;,citation_author=Weizhu Chen;,citation_author=Xiaodong Liu;,citation_author=Jianfeng Gao;,citation_author=Jiawei Han;,citation_publication_date=2019-08;,citation_cover_date=2019-08;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Efficient Transformers: A Survey;,citation_author=Yi Tay;,citation_author=Mostafa Dehghani;,citation_author=Dara Bahri;,citation_author=Donald Metzler;,citation_publication_date=2020-09;,citation_cover_date=2020-09;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding;,citation_author=Jacob Devlin;,citation_author=Ming-Wei Chang;,citation_author=Kenton Lee;,citation_author=Kristina Toutanova;,citation_publication_date=2018-10;,citation_cover_date=2018-10;,citation_year=2018;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale;,citation_author=Alexey Dosovitskiy;,citation_author=Lucas Beyer;,citation_author=Alexander Kolesnikov;,citation_author=Dirk Weissenborn;,citation_author=Xiaohua Zhai;,citation_author=Thomas Unterthiner;,citation_author=Mostafa Dehghani;,citation_author=Matthias Minderer;,citation_author=Georg Heigold;,citation_author=Sylvain Gelly;,citation_author=Jakob Uszkoreit;,citation_author=Neil Houlsby;,citation_publication_date=2020-10;,citation_cover_date=2020-10;,citation_year=2020;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Attention is not Explanation;,citation_author=Sarthak Jain;,citation_author=Byron C. Wallace;,citation_publication_date=2019-02;,citation_cover_date=2019-02;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Attention is not not Explanation;,citation_author=Sarah Wiegreffe;,citation_author=Yuval Pinter;,citation_publication_date=2019-08;,citation_cover_date=2019-08;,citation_year=2019;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Multivariate adaptive regression splines;,citation_author=Jerome H. Friedman;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_issue=1;,citation_volume=19;,citation_journal_title=The annals of statistics;">
<meta name="citation_reference" content="citation_title=Learning model trees from evolving data streams;,citation_author=Elena Ikonomovska;,citation_author=João Gama;,citation_author=Sašo Džeroski;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=1;,citation_volume=23;,citation_journal_title=Data Mining and Knowledge Discovery;">
<meta name="citation_reference" content="citation_title=An Introduction to Statistical Learning with Applications in R;,citation_author=Gareth James;,citation_author=Daniela Witten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;">
<meta name="citation_reference" content="citation_title=Deep Learning with Python;,citation_author=Francoise Chollet;,citation_author=J. J. Allaire;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=A survey of cross-validation procedures for model selection;,citation_author=Sylvain Arlot;,citation_author=Alain Celisse;,citation_author=others;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_volume=4;,citation_journal_title=Statistics surveys;">
<meta name="citation_reference" content="citation_title=A cross-validatory method for dependent data;,citation_author=PRABIR BURMAN;,citation_author=EDMOND CHOW;,citation_author=DEBORAH NOLAN;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=2;,citation_volume=81;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection;,citation_author=Ron Kohavi;,citation_publication_date=1995;,citation_cover_date=1995;,citation_year=1995;,citation_conference_title=Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2;,citation_conference=Morgan Kaufmann Publishers Inc.;,citation_series_title=IJCAI’95;">
<meta name="citation_reference" content="citation_title=Kriging-based sequential design strategies using fast cross-validation techniques with extensions to multi-fidelity computer codes ;,citation_author=Loic Le Gratiet;,citation_author=Claire Cannamela;,citation_publication_date=2012-10;,citation_cover_date=2012-10;,citation_year=2012;">
<meta name="citation_reference" content="citation_title=Cross-Validation of Regression Models;,citation_author=Richard R. Picard;,citation_author=R. Dennis Cook;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_issue=387;,citation_volume=79;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=The Elements of Statistical Learning;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_author=Jerome Friedman;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;">
<meta name="citation_reference" content="citation_title=Deep Residual Learning for Image Recognition;,citation_author=Kaiming He;,citation_author=Xiangyu Zhang;,citation_author=Shaoqing Ren;,citation_author=Jian Sun;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;">
<meta name="citation_reference" content="citation_title=Identity Mappings in Deep Residual Networks;,citation_author=Kaiming He;,citation_author=Xiangyu Zhang;,citation_author=Shaoqing Ren;,citation_author=Jian Sun;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Neural Ordinary Differential Equations;,citation_author=Ricky T. Q. Chen;,citation_author=Yulia Rubanova;,citation_author=Jesse Bettencourt;,citation_author=David Duvenaud;,citation_publication_date=2018-06;,citation_cover_date=2018-06;,citation_year=2018;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Mathematical Theory of Optimal Processes;,citation_author=undefined Pontryagin;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;">
<meta name="citation_reference" content="citation_title=On Neural Differential Equations;,citation_author=Patrick Kidger;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_journal_title=arXiv e-prints;">
<meta name="citation_reference" content="citation_title=Stochastic simulation optimization: an optimal computing budget allocation;,citation_author=Chun Hung Chen;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;">
<meta name="citation_reference" content="citation_title=Sequential Parameter Optimization and Optimal Computational Budget Allocation for Noisy Optimization Problems;,citation_author=Thomas Bartz-Beielstein;,citation_author=Martina Friese;,citation_publication_date=2011-01;,citation_cover_date=2011-01;,citation_year=2011;">
<meta name="citation_reference" content="citation_title=Statistik;,citation_author=Joachim Hartung;,citation_author=Bärbel Elpert;,citation_author=Karl-Heinz Klösener;,citation_publication_date=1995;,citation_cover_date=1995;,citation_year=1995;">
<meta name="citation_reference" content="citation_title=Partial correlation — Wikipedia, The Free Encyclopedia;,citation_author=Wikipedia contributors;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://en.wikipedia.org/w/index.php?title=Partial_correlation&amp;amp;amp;oldid=1253637419;">
<meta name="citation_reference" content="citation_title=Understanding Correlation;,citation_author=R. J. Rummel;,citation_publication_date=1976;,citation_cover_date=1976;,citation_year=1976;,citation_fulltext_html_url=https://www.hawaii.edu/powerkills/UC.HTM;">
<meta name="citation_reference" content="citation_title=Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies;,citation_author=Zhiqiang Wang;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=2;,citation_volume=7;,citation_journal_title=The Stata Journal;">
<meta name="citation_reference" content="citation_title=Response surface methodology: process and product optimization using designed experiments;,citation_author=Raymond H Myers;,citation_author=Douglas C Montgomery;,citation_author=Christine M Anderson-Cook;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=desirability: Function Optimization and Ranking via Desirability Functions;,citation_author=Max Kuhn;,citation_publication_date=2016-09;,citation_cover_date=2016-09;,citation_year=2016;">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./100_ddmo_eda.html">Data-Driven Modeling and Optimization</a></li><li class="breadcrumb-item"><a href="./100_ddmo_eda.html"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Basic Statistics and Data Analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hyperparameter Tuning Cookbook</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sequential-parameter-optimization/spotpython" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Hyperparameter-Tuning-Cookbook.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_optimization_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002_awwe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Aircraft Wing Weight Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003_scipy_optimize_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to <code>scipy.optimize</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004_spot_sklearn_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Using <code>scipy</code> Optimizers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Numerical Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_num_rsm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction: Numerical Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_num_spot_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to spotpython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008_num_spot_multidim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multi-dimensional Functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009_num_spot_anisotropic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Isotropic and Anisotropic Kriging</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010_num_spot_sklearn_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Using <code>sklearn</code> Surrogates in <code>spotpython</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_num_spot_sklearn_gaussian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Gaussian Process Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_num_spot_ei.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Expected Improvement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_num_spot_noisy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Handling Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014_num_spot_ocba.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Optimal Computational Budget Allocation in <code>Spot</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_num_spot_correlation_p.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Kriging with Varying Correlation-p</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016_num_spot_factorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Factorial Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017_num_spot_user_function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">User-Specified Functions: Extending the Analytical Class</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Data-Driven Modeling and Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_ddmo_eda.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Basic Statistics and Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_ddmo_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_ddmo_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_ddmo_clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning and AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./200_mlai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Machine Learning and Artificial Intelligence</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Hyperparameter Tuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./300_hpt_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with Sklearn</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./400_spot_hpt_sklearn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">HPT: sklearn</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./401_spot_hpt_sklearn_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">HPT: sklearn SVC on Moons Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./401_spot_hpt_sklearn_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Step 2: Initialization of the Empty <code>fun_control</code> Dictionary</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with River</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./500_spot_hpt_river.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">HPT: River</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./501_spot_river_gui.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Simplifying Hyperparameter Tuning in Online Machine Learning—The spotRiverGUI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./502_spot_hpt_river_friedman_htr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title"><code>river</code> Hyperparameter Tuning: Hoeffding Tree Regressor with Friedman Drift Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./503_spot_hpt_river_friedman_amfr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">The Friedman Drift Data Set</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Hyperparameter Tuning with PyTorch Lightning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./700_lightning_basic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Basic Lightning Module</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./701_lightning_details.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Details of the Lightning Module Integration in spotpython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./702_lightning_user_datamodule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">User Specified Basic Lightning Module With spotpython</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./600_spot_lightning_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning: Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_diabetes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_user_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning and User Data Sets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_user_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning and User Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning: ResNets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_neural_ode.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Neural ODEs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_neural_ode_example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Neural ODE Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_pinn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Physics Informed Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./601_spot_hpt_light_pinn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with PyTorch Lightning: Physics Informed Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./602_spot_lightning_xai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Explainable AI with SpotPython and Pytorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./603_spot_lightning_transformer_introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">HPT PyTorch Lightning Transformer: Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./603_spot_lightning_transformer_hpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning of a Transformer Network with PyTorch Lightning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./604_spot_lightning_save_load_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Saving and Loading</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./605_spot_hpt_light_diabetes_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set Using a ResNet Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./606_spot_hpt_light_diabetes_user_resnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning for the Diabetes Data Set Using a User Specified ResNet Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./608_spot_hpt_light_condnet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning with <code>spotpython</code> and <code>PyTorch</code> Lightning Using a CondNet Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_01_intro_to_notebooks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Introduction to Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_02_git_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Git Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_03_python_intro_en.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Python Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_04_spot_doc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Documentation of the Sequential Parameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_05_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_06_slurm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Using Slurm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_07_package.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Python Package Building</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a_99_solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Solutions to Selected Exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link active" data-scroll-target="#exploratory-data-analysis"><span class="header-section-number">18.1</span> Exploratory Data Analysis</a>
  <ul class="collapse">
  <li><a href="#histograms" id="toc-histograms" class="nav-link" data-scroll-target="#histograms"><span class="header-section-number">18.1.1</span> Histograms</a></li>
  <li><a href="#boxplots" id="toc-boxplots" class="nav-link" data-scroll-target="#boxplots"><span class="header-section-number">18.1.2</span> Boxplots</a></li>
  </ul></li>
  <li><a href="#probability-distributions" id="toc-probability-distributions" class="nav-link" data-scroll-target="#probability-distributions"><span class="header-section-number">18.2</span> Probability Distributions</a>
  <ul class="collapse">
  <li><a href="#sampling-from-a-distribution" id="toc-sampling-from-a-distribution" class="nav-link" data-scroll-target="#sampling-from-a-distribution"><span class="header-section-number">18.2.1</span> Sampling from a Distribution</a></li>
  </ul></li>
  <li><a href="#discrete-distributions" id="toc-discrete-distributions" class="nav-link" data-scroll-target="#discrete-distributions"><span class="header-section-number">18.3</span> Discrete Distributions</a>
  <ul class="collapse">
  <li><a href="#bernoulli-distribution" id="toc-bernoulli-distribution" class="nav-link" data-scroll-target="#bernoulli-distribution"><span class="header-section-number">18.3.1</span> Bernoulli Distribution</a></li>
  <li><a href="#binomial-distribution" id="toc-binomial-distribution" class="nav-link" data-scroll-target="#binomial-distribution"><span class="header-section-number">18.3.2</span> Binomial Distribution</a></li>
  </ul></li>
  <li><a href="#continuous-distributions" id="toc-continuous-distributions" class="nav-link" data-scroll-target="#continuous-distributions"><span class="header-section-number">18.4</span> Continuous Distributions</a>
  <ul class="collapse">
  <li><a href="#distribution-functions-pdfs-and-cdfs" id="toc-distribution-functions-pdfs-and-cdfs" class="nav-link" data-scroll-target="#distribution-functions-pdfs-and-cdfs"><span class="header-section-number">18.4.1</span> Distribution functions: PDFs and CDFs</a></li>
  </ul></li>
  <li><a href="#background-expectation-mean-standard-deviation" id="toc-background-expectation-mean-standard-deviation" class="nav-link" data-scroll-target="#background-expectation-mean-standard-deviation"><span class="header-section-number">18.5</span> Background: Expectation, Mean, Standard Deviation</a></li>
  <li><a href="#distributions-and-random-numbers-in-python" id="toc-distributions-and-random-numbers-in-python" class="nav-link" data-scroll-target="#distributions-and-random-numbers-in-python"><span class="header-section-number">18.6</span> Distributions and Random Numbers in Python</a>
  <ul class="collapse">
  <li><a href="#calculation-of-the-standard-deviation-with-python" id="toc-calculation-of-the-standard-deviation-with-python" class="nav-link" data-scroll-target="#calculation-of-the-standard-deviation-with-python"><span class="header-section-number">18.6.1</span> Calculation of the Standard Deviation with Python</a></li>
  <li><a href="#the-argument-axis" id="toc-the-argument-axis" class="nav-link" data-scroll-target="#the-argument-axis"><span class="header-section-number">18.6.2</span> The Argument “axis”</a></li>
  </ul></li>
  <li><a href="#expectation-continuous" id="toc-expectation-continuous" class="nav-link" data-scroll-target="#expectation-continuous"><span class="header-section-number">18.7</span> Expectation (Continuous)</a></li>
  <li><a href="#variance-and-standard-deviation-continuous" id="toc-variance-and-standard-deviation-continuous" class="nav-link" data-scroll-target="#variance-and-standard-deviation-continuous"><span class="header-section-number">18.8</span> Variance and Standard Deviation (Continuous)</a>
  <ul class="collapse">
  <li><a href="#the-uniform-distribution" id="toc-the-uniform-distribution" class="nav-link" data-scroll-target="#the-uniform-distribution"><span class="header-section-number">18.8.1</span> The Uniform Distribution</a></li>
  </ul></li>
  <li><a href="#the-uniform-distribution-2" id="toc-the-uniform-distribution-2" class="nav-link" data-scroll-target="#the-uniform-distribution-2"><span class="header-section-number">18.9</span> The Uniform Distribution</a>
  <ul class="collapse">
  <li><a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution"><span class="header-section-number">18.9.1</span> The Normal Distribution</a></li>
  <li><a href="#visualization-of-the-standard-deviation" id="toc-visualization-of-the-standard-deviation" class="nav-link" data-scroll-target="#visualization-of-the-standard-deviation"><span class="header-section-number">18.9.2</span> Visualization of the Standard Deviation</a></li>
  <li><a href="#realizations-of-a-normal-distribution" id="toc-realizations-of-a-normal-distribution" class="nav-link" data-scroll-target="#realizations-of-a-normal-distribution"><span class="header-section-number">18.9.3</span> Realizations of a Normal Distribution</a></li>
  </ul></li>
  <li><a href="#the-normal-distribution-2" id="toc-the-normal-distribution-2" class="nav-link" data-scroll-target="#the-normal-distribution-2"><span class="header-section-number">18.10</span> The Normal Distribution</a></li>
  <li><a href="#the-exponential-distribution" id="toc-the-exponential-distribution" class="nav-link" data-scroll-target="#the-exponential-distribution"><span class="header-section-number">18.11</span> The Exponential Distribution</a>
  <ul class="collapse">
  <li><a href="#standardization-of-random-variables" id="toc-standardization-of-random-variables" class="nav-link" data-scroll-target="#standardization-of-random-variables"><span class="header-section-number">18.11.1</span> Standardization of Random Variables</a></li>
  </ul></li>
  <li><a href="#covariance-and-correlation" id="toc-covariance-and-correlation" class="nav-link" data-scroll-target="#covariance-and-correlation"><span class="header-section-number">18.12</span> Covariance and Correlation</a>
  <ul class="collapse">
  <li><a href="#the-multivariate-normal-distribution" id="toc-the-multivariate-normal-distribution" class="nav-link" data-scroll-target="#the-multivariate-normal-distribution"><span class="header-section-number">18.12.1</span> The Multivariate Normal Distribution</a></li>
  </ul></li>
  <li><a href="#covariance" id="toc-covariance" class="nav-link" data-scroll-target="#covariance"><span class="header-section-number">18.13</span> Covariance</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation"><span class="header-section-number">18.14</span> Correlation</a>
  <ul class="collapse">
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">18.14.1</span> Definitions</a></li>
  <li><a href="#computations" id="toc-computations" class="nav-link" data-scroll-target="#computations"><span class="header-section-number">18.14.2</span> Computations</a></li>
  <li><a href="#the-outer-product-and-the-np.outer-function" id="toc-the-outer-product-and-the-np.outer-function" class="nav-link" data-scroll-target="#the-outer-product-and-the-np.outer-function"><span class="header-section-number">18.14.3</span> The Outer-product and the <code>np.outer</code> Function</a></li>
  <li><a href="#correlation-and-independence" id="toc-correlation-and-independence" class="nav-link" data-scroll-target="#correlation-and-independence"><span class="header-section-number">18.14.4</span> Correlation and Independence</a></li>
  <li><a href="#pearsons-correlation" id="toc-pearsons-correlation" class="nav-link" data-scroll-target="#pearsons-correlation"><span class="header-section-number">18.14.5</span> Pearson’s Correlation</a></li>
  <li><a href="#interpreting-the-correlation-correlation-squared" id="toc-interpreting-the-correlation-correlation-squared" class="nav-link" data-scroll-target="#interpreting-the-correlation-correlation-squared"><span class="header-section-number">18.14.6</span> Interpreting the Correlation: Correlation Squared</a></li>
  <li><a href="#partial-correlation" id="toc-partial-correlation" class="nav-link" data-scroll-target="#partial-correlation"><span class="header-section-number">18.14.7</span> Partial Correlation</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing-and-the-null-hypothesis" id="toc-hypothesis-testing-and-the-null-hypothesis" class="nav-link" data-scroll-target="#hypothesis-testing-and-the-null-hypothesis"><span class="header-section-number">18.15</span> Hypothesis Testing and the Null-Hypothesis</a>
  <ul class="collapse">
  <li><a href="#alternative-hypotheses-main-ideas" id="toc-alternative-hypotheses-main-ideas" class="nav-link" data-scroll-target="#alternative-hypotheses-main-ideas"><span class="header-section-number">18.15.1</span> Alternative Hypotheses, Main Ideas</a></li>
  <li><a href="#p-values-what-they-are-and-how-to-interpret-them" id="toc-p-values-what-they-are-and-how-to-interpret-them" class="nav-link" data-scroll-target="#p-values-what-they-are-and-how-to-interpret-them"><span class="header-section-number">18.15.2</span> p-values: What they are and how to interpret them</a></li>
  </ul></li>
  <li><a href="#statistical-power" id="toc-statistical-power" class="nav-link" data-scroll-target="#statistical-power"><span class="header-section-number">18.16</span> Statistical Power</a></li>
  <li><a href="#the-central-limit-theorem" id="toc-the-central-limit-theorem" class="nav-link" data-scroll-target="#the-central-limit-theorem"><span class="header-section-number">18.17</span> The Central Limit Theorem</a></li>
  <li><a href="#maximum-likelihood" id="toc-maximum-likelihood" class="nav-link" data-scroll-target="#maximum-likelihood"><span class="header-section-number">18.18</span> Maximum Likelihood</a></li>
  <li><a href="#maximum-likelihood-estimation-multivariate-normal-distribution" id="toc-maximum-likelihood-estimation-multivariate-normal-distribution" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-multivariate-normal-distribution"><span class="header-section-number">18.19</span> Maximum Likelihood Estimation: Multivariate Normal Distribution</a>
  <ul class="collapse">
  <li><a href="#the-joint-probability-density-function-of-the-multivariate-normal-distribution" id="toc-the-joint-probability-density-function-of-the-multivariate-normal-distribution" class="nav-link" data-scroll-target="#the-joint-probability-density-function-of-the-multivariate-normal-distribution"><span class="header-section-number">18.19.1</span> The Joint Probability Density Function of the Multivariate Normal Distribution</a></li>
  <li><a href="#the-log-likelihood-function" id="toc-the-log-likelihood-function" class="nav-link" data-scroll-target="#the-log-likelihood-function"><span class="header-section-number">18.19.2</span> The Log-Likelihood Function</a></li>
  </ul></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">18.20</span> Cross-Validation</a></li>
  <li><a href="#mutual-information" id="toc-mutual-information" class="nav-link" data-scroll-target="#mutual-information"><span class="header-section-number">18.21</span> Mutual Information</a></li>
  <li><a href="#principal-component-analysis-pca" id="toc-principal-component-analysis-pca" class="nav-link" data-scroll-target="#principal-component-analysis-pca"><span class="header-section-number">18.22</span> Principal Component Analysis (PCA)</a></li>
  <li><a href="#t-sne" id="toc-t-sne" class="nav-link" data-scroll-target="#t-sne"><span class="header-section-number">18.23</span> t-SNE</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./100_ddmo_eda.html">Data-Driven Modeling and Optimization</a></li><li class="breadcrumb-item"><a href="./100_ddmo_eda.html"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Basic Statistics and Data Analysis</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Basic Statistics and Data Analysis</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter covers basic statistical concepts, namely descriptive statistics, probability distributions, and hypothesis testing. These concepts are fundamental to understanding data and making informed decisions based on data analysis. The chapter also introduces the concept of exploratory data analysis (EDA), data preprocessing (Principal Component Analysis), and data visualization techniques.</p>
<section id="exploratory-data-analysis" class="level2" data-number="18.1">
<h2 data-number="18.1" class="anchored" data-anchor-id="exploratory-data-analysis"><span class="header-section-number">18.1</span> Exploratory Data Analysis</h2>
<section id="histograms" class="level3" data-number="18.1.1">
<h3 data-number="18.1.1" class="anchored" data-anchor-id="histograms"><span class="header-section-number">18.1.1</span> Histograms</h3>
<p>Creating a histogram and calculating the probabilities from a dataset can be approached with scientific precision</p>
<ol type="1">
<li><p>Data Collection: Obtain the dataset you wish to analyze. This dataset could represent any quantitative measure, such to examine its distribution.</p></li>
<li><p>Decide on the Number of Bins: The number of bins influences the histogram’s granularity. There are several statistical rules to determine an optimal number of bins:</p>
<ul>
<li>Square-root rule: suggests using the square root of the number of data points as the number of bins.</li>
<li>Sturges’ formula: <span class="math inline">\(k = 1 + 3.322 \log_{10}(n)\)</span>, where <span class="math inline">\(n\)</span> is the number of data points and <span class="math inline">\(k\)</span> is the suggested number of bins.</li>
<li>Freedman-Diaconis rule: uses the interquartile range (IQR) and the cube root of the number of data points <span class="math inline">\(n\)</span> to calculate bin width as <span class="math inline">\(2 \dfrac{IQR}{n^{1/3}}\)</span>.</li>
</ul></li>
<li><p>Determine Range and Bin Width: Calculate the range of data by subtracting the minimum data point value from the maximum. Divide this range by the number of bins to determine the width of each bin.</p></li>
<li><p>Allocate Data Points to Bins: Iterate through the data, sorting each data point into the appropriate bin based on its value.</p></li>
<li><p>Draw the Histogram: Use a histogram to visualize the frequency or relative frequency (probability) of data points within each bin.</p></li>
<li><p>Calculate Probabilities: The relative frequency of data within each bin represents the probability of a randomly selected data point falling within that bin’s range.</p></li>
</ol>
<p>Below is a Python script that demonstrates how to generate a histogram and compute probabilities using the <code>matplotlib</code> library for visualization and <code>numpy</code> for data manipulation.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data: Randomly generated for demonstration</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)  <span class="co"># 1000 data points with a normal distribution</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Decide on the number of bins</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>num_bins <span class="op">=</span> <span class="bu">int</span>(np.ceil(<span class="dv">1</span> <span class="op">+</span> <span class="fl">3.322</span> <span class="op">*</span> np.log10(<span class="bu">len</span>(data))))  <span class="co"># Sturges' formula</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Determine range and bin width -- handled internally by matplotlib</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Steps 4 &amp; 5: Sort data into bins and draw the histogram</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>n, bins, patches <span class="op">=</span> ax.hist(data, bins<span class="op">=</span>num_bins, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.75</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate probabilities (relative frequencies) manually, if needed</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>bin_width <span class="op">=</span> np.diff(bins)  <span class="co"># np.diff finds the difference between adjacent bin boundaries</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> n <span class="op">*</span> bin_width  <span class="co"># n is already normalized to form a probability density if `density=True`</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding labels and title for clarity</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Data Value'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Histogram with Probability Density'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-histogram" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="1">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="fig-histogram-1" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="1">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-histogram-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<pre id="fig-histogram-1" class="cell-output cell-output-display" data-execution_count="1"><code>Text(0.5, 1.0, 'Histogram with Probability Density')</code></pre>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-histogram-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Histogram with Probability Density
</figcaption>
</figure>
</div>
<div class="cell-output cell-output-display">
<div id="fig-histogram-2" class="quarto-float quarto-figure quarto-figure-center anchored" height="449" width="597">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-histogram-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-histogram-output-2.png" id="fig-histogram-2" data-ref-parent="fig-histogram" width="597" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-histogram-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.1
</figcaption>
</figure>
</div>
</div>
<div id="ad4733de" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, prob <span class="kw">in</span> <span class="bu">enumerate</span>(probabilities):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Bin </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> Probability: </span><span class="sc">{</span>prob<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure probabilities sum to 1 (or very close, due to floating-point arithmetic)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sum of probabilities: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(probabilities)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bin 1 Probability: 0.0060
Bin 2 Probability: 0.0170
Bin 3 Probability: 0.0590
Bin 4 Probability: 0.1190
Bin 5 Probability: 0.1950
Bin 6 Probability: 0.2450
Bin 7 Probability: 0.1830
Bin 8 Probability: 0.1210
Bin 9 Probability: 0.0360
Bin 10 Probability: 0.0140
Bin 11 Probability: 0.0050
Sum of probabilities: 1.0</code></pre>
</div>
</div>
<p>This code segment goes through the necessary steps to generate a histogram and calculate probabilities for a synthetic dataset. It demonstrates important scientific and computational practices including binning, visualization, and probability calculation in Python.</p>
<p>Key Points:</p>
<ul>
<li><p>The histogram represents the distribution of data, with the histogram’s bins outlining the data’s spread and density.</p></li>
<li><p>The option <code>density=True</code> in <code>ax.hist()</code> normalizes the histogram so that the total area under the histogram sums to 1, thereby converting frequencies to probability densities.</p></li>
<li><p>The choice of bin number and width has a significant influence on the histogram’s shape and the insights that can be drawn from it, highlighting the importance of selecting appropriate binning strategies based on the dataset’s characteristics and the analysis objectives.</p></li>
<li><p>Video: <a href="https://youtu.be/qBigTkBLU6g">Histograms, Clearly Explained</a></p></li>
</ul>
</section>
<section id="boxplots" class="level3" data-number="18.1.2">
<h3 data-number="18.1.2" class="anchored" data-anchor-id="boxplots"><span class="header-section-number">18.1.2</span> Boxplots</h3>
<ul>
<li>Video: <a href="https://youtu.be/fHLhBnmwUM0?si=QB5ccKIxL1FaIc0M">Boxplots are Awesome</a></li>
</ul>
</section>
</section>
<section id="probability-distributions" class="level2" data-number="18.2">
<h2 data-number="18.2" class="anchored" data-anchor-id="probability-distributions"><span class="header-section-number">18.2</span> Probability Distributions</h2>
<p>What happens when we use smaller bins in a histogram? The histogram becomes more detailed, revealing the distribution of data points with greater precision. However, as the bin size decreases, the number of data points within each bin may decrease, leading to sparse or empty bins. This sparsity can make it challenging to estimate probabilities accurately, especially for data points that fall within these empty bins.</p>
<p>Advantages, when using a probability distribution, include:</p>
<ul>
<li>Blanks can be filled</li>
<li>Probabilities can be calculated</li>
<li>Parameters are sufficient to describe the distribution, e.g., mean and variance for the normal distribution</li>
</ul>
<p>Probability distributions offer a powerful solution to the challenges posed by limited data in estimating probabilities. When data is scarce, constructing a histogram to determine the probability of certain outcomes can lead to inaccurate or unreliable results due to the lack of detail in the dataset. However, collecting vast amounts of data to populate a histogram for more precise estimates can often be impractical, time-consuming, and expensive.</p>
<p>A probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes for an experiment. It is a more efficient approach to understanding the likelihood of various outcomes than relying solely on extensive data collection. For continuous data, this is often represented graphically by a smooth curve.</p>
<ul>
<li>Video: <a href="https://youtu.be/oI3hZJqXJuc">The Main Ideas behind Probability Distributions</a></li>
</ul>
<section id="sampling-from-a-distribution" class="level3" data-number="18.2.1">
<h3 data-number="18.2.1" class="anchored" data-anchor-id="sampling-from-a-distribution"><span class="header-section-number">18.2.1</span> Sampling from a Distribution</h3>
<ul>
<li>Video: <a href="https://youtu.be/XLCWeSVzHUU">Sampling from a Distribution, Clearly Explained!!!</a></li>
</ul>
</section>
</section>
<section id="discrete-distributions" class="level2" data-number="18.3">
<h2 data-number="18.3" class="anchored" data-anchor-id="discrete-distributions"><span class="header-section-number">18.3</span> Discrete Distributions</h2>
<p>Discrete probability distributions are essential tools in statistics, providing a mathematical foundation to model and analyze situations with discrete outcomes. Histograms, which can be seen as discrete distributions with data organized into bins, offer a way to visualize and estimate probabilities based on the collected data. However, they come with limitations, especially when data is scarce or when we encounter gaps in the data (blank spaces in histograms). These gaps can make it challenging to accurately estimate probabilities.</p>
<p>A more efficient approach, especially for discrete data, is to use mathematical equations—particularly those defining discrete probability distributions—to calculate probabilities directly, thus bypassing the intricacies of data collection and histogram interpretation.</p>
<section id="bernoulli-distribution" class="level3" data-number="18.3.1">
<h3 data-number="18.3.1" class="anchored" data-anchor-id="bernoulli-distribution"><span class="header-section-number">18.3.1</span> Bernoulli Distribution</h3>
<p>The Bernoulli distribution, named after Swiss scientist Jacob Bernoulli, is a discrete probability distribution, which takes value <span class="math inline">\(1\)</span> with success probability <span class="math inline">\(p\)</span> and value <span class="math inline">\(0\)</span> with failure probability <span class="math inline">\(q = 1-p\)</span>. So if <span class="math inline">\(X\)</span> is a random variable with this distribution, we have: <span class="math display">\[
P(X=1) = 1-P(X=0) = p = 1-q.
\]</span></p>
</section>
<section id="binomial-distribution" class="level3" data-number="18.3.2">
<h3 data-number="18.3.2" class="anchored" data-anchor-id="binomial-distribution"><span class="header-section-number">18.3.2</span> Binomial Distribution</h3>
<p>The Binomial Distribution is a prime example of a discrete probability distribution that is particularly useful for binary outcomes (e.g., success/failure, yes/no, pumpkin pie/blueberry pie). It leverages simple mathematical principles to calculate the probability of observing a specific number of successes (preferred outcomes) in a fixed number of trials, given the probability of success in each trial.</p>
<div id="exm-binom" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.1 (Pie Preference)</strong></span> Consider a scenario from “StatLand” where 70% of people prefer pumpkin pie over blueberry pie. The question is: What is the probability that, out of three people asked, the first two prefer pumpkin pie and the third prefers blueberry pie?</p>
<p>Using the concept of the Binomial Distribution, the probability of such an outcome can be calculated without the need to layout every possible combination by hand. This process not only simplifies calculations but also provides a clear and precise method to determine probabilities in scenarios involving discrete choices. We will use Python to calculate the probability of observing exactly two out of three people prefer pumpkin pie, given the 70% preference rate:</p>
<div id="7653af01" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Number of trials (people asked)</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.7</span>  <span class="co"># Probability of success (preferring pumpkin pie)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Number of successes (people preferring pumpkin pie)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability calculation using Binomial Distribution</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> binom.pmf(x, n, p)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The probability that exactly 2 out of 3 people prefer pumpkin pie is: </span><span class="sc">{</span>prob<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The probability that exactly 2 out of 3 people prefer pumpkin pie is: 0.441</code></pre>
</div>
</div>
<p>This code uses the <code>binom.pmf()</code> function from <code>scipy.stats</code> to calculate the probability mass function (PMF) of observing exactly <code>x</code> successes in <code>n</code> trials, where each trial has a success probability of <code>p</code>.</p>
</div>
<p>A Binomial random variable is the sum of <span class="math inline">\(n\)</span> independent, identically distributed Bernoulli random variables, each with probability <span class="math inline">\(p\)</span> of success. We may indicate a random variable <span class="math inline">\(X\)</span> with Bernoulli distribution using the notation <span class="math inline">\(X \sim \mathrm{Bi}(1,\theta)\)</span>. Then, the notation for the Binomial is <span class="math inline">\(X \sim \mathrm{Bi}(n,\theta)\)</span>. Its probability and distribution functions are, respectively, <span class="math display">\[
p_X(x) = {n\choose x}\theta^x(1-\theta)^{n-x}, \qquad F_X(x) = \Pr\{X \le x\} = \sum_{i=0}^{x} {n\choose i}\theta^i(1-\theta)^{n-i}.
\]</span></p>
<p>The mean of the binomial distribution is <span class="math inline">\(\text{E}[X] = n\theta\)</span>. The variance of the distribution is <span class="math inline">\(\text{Var}[X] = n\theta(1-\theta)\)</span> (see next section).</p>
<p>A process consists of a sequence of <span class="math inline">\(n\)</span> independent trials, i.e., the outcome of each trial does not depend on the outcome of previous trials. The outcome of each trial is either a success or a failure. The probability of success is denoted as <span class="math inline">\(p\)</span>, and <span class="math inline">\(p\)</span> is constant for each trial. Coin tossing is a classical example for this setting.</p>
<p>The binomial distribution is a statistical distribution giving the probability of obtaining a specified number of successes in a binomial experiment; written Binomial(n, p), where <span class="math inline">\(n\)</span> is the number of trials, and <span class="math inline">\(p\)</span> the probability of success in each.</p>
<div id="def-binom" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.1 (Binomial Distribution)</strong></span> The binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, where <span class="math inline">\(n\)</span> is the number of trials, and <span class="math inline">\(p\)</span> the probability of success in each, is <span class="math display">\[\begin{equation}
p(x) = { n \choose k } p^x(1-p)^{n-x} \qquad x = 0,1, \ldots, n.
\end{equation}\]</span> The mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span> of the binomial distribution are <span class="math display">\[\begin{equation}
\mu = np
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
\sigma^2 = np(1-p).
\end{equation}\]</span></p>
</div>
<p>Note, the Bernoulli distribution is simply Binomial(1,p).</p>
</section>
</section>
<section id="continuous-distributions" class="level2" data-number="18.4">
<h2 data-number="18.4" class="anchored" data-anchor-id="continuous-distributions"><span class="header-section-number">18.4</span> Continuous Distributions</h2>
<p>Our considerations regarding probability distributions, expectations, and standard deviations will be extended from discrete distributions to continuous distributions. One simple example of a continuous distribution is the uniform distribution. Continuous distributions are defined by probability density functions.</p>
<section id="distribution-functions-pdfs-and-cdfs" class="level3" data-number="18.4.1">
<h3 data-number="18.4.1" class="anchored" data-anchor-id="distribution-functions-pdfs-and-cdfs"><span class="header-section-number">18.4.1</span> Distribution functions: PDFs and CDFs</h3>
<p>The density for a continuous distribution is a measure of the relative probability of “getting a value close to <span class="math inline">\(x\)</span>.” Probability density functions <span class="math inline">\(f\)</span> and cumulative distribution function <span class="math inline">\(F\)</span> are related as follows. <span class="math display">\[\begin{equation}
f(x) = \frac{d}{dx} F(x)
\end{equation}\]</span></p>
</section>
</section>
<section id="background-expectation-mean-standard-deviation" class="level2" data-number="18.5">
<h2 data-number="18.5" class="anchored" data-anchor-id="background-expectation-mean-standard-deviation"><span class="header-section-number">18.5</span> Background: Expectation, Mean, Standard Deviation</h2>
<p>The distribution of a random vector is characterized by some indexes. These are the expectation, the mean, and the standard deviation. The expectation is a measure of the central tendency of a random variable, while the standard deviation quantifies the spread of the distribution. These indexes are essential for understanding the behavior of random variables and making predictions based on them.</p>
<div id="def-random-variable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.2 (Random Variable)</strong></span> A random variable <span class="math inline">\(X\)</span> is a mapping from the sample space of a random experiment to the real numbers. It assigns a numerical value to each outcome of the experiment. Random variables can be either:</p>
<ul>
<li>Discrete: If <span class="math inline">\(X\)</span> takes on a countable number of distinct values.</li>
<li>Continuous: If <span class="math inline">\(X\)</span> takes on an uncountable number of values.</li>
</ul>
<p>Mathematically, a random variable is a function <span class="math inline">\(X: \Omega \rightarrow \mathbb{R}\)</span>, where <span class="math inline">\(\Omega\)</span> is the sample space.</p>
</div>
<div id="def-probability-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.3 (Probability Distribution)</strong></span> A probability distribution describes how the values of a random variable are distributed. It is characterized for a discrete random variable <span class="math inline">\(X\)</span> by the probability mass function (PMF) <span class="math inline">\(p_X(x)\)</span> and for a continuous random variable <span class="math inline">\(X\)</span> by the probability density function (PDF) <span class="math inline">\(f_X(x)\)</span>.</p>
</div>
<div id="def-probability-mass-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.4 (Probability Mass Function (PMF))</strong></span> <span class="math inline">\(p_X(x) = P(X = x)\)</span> gives the probability that <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(x\)</span>.</p>
</div>
<div id="def-probability-density-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.5 (Probability Density Function (PDF):)</strong></span> <span class="math inline">\(f_X(x)\)</span> is a function such that for any interval <span class="math inline">\([a, b]\)</span>, the probability that <span class="math inline">\(X\)</span> falls within this interval is given by the integral <span class="math inline">\(\int_a^b f_X(x) \mathrm{d}x\)</span>.</p>
</div>
<p>The distribution function must satisfy: <span class="math display">\[
\sum_{x \in D_X} p_X(x) = 1
\]</span> for discrete random variables, where <span class="math inline">\(D_X\)</span> is the domain of <span class="math inline">\(X\)</span> and <span class="math display">\[
\int_{-\infty}^{\infty} f_X(x) \mathrm{d}x = 1
\]</span> for continuous random variables.</p>
<p>With these definitions in place, we can now introduce the definition of the expectation, which is a fundamental measure of the central tendency of a random variable.</p>
<div id="def-expectation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.6 (Expectation)</strong></span> The expectation or expected value of a random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(E[X]\)</span>, is defined as follows:</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>: <span class="math display">\[
E[X] = \sum_{x \in D_X} x p_X(x) \quad \text{if $X$ is discrete}.
\]</span></p>
<p>For a continuous random variable <span class="math inline">\(X\)</span>: <span class="math display">\[
E[X] = \int_{x \in D_X} x f_X(x) \mathrm{d}x \quad \text{if $X$ is continuous.}
\]</span></p>
</div>
<p>The mean, <span class="math inline">\(\mu\)</span>, of a probability distribution is a measure of its central tendency or location. That is, <span class="math inline">\(E(X)\)</span> is defined as the average of all possible values of <span class="math inline">\(X\)</span>, weighted by their probabilities.</p>
<div id="exm-expectation" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.2 (Expectation)</strong></span> Let <span class="math inline">\(X\)</span> denote the number produced by rolling a fair die. Then <span class="math display">\[
E(X) = 1 \times 1/6 + 2 \times 1/6 + 3 \times 1/6 + 4 \times 1/6 + 5 \times 1/6 + 6\times 1/6 = 3.5.
\]</span></p>
</div>
<div id="def-sample-mean" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.7 (Sample Mean)</strong></span> The sample mean is an important estimate of the population mean. The sample mean of a sample <span class="math inline">\(\{x_i\}\)</span> (<span class="math inline">\(i=1,2,\ldots,n\)</span>) is defined as <span class="math display">\[
\overline{x}  = \frac{1}{n} \sum_i x_i.
\]</span></p>
</div>
<p>While both the expectation of a random variable and the sample mean provide measures of central tendency, they differ in their context, calculation, and interpretation.</p>
<ul>
<li>The expectation is a theoretical measure that characterizes the average value of a random variable over an infinite number of repetitions of an experiment. The expectation is calculated using a probability distribution and provides a parameter of the entire population or distribution. It reflects the long-term average or central value of the outcomes generated by the random process.</li>
<li>The sample mean is a statistic. It provides an estimate of the population mean based on a finite sample of data. It is computed directly from the data sample, and its value can vary between different samples from the same population. It serves as an approximation or estimate of the population mean. It is used in statistical inference to make conclusions about the population mean based on sample data.</li>
</ul>
<p>If we are trying to predict the value of a random variable <span class="math inline">\(X\)</span> by its mean <span class="math inline">\(\mu = E(X)\)</span>, the error will be <span class="math inline">\(X-\mu\)</span>. In many situations it is useful to have an idea how large this deviation or error is. Since <span class="math inline">\(E(X-\mu) = E(X) -\mu = 0\)</span>, it is necessary to use the absolute value or the square of (<span class="math inline">\(X-\mu\)</span>). The squared error is the first choice, because the derivatives are easier to calculate. These considerations motivate the definition of the variance:</p>
<div id="def-variance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.8 (Variance)</strong></span> The variance of a random variable <span class="math inline">\(X\)</span> is the mean squared deviation of <span class="math inline">\(X\)</span> from its expected value <span class="math inline">\(\mu = E(X)\)</span>. <span class="math display">\[\begin{equation}
Var(X) = E[ (X-\mu)^2].
\end{equation}\]</span></p>
</div>
<p>The variance is a measure of the spread of a distribution. It quantifies how much the values of a random variable differ from the mean. A high variance indicates that the values are spread out over a wide range, while a low variance indicates that the values are clustered closely around the mean.</p>
<div id="def-standard-deviation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.9 (Standard Deviation)</strong></span> Taking the square root of the variance to get back to the same scale of units as <span class="math inline">\(X\)</span> gives the standard deviation. The standard deviation of <span class="math inline">\(X\)</span> is the square root of the variance of <span class="math inline">\(X\)</span>. <span class="math display">\[\begin{equation}
sd(X) = \sqrt{Var(X)}.
\end{equation}\]</span></p>
</div>
</section>
<section id="distributions-and-random-numbers-in-python" class="level2" data-number="18.6">
<h2 data-number="18.6" class="anchored" data-anchor-id="distributions-and-random-numbers-in-python"><span class="header-section-number">18.6</span> Distributions and Random Numbers in Python</h2>
<p>Results from computers are deterministic, so it sounds like a contradiction in terms to generate random numbers on a computer. Standard computers generate pseudo-randomnumbers, i.e., numbers that behave as if they were drawn randomly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deterministic Random Numbers
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Idea: Generate deterministically numbers that <strong>look</strong> (behave) as if they were drawn randomly.</li>
</ul>
</div>
</div>
<section id="calculation-of-the-standard-deviation-with-python" class="level3" data-number="18.6.1">
<h3 data-number="18.6.1" class="anchored" data-anchor-id="calculation-of-the-standard-deviation-with-python"><span class="header-section-number">18.6.1</span> Calculation of the Standard Deviation with Python</h3>
<p>The function <code>numpy.std</code> returns the standard deviation, a measure of the spread of a distribution, of the array elements. The argument <code>ddof</code> specifies the Delta Degrees of Freedom. The divisor used in calculations is <code>N - ddof</code>, where <code>N</code> represents the number of elements. By default <code>ddof</code> is zero, i.e., <code>std</code> uses the formula <span class="math display">\[
\sqrt{  \frac{1}{N} \sum_i \left( x_i - \bar{x} \right)^2  } \qquad \text{with } \quad \bar{x} = \sum_{i=1}^N x_i /N.
\]</span></p>
<div id="exm-std-python" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.3 (Standard Deviation with Python)</strong></span> Consider the array <span class="math inline">\([1,2,3]\)</span>: Since <span class="math inline">\(\bar{x} = 2\)</span>, the following value is computed: <span class="math display">\[ \sqrt{1/3 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/3}.\]</span></p>
<div id="87329e23" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>np.std(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>np.float64(0.816496580927726)</code></pre>
</div>
</div>
</div>
<p>The empirical standard deviation (which uses <span class="math inline">\(N-1\)</span>), <span class="math inline">\(\sqrt{1/2 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/2}\)</span>, can be calculated in Python as follows:</p>
<div id="7d33da84" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>np.std(a, ddof<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>np.float64(1.0)</code></pre>
</div>
</div>
</section>
<section id="the-argument-axis" class="level3" data-number="18.6.2">
<h3 data-number="18.6.2" class="anchored" data-anchor-id="the-argument-axis"><span class="header-section-number">18.6.2</span> The Argument “axis”</h3>
<p>When you compute <code>np.std</code> with <code>axis=0</code>, it calculates the standard deviation along the vertical axis, meaning it computes the standard deviation for each column of the array. On the other hand, when you compute <code>np.std</code> with <code>axis=1</code>, it calculates the standard deviation along the horizontal axis, meaning it computes the standard deviation for each row of the array. If the axis parameter is not specified, <code>np.std</code> computes the standard deviation of the flattened array, i.e., it calculates the standard deviation of all the elements in the array.</p>
<div id="exm-std-axis" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.4 (Axes along which the standard deviation is computed)</strong></span> &nbsp;</p>
<div id="9323af9b" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([[1, 2],
       [3, 4]])</code></pre>
</div>
</div>
<p>First, we calculate the standard deviation of all elements in the array:</p>
<div id="49d27f8b" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>np.std(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>np.float64(1.118033988749895)</code></pre>
</div>
</div>
<p>Setting <code>axis=0</code> calculates the standard deviation along the vertical axis (column-wise):</p>
<div id="a668a82e" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>np.std(A, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([1., 1.])</code></pre>
</div>
</div>
<p>Finally, setting <code>axis=1</code> calculates the standard deviation along the horizontal axis (row-wise):</p>
<div id="4e593dd3" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>np.std(A, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([0.5, 0.5])</code></pre>
</div>
</div>
</div>
</section>
</section>
<section id="expectation-continuous" class="level2" data-number="18.7">
<h2 data-number="18.7" class="anchored" data-anchor-id="expectation-continuous"><span class="header-section-number">18.7</span> Expectation (Continuous)</h2>
<div id="def-expectation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.10 (Expectation (Continuous))</strong></span> <span class="math display">\[\begin{equation}
  \text{E}(X) = \int_{-\infty}^\infty x f(x) \, dx
  \end{equation}\]</span></p>
</div>
</section>
<section id="variance-and-standard-deviation-continuous" class="level2" data-number="18.8">
<h2 data-number="18.8" class="anchored" data-anchor-id="variance-and-standard-deviation-continuous"><span class="header-section-number">18.8</span> Variance and Standard Deviation (Continuous)</h2>
<div id="def-variance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.11 (Variance (Continuous))</strong></span> Variance can be calculated with <span class="math inline">\(\text{E}(X)\)</span> and <span class="math display">\[\begin{equation}
  \text{E}(X^2) = \int_{-\infty}^\infty x^2 f(x) \, dx
\end{equation}\]</span> as <span class="math display">\[\begin{equation*}
  \text{Var}(X) = \text{E}(X^2) - [ E(X)]^2.
  \end{equation*}\]</span> <span class="math inline">\(\Box\)</span></p>
</div>
<div id="def-standard-deviation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.12 (Standard Deviation (Continuous))</strong></span> Standard deviation can be calculated as <span class="math display">\[\begin{equation*}
  \text{sd}(X) = \sqrt{\text{Var}(X)}.
  \end{equation*}\]</span> <span class="math inline">\(\Box\)</span></p>
</div>
<ul>
<li>Video: <a href="https://youtu.be/vikkiwjQqfU">Population and Estimated Parameters, Clearly Explained</a></li>
<li>Video: <a href="https://youtu.be/SzZ6GpcfoQY">Calculating the Mean, Variance, and Standard Deviation</a></li>
</ul>
<section id="the-uniform-distribution" class="level3" data-number="18.8.1">
<h3 data-number="18.8.1" class="anchored" data-anchor-id="the-uniform-distribution"><span class="header-section-number">18.8.1</span> The Uniform Distribution</h3>
<div id="def-uniform-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.13 (The Uniform Distribution)</strong></span> The probability density function of the uniform distribution is defined as: <span class="math display">\[
f_X(x) = \frac{1}{b-a} \qquad \text{for $x \in [a,b]$}.
\]</span></p>
</div>
<p>Generate 10 random numbers from a uniform distribution between <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=1\)</span>:</p>
<div id="8afb0e72" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the random number generator</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">123456789</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.uniform(low<span class="op">=</span><span class="fl">0.0</span>, high<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>n)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([0.02771274, 0.90670006, 0.88139355, 0.62489728, 0.79071481,
       0.82590801, 0.84170584, 0.47172795, 0.95722878, 0.94659153])</code></pre>
</div>
</div>
<p>Generate 10,000 random numbers from a uniform distribution between 0 and 10 and plot a histogram of the numbers:</p>
<div id="3eca23f6" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the random number generator</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">123456789</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers from a uniform distribution</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.uniform(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a histogram of the numbers</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>plt.hist(x, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Uniform Distribution [0,10]'</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="100_ddmo_eda_files/figure-html/cell-12-output-1.png" width="597" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="the-uniform-distribution-2" class="level2" data-number="18.9">
<h2 data-number="18.9" class="anchored" data-anchor-id="the-uniform-distribution-2"><span class="header-section-number">18.9</span> The Uniform Distribution</h2>
<p>This variable is defined in the interval <span class="math inline">\([a,b]\)</span>. We write it as <span class="math inline">\(X \sim U[a,b]\)</span>. Its density and cumulative distribution functions are, respectively, <span class="math display">\[
f_X(x) = \frac{I_{[a,b]}(x)}{b-a},  \quad\quad F_X(x) = \frac{1}{b-a}\int\limits_{-\infty}\limits^x I_{[a,b]}(t) \mathrm{d}t = \frac{x-a}{b-a},
\]</span> where <span class="math inline">\(I_{[a,b]}(\cdot)\)</span> is the indicator function of the interval <span class="math inline">\([a,b]\)</span>. Note that, if we set <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=1\)</span>, we obtain <span class="math inline">\(F_X(x) = x\)</span>, <span class="math inline">\(x\)</span> <span class="math inline">\(\in\)</span> <span class="math inline">\([0,1]\)</span>.</p>
<p>A typical example is the following: the cdf of a continuous r.v. is uniformly distributed in <span class="math inline">\([0,1]\)</span>. The proof of this statement is as follows: For <span class="math inline">\(u\)</span> <span class="math inline">\(\in\)</span> <span class="math inline">\([0,1]\)</span>, we have <span class="math display">\[\begin{eqnarray*}
\Pr\{F_X(X) \leq u\} &amp;=&amp; \Pr\{F_X^{-1}(F_X(X)) \leq F_X^{-1}(u)\} = \Pr\{X \leq F_X^{-1}(u)\} \\
                      &amp;=&amp; F_X(F_X^{-1}(u)) = u.     
\end{eqnarray*}\]</span> This means that, when <span class="math inline">\(X\)</span> is continuous, there is a one-to-one relationship (given by the cdf) between <span class="math inline">\(x\)</span> <span class="math inline">\(\in\)</span> <span class="math inline">\(D_X\)</span> and <span class="math inline">\(u\)</span> <span class="math inline">\(\in\)</span> <span class="math inline">\([0,1]\)</span>.</p>
<p>The has a constant density over a specified interval, say <span class="math inline">\([a,b]\)</span>. The uniform <span class="math inline">\(U(a,b)\)</span> distribution has density <span class="math display">\[\begin{equation}
f(x) =
\left\{
  \begin{array}{ll}
  1/(b-a) &amp; \textrm{ if } a &lt; x &lt; b,\\
  0 &amp; \textrm{ otherwise}
  \end{array}
  \right.
  \end{equation}\]</span></p>
<section id="the-normal-distribution" class="level3" data-number="18.9.1">
<h3 data-number="18.9.1" class="anchored" data-anchor-id="the-normal-distribution"><span class="header-section-number">18.9.1</span> The Normal Distribution</h3>
<p>A normally distributed random variable is a random variable whose associated probability distribution is the normal (or Gaussian) distribution. The normal distribution is a continuous probability distribution characterized by a symmetric bell-shaped curve.</p>
<p>The distribution is defined by two parameters: the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. The mean indicates the center of the distribution, while the standard deviation measures the spread or dispersion of the distribution.</p>
<p>This distribution is widely used in statistics and the natural and social sciences as a simple model for random variables with unknown distributions.</p>
<div id="def-normal-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.14 (The Normal Distribution)</strong></span> The probability density function of the normal distribution is defined as: <span id="eq-normal-one"><span class="math display">\[
f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2\right),
\tag{18.1}\]</span></span> where: <span class="math inline">\(\mu\)</span> is the mean; <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
</div>
<p>To generate ten random numbers from a normal distribution, the following command can be used.</p>
<div id="cell-gen-normal-10" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng()</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">2</span>, <span class="fl">0.1</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rng.normal(mu, sigma, n)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="gen-normal-10" class="cell-output cell-output-display" data-execution_count="12">
<pre><code>array([2.05223464, 2.15531788, 2.07488744, 1.98054396, 2.10056962,
       1.93558753, 2.05133421, 1.96010163, 2.15573863, 1.92093907])</code></pre>
</div>
</div>
<p>Verify the mean:</p>
<div id="b3f70249" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(mu <span class="op">-</span> np.mean(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>np.float64(0.038725459897006154)</code></pre>
</div>
</div>
<p>Note: To verify the standard deviation, we use <code>ddof = 1</code> (empirical standard deviation):</p>
<div id="80a07f01" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">abs</span>(sigma <span class="op">-</span> np.std(x, ddof<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>np.float64(0.01382343605435192)</code></pre>
</div>
</div>
<div id="ac40a915" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plot_normal_distribution(mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">1</span>, num_samples<span class="op">=</span><span class="dv">10000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="100_ddmo_eda_files/figure-html/cell-17-output-1.png" width="579" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="visualization-of-the-standard-deviation" class="level3" data-number="18.9.2">
<h3 data-number="18.9.2" class="anchored" data-anchor-id="visualization-of-the-standard-deviation"><span class="header-section-number">18.9.2</span> Visualization of the Standard Deviation</h3>
<p>The standard deviation of normal distributed can be visualized in terms of the histogram of <span class="math inline">\(X\)</span>:</p>
<ul>
<li>about 68% of the values will lie in the interval within one standard deviation of the mean</li>
<li>95% lie within two standard deviation of the mean</li>
<li>and 99.9% lie within 3 standard deviations of the mean.</li>
</ul>
<div id="47e1e20c" class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="100_ddmo_eda_files/figure-html/cell-18-output-1.png" width="662" height="469" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="realizations-of-a-normal-distribution" class="level3" data-number="18.9.3">
<h3 data-number="18.9.3" class="anchored" data-anchor-id="realizations-of-a-normal-distribution"><span class="header-section-number">18.9.3</span> Realizations of a Normal Distribution</h3>
<p>Realizations of a normal distribution refers to the actual values that you get when you draw samples from a normal distribution. Each sample drawn from the distribution is a realization of that distribution.</p>
<div id="exm-realizations" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.5 (Realizations of a Normal Distribution)</strong></span> If you have a normal distribution with a mean of 0 and a standard deviation of 1, each number you draw from that distribution is a realization. Here is a Python example that generates 10 realizations of a normal distribution with a mean of 0 and a standard deviation of 1:</p>
<div id="2d959e8f" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>realizations <span class="op">=</span> np.random.normal(mu, sigma, <span class="dv">10</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(realizations)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 0.48951662  0.23879586 -0.44811181 -0.610795   -2.02994507  0.60794659
 -0.35410888  0.15258149  0.50127485 -0.78640277]</code></pre>
</div>
</div>
<p>In this code, <code>np.random.normal</code> generates ten realizations of a normal distribution with a mean of 0 and a standard deviation of 1. The realizations array contains the actual values drawn from the distribution.</p>
</div>
</section>
</section>
<section id="the-normal-distribution-2" class="level2" data-number="18.10">
<h2 data-number="18.10" class="anchored" data-anchor-id="the-normal-distribution-2"><span class="header-section-number">18.10</span> The Normal Distribution</h2>
<p>A commonly encountered probability distribution is the normal distribution, known for its characteristic bell-shaped curve. This curve represents how the values of a variable are distributed: most of the observations cluster around the mean (or center) of the distribution, with frequencies gradually decreasing as values move away from the mean.</p>
<p>The normal distribution is particularly useful because of its defined mathematical properties. It is determined entirely by its mean (mu, <span class="math inline">\(\mu\)</span>) and its standard deviation (sigma, <span class="math inline">\(\sigma\)</span>). The area under the curve represents probability, making it possible to calculate the likelihood of a random variable falling within a specific range.</p>
<ul>
<li>Video: <a href="https://youtu.be/rzFX5NWojp0">The Normal Distribution, Clearly Explained!!!</a></li>
</ul>
<div id="exm-estimat-prob" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.6 (Normal Distribution: Estimating Probabilities)</strong></span> Consider we are interested in the heights of adults in a population. Instead of measuring the height of every adult (which would be impractical), we can use the normal distribution to estimate the probability of adults’ heights falling within certain intervals, assuming we know the mean and standard deviation of the heights.</p>
<div id="cell-fig-normal-distribution" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">170</span>  <span class="co"># e.g., mu height of adults in cm</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> <span class="dv">10</span>  <span class="co"># e.g., standard deviation of heights in cm</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>heights <span class="op">=</span> np.linspace(mu <span class="op">-</span> <span class="dv">3</span><span class="op">*</span>sd, mu <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>sd, <span class="dv">1000</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the probability density function for the normal distribution</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> norm.pdf(heights, mu, sd)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the normal distribution curve</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>plt.plot(heights, pdf, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>plt.fill_between(heights, pdf, where<span class="op">=</span>(heights <span class="op">&gt;=</span> mu <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> sd) <span class="op">&amp;</span> (heights <span class="op">&lt;=</span> mu <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>sd), color<span class="op">=</span><span class="st">'grey'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Height (cm)'</span>)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-normal-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-normal-distribution-output-1.png" width="606" height="429" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.2: Normal Distribution Curve with Highlighted Probability Area. 95 percent of the data falls within two standard deviations of the mean.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This Python code snippet generates a plot of the normal distribution for adult heights, with a mean of 170 cm and a standard deviation of 10 cm. It visually approximates a histogram with a blue bell-shaped curve, and highlights (in grey) the area under the curve between <span class="math inline">\(\mu \pm 2 \times \sigma\)</span>. This area corresponds to the probability of randomly selecting an individual whose height falls within this range.</p>
<p>By using the area under the curve, we can efficiently estimate probabilities without needing to collect and analyze a vast amount of data. This method not only saves time and resources but also provides a clear and intuitive way to understand and communicate statistical probabilities.</p>
</div>
<div id="def-normal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.15 (Normal Distribution)</strong></span> This variable is defined on the support <span class="math inline">\(D_X = \mathbb{R}\)</span> and its density function is given by <span class="math display">\[
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left \{-\frac{1}{2\sigma^2}(x-\mu)^2 \right \}.
\]</span> The density function is identified by the pair of parameters <span class="math inline">\((\mu,\sigma^2)\)</span>, where <span class="math inline">\(\mu\)</span> <span class="math inline">\(\in\)</span> <span class="math inline">\(\mathbb{R}\)</span> is the mean (or location parameter) and <span class="math inline">\(\sigma^2 &gt; 0\)</span> is the variance (or dispersion parameter) of <span class="math inline">\(X\)</span>. <span class="math inline">\(\Box\)</span></p>
</div>
<p>The density function is symmetric around <span class="math inline">\(\mu\)</span>. The normal distribution belongs to the location-scale family distributions. This means that, if <span class="math inline">\(Z \sim N(0,1)\)</span> (read, <span class="math inline">\(Z\)</span> has a standard normal distribution; i.e., with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>), and we consider the linear transformation <span class="math inline">\(X = \mu + \sigma Z\)</span>, then <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span> (read, <span class="math inline">\(X\)</span> has a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>). This means that one can obtain the probability of any interval <span class="math inline">\((-\infty,x]\)</span>, <span class="math inline">\(x\)</span> <span class="math inline">\(\in\)</span> <span class="math inline">\(R\)</span> for any normal distribution (i.e., for any pair of the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>) once the quantiles of the standard normal distribution are known. Indeed <span class="math display">\[\begin{eqnarray*}
F_X(x) &amp;=&amp; \Pr\left\{X \leq x \right\} = \Pr\left\{\frac{X-\mu}{\sigma} \leq \frac{x-\mu}{\sigma} \right\} \\
           &amp;=&amp; \Pr\left\{Z \leq \frac{x-\mu}{\sigma}\right\}  = F_Z\left(\frac{x-\mu}{\sigma}\right)    \qquad x \in \mathbb{R}.
\end{eqnarray*}\]</span> The quantiles of the standard normal distribution are available in any statistical program. The density and cumulative distribution function of the standard normal r.v.~at point <span class="math inline">\(x\)</span> are usually denoted by the symbols <span class="math inline">\(\phi(x)\)</span> and <span class="math inline">\(\Phi(x)\)</span>.</p>
<p>The standard normal distribution is based on the <span id="eq-standardization"><span class="math display">\[
\varphi(z) = \frac{1}{\sqrt{2\pi}} \exp \left(- \frac{z^2}{2} \right).
\tag{18.2}\]</span></span></p>
<p>An important application of the standardization introduced in <a href="#eq-standardization" class="quarto-xref">Equation&nbsp;<span>18.2</span></a> reads as follows. In case the distribution of <span class="math inline">\(X\)</span> is approximately normal, the distribution of X^{*} is approximately standard normal. That is <span class="math display">\[\begin{equation*}
  P(X\leq b) = P( \frac{X-\mu}{\sigma} \leq \frac{b-\mu}{\sigma}) = P(X^{*} \leq \frac{b-\mu}{\sigma})
\end{equation*}\]</span> The probability <span class="math inline">\(P(X\leq b)\)</span> can be approximated by <span class="math inline">\(\Phi(\frac{b-\mu}{\sigma})\)</span>, where <span class="math inline">\(\Phi\)</span> is the standard normal cumulative distribution function.</p>
<p>If <span class="math inline">\(X\)</span> is a normal random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, i.e., <span class="math inline">\(X \sim \cal{N} (\mu, \sigma^2)\)</span>, then <span class="math display">\[\begin{equation}
  X = \mu + \sigma Z \textrm{ where } Z \sim \cal{N}(0,1).
  \end{equation}\]</span></p>
<p>If <span class="math inline">\(Z \sim \cal{N}(0,1)\)</span> and <span class="math inline">\(X\sim \cal{N}(\mu, \sigma^2)\)</span>, then <span class="math display">\[\begin{equation*}
  X = \mu + \sigma Z.
\end{equation*}\]</span></p>
<p>The probability of getting a value in a particular interval is the area under the corresponding part of the curve. Consider the density function of the normal distribution. It can be plotted using the following commands. The result is shown in <a href="#fig-normal-density" class="quarto-xref">Figure&nbsp;<span>18.3</span></a>.</p>
<div id="cell-fig-normal-density" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="fl">0.1</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating the normal distribution's density function values for each point in x</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> norm.pdf(x, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Normal Distribution'</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-normal-density" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-density-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-normal-density-output-1.png" width="597" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-density-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.3: Normal Distribution Density Function
</figcaption>
</figure>
</div>
</div>
</div>
<p>The (CDF) describes the probability of “hitting” <span class="math inline">\(x\)</span> or less in a given distribution. We consider the CDF function of the normal distribution. It can be plotted using the following commands. The result is shown in <a href="#fig-normal-cdf" class="quarto-xref">Figure&nbsp;<span>18.4</span></a>.</p>
<div id="cell-fig-normal-cdf" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating a sequence of numbers from -4 to 4 with 0.1 intervals</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="fl">0.1</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating the cumulative distribution function value of the normal distribution for each point in x</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> norm.cdf(x, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># mean=0, stddev=1</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the results. The equivalent of 'type="l"' in R (line plot) becomes the default plot type in matplotlib.</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Normal Distribution CDF'</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cumulative Probability'</span>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-normal-cdf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-cdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-normal-cdf-output-1.png" width="589" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-cdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.4: Normal Distribution Cumulative Distribution Function
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-exponential-distribution" class="level2" data-number="18.11">
<h2 data-number="18.11" class="anchored" data-anchor-id="the-exponential-distribution"><span class="header-section-number">18.11</span> The Exponential Distribution</h2>
<p>The exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process, where events occur continuously and independently at a constant average rate. It is characterized by a single parameter, the rate parameter <span class="math inline">\(\lambda\)</span>, which represents the average number of events per unit time.</p>
<section id="standardization-of-random-variables" class="level3" data-number="18.11.1">
<h3 data-number="18.11.1" class="anchored" data-anchor-id="standardization-of-random-variables"><span class="header-section-number">18.11.1</span> Standardization of Random Variables</h3>
<p>To compare statistical properties of random variables which use different units, it is a common practice to transform these random variables into standardized variables.</p>
<div id="def-standard-units" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.16 (Standard Units)</strong></span> If a random variable <span class="math inline">\(X\)</span> has expectation <span class="math inline">\(E(X) = \mu\)</span> and standard deviation <span class="math inline">\(sd(X) = \sigma &gt;0\)</span>, the random variable <span class="math display">\[
X^{\ast} = (X-\mu)/\sigma
\]</span> is called <span class="math inline">\(X\)</span> in standard units. It has <span class="math inline">\(E(X^{\ast}) = 0\)</span> and <span class="math inline">\(sd(X^{\ast}) =1\)</span>.</p>
</div>
</section>
</section>
<section id="covariance-and-correlation" class="level2" data-number="18.12">
<h2 data-number="18.12" class="anchored" data-anchor-id="covariance-and-correlation"><span class="header-section-number">18.12</span> Covariance and Correlation</h2>
<section id="the-multivariate-normal-distribution" class="level3" data-number="18.12.1">
<h3 data-number="18.12.1" class="anchored" data-anchor-id="the-multivariate-normal-distribution"><span class="header-section-number">18.12.1</span> The Multivariate Normal Distribution</h3>
<p>The multivariate normal, multinormal, or Gaussian distribution serves as a generalization of the one-dimensional normal distribution to higher dimensions. We will consider <span class="math inline">\(k\)</span>-dimensional random vectors <span class="math inline">\(X = (X_1, X_2, \ldots, X_k)\)</span>. When drawing samples from this distribution, it results in a set of values represented as <span class="math inline">\(\{x_1, x_2, \ldots, x_k\}\)</span>. To fully define this distribution, it is necessary to specify its mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. These parameters are analogous to the mean, which represents the central location, and the variance (squared standard deviation) of the one-dimensional normal distribution introduced in <a href="#eq-normal-one" class="quarto-xref">Equation&nbsp;<span>18.1</span></a>.</p>
<div id="def-multivariate-normal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.17 (The Multivariate Normal Distribution)</strong></span> The probability density function (PDF) of the multivariate normal distribution is defined as: <span class="math display">\[
f_X(x) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp\left(-\frac{1}{2} (x-\mu)^T\Sigma^{-1} (x-\mu)\right),
\]</span> where: <span class="math inline">\(\mu\)</span> is the <span class="math inline">\(k \times 1\)</span> mean vector; <span class="math inline">\(\Sigma\)</span> is the <span class="math inline">\(k \times k\)</span> covariance matrix. The covariance matrix <span class="math inline">\(\Sigma\)</span> is assumed to be positive definite, so that its determinant is strictly positive.</p>
</div>
<p>In the context of the multivariate normal distribution, the mean takes the form of a coordinate within an <span class="math inline">\(k\)</span>-dimensional space. This coordinate represents the location where samples are most likely to be generated, akin to the peak of the bell curve in a one-dimensional or univariate normal distribution.</p>
<div id="def-covariance-2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.18 (Covariance of two random variables)</strong></span> For two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values: <span class="math display">\[
\operatorname{cov}(X, Y) = \operatorname{E}{\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]}
\]</span></p>
<p>For discrete random variables, covariance can be written as: <span class="math display">\[
\operatorname{cov} (X,Y) = \frac{1}{n}\sum_{i=1}^n (x_i-E(X)) (y_i-E(Y)).
\]</span></p>
</div>
<p>The covariance within the multivariate normal distribution denotes the extent to which two variables vary together. The elements of the covariance matrix, such as <span class="math inline">\(\Sigma_{ij}\)</span>, represent the covariances between the variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>. These covariances describe how the different variables in the distribution are related to each other in terms of their variability.</p>
<div id="exm-bivariate-normal-cov-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.7 (The Bivariate Normal Distribution with Positive Covariances)</strong></span> <a href="#fig-bi9040" class="quarto-xref">Figure&nbsp;<span>18.5</span></a> shows draws from a bivariate normal distribution with <span class="math inline">\(\mu = \begin{pmatrix}0 \\ 0\end{pmatrix}\)</span> and <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9 \end{pmatrix}\)</span>.</p>
<div id="cell-fig-bi9040" class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<div id="fig-bi9040" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9040-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-bi9040-output-1.png" width="601" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9040-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.5: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The covariance matrix of a bivariate normal distribution determines the shape, orientation, and spread of the distribution in the two-dimensional space.</p>
<p>The diagonal elements of the covariance matrix (<span class="math inline">\(\sigma_1^2\)</span>, <span class="math inline">\(\sigma_2^2\)</span>) are the variances of the individual variables. They determine the spread of the distribution along each axis. A larger variance corresponds to a greater spread along that axis.</p>
<p>The off-diagonal elements of the covariance matrix (<span class="math inline">\(\sigma_{12}, \sigma_{21}\)</span>) are the covariances between the variables. They determine the orientation and shape of the distribution. If the covariance is positive, the distribution is stretched along the line <span class="math inline">\(y=x\)</span>, indicating that the variables tend to increase together. If the covariance is negative, the distribution is stretched along the line <span class="math inline">\(y=-x\)</span>, indicating that one variable tends to decrease as the other increases. If the covariance is zero, the variables are uncorrelated and the distribution is axis-aligned.</p>
<p>In <a href="#fig-bi9040" class="quarto-xref">Figure&nbsp;<span>18.5</span></a>, the variances are identical and the variables are correlated (covariance is 4), so the distribution is stretched along the line <span class="math inline">\(y=x\)</span>.</p>
<div id="cell-fig-bi90403d" class="cell" data-execution_count="23">
<div class="cell-output cell-output-display">
<div id="fig-bi90403d" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi90403d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-bi90403d-output-1.png" width="481" height="416" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi90403d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.6: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 4 \\ 4 &amp; 9\end{pmatrix}\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-bivariate-normal-zero" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.8 (The Bivariate Normal Distribution with Mean Zero and Zero Covariances)</strong></span> The Bivariate Normal Distribution with Mean Zero and Zero Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = 0\)</span>.</p>
<p><span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 0 \\ 0 &amp; 9\end{pmatrix}\)</span></p>
<div id="cell-fig-bi9000" class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<div id="fig-bi9000" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-bi9000-output-1.png" width="590" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9000-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.7: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; 0 \\ 0 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-bivariate-normal-zero-neg" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.9 (The Bivariate Normal Distribution with Mean Zero and Negative Covariances)</strong></span> The Bivariate Normal Distribution with Mean Zero and Negative Covariances <span class="math inline">\(\sigma_{12} = \sigma_{21} = -4\)</span>.</p>
<p><span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; -4 \\ -4 &amp; 9\end{pmatrix}\)</span></p>
<div id="cell-fig-bi9449" class="cell" data-execution_count="25">
<div class="cell-output cell-output-display">
<div id="fig-bi9449" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bi9449-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="100_ddmo_eda_files/figure-html/fig-bi9449-output-1.png" width="577" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bi9449-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18.8: Bivariate Normal. Mean zero and covariance <span class="math inline">\(\Sigma=\begin{pmatrix} 9 &amp; -4 \\ -4 &amp; 9\end{pmatrix}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="covariance" class="level2" data-number="18.13">
<h2 data-number="18.13" class="anchored" data-anchor-id="covariance"><span class="header-section-number">18.13</span> Covariance</h2>
<p>In statistics, understanding the relationship between random variables is crucial for making inferences and predictions. Two common measures of such relationships are covariance and correlation. Covariance is a measure of how much two random variables change together. If the variables tend to show similar behavior (i.e., when one increases, the other tends to increase), the covariance is positive. Conversely, if they tend to move in opposite directions, the covariance is negative.</p>
<div id="def-covariance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.19 (Covariance)</strong></span> Covariance is calculated as:</p>
<p><span class="math display">\[
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
\]</span></p>
<p>Here, <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[Y]\)</span> are the expected values (means) of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively. Covariance has units that are the product of the units of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<p>For a vector of random variables <span class="math inline">\(\mathbf{Y} = \begin{pmatrix} Y^{(1)}, \ldots, Y^{(n)} \end{pmatrix}^T\)</span>, the covariance matrix <span class="math inline">\(\Sigma\)</span> encapsulates the covariances between each pair of variables:</p>
<p><span class="math display">\[
\Sigma = \text{Cov}(\mathbf{Y}, \mathbf{Y}) =
\begin{pmatrix}
\text{Var}(Y^{(1)}) &amp; \text{Cov}(Y^{(1)}, Y^{(2)}) &amp; \ldots \\
\text{Cov}(Y^{(2)}, Y^{(1)}) &amp; \text{Var}(Y^{(2)}) &amp; \ldots \\
\vdots &amp; \vdots &amp; \ddots
\end{pmatrix}
\]</span></p>
<p>The diagonal elements represent the variances, while the off-diagonal elements are the covariances.</p>
</section>
<section id="correlation" class="level2" data-number="18.14">
<h2 data-number="18.14" class="anchored" data-anchor-id="correlation"><span class="header-section-number">18.14</span> Correlation</h2>
<section id="definitions" class="level3" data-number="18.14.1">
<h3 data-number="18.14.1" class="anchored" data-anchor-id="definitions"><span class="header-section-number">18.14.1</span> Definitions</h3>
<div id="def-correlation-coefficient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.20 ((Pearson) Correlation Coefficient)</strong></span> The Pearson correlation coefficient, often denoted by <span class="math inline">\(\rho\)</span> for the population or <span class="math inline">\(r\)</span> for a sample, is calculated by dividing the covariance of two variables by the product of their standard deviations.</p>
<p><span id="eq-pears-corr"><span class="math display">\[
\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y},
\tag{18.3}\]</span></span></p>
<p>where <span class="math inline">\(\text{Cov}(X, Y)\)</span> is the covariance between variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively.</p>
</div>
<p>Correlation, specifically the correlation coefficient, is a normalized measure of the linear relationship between two variables. It provides a value ranging from <span class="math inline">\(-1\)</span> to <span class="math inline">\(1\)</span>, which is scale-free, making it easier to interpret:</p>
<ul>
<li><span class="math inline">\(-1\)</span>: Perfect negative correlation, indicating that as one variable increases, the other decreases.</li>
<li><span class="math inline">\(0\)</span>: No correlation, indicating no linear relationship between the variables.</li>
<li><span class="math inline">\(1\)</span>: Perfect positive correlation, indicating that both variables increase together.</li>
</ul>
<p>The correlation matrix <span class="math inline">\(\Psi\)</span> provides a way to quantify the linear relationship between multiple variables, extending the notion of the correlation coefficient beyond just pairs of variables. It is derived from the covariance matrix <span class="math inline">\(\Sigma\)</span> by normalizing each element with respect to the variances of the relevant variables.</p>
<div id="def-correlation-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.21 (The Correlation Matrix <span class="math inline">\(\Psi\)</span>)</strong></span> Given a set of random variables <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, the covariance matrix <span class="math inline">\(\Sigma\)</span> is:</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1n} \\
\sigma_{21} &amp; \sigma_{22} &amp; \cdots &amp; \sigma_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{n1} &amp; \sigma_{n2} &amp; \cdots &amp; \sigma_{nn}
\end{pmatrix},
\]</span></p>
<p>where <span class="math inline">\(\sigma_{ij} = \text{cov}(X_i, X_j)\)</span> is the covariance between the <span class="math inline">\(i^{\text{th}}\)</span> and <span class="math inline">\(j^{\text{th}}\)</span> variables. The correlation matrix <span class="math inline">\(\Psi\)</span> is then defined as:</p>
<p><span class="math display">\[
\Psi = \begin{pmatrix} \rho_{ij} \end{pmatrix} = \begin{pmatrix} \frac{\sigma_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}} \end{pmatrix},
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\rho_{ij}\)</span> is the correlation coefficient between the <span class="math inline">\(i^{\text{th}}\)</span> and <span class="math inline">\(j^{\text{th}}\)</span> variables.</li>
<li><span class="math inline">\(\sigma_{ii}\)</span> is the variance of the <span class="math inline">\(i^{\text{th}}\)</span> variable, i.e., <span class="math inline">\(\sigma_i^2\)</span>.</li>
<li><span class="math inline">\(\sqrt{\sigma_{ii}}\)</span> is the standard deviation of the <span class="math inline">\(i^{\text{th}}\)</span> variable, denoted as <span class="math inline">\(\sigma_i\)</span>.</li>
</ul>
<p>Thus, <span class="math inline">\(\Psi\)</span> can also be expressed as:</p>
<p><span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; \frac{\sigma_{12}}{\sigma_1 \sigma_2} &amp; \cdots &amp; \frac{\sigma_{1n}}{\sigma_1 \sigma_n} \\
\frac{\sigma_{21}}{\sigma_2 \sigma_1} &amp; 1 &amp; \cdots &amp; \frac{\sigma_{2n}}{\sigma_2 \sigma_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\sigma_{n1}}{\sigma_n \sigma_1} &amp; \frac{\sigma_{n2}}{\sigma_n \sigma_2} &amp; \cdots &amp; 1
\end{pmatrix}
\]</span></p>
</div>
<p>The correlation matrix <span class="math inline">\(\Psi\)</span> has the following properties:</p>
<ul>
<li>The matrix <span class="math inline">\(\Psi\)</span> is symmetric, meaning <span class="math inline">\(\rho_{ij} = \rho_{ji}\)</span>.</li>
<li>The diagonal elements are all 1, as <span class="math inline">\(\rho_{ii} = \frac{\sigma_{ii}}{\sigma_i \sigma_i} = 1\)</span>.</li>
<li>Each off-diagonal element is constrained between -1 and 1, indicating the strength and direction of the linear relationship between pairs of variables.</li>
</ul>
</section>
<section id="computations" class="level3" data-number="18.14.2">
<h3 data-number="18.14.2" class="anchored" data-anchor-id="computations"><span class="header-section-number">18.14.2</span> Computations</h3>
<div id="exm-covariance-correlation-matrix" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.10 (Computing a Correlation Matrix)</strong></span> Suppose you have a dataset consisting of three variables: <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Z\)</span>. You can compute the correlation matrix as follows:</p>
<ol type="1">
<li>Calculate the covariance matrix <span class="math inline">\(\Sigma\)</span>, which contains covariances between all pairs of variables.</li>
<li>Extract the standard deviations for each variable from the diagonal elements of <span class="math inline">\(\Sigma\)</span>.</li>
<li>Use the standard deviations to compute the correlation matrix <span class="math inline">\(\Psi\)</span>.</li>
</ol>
<p>Suppose we have two sets of data points:</p>
<ul>
<li><span class="math inline">\(X = [1, 2, 3]\)</span></li>
<li><span class="math inline">\(Y = [4, 5, 6]\)</span></li>
</ul>
<p>We want to compute the correlation matrix <span class="math inline">\(\Psi\)</span> for these variables. First, calculate the mean of each variable.</p>
<p><span class="math display">\[
\bar{X} = \frac{1 + 2 + 3}{3} = 2
\]</span></p>
<p><span class="math display">\[
\bar{Y} = \frac{4 + 5 + 6}{3} = 5
\]</span></p>
<p>Second, compute the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The covariance is calculated as:</p>
<p><span class="math display">\[
\text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})
\]</span></p>
<p>For our data:</p>
<p><span class="math display">\[
\text{Cov}(X, Y) = \frac{1}{3-1} \left[(1 - 2)(4 - 5) + (2 - 2)(5 - 5) + (3 - 2)(6 - 5)\right]
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2} \left[1 + 0 + 1\right] = 1
\]</span></p>
<p>Third, calculate the variances of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Variance is calculated as:</p>
<p><span class="math display">\[
\text{Var}(X) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2} \left[(1-2)^2 + (2-2)^2 + (3-2)^2\right] = \frac{1}{2} (1 + 0 + 1) = 1
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
\text{Var}(Y) = \frac{1}{2} \left[(4-5)^2 + (5-5)^2 + (6-5)^2\right] = \frac{1}{2} (1 + 0 + 1) = 1
\]</span></p>
<p>Then, compute the correlation coefficient. The correlation coefficient <span class="math inline">\(\rho_{XY}\)</span> is:</p>
<p><span class="math display">\[
\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \cdot \sqrt{\text{Var}(Y)}}
\]</span></p>
<p><span class="math display">\[
= \frac{1}{\sqrt{1} \cdot \sqrt{1}} = 1
\]</span></p>
<p>Finally, construct the correlation matrix. The correlation matrix <span class="math inline">\(\Psi\)</span> is given as:</p>
<p><span class="math display">\[
\Psi = \begin{pmatrix}
1 &amp; \rho_{XY} \\
\rho_{XY} &amp; 1
\end{pmatrix}
= \begin{pmatrix}
1 &amp; 1 \\
1 &amp; 1
\end{pmatrix}
\]</span></p>
<p>Thus, for these two variables, the correlation matrix indicates a perfect positive linear relationship (correlation coefficient of 1) between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
</section>
<section id="the-outer-product-and-the-np.outer-function" class="level3" data-number="18.14.3">
<h3 data-number="18.14.3" class="anchored" data-anchor-id="the-outer-product-and-the-np.outer-function"><span class="header-section-number">18.14.3</span> The Outer-product and the <code>np.outer</code> Function</h3>
<p>The function <code>np.outer</code> from the NumPy library computes the outer product of two vectors. The outer product of two vectors results in a matrix, where each element is the product of an element from the first vector and an element from the second vector.</p>
<div id="def-outer-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.22 (Outer Product)</strong></span> For two vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, the outer product is defined in terms of their elements as:</p>
<p><span class="math display">\[
\text{outer}(\mathbf{a}, \mathbf{b}) = \begin{pmatrix}
a_1 \cdot b_1 &amp; a_1 \cdot b_2 &amp; \cdots &amp; a_1 \cdot b_n \\
a_2 \cdot b_1 &amp; a_2 \cdot b_2 &amp; \cdots &amp; a_2 \cdot b_n \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_m \cdot b_1 &amp; a_m \cdot b_2 &amp; \cdots &amp; a_m \cdot b_n
\end{pmatrix},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{a}\)</span> is a vector of length <span class="math inline">\(m\)</span> and <span class="math inline">\(\mathbf{b}\)</span> is a vector of length <span class="math inline">\(n\)</span>.</p>
</div>
<div id="exm-outer-product" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.11 (Computing the Outer Product)</strong></span> We will consider two vectors, <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>:</p>
<div id="55c78ed2" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>outer_product <span class="op">=</span> np.outer(a, b)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vector a:"</span>, a)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vector b:"</span>, b)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Outer Product:</span><span class="ch">\n</span><span class="st">"</span>, outer_product)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Vector a: [1 2 3]
Vector b: [4 5]
Outer Product:
 [[ 4  5]
 [ 8 10]
 [12 15]]</code></pre>
</div>
</div>
<p>For the vectors defined:</p>
<p><span class="math display">\[
\mathbf{a} = [1, 2, 3], \quad \mathbf{b} = [4, 5]
\]</span></p>
<p>The outer product will be:</p>
<p><span class="math display">\[
\begin{pmatrix}
1 \cdot 4 &amp; 1 \cdot 5 \\
2 \cdot 4 &amp; 2 \cdot 5 \\
3 \cdot 4 &amp; 3 \cdot 5
\end{pmatrix}
=
\begin{pmatrix}
4 &amp; 5 \\
8 &amp; 10 \\
12 &amp; 15
\end{pmatrix}.
\]</span></p>
</div>
<p>Thus, <code>np.outer</code> creates a matrix with dimensions <span class="math inline">\(m \times n\)</span>, where <span class="math inline">\(m\)</span> is the length of the first vector and <span class="math inline">\(n\)</span> is the length of the second vector. The function is particularly useful in various mathematical and scientific computations where matrix representations of vector relationships are needed.</p>
<div id="exm-covariance-correlation" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.12 (Computing the Covariance and the Correlation Matrix)</strong></span> The following Python code computes the covariance and correlation matrices using the NumPy library.</p>
<div id="68e48b01" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_cov_corr_matrices(data, rowvar<span class="op">=</span><span class="va">False</span>)<span class="op">-&gt;</span>(np.array, np.array):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the covariance and correlation matrices of the input data.</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co">        data (np.array):</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co">            Input data array.</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co">        rowvar (bool):</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co">            Whether the data is row-wise or column-wise.</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co">            Default is False (column-wise).</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array: Covariance matrix.</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array: Correlation matrix.</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Examples:</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; data = np.array([[1, 2, 3],</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a><span class="co">                         [4, 5, 6],</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="co">                         [7, 8, 9]])</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; calculate_cov_corr_matrices(data)</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span>   </span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    cov_matrix <span class="op">=</span> np.cov(data, rowvar<span class="op">=</span>rowvar)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>    std_devs <span class="op">=</span> np.sqrt(np.diag(cov_matrix))</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check whether the standard deviations are zero</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and throw an error if they are</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.<span class="bu">any</span>(std_devs <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Correlation matrix cannot be computed,"</span><span class="op">+</span></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>                         <span class="st">" because one or more variables have zero variance."</span>)</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    corr_matrix <span class="op">=</span> cov_matrix <span class="op">/</span> np.outer(std_devs, std_devs)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cov_matrix, corr_matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b38803dc" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>                 [<span class="dv">1</span>,<span class="dv">0</span>]])</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>Sigma, Psi <span class="op">=</span> calculate_cov_corr_matrices(A)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Covariance matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Sigma<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Psi<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix:
 [[0 1]
 [1 0]]
Covariance matrix:
 [[ 0.5 -0.5]
 [-0.5  0.5]]
Correlation matrix:
 [[ 1. -1.]
 [-1.  1.]]</code></pre>
</div>
</div>
</div>
</section>
<section id="correlation-and-independence" class="level3" data-number="18.14.4">
<h3 data-number="18.14.4" class="anchored" data-anchor-id="correlation-and-independence"><span class="header-section-number">18.14.4</span> Correlation and Independence</h3>
<div id="def-statistical-independence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.23 (Statistical Independence (Independence of Random Vectors))</strong></span> Two random vectors are statistically independent if the joint distribution of the vectors is equal to the product of their marginal distributions.</p>
</div>
<p>This means that knowing the realization of one vector gives you no information about the realization of the other vector. This independence is a probabilistic concept used in statistics and probability theory to denote that two sets of random variables do not affect each other. Independence implies that all pairwise covariances between the components of the two vectors are zero, but zero covariance does not imply independence unless certain conditions are met (e.g., normality). Statistical independence is a stronger condition than zero covariance. Statistical independence is not related to the linear independence of vectors in linear algebra.</p>
<div id="exm-cov-independent" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.13 (Covariance of Independent Variables)</strong></span> Consider a covariance matrix where variables are independent:</p>
<div id="8d266a85" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,<span class="dv">0</span>],</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,<span class="dv">1</span>],</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>[<span class="dv">4</span>,<span class="dv">0</span>],</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>[<span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>Sigma, Psi <span class="op">=</span> calculate_cov_corr_matrices(A)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Covariance matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Sigma<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Psi<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix:
 [[ 1 -1]
 [ 2  0]
 [ 3  1]
 [ 4  0]
 [ 5 -1]]
Covariance matrix:
 [[2.5 0. ]
 [0.  0.7]]
Correlation matrix:
 [[1. 0.]
 [0. 1.]]</code></pre>
</div>
</div>
<p>Here, since the off-diagonal elements are 0, the variables are uncorrelated. <span class="math inline">\(X\)</span> increases linearly, while <span class="math inline">\(Y\)</span> alternates in a simple pattern with no trend that is linearly related to <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="exm-cov-strong" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.14 (Strong Correlation)</strong></span> For a covariance matrix with strong positive correlation:</p>
<div id="5b4ddb94" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">10</span>,<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>[<span class="dv">20</span>,<span class="dv">0</span>],</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>[<span class="dv">30</span>,<span class="dv">1</span>],</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>[<span class="dv">40</span>,<span class="dv">2</span>],</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>[<span class="dv">50</span>,<span class="dv">3</span>]])</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>Sigma, Psi <span class="op">=</span> calculate_cov_corr_matrices(A)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Covariance matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Sigma<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Psi<span class="sc">}</span><span class="ss">"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix:
 [[10 -1]
 [20  0]
 [30  1]
 [40  2]
 [50  3]]
Covariance matrix:
 [[250.   25. ]
 [ 25.    2.5]]
Correlation matrix:
 [[1. 1.]
 [1. 1.]]</code></pre>
</div>
</div>
<p>A value close to 1 suggests a strong positive relationship between the variables.</p>
</div>
<div id="exm-cov-negative" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.15 (Strong Negative Correlation)</strong></span> &nbsp;</p>
<div id="0ebb7873" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">10</span>,<span class="dv">1</span>],</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>[<span class="dv">20</span>,<span class="dv">0</span>],</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>[<span class="dv">30</span>,<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>[<span class="dv">40</span>,<span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>[<span class="dv">50</span>,<span class="op">-</span><span class="dv">3</span>]])</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>Sigma, Psi <span class="op">=</span> calculate_cov_corr_matrices(A)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Covariance matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Sigma<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation matrix:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>Psi<span class="sc">}</span><span class="ss">"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input matrix:
 [[10  1]
 [20  0]
 [30 -1]
 [40 -2]
 [50 -3]]
Covariance matrix:
 [[250.  -25. ]
 [-25.    2.5]]
Correlation matrix:
 [[ 1. -1.]
 [-1.  1.]]</code></pre>
</div>
</div>
<p>This matrix indicates a perfect negative correlation where one variable increases as the other decreases.</p>
</div>
</section>
<section id="pearsons-correlation" class="level3" data-number="18.14.5">
<h3 data-number="18.14.5" class="anchored" data-anchor-id="pearsons-correlation"><span class="header-section-number">18.14.5</span> Pearson’s Correlation</h3>
<ul>
<li>Video: [Pearson’s Correlation, Clearly Explained]</li>
</ul>
</section>
<section id="interpreting-the-correlation-correlation-squared" class="level3" data-number="18.14.6">
<h3 data-number="18.14.6" class="anchored" data-anchor-id="interpreting-the-correlation-correlation-squared"><span class="header-section-number">18.14.6</span> Interpreting the Correlation: Correlation Squared</h3>
<p><span class="citation" data-cites="rumm76a">Rummel (<a href="references.html#ref-rumm76a" role="doc-biblioref">1976</a>)</span> describes how to interpret correlations as follows:</p>
<p>Seldom, indeed, will a correlation be zero or perfect. Usually, the covariation between things will be something like <span class="math inline">\(.56\)</span> or <span class="math inline">\(-.16\)</span>. Clearly <span class="math inline">\(.56\)</span> is positive, indicating positive covariation; <span class="math inline">\(-.16\)</span> is negative, indicating some negative covariation. Moreover, we can say that the positive correlation is greater than the negative. But, we require more than. If we have a correlation of <span class="math inline">\(.56\)</span> between two variables, for example, what precisely can we say other than the correlation is positive and <span class="math inline">\(.56\)</span>? The squared correlation describes the proportion of variance in common between the two variables. If we multiply this by 100 we then get the percent of variance in common between two variables. That is:</p>
<p><span class="math display">\[
r^2_{XY} \times  100 = \text{percent of variance in common between} X \text{ and } Y.
\]</span></p>
<p>For example, we found that the correlation between a nation’s power and its defense budget was <span class="math inline">\(.66\)</span>. This correlation squared is <span class="math inline">\(.45\)</span>, which means that across the fourteen nations constituting the sample <span class="math inline">\(45\)</span> percent of their variance on the two variables is in common (or <span class="math inline">\(55\)</span> percent is not in common). In thus squaring correlations and transforming covariance to percentage terms we have an easy to understand meaning of correlation. And we are then in a position to evaluate a particular correlation. As a matter of routine it is the squared correlations that should be interpreted. This is because the correlation coefficient is misleading in suggesting the existence of more covariation than exists, and this problem gets worse as the correlation approaches zero.</p>
<div id="exm-explain-cov" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.16 (The relationship between study time and test scores)</strong></span> Imagine we are examining the relationship between the number of hours students study for a subject (Variable <span class="math inline">\(A\)</span>) and their scores on a test (Variable <span class="math inline">\(B\)</span>). After analyzing the data, we calculate a correlation of 0.8 between study time and test scores. When we square this correlation coefficient (<span class="math inline">\(0.8^2\)</span> = 0.64), we get 0.64 or 64%. This means that 64% of the variability in test scores can be accounted for by the variability in study hours. This indicates that a substantial part of why students score differently on the test can be explained by how much they studied. However, there remains 36% of the variability in test scores that needs to be explained by other factors, such as individual abilities, the difficulty of the test, or other external influences.</p>
</div>
</section>
<section id="partial-correlation" class="level3" data-number="18.14.7">
<h3 data-number="18.14.7" class="anchored" data-anchor-id="partial-correlation"><span class="header-section-number">18.14.7</span> Partial Correlation</h3>
<p>Often, a correlation between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be found only because both variables are correlated with a third variable <span class="math inline">\(Z\)</span>. The correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is then a spurious correlation. Therefore, it is often of interest to determine the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> while partializing a variable <span class="math inline">\(Z\)</span>, i.e., the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> that exists without the influence of <span class="math inline">\(Z\)</span>. Such a correlation <span class="math inline">\(\rho_{(X,Y)/Z}\)</span> is called the partial correlation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> while holding <span class="math inline">\(Z\)</span> constant. It is given by <span class="math display">\[\begin{equation}
\rho_{(X,Y)/Z} = \frac{\rho_{XY} - \rho_{XZ}\rho_{YZ}}{\sqrt{(1-\rho_{XZ}^2)(1-\rho_{YZ}^2)}},
\end{equation}\]</span> where <span class="math inline">\(\rho_{XY}\)</span> is the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, <span class="math inline">\(\rho_{XZ}\)</span> is the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, and <span class="math inline">\(\rho_{YZ}\)</span> is the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> <span class="citation" data-cites="Hart95a">(<a href="references.html#ref-Hart95a" role="doc-biblioref">Hartung, Elpert, and Klösener 1995</a>)</span>.</p>
<p>If the variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are jointly normally distributed in the population of interest, one can estimate <span class="math inline">\(\rho_{(X,Y)/Z}\)</span> based on <span class="math inline">\(n\)</span> realizations <span class="math inline">\(x_1, \ldots, x_n\)</span>, <span class="math inline">\(y_1, \ldots, y_n\)</span> and <span class="math inline">\(z_1, \ldots, z_n\)</span> of the random variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> by replacing the simple correlations <span class="math inline">\(\rho_{XY}\)</span>, <span class="math inline">\(\rho_{XZ}\)</span> and <span class="math inline">\(\rho_{YZ}\)</span> with the empirical correlations <span class="math inline">\(\hat{\rho}_{XY}\)</span>, <span class="math inline">\(\hat{\rho}_{XZ}\)</span> and <span class="math inline">\(\hat{\rho}_{YZ}\)</span>. The partial correlation coefficient <span class="math inline">\(\hat{\rho}_{(X,Y)/Z}\)</span> is then estimated using <span class="math display">\[\begin{equation}
r_{(X,Y)/Z} = \frac{r_{XY} - r_{XZ}r_{YZ}}{\sqrt{(1-r_{XZ}^2)(1-r_{YZ}^2)}}.
\end{equation}\]</span> Based on this estimated value for the partial correlation, a test at the <span class="math inline">\(\alpha\)</span> level for partial uncorrelatedness or independence of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> under <span class="math inline">\(Z\)</span> can also be carried out. The hypothesis</p>
<p><span class="math display">\[\begin{equation}
H_0: \rho_{(X,Y)/Z} = 0
\end{equation}\]</span> is tested against the alternative <span class="math display">\[\begin{equation}
H_1: \rho_{(X,Y)/Z} \neq 0
\end{equation}\]</span> at the level <span class="math inline">\(\alpha\)</span> is discarded if <span class="math display">\[
\left|
\frac{r_{(X,Y)/Z} \sqrt{n-3}}{\sqrt{1-r_{(X,Y)/Z}^2}}
\right| &gt; t_{n-3, 1-\alpha/2}
\]</span> applies. Here <span class="math inline">\(t_{n-3, 1-\alpha/2}\)</span> is the (<span class="math inline">\(1-\alpha/2\)</span>)-quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-3\)</span> degrees of freedom.</p>
<div id="exm-partial-corr1" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.17</strong></span> For example, given economic data on the consumption <span class="math inline">\(X\)</span>, income <span class="math inline">\(Y\)</span>, and wealth <span class="math inline">\(Z\)</span> of various individuals, consider the relationship between consumption and income. Failing to control for wealth when computing a correlation coefficient between consumption and income would give a misleading result, since income might be numerically related to wealth which in turn might be numerically related to consumption; a measured correlation between consumption and income might actually be contaminated by these other correlations. The use of a partial correlation avoids this problem <span class="citation" data-cites="wiki25a">(<a href="references.html#ref-wiki25a" role="doc-biblioref">Wikipedia contributors 2024</a>)</span>.</p>
</div>
<div id="exm-partial-correlation" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.18 (Partial Correlation. Numerical Example)</strong></span> Given the following data, calculate the partial correlation between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, controlling for <span class="math inline">\(C\)</span>. <span class="math display">\[
A = \begin{pmatrix}
2\\
4\\
15\\
20
\end{pmatrix}, \quad B = \begin{pmatrix}
1\\
2\\
3\\
4
\end{pmatrix}, \quad C = \begin{pmatrix}
0\\
0\\
1\\
1
\end{pmatrix}
\]</span></p>
<div id="8f8aac67" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotpython.utils.stats <span class="im">import</span> partial_correlation</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'A'</span>: [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">15</span>, <span class="dv">20</span>],</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'B'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>],</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'C'</span>: [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation between A and B: </span><span class="sc">{</span>data[<span class="st">'A'</span>]<span class="sc">.</span>corr(data[<span class="st">'B'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>pc <span class="op">=</span> partial_correlation(data, method<span class="op">=</span><span class="st">'pearson'</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Partial Correlation between A and B: </span><span class="sc">{</span>pc[<span class="st">"estimate"</span>][<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation between A and B: 0.9695015519208121
Partial Correlation between A and B: 0.9191450300180576</code></pre>
</div>
</div>
</div>
<p>Instead of considering only one variable <span class="math inline">\(Z\)</span>, multiple variables <span class="math inline">\(Z_i\)</span> can be considered. The formal definiton of partial correlation reads as follows:</p>
<div id="def-partial-correlation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.24 (Partial Correlation)</strong></span> Formally, the partial correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given a set of <span class="math inline">\(n\)</span> controlling variables <span class="math inline">\(\mathbf{Z} = \{Z_1, Z_2, \ldots, Z_n\}\)</span>, written <span class="math inline">\(\rho_{XY \cdot \mathbf{Z}}\)</span>, is the correlation between the residuals <span class="math inline">\(e_X\)</span> and <span class="math inline">\(e_Y\)</span> resulting from the linear regression of <span class="math inline">\(X\)</span> with <span class="math inline">\(\mathbf{Z}\)</span> and of <span class="math inline">\(Y\)</span> with <span class="math inline">\(\mathbf{Z}\)</span>, respectively. The first-order partial correlation (i.e., when <span class="math inline">\(n = 1\)</span>) is the difference between a correlation and the product of the removable correlations divided by the product of the coefficients of alienation of the removable correlations <span class="citation" data-cites="wiki25a">(<a href="references.html#ref-wiki25a" role="doc-biblioref">Wikipedia contributors 2024</a>)</span>.</p>
</div>
<p>Like the correlation coefficient, the partial correlation coefficient takes on a value in the range from -1 to 1. The value -1 conveys a perfect negative correlation controlling for some variables (that is, an exact linear relationship in which higher values of one variable are associated with lower values of the other); the value 1 conveys a perfect positive linear relationship, and the value 0 conveys that there is no linear relationship <span class="citation" data-cites="wiki25a">(<a href="references.html#ref-wiki25a" role="doc-biblioref">Wikipedia contributors 2024</a>)</span>.</p>
<div id="lem-matrix-part-corr" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 18.1 (Matrix Representation of the Partial Correlation)</strong></span> The partial correlation can also be written in terms of the joint precision matrix <span class="citation" data-cites="wiki25a">(<a href="references.html#ref-wiki25a" role="doc-biblioref">Wikipedia contributors 2024</a>)</span>. Consider a set of random variables, <span class="math inline">\(\mathbf{V} = \{X_1,\dots, X_n\}\)</span> of cardinality <span class="math inline">\(n\)</span>. We want the partial correlation between two variables <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> given all others, i.e., <span class="math inline">\(\mathbf{V} \setminus \{X_i, X_j\}\)</span>. Suppose the (joint/full) covariance matrix <span class="math inline">\(\Sigma = (\sigma_{ij})\)</span> is positive definite and therefore invertible. If the precision matrix is defined as <span class="math inline">\(\Omega = (p_{ij}) = \Sigma^{-1}\)</span>, then <span class="math display">\[\begin{equation}
\rho_{X_i X_j \cdot \mathbf{V} \setminus \{X_i,X_j\}} = - \frac{p_{ij}}{\sqrt{p_{ii}p_{jj}}}
\end{equation}\]</span></p>
</div>
<p>The semipartial correlation statistic is similar to the partial correlation statistic; both compare variations of two variables after certain factors are controlled for. However, to calculate the semipartial correlation, one holds the third variable constant for either X or Y but not both; whereas for the partial correlation, one holds the third variable constant for both <span class="citation" data-cites="wiki25a">(<a href="references.html#ref-wiki25a" role="doc-biblioref">Wikipedia contributors 2024</a>)</span>.</p>
</section>
</section>
<section id="hypothesis-testing-and-the-null-hypothesis" class="level2" data-number="18.15">
<h2 data-number="18.15" class="anchored" data-anchor-id="hypothesis-testing-and-the-null-hypothesis"><span class="header-section-number">18.15</span> Hypothesis Testing and the Null-Hypothesis</h2>
<section id="alternative-hypotheses-main-ideas" class="level3" data-number="18.15.1">
<h3 data-number="18.15.1" class="anchored" data-anchor-id="alternative-hypotheses-main-ideas"><span class="header-section-number">18.15.1</span> Alternative Hypotheses, Main Ideas</h3>
</section>
<section id="p-values-what-they-are-and-how-to-interpret-them" class="level3" data-number="18.15.2">
<h3 data-number="18.15.2" class="anchored" data-anchor-id="p-values-what-they-are-and-how-to-interpret-them"><span class="header-section-number">18.15.2</span> p-values: What they are and how to interpret them</h3>
<section id="how-to-calculate-p-values" class="level4" data-number="18.15.2.1">
<h4 data-number="18.15.2.1" class="anchored" data-anchor-id="how-to-calculate-p-values"><span class="header-section-number">18.15.2.1</span> How to calculate p-values</h4>
</section>
<section id="p-hacking-what-it-is-and-how-to-avoid-it" class="level4" data-number="18.15.2.2">
<h4 data-number="18.15.2.2" class="anchored" data-anchor-id="p-hacking-what-it-is-and-how-to-avoid-it"><span class="header-section-number">18.15.2.2</span> p-hacking: What it is and how to avoid it</h4>
</section>
</section>
</section>
<section id="statistical-power" class="level2" data-number="18.16">
<h2 data-number="18.16" class="anchored" data-anchor-id="statistical-power"><span class="header-section-number">18.16</span> Statistical Power</h2>
<ul>
<li>Video: <a href="https://youtu.be/Rsc5znwR5FA?si=Ca4e-EopumAtgl8Q">Statistical Power, Clearly Explained</a></li>
</ul>
<section id="power-analysis" class="level4" data-number="18.16.0.1">
<h4 data-number="18.16.0.1" class="anchored" data-anchor-id="power-analysis"><span class="header-section-number">18.16.0.1</span> Power Analysis</h4>
<ul>
<li>Video: <a href="https://youtu.be/VX_M3tIyiYk?si=Vb6Fr1aJWQU5Ujjp">Power Analysis, Clearly Explained!!!</a></li>
</ul>
</section>
</section>
<section id="the-central-limit-theorem" class="level2" data-number="18.17">
<h2 data-number="18.17" class="anchored" data-anchor-id="the-central-limit-theorem"><span class="header-section-number">18.17</span> The Central Limit Theorem</h2>
<ul>
<li>Video: <a href="https://youtu.be/YAlJCEDH2uY?si=NRYvP7Y0Mow32jV2">The Central Limit Theorem, Clearly Explained!!!</a></li>
</ul>
</section>
<section id="maximum-likelihood" class="level2" data-number="18.18">
<h2 data-number="18.18" class="anchored" data-anchor-id="maximum-likelihood"><span class="header-section-number">18.18</span> Maximum Likelihood</h2>
<p>Maximum likelihood estimation is a method used to estimate the parameters of a statistical model. It is based on the principle of choosing the parameter values that maximize the likelihood of the observed data. The likelihood function represents the probability of observing the data given the model parameters. By maximizing this likelihood, we can find the parameter values that best explain the observed data.</p>
<div id="exm-MaxLike1Bern" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.19 (Maximum Likelihood Estimation: Bernoulli Experiment)</strong></span> Bernoulli experiment for the event <span class="math inline">\(A\)</span>, repeated <span class="math inline">\(n\)</span> times, with the probability of success <span class="math inline">\(p\)</span>. Result given as <span class="math inline">\(n\)</span> tuple with entries <span class="math inline">\(A\)</span> and <span class="math inline">\(\overline{A}\)</span>. <span class="math inline">\(A\)</span> appears <span class="math inline">\(k\)</span> times. The probability of this event is given by <span class="math display">\[\begin{equation}
L(p) = p^k (1-p)^{n-k}
\end{equation}\]</span> Applying maximum likelihood estimation, we find the maximum of the likelihood function <span class="math inline">\(L(p)\)</span>, i.e., we are trying to find the value of <span class="math inline">\(p\)</span> that maximizes the probability of observing the data. This value will be denoted as <span class="math inline">\(\hat{p}\)</span>.</p>
<p>Differentiating the likelihood function with respect to <span class="math inline">\(p\)</span> and setting the derivative to zero, we find the maximum likelihood estimate <span class="math inline">\(\hat{p}\)</span>. We get <span class="math display">\[\begin{align}
\frac{d}{dp} L(p) &amp; = k p^{k-1} (1-p)^{n-k} - p^k (n-k) (1-p)^{n-k-1}\\
                  &amp; = p^{k-1} (1-p)^{n-k-1} \left(k(1-p) - p(n-k)\right) = 0
\end{align}\]</span></p>
<p>Because <span class="math display">\[
p \neq 0 \text{ and } (1-p) p \neq 0,
\]</span> we can divide by <span class="math inline">\(p^{k-1} (1-p)^{n-k-1}\)</span> and get <span class="math display">\[\begin{equation}
k(1-p) - p(n-k) = 0.
\end{equation}\]</span> Solving for <span class="math inline">\(p\)</span> gives <span class="math display">\[\begin{equation}
\hat{p} = \frac{k}{n}
\end{equation}\]</span></p>
<p>Therefore, the maximum likelihood estimate for the probability of success in a Bernoulli experiment is the ratio of the number of successes to the total number of trials.</p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<div id="exm-MaxLikeNormal" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.20 (Maximum Likelihood Estimation: Normal Distribution)</strong></span> Random variable <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span> with <span class="math inline">\(n\)</span> observations <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>. The likelihood function is given by <span class="math display">\[\begin{equation}
L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
\end{equation}\]</span></p>
<p>Taking the logarithm of the likelihood function, we get <span class="math display">\[\begin{equation}
\log L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
\end{equation}\]</span></p>
<p>Partial derivative with respect to <span class="math inline">\(\mu\)</span> is <span class="math display">\[\begin{align}
\frac{\partial}{\partial \mu} \log L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) &amp; = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0
\end{align}\]</span> We obtain the maximum likelihood estimate for <span class="math inline">\(\mu\)</span> as <span class="math display">\[\begin{equation}
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i
\end{equation}\]</span></p>
<p>The partial derivative with respect to <span class="math inline">\(\sigma^2\)</span> is <span class="math display">\[\begin{align}
\frac{\partial}{\partial \sigma^2} \log L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) &amp; = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (x_i - \mu)^2 = 0
\end{align}\]</span> This can be simplified to <span class="math display">\[\begin{align}
-n + \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 = 0\\
\Rightarrow n \sigma^2 = \sum_{i=1}^n (x_i - \mu)^2
\end{align}\]</span> Using the maximum likelihood estimate for <span class="math inline">\(\mu\)</span>, we get <span class="math display">\[\begin{equation}
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2
\end{equation}\]</span> <span class="math display">\[\begin{equation}
= \frac{n-1}{n} \frac{\sum_{i=1}^n (x_i - \hat{\mu})^2{n-1}} = \frac{n-1}{n} s^2,
\end{equation}\]</span> where <span class="math display">\[\begin{equation}
s = \sqrt{\frac{\sum_{i=1}^n (x_i- \overline{x})}{n-1}}
\end{equation}\]</span> is the sample standard deviation. We obtain the maximum likelihood estimate for <span class="math inline">\(\sigma^2\)</span> as <span class="math display">\[\begin{equation}
\hat{\sigma}^2 = \frac{n-1}{n} s^2
\end{equation}\]</span></p>
</div>
<ul>
<li><p>Video: <a href="https://youtu.be/XepXtl9YKwc?si=ADMYC10DscaxSTvk">Maximum Likelihood, clearly explained!!!</a></p></li>
<li><p>Video: <a href="https://youtu.be/pYxNSUDSFH4?si=eEan9lAUp1NNGEjY">Probability is not Likelihood. Find out why!!!</a></p></li>
</ul>
</section>
<section id="maximum-likelihood-estimation-multivariate-normal-distribution" class="level2" data-number="18.19">
<h2 data-number="18.19" class="anchored" data-anchor-id="maximum-likelihood-estimation-multivariate-normal-distribution"><span class="header-section-number">18.19</span> Maximum Likelihood Estimation: Multivariate Normal Distribution</h2>
<section id="the-joint-probability-density-function-of-the-multivariate-normal-distribution" class="level3" data-number="18.19.1">
<h3 data-number="18.19.1" class="anchored" data-anchor-id="the-joint-probability-density-function-of-the-multivariate-normal-distribution"><span class="header-section-number">18.19.1</span> The Joint Probability Density Function of the Multivariate Normal Distribution</h3>
<p>Consider the first <span class="math inline">\(n\)</span> terms of an identically and independently distributed (i.i..d.) sequence <span class="math inline">\({X^{(j)}}\)</span> of <span class="math inline">\(k\)</span>-dimensional multivariate normal random vectors, i.e., <span id="eq-mvn"><span class="math display">\[
X^{(j)} \sim N(\mu, \Sigma), j=1,2,\ldots.
\tag{18.4}\]</span></span></p>
<p>The joint probability density function of the <span class="math inline">\(j\)</span>-th term of the sequence is <span class="math display">\[
f_X(x_j) = \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right),
\]</span></p>
<p>where: <span class="math inline">\(\mu\)</span> is the <span class="math inline">\(k \times 1\)</span> mean vector; <span class="math inline">\(\Sigma\)</span> is the <span class="math inline">\(k \times k\)</span> covariance matrix. The covariance matrix <span class="math inline">\(\Sigma\)</span> is assumed to be positive definite, so that its determinant is strictly positive. We use <span class="math inline">\(x_1, \ldots x_n\)</span>, i.e., the realizations of the first <span class="math inline">\(n\)</span> random vectors in the sequence, to estimate the two unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>.</p>
</section>
<section id="the-log-likelihood-function" class="level3" data-number="18.19.2">
<h3 data-number="18.19.2" class="anchored" data-anchor-id="the-log-likelihood-function"><span class="header-section-number">18.19.2</span> The Log-Likelihood Function</h3>
<div id="def-likelihood" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.25 (Likelihood Function)</strong></span> The likelihood function is defined as the joint probability density function of the observed data, viewed as a function of the unknown parameters.</p>
</div>
<p>Since the terms in the sequence <a href="#eq-mvn" class="quarto-xref">Equation&nbsp;<span>18.4</span></a> are independent, their joint density is equal to the product of their marginal densities. As a consequence, the likelihood function can be written as the product of the individual densities:</p>
<p><span class="math display">\[
L(\mu, \Sigma) = \prod_{j=1}^n f_X(x_j) = \prod_{j=1}^n \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right)
\]</span> <span id="eq-likelihood-mvn"><span class="math display">\[
= \frac{1}{(2\pi)^{nk/2} \det(\Sigma)^{n/2}} \exp\left(-\frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right).
\tag{18.5}\]</span></span></p>
<p>Taking the natural logarithm of the likelihood function, we obtain the log-likelihood function:</p>
<div id="exm-log-likelihood" class="theorem example">
<p><span class="theorem-title"><strong>Example 18.21 (Log-Likelihood Function of the Multivariate Normal Distribution)</strong></span> The log-likelihood function of the multivariate normal distribution is given by <span class="math display">\[
\ell(\mu, \Sigma) = -\frac{nk}{2} \ln(2\pi) - \frac{n}{2} \ln(\det(\Sigma)) - \frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu).
\]</span></p>
</div>
<p>The likelihood function is well-defined only if <span class="math inline">\(\det(\Sigma)&gt;0\)</span>.</p>
</section>
</section>
<section id="cross-validation" class="level2" data-number="18.20">
<h2 data-number="18.20" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">18.20</span> Cross-Validation</h2>
<ul>
<li>Video: <a href="https://youtu.be/fSytzGwwBVw?si=a8U5yCIEhwAw4AyU">Machine Learning Fundamentals: Cross Validation</a></li>
</ul>
<section id="bias-and-variance" class="level4" data-number="18.20.0.1">
<h4 data-number="18.20.0.1" class="anchored" data-anchor-id="bias-and-variance"><span class="header-section-number">18.20.0.1</span> Bias and Variance</h4>
<ul>
<li>Video: <a href="https://youtu.be/EuBBz3bI-aA?si=7MVv_J1HbzMSQS4K">Machine Learning Fundamentals: Bias and Variance</a></li>
</ul>
</section>
</section>
<section id="mutual-information" class="level2" data-number="18.21">
<h2 data-number="18.21" class="anchored" data-anchor-id="mutual-information"><span class="header-section-number">18.21</span> Mutual Information</h2>
<p><span class="math inline">\(R^2\)</span> works only for numerical data. Mutual information can also be used, when the dependent variable is boolean or categorical. Mutual information provides a way to quantify the relationship of a mixture of continuous and discrete variables to a target variable. Mutual information explains how closely related two variables are. It is a measure of the amount of information that one variable provides about another variable.</p>
<div id="def-mutual-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 18.26 (Mutual Information)</strong></span> <span id="eq-mutual-information"><span class="math display">\[
\text{MI} =
\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \left(\frac{p(x, y)}{p(x)p(y)}\right)
\tag{18.6}\]</span></span></p>
</div>
<p>The terms in the nominator and denominator in <a href="#eq-mutual-information" class="quarto-xref">Equation&nbsp;<span>18.6</span></a> are the joint probability, <span class="math inline">\(p(x, y)\)</span>, the marginal probability of <span class="math inline">\(X\)</span>, <span class="math inline">\(p(x)\)</span> and the marginal probability of <span class="math inline">\(Y\)</span>, <span class="math inline">\(p(y)\)</span> respectively. Joint probabilites are the probabilities of two events happening together. Marginal probabilities are the probabilities of individual events happening. If <span class="math inline">\(p(x,y) = p(x)p(y)\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent. In this case, the mutual information is one.</p>
<p>The mutual information of two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math display">\[\begin{align}
\text{MI}(X, Y) =
p(X,Y) \log \left(\frac{p(X,Y)}{p(X)p(Y)}\right) +
p(X, \overline{Y}) \log \left(\frac{p(X, \overline{Y})}{p(X)p(\overline{Y})}\right) +  \\
p(\overline{X}, Y) \log \left(\frac{p(\overline{X}, Y)}{p(\overline{X})p(Y)}\right) +
p(\overline{X}, \overline{Y}) \log \left(\frac{p(\overline{X}, \overline{Y})}{p(\overline{X})p(\overline{Y})}\right)
\end{align}\]</span></p>
<p>In general, when at least one of the two features has no variance (i.e., it is constant), the mutual information is zero, because something that never changes cannot tell us about something that does. When two features change, but change in exact the same way, then <span class="math inline">\(\text{MI}\)</span> is <span class="math inline">\(1/2\)</span>. When two factors change, but in exact the opposite way, then <span class="math inline">\(\text{MI}\)</span> is <span class="math inline">\(1/2\)</span>. When both features change, it does not matter if they change in the exact same or exact opposite ways; both result in the same <span class="math inline">\(\text{MI}\)</span> value</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mutual Information
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Video: <a href="https://youtu.be/eJIp_mgVLwE?si=KaeiRN0st1gqkj4c">Mutual Information, Clearly Explained</a></li>
</ul>
</div>
</div>
</section>
<section id="principal-component-analysis-pca" class="level2" data-number="18.22">
<h2 data-number="18.22" class="anchored" data-anchor-id="principal-component-analysis-pca"><span class="header-section-number">18.22</span> Principal Component Analysis (PCA)</h2>
<ul>
<li><p>Video: <a href="https://youtu.be/FgakZw6K1QQ?si=lmXhc-bpOqb7RmDP">Principal Component Analysis (PCA), Step-by-Step</a></p></li>
<li><p>Vidoe: <a href="https://youtu.be/oRvgq966yZg?si=TIUsxNItfyYOjTLt">PCA - Practical Tips</a></p></li>
<li><p>Video: <a href="https://youtu.be/Lsue2gEM9D0?si=_fV_RzK8j1jwcb-e">PCA in Python</a></p></li>
</ul>
</section>
<section id="t-sne" class="level2" data-number="18.23">
<h2 data-number="18.23" class="anchored" data-anchor-id="t-sne"><span class="header-section-number">18.23</span> t-SNE</h2>
<ul>
<li>Video: <a href="https://youtu.be/NEaUSP4YerM?si=f8-6ewwv5TMD7gdL">t-SNE, Clearly Explained</a></li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Hart95a" class="csl-entry" role="listitem">
Hartung, Joachim, Bärbel Elpert, and Karl-Heinz Klösener. 1995. <em><span>Statistik</span></em>. Oldenbourg.
</div>
<div id="ref-rumm76a" class="csl-entry" role="listitem">
Rummel, R. J. 1976. <span>“Understanding Correlation.”</span> <a href="https://www.hawaii.edu/powerkills/UC.HTM">https://www.hawaii.edu/powerkills/UC.HTM</a>.
</div>
<div id="ref-wiki25a" class="csl-entry" role="listitem">
Wikipedia contributors. 2024. <span>“Partial Correlation — <span>Wikipedia</span><span>,</span> the Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=Partial_correlation&amp;oldid=1253637419">https://en.wikipedia.org/w/index.php?title=Partial_correlation&amp;oldid=1253637419</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./017_num_spot_user_function.html" class="pagination-link" aria-label="User-Specified Functions: Extending the Analytical Class">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">User-Specified Functions: Extending the Analytical Class</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./100_ddmo_regression.html" class="pagination-link" aria-label="Regression">
        <span class="nav-page-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, T. Bartz-Beielstein</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/bartzbeielstein">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>