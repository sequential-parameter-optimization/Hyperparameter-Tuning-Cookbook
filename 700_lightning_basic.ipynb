{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Basic Lightning Module\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This chapter implements a basic Pytorch Lightning module. It is based on the Lightning documentation [LIGHTNINGMODULE](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html).\n",
        "\n",
        "\n",
        "A `LightningModule` organizes your `PyTorch` code into six sections:\n",
        "\n",
        "* Initialization (`__init__` and `setup()`).\n",
        "* Train Loop (`training_step()`)\n",
        "* Validation Loop (`validation_step()`)\n",
        "* Test Loop (`test_step()`)\n",
        "* Prediction Loop (`predict_step()`)\n",
        "* Optimizers and LR Schedulers (`configure_optimizers()`)\n",
        "\n",
        "The `Trainer` automates every required step in a clear and reproducible way. It is the most important part of PyTorch Lightning. It is responsible for training, testing, and validating the model.\n",
        "The `Lightning` core structure looks like this:\n",
        "\n",
        "```python\n",
        "net = MyLightningModuleNet()\n",
        "trainer = Trainer()\n",
        "trainer.fit(net)\n",
        "```\n",
        "There are no `.cuda()` or `.to(device)` calls required. Lightning does these for you.\n",
        "\n",
        "```python\n",
        "# don't do in Lightning\n",
        "x = torch.Tensor(2, 3)\n",
        "x = x.cuda()\n",
        "x = x.to(device)\n",
        "\n",
        "# do this instead\n",
        "x = x  # leave it alone!\n",
        "\n",
        "# or to init a new tensor\n",
        "new_x = torch.Tensor(2, 3)\n",
        "new_x = new_x.to(x)\n",
        "```\n",
        "\n",
        "A LightningModule is a `torch.nn.Module` but with added functionality. For example:\n",
        "\n",
        "```python\n",
        "net = Net.load_from_checkpoint(PATH)\n",
        "net.freeze()\n",
        "out = net(x)\n",
        "```\n",
        "\n",
        "## Starter Example: Transformer\n",
        "\n",
        "Here are the only required methods for setting up a transfomer model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: transformer-setup\n",
        "#| eval: true\n",
        "import lightning as L\n",
        "import torch\n",
        "\n",
        "from lightning.pytorch.demos import Transformer\n",
        "\n",
        "\n",
        "class LightningTransformer(L.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = Transformer(vocab_size=vocab_size)\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        return self.model(inputs, target)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, target = batch\n",
        "        output = self(inputs, target)\n",
        "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.model.parameters(), lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `LightningTransformer` class is a subclass of `LightningModule`. It can be trainted as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: transformer-train\n",
        "#| eval: true\n",
        "from lightning.pytorch.demos import WikiText2\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = WikiText2()\n",
        "dataloader = DataLoader(dataset)\n",
        "model = LightningTransformer(vocab_size=dataset.vocab_size)\n",
        "\n",
        "trainer = L.Trainer(fast_dev_run=100)\n",
        "trainer.fit(model=model, train_dataloaders=dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Core Methods\n",
        "\n",
        "The LightningModule has many convenient methods, but the core ones you need to know about are shown in @tbl-lm-core-methods.\n",
        "\n",
        "| Method | Description |\n",
        "| :--- | :--- |\n",
        "| `__init__` and `setup` | Initializes the model. |\n",
        "| `forward` | Performs a forward pass through the model. To run data through your model only (separate from `training_step`). |\n",
        "| `training_step` | Performs a complete training step. |\n",
        "| `validation_step` | Performs a complete validation step. |\n",
        "| `test_step` | Performs a complete test step. |\n",
        "| `predict_step` | Performs a complete prediction step. |\n",
        "| `configure_optimizers` | Configures the optimizers and learning-rate schedulers. |\n",
        ": The core methods of a LightningModule {#tbl-lm-core-methods}\n",
        "\n",
        "We will take a closer look at thes methods.\n",
        "\n",
        "### Training Step\n",
        "\n",
        "#### Basics\n",
        "\n",
        "To activate the training loop, override the `training_step()` method.\n",
        "\\index{training\\_step()}\n",
        "If you want to calculate epoch-level metrics and log them, use `log()`.\n",
        "\n",
        "```python\n",
        "class LightningTransformer(L.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = Transformer(vocab_size=vocab_size)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, target = batch\n",
        "        output = self.model(inputs, target)\n",
        "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
        "\n",
        "        # logs metrics for each training_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "```\n",
        "\n",
        "The `log()` method automatically reduces the requested metrics across a complete epoch and devices. \n",
        "\n",
        "#### Background\n",
        "\n",
        "* Here is the pseudocode of what the `log()` method does under the hood:\n",
        "\\index{log()}\n",
        "```python\n",
        "outs = []\n",
        "for batch_idx, batch in enumerate(train_dataloader):\n",
        "    # forward\n",
        "    loss = training_step(batch, batch_idx)\n",
        "    outs.append(loss.detach())\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "# note: in reality, we do this incrementally, instead of keeping all outputs in memory\n",
        "epoch_metric = torch.mean(torch.stack(outs))\n",
        "``` \n",
        "\n",
        "* In the case that you need to make use of all the outputs from each `training_step()`, override the `on_train_epoch_end()` method.\n",
        "\\index{on\\_train\\_epoch\\_end()}\n",
        "\n",
        "```python\n",
        "class LightningTransformer(L.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = Transformer(vocab_size=vocab_size)\n",
        "        self.training_step_outputs = []\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, target = batch\n",
        "        output = self.model(inputs, target)\n",
        "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
        "        preds = ...\n",
        "        self.training_step_outputs.append(preds)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        all_preds = torch.stack(self.training_step_outputs)\n",
        "        # do something with all preds\n",
        "        ...\n",
        "        self.training_step_outputs.clear()  # free memory\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Validation Step\n",
        "\n",
        "#### Basics\n",
        "\n",
        "To activate the validation loop while training, override the `validation_step()` method.\n",
        "\n",
        "\\index{validation\\_step()}\n",
        "\n",
        "```python\n",
        "class LightningTransformer(L.LightningModule):\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, target = batch\n",
        "        output = self.model(inputs, target)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return loss\n",
        "``` \n",
        "\n",
        "#### Background\n",
        "\n",
        "* You can also run just the validation loop on your validation dataloaders by overriding `validation_step()` and calling `validate()`.\n",
        "\n",
        "\\index{validate()}\n",
        "\n",
        "```python\n",
        "model = LightningTransformer(vocab_size=dataset.vocab_size)\n",
        "trainer = L.Trainer()\n",
        "trainer.validate(model)\n",
        "```\n",
        "\n",
        "* In the case that you need to make use of all the outputs from each `validation_step()`, override the `on_validation_epoch_end()` method. Note that this method is called before `on_train_epoch_end()`.\n",
        "\n",
        "\\index{on\\_validation\\_epoch\\_end()}\n",
        "\n",
        "```python\n",
        "class LightningTransformer(L.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = Transformer(vocab_size=vocab_size)\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        inputs, target = batch\n",
        "        output = self.model(inputs, target)\n",
        "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
        "        pred = ...\n",
        "        self.validation_step_outputs.append(pred)\n",
        "        return pred\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        all_preds = torch.stack(self.validation_step_outputs)\n",
        "        # do something with all preds\n",
        "        ...\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "```\n",
        "\n",
        "### Test Step\n",
        "\n",
        "The process for enabling a test loop is the same as the process for enabling a validation loop.\n",
        "For this you need to override the `test_step()` method.\n",
        "The only difference is that the test loop is only called when `test()` is used.\n",
        "\\index{test\\_step()}\n",
        "\n",
        "```python\n",
        "def test_step(self, batch, batch_idx):\n",
        "    inputs, target = batch\n",
        "    output = self.model(inputs, target)\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    self.log(\"test_loss\", loss)\n",
        "    return loss\n",
        "```\n",
        "\n",
        "### Predict Step\n",
        "\n",
        "#### Basics\n",
        "\n",
        "By default, the `predict_step()` method runs the `forward()` method. In order to customize this behaviour, simply override the `predict_step()` method.\n",
        "\n",
        "\\index{predict\\_step()}\n",
        "\n",
        "```python\n",
        "class LightningTransformer(L.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = Transformer(vocab_size=vocab_size)\n",
        "\n",
        "    def predict_step(self, batch):\n",
        "        inputs, target = batch\n",
        "        return self.model(inputs, target)\n",
        "```\n",
        "\n",
        "#### Background\n",
        "\n",
        "* If you want to perform inference with the system, you can add a `forward` method to the LightningModule.\n",
        "* When using forward, you are responsible to call `eval()` and use the `no_grad()` context manager.\n",
        "\n",
        "```python\n",
        "class LightningTransformer(L.LightningModule):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = Transformer(vocab_size=vocab_size)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        inputs, target = batch\n",
        "        return self.model(inputs, target)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, target = batch\n",
        "        output = self.model(inputs, target)\n",
        "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.model.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "model = LightningTransformer(vocab_size=dataset.vocab_size)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch = dataloader.dataset[0]\n",
        "    pred = model(batch)\n",
        "```\n",
        "\n",
        "\n",
        "## Lightning Extras\n",
        "\n",
        "This section covers some additional features of Lightning.\n",
        "\n",
        "### Lightning: Save Hyperparameters\n",
        "\n",
        "Often times we train many versions of a model. You might share that model or come back to it a few months later at which point it is very useful to know how that model was trained (i.e.: what learning rate, neural network, etc.).\n",
        "\n",
        "Lightning has a standardized way of saving the information for you in checkpoints and YAML files. The goal here is to improve readability and reproducibility.\n",
        "\n",
        "\n",
        "Use `save_hyperparameters()` within your `LightningModule`’s `__init__` method.\n",
        "\\index{save\\_hyperparameters()}\n",
        "It will enable Lightning to store all the provided arguments under the `self.hparams` attribute.\n",
        "These hyperparameters will also be stored within the model checkpoint, which simplifies model re-instantiation after training.\n",
        "\n",
        "```python\n",
        "class LitMNIST(L.LightningModule):\n",
        "    def __init__(self, layer_1_dim=128, learning_rate=1e-2):\n",
        "        super().__init__()\n",
        "        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # equivalent\n",
        "        self.save_hyperparameters(\"layer_1_dim\", \"learning_rate\")\n",
        "\n",
        "        # Now possible to access layer_1_dim from hparams\n",
        "        self.hparams.layer_1_dim\n",
        "```\n",
        "\n",
        "### Lightning: Model Loading\n",
        "\n",
        "LightningModules that have hyperparameters automatically saved with `save_hyperparameters()` can conveniently be loaded and instantiated directly from a checkpoint with `load_from_checkpoint()`:\n",
        "\n",
        "\\index{load\\_from\\_checkpoint()}\n",
        "\n",
        "```python\n",
        "# to load specify the other args\n",
        "model = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Starter Example: Linear Neural Network\n",
        "\n",
        "\n",
        "We will use the `LightningModule` to create a simple neural network for regression.\n",
        "It will be implemented as the `LightningBasic` class.\n",
        "\n",
        "### Hidden Layers\n",
        "\n",
        "To specify the number of hidden layers, we will use the hyperparameter `l1` and the function `get_hidden_sizes()`  [[DOC]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/hyperparameters/architecture/#spotpython.hyperparameters.architecture.get_hidden_sizes) from the `spotpython` package.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: get_hidden_sizes\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
        "_L_in = 10\n",
        "l1 = 20\n",
        "max_n = 4\n",
        "get_hidden_sizes(_L_in, l1, max_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters\n",
        "\n",
        "The argument `l1` will be treated as a hyperparameter, so it will be tuned in the following steps.\n",
        "Besides `l1`, additonal hyperparameters are `act_fn` and `dropout_prob`.\n",
        "\n",
        " The arguments `_L_in`, `_L_out`, and `_torchmetric` are not hyperparameters, but are needed to create the network. The first two are specified by the data and the latter by user preferences (the desired evaluation metric).\n",
        "\n",
        "\n",
        "### The LightningBasic Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_example_regression\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "import lightning as L\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics.functional.regression\n",
        "from torch import nn\n",
        "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
        "\n",
        "class LightningBasic(L.LightningModule):\n",
        "    def __init__(\n",
        "    self,\n",
        "    l1: int,\n",
        "    act_fn: nn.Module,\n",
        "    dropout_prob: float,\n",
        "    _L_in: int,\n",
        "    _L_out: int,\n",
        "    _torchmetric: str,\n",
        "    *args,\n",
        "    **kwargs):\n",
        "        super().__init__()\n",
        "        self._L_in = _L_in\n",
        "        self._L_out = _L_out\n",
        "        self._torchmetric = _torchmetric\n",
        "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
        "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
        "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
        "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
        "        # set dummy input array for Tensorboard Graphs\n",
        "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
        "        hidden_sizes = get_hidden_sizes(_L_in=self._L_in, l1=l1, max_n=4)\n",
        "        # Create the network based on the specified hidden sizes\n",
        "        layers = []\n",
        "        layer_sizes = [self._L_in] + hidden_sizes\n",
        "        layer_size_last = layer_sizes[0]\n",
        "        for layer_size in layer_sizes[1:]:\n",
        "            layers += [\n",
        "                nn.Linear(layer_size_last, layer_size),\n",
        "                self.hparams.act_fn,\n",
        "                nn.Dropout(self.hparams.dropout_prob),\n",
        "            ]\n",
        "            layer_size_last = layer_size\n",
        "        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
        "        # nn.Sequential summarizes a list of modules into a single module,\n",
        "        # applying them in sequence\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def _calculate_loss(self, batch):\n",
        "        x, y = batch\n",
        "        y = y.view(len(y), 1)\n",
        "        y_hat = self.layers(x)\n",
        "        loss = self.metric(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(x)\n",
        "\n",
        "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
        "        loss = self._calculate_loss(batch)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: tuple) -> torch.Tensor:\n",
        "        loss = self._calculate_loss(batch)\n",
        "        # logs metrics for each training_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self._calculate_loss(batch)\n",
        "        # logs metrics for each training_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x, _ = batch\n",
        "        y_hat = self.layers(x)\n",
        "        return y_hat\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.layers.parameters(), lr=0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can instantiate the `LightningBasic` class as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_instantiate\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "model_base = LightningBasic(\n",
        "    l1=20,\n",
        "    act_fn=nn.ReLU(),\n",
        "    dropout_prob=0.01,\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    _torchmetric=\"mean_squared_error\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It has the following structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_print_model\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "print(model_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_model_architecture_plot\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "from spotpython.plot.xai import viz_net\n",
        "viz_net(net=model_base,\n",
        "    device=\"cpu\",\n",
        "    filename=\"model_architecture700\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Model architecture](./model_architecture700.png)\n",
        "\n",
        "### The Data Set: Diabetes\n",
        "\n",
        "We will use the `Diabetes` [[DOC]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/data/diabetes/) data set from the `spotpython` package, which is a PyTorch Dataset for regression based on a data set from `scikit-learn`.\n",
        "It consists of DataFrame entries, which were converted to PyTorch tensors.\n",
        "\n",
        "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
        "\n",
        "The `Diabetes` data set has the following properties:\n",
        "\n",
        "* Number of Instances: 442\n",
        "* Number of Attributes: First 10 columns are numeric predictive values.\n",
        "* Target: Column 11 is a quantitative measure of disease progression one year after baseline.\n",
        "* Attribute Information:\n",
        "    * age age in years\n",
        "    * sex\n",
        "    * bmi body mass index\n",
        "    * bp average blood pressure\n",
        "    * s1 tc, total serum cholesterol\n",
        "    * s2 ldl, low-density lipoproteins\n",
        "    * s3 hdl, high-density lipoproteins\n",
        "    * s4 tch, total cholesterol / HDL\n",
        "    * s5 ltg, possibly log of serum triglycerides level\n",
        "    * s6 glu, blood sugar level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_diabetes_dataset\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "import torch\n",
        "dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n",
        "# Set batch size for DataLoader to 2 for demonstration purposes\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The DataLoaders\n",
        "\n",
        "Before we can call the `Trainer` to fit, validate, and test the model, we need to create the `DataLoaders` for each of these steps. \n",
        "The `DataLoaders` are used to load the data into the model in batches and need the `batch_size`.\n",
        "\n",
        "\\index{DataLoaders}\n",
        "\\index{batch\\_size}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_dataloaders\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "import torch\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "dataset = Diabetes(target_type=torch.float)\n",
        "train1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\n",
        "train_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\n",
        "print(f\"Full Data Set: {len(dataset)}\")\n",
        "print(f\"Train Set: {len(train_set)}\")\n",
        "print(f\"Validation Set: {len(val_set)}\")\n",
        "print(f\"Test Set: {len(test_set)}\")\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Trainer\n",
        "\n",
        "Now we are ready to train the model. We will use the `Trainer` class from the `lightning` package.\n",
        "For demonstration purposes, we will train the model for 100 epochs only.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_train\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "epochs = 100\n",
        "\n",
        "trainer = L.Trainer(max_epochs=epochs, enable_progress_bar=True)\n",
        "trainer.fit(model=model_base, train_dataloaders=train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.validate(model_base, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_test\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "# automatically loads the best weights for you\n",
        "out = trainer.test(model_base, test_loader, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "yhat = trainer.predict(model_base, test_loader)\n",
        "# convert the list of tensors to a numpy array\n",
        "yhat = torch.cat(yhat).numpy()\n",
        "yhat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using a DataModule\n",
        "\n",
        "Instead of creating the three `DataLoaders` manually, we can use the `LightDataModule` class from the `spotpython` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_datamodule\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "dataset = Diabetes(target_type=torch.float)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.4)\n",
        "data_module.setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a minor difference in the sizes of the data sets due to the random split as can be seen in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_datamodule_print_sizes\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "print(f\"Full Data Set: {len(dataset)}\")\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `DataModule` can be used to train the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_train_datamodule\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "trainer = L.Trainer(max_epochs=epochs, enable_progress_bar=False)\n",
        "trainer.fit(model=model_base, datamodule=data_module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_validate_datamodule\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "trainer.validate(model=model_base, datamodule=data_module, verbose=True, ckpt_path=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_test_datamodule\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "trainer.test(model=model_base, datamodule=data_module, verbose=True, ckpt_path=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using spotpython with Pytorch Lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_imports_spotpython\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "import os\n",
        "from math import inf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
        "from spotpython.utils.eda import print_exp_table, print_res_table\n",
        "from spotpython.spot import Spot\n",
        "from spotpython.utils.file import get_experiment_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: lightning_starter_full_spot\n",
        "#| eval: true\n",
        "#| echo: true\n",
        "PREFIX=\"700\"\n",
        "data_set = Diabetes()\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    save_experiment=True,\n",
        "    fun_evals=inf,\n",
        "    fun_repeats=2,\n",
        "    max_time=1,\n",
        "    data_set=data_set,\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    tensorboard_log=True,\n",
        "    noise=True,\n",
        "    ocba_delta = 1,  )\n",
        "fun = HyperLight().fun\n",
        "from spotpython.hyperparameters.values import set_hyperparameter\n",
        "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
        "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
        "set_hyperparameter(fun_control, \"epochs\", [3,7])\n",
        "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
        "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
        "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
        "\n",
        "design_control = design_control_init(init_size=10, repeats=2)\n",
        "\n",
        "print_exp_table(fun_control)\n",
        "\n",
        "spot_tuner = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
        "res = spot_tuner.run()\n",
        "spot_tuner.plot_progress()\n",
        "print_res_table(spot_tuner)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}