{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Lightning Module\n",
    "\n",
    "This chapter implements a basic Pytorch Lightning module. It is based on the Lightning documentation [LIGHTNINGMODULE](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `LightningModule` organizes your `PyTorch` code into six sections:\n",
    "\n",
    "* Initialization (`__init__` and `setup()`).\n",
    "* Train Loop (`training_step()`)\n",
    "* Validation Loop (`validation_step()`)\n",
    "* Test Loop (`test_step()`)\n",
    "* Prediction Loop (`predict_step()`)\n",
    "* Optimizers and LR Schedulers (`configure_optimizers()`)\n",
    "\n",
    "A LightningModule is a torch.nn.Module but with added functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional.regression\n",
    "from torch import nn\n",
    "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
    "\n",
    "class LightningBasic(L.LightningModule):\n",
    "    def __init__(\n",
    "    self,\n",
    "    l1: int,\n",
    "    act_fn: nn.Module,\n",
    "    dropout_prob: float,\n",
    "    _L_in: int,\n",
    "    _L_out: int,\n",
    "    _torchmetric: str,\n",
    "    *args,\n",
    "    **kwargs):\n",
    "        super().__init__()\n",
    "        self._L_in = _L_in\n",
    "        self._L_out = _L_out\n",
    "        if _torchmetric is None:\n",
    "            _torchmetric = \"mean_squared_error\"\n",
    "        self._torchmetric = _torchmetric\n",
    "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
    "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
    "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
    "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
    "        # set dummy input array for Tensorboard Graphs\n",
    "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
    "        hidden_sizes = get_hidden_sizes(_L_in=self._L_in, l1=l1, n=10)\n",
    "        # Create the network based on the specified hidden sizes\n",
    "        layers = []\n",
    "        layer_sizes = [self._L_in] + hidden_sizes\n",
    "        layer_size_last = layer_sizes[0]\n",
    "        for layer_size in layer_sizes[1:]:\n",
    "            layers += [\n",
    "                nn.Linear(layer_size_last, layer_size),\n",
    "                self.hparams.act_fn,\n",
    "                nn.Dropout(self.hparams.dropout_prob),\n",
    "            ]\n",
    "            layer_size_last = layer_size\n",
    "        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
    "        # nn.Sequential summarizes a list of modules into a single module,\n",
    "        # applying them in sequence\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def _calculate_loss(self, batch):\n",
    "        x, y = batch\n",
    "        y = y.view(len(y), 1)\n",
    "        y_hat = self.layers(x)\n",
    "        loss = self.metric(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
    "        loss = self._calculate_loss(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: tuple) -> torch.Tensor:\n",
    "        loss = self._calculate_loss(batch)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, _ = batch\n",
    "        y_hat = self.layers(x)\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.layers.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = Diabetes(target_type=torch.float)\n",
    "print(f\"Full Data Set: {len(dataset)}\")\n",
    "train1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\n",
    "# print the size of the test set\n",
    "print(f\"Test Set: {len(test_set)}\")\n",
    "train_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\n",
    "print(f\"Train Set: {len(train_set)}\")\n",
    "print(f\"Validation Set: {len(val_set)}\")\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = LightningBasic(\n",
    "    l1=10,\n",
    "    act_fn=nn.ReLU(),\n",
    "    dropout_prob=0.01,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = L.Trainer(max_epochs=epochs, enable_progress_bar=True)\n",
    "trainer.fit(model=model_base, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(model_base, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically loads the best weights for you\n",
    "trainer.test(model_base, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = trainer.predict(model_base, test_loader)\n",
    "# convert the list of tensors to a numpy array\n",
    "yhat = torch.cat(yhat).numpy()\n",
    "yhat.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Methods\n",
    "\n",
    "### Train Epoch-level Operations\n",
    "\n",
    "* In the case that you need to make use of all the outputs from each training_step(), override the on_train_epoch_end() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration in spotpython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. spotpython.fun.hyperlight.HyperLight.fun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Hyperlight` provides the method `fun`, which takes `X` (`np.ndarray`) and `fun_control` (`dict`) as arguments.\n",
    "It calls the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_11_21_23_22_03\n",
      "Created spot_tensorboard_path: runs/spot_logs/000_p040025_2024-11-21_23-22-03 for SummaryWriter()\n",
      "module_name: light\n",
      "submodule_name: regression\n",
      "model_name: NNLinearRegressor\n",
      "| name           | type   | default   |   lower |   upper | transform             |\n",
      "|----------------|--------|-----------|---------|---------|-----------------------|\n",
      "| l1             | int    | 3         |     3   |    8    | transform_power_2_int |\n",
      "| epochs         | int    | 4         |     4   |    9    | transform_power_2_int |\n",
      "| batch_size     | int    | 4         |     1   |    4    | transform_power_2_int |\n",
      "| act_fn         | factor | ReLU      |     0   |    5    | None                  |\n",
      "| optimizer      | factor | SGD       |     0   |   11    | None                  |\n",
      "| dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |\n",
      "| lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |\n",
      "| patience       | int    | 2         |     2   |    6    | transform_power_2_int |\n",
      "| batch_norm     | factor | 0         |     0   |    1    | None                  |\n",
      "| initialization | factor | Default   |     0   |    4    | None                  |\n",
      "X: [[3.0e+00 4.0e+00 4.0e+00 2.0e+00 1.1e+01 1.0e-02 1.0e+00 2.0e+00 0.0e+00\n",
      "  0.0e+00]]\n",
      "X: [[3.0e+00 4.0e+00 4.0e+00 2.0e+00 1.1e+01 1.0e-02 1.0e+00 2.0e+00 0.0e+00\n",
      "  0.0e+00]\n",
      " [3.0e+00 4.0e+00 4.0e+00 2.0e+00 1.1e+01 1.0e-02 1.0e+00 2.0e+00 0.0e+00\n",
      "  0.0e+00]\n",
      " [3.0e+00 4.0e+00 4.0e+00 2.0e+00 1.1e+01 1.0e-02 1.0e+00 2.0e+00 0.0e+00\n",
      "  0.0e+00]]\n"
     ]
    }
   ],
   "source": [
    "from math import inf\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "\n",
    "PREFIX=\"000\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    seed=42,)\n",
    "\n",
    "print(gen_design_table(fun_control))\n",
    "\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# set epochs to 2^8:\n",
    "# X[0, 1] = 8\n",
    "# set patience to 2^10:\n",
    "# X[0, 7] = 10\n",
    "\n",
    "print(f\"X: {X}\")\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "# so that two values are returned\n",
    "X = np.vstack((X, X, X))\n",
    "print(f\"X: {X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 337577312.0, 'hp_metric': 337577312.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 942629568.0, 'hp_metric': 942629568.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 20527.919921875, 'hp_metric': 20527.919921875}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.37577312e+08, 9.42629568e+08, 2.05279199e+04])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hyper_light = HyperLight(seed=125, log_level=50)\n",
    "hyper_light.fun(X, fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the same seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 35292.05859375, 'hp_metric': 35292.05859375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 857364800.0, 'hp_metric': 857364800.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 106484024.0, 'hp_metric': 106484024.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.52920586e+04, 8.57364800e+08, 1.06484024e+08])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hyper_light = HyperLight(seed=125, log_level=50)\n",
    "hyper_light.fun(X, fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a different seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyper_light = HyperLight(seed=123, log_level=50)\n",
    "hyper_light.fun(X, fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. spotpython.light.trainmodel.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_11_21_23_24_44\n",
      "Created spot_tensorboard_path: runs/spot_logs/000_p040025_2024-11-21_23-24-44 for SummaryWriter()\n",
      "module_name: light\n",
      "submodule_name: regression\n",
      "model_name: NNLinearRegressor\n",
      "| name           | type   | default   |   lower |   upper | transform             |\n",
      "|----------------|--------|-----------|---------|---------|-----------------------|\n",
      "| l1             | int    | 3         |     3   |    8    | transform_power_2_int |\n",
      "| epochs         | int    | 4         |     4   |    9    | transform_power_2_int |\n",
      "| batch_size     | int    | 4         |     1   |    4    | transform_power_2_int |\n",
      "| act_fn         | factor | ReLU      |     0   |    5    | None                  |\n",
      "| optimizer      | factor | SGD       |     0   |   11    | None                  |\n",
      "| dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |\n",
      "| lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |\n",
      "| patience       | int    | 2         |     2   |    6    | transform_power_2_int |\n",
      "| batch_norm     | factor | 0         |     0   |    1    | None                  |\n",
      "| initialization | factor | Default   |     0   |    4    | None                  |\n",
      "X: [[3.0e+00 4.0e+00 4.0e+00 2.0e+00 1.1e+01 1.0e-02 1.0e+00 2.0e+00 0.0e+00\n",
      "  0.0e+00]]\n",
      "{'act_fn': ReLU(),\n",
      " 'batch_norm': False,\n",
      " 'batch_size': 16,\n",
      " 'dropout_prob': 0.01,\n",
      " 'epochs': 16,\n",
      " 'initialization': 'Default',\n",
      " 'l1': 8,\n",
      " 'lr_mult': 1.0,\n",
      " 'optimizer': 'SGD',\n",
      " 'patience': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 337577312.0, 'hp_metric': 337577312.0}\n",
      "{'act_fn': ReLU(),\n",
      " 'batch_norm': False,\n",
      " 'batch_size': 16,\n",
      " 'dropout_prob': 0.01,\n",
      " 'epochs': 16,\n",
      " 'initialization': 'Default',\n",
      " 'l1': 8,\n",
      " 'lr_mult': 1.0,\n",
      " 'optimizer': 'SGD',\n",
      " 'patience': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=16` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 942629568.0, 'hp_metric': 942629568.0}\n"
     ]
    }
   ],
   "source": [
    "from math import inf\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
    "from spotpython.light.trainmodel import train_model\n",
    "import pprint\n",
    "\n",
    "PREFIX=\"000\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    seed=42,)\n",
    "\n",
    "print(gen_design_table(fun_control))\n",
    "\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# set epochs to 2^8:\n",
    "# X[0, 1] = 8\n",
    "# set patience to 2^10:\n",
    "# X[0, 7] = 10\n",
    "\n",
    "print(f\"X: {X}\")\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "# so that two values are returned\n",
    "X = np.vstack((X, X))\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "    pprint.pprint(config)\n",
    "    y = train_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. trainer: fit and validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate the `config` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_11_21_23_25_42\n",
      "Created spot_tensorboard_path: runs/spot_logs/000_p040025_2024-11-21_23-25-42 for SummaryWriter()\n",
      "module_name: light\n",
      "submodule_name: regression\n",
      "model_name: NNLinearRegressor\n",
      "| name           | type   | default   |   lower |   upper | transform             |\n",
      "|----------------|--------|-----------|---------|---------|-----------------------|\n",
      "| l1             | int    | 3         |     3   |    8    | transform_power_2_int |\n",
      "| epochs         | int    | 4         |     4   |    9    | transform_power_2_int |\n",
      "| batch_size     | int    | 4         |     1   |    4    | transform_power_2_int |\n",
      "| act_fn         | factor | ReLU      |     0   |    5    | None                  |\n",
      "| optimizer      | factor | SGD       |     0   |   11    | None                  |\n",
      "| dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |\n",
      "| lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |\n",
      "| patience       | int    | 2         |     2   |    6    | transform_power_2_int |\n",
      "| batch_norm     | factor | 0         |     0   |    1    | None                  |\n",
      "| initialization | factor | Default   |     0   |    4    | None                  |\n",
      "X: [[3.0e+00 1.0e+01 4.0e+00 2.0e+00 1.1e+01 1.0e-02 1.0e+00 1.0e+01 0.0e+00\n",
      "  0.0e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l1': 8,\n",
       " 'epochs': 1024,\n",
       " 'batch_size': 16,\n",
       " 'act_fn': ReLU(),\n",
       " 'optimizer': 'SGD',\n",
       " 'dropout_prob': 0.01,\n",
       " 'lr_mult': 1.0,\n",
       " 'patience': 1024,\n",
       " 'batch_norm': False,\n",
       " 'initialization': 'Default'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import inf\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
    "from spotpython.light.trainmodel import train_model\n",
    "\n",
    "PREFIX=\"000\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    seed=42,)\n",
    "print(gen_design_table(fun_control))\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# set epochs to 2^8:\n",
    "X[0, 1] = 10\n",
    "# set patience to 2^10:\n",
    "X[0, 7] = 10\n",
    "print(f\"X: {X}\")\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 10\n",
    "_L_out = 1\n",
    "_L_cond = 0\n",
    "_torchmetric = \"mean_squared_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commented: Using the fun_control dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NNLinearRegressor(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.01, inplace=False)\n",
       "    (3): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.01, inplace=False)\n",
       "    (6): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.01, inplace=False)\n",
       "    (9): Linear(in_features=4, out_features=2, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.01, inplace=False)\n",
       "    (12): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.01, inplace=False)\n",
       "    (15): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (16): ReLU()\n",
       "    (17): Dropout(p=0.01, inplace=False)\n",
       "    (18): Linear(in_features=2, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
    "import torchmetrics.functional.regression\n",
    "import torch.optim as optim\n",
    "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
    "\n",
    "\n",
    "class NNLinearRegressor(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        l1: int,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        initialization: str,\n",
    "        act_fn: nn.Module,\n",
    "        optimizer: str,\n",
    "        dropout_prob: float,\n",
    "        lr_mult: float,\n",
    "        patience: int,\n",
    "        batch_norm: bool,\n",
    "        _L_in: int,\n",
    "        _L_out: int,\n",
    "        _torchmetric: str,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n",
    "        # checkpointing. It is recommended to ignore them\n",
    "        # using `self.save_hyperparameters(ignore=['act_fn'])`\n",
    "        # self.save_hyperparameters(ignore=[\"act_fn\"])\n",
    "        #\n",
    "        self._L_in = _L_in\n",
    "        self._L_out = _L_out\n",
    "        if _torchmetric is None:\n",
    "            _torchmetric = \"mean_squared_error\"\n",
    "        self._torchmetric = _torchmetric\n",
    "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
    "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
    "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
    "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
    "        # set dummy input array for Tensorboard Graphs\n",
    "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
    "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
    "        if self.hparams.l1 < 4:\n",
    "            raise ValueError(\"l1 must be at least 4\")\n",
    "        hidden_sizes = get_hidden_sizes(_L_in=self._L_in, l1=l1, n=10)\n",
    "\n",
    "        if batch_norm:\n",
    "            # Add batch normalization layers\n",
    "            layers = []\n",
    "            layer_sizes = [self._L_in] + hidden_sizes\n",
    "            for i in range(len(layer_sizes) - 1):\n",
    "                current_layer_size = layer_sizes[i]\n",
    "                next_layer_size = layer_sizes[i + 1]\n",
    "                layers += [\n",
    "                    nn.Linear(current_layer_size, next_layer_size),\n",
    "                    nn.BatchNorm1d(next_layer_size),\n",
    "                    self.hparams.act_fn,\n",
    "                    nn.Dropout(self.hparams.dropout_prob),\n",
    "                ]\n",
    "            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
    "        else:\n",
    "            layers = []\n",
    "            layer_sizes = [self._L_in] + hidden_sizes\n",
    "            for i in range(len(layer_sizes) - 1):\n",
    "                current_layer_size = layer_sizes[i]\n",
    "                next_layer_size = layer_sizes[i + 1]\n",
    "                layers += [\n",
    "                    nn.Linear(current_layer_size, next_layer_size),\n",
    "                    self.hparams.act_fn,\n",
    "                    nn.Dropout(self.hparams.dropout_prob),\n",
    "                ]\n",
    "            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
    "\n",
    "        # Wrap the layers into a sequential container\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialization (Xavier, Kaiming, or Default)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if self.hparams.initialization == \"xavier_uniform\":\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "            elif self.hparams.initialization == \"xavier_normal\":\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "            elif self.hparams.initialization == \"kaiming_uniform\":\n",
    "                nn.init.kaiming_uniform_(module.weight)\n",
    "            elif self.hparams.initialization == \"kaiming_normal\":\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "            else:  # \"Default\"\n",
    "                nn.init.uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A tensor containing a batch of input data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the output of the model.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def _calculate_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Calculate the loss for the given batch.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "            mode (str, optional): The mode of the model. Defaults to \"train\".\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y = y.view(len(y), 1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.metric(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single training step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "\n",
    "        \"\"\"\n",
    "        loss = self._calculate_loss(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "\n",
    "        \"\"\"\n",
    "        loss = self._calculate_loss(batch)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
    "        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single test step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the loss for this batch.\n",
    "        \"\"\"\n",
    "        loss = self._calculate_loss(batch)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
    "        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single prediction step.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing a batch of input data and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the prediction for this batch.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        yhat = self(x)\n",
    "        y = y.view(len(y), 1)\n",
    "        yhat = yhat.view(len(yhat), 1)\n",
    "        print(f\"Predict step x: {x}\")\n",
    "        print(f\"Predict step y: {y}\")\n",
    "        print(f\"Predict step y_hat: {yhat}\")\n",
    "        # pred_loss = F.mse_loss(y_hat, y)\n",
    "        # pred loss not registered\n",
    "        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n",
    "        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n",
    "        # MisconfigurationException: You are trying to `self.log()`\n",
    "        # but the loop's result collection is not registered yet.\n",
    "        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n",
    "        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n",
    "        return (x, y, yhat)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Configures the optimizer for the model.\n",
    "\n",
    "        Notes:\n",
    "            The default Lightning way is to define an optimizer as\n",
    "            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n",
    "            spotpython uses an optimizer handler to create the optimizer, which\n",
    "            adapts the learning rate according to the lr_mult hyperparameter as\n",
    "            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: The optimizer to use during training.\n",
    "\n",
    "        \"\"\"\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n",
    "\n",
    "        num_milestones = 3  # Number of milestones to divide the epochs\n",
    "        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNLinearRegressor(**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "\n",
    "data_set = Diabetes()\n",
    "dm = LightDataModule(\n",
    "    dataset=data_set,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    test_size=fun_control[\"test_size\"],\n",
    "    test_seed=fun_control[\"test_seed\"],\n",
    "    scaler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using `callbacks` for early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = True\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "if not timestamp:\n",
    "    # add ModelCheckpoint only if timestamp is False\n",
    "    callbacks.append(ModelCheckpoint(dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True))  # Save the last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.eda import generate_config_id\n",
    "if timestamp:\n",
    "    # config id is unique. Since the model is not loaded from a checkpoint,\n",
    "    # the config id is generated here with a timestamp.\n",
    "    config_id = generate_config_id(config, timestamp=True)\n",
    "else:\n",
    "    # config id is not time-dependent and therefore unique,\n",
    "    # so that the model can be loaded from a checkpoint,\n",
    "    # the config id is generated here without a timestamp.\n",
    "    config_id = generate_config_id(config, timestamp=False) + \"_TRAIN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import lightning as L\n",
    "import os\n",
    "trainer = L.Trainer(\n",
    "    # Where to save models\n",
    "    default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n",
    "    max_epochs=model.hparams.epochs,\n",
    "    accelerator=fun_control[\"accelerator\"],\n",
    "    devices=fun_control[\"devices\"],\n",
    "    strategy=fun_control[\"strategy\"],\n",
    "    num_nodes=fun_control[\"num_nodes\"],\n",
    "    precision=fun_control[\"precision\"],\n",
    "    logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n",
    "    callbacks=callbacks,\n",
    "    enable_progress_bar=False,\n",
    "    num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n",
    "    log_every_n_steps=fun_control[\"log_every_n_steps\"],\n",
    "    gradient_clip_val=None,\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=1024` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, datamodule=dm, ckpt_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">      Validate metric      </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         hp_metric         </span><span style=\"color: #800080; text-decoration-color: #800080\">     6561.97509765625      </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span><span style=\"color: #800080; text-decoration-color: #800080\">     6561.97509765625      </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m        hp_metric        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    6561.97509765625     \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    6561.97509765625     \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 6561.97509765625, 'hp_metric': 6561.97509765625}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model=model, datamodule=dm, verbose=True, ckpt_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate the `config` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import os\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
    "from spotpython.light.trainmodel import train_model, generate_config_id_with_timestamp\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "PREFIX=\"000\"\n",
    "data_set = Diabetes()\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    seed=42,)\n",
    "print(gen_design_table(fun_control))\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# set epochs to 2^8:\n",
    "X[0, 1] = 10\n",
    "# set patience to 2^10:\n",
    "X[0, 7] = 10\n",
    "print(f\"X: {X}\")\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_L_cond = fun_control[\"_L_cond\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n",
    "dm = LightDataModule(\n",
    "    dataset=fun_control[\"data_set\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=fun_control[\"num_workers\"],\n",
    "    test_size=fun_control[\"test_size\"],\n",
    "    test_seed=fun_control[\"test_seed\"],\n",
    "    scaler=fun_control[\"scaler\"],\n",
    ")\n",
    "config_id = generate_config_id_with_timestamp(config, timestamp=True)\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)]\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n",
    "    max_epochs=model.hparams.epochs,\n",
    "    accelerator=fun_control[\"accelerator\"],\n",
    "    devices=fun_control[\"devices\"],\n",
    "    strategy=fun_control[\"strategy\"],\n",
    "    num_nodes=fun_control[\"num_nodes\"],\n",
    "    precision=fun_control[\"precision\"],\n",
    "    logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n",
    "    callbacks=callbacks,\n",
    "    enable_progress_bar=False,\n",
    "    num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n",
    "    log_every_n_steps=fun_control[\"log_every_n_steps\"],\n",
    "    gradient_clip_val=None,\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    ")\n",
    "trainer.fit(model=model, datamodule=dm, ckpt_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode  | In sizes | Out sizes\n",
      "---------------------------------------------------------------------\n",
      "0 | layers | Sequential | 169    | train | [16, 10] | [16, 1]  \n",
      "---------------------------------------------------------------------\n",
      "169       Trainable params\n",
      "0         Non-trainable params\n",
      "169       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_name: light\n",
      "submodule_name: regression\n",
      "model_name: NNLinearRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=32` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">      Validate metric      </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         hp_metric         </span><span style=\"color: #800080; text-decoration-color: #800080\">        1054243.125        </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span><span style=\"color: #800080; text-decoration-color: #800080\">        1054243.125        </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       "\u001b[36m \u001b[0m\u001b[36m        hp_metric        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m       1054243.125       \u001b[0m\u001b[35m \u001b[0m\n",
       "\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m       1054243.125       \u001b[0m\u001b[35m \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1054243.125, 'hp_metric': 1054243.125}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import inf\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.utils.scaler import TorchStandardScaler\n",
    "PREFIX=\"000\"\n",
    "data_set = Diabetes()\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "X = np.array([[3.0e+00, 5.0, 4.0e+00, 2.0e+00, 1.1e+01, 1.0e-02, 1.0e+00, 1.0e+01, 0.0e+00,\n",
    "  0.0e+00]])\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n",
    "_torchmetric = \"mean_squared_error\"\n",
    "model = fun_control[\"core_model\"](**config, _L_in=10, _L_out=1, _L_cond=None, _torchmetric=_torchmetric)\n",
    "dm = LightDataModule(\n",
    "    dataset=data_set,\n",
    "    batch_size=16,\n",
    "    test_size=0.6,\n",
    "    scaler=TorchStandardScaler())\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=32,\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "trainer.fit(model=model, datamodule=dm, ckpt_path=None)\n",
    "trainer.validate(model=model, datamodule=dm, ckpt_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spot312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
