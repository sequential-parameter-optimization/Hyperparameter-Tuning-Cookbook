{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# `river` Hyperparameter Tuning: Hoeffding Tree Regressor with Friedman Drift Data\n"
      ],
      "id": "168b590e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| label: 024_imports\n",
        "import os\n",
        "from math import inf\n",
        "import numpy as np\n",
        "import warnings\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "imports",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This chapter demonstrates hyperparameter tuning for `river`'s `Hoeffding Tree Regressor (HTR)` with the Friedman drift data set [[SOURCE]](https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/). The `Hoeffding Tree Regressor` is a regression tree that uses the Hoeffding bound to limit the number of splits evaluated at each node, i.e., it predicts a real value for each sample.\n",
        "\n",
        "\n",
        "## The Friedman Drift Data Set {#sec-the-friedman-drift-data-set-24}\n",
        "\n",
        "We will use the Friedman synthetic dataset with concept drifts, which is described in detail in @sec-a-05-friedman.\n",
        "The following parameters are used to generate and handle the data set:\n",
        "\n",
        "* `position`: The positions of the concept drifts.\n",
        "* `n_train`: The number of samples used for training.\n",
        "* `n_test`: The number of samples used for testing.\n",
        "* `seed`: The seed for the random number generator.\n",
        "* `target_column`: The name of the target column.\n",
        "* `drift_type`: The type of the concept drift.\n",
        "\n",
        "We will use `spotRiver`'s `convert_to_df` function [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/utils/data_conversion.py) to convert the `river` data set to a `pandas` data frame.\n",
        "Then we add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column of the dataframe.\n",
        "\n",
        "This data generation is independently repeated for the training and test data sets, because the data sets are generated with concept drifts and the usual train-test split would not work.\n"
      ],
      "id": "1667c8c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_data_set\n",
        "\n",
        "from river.datasets import synth\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from spotRiver.utils.data_conversion import convert_to_df\n",
        "\n",
        "n_train = 6_000\n",
        "n_test = 4_000\n",
        "n_samples = n_train + n_test\n",
        "target_column = \"y\"\n",
        "\n",
        "dataset = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=(n_train/4, n_train/2),\n",
        "   seed=123\n",
        ")\n",
        "\n",
        "train = convert_to_df(dataset, n_total=n_train)\n",
        "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]"
      ],
      "id": "data_set",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_data_set_testing\n",
        "dataset = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=(n_test/4, n_test/2),\n",
        "   seed=123\n",
        ")\n",
        "test = convert_to_df(dataset, n_total=n_test)\n",
        "test.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]"
      ],
      "id": "data_set_testing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "### The Data Set\n",
        "\n",
        "Data sets that are available as `pandas` dataframes can easily be passed to the `spot` hyperparameter tuner.\n",
        "`spotpython` requires a `train` and a `test` data set, where the column names must be identical.\n",
        ":::\n",
        "\n",
        "We combine the train and test data sets and save them to a csv file.\n"
      ],
      "id": "63158aa0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_save_data\n",
        "#| eval: false\n",
        "df = pd.concat([train, test])\n",
        "df.to_csv(\"./userData/friedman.csv\", index=False)"
      ],
      "id": "save_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Friedman Drift data set described in this section is avaialble as a `csv` data file and can be downloaded from github: [friedman.csv](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userData/friedman.csv).\n",
        "\n",
        "\n",
        "## Setup{#sec-setup-24}\n",
        "\n",
        "### General Experiment Setup{#sec-general-experiment-setup-24}\n",
        "\n",
        "To keep track of the different experiments, we use a `PREFIX` for the experiment name. The `PREFIX` is used to create a unique experiment name. The `PREFIX` is also used to create a unique TensorBoard folder, which is used to store the TensorBoard log files.\n",
        "\n",
        "`spotpython` allows the specification of two different types of stopping criteria: first, the number of function evaluations (`fun_evals`), and second, the maximum run time in seconds (`max_time`). Here, we will set the number of function evaluations to infinity and the maximum run time to one minute.\n",
        "\n",
        "Furthermore, we set the initial design size (`init_size`) to 10. The initial design is used to train the surrogate model. The surrogate model is used to predict the performance of the hyperparameter configurations. The initial design is also used to train the first model. Since the `init_size` belongs to the experimental design, it is set in the `design_control` dictionary, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.design_control_init).\n",
        "\n",
        "`max_time` is set to one minute for demonstration purposes and `init_size` is set to 10 for demonstration purposes. For real experiments, these values should be increased.\n",
        "Note,  the total run time may exceed the specified `max_time`, because the initial design is always evaluated, even if this takes longer than `max_time`.\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "### Summary: General Experiment Setup\n",
        "\n",
        "The following parameters are used to specify the general experiment setup:\n"
      ],
      "id": "74f7e4d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_sum_exp\n",
        "PREFIX = \"024\"\n",
        "fun_evals = inf\n",
        "max_time = 1\n",
        "init_size = 10"
      ],
      "id": "sum_exp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### Data Setup\n",
        "\n",
        "We use the `StandardScaler` [[SOURCE]](https://riverml.xyz/dev/api/preprocessing/StandardScaler/) from `river` as the data-preprocessing model. The `StandardScaler` is used to standardize the data set, i.e., it has zero mean and unit variance.\n",
        "\n",
        "The names of the training and test data sets are `train` and `test`, respectively. They are available as `pandas` dataframes.\n",
        "Both must use the same column names. The column names were set to `x1` to `x10` for the features and `y` for the target column during the data set generation in @sec-the-friedman-drift-data-set-24. Therefore, the `target_column` is set to `y` (as above).\n",
        "\n",
        "::: {.callout-note}\n",
        "### Summary: Data Setup\n",
        "\n",
        "The following parameters are used to specify the data setup:\n"
      ],
      "id": "44ade218"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_sum_data\n",
        "prep_model_name = \"StandardScaler\"\n",
        "test = test\n",
        "train = train\n",
        "target_column = \"y\""
      ],
      "id": "sum_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Evaluation Setup\n",
        "\n",
        "Here we use the `mean_absolute_error` [[SOURCE]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) as the evaluation metric.\n",
        "Internally, this metric is passed to the objective (or loss) function `fun_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py) and further to the evaluation function `eval_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_bml.py).\n",
        "\n",
        "`spotRiver` also supports additional metrics. For example, the `metric_river` is used for the river based evaluation via `eval_oml_iter_progressive` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_oml.py). The `metric_river` is implemented to simulate the behaviour of the \"original\" `river` metrics.\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "### Summary: Evaluation Setup\n",
        "\n",
        "The following parameter are used to select the evaluation metric:\n"
      ],
      "id": "935a5df0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_sum_eval\n",
        "metric_sklearn_name = \"mean_absolute_error\""
      ],
      "id": "sum_eval",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### River-Specific Setup {#sec-river-specific-setup-24}\n",
        "\n",
        "In the online-machine-learning (OML) setup, the model is trained on a fixed number of observations and then evaluated on a fixed number of observations. The `horizon` defines the number of observations that are used for the evaluation. Here, a horizon of 7*24 is used, which corresponds to one week of data.\n",
        "\n",
        "The `oml_grace_period` defines the number of observations that are used for the initial training of the model.\n",
        "This value is relatively small, since the online-machine-learning is trained on the incoming data and the model is updated continuously.\n",
        "However, it needs a certain number of observations to start the training process.\n",
        "Therefore, this short training period aka `oml_grace_period` is set to the horizon, i.e., the number of observations that are used for the evaluation.\n",
        "In this case, we use a horizon of 7*24.\n",
        "\n",
        "The `weights` provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased. `spotRiver` stores information about the model' s score (metric), memory, and time. The hyperparamter tuner requires a single objective.\n",
        "Therefore, a weighted sum of the metric, memory, and time is computed. The weights are defined in the `weights` array.\n",
        "The `weights` provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased.\n",
        "\n",
        "The `weight_coeff` defines a multiplier for the results: results are multiplied by (step/n_steps)**weight_coeff, where n_steps is the total number of iterations.\n",
        "Results from the beginning have a lower weight than results from the end if weight_coeff > 1. If weight_coeff == 0, all results have equal weight. Note, that the `weight_coeff` is only used internally for the tuner and does not affect the results that are used for the evaluation or comparisons.\n",
        "\n",
        "::: {.callout-note}\n",
        "### Summary: River-Specific Setup\n",
        "\n",
        "The following parameters are used:\n"
      ],
      "id": "c3867072"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_sum_river-setup\n",
        "horizon = 7*24\n",
        "oml_grace_period = 7*24\n",
        "weights = np.array([1, 0.01, 0.01])\n",
        "weight_coeff = 0.0"
      ],
      "id": "sum_river-setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### Model Setup\n",
        "\n",
        "By using `core_model_name = \"tree.HoeffdingTreeRegressor\"`, the `river` model class `HoeffdingTreeRegressor` [[SOURCE]](https://riverml.xyz/dev/api/tree/HoeffdingTreeRegressor/) from the `tree` module is selected.\n",
        "For a given `core_model_name`, the corresponding hyperparameters are automatically loaded from the associated dictionary, which is stored as a JSON file. The JSON file contains hyperparameter type information, names, and bounds. For `river` models, the hyperparameters are stored in the `RiverHyperDict`, see [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json)\n",
        "\n",
        "Alternatively, you can load a local hyper_dict. Simply set `river_hyper_dict.json` as the filename. If `filename`is set to `None`, which is the default, the hyper_dict [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json) is loaded from the `spotRiver` package.\n",
        "\n",
        "How hyperparameter levels can be modified is described in @sec-modifying-hyperparameter-levels.\n",
        "\n",
        "::: {.callout-note}\n",
        "### Summary: Model Setup\n",
        "\n",
        "The following parameters are used for the model setup:\n"
      ],
      "id": "651c26fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_sum_model-setup\n",
        "from spotRiver.fun.hyperriver import HyperRiver\n",
        "from spotRiver.hyperdict.river_hyper_dict import RiverHyperDict\n",
        "core_model_name = \"tree.HoeffdingTreeRegressor\"\n",
        "hyperdict = RiverHyperDict"
      ],
      "id": "sum_model-setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Objective Function Setup\n",
        "\n",
        "The loss function (metric) values are passed to the objective function `fun_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py), which combines information about the loss, required memory and time as described in @sec-river-specific-setup-24.\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "### Summary: Objective Function Setup\n",
        "\n",
        "The following parameters are used:\n"
      ],
      "id": "65b35519"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_sum_fun-setup\n",
        "fun = HyperRiver().fun_oml_horizon"
      ],
      "id": "sum_fun-setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Surrogate Model Setup\n",
        "\n",
        "The default surrogate model is the `Kriging` model, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/build/kriging/). We specify `noise` as `True` to include noise in the model. An `anisotropic` kernel is used, which allows different length scales for each dimension, by setting `n_theta = 2`. Furthermore, the interval for the `Lambda` value is set to `[1e-3, 1e2]`.\n",
        "\n",
        "These parameters are set in the `surrogate_control` dictionary and therefore passed  to the `surrogate_control_init` function [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.surrogate_control_init).\n"
      ],
      "id": "37175c22"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_surrogate_control_setup\n",
        "noise = True\n",
        "n_theta = 2\n",
        "min_Lambda = 1e-3\n",
        "max_Lambda = 10"
      ],
      "id": "surrogate_control_setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### Summary: Setting up the Experiment {#sec-summary-setting-up-the-experiment-24}\n",
        "\n",
        "At this stage, all required information is available to set up the dictionaries for the hyperparameter tuning.\n",
        "Altogether, the `fun_control`, `design_control`, `surrogate_control`, and `optimize_control` dictionaries are initialized as follows:\n"
      ],
      "id": "3ca77b7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_summary_control\n",
        "from spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
        "\n",
        "fun = HyperRiver().fun_oml_horizon\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=\"024\",\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "\n",
        "    prep_model_name=\"StandardScaler\",\n",
        "    test=test,\n",
        "    train=train,\n",
        "    target_column=target_column,\n",
        "\n",
        "    metric_sklearn_name=\"mean_absolute_error\",\n",
        "    horizon=7*24,\n",
        "    oml_grace_period=7*24,\n",
        "    weight_coeff=0.0,\n",
        "    weights=np.array([1, 0.01, 0.01]),\n",
        "\n",
        "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
        "    hyperdict=RiverHyperDict,\n",
        "   )\n",
        "\n",
        "\n",
        "design_control = design_control_init(\n",
        "    init_size=10,\n",
        ")\n",
        "\n",
        "surrogate_control = surrogate_control_init(\n",
        "    noise=True,\n",
        "    n_theta=2,\n",
        "    min_Lambda=1e-3,\n",
        "    max_Lambda=10,\n",
        ")\n",
        "\n",
        "optimizer_control = optimizer_control_init()"
      ],
      "id": "summary_control",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the `Spot` Optimizer\n",
        "\n",
        "The class `Spot` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py) is the hyperparameter tuning workhorse. It is initialized with the following parameters, which were specified above.\n",
        "\n",
        "* `fun`: the objective function\n",
        "* `fun_control`: the dictionary with the control parameters for the objective function\n",
        "* `design_control`: the dictionary with the control parameters for the experimental design\n",
        "* `surrogate_control`: the dictionary with the control parameters for the surrogate model\n",
        "* `optimizer_control`: the dictionary with the control parameters for the optimizer\n",
        "\n",
        "`spotpython` allows maximum flexibility in the definition of the hyperparameter tuning setup. Alternative surrogate models, optimizers, and experimental designs can be used. Thus, interfaces for the `surrogate` model, experimental `design`, and `optimizer` are provided. The default surrogate model is the kriging model, the default optimizer is the differential evolution, and default experimental design is the Latin hypercube design.\n",
        "\n",
        "::: {.callout-note}\n",
        "### Summary: `Spot` Setup\n",
        "\n",
        "The following parameters are used for the `Spot` setup. These were specified above:\n"
      ],
      "id": "b6cba192"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_sum_spot-setup\n",
        "fun = fun\n",
        "fun_control = fun_control\n",
        "design_control = design_control\n",
        "surrogate_control = surrogate_control\n",
        "optimizer_control = optimizer_control"
      ],
      "id": "sum_spot-setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n"
      ],
      "id": "26da29c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_spot_run\n",
        "from spotPython.spot import spot\n",
        "spot_tuner = spot.Spot(\n",
        "    fun=fun,\n",
        "    fun_control=fun_control,\n",
        "    design_control=design_control,\n",
        "    surrogate_control=surrogate_control,\n",
        "    optimizer_control=optimizer_control,\n",
        ")\n",
        "res = spot_tuner.run()"
      ],
      "id": "spot_run",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the `spotgui`\n",
        "\n",
        "The `spotgui` [[github]](https://github.com/sequential-parameter-optimization/spotGUI) provides a convenient way to interact with the hyperparameter tuning process.\n",
        "To obtain the settings from @sec-summary-setting-up-the-experiment-24, the `spotgui` can be started as shown in @fig-spotgui.\n",
        "\n",
        "![spotgui](./figures_static/024_gui.png){width=100% #fig-spotgui}\n",
        "\n",
        "\n",
        "## Results\n",
        "\n",
        "After the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized with `spotpython`'s method `plot_progress`. The black points represent the performace values (score or metric) of  hyperparameter configurations from the initial design, whereas the red points represents the  hyperparameter configurations found by the surrogate model based optimization.\n"
      ],
      "id": "6bdac4c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_tuner.plot_progress(log_y=True, filename=None)"
      ],
      "id": "d6f073a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results can be printed in tabular form.\n"
      ],
      "id": "bf1b62da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ],
      "id": "31350897",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A histogram can be used to visualize the most important hyperparameters.\n"
      ],
      "id": "c05e98a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_tuner.plot_importance(threshold=10.0)"
      ],
      "id": "6dddddde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance of the Model with Default Hyperparameters\n",
        "\n",
        "### Get Default Hyperparameters and Fit the Model\n",
        "\n",
        "The default hyperparameters, which will be used for a comparion with the tuned hyperparameters, can be obtained with the following commands:\n"
      ],
      "id": "17a1b500"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n",
        "X_start = get_default_hyperparameters_as_array(fun_control)"
      ],
      "id": "69323c73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`spotPython` tunes numpy arrays, i.e., the hyperparameters are stored in a numpy array.\n"
      ],
      "id": "e5a7ea6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
        "model_default = get_one_core_model_from_X(X_start, fun_control, default=True)"
      ],
      "id": "7606d44a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the Model with Default Hyperparameters\n",
        "\n",
        "The model with the default hyperparameters can be trained and evaluated.\n",
        "The evaluation function `eval_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_bml.py) is the same function that was used for the hyperparameter tuning.\n",
        "During the hyperparameter tuning, the evaluation function was called from the objective (or loss) function `fun_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py).\n"
      ],
      "id": "e3108040"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_eval_default\n",
        "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
        "\n",
        "df_eval_default, df_true_default = eval_oml_horizon(\n",
        "                    model=model_default,\n",
        "                    train=fun_control[\"train\"],\n",
        "                    test=fun_control[\"test\"],\n",
        "                    target_column=fun_control[\"target_column\"],\n",
        "                    horizon=fun_control[\"horizon\"],\n",
        "                    oml_grace_period=fun_control[\"oml_grace_period\"],\n",
        "                    metric=fun_control[\"metric_sklearn\"],\n",
        "                )"
      ],
      "id": "eval_default",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The three performance criteria, i.e., score (metric), runtime, and memory consumption, can be visualized with the following commands:\n"
      ],
      "id": "576dfc5d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_plot_bml_oml_horizon_metrics_default\n",
        "from spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\n",
        "df_labels=[\"default\"]\n",
        "plot_bml_oml_horizon_metrics(df_eval = [df_eval_default], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])"
      ],
      "id": "plot_bml_oml_horizon_metrics_default",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show Predictions of the Model with Default Hyperparameters\n",
        "\n",
        "* Select a subset of the data set for the visualization of the predictions:\n",
        "    * We use the mean, $m$, of the data set as the center of the visualization.\n",
        "    * We use 100 data points, i.e., $m \\pm 50$ as the visualization window.\n"
      ],
      "id": "8f0105c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_plot_bml_oml_horizon_predictions_default\n",
        "m = fun_control[\"test\"].shape[0]\n",
        "a = int(m/2)-50\n",
        "b = int(m/2)+50\n",
        "plot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b]], target_column=target_column,  df_labels=df_labels)"
      ],
      "id": "plot_bml_oml_horizon_predictions_default",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get SPOT Results\n",
        "\n",
        "In a similar way, we can obtain the hyperparameters found by `spotPython`.\n"
      ],
      "id": "c05974c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_get_one_core_model_from_X\n",
        "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
        "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
        "model_spot = get_one_core_model_from_X(X, fun_control)"
      ],
      "id": "get_one_core_model_from_X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_eval_om_horizon\n",
        "df_eval_spot, df_true_spot = eval_oml_horizon(\n",
        "                    model=model_spot,\n",
        "                    train=fun_control[\"train\"],\n",
        "                    test=fun_control[\"test\"],\n",
        "                    target_column=fun_control[\"target_column\"],\n",
        "                    horizon=fun_control[\"horizon\"],\n",
        "                    oml_grace_period=fun_control[\"oml_grace_period\"],\n",
        "                    metric=fun_control[\"metric_sklearn\"],\n",
        "                )"
      ],
      "id": "eval_om_horizon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_plot_bml_oml_horizon_metrics\n",
        "df_labels=[\"default\", \"spot\"]\n",
        "plot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])"
      ],
      "id": "plot_bml_oml_horizon_metrics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_plot_bml_oml_horizon_predictions\n",
        "plot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b], df_true_spot[a:b]], target_column=target_column,  df_labels=df_labels)"
      ],
      "id": "plot_bml_oml_horizon_predictions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_plot_actual_vs_predicted\n",
        "from spotPython.plot.validation import plot_actual_vs_predicted\n",
        "plot_actual_vs_predicted(y_test=df_true_default[target_column], y_pred=df_true_default[\"Prediction\"], title=\"Default\")\n",
        "plot_actual_vs_predicted(y_test=df_true_spot[target_column], y_pred=df_true_spot[\"Prediction\"], title=\"SPOT\")"
      ],
      "id": "plot_actual_vs_predicted",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Regression Trees\n"
      ],
      "id": "4a516748"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_model_default_learn_one\n",
        "dataset_f = dataset.take(n_samples)\n",
        "print(f\"n_samples: {n_samples}\")\n",
        "for x, y in dataset_f:\n",
        "    model_default.learn_one(x, y)"
      ],
      "id": "model_default_learn_one",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-caution}\n",
        "### Caution: Large Trees\n",
        "* Since the trees are large, the visualization is suppressed by default.\n",
        "* To visualize the trees, uncomment the following line.\n",
        ":::\n"
      ],
      "id": "a8559410"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# model_default.draw()"
      ],
      "id": "8d0dde3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_model_default_summary\n",
        "model_default.summary"
      ],
      "id": "model_default_summary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spot Model\n"
      ],
      "id": "66b10548"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_model_spot_learn_one\n",
        "print(f\"n_samples: {n_samples}\")\n",
        "dataset_f = dataset.take(n_samples)\n",
        "for x, y in dataset_f:\n",
        "    model_spot.learn_one(x, y)"
      ],
      "id": "model_spot_learn_one",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-caution}\n",
        "### Caution: Large Trees\n",
        "* Since the trees are large, the visualization is suppressed by default.\n",
        "* To visualize the trees, uncomment the following line.\n",
        ":::\n"
      ],
      "id": "c5e6db32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# model_spot.draw()"
      ],
      "id": "8eeb823d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_model_spot_summary\n",
        "model_spot.summary"
      ],
      "id": "model_spot_summary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_compare_two_tree_models\n",
        "from spotPython.utils.eda import compare_two_tree_models\n",
        "print(compare_two_tree_models(model_default, model_spot))"
      ],
      "id": "compare_two_tree_models",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Hyperparameter Plots\n"
      ],
      "id": "03eecbd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_plot_important_hyperparameter_contour\n",
        "spot_tuner.plot_important_hyperparameter_contour(max_imp=3)"
      ],
      "id": "plot_important_hyperparameter_contour",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel Coordinates Plots\n"
      ],
      "id": "c5ba64e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 024_parallel_plot\n",
        "spot_tuner.parallel_plot()"
      ],
      "id": "parallel_plot",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spotCondaEnv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}