digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5456419072 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5277402352 [label=AddmmBackward0]
	5456175664 -> 5277402352
	5456417472 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5456417472 -> 5456175664
	5456175664 [label=AccumulateGrad]
	5456313456 -> 5277402352
	5456313456 [label=AddBackward0]
	5456313312 -> 5456313456
	5456313312 [label=MmBackward0]
	5456313552 -> 5456313312
	5456313552 [label=ReluBackward0]
	5208709568 -> 5456313552
	5208709568 [label=NativeLayerNormBackward0]
	5456313120 -> 5208709568
	5456313120 [label=MmBackward0]
	5456313072 -> 5456313120
	5456313072 [label=ReluBackward0]
	5456313168 -> 5456313072
	5456313168 [label=NativeLayerNormBackward0]
	5456313360 -> 5456313168
	5456313360 [label=AddBackward0]
	5456313984 -> 5456313360
	5456313984 [label=MmBackward0]
	5456314128 -> 5456313984
	5456314128 [label=ReluBackward0]
	5456314272 -> 5456314128
	5456314272 [label=NativeLayerNormBackward0]
	5456310192 -> 5456314272
	5456310192 [label=MmBackward0]
	5456312112 -> 5456310192
	5456312112 [label=ReluBackward0]
	5276732048 -> 5456312112
	5276732048 [label=NativeLayerNormBackward0]
	5456313840 -> 5276732048
	5456313840 [label=AddmmBackward0]
	5456593104 -> 5456313840
	5456416032 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5456416032 -> 5456593104
	5456593104 [label=AccumulateGrad]
	5456593056 -> 5456313840
	5456593056 [label=TBackward0]
	5456593200 -> 5456593056
	5456414272 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5456414272 -> 5456593200
	5456593200 [label=AccumulateGrad]
	5455862880 -> 5276732048
	4351893792 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4351893792 -> 5455862880
	5455862880 [label=AccumulateGrad]
	5455863792 -> 5276732048
	4351892992 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4351892992 -> 5455863792
	5455863792 [label=AccumulateGrad]
	5456310048 -> 5456310192
	5456310048 [label=TBackward0]
	5455870656 -> 5456310048
	5209573008 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5209573008 -> 5455870656
	5455870656 [label=AccumulateGrad]
	5456314320 -> 5456314272
	5209571568 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5209571568 -> 5456314320
	5456314320 [label=AccumulateGrad]
	5456314176 -> 5456314272
	5209575808 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5209575808 -> 5456314176
	5456314176 [label=AccumulateGrad]
	5456314080 -> 5456313984
	5456314080 [label=TBackward0]
	5456310240 -> 5456314080
	5209571728 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5209571728 -> 5456310240
	5456310240 [label=AccumulateGrad]
	5456313840 -> 5456313360
	5456313696 -> 5456313168
	5180761712 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5180761712 -> 5456313696
	5456313696 [label=AccumulateGrad]
	5456311680 -> 5456313168
	4351893712 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4351893712 -> 5456311680
	5456311680 [label=AccumulateGrad]
	5456312688 -> 5456313120
	5456312688 [label=TBackward0]
	5456314032 -> 5456312688
	5456414672 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5456414672 -> 5456314032
	5456314032 [label=AccumulateGrad]
	5456307216 -> 5208709568
	5456414752 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5456414752 -> 5456307216
	5456307216 [label=AccumulateGrad]
	5456312496 -> 5208709568
	5456413872 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5456413872 -> 5456312496
	5456312496 [label=AccumulateGrad]
	5456312832 -> 5456313312
	5456312832 [label=TBackward0]
	5456313216 -> 5456312832
	5456416112 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5456416112 -> 5456313216
	5456313216 [label=AccumulateGrad]
	5456313360 -> 5456313456
	5456313504 -> 5277402352
	5456313504 [label=TBackward0]
	5456312736 -> 5456313504
	5456415952 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5456415952 -> 5456312736
	5456312736 [label=AccumulateGrad]
	5277402352 -> 5456419072
}
