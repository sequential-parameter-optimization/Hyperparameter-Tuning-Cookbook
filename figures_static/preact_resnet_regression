digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4432828352 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5264072400 [label=AddmmBackward0]
	5264068080 -> 5264072400
	5289711312 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5289711312 -> 5264068080
	5264068080 [label=AccumulateGrad]
	5264067072 -> 5264072400
	5264067072 [label=AddBackward0]
	5289621872 -> 5264067072
	5289621872 [label=MmBackward0]
	5289621968 -> 5289621872
	5289621968 [label=ReluBackward0]
	5289622160 -> 5289621968
	5289622160 [label=NativeLayerNormBackward0]
	5289622256 -> 5289622160
	5289622256 [label=MmBackward0]
	5289621056 -> 5289622256
	5289621056 [label=ReluBackward0]
	5289622352 -> 5289621056
	5289622352 [label=NativeLayerNormBackward0]
	5289618560 -> 5289622352
	5289618560 [label=AddBackward0]
	5289622688 -> 5289618560
	5289622688 [label=MmBackward0]
	5289622832 -> 5289622688
	5289622832 [label=ReluBackward0]
	5289622976 -> 5289622832
	5289622976 [label=NativeLayerNormBackward0]
	5289623072 -> 5289622976
	5289623072 [label=MmBackward0]
	5289623264 -> 5289623072
	5289623264 [label=ReluBackward0]
	5289623408 -> 5289623264
	5289623408 [label=NativeLayerNormBackward0]
	5289622640 -> 5289623408
	5289622640 [label=AddmmBackward0]
	5289619184 -> 5289622640
	5289708912 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5289708912 -> 5289619184
	5289619184 [label=AccumulateGrad]
	5289619232 -> 5289622640
	5289619232 [label=TBackward0]
	5289618944 -> 5289619232
	5289709072 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5289709072 -> 5289618944
	5289618944 [label=AccumulateGrad]
	5289623504 -> 5289623408
	4398459840 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4398459840 -> 5289623504
	5289623504 [label=AccumulateGrad]
	5289623456 -> 5289623408
	5092409280 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	5092409280 -> 5289623456
	5289623456 [label=AccumulateGrad]
	5289623216 -> 5289623072
	5289623216 [label=TBackward0]
	5289619136 -> 5289623216
	4425952032 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4425952032 -> 5289619136
	5289619136 [label=AccumulateGrad]
	5289623024 -> 5289622976
	4398459760 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4398459760 -> 5289623024
	5289623024 [label=AccumulateGrad]
	5289622880 -> 5289622976
	4398459920 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4398459920 -> 5289622880
	5289622880 [label=AccumulateGrad]
	5289622784 -> 5289622688
	5289622784 [label=TBackward0]
	5289623312 -> 5289622784
	4498798336 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4498798336 -> 5289623312
	5289623312 [label=AccumulateGrad]
	5289622640 -> 5289618560
	5289622496 -> 5289622352
	5289709632 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5289709632 -> 5289622496
	5289622496 [label=AccumulateGrad]
	5289622448 -> 5289622352
	5289709712 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5289709712 -> 5289622448
	5289622448 [label=AccumulateGrad]
	5289621104 -> 5289622256
	5289621104 [label=TBackward0]
	5289622736 -> 5289621104
	5289711152 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5289711152 -> 5289622736
	5289622736 [label=AccumulateGrad]
	5289622208 -> 5289622160
	5289710752 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5289710752 -> 5289622208
	5289622208 [label=AccumulateGrad]
	5289622016 -> 5289622160
	5289711072 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5289711072 -> 5289622016
	5289622016 [label=AccumulateGrad]
	5289621920 -> 5289621872
	5289621920 [label=TBackward0]
	5289622064 -> 5289621920
	5289709392 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5289709392 -> 5289622064
	5289622064 [label=AccumulateGrad]
	5289618560 -> 5264067072
	5289619424 -> 5264072400
	5289619424 [label=TBackward0]
	5289621344 -> 5289619424
	5289711232 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5289711232 -> 5289621344
	5289621344 [label=AccumulateGrad]
	5264072400 -> 4432828352
}
