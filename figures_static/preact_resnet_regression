digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4645139024 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4647524992 [label=AddmmBackward0]
	4647722272 -> 4647524992
	4647827440 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4647827440 -> 4647722272
	4647722272 [label=AccumulateGrad]
	4647721888 -> 4647524992
	4647721888 [label=AddBackward0]
	4647722416 -> 4647721888
	4647722416 [label=MmBackward0]
	4647723376 -> 4647722416
	4647723376 [label=ReluBackward0]
	4647723520 -> 4647723376
	4647723520 [label=NativeLayerNormBackward0]
	4647722752 -> 4647723520
	4647722752 [label=MmBackward0]
	4647722896 -> 4647722752
	4647722896 [label=ReluBackward0]
	4647723712 -> 4647722896
	4647723712 [label=NativeLayerNormBackward0]
	4647722464 -> 4647723712
	4647722464 [label=AddBackward0]
	4647722080 -> 4647722464
	4647722080 [label=MmBackward0]
	4647724144 -> 4647722080
	4647724144 [label=ReluBackward0]
	4647724288 -> 4647724144
	4647724288 [label=NativeLayerNormBackward0]
	4647724384 -> 4647724288
	4647724384 [label=MmBackward0]
	4647724576 -> 4647724384
	4647724576 [label=ReluBackward0]
	4647724720 -> 4647724576
	4647724720 [label=NativeLayerNormBackward0]
	4647723952 -> 4647724720
	4647723952 [label=AddmmBackward0]
	4647724960 -> 4647723952
	4647826960 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4647826960 -> 4647724960
	4647724960 [label=AccumulateGrad]
	4647724912 -> 4647723952
	4647724912 [label=TBackward0]
	4647725056 -> 4647724912
	4647825520 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4647825520 -> 4647725056
	4647725056 [label=AccumulateGrad]
	4647724816 -> 4647724720
	4387063200 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4387063200 -> 4647724816
	4647724816 [label=AccumulateGrad]
	4647724768 -> 4647724720
	4387065120 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4387065120 -> 4647724768
	4647724768 [label=AccumulateGrad]
	4647724528 -> 4647724384
	4647724528 [label=TBackward0]
	4647725008 -> 4647724528
	4634947456 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4634947456 -> 4647725008
	4647725008 [label=AccumulateGrad]
	4647724336 -> 4647724288
	4645139504 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4645139504 -> 4647724336
	4647724336 [label=AccumulateGrad]
	4647724192 -> 4647724288
	4645139584 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4645139584 -> 4647724192
	4647724192 [label=AccumulateGrad]
	4647724096 -> 4647722080
	4647724096 [label=TBackward0]
	4647724624 -> 4647724096
	4647827040 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4647827040 -> 4647724624
	4647724624 [label=AccumulateGrad]
	4647723952 -> 4647722464
	4647723808 -> 4647723712
	4647824000 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4647824000 -> 4647723808
	4647723808 [label=AccumulateGrad]
	4647723760 -> 4647723712
	4647823520 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4647823520 -> 4647723760
	4647723760 [label=AccumulateGrad]
	4647722944 -> 4647722752
	4647722944 [label=TBackward0]
	4647724048 -> 4647722944
	4647825360 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4647825360 -> 4647724048
	4647724048 [label=AccumulateGrad]
	4647723568 -> 4647723520
	4647827120 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4647827120 -> 4647723568
	4647723568 [label=AccumulateGrad]
	4647723424 -> 4647723520
	4647827200 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4647827200 -> 4647723424
	4647723424 [label=AccumulateGrad]
	4647723328 -> 4647722416
	4647723328 [label=TBackward0]
	4647722848 -> 4647723328
	4647827280 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4647827280 -> 4647722848
	4647722848 [label=AccumulateGrad]
	4647722464 -> 4647721888
	4647722608 -> 4647524992
	4647722608 [label=TBackward0]
	4647722704 -> 4647722608
	4647827360 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4647827360 -> 4647722704
	4647722704 [label=AccumulateGrad]
	4647524992 -> 4645139024
}
