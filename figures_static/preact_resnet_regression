digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4610204896 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4607608528 [label=AddmmBackward0]
	4435911792 -> 4607608528
	4610202416 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4610202416 -> 4435911792
	4435911792 [label=AccumulateGrad]
	4607608288 -> 4607608528
	4607608288 [label=AddBackward0]
	4607608576 -> 4607608288
	4607608576 [label=MmBackward0]
	4607610832 -> 4607608576
	4607610832 [label=ReluBackward0]
	4607616112 -> 4607610832
	4607616112 [label=NativeLayerNormBackward0]
	4607610208 -> 4607616112
	4607610208 [label=MmBackward0]
	4607151696 -> 4607610208
	4607151696 [label=ReluBackward0]
	4610080528 -> 4607151696
	4610080528 [label=NativeLayerNormBackward0]
	4607608480 -> 4610080528
	4607608480 [label=AddBackward0]
	4610077456 -> 4607608480
	4610077456 [label=MmBackward0]
	4610077648 -> 4610077456
	4610077648 [label=ReluBackward0]
	4610078608 -> 4610077648
	4610078608 [label=NativeLayerNormBackward0]
	4610359552 -> 4610078608
	4610359552 [label=MmBackward0]
	4610359744 -> 4610359552
	4610359744 [label=ReluBackward0]
	4610359888 -> 4610359744
	4610359888 [label=NativeLayerNormBackward0]
	4610077840 -> 4610359888
	4610077840 [label=AddmmBackward0]
	4610360128 -> 4610077840
	4607549168 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4607549168 -> 4610360128
	4610360128 [label=AccumulateGrad]
	4610360080 -> 4610077840
	4610360080 [label=TBackward0]
	4610360224 -> 4610360080
	4610200976 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4610200976 -> 4610360224
	4610360224 [label=AccumulateGrad]
	4610359984 -> 4610359888
	4365671856 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4365671856 -> 4610359984
	4610359984 [label=AccumulateGrad]
	4610359936 -> 4610359888
	4346177536 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4346177536 -> 4610359936
	4610359936 [label=AccumulateGrad]
	4610359696 -> 4610359552
	4610359696 [label=TBackward0]
	4610360176 -> 4610359696
	4346177456 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4346177456 -> 4610360176
	4610360176 [label=AccumulateGrad]
	4610359456 -> 4610078608
	4607549568 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4607549568 -> 4610359456
	4610359456 [label=AccumulateGrad]
	4610359408 -> 4610078608
	4607545488 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4607545488 -> 4610359408
	4610359408 [label=AccumulateGrad]
	4610076592 -> 4610077456
	4610076592 [label=TBackward0]
	4610359792 -> 4610076592
	4607549488 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4607549488 -> 4610359792
	4610359792 [label=AccumulateGrad]
	4610077840 -> 4607608480
	4610078560 -> 4610080528
	4607549248 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4607549248 -> 4610078560
	4610078560 [label=AccumulateGrad]
	4610076400 -> 4610080528
	4610200496 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4610200496 -> 4610076400
	4610076400 [label=AccumulateGrad]
	4607904688 -> 4607610208
	4607904688 [label=TBackward0]
	4610077888 -> 4607904688
	4610202096 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4610202096 -> 4610077888
	4610077888 [label=AccumulateGrad]
	4607610928 -> 4607616112
	4610200336 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4610200336 -> 4607610928
	4607610928 [label=AccumulateGrad]
	4607610784 -> 4607616112
	4610202256 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4610202256 -> 4607610784
	4607610784 [label=AccumulateGrad]
	4607611168 -> 4607608576
	4607611168 [label=TBackward0]
	4607609872 -> 4607611168
	4610202176 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4610202176 -> 4607609872
	4607609872 [label=AccumulateGrad]
	4607608480 -> 4607608288
	4607610544 -> 4607608528
	4607610544 [label=TBackward0]
	4607609248 -> 4607610544
	4610202336 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4610202336 -> 4607609248
	4607609248 [label=AccumulateGrad]
	4607608528 -> 4610204896
}
