digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5231619968 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5231595376 [label=AddmmBackward0]
	5231597296 -> 5231595376
	5231614848 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5231614848 -> 5231597296
	5231597296 [label=AccumulateGrad]
	5231596672 -> 5231595376
	5231596672 [label=AddBackward0]
	5231596480 -> 5231596672
	5231596480 [label=MmBackward0]
	5231594224 -> 5231596480
	5231594224 [label=ReluBackward0]
	5231595328 -> 5231594224
	5231595328 [label=NativeLayerNormBackward0]
	5226683872 -> 5231595328
	5226683872 [label=MmBackward0]
	5231594176 -> 5226683872
	5231594176 [label=ReluBackward0]
	5231594080 -> 5231594176
	5231594080 [label=NativeLayerNormBackward0]
	5231597440 -> 5231594080
	5231597440 [label=AddBackward0]
	5231595712 -> 5231597440
	5231595712 [label=MmBackward0]
	5231595520 -> 5231595712
	5231595520 [label=ReluBackward0]
	5231597632 -> 5231595520
	5231597632 [label=NativeLayerNormBackward0]
	5231597728 -> 5231597632
	5231597728 [label=MmBackward0]
	5231597920 -> 5231597728
	5231597920 [label=ReluBackward0]
	5231598064 -> 5231597920
	5231598064 [label=NativeLayerNormBackward0]
	5231595760 -> 5231598064
	5231595760 [label=AddmmBackward0]
	5231598304 -> 5231595760
	5231616928 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5231616928 -> 5231598304
	5231598304 [label=AccumulateGrad]
	5231598256 -> 5231595760
	5231598256 [label=TBackward0]
	5231598400 -> 5231598256
	5231616608 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5231616608 -> 5231598400
	5231598400 [label=AccumulateGrad]
	5231598160 -> 5231598064
	4374943520 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4374943520 -> 5231598160
	5231598160 [label=AccumulateGrad]
	5231598112 -> 5231598064
	4374943200 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4374943200 -> 5231598112
	5231598112 [label=AccumulateGrad]
	5231597872 -> 5231597728
	5231597872 [label=TBackward0]
	5231598352 -> 5231597872
	5231618448 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5231618448 -> 5231598352
	5231598352 [label=AccumulateGrad]
	5231597680 -> 5231597632
	5231618608 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5231618608 -> 5231597680
	5231597680 [label=AccumulateGrad]
	5231597536 -> 5231597632
	5231618768 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5231618768 -> 5231597536
	5231597536 [label=AccumulateGrad]
	5231595568 -> 5231595712
	5231595568 [label=TBackward0]
	5231597968 -> 5231595568
	5231618688 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5231618688 -> 5231597968
	5231597968 [label=AccumulateGrad]
	5231595760 -> 5231597440
	5231595904 -> 5231594080
	5231616128 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5231616128 -> 5231595904
	5231595904 [label=AccumulateGrad]
	5231595952 -> 5231594080
	5231618848 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5231618848 -> 5231595952
	5231595952 [label=AccumulateGrad]
	5231594992 -> 5226683872
	5231594992 [label=TBackward0]
	5231595664 -> 5231594992
	5231618528 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5231618528 -> 5231595664
	5231595664 [label=AccumulateGrad]
	5231595280 -> 5231595328
	5229248992 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5229248992 -> 5231595280
	5231595280 [label=AccumulateGrad]
	5231596240 -> 5231595328
	5229259792 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5229259792 -> 5231596240
	5231596240 [label=AccumulateGrad]
	5231594272 -> 5231596480
	5231594272 [label=TBackward0]
	5231596192 -> 5231594272
	5229259872 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5229259872 -> 5231596192
	5231596192 [label=AccumulateGrad]
	5231597440 -> 5231596672
	5231597392 -> 5231595376
	5231597392 [label=TBackward0]
	5231594896 -> 5231597392
	4608152336 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4608152336 -> 5231594896
	5231594896 [label=AccumulateGrad]
	5231595376 -> 5231619968
}
