digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5206566752 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5206598048 [label=AddmmBackward0]
	5206599632 -> 5206598048
	5218597008 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5218597008 -> 5206599632
	5206599632 [label=AccumulateGrad]
	5206592672 -> 5206598048
	5206592672 [label=AddBackward0]
	5187216544 -> 5206592672
	5187216544 [label=MmBackward0]
	5218618720 -> 5187216544
	5218618720 [label=ReluBackward0]
	5218616320 -> 5218618720
	5218616320 [label=NativeLayerNormBackward0]
	5218616704 -> 5218616320
	5218616704 [label=MmBackward0]
	5218617616 -> 5218616704
	5218617616 [label=ReluBackward0]
	5218616752 -> 5218617616
	5218616752 [label=NativeLayerNormBackward0]
	5218618336 -> 5218616752
	5218618336 [label=AddBackward0]
	5218618960 -> 5218618336
	5218618960 [label=MmBackward0]
	5218619200 -> 5218618960
	5218619200 [label=ReluBackward0]
	5218619392 -> 5218619200
	5218619392 [label=NativeLayerNormBackward0]
	5218619488 -> 5218619392
	5218619488 [label=MmBackward0]
	5218619680 -> 5218619488
	5218619680 [label=ReluBackward0]
	5218619824 -> 5218619680
	5218619824 [label=NativeLayerNormBackward0]
	5218617520 -> 5218619824
	5218617520 [label=AddmmBackward0]
	5218620064 -> 5218617520
	5218595408 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5218595408 -> 5218620064
	5218620064 [label=AccumulateGrad]
	5218620016 -> 5218617520
	5218620016 [label=TBackward0]
	5218620160 -> 5218620016
	5218593408 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5218593408 -> 5218620160
	5218620160 [label=AccumulateGrad]
	5218619920 -> 5218619824
	5206570592 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	5206570592 -> 5218619920
	5218619920 [label=AccumulateGrad]
	5218619872 -> 5218619824
	5206566512 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	5206566512 -> 5218619872
	5218619872 [label=AccumulateGrad]
	5218619632 -> 5218619488
	5218619632 [label=TBackward0]
	5218620112 -> 5218619632
	5206566672 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5206566672 -> 5218620112
	5218620112 [label=AccumulateGrad]
	5218619440 -> 5218619392
	5206570992 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5206570992 -> 5218619440
	5218619440 [label=AccumulateGrad]
	5218619296 -> 5218619392
	5206566432 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5206566432 -> 5218619296
	5218619296 [label=AccumulateGrad]
	5218619152 -> 5218618960
	5218619152 [label=TBackward0]
	5218619728 -> 5218619152
	4358506688 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4358506688 -> 5218619728
	5218619728 [label=AccumulateGrad]
	5218617520 -> 5218618336
	5218617424 -> 5218616752
	4358507488 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4358507488 -> 5218617424
	5218617424 [label=AccumulateGrad]
	5218618624 -> 5218616752
	4358507568 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4358507568 -> 5218618624
	5218618624 [label=AccumulateGrad]
	5218617568 -> 5218616704
	5218617568 [label=TBackward0]
	5218619056 -> 5218617568
	5218593888 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5218593888 -> 5218619056
	5218619056 [label=AccumulateGrad]
	5218616992 -> 5218616320
	5218595248 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5218595248 -> 5218616992
	5218616992 [label=AccumulateGrad]
	5218618672 -> 5218616320
	5218595328 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5218595328 -> 5218618672
	5218618672 [label=AccumulateGrad]
	5218618768 -> 5187216544
	5218618768 [label=TBackward0]
	5218618528 -> 5218618768
	5218595488 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5218595488 -> 5218618528
	5218618528 [label=AccumulateGrad]
	5218618336 -> 5206592672
	5206598192 -> 5206598048
	5206598192 [label=TBackward0]
	5218618864 -> 5206598192
	5218595568 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5218595568 -> 5218618864
	5218618864 [label=AccumulateGrad]
	5206598048 -> 5206566752
}
