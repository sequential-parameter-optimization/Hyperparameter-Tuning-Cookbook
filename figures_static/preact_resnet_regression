digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4642006400 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4644415824 [label=AddmmBackward0]
	4643326560 -> 4644415824
	4644659808 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4644659808 -> 4643326560
	4643326560 [label=AccumulateGrad]
	4643335728 -> 4644415824
	4643335728 [label=AddBackward0]
	4644536224 -> 4643335728
	4644536224 [label=MmBackward0]
	4644533728 -> 4644536224
	4644533728 [label=ReluBackward0]
	4644535888 -> 4644533728
	4644535888 [label=NativeLayerNormBackward0]
	4644536272 -> 4644535888
	4644536272 [label=MmBackward0]
	4644534640 -> 4644536272
	4644534640 [label=ReluBackward0]
	4590154272 -> 4644534640
	4590154272 [label=NativeLayerNormBackward0]
	4644535840 -> 4590154272
	4644535840 [label=AddBackward0]
	4644815152 -> 4644535840
	4644815152 [label=MmBackward0]
	4644815296 -> 4644815152
	4644815296 [label=ReluBackward0]
	4644815440 -> 4644815296
	4644815440 [label=NativeLayerNormBackward0]
	4644815536 -> 4644815440
	4644815536 [label=MmBackward0]
	4644815728 -> 4644815536
	4644815728 [label=ReluBackward0]
	4644815872 -> 4644815728
	4644815872 [label=NativeLayerNormBackward0]
	4644815104 -> 4644815872
	4644815104 [label=AddmmBackward0]
	4644816112 -> 4644815104
	4644659408 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4644659408 -> 4644816112
	4644816112 [label=AccumulateGrad]
	4644816064 -> 4644815104
	4644816064 [label=TBackward0]
	4644816208 -> 4644816064
	4644659488 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4644659488 -> 4644816208
	4644816208 [label=AccumulateGrad]
	4644815968 -> 4644815872
	4380946144 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4380946144 -> 4644815968
	4644815968 [label=AccumulateGrad]
	4644815920 -> 4644815872
	4380946304 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4380946304 -> 4644815920
	4644815920 [label=AccumulateGrad]
	4644815680 -> 4644815536
	4644815680 [label=TBackward0]
	4644816160 -> 4644815680
	4642006720 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4642006720 -> 4644816160
	4644816160 [label=AccumulateGrad]
	4644815488 -> 4644815440
	4642005840 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4642005840 -> 4644815488
	4644815488 [label=AccumulateGrad]
	4644815344 -> 4644815440
	4642006640 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4642006640 -> 4644815344
	4644815344 [label=AccumulateGrad]
	4644815248 -> 4644815152
	4644815248 [label=TBackward0]
	4644815776 -> 4644815248
	4642006560 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4642006560 -> 4644815776
	4644815776 [label=AccumulateGrad]
	4644815104 -> 4644535840
	4644535408 -> 4590154272
	4633114288 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4633114288 -> 4644535408
	4644535408 [label=AccumulateGrad]
	4644815008 -> 4590154272
	4644659328 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4644659328 -> 4644815008
	4644815008 [label=AccumulateGrad]
	4644532432 -> 4644536272
	4644532432 [label=TBackward0]
	4644532096 -> 4644532432
	4644657568 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4644657568 -> 4644532096
	4644532096 [label=AccumulateGrad]
	4644535936 -> 4644535888
	4644657488 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4644657488 -> 4644535936
	4644535936 [label=AccumulateGrad]
	4644533680 -> 4644535888
	4644659648 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4644659648 -> 4644533680
	4644533680 [label=AccumulateGrad]
	4644535552 -> 4644536224
	4644535552 [label=TBackward0]
	4644535792 -> 4644535552
	4644659568 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4644659568 -> 4644535792
	4644535792 [label=AccumulateGrad]
	4644535840 -> 4643335728
	4643337120 -> 4644415824
	4643337120 [label=TBackward0]
	4644532624 -> 4643337120
	4644659728 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4644659728 -> 4644532624
	4644532624 [label=AccumulateGrad]
	4644415824 -> 4642006400
}
