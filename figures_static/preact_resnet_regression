digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4670991536 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4670864736 [label=AddmmBackward0]
	4670864688 -> 4670864736
	4670990976 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4670990976 -> 4670864688
	4670864688 [label=AccumulateGrad]
	4670862912 -> 4670864736
	4670862912 [label=AddBackward0]
	4670864976 -> 4670862912
	4670864976 [label=MmBackward0]
	4670864544 -> 4670864976
	4670864544 [label=ReluBackward0]
	4670858400 -> 4670864544
	4670858400 [label=NativeLayerNormBackward0]
	4670862384 -> 4670858400
	4670862384 [label=MmBackward0]
	4668410800 -> 4670862384
	4668410800 [label=ReluBackward0]
	4668411088 -> 4668410800
	4668411088 [label=NativeLayerNormBackward0]
	4670864928 -> 4668411088
	4670864928 [label=AddBackward0]
	4671144432 -> 4670864928
	4671144432 [label=MmBackward0]
	4671144528 -> 4671144432
	4671144528 [label=ReluBackward0]
	4671144672 -> 4671144528
	4671144672 [label=NativeLayerNormBackward0]
	4671144768 -> 4671144672
	4671144768 [label=MmBackward0]
	4671144960 -> 4671144768
	4671144960 [label=ReluBackward0]
	4671145104 -> 4671144960
	4671145104 [label=NativeLayerNormBackward0]
	4671144048 -> 4671145104
	4671144048 [label=AddmmBackward0]
	4671145344 -> 4671144048
	4670989216 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4670989216 -> 4671145344
	4671145344 [label=AccumulateGrad]
	4671145296 -> 4671144048
	4671145296 [label=TBackward0]
	4669199968 -> 4671145296
	4670989296 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4670989296 -> 4669199968
	4669199968 [label=AccumulateGrad]
	4671145200 -> 4671145104
	4392284352 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4392284352 -> 4671145200
	4671145200 [label=AccumulateGrad]
	4671145152 -> 4671145104
	4392285152 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4392285152 -> 4671145152
	4671145152 [label=AccumulateGrad]
	4671144912 -> 4671144768
	4671144912 [label=TBackward0]
	4671145392 -> 4671144912
	4668332208 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4668332208 -> 4671145392
	4671145392 [label=AccumulateGrad]
	4671144720 -> 4671144672
	4668336528 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4668336528 -> 4671144720
	4671144720 [label=AccumulateGrad]
	4671144576 -> 4671144672
	4668333568 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4668333568 -> 4671144576
	4671144576 [label=AccumulateGrad]
	4671144480 -> 4671144432
	4671144480 [label=TBackward0]
	4671145008 -> 4671144480
	4668336368 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4668336368 -> 4671145008
	4671145008 [label=AccumulateGrad]
	4671144048 -> 4670864928
	4671144192 -> 4668411088
	4670986976 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4670986976 -> 4671144192
	4671144192 [label=AccumulateGrad]
	4671144240 -> 4668411088
	4670987856 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4670987856 -> 4671144240
	4671144240 [label=AccumulateGrad]
	4668410992 -> 4670862384
	4668410992 [label=TBackward0]
	4668412720 -> 4668410992
	4670987936 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4670987936 -> 4668412720
	4668412720 [label=AccumulateGrad]
	4670864448 -> 4670858400
	4670988976 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4670988976 -> 4670864448
	4670864448 [label=AccumulateGrad]
	4668410896 -> 4670858400
	4670989456 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4670989456 -> 4668410896
	4668410896 [label=AccumulateGrad]
	4670864496 -> 4670864976
	4670864496 [label=TBackward0]
	4668409120 -> 4670864496
	4670989376 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4670989376 -> 4668409120
	4668409120 [label=AccumulateGrad]
	4670864928 -> 4670862912
	4670860848 -> 4670864736
	4670860848 [label=TBackward0]
	4668410848 -> 4670860848
	4670990896 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4670990896 -> 4668410848
	4668410848 [label=AccumulateGrad]
	4670864736 -> 4670991536
}
