digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4690390912 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4689491328 [label=AddmmBackward0]
	4690374000 -> 4689491328
	4690393552 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4690393552 -> 4690374000
	4690374000 [label=AccumulateGrad]
	4690373952 -> 4689491328
	4690373952 [label=AddBackward0]
	4689908672 -> 4690373952
	4689908672 [label=MmBackward0]
	4690374288 -> 4689908672
	4690374288 [label=ReluBackward0]
	4690373520 -> 4690374288
	4690373520 [label=NativeLayerNormBackward0]
	4690371552 -> 4690373520
	4690371552 [label=MmBackward0]
	4690371408 -> 4690371552
	4690371408 [label=ReluBackward0]
	4690374720 -> 4690371408
	4690374720 [label=NativeLayerNormBackward0]
	4690374192 -> 4690374720
	4690374192 [label=AddBackward0]
	4690374960 -> 4690374192
	4690374960 [label=MmBackward0]
	4690375104 -> 4690374960
	4690375104 [label=ReluBackward0]
	4690375248 -> 4690375104
	4690375248 [label=NativeLayerNormBackward0]
	4690375344 -> 4690375248
	4690375344 [label=MmBackward0]
	4690375536 -> 4690375344
	4690375536 [label=ReluBackward0]
	4690375680 -> 4690375536
	4690375680 [label=NativeLayerNormBackward0]
	4690375008 -> 4690375680
	4690375008 [label=AddmmBackward0]
	4690375920 -> 4690375008
	4690390272 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4690390272 -> 4690375920
	4690375920 [label=AccumulateGrad]
	4690375968 -> 4690375008
	4690375968 [label=TBackward0]
	4687345456 -> 4690375968
	4690390992 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4690390992 -> 4687345456
	4687345456 [label=AccumulateGrad]
	4690375776 -> 4690375680
	4328871712 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4328871712 -> 4690375776
	4690375776 [label=AccumulateGrad]
	4690375824 -> 4690375680
	4685719232 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4685719232 -> 4690375824
	4690375824 [label=AccumulateGrad]
	4690375584 -> 4690375344
	4690375584 [label=TBackward0]
	4690375632 -> 4690375584
	4328871792 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4328871792 -> 4690375632
	4690375632 [label=AccumulateGrad]
	4690375392 -> 4690375248
	4328870992 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4328870992 -> 4690375392
	4690375392 [label=AccumulateGrad]
	4690375200 -> 4690375248
	4690391232 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4690391232 -> 4690375200
	4690375200 [label=AccumulateGrad]
	4690375152 -> 4690374960
	4690375152 [label=TBackward0]
	4690375440 -> 4690375152
	4690389872 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4690389872 -> 4690375440
	4690375440 [label=AccumulateGrad]
	4690375008 -> 4690374192
	4690374816 -> 4690374720
	4690392992 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4690392992 -> 4690374816
	4690374816 [label=AccumulateGrad]
	4690374864 -> 4690374720
	4690393152 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4690393152 -> 4690374864
	4690374864 [label=AccumulateGrad]
	4690370016 -> 4690371552
	4690370016 [label=TBackward0]
	4690374624 -> 4690370016
	4690393072 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4690393072 -> 4690374624
	4690374624 [label=AccumulateGrad]
	4690378272 -> 4690373520
	4690393232 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4690393232 -> 4690378272
	4690378272 [label=AccumulateGrad]
	4690373568 -> 4690373520
	4690393312 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4690393312 -> 4690373568
	4690373568 [label=AccumulateGrad]
	4690374240 -> 4689908672
	4690374240 [label=TBackward0]
	4690373904 -> 4690374240
	4690393392 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4690393392 -> 4690373904
	4690373904 [label=AccumulateGrad]
	4690374192 -> 4690373952
	4690374048 -> 4689491328
	4690374048 [label=TBackward0]
	4690378320 -> 4690374048
	4690393472 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4690393472 -> 4690378320
	4690378320 [label=AccumulateGrad]
	4689491328 -> 4690390912
}
