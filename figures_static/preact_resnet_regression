digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4755319264 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4755301776 [label=AddmmBackward0]
	4755301296 -> 4755301776
	4755322464 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4755322464 -> 4755301296
	4755301296 [label=AccumulateGrad]
	4755301584 -> 4755301776
	4755301584 [label=AddBackward0]
	4755299280 -> 4755301584
	4755299280 [label=MmBackward0]
	4755302016 -> 4755299280
	4755302016 [label=ReluBackward0]
	4755302544 -> 4755302016
	4755302544 [label=NativeLayerNormBackward0]
	4755302640 -> 4755302544
	4755302640 [label=MmBackward0]
	4755302736 -> 4755302640
	4755302736 [label=ReluBackward0]
	4755302880 -> 4755302736
	4755302880 [label=NativeLayerNormBackward0]
	4755302160 -> 4755302880
	4755302160 [label=AddBackward0]
	4755303120 -> 4755302160
	4755303120 [label=MmBackward0]
	4755303264 -> 4755303120
	4755303264 [label=ReluBackward0]
	4755303408 -> 4755303264
	4755303408 [label=NativeLayerNormBackward0]
	4755303504 -> 4755303408
	4755303504 [label=MmBackward0]
	4755303696 -> 4755303504
	4755303696 [label=ReluBackward0]
	4755303840 -> 4755303696
	4755303840 [label=NativeLayerNormBackward0]
	4755303168 -> 4755303840
	4755303168 [label=AddmmBackward0]
	4755304080 -> 4755303168
	4755319344 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4755319344 -> 4755304080
	4755304080 [label=AccumulateGrad]
	4755304128 -> 4755303168
	4755304128 [label=TBackward0]
	4755304272 -> 4755304128
	4751161488 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4751161488 -> 4755304272
	4755304272 [label=AccumulateGrad]
	4755303936 -> 4755303840
	4393687296 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4393687296 -> 4755303936
	4755303936 [label=AccumulateGrad]
	4755303984 -> 4755303840
	4393687216 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4393687216 -> 4755303984
	4755303984 [label=AccumulateGrad]
	4755303744 -> 4755303504
	4755303744 [label=TBackward0]
	4755303792 -> 4755303744
	4393686496 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4393686496 -> 4755303792
	4755303792 [label=AccumulateGrad]
	4755303552 -> 4755303408
	4751155008 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4751155008 -> 4755303552
	4755303552 [label=AccumulateGrad]
	4755303360 -> 4755303408
	4755320224 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4755320224 -> 4755303360
	4755303360 [label=AccumulateGrad]
	4755303312 -> 4755303120
	4755303312 [label=TBackward0]
	4755303600 -> 4755303312
	4755320624 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4755320624 -> 4755303600
	4755303600 [label=AccumulateGrad]
	4755303168 -> 4755302160
	4755302976 -> 4755302880
	4755319424 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4755319424 -> 4755302976
	4755302976 [label=AccumulateGrad]
	4755303024 -> 4755302880
	4755320464 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4755320464 -> 4755303024
	4755303024 [label=AccumulateGrad]
	4755302784 -> 4755302640
	4755302784 [label=TBackward0]
	4755302832 -> 4755302784
	4755319824 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4755319824 -> 4755302832
	4755302832 [label=AccumulateGrad]
	4755302352 -> 4755302544
	4755322144 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4755322144 -> 4755302352
	4755302352 [label=AccumulateGrad]
	4755302496 -> 4755302544
	4755322304 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4755322304 -> 4755302496
	4755302496 [label=AccumulateGrad]
	4755302064 -> 4755299280
	4755302064 [label=TBackward0]
	4755301680 -> 4755302064
	4755322224 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4755322224 -> 4755301680
	4755301680 [label=AccumulateGrad]
	4755302160 -> 4755301584
	4755301824 -> 4755301776
	4755301824 [label=TBackward0]
	4755301344 -> 4755301824
	4755322384 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4755322384 -> 4755301344
	4755301344 [label=AccumulateGrad]
	4755301776 -> 4755319264
}
