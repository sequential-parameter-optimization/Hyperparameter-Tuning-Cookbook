digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	6113900688 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	6113357456 [label=AddmmBackward0]
	4372353120 -> 6113357456
	6113902368 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	6113902368 -> 4372353120
	4372353120 [label=AccumulateGrad]
	6113802896 -> 6113357456
	6113802896 [label=AddBackward0]
	6113803664 -> 6113802896
	6113803664 [label=MmBackward0]
	6110469600 -> 6113803664
	6110469600 [label=ReluBackward0]
	6113804000 -> 6110469600
	6113804000 [label=NativeLayerNormBackward0]
	6113802992 -> 6113804000
	6113802992 [label=MmBackward0]
	6113803280 -> 6113802992
	6113803280 [label=ReluBackward0]
	6113804144 -> 6113803280
	6113804144 [label=NativeLayerNormBackward0]
	6113802800 -> 6113804144
	6113802800 [label=AddBackward0]
	6113802272 -> 6113802800
	6113802272 [label=MmBackward0]
	6109511936 -> 6113802272
	6109511936 [label=ReluBackward0]
	6109527536 -> 6109511936
	6109527536 [label=NativeLayerNormBackward0]
	6109512368 -> 6109527536
	6109512368 [label=MmBackward0]
	6109518752 -> 6109512368
	6109518752 [label=ReluBackward0]
	6114066688 -> 6109518752
	6114066688 [label=NativeLayerNormBackward0]
	6113802320 -> 6114066688
	6113802320 [label=AddmmBackward0]
	6114066928 -> 6113802320
	6109507776 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	6109507776 -> 6114066928
	6114066928 [label=AccumulateGrad]
	6114066880 -> 6113802320
	6114066880 [label=TBackward0]
	6114067024 -> 6114066880
	6109508336 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	6109508336 -> 6114067024
	6114067024 [label=AccumulateGrad]
	6114066784 -> 6114066688
	4477585440 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4477585440 -> 6114066784
	6114066784 [label=AccumulateGrad]
	6114066736 -> 6114066688
	4477585120 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4477585120 -> 6114066736
	6114066736 [label=AccumulateGrad]
	6114066544 -> 6109512368
	6114066544 [label=TBackward0]
	6114066976 -> 6114066544
	6109508176 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	6109508176 -> 6114066976
	6114066976 [label=AccumulateGrad]
	6109513088 -> 6109527536
	6109508016 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	6109508016 -> 6109513088
	6109513088 [label=AccumulateGrad]
	6109512224 -> 6109527536
	6113900368 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	6113900368 -> 6109512224
	6109512224 [label=AccumulateGrad]
	6109512320 -> 6113802272
	6109512320 [label=TBackward0]
	6109512128 -> 6109512320
	6113900528 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	6113900528 -> 6109512128
	6109512128 [label=AccumulateGrad]
	6113802320 -> 6113802800
	6113802176 -> 6113804144
	4510805648 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4510805648 -> 6113802176
	6113802176 [label=AccumulateGrad]
	6113804192 -> 6113804144
	6113900608 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	6113900608 -> 6113804192
	6113804192 [label=AccumulateGrad]
	6113803328 -> 6113802992
	6113803328 [label=TBackward0]
	6113802080 -> 6113803328
	6113897808 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	6113897808 -> 6113802080
	6113802080 [label=AccumulateGrad]
	6113803184 -> 6113804000
	6113897888 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	6113897888 -> 6113803184
	6113803184 [label=AccumulateGrad]
	6113803904 -> 6113804000
	6113902128 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	6113902128 -> 6113803904
	6113803904 [label=AccumulateGrad]
	6113803856 -> 6113803664
	6113803856 [label=TBackward0]
	6109512944 -> 6113803856
	6113902208 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	6113902208 -> 6109512944
	6109512944 [label=AccumulateGrad]
	6113802800 -> 6113802896
	6113801312 -> 6113357456
	6113801312 [label=TBackward0]
	6109512272 -> 6113801312
	6113902288 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	6113902288 -> 6109512272
	6109512272 [label=AccumulateGrad]
	6113357456 -> 6113900688
}
