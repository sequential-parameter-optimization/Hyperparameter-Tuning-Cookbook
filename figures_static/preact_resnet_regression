digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4578963248 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4602229376 [label=AddmmBackward0]
	4602231056 -> 4602229376
	4602348720 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4602348720 -> 4602231056
	4602231056 [label=AccumulateGrad]
	4602231104 -> 4602229376
	4602231104 [label=AddBackward0]
	4602230912 -> 4602231104
	4602230912 [label=MmBackward0]
	4602230336 -> 4602230912
	4602230336 [label=ReluBackward0]
	4602231392 -> 4602230336
	4602231392 [label=NativeLayerNormBackward0]
	4602231488 -> 4602231392
	4602231488 [label=MmBackward0]
	4602227600 -> 4602231488
	4602227600 [label=ReluBackward0]
	4602228608 -> 4602227600
	4602228608 [label=NativeLayerNormBackward0]
	4602230960 -> 4602228608
	4602230960 [label=AddBackward0]
	4602231680 -> 4602230960
	4602231680 [label=MmBackward0]
	4602231824 -> 4602231680
	4602231824 [label=ReluBackward0]
	4602231968 -> 4602231824
	4602231968 [label=NativeLayerNormBackward0]
	4602232064 -> 4602231968
	4602232064 [label=MmBackward0]
	4602232256 -> 4602232064
	4602232256 [label=ReluBackward0]
	4602232400 -> 4602232256
	4602232400 [label=NativeLayerNormBackward0]
	4602231632 -> 4602232400
	4602231632 [label=AddmmBackward0]
	4602232448 -> 4602231632
	4602348560 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4602348560 -> 4602232448
	4602232448 [label=AccumulateGrad]
	4602228320 -> 4602231632
	4602228320 [label=TBackward0]
	4602232544 -> 4602228320
	4602347760 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4602347760 -> 4602232544
	4602232544 [label=AccumulateGrad]
	4602228464 -> 4602232400
	4332967632 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4332967632 -> 4602228464
	4602228464 [label=AccumulateGrad]
	4602228416 -> 4602232400
	4332969472 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4332969472 -> 4602228416
	4602228416 [label=AccumulateGrad]
	4602232208 -> 4602232064
	4602232208 [label=TBackward0]
	4602232496 -> 4602232208
	4602248752 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4602248752 -> 4602232496
	4602232496 [label=AccumulateGrad]
	4602232016 -> 4602231968
	4602248112 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4602248112 -> 4602232016
	4602232016 [label=AccumulateGrad]
	4602231872 -> 4602231968
	4596094512 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4596094512 -> 4602231872
	4602231872 [label=AccumulateGrad]
	4602231776 -> 4602231680
	4602231776 [label=TBackward0]
	4602232304 -> 4602231776
	4599561136 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4599561136 -> 4602232304
	4602232304 [label=AccumulateGrad]
	4602231632 -> 4602230960
	4602230816 -> 4602228608
	4599561536 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4599561536 -> 4602230816
	4602230816 [label=AccumulateGrad]
	4602230864 -> 4602228608
	4599762048 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4599762048 -> 4602230864
	4602230864 [label=AccumulateGrad]
	4602230768 -> 4602231488
	4602230768 [label=TBackward0]
	4602231728 -> 4602230768
	4599761888 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4599761888 -> 4602231728
	4602231728 [label=AccumulateGrad]
	4602230288 -> 4602231392
	4361555232 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4361555232 -> 4602230288
	4602230288 [label=AccumulateGrad]
	4602230240 -> 4602231392
	4367356048 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4367356048 -> 4602230240
	4602230240 [label=AccumulateGrad]
	4602231248 -> 4602230912
	4602231248 [label=TBackward0]
	4602229136 -> 4602231248
	4360461664 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4360461664 -> 4602229136
	4602229136 [label=AccumulateGrad]
	4602230960 -> 4602231104
	4602231152 -> 4602229376
	4602231152 [label=TBackward0]
	4602230720 -> 4602231152
	4602348640 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4602348640 -> 4602230720
	4602230720 [label=AccumulateGrad]
	4602229376 -> 4578963248
}
