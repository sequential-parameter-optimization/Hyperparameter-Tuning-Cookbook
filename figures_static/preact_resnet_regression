digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5377885440 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5377868608 [label=AddmmBackward0]
	5377868800 -> 5377868608
	5377889520 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5377889520 -> 5377868800
	5377868800 [label=AccumulateGrad]
	5377869136 -> 5377868608
	5377869136 [label=AddBackward0]
	5377870096 -> 5377869136
	5377870096 [label=MmBackward0]
	5377870288 -> 5377870096
	5377870288 [label=ReluBackward0]
	5372400912 -> 5377870288
	5372400912 [label=NativeLayerNormBackward0]
	5377870336 -> 5372400912
	5377870336 [label=MmBackward0]
	5377870384 -> 5377870336
	5377870384 [label=ReluBackward0]
	5377870576 -> 5377870384
	5377870576 [label=NativeLayerNormBackward0]
	5377869808 -> 5377870576
	5377869808 [label=AddBackward0]
	5377870864 -> 5377869808
	5377870864 [label=MmBackward0]
	5377871008 -> 5377870864
	5377871008 [label=ReluBackward0]
	5377871152 -> 5377871008
	5377871152 [label=NativeLayerNormBackward0]
	5377871248 -> 5377871152
	5377871248 [label=MmBackward0]
	5377871440 -> 5377871248
	5377871440 [label=ReluBackward0]
	5377871584 -> 5377871440
	5377871584 [label=NativeLayerNormBackward0]
	5377870816 -> 5377871584
	5377870816 [label=AddmmBackward0]
	5377871824 -> 5377870816
	5377888960 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5377888960 -> 5377871824
	5377871824 [label=AccumulateGrad]
	5377871776 -> 5377870816
	5377871776 [label=TBackward0]
	5377871920 -> 5377871776
	5377887440 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5377887440 -> 5377871920
	5377871920 [label=AccumulateGrad]
	5377871680 -> 5377871584
	4321663232 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4321663232 -> 5377871680
	5377871680 [label=AccumulateGrad]
	5377871632 -> 5377871584
	4321663152 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4321663152 -> 5377871632
	5377871632 [label=AccumulateGrad]
	5377871392 -> 5377871248
	5377871392 [label=TBackward0]
	5377871872 -> 5377871392
	5372594768 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5372594768 -> 5377871872
	5377871872 [label=AccumulateGrad]
	5377871200 -> 5377871152
	4602927200 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4602927200 -> 5377871200
	5377871200 [label=AccumulateGrad]
	5377871056 -> 5377871152
	4355105696 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4355105696 -> 5377871056
	5377871056 [label=AccumulateGrad]
	5377870960 -> 5377870864
	5377870960 [label=TBackward0]
	5377871488 -> 5377870960
	5372387776 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5372387776 -> 5377871488
	5377871488 [label=AccumulateGrad]
	5377870816 -> 5377869808
	5377870720 -> 5377870576
	5372387136 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5372387136 -> 5377870720
	5377870720 [label=AccumulateGrad]
	5377870624 -> 5377870576
	5377886720 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5377886720 -> 5377870624
	5377870624 [label=AccumulateGrad]
	5377868176 -> 5377870336
	5377868176 [label=TBackward0]
	5377870912 -> 5377868176
	5377887520 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5377887520 -> 5377870912
	5377870912 [label=AccumulateGrad]
	5377868368 -> 5372400912
	5377889200 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5377889200 -> 5377868368
	5377868368 [label=AccumulateGrad]
	5377868320 -> 5372400912
	5377889280 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5377889280 -> 5377868320
	5377868320 [label=AccumulateGrad]
	5377870240 -> 5377870096
	5377870240 [label=TBackward0]
	5377870432 -> 5377870240
	5377889360 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5377889360 -> 5377870432
	5377870432 [label=AccumulateGrad]
	5377869808 -> 5377869136
	5377868512 -> 5377868608
	5377868512 [label=TBackward0]
	5377867984 -> 5377868512
	5377889440 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5377889440 -> 5377867984
	5377867984 [label=AccumulateGrad]
	5377868608 -> 5377885440
}
