digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4747622704 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4744560448 [label=AddmmBackward0]
	4747604992 -> 4744560448
	4747621504 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4747621504 -> 4747604992
	4747604992 [label=AccumulateGrad]
	4747604944 -> 4744560448
	4747604944 [label=AddBackward0]
	4747604320 -> 4747604944
	4747604320 [label=MmBackward0]
	4747605136 -> 4747604320
	4747605136 [label=ReluBackward0]
	4747603648 -> 4747605136
	4747603648 [label=NativeLayerNormBackward0]
	4747605280 -> 4747603648
	4747605280 [label=MmBackward0]
	4747605664 -> 4747605280
	4747605664 [label=ReluBackward0]
	4747605808 -> 4747605664
	4747605808 [label=NativeLayerNormBackward0]
	4747604848 -> 4747605808
	4747604848 [label=AddBackward0]
	4747606048 -> 4747604848
	4747606048 [label=MmBackward0]
	4747606192 -> 4747606048
	4747606192 [label=ReluBackward0]
	4747606336 -> 4747606192
	4747606336 [label=NativeLayerNormBackward0]
	4747606432 -> 4747606336
	4747606432 [label=MmBackward0]
	4747606624 -> 4747606432
	4747606624 [label=ReluBackward0]
	4747606768 -> 4747606624
	4747606768 [label=NativeLayerNormBackward0]
	4747606096 -> 4747606768
	4747606096 [label=AddmmBackward0]
	4747607008 -> 4747606096
	4747618704 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4747618704 -> 4747607008
	4747607008 [label=AccumulateGrad]
	4747607056 -> 4747606096
	4747607056 [label=TBackward0]
	4747607200 -> 4747607056
	4747618624 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4747618624 -> 4747607200
	4747607200 [label=AccumulateGrad]
	4747606864 -> 4747606768
	4386706832 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4386706832 -> 4747606864
	4747606864 [label=AccumulateGrad]
	4747606912 -> 4747606768
	4386706912 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4386706912 -> 4747606912
	4747606912 [label=AccumulateGrad]
	4747606672 -> 4747606432
	4747606672 [label=TBackward0]
	4747606720 -> 4747606672
	4386706112 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4386706112 -> 4747606720
	4747606720 [label=AccumulateGrad]
	4747606480 -> 4747606336
	4742828016 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4742828016 -> 4747606480
	4747606480 [label=AccumulateGrad]
	4747606288 -> 4747606336
	4747619184 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4747619184 -> 4747606288
	4747606288 [label=AccumulateGrad]
	4747606240 -> 4747606048
	4747606240 [label=TBackward0]
	4747606528 -> 4747606240
	4747619104 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4747619104 -> 4747606528
	4747606528 [label=AccumulateGrad]
	4747606096 -> 4747604848
	4747605904 -> 4747605808
	4743135088 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4743135088 -> 4747605904
	4747605904 [label=AccumulateGrad]
	4747605952 -> 4747605808
	4747619584 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4747619584 -> 4747605952
	4747605952 [label=AccumulateGrad]
	4747605712 -> 4747605280
	4747605712 [label=TBackward0]
	4747605760 -> 4747605712
	4747619664 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4747619664 -> 4747605760
	4747605760 [label=AccumulateGrad]
	4747605088 -> 4747603648
	4747619744 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4747619744 -> 4747605088
	4747605088 [label=AccumulateGrad]
	4747604800 -> 4747603648
	4747619824 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4747619824 -> 4747604800
	4747604800 [label=AccumulateGrad]
	4747604752 -> 4747604320
	4747604752 [label=TBackward0]
	4747605520 -> 4747604752
	4747619904 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4747619904 -> 4747605520
	4747605520 [label=AccumulateGrad]
	4747604848 -> 4747604944
	4747605040 -> 4744560448
	4747605040 [label=TBackward0]
	4747604368 -> 4747605040
	4747621344 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4747621344 -> 4747604368
	4747604368 [label=AccumulateGrad]
	4744560448 -> 4747622704
}
