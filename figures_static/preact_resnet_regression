digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5181879984 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5179334976 [label=AddmmBackward0]
	5179335888 -> 5179334976
	5181881904 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5181881904 -> 5179335888
	5179335888 [label=AccumulateGrad]
	5181780080 -> 5179334976
	5181780080 [label=AddBackward0]
	5181915920 -> 5181780080
	5181915920 [label=MmBackward0]
	5181917744 -> 5181915920
	5181917744 [label=ReluBackward0]
	5179335936 -> 5181917744
	5179335936 [label=NativeLayerNormBackward0]
	5181918032 -> 5179335936
	5181918032 [label=MmBackward0]
	5181915632 -> 5181918032
	5181915632 [label=ReluBackward0]
	5181918128 -> 5181915632
	5181918128 [label=NativeLayerNormBackward0]
	5181917120 -> 5181918128
	5181917120 [label=AddBackward0]
	5181918464 -> 5181917120
	5181918464 [label=MmBackward0]
	5181918608 -> 5181918464
	5181918608 [label=ReluBackward0]
	5181918752 -> 5181918608
	5181918752 [label=NativeLayerNormBackward0]
	5181918848 -> 5181918752
	5181918848 [label=MmBackward0]
	5181919040 -> 5181918848
	5181919040 [label=ReluBackward0]
	5181919184 -> 5181919040
	5181919184 [label=NativeLayerNormBackward0]
	5181918368 -> 5181919184
	5181918368 [label=AddmmBackward0]
	5181919424 -> 5181918368
	5179240640 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5179240640 -> 5181919424
	5181919424 [label=AccumulateGrad]
	5181919376 -> 5181918368
	5181919376 [label=TBackward0]
	5181919520 -> 5181919376
	5181878304 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5181878304 -> 5181919520
	5181919520 [label=AccumulateGrad]
	5181919280 -> 5181919184
	4332161376 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4332161376 -> 5181919280
	5181919280 [label=AccumulateGrad]
	5181919232 -> 5181919184
	4332161216 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4332161216 -> 5181919232
	5181919232 [label=AccumulateGrad]
	5181918992 -> 5181918848
	5181918992 [label=TBackward0]
	5181919472 -> 5181918992
	4332162176 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4332162176 -> 5181919472
	5181919472 [label=AccumulateGrad]
	5181918800 -> 5181918752
	5143725632 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5143725632 -> 5181918800
	5181918800 [label=AccumulateGrad]
	5181918656 -> 5181918752
	5161020032 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5161020032 -> 5181918656
	5181918656 [label=AccumulateGrad]
	5181918560 -> 5181918464
	5181918560 [label=TBackward0]
	5181919088 -> 5181918560
	5161024112 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5161024112 -> 5181919088
	5181919088 [label=AccumulateGrad]
	5181918368 -> 5181917120
	5181918224 -> 5181918128
	5181878224 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5181878224 -> 5181918224
	5181918224 [label=AccumulateGrad]
	5181918176 -> 5181918128
	5181878864 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5181878864 -> 5181918176
	5181918176 [label=AccumulateGrad]
	5181917648 -> 5181918032
	5181917648 [label=TBackward0]
	5181918512 -> 5181917648
	5181877904 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5181877904 -> 5181918512
	5181918512 [label=AccumulateGrad]
	5181917600 -> 5179335936
	5181880224 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5181880224 -> 5181917600
	5181917600 [label=AccumulateGrad]
	5181917504 -> 5179335936
	5181880304 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5181880304 -> 5181917504
	5181917504 [label=AccumulateGrad]
	5181915872 -> 5181915920
	5181915872 [label=TBackward0]
	5181916016 -> 5181915872
	5181880384 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5181880384 -> 5181916016
	5181916016 [label=AccumulateGrad]
	5181917120 -> 5181780080
	5181917984 -> 5179334976
	5181917984 [label=TBackward0]
	5181915536 -> 5181917984
	5181880464 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5181880464 -> 5181915536
	5181915536 [label=AccumulateGrad]
	5179334976 -> 5181879984
}
