digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4624205216 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4622098848 [label=AddmmBackward0]
	4624092080 -> 4622098848
	4624204256 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4624204256 -> 4624092080
	4624092080 [label=AccumulateGrad]
	4624092128 -> 4622098848
	4624092128 [label=AddBackward0]
	4624090112 -> 4624092128
	4624090112 [label=MmBackward0]
	4624092032 -> 4624090112
	4624092032 [label=ReluBackward0]
	4624091408 -> 4624092032
	4624091408 [label=NativeLayerNormBackward0]
	4624092848 -> 4624091408
	4624092848 [label=MmBackward0]
	4624089680 -> 4624092848
	4624089680 [label=ReluBackward0]
	4624091312 -> 4624089680
	4624091312 [label=NativeLayerNormBackward0]
	4624090016 -> 4624091312
	4624090016 [label=AddBackward0]
	4624093136 -> 4624090016
	4624093136 [label=MmBackward0]
	4624093328 -> 4624093136
	4624093328 [label=ReluBackward0]
	4624093472 -> 4624093328
	4624093472 [label=NativeLayerNormBackward0]
	4624093568 -> 4624093472
	4624093568 [label=MmBackward0]
	4624093760 -> 4624093568
	4624093760 [label=ReluBackward0]
	4624093904 -> 4624093760
	4624093904 [label=NativeLayerNormBackward0]
	4624092416 -> 4624093904
	4624092416 [label=AddmmBackward0]
	4624094144 -> 4624092416
	4624038592 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4624038592 -> 4624094144
	4624094144 [label=AccumulateGrad]
	4624094096 -> 4624092416
	4624094096 [label=TBackward0]
	4624094240 -> 4624094096
	4624036752 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4624036752 -> 4624094240
	4624094240 [label=AccumulateGrad]
	4624094000 -> 4624093904
	4355511776 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4355511776 -> 4624094000
	4624094000 [label=AccumulateGrad]
	4624093952 -> 4624093904
	4355509776 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4355509776 -> 4624093952
	4624093952 [label=AccumulateGrad]
	4624093712 -> 4624093568
	4624093712 [label=TBackward0]
	4624094192 -> 4624093712
	4355511616 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4355511616 -> 4624094192
	4624094192 [label=AccumulateGrad]
	4624093520 -> 4624093472
	4621414272 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4621414272 -> 4624093520
	4624093520 [label=AccumulateGrad]
	4624093376 -> 4624093472
	4621414832 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4621414832 -> 4624093376
	4624093376 [label=AccumulateGrad]
	4624093280 -> 4624093136
	4624093280 [label=TBackward0]
	4624093808 -> 4624093280
	4417118256 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4417118256 -> 4624093808
	4624093808 [label=AccumulateGrad]
	4624092416 -> 4624090016
	4624092464 -> 4624091312
	4624037312 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4624037312 -> 4624092464
	4624092464 [label=AccumulateGrad]
	4624091984 -> 4624091312
	4624037152 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4624037152 -> 4624091984
	4624091984 [label=AccumulateGrad]
	4624093040 -> 4624092848
	4624093040 [label=TBackward0]
	4624093232 -> 4624093040
	4624036912 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4624036912 -> 4624093232
	4624093232 [label=AccumulateGrad]
	4624091216 -> 4624091408
	4624036592 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4624036592 -> 4624091216
	4624091216 [label=AccumulateGrad]
	4624091648 -> 4624091408
	4624038832 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4624038832 -> 4624091648
	4624091648 [label=AccumulateGrad]
	4624091168 -> 4624090112
	4624091168 [label=TBackward0]
	4624091888 -> 4624091168
	4624038912 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4624038912 -> 4624091888
	4624091888 [label=AccumulateGrad]
	4624090016 -> 4624092128
	4624092224 -> 4622098848
	4624092224 [label=TBackward0]
	4624092944 -> 4624092224
	4504939056 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4504939056 -> 4624092944
	4624092944 [label=AccumulateGrad]
	4622098848 -> 4624205216
}
