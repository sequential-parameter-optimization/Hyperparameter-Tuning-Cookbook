digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4674182576 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4676745152 [label=AddmmBackward0]
	4675069040 -> 4676745152
	4676870832 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4676870832 -> 4675069040
	4675069040 [label=AccumulateGrad]
	4676745872 -> 4676745152
	4676745872 [label=AddBackward0]
	4676746064 -> 4676745872
	4676746064 [label=MmBackward0]
	4676745392 -> 4676746064
	4676745392 [label=ReluBackward0]
	4676744240 -> 4676745392
	4676744240 [label=NativeLayerNormBackward0]
	4676744816 -> 4676744240
	4676744816 [label=MmBackward0]
	4676746784 -> 4676744816
	4676746784 [label=ReluBackward0]
	4676743808 -> 4676746784
	4676743808 [label=NativeLayerNormBackward0]
	4676745968 -> 4676743808
	4676745968 [label=AddBackward0]
	4676746976 -> 4676745968
	4676746976 [label=MmBackward0]
	4676747120 -> 4676746976
	4676747120 [label=ReluBackward0]
	4676742608 -> 4676747120
	4676742608 [label=NativeLayerNormBackward0]
	4676742512 -> 4676742608
	4676742512 [label=MmBackward0]
	4426281696 -> 4676742512
	4426281696 [label=ReluBackward0]
	4672166048 -> 4426281696
	4672166048 [label=NativeLayerNormBackward0]
	4676746208 -> 4672166048
	4676746208 [label=AddmmBackward0]
	4677009664 -> 4676746208
	4676870512 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4676870512 -> 4677009664
	4677009664 [label=AccumulateGrad]
	4677009616 -> 4676746208
	4677009616 [label=TBackward0]
	4677009760 -> 4677009616
	4676870592 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4676870592 -> 4677009760
	4677009760 [label=AccumulateGrad]
	4672160624 -> 4672166048
	4398658608 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4398658608 -> 4672160624
	4672160624 [label=AccumulateGrad]
	4677009520 -> 4672166048
	4398657648 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4398657648 -> 4677009520
	4677009520 [label=AccumulateGrad]
	4672172288 -> 4676742512
	4672172288 [label=TBackward0]
	4672172144 -> 4672172288
	4674181536 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4674181536 -> 4672172144
	4672172144 [label=AccumulateGrad]
	4676742464 -> 4676742608
	4674185536 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4674185536 -> 4676742464
	4676742464 [label=AccumulateGrad]
	4676747168 -> 4676742608
	4674185456 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4674185456 -> 4676747168
	4676747168 [label=AccumulateGrad]
	4676747072 -> 4676746976
	4676747072 [label=TBackward0]
	4672161392 -> 4676747072
	4659086272 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4659086272 -> 4672161392
	4672161392 [label=AccumulateGrad]
	4676746208 -> 4676745968
	4676746928 -> 4676743808
	4670519280 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4670519280 -> 4676746928
	4676746928 [label=AccumulateGrad]
	4676746112 -> 4676743808
	4670517120 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4670517120 -> 4676746112
	4676746112 [label=AccumulateGrad]
	4676746736 -> 4676744816
	4676746736 [label=TBackward0]
	4673845712 -> 4676746736
	4656759264 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4656759264 -> 4673845712
	4673845712 [label=AccumulateGrad]
	4676746496 -> 4676744240
	4676868912 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4676868912 -> 4676746496
	4676746496 [label=AccumulateGrad]
	4676745248 -> 4676744240
	4676869472 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4676869472 -> 4676745248
	4676745248 [label=AccumulateGrad]
	4676745776 -> 4676746064
	4676745776 [label=TBackward0]
	4676746832 -> 4676745776
	4676870752 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4676870752 -> 4676746832
	4676746832 [label=AccumulateGrad]
	4676745968 -> 4676745872
	4676745920 -> 4676745152
	4676745920 [label=TBackward0]
	4676746592 -> 4676745920
	4676870672 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4676870672 -> 4676746592
	4676746592 [label=AccumulateGrad]
	4676745152 -> 4674182576
}
