digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4667320000 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4669878192 [label=AddmmBackward0]
	4669879920 -> 4669878192
	4669989472 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4669989472 -> 4669879920
	4669879920 [label=AccumulateGrad]
	4669878864 -> 4669878192
	4669878864 [label=AddBackward0]
	4669879584 -> 4669878864
	4669879584 [label=MmBackward0]
	4667379184 -> 4669879584
	4667379184 [label=ReluBackward0]
	4668193280 -> 4667379184
	4668193280 [label=NativeLayerNormBackward0]
	4667376976 -> 4668193280
	4667376976 [label=MmBackward0]
	4667386192 -> 4667376976
	4667386192 [label=ReluBackward0]
	4667384416 -> 4667386192
	4667384416 [label=NativeLayerNormBackward0]
	4669877616 -> 4667384416
	4669877616 [label=AddBackward0]
	4667385616 -> 4669877616
	4667385616 [label=MmBackward0]
	4667377792 -> 4667385616
	4667377792 [label=ReluBackward0]
	4670128384 -> 4667377792
	4670128384 [label=NativeLayerNormBackward0]
	4670128480 -> 4670128384
	4670128480 [label=MmBackward0]
	4670128672 -> 4670128480
	4670128672 [label=ReluBackward0]
	4670128816 -> 4670128672
	4670128816 [label=NativeLayerNormBackward0]
	4667379232 -> 4670128816
	4667379232 [label=AddmmBackward0]
	4670129056 -> 4667379232
	4669989312 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4669989312 -> 4670129056
	4670129056 [label=AccumulateGrad]
	4670129008 -> 4667379232
	4670129008 [label=TBackward0]
	4670129152 -> 4670129008
	4669989392 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4669989392 -> 4670129152
	4670129152 [label=AccumulateGrad]
	4670128912 -> 4670128816
	4391546992 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4391546992 -> 4670128912
	4670128912 [label=AccumulateGrad]
	4670128864 -> 4670128816
	4391546832 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4391546832 -> 4670128864
	4670128864 [label=AccumulateGrad]
	4670128624 -> 4670128480
	4670128624 [label=TBackward0]
	4670129104 -> 4670128624
	4391547792 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4391547792 -> 4670129104
	4670129104 [label=AccumulateGrad]
	4670128432 -> 4670128384
	4665480832 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4665480832 -> 4670128432
	4670128432 [label=AccumulateGrad]
	4670128288 -> 4670128384
	4426269232 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4426269232 -> 4670128288
	4670128288 [label=AccumulateGrad]
	4670128240 -> 4667385616
	4670128240 [label=TBackward0]
	4670128720 -> 4670128240
	4667315600 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4667315600 -> 4670128720
	4670128720 [label=AccumulateGrad]
	4667379232 -> 4669877616
	4667376544 -> 4667384416
	4667315760 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4667315760 -> 4667376544
	4667376544 [label=AccumulateGrad]
	4667376736 -> 4667384416
	4667320080 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4667320080 -> 4667376736
	4667376736 [label=AccumulateGrad]
	4667378176 -> 4667376976
	4667378176 [label=TBackward0]
	4667379280 -> 4667378176
	4669987952 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4669987952 -> 4667379280
	4667379280 [label=AccumulateGrad]
	4667378224 -> 4668193280
	4669988032 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4669988032 -> 4667378224
	4667378224 [label=AccumulateGrad]
	4667378944 -> 4668193280
	4669990912 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4669990912 -> 4667378944
	4667378944 [label=AccumulateGrad]
	4667376640 -> 4669879584
	4667376640 [label=TBackward0]
	4667376352 -> 4667376640
	4669987472 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4669987472 -> 4667376352
	4667376352 [label=AccumulateGrad]
	4669877616 -> 4669878864
	4669879536 -> 4669878192
	4669879536 [label=TBackward0]
	4667378608 -> 4669879536
	4669987152 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4669987152 -> 4667378608
	4667378608 [label=AccumulateGrad]
	4669878192 -> 4667320000
}
