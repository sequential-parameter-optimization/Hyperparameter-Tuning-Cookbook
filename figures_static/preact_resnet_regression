digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5163341120 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5163405120 [label=AddmmBackward0]
	5163406800 -> 5163405120
	5186967968 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5186967968 -> 5163406800
	5163406800 [label=AccumulateGrad]
	5163406272 -> 5163405120
	5163406272 [label=AddBackward0]
	5186844896 -> 5163406272
	5186844896 [label=MmBackward0]
	5186845952 -> 5186844896
	5186845952 [label=ReluBackward0]
	5186846096 -> 5186845952
	5186846096 [label=NativeLayerNormBackward0]
	5186846528 -> 5186846096
	5186846528 [label=MmBackward0]
	5186845184 -> 5186846528
	5186845184 [label=ReluBackward0]
	5186845088 -> 5186845184
	5186845088 [label=NativeLayerNormBackward0]
	5186844752 -> 5186845088
	5186844752 [label=AddBackward0]
	5186843360 -> 5186844752
	5186843360 [label=MmBackward0]
	5186842256 -> 5186843360
	5186842256 [label=ReluBackward0]
	4361028608 -> 5186842256
	4361028608 [label=NativeLayerNormBackward0]
	5162955440 -> 4361028608
	5162955440 [label=MmBackward0]
	5187125504 -> 5162955440
	5187125504 [label=ReluBackward0]
	5187125648 -> 5187125504
	5187125648 [label=NativeLayerNormBackward0]
	5186845232 -> 5187125648
	5186845232 [label=AddmmBackward0]
	5187125888 -> 5186845232
	5186966928 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5186966928 -> 5187125888
	5187125888 [label=AccumulateGrad]
	5187125840 -> 5186845232
	5187125840 [label=TBackward0]
	5187125984 -> 5187125840
	5186965808 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5186965808 -> 5187125984
	5187125984 [label=AccumulateGrad]
	5187125744 -> 5187125648
	5160235680 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	5160235680 -> 5187125744
	5187125744 [label=AccumulateGrad]
	5187125696 -> 5187125648
	4356374384 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4356374384 -> 5187125696
	5187125696 [label=AccumulateGrad]
	5187125456 -> 5162955440
	5187125456 [label=TBackward0]
	5187125936 -> 5187125456
	5163341040 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5163341040 -> 5187125936
	5187125936 [label=AccumulateGrad]
	5186842544 -> 4361028608
	4680092560 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4680092560 -> 5186842544
	5186842544 [label=AccumulateGrad]
	5187125312 -> 4361028608
	4320686832 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4320686832 -> 5187125312
	5187125312 [label=AccumulateGrad]
	5186846672 -> 5186843360
	5186846672 [label=TBackward0]
	5186844464 -> 5186846672
	4320685792 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4320685792 -> 5186844464
	5186844464 [label=AccumulateGrad]
	5186845232 -> 5186844752
	5186843072 -> 5186845088
	4320685872 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4320685872 -> 5186843072
	5186843072 [label=AccumulateGrad]
	5186846144 -> 5186845088
	5137696240 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5137696240 -> 5186846144
	5186846144 [label=AccumulateGrad]
	5186846240 -> 5186846528
	5186846240 [label=TBackward0]
	5186845520 -> 5186846240
	5186965968 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5186965968 -> 5186845520
	5186845520 [label=AccumulateGrad]
	5186846480 -> 5186846096
	5186966448 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5186966448 -> 5186846480
	5186846480 [label=AccumulateGrad]
	5186846000 -> 5186846096
	5186967408 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5186967408 -> 5186846000
	5186846000 [label=AccumulateGrad]
	5186845856 -> 5186844896
	5186845856 [label=TBackward0]
	5186845040 -> 5186845856
	5186966528 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5186966528 -> 5186845040
	5186845040 [label=AccumulateGrad]
	5186844752 -> 5163406272
	5163405024 -> 5163405120
	5163405024 [label=TBackward0]
	5186846384 -> 5163405024
	5186968048 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5186968048 -> 5186846384
	5186846384 [label=AccumulateGrad]
	5163405120 -> 5163341120
}
