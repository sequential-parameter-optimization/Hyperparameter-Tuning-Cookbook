digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	13036416816 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	13025897264 [label=AddmmBackward0]
	13036301536 -> 13025897264
	13036416016 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	13036416016 -> 13036301536
	13036301536 [label=AccumulateGrad]
	13036300048 -> 13025897264
	13036300048 [label=AddBackward0]
	13036300672 -> 13036300048
	13036300672 [label=MmBackward0]
	13036300096 -> 13036300672
	13036300096 [label=ReluBackward0]
	13036299760 -> 13036300096
	13036299760 [label=NativeLayerNormBackward0]
	13036299664 -> 13036299760
	13036299664 [label=MmBackward0]
	13036301872 -> 13036299664
	13036301872 [label=ReluBackward0]
	13036301776 -> 13036301872
	13036301776 [label=NativeLayerNormBackward0]
	13036299376 -> 13036301776
	13036299376 [label=AddBackward0]
	13036299184 -> 13036299376
	13036299184 [label=MmBackward0]
	13036298848 -> 13036299184
	13036298848 [label=ReluBackward0]
	13036300720 -> 13036298848
	13036300720 [label=NativeLayerNormBackward0]
	13036298608 -> 13036300720
	13036298608 [label=MmBackward0]
	13036300480 -> 13036298608
	13036300480 [label=ReluBackward0]
	13036300384 -> 13036300480
	13036300384 [label=NativeLayerNormBackward0]
	13036299232 -> 13036300384
	13036299232 [label=AddmmBackward0]
	13036302256 -> 13036299232
	13036415456 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	13036415456 -> 13036302256
	13036302256 [label=AccumulateGrad]
	13036302208 -> 13036299232
	13036302208 [label=TBackward0]
	13036302352 -> 13036302208
	13036413856 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	13036413856 -> 13036302352
	13036302352 [label=AccumulateGrad]
	13036302112 -> 13036300384
	4683429472 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4683429472 -> 13036302112
	13036302112 [label=AccumulateGrad]
	13036298320 -> 13036300384
	4683429072 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4683429072 -> 13036298320
	13036298320 [label=AccumulateGrad]
	13036300528 -> 13036298608
	13036300528 [label=TBackward0]
	13036302304 -> 13036300528
	13034007248 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	13034007248 -> 13036302304
	13036302304 [label=AccumulateGrad]
	13036298656 -> 13036300720
	13036411136 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	13036411136 -> 13036298656
	13036298656 [label=AccumulateGrad]
	13036298752 -> 13036300720
	13036410896 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	13036410896 -> 13036298752
	13036298752 [label=AccumulateGrad]
	13036298992 -> 13036299184
	13036298992 [label=TBackward0]
	13036298416 -> 13036298992
	13036413936 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	13036413936 -> 13036298416
	13036298416 [label=AccumulateGrad]
	13036299232 -> 13036299376
	13036299424 -> 13036301776
	13036415376 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	13036415376 -> 13036299424
	13036299424 [label=AccumulateGrad]
	13036301824 -> 13036301776
	13036415616 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	13036415616 -> 13036301824
	13036301824 [label=AccumulateGrad]
	13036299520 -> 13036299664
	13036299520 [label=TBackward0]
	13036299040 -> 13036299520
	13036415536 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	13036415536 -> 13036299040
	13036299040 [label=AccumulateGrad]
	13036299712 -> 13036299760
	13036415696 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	13036415696 -> 13036299712
	13036299712 [label=AccumulateGrad]
	13036299904 -> 13036299760
	13036415776 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	13036415776 -> 13036299904
	13036299904 [label=AccumulateGrad]
	13036299472 -> 13036300672
	13036299472 [label=TBackward0]
	13036301968 -> 13036299472
	13036415856 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	13036415856 -> 13036301968
	13036301968 [label=AccumulateGrad]
	13036299376 -> 13036300048
	13036301680 -> 13025897264
	13036301680 [label=TBackward0]
	13036299616 -> 13036301680
	13036415936 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	13036415936 -> 13036299616
	13036299616 [label=AccumulateGrad]
	13025897264 -> 13036416816
}
