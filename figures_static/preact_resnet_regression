digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4677544896 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4677418000 [label=AddmmBackward0]
	4677416512 -> 4677418000
	4677544016 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4677544016 -> 4677416512
	4677416512 [label=AccumulateGrad]
	4677417952 -> 4677418000
	4677417952 [label=AddBackward0]
	4677417424 -> 4677417952
	4677417424 [label=MmBackward0]
	4677418768 -> 4677417424
	4677418768 [label=ReluBackward0]
	4677418624 -> 4677418768
	4677418624 [label=NativeLayerNormBackward0]
	4677418480 -> 4677418624
	4677418480 [label=MmBackward0]
	4677417184 -> 4677418480
	4677417184 [label=ReluBackward0]
	4677416032 -> 4677417184
	4677416032 [label=NativeLayerNormBackward0]
	4677417376 -> 4677416032
	4677417376 [label=AddBackward0]
	4677418960 -> 4677417376
	4677418960 [label=MmBackward0]
	4677414400 -> 4677418960
	4677414400 [label=ReluBackward0]
	4677418288 -> 4677414400
	4677418288 [label=NativeLayerNormBackward0]
	4677418240 -> 4677418288
	4677418240 [label=MmBackward0]
	4677681312 -> 4677418240
	4677681312 [label=ReluBackward0]
	4677681456 -> 4677681312
	4677681456 [label=NativeLayerNormBackward0]
	4677414256 -> 4677681456
	4677414256 [label=AddmmBackward0]
	4677681696 -> 4677414256
	4674853680 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4674853680 -> 4677681696
	4677681696 [label=AccumulateGrad]
	4677681648 -> 4677414256
	4677681648 [label=TBackward0]
	4677681792 -> 4677681648
	4674856880 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4674856880 -> 4677681792
	4677681792 [label=AccumulateGrad]
	4677681552 -> 4677681456
	4398790160 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4398790160 -> 4677681552
	4677681552 [label=AccumulateGrad]
	4677681504 -> 4677681456
	4398789360 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4398789360 -> 4677681504
	4677681504 [label=AccumulateGrad]
	4677681264 -> 4677418240
	4677681264 [label=TBackward0]
	4677681744 -> 4677681264
	4467748976 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4467748976 -> 4677681744
	4677681744 [label=AccumulateGrad]
	4677418336 -> 4677418288
	4674852880 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4674852880 -> 4677418336
	4677418336 [label=AccumulateGrad]
	4677417904 -> 4677418288
	4674856560 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4674856560 -> 4677417904
	4677417904 [label=AccumulateGrad]
	4677415456 -> 4677418960
	4677415456 [label=TBackward0]
	4676392256 -> 4677415456
	4677542016 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4677542016 -> 4676392256
	4676392256 [label=AccumulateGrad]
	4677414256 -> 4677417376
	4677414304 -> 4677416032
	4677541056 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4677541056 -> 4677414304
	4677414304 [label=AccumulateGrad]
	4677413968 -> 4677416032
	4677540496 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4677540496 -> 4677413968
	4677413968 [label=AccumulateGrad]
	4677417232 -> 4677418480
	4677417232 [label=TBackward0]
	4677417040 -> 4677417232
	4677540656 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4677540656 -> 4677417040
	4677417040 [label=AccumulateGrad]
	4677418528 -> 4677418624
	4677542256 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4677542256 -> 4677418528
	4677418528 [label=AccumulateGrad]
	4677418720 -> 4677418624
	4677542416 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4677542416 -> 4677418720
	4677418720 [label=AccumulateGrad]
	4677415168 -> 4677417424
	4677415168 [label=TBackward0]
	4677417856 -> 4677415168
	4677542496 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4677542496 -> 4677417856
	4677417856 [label=AccumulateGrad]
	4677417376 -> 4677417952
	4677418048 -> 4677418000
	4677418048 [label=TBackward0]
	4677417808 -> 4677418048
	4677543936 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4677543936 -> 4677417808
	4677417808 [label=AccumulateGrad]
	4677418000 -> 4677544896
}
