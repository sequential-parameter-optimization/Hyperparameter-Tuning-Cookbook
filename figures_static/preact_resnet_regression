digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5098627840 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5252541856 [label=AddmmBackward0]
	5253003152 -> 5252541856
	5253066704 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5253066704 -> 5253003152
	5253003152 [label=AccumulateGrad]
	5253001520 -> 5252541856
	5253001520 [label=AddBackward0]
	5253005120 -> 5253001520
	5253005120 [label=MmBackward0]
	5253002144 -> 5253005120
	5253002144 [label=ReluBackward0]
	5253002096 -> 5253002144
	5253002096 [label=NativeLayerNormBackward0]
	5253005264 -> 5253002096
	5253005264 [label=MmBackward0]
	5253004880 -> 5253005264
	5253004880 [label=ReluBackward0]
	5253004784 -> 5253004880
	5253004784 [label=NativeLayerNormBackward0]
	5253002624 -> 5253004784
	5253002624 [label=AddBackward0]
	5252878528 -> 5253002624
	5252878528 [label=MmBackward0]
	5098561328 -> 5252878528
	5098561328 [label=ReluBackward0]
	5098563728 -> 5098561328
	5098563728 [label=NativeLayerNormBackward0]
	5098561904 -> 5098563728
	5098561904 [label=MmBackward0]
	5098561760 -> 5098561904
	5098561760 [label=ReluBackward0]
	5098561616 -> 5098561760
	5098561616 [label=NativeLayerNormBackward0]
	5252889280 -> 5098561616
	5252889280 [label=AddmmBackward0]
	5098560272 -> 5252889280
	5253066624 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5253066624 -> 5098560272
	5098560272 [label=AccumulateGrad]
	5098562000 -> 5252889280
	5098562000 [label=TBackward0]
	5098559504 -> 5098562000
	5253065744 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5253065744 -> 5098559504
	5098559504 [label=AccumulateGrad]
	5098562144 -> 5098561616
	4406611632 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4406611632 -> 5098562144
	5098562144 [label=AccumulateGrad]
	5098565984 -> 5098561616
	5253064464 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	5253064464 -> 5098565984
	5098565984 [label=AccumulateGrad]
	5098561808 -> 5098561904
	5098561808 [label=TBackward0]
	5099514288 -> 5098561808
	5098631600 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5098631600 -> 5099514288
	5099514288 [label=AccumulateGrad]
	5098561952 -> 5098563728
	5098631760 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5098631760 -> 5098561952
	5098561952 [label=AccumulateGrad]
	5098562288 -> 5098563728
	5098631680 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5098631680 -> 5098562288
	5098562288 [label=AccumulateGrad]
	5098560224 -> 5252878528
	5098560224 [label=TBackward0]
	5098561568 -> 5098560224
	5098628400 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5098628400 -> 5098561568
	5098561568 [label=AccumulateGrad]
	5252889280 -> 5253002624
	5252875408 -> 5253004784
	5060808768 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5060808768 -> 5252875408
	5252875408 [label=AccumulateGrad]
	5252890240 -> 5253004784
	4406611712 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4406611712 -> 5252890240
	5252890240 [label=AccumulateGrad]
	5253004592 -> 5253005264
	5253004592 [label=TBackward0]
	5252886352 -> 5253004592
	4406611472 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4406611472 -> 5252886352
	5252886352 [label=AccumulateGrad]
	5253005216 -> 5253002096
	5253066304 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5253066304 -> 5253005216
	5253005216 [label=AccumulateGrad]
	5253001328 -> 5253002096
	5253066224 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5253066224 -> 5253001328
	5253001328 [label=AccumulateGrad]
	5252995376 -> 5253005120
	5252995376 [label=TBackward0]
	5253003776 -> 5252995376
	5253065824 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5253065824 -> 5253003776
	5253003776 [label=AccumulateGrad]
	5253002624 -> 5253001520
	5253004496 -> 5252541856
	5253004496 [label=TBackward0]
	5253004640 -> 5253004496
	5253066544 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5253066544 -> 5253004640
	5253004640 [label=AccumulateGrad]
	5252541856 -> 5098627840
}
