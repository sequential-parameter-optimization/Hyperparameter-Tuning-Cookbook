digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5189238560 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5189259120 [label=AddmmBackward0]
	5189259360 -> 5189259120
	5189236240 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5189236240 -> 5189259360
	5189259360 [label=AccumulateGrad]
	5189258976 -> 5189259120
	5189258976 [label=AddBackward0]
	5189258592 -> 5189258976
	5189258592 [label=MmBackward0]
	5189258448 -> 5189258592
	5189258448 [label=ReluBackward0]
	5189259648 -> 5189258448
	5189259648 [label=NativeLayerNormBackward0]
	5189258784 -> 5189259648
	5189258784 [label=MmBackward0]
	5189258352 -> 5189258784
	5189258352 [label=ReluBackward0]
	5189258880 -> 5189258352
	5189258880 [label=NativeLayerNormBackward0]
	5189258544 -> 5189258880
	5189258544 [label=AddBackward0]
	5189259984 -> 5189258544
	5189259984 [label=MmBackward0]
	5189260128 -> 5189259984
	5189260128 [label=ReluBackward0]
	5189260272 -> 5189260128
	5189260272 [label=NativeLayerNormBackward0]
	5189260368 -> 5189260272
	5189260368 [label=MmBackward0]
	5189260560 -> 5189260368
	5189260560 [label=ReluBackward0]
	5189260704 -> 5189260560
	5189260704 [label=NativeLayerNormBackward0]
	5189259936 -> 5189260704
	5189259936 [label=AddmmBackward0]
	5189260944 -> 5189259936
	5189234000 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5189234000 -> 5189260944
	5189260944 [label=AccumulateGrad]
	5189260896 -> 5189259936
	5189260896 [label=TBackward0]
	4389928672 -> 5189260896
	5189235920 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5189235920 -> 4389928672
	4389928672 [label=AccumulateGrad]
	5189260800 -> 5189260704
	4424348080 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4424348080 -> 5189260800
	5189260800 [label=AccumulateGrad]
	5189260752 -> 5189260704
	5185520896 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	5185520896 -> 5189260752
	5189260752 [label=AccumulateGrad]
	5189260512 -> 5189260368
	5189260512 [label=TBackward0]
	5189260992 -> 5189260512
	4355442800 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4355442800 -> 5189260992
	5189260992 [label=AccumulateGrad]
	5189260320 -> 5189260272
	5185531776 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5185531776 -> 5189260320
	5189260320 [label=AccumulateGrad]
	5189260176 -> 5189260272
	5185530416 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5185530416 -> 5189260176
	5189260176 [label=AccumulateGrad]
	5189260080 -> 5189259984
	5189260080 [label=TBackward0]
	5189260608 -> 5189260080
	5185534416 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5185534416 -> 5189260608
	5189260608 [label=AccumulateGrad]
	5189259936 -> 5189258544
	5189259840 -> 5189258880
	5185534656 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5185534656 -> 5189259840
	5189259840 [label=AccumulateGrad]
	5189259792 -> 5189258880
	5179029248 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5179029248 -> 5189259792
	5189259792 [label=AccumulateGrad]
	5189257488 -> 5189258784
	5189257488 [label=TBackward0]
	5189260032 -> 5189257488
	5189234080 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5189234080 -> 5189260032
	5189260032 [label=AccumulateGrad]
	5189259552 -> 5189259648
	5189236080 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5189236080 -> 5189259552
	5189259552 [label=AccumulateGrad]
	5189258400 -> 5189259648
	5189234240 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5189234240 -> 5189258400
	5189258400 [label=AccumulateGrad]
	5189259264 -> 5189258592
	5189259264 [label=TBackward0]
	5189258832 -> 5189259264
	5189236000 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5189236000 -> 5189258832
	5189258832 [label=AccumulateGrad]
	5189258544 -> 5189258976
	5189259024 -> 5189259120
	5189259024 [label=TBackward0]
	5189258928 -> 5189259024
	5189236160 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5189236160 -> 5189258928
	5189258928 [label=AccumulateGrad]
	5189259120 -> 5189238560
}
