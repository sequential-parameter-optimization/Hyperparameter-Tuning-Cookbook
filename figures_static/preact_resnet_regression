digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5465850976 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5465595840 [label=AddmmBackward0]
	5465598576 -> 5465595840
	5465701856 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5465701856 -> 5465598576
	5465598576 [label=AccumulateGrad]
	5465598480 -> 5465595840
	5465598480 [label=AddBackward0]
	5465602272 -> 5465598480
	5465602272 [label=MmBackward0]
	5465595552 -> 5465602272
	5465595552 [label=ReluBackward0]
	5464459104 -> 5465595552
	5464459104 [label=NativeLayerNormBackward0]
	5463858656 -> 5464459104
	5463858656 [label=MmBackward0]
	5465598864 -> 5463858656
	5465598864 [label=ReluBackward0]
	5465597424 -> 5465598864
	5465597424 [label=NativeLayerNormBackward0]
	5465602224 -> 5465597424
	5465602224 [label=AddBackward0]
	5465595120 -> 5465602224
	5465595120 [label=MmBackward0]
	5465597088 -> 5465595120
	5465597088 [label=ReluBackward0]
	5465596800 -> 5465597088
	5465596800 [label=NativeLayerNormBackward0]
	5465596656 -> 5465596800
	5465596656 [label=MmBackward0]
	5465596416 -> 5465596656
	5465596416 [label=ReluBackward0]
	5465596272 -> 5465596416
	5465596272 [label=NativeLayerNormBackward0]
	5465597184 -> 5465596272
	5465597184 [label=AddmmBackward0]
	5465599200 -> 5465597184
	5465700096 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5465700096 -> 5465599200
	5465599200 [label=AccumulateGrad]
	5465595936 -> 5465597184
	5465595936 [label=TBackward0]
	5465599296 -> 5465595936
	5465699936 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5465699936 -> 5465599296
	5465599296 [label=AccumulateGrad]
	5465596128 -> 5465596272
	4684100176 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4684100176 -> 5465596128
	5465596128 [label=AccumulateGrad]
	5465596224 -> 5465596272
	5423892352 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	5423892352 -> 5465596224
	5465596224 [label=AccumulateGrad]
	5465596464 -> 5465596656
	5465596464 [label=TBackward0]
	5465599248 -> 5465596464
	4609503584 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4609503584 -> 5465599248
	5465599248 [label=AccumulateGrad]
	5465596704 -> 5465596800
	5463195504 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5463195504 -> 5465596704
	5465596704 [label=AccumulateGrad]
	5465596896 -> 5465596800
	4684100096 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4684100096 -> 5465596896
	5465596896 [label=AccumulateGrad]
	5465595072 -> 5465595120
	5465595072 [label=TBackward0]
	5465596368 -> 5465595072
	4684099776 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4684099776 -> 5465596368
	5465596368 [label=AccumulateGrad]
	5465597184 -> 5465602224
	5465597280 -> 5465597424
	5465698576 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5465698576 -> 5465597280
	5465597280 [label=AccumulateGrad]
	5465595312 -> 5465597424
	5465698496 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5465698496 -> 5465595312
	5465595312 [label=AccumulateGrad]
	5465598912 -> 5463858656
	5465598912 [label=TBackward0]
	5465597136 -> 5465598912
	5465699856 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5465699856 -> 5465597136
	5465597136 [label=AccumulateGrad]
	5465598624 -> 5464459104
	5465701616 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5465701616 -> 5465598624
	5465598624 [label=AccumulateGrad]
	5465595504 -> 5464459104
	5465701536 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5465701536 -> 5465595504
	5465595504 [label=AccumulateGrad]
	5465595696 -> 5465602272
	5465595696 [label=TBackward0]
	5465598816 -> 5465595696
	5465701696 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5465701696 -> 5465598816
	5465598816 [label=AccumulateGrad]
	5465602224 -> 5465598480
	5465599152 -> 5465595840
	5465599152 [label=TBackward0]
	5465598288 -> 5465599152
	5465701776 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5465701776 -> 5465598288
	5465598288 [label=AccumulateGrad]
	5465595840 -> 5465850976
}
