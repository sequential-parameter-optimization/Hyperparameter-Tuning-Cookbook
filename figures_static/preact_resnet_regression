digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5333329680 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5345949488 [label=AddmmBackward0]
	5346160704 -> 5345949488
	5333329840 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5333329840 -> 5346160704
	5346160704 [label=AccumulateGrad]
	5347110064 -> 5345949488
	5347110064 [label=AddBackward0]
	5347109296 -> 5347110064
	5347109296 [label=MmBackward0]
	5347106608 -> 5347109296
	5347106608 [label=ReluBackward0]
	5347107952 -> 5347106608
	5347107952 [label=NativeLayerNormBackward0]
	5347107904 -> 5347107952
	5347107904 [label=MmBackward0]
	5347107664 -> 5347107904
	5347107664 [label=ReluBackward0]
	5347106320 -> 5347107664
	5347106320 [label=NativeLayerNormBackward0]
	5347109344 -> 5347106320
	5347109344 [label=AddBackward0]
	5347107424 -> 5347109344
	5347107424 [label=MmBackward0]
	5347107280 -> 5347107424
	5347107280 [label=ReluBackward0]
	5347106896 -> 5347107280
	5347106896 [label=NativeLayerNormBackward0]
	5347110208 -> 5347106896
	5347110208 [label=MmBackward0]
	5347110400 -> 5347110208
	5347110400 [label=ReluBackward0]
	5347110544 -> 5347110400
	5347110544 [label=NativeLayerNormBackward0]
	5347107472 -> 5347110544
	5347107472 [label=AddmmBackward0]
	5347110784 -> 5347107472
	5347205328 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5347205328 -> 5347110784
	5347110784 [label=AccumulateGrad]
	5347110736 -> 5347107472
	5347110736 [label=TBackward0]
	5347110880 -> 5347110736
	5347202528 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5347202528 -> 5347110880
	5347110880 [label=AccumulateGrad]
	5347110640 -> 5347110544
	4316937536 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4316937536 -> 5347110640
	5347110640 [label=AccumulateGrad]
	5347110592 -> 5347110544
	4316936736 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4316936736 -> 5347110592
	5347110592 [label=AccumulateGrad]
	5347110352 -> 5347110208
	5347110352 [label=TBackward0]
	5347110832 -> 5347110352
	5347206848 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5347206848 -> 5347110832
	5347110832 [label=AccumulateGrad]
	5347110160 -> 5347106896
	5347206928 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5347206928 -> 5347110160
	5347110160 [label=AccumulateGrad]
	5347107088 -> 5347106896
	5347207008 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5347207008 -> 5347107088
	5347107088 [label=AccumulateGrad]
	5347107328 -> 5347107424
	5347107328 [label=TBackward0]
	5347110448 -> 5347107328
	5347207088 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5347207088 -> 5347110448
	5347110448 [label=AccumulateGrad]
	5347107472 -> 5347109344
	5347108768 -> 5347106320
	5347206768 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5347206768 -> 5347108768
	5347108768 [label=AccumulateGrad]
	5347106272 -> 5347106320
	5347207168 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5347207168 -> 5347106272
	5347106272 [label=AccumulateGrad]
	5347107712 -> 5347107904
	5347107712 [label=TBackward0]
	5347107376 -> 5347107712
	5347207248 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5347207248 -> 5347107376
	5347107376 [label=AccumulateGrad]
	5347106032 -> 5347107952
	5347205168 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5347205168 -> 5347106032
	5347106032 [label=AccumulateGrad]
	5347108864 -> 5347107952
	5347207408 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5347207408 -> 5347108864
	5347108864 [label=AccumulateGrad]
	5347106560 -> 5347109296
	5347106560 [label=TBackward0]
	5347107568 -> 5347106560
	5347207328 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5347207328 -> 5347107568
	5347107568 [label=AccumulateGrad]
	5347109344 -> 5347110064
	5347106368 -> 5345949488
	5347106368 [label=TBackward0]
	5347107856 -> 5347106368
	5331976048 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5331976048 -> 5347107856
	5347107856 [label=AccumulateGrad]
	5345949488 -> 5333329680
}
