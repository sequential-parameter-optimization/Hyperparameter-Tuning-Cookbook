digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4428792048 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4663424048 [label=AddmmBackward0]
	4663557616 -> 4663424048
	4663755312 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4663755312 -> 4663557616
	4663557616 [label=AccumulateGrad]
	4663557664 -> 4663424048
	4663557664 [label=AddBackward0]
	4663557472 -> 4663557664
	4663557472 [label=MmBackward0]
	4663556752 -> 4663557472
	4663556752 [label=ReluBackward0]
	4663557808 -> 4663556752
	4663557808 [label=NativeLayerNormBackward0]
	4663558000 -> 4663557808
	4663558000 [label=MmBackward0]
	4663556176 -> 4663558000
	4663556176 [label=ReluBackward0]
	4663557424 -> 4663556176
	4663557424 [label=NativeLayerNormBackward0]
	4663557520 -> 4663557424
	4663557520 [label=AddBackward0]
	4663542016 -> 4663557520
	4663542016 [label=MmBackward0]
	4498483392 -> 4663542016
	4498483392 [label=ReluBackward0]
	4663853264 -> 4498483392
	4663853264 [label=NativeLayerNormBackward0]
	4663853360 -> 4663853264
	4663853360 [label=MmBackward0]
	4663853552 -> 4663853360
	4663853552 [label=ReluBackward0]
	4663853696 -> 4663853552
	4663853696 [label=NativeLayerNormBackward0]
	4663553968 -> 4663853696
	4663553968 [label=AddmmBackward0]
	4663853936 -> 4663553968
	4663620576 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4663620576 -> 4663853936
	4663853936 [label=AccumulateGrad]
	4663853888 -> 4663553968
	4663853888 [label=TBackward0]
	4663854032 -> 4663853888
	4663622496 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4663622496 -> 4663854032
	4663854032 [label=AccumulateGrad]
	4663853792 -> 4663853696
	4394324912 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4394324912 -> 4663853792
	4663853792 [label=AccumulateGrad]
	4663853744 -> 4663853696
	4394324992 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4394324992 -> 4663853744
	4663853744 [label=AccumulateGrad]
	4663853504 -> 4663853360
	4663853504 [label=TBackward0]
	4663853984 -> 4663853504
	4661281088 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4661281088 -> 4663853984
	4663853984 [label=AccumulateGrad]
	4663853312 -> 4663853264
	4660950464 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4660950464 -> 4663853312
	4663853312 [label=AccumulateGrad]
	4663853168 -> 4663853264
	4660949424 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4660949424 -> 4663853168
	4663853168 [label=AccumulateGrad]
	4498474080 -> 4663542016
	4498474080 [label=TBackward0]
	4663853600 -> 4498474080
	4663620656 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4663620656 -> 4663853600
	4663853600 [label=AccumulateGrad]
	4663553968 -> 4663557520
	4663553920 -> 4663557424
	4663621136 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4663621136 -> 4663553920
	4663553920 [label=AccumulateGrad]
	4663557280 -> 4663557424
	4663621216 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4663621216 -> 4663557280
	4663557280 [label=AccumulateGrad]
	4663557376 -> 4663558000
	4663557376 [label=TBackward0]
	4663555984 -> 4663557376
	4663622416 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4663622416 -> 4663555984
	4663555984 [label=AccumulateGrad]
	4663557952 -> 4663557808
	4663622576 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4663622576 -> 4663557952
	4663557952 [label=AccumulateGrad]
	4663556800 -> 4663557808
	4663620496 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4663620496 -> 4663556800
	4663556800 [label=AccumulateGrad]
	4663557760 -> 4663557472
	4663557760 [label=TBackward0]
	4663556368 -> 4663557760
	4663622656 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4663622656 -> 4663556368
	4663556368 [label=AccumulateGrad]
	4663557520 -> 4663557664
	4663556848 -> 4663424048
	4663556848 [label=TBackward0]
	4663556560 -> 4663556848
	4663622736 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4663622736 -> 4663556560
	4663556560 [label=AccumulateGrad]
	4663424048 -> 4428792048
}
