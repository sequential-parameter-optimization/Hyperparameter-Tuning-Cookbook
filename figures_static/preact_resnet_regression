digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4638004064 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4645347744 [label=AddmmBackward0]
	4645777280 -> 4645347744
	4645906192 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4645906192 -> 4645777280
	4645777280 [label=AccumulateGrad]
	4645778384 -> 4645347744
	4645778384 [label=AddBackward0]
	4645781312 -> 4645778384
	4645781312 [label=MmBackward0]
	4635087184 -> 4645781312
	4635087184 [label=ReluBackward0]
	4638066576 -> 4635087184
	4638066576 [label=NativeLayerNormBackward0]
	4638069024 -> 4638066576
	4638069024 [label=MmBackward0]
	4638067920 -> 4638069024
	4638067920 [label=ReluBackward0]
	4638066192 -> 4638067920
	4638066192 [label=NativeLayerNormBackward0]
	4645777760 -> 4638066192
	4645777760 [label=AddBackward0]
	4638066816 -> 4645777760
	4638066816 [label=MmBackward0]
	4646060240 -> 4638066816
	4646060240 [label=ReluBackward0]
	4646060384 -> 4646060240
	4646060384 [label=NativeLayerNormBackward0]
	4646060480 -> 4646060384
	4646060480 [label=MmBackward0]
	4646060672 -> 4646060480
	4646060672 [label=ReluBackward0]
	4646060816 -> 4646060672
	4646060816 [label=NativeLayerNormBackward0]
	4638068928 -> 4646060816
	4638068928 [label=AddmmBackward0]
	4646061056 -> 4638068928
	4645902512 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4645902512 -> 4646061056
	4646061056 [label=AccumulateGrad]
	4646061008 -> 4638068928
	4646061008 [label=TBackward0]
	4646061152 -> 4646061008
	4645904432 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4645904432 -> 4646061152
	4646061152 [label=AccumulateGrad]
	4646060912 -> 4646060816
	4428132464 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4428132464 -> 4646060912
	4646060912 [label=AccumulateGrad]
	4646060864 -> 4646060816
	4638004704 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4638004704 -> 4646060864
	4646060864 [label=AccumulateGrad]
	4646060624 -> 4646060480
	4646060624 [label=TBackward0]
	4646061104 -> 4646060624
	4638008544 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4638008544 -> 4646061104
	4646061104 [label=AccumulateGrad]
	4646060432 -> 4646060384
	4428132784 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4428132784 -> 4646060432
	4646060432 [label=AccumulateGrad]
	4646060288 -> 4646060384
	4638008464 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4638008464 -> 4646060288
	4646060288 [label=AccumulateGrad]
	4646060192 -> 4638066816
	4646060192 [label=TBackward0]
	4646060720 -> 4646060192
	4638006304 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4638006304 -> 4646060720
	4646060720 [label=AccumulateGrad]
	4638068928 -> 4645777760
	4638066336 -> 4638066192
	4645904192 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4645904192 -> 4638066336
	4638066336 [label=AccumulateGrad]
	4638066144 -> 4638066192
	4645902832 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4645902832 -> 4638066144
	4638066144 [label=AccumulateGrad]
	4638068592 -> 4638069024
	4638068592 [label=TBackward0]
	4638066528 -> 4638068592
	4645903152 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4645903152 -> 4638066528
	4638066528 [label=AccumulateGrad]
	4638066240 -> 4638066576
	4645904592 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4645904592 -> 4638066240
	4638066240 [label=AccumulateGrad]
	4638066720 -> 4638066576
	4645904512 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4645904512 -> 4638066720
	4638066720 [label=AccumulateGrad]
	4645781120 -> 4645781312
	4645781120 [label=TBackward0]
	4638067200 -> 4645781120
	4645906032 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4645906032 -> 4638067200
	4638067200 [label=AccumulateGrad]
	4645777760 -> 4645778384
	4645779008 -> 4645347744
	4645779008 [label=TBackward0]
	4645776848 -> 4645779008
	4645906112 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4645906112 -> 4645776848
	4645776848 [label=AccumulateGrad]
	4645347744 -> 4638004064
}
