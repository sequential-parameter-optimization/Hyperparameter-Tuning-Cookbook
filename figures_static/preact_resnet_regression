digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4426932992 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	4670452784 [label=AddmmBackward0]
	4670897024 -> 4670452784
	4671004080 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4671004080 -> 4670897024
	4670897024 [label=AccumulateGrad]
	4670897600 -> 4670452784
	4670897600 [label=AddBackward0]
	4670896352 -> 4670897600
	4670896352 [label=MmBackward0]
	4670894432 -> 4670896352
	4670894432 [label=ReluBackward0]
	4670897744 -> 4670894432
	4670897744 [label=NativeLayerNormBackward0]
	4670896256 -> 4670897744
	4670896256 [label=MmBackward0]
	4670897936 -> 4670896256
	4670897936 [label=ReluBackward0]
	4670897984 -> 4670897936
	4670897984 [label=NativeLayerNormBackward0]
	4670896784 -> 4670897984
	4670896784 [label=AddBackward0]
	4670898080 -> 4670896784
	4670898080 [label=MmBackward0]
	4670895104 -> 4670898080
	4670895104 [label=ReluBackward0]
	4670893760 -> 4670895104
	4670893760 [label=NativeLayerNormBackward0]
	4670894144 -> 4670893760
	4670894144 [label=MmBackward0]
	4671160624 -> 4670894144
	4671160624 [label=ReluBackward0]
	4671160768 -> 4671160624
	4671160768 [label=NativeLayerNormBackward0]
	4670898032 -> 4671160768
	4670898032 [label=AddmmBackward0]
	4671161008 -> 4670898032
	4671004160 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4671004160 -> 4671161008
	4671161008 [label=AccumulateGrad]
	4671160960 -> 4670898032
	4671160960 [label=TBackward0]
	4671161104 -> 4671160960
	4671002240 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4671002240 -> 4671161104
	4671161104 [label=AccumulateGrad]
	4671160864 -> 4671160768
	4407602832 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4407602832 -> 4671160864
	4671160864 [label=AccumulateGrad]
	4671160816 -> 4671160768
	4668351152 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4668351152 -> 4671160816
	4671160816 [label=AccumulateGrad]
	4671160576 -> 4670894144
	4671160576 [label=TBackward0]
	4671161056 -> 4671160576
	4668348112 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4668348112 -> 4671161056
	4671161056 [label=AccumulateGrad]
	4671160432 -> 4670893760
	4668350992 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	4668350992 -> 4671160432
	4671160432 [label=AccumulateGrad]
	4671160384 -> 4670893760
	4668351392 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	4668351392 -> 4671160384
	4671160384 [label=AccumulateGrad]
	4670894000 -> 4670898080
	4670894000 [label=TBackward0]
	4670893712 -> 4670894000
	4407602912 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4407602912 -> 4670893712
	4670893712 [label=AccumulateGrad]
	4670898032 -> 4670896784
	4670897264 -> 4670897984
	4668351472 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	4668351472 -> 4670897264
	4670897264 [label=AccumulateGrad]
	4670897312 -> 4670897984
	4407602672 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	4407602672 -> 4670897312
	4670897312 [label=AccumulateGrad]
	4670897888 -> 4670896256
	4670897888 [label=TBackward0]
	4670897216 -> 4670897888
	4654538320 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	4654538320 -> 4670897216
	4670897216 [label=AccumulateGrad]
	4670897792 -> 4670897744
	4671003920 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	4671003920 -> 4670897792
	4670897792 [label=AccumulateGrad]
	4670893616 -> 4670897744
	4671002160 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	4671002160 -> 4670893616
	4670893616 [label=AccumulateGrad]
	4670896928 -> 4670896352
	4670896928 [label=TBackward0]
	4670897120 -> 4670896928
	4671002320 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	4671002320 -> 4670897120
	4670897120 [label=AccumulateGrad]
	4670896784 -> 4670897600
	4670896688 -> 4670452784
	4670896688 [label=TBackward0]
	4670894192 -> 4670896688
	4671004240 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4671004240 -> 4670894192
	4670894192 [label=AccumulateGrad]
	4670452784 -> 4426932992
}
