digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5049540336 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	5049485392 [label=AddmmBackward0]
	5049487072 -> 5049485392
	5049539936 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5049539936 -> 5049487072
	5049487072 [label=AccumulateGrad]
	5049487120 -> 5049485392
	5049487120 [label=AddBackward0]
	5049484048 -> 5049487120
	5049484048 [label=MmBackward0]
	5049486544 -> 5049484048
	5049486544 [label=ReluBackward0]
	5049231312 -> 5049486544
	5049231312 [label=NativeLayerNormBackward0]
	5049487936 -> 5049231312
	5049487936 [label=MmBackward0]
	5049487168 -> 5049487936
	5049487168 [label=ReluBackward0]
	5049487360 -> 5049487168
	5049487360 [label=NativeLayerNormBackward0]
	5049484432 -> 5049487360
	5049484432 [label=AddBackward0]
	5049488272 -> 5049484432
	5049488272 [label=MmBackward0]
	5049488512 -> 5049488272
	5049488512 [label=ReluBackward0]
	5049488656 -> 5049488512
	5049488656 [label=NativeLayerNormBackward0]
	5049488752 -> 5049488656
	5049488752 [label=MmBackward0]
	5049488944 -> 5049488752
	5049488944 [label=ReluBackward0]
	5049489088 -> 5049488944
	5049489088 [label=NativeLayerNormBackward0]
	5049488224 -> 5049489088
	5049488224 [label=AddmmBackward0]
	5049489328 -> 5049488224
	5049537936 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5049537936 -> 5049489328
	5049489328 [label=AccumulateGrad]
	5049489280 -> 5049488224
	5049489280 [label=TBackward0]
	5049489424 -> 5049489280
	5049538016 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5049538016 -> 5049489424
	5049489424 [label=AccumulateGrad]
	5049489184 -> 5049489088
	4357925344 [label="blocks.0.net.0.weight
 (64)" fillcolor=lightblue]
	4357925344 -> 5049489184
	5049489184 [label=AccumulateGrad]
	5049489136 -> 5049489088
	4357925264 [label="blocks.0.net.0.bias
 (64)" fillcolor=lightblue]
	4357925264 -> 5049489136
	5049489136 [label=AccumulateGrad]
	5049488896 -> 5049488752
	5049488896 [label=TBackward0]
	5049489376 -> 5049488896
	5036305664 [label="blocks.0.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5036305664 -> 5049489376
	5049489376 [label=AccumulateGrad]
	5049488704 -> 5049488656
	5049536656 [label="blocks.0.net.3.weight
 (64)" fillcolor=lightblue]
	5049536656 -> 5049488704
	5049488704 [label=AccumulateGrad]
	5049488560 -> 5049488656
	5049536416 [label="blocks.0.net.3.bias
 (64)" fillcolor=lightblue]
	5049536416 -> 5049488560
	5049488560 [label=AccumulateGrad]
	5049488416 -> 5049488272
	5049488416 [label=TBackward0]
	5049488992 -> 5049488416
	5049536496 [label="blocks.0.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5049536496 -> 5049488992
	5049488992 [label=AccumulateGrad]
	5049488224 -> 5049484432
	5049488128 -> 5049487360
	5049536096 [label="blocks.1.net.0.weight
 (64)" fillcolor=lightblue]
	5049536096 -> 5049488128
	5049488128 [label=AccumulateGrad]
	5049487312 -> 5049487360
	5049538176 [label="blocks.1.net.0.bias
 (64)" fillcolor=lightblue]
	5049538176 -> 5049487312
	5049487312 [label=AccumulateGrad]
	5049487216 -> 5049487936
	5049487216 [label=TBackward0]
	5049488368 -> 5049487216
	5049538096 [label="blocks.1.net.2.weight
 (64, 64)" fillcolor=lightblue]
	5049538096 -> 5049488368
	5049488368 [label=AccumulateGrad]
	5049487888 -> 5049231312
	5049539616 [label="blocks.1.net.3.weight
 (64)" fillcolor=lightblue]
	5049539616 -> 5049487888
	5049487888 [label=AccumulateGrad]
	5049487744 -> 5049231312
	5049539696 [label="blocks.1.net.3.bias
 (64)" fillcolor=lightblue]
	5049539696 -> 5049487744
	5049487744 [label=AccumulateGrad]
	5049487792 -> 5049484048
	5049487792 [label=TBackward0]
	5049488080 -> 5049487792
	5049539776 [label="blocks.1.net.5.weight
 (64, 64)" fillcolor=lightblue]
	5049539776 -> 5049488080
	5049488080 [label=AccumulateGrad]
	5049484432 -> 5049487120
	5049487024 -> 5049485392
	5049487024 [label=TBackward0]
	5049487984 -> 5049487024
	5049539856 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5049539856 -> 5049487984
	5049487984 [label=AccumulateGrad]
	5049485392 -> 5049540336
}
