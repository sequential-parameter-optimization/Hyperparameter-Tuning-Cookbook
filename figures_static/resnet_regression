digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5347071216 [label="
 (2, 1)" fillcolor=darkolivegreen1]
	5333241424 [label=AddmmBackward0]
	5346160704 -> 5333241424
	5333325360 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5333325360 -> 5346160704
	5346160704 [label=AccumulateGrad]
	5347106272 -> 5333241424
	5347106272 [label=ReluBackward0]
	5347106080 -> 5347106272
	5347106080 [label=AddBackward0]
	5347106320 -> 5347106080
	5347106320 [label=NativeBatchNormBackward0]
	5347106560 -> 5347106320
	5347106560 [label=MmBackward0]
	5347106752 -> 5347106560
	5347106752 [label=ReluBackward0]
	5347106896 -> 5347106752
	5347106896 [label=NativeBatchNormBackward0]
	5347106992 -> 5347106896
	5347106992 [label=MmBackward0]
	5347106032 -> 5347106992
	5347106032 [label=ReluBackward0]
	5347107280 -> 5347106032
	5347107280 [label=AddBackward0]
	5347107376 -> 5347107280
	5347107376 [label=NativeBatchNormBackward0]
	5347107520 -> 5347107376
	5347107520 [label=MmBackward0]
	5347107712 -> 5347107520
	5347107712 [label=ReluBackward0]
	5347107856 -> 5347107712
	5347107856 [label=NativeBatchNormBackward0]
	5347107952 -> 5347107856
	5347107952 [label=MmBackward0]
	5347107328 -> 5347107952
	5347107328 [label=AddmmBackward0]
	5347108240 -> 5347107328
	5333321440 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5333321440 -> 5347108240
	5347108240 [label=AccumulateGrad]
	5347108192 -> 5347107328
	5347108192 [label=TBackward0]
	5347108336 -> 5347108192
	5333321120 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5333321120 -> 5347108336
	5347108336 [label=AccumulateGrad]
	5347108144 -> 5347107952
	5347108144 [label=TBackward0]
	5347108480 -> 5347108144
	5333322400 [label="blocks.0.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5333322400 -> 5347108480
	5347108480 [label=AccumulateGrad]
	5347107904 -> 5347107856
	5333322560 [label="blocks.0.net.1.weight
 (64)" fillcolor=lightblue]
	5333322560 -> 5347107904
	5347107904 [label=AccumulateGrad]
	5347107760 -> 5347107856
	5333322480 [label="blocks.0.net.1.bias
 (64)" fillcolor=lightblue]
	5333322480 -> 5347107760
	5347107760 [label=AccumulateGrad]
	5347107664 -> 5347107520
	5347107664 [label=TBackward0]
	5347108288 -> 5347107664
	5333323120 [label="blocks.0.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5333323120 -> 5347108288
	5347108288 [label=AccumulateGrad]
	5347107472 -> 5347107376
	5333323200 [label="blocks.0.net.4.weight
 (64)" fillcolor=lightblue]
	5333323200 -> 5347107472
	5347107472 [label=AccumulateGrad]
	5347107424 -> 5347107376
	5333323280 [label="blocks.0.net.4.bias
 (64)" fillcolor=lightblue]
	5333323280 -> 5347107424
	5347107424 [label=AccumulateGrad]
	5347107328 -> 5347107280
	5347107184 -> 5347106992
	5347107184 [label=TBackward0]
	5347107568 -> 5347107184
	5333323840 [label="blocks.1.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5333323840 -> 5347107568
	5347107568 [label=AccumulateGrad]
	5347106944 -> 5347106896
	5333323920 [label="blocks.1.net.1.weight
 (64)" fillcolor=lightblue]
	5333323920 -> 5347106944
	5347106944 [label=AccumulateGrad]
	5347106800 -> 5347106896
	5333324000 [label="blocks.1.net.1.bias
 (64)" fillcolor=lightblue]
	5333324000 -> 5347106800
	5347106800 [label=AccumulateGrad]
	5347106704 -> 5347106560
	5347106704 [label=TBackward0]
	5347107088 -> 5347106704
	5333324400 [label="blocks.1.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5333324400 -> 5347107088
	5347107088 [label=AccumulateGrad]
	5347106512 -> 5347106320
	5333324480 [label="blocks.1.net.4.weight
 (64)" fillcolor=lightblue]
	5333324480 -> 5347106512
	5347106512 [label=AccumulateGrad]
	5347106368 -> 5347106320
	5333324560 [label="blocks.1.net.4.bias
 (64)" fillcolor=lightblue]
	5333324560 -> 5347106368
	5347106368 [label=AccumulateGrad]
	5347106032 -> 5347106080
	5347106224 -> 5333241424
	5347106224 [label=TBackward0]
	5347106608 -> 5347106224
	5333325280 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5333325280 -> 5347106608
	5347106608 [label=AccumulateGrad]
	5333241424 -> 5347071216
}
