digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4607549888 [label="
 (2, 1)" fillcolor=darkolivegreen1]
	4419191184 [label=AddmmBackward0]
	4607616112 -> 4419191184
	4607549088 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4607549088 -> 4607616112
	4607616112 [label=AccumulateGrad]
	4610076688 -> 4419191184
	4610076688 [label=ReluBackward0]
	4610065264 -> 4610076688
	4610065264 [label=AddBackward0]
	4610076640 -> 4610065264
	4610076640 [label=NativeBatchNormBackward0]
	4610076928 -> 4610076640
	4610076928 [label=MmBackward0]
	4610077120 -> 4610076928
	4610077120 [label=ReluBackward0]
	4610077216 -> 4610077120
	4610077216 [label=NativeBatchNormBackward0]
	4610077312 -> 4610077216
	4610077312 [label=MmBackward0]
	4610076832 -> 4610077312
	4610076832 [label=ReluBackward0]
	4610077600 -> 4610076832
	4610077600 [label=AddBackward0]
	4610077696 -> 4610077600
	4610077696 [label=NativeBatchNormBackward0]
	4610077840 -> 4610077696
	4610077840 [label=MmBackward0]
	4610078032 -> 4610077840
	4610078032 [label=ReluBackward0]
	4610078176 -> 4610078032
	4610078176 [label=NativeBatchNormBackward0]
	4610078272 -> 4610078176
	4610078272 [label=MmBackward0]
	4610077648 -> 4610078272
	4610077648 [label=AddmmBackward0]
	4610078560 -> 4610077648
	4607546688 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4607546688 -> 4610078560
	4610078560 [label=AccumulateGrad]
	4610078512 -> 4610077648
	4610078512 [label=TBackward0]
	4610078656 -> 4610078512
	4607545648 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4607545648 -> 4610078656
	4610078656 [label=AccumulateGrad]
	4610078464 -> 4610078272
	4610078464 [label=TBackward0]
	4610078800 -> 4610078464
	4365670896 [label="blocks.0.net.0.weight
 (64, 64)" fillcolor=lightblue]
	4365670896 -> 4610078800
	4610078800 [label=AccumulateGrad]
	4610078224 -> 4610078176
	4359886544 [label="blocks.0.net.1.weight
 (64)" fillcolor=lightblue]
	4359886544 -> 4610078224
	4610078224 [label=AccumulateGrad]
	4610078080 -> 4610078176
	4365674096 [label="blocks.0.net.1.bias
 (64)" fillcolor=lightblue]
	4365674096 -> 4610078080
	4610078080 [label=AccumulateGrad]
	4610077984 -> 4610077840
	4610077984 [label=TBackward0]
	4610078608 -> 4610077984
	4607545008 [label="blocks.0.net.3.weight
 (64, 64)" fillcolor=lightblue]
	4607545008 -> 4610078608
	4610078608 [label=AccumulateGrad]
	4610077792 -> 4610077696
	4607546448 [label="blocks.0.net.4.weight
 (64)" fillcolor=lightblue]
	4607546448 -> 4610077792
	4610077792 [label=AccumulateGrad]
	4610077744 -> 4610077696
	4607546768 [label="blocks.0.net.4.bias
 (64)" fillcolor=lightblue]
	4607546768 -> 4610077744
	4610077744 [label=AccumulateGrad]
	4610077648 -> 4610077600
	4610077504 -> 4610077312
	4610077504 [label=TBackward0]
	4610077888 -> 4610077504
	4607548048 [label="blocks.1.net.0.weight
 (64, 64)" fillcolor=lightblue]
	4607548048 -> 4610077888
	4610077888 [label=AccumulateGrad]
	4610077264 -> 4610077216
	4607548128 [label="blocks.1.net.1.weight
 (64)" fillcolor=lightblue]
	4607548128 -> 4610077264
	4610077264 [label=AccumulateGrad]
	4610076592 -> 4610077216
	4607548208 [label="blocks.1.net.1.bias
 (64)" fillcolor=lightblue]
	4607548208 -> 4610076592
	4610076592 [label=AccumulateGrad]
	4610077072 -> 4610076928
	4610077072 [label=TBackward0]
	4610077408 -> 4610077072
	4607548608 [label="blocks.1.net.3.weight
 (64, 64)" fillcolor=lightblue]
	4607548608 -> 4610077408
	4610077408 [label=AccumulateGrad]
	4610076880 -> 4610076640
	4607548688 [label="blocks.1.net.4.weight
 (64)" fillcolor=lightblue]
	4607548688 -> 4610076880
	4610076880 [label=AccumulateGrad]
	4610076784 -> 4610076640
	4607548768 [label="blocks.1.net.4.bias
 (64)" fillcolor=lightblue]
	4607548768 -> 4610076784
	4610076784 [label=AccumulateGrad]
	4610076832 -> 4610065264
	4610076544 -> 4419191184
	4610076544 [label=TBackward0]
	4610076976 -> 4610076544
	4607549008 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4607549008 -> 4610076976
	4610076976 [label=AccumulateGrad]
	4419191184 -> 4607549888
}
