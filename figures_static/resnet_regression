digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5142873504 [label="
 (2, 1)" fillcolor=darkolivegreen1]
	5143869424 [label=AddmmBackward0]
	5142645392 -> 5143869424
	5142872864 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5142872864 -> 5142645392
	5142645392 [label=AccumulateGrad]
	5145682096 -> 5143869424
	5145682096 [label=ReluBackward0]
	5145682336 -> 5145682096
	5145682336 [label=AddBackward0]
	5145682240 -> 5145682336
	5145682240 [label=NativeBatchNormBackward0]
	5145682480 -> 5145682240
	5145682480 [label=MmBackward0]
	5145682192 -> 5145682480
	5145682192 [label=ReluBackward0]
	5145682720 -> 5145682192
	5145682720 [label=NativeBatchNormBackward0]
	5145682816 -> 5145682720
	5145682816 [label=MmBackward0]
	5145682384 -> 5145682816
	5145682384 [label=ReluBackward0]
	5145683104 -> 5145682384
	5145683104 [label=AddBackward0]
	5145683200 -> 5145683104
	5145683200 [label=NativeBatchNormBackward0]
	5145683344 -> 5145683200
	5145683344 [label=MmBackward0]
	5145683536 -> 5145683344
	5145683536 [label=ReluBackward0]
	5145683680 -> 5145683536
	5145683680 [label=NativeBatchNormBackward0]
	5145683776 -> 5145683680
	5145683776 [label=MmBackward0]
	5145683248 -> 5145683776
	5145683248 [label=AddmmBackward0]
	5145684064 -> 5145683248
	5139658976 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5139658976 -> 5145684064
	5145684064 [label=AccumulateGrad]
	5145684112 -> 5145683248
	5145684112 [label=TBackward0]
	5145684256 -> 5145684112
	5139660096 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5139660096 -> 5145684256
	5145684256 [label=AccumulateGrad]
	5145683968 -> 5145683776
	5145683968 [label=TBackward0]
	5145684160 -> 5145683968
	5139661040 [label="blocks.0.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5139661040 -> 5145684160
	5145684160 [label=AccumulateGrad]
	5145683824 -> 5145683680
	5140424384 [label="blocks.0.net.1.weight
 (64)" fillcolor=lightblue]
	5140424384 -> 5145683824
	5145683824 [label=AccumulateGrad]
	5145683632 -> 5145683680
	5139922080 [label="blocks.0.net.1.bias
 (64)" fillcolor=lightblue]
	5139922080 -> 5145683632
	5145683632 [label=AccumulateGrad]
	5145683584 -> 5145683344
	5145683584 [label=TBackward0]
	5145683872 -> 5145683584
	5139658016 [label="blocks.0.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5139658016 -> 5145683872
	5145683872 [label=AccumulateGrad]
	5145683392 -> 5145683200
	5142673472 [label="blocks.0.net.4.weight
 (64)" fillcolor=lightblue]
	5142673472 -> 5145683392
	5145683392 [label=AccumulateGrad]
	5145683296 -> 5145683200
	5142672752 [label="blocks.0.net.4.bias
 (64)" fillcolor=lightblue]
	5142672752 -> 5145683296
	5145683296 [label=AccumulateGrad]
	5145683248 -> 5145683104
	5145683008 -> 5145682816
	5145683008 [label=TBackward0]
	5145682960 -> 5145683008
	5142675072 [label="blocks.1.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5142675072 -> 5145682960
	5145682960 [label=AccumulateGrad]
	5145682864 -> 5145682720
	5142675152 [label="blocks.1.net.1.weight
 (64)" fillcolor=lightblue]
	5142675152 -> 5145682864
	5145682864 [label=AccumulateGrad]
	5145682624 -> 5145682720
	5142675232 [label="blocks.1.net.1.bias
 (64)" fillcolor=lightblue]
	5142675232 -> 5145682624
	5145682624 [label=AccumulateGrad]
	5145682672 -> 5145682480
	5145682672 [label=TBackward0]
	5145682912 -> 5145682672
	5142872304 [label="blocks.1.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5142872304 -> 5145682912
	5145682912 [label=AccumulateGrad]
	5145682528 -> 5145682240
	5142872384 [label="blocks.1.net.4.weight
 (64)" fillcolor=lightblue]
	5142872384 -> 5145682528
	5145682528 [label=AccumulateGrad]
	5145682432 -> 5145682240
	5142872464 [label="blocks.1.net.4.bias
 (64)" fillcolor=lightblue]
	5142872464 -> 5145682432
	5145682432 [label=AccumulateGrad]
	5145682384 -> 5145682336
	5145682144 -> 5143869424
	5145682144 [label=TBackward0]
	5145682288 -> 5145682144
	5142872784 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5142872784 -> 5145682288
	5145682288 [label=AccumulateGrad]
	5143869424 -> 5142873504
}
