digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4672918928 [label="
 (2, 1)" fillcolor=darkolivegreen1]
	4672949792 [label=AddmmBackward0]
	4672949744 -> 4672949792
	4672918208 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	4672918208 -> 4672949744
	4672949744 [label=AccumulateGrad]
	4672953872 -> 4672949792
	4672953872 [label=ReluBackward0]
	4658233696 -> 4672953872
	4658233696 [label=AddBackward0]
	4675414928 -> 4658233696
	4675414928 [label=NativeBatchNormBackward0]
	4675415264 -> 4675414928
	4675415264 [label=MmBackward0]
	4675415456 -> 4675415264
	4675415456 [label=ReluBackward0]
	4675415600 -> 4675415456
	4675415600 [label=NativeBatchNormBackward0]
	4675415696 -> 4675415600
	4675415696 [label=MmBackward0]
	4675415024 -> 4675415696
	4675415024 [label=ReluBackward0]
	4675415984 -> 4675415024
	4675415984 [label=AddBackward0]
	4675416080 -> 4675415984
	4675416080 [label=NativeBatchNormBackward0]
	4675416224 -> 4675416080
	4675416224 [label=MmBackward0]
	4675416416 -> 4675416224
	4675416416 [label=ReluBackward0]
	4675416560 -> 4675416416
	4675416560 [label=NativeBatchNormBackward0]
	4675416656 -> 4675416560
	4675416656 [label=MmBackward0]
	4675416032 -> 4675416656
	4675416032 [label=AddmmBackward0]
	4675416944 -> 4675416032
	4672915728 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4672915728 -> 4675416944
	4675416944 [label=AccumulateGrad]
	4675416896 -> 4675416032
	4675416896 [label=TBackward0]
	4675417040 -> 4675416896
	4672914768 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4672914768 -> 4675417040
	4675417040 [label=AccumulateGrad]
	4675416848 -> 4675416656
	4675416848 [label=TBackward0]
	4675417184 -> 4675416848
	4431147200 [label="blocks.0.net.0.weight
 (64, 64)" fillcolor=lightblue]
	4431147200 -> 4675417184
	4675417184 [label=AccumulateGrad]
	4675416608 -> 4675416560
	4466973888 [label="blocks.0.net.1.weight
 (64)" fillcolor=lightblue]
	4466973888 -> 4675416608
	4675416608 [label=AccumulateGrad]
	4675416464 -> 4675416560
	4672721040 [label="blocks.0.net.1.bias
 (64)" fillcolor=lightblue]
	4672721040 -> 4675416464
	4675416464 [label=AccumulateGrad]
	4675416368 -> 4675416224
	4675416368 [label=TBackward0]
	4675416992 -> 4675416368
	4669202080 [label="blocks.0.net.3.weight
 (64, 64)" fillcolor=lightblue]
	4669202080 -> 4675416992
	4675416992 [label=AccumulateGrad]
	4675416176 -> 4675416080
	4672915088 [label="blocks.0.net.4.weight
 (64)" fillcolor=lightblue]
	4672915088 -> 4675416176
	4675416176 [label=AccumulateGrad]
	4675416128 -> 4675416080
	4672914128 [label="blocks.0.net.4.bias
 (64)" fillcolor=lightblue]
	4672914128 -> 4675416128
	4675416128 [label=AccumulateGrad]
	4675416032 -> 4675415984
	4675415888 -> 4675415696
	4675415888 [label=TBackward0]
	4675416272 -> 4675415888
	4672917008 [label="blocks.1.net.0.weight
 (64, 64)" fillcolor=lightblue]
	4672917008 -> 4675416272
	4675416272 [label=AccumulateGrad]
	4675415648 -> 4675415600
	4672917088 [label="blocks.1.net.1.weight
 (64)" fillcolor=lightblue]
	4672917088 -> 4675415648
	4675415648 [label=AccumulateGrad]
	4675415504 -> 4675415600
	4672917168 [label="blocks.1.net.1.bias
 (64)" fillcolor=lightblue]
	4672917168 -> 4675415504
	4675415504 [label=AccumulateGrad]
	4675415408 -> 4675415264
	4675415408 [label=TBackward0]
	4675415792 -> 4675415408
	4672917568 [label="blocks.1.net.3.weight
 (64, 64)" fillcolor=lightblue]
	4672917568 -> 4675415792
	4675415792 [label=AccumulateGrad]
	4675415168 -> 4675414928
	4672917648 [label="blocks.1.net.4.weight
 (64)" fillcolor=lightblue]
	4672917648 -> 4675415168
	4675415168 [label=AccumulateGrad]
	4675415120 -> 4675414928
	4672917728 [label="blocks.1.net.4.bias
 (64)" fillcolor=lightblue]
	4672917728 -> 4675415120
	4675415120 [label=AccumulateGrad]
	4675415024 -> 4658233696
	4672952144 -> 4672949792
	4672952144 [label=TBackward0]
	4675415360 -> 4672952144
	4672918128 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	4672918128 -> 4675415360
	4675415360 [label=AccumulateGrad]
	4672949792 -> 4672918928
}
