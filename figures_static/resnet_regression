digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5209576048 [label="
 (2, 1)" fillcolor=darkolivegreen1]
	5210341776 [label=AddmmBackward0]
	5185262624 -> 5210341776
	5209575648 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5209575648 -> 5185262624
	5185262624 [label=AccumulateGrad]
	5456309952 -> 5210341776
	5456309952 [label=ReluBackward0]
	5456298528 -> 5456309952
	5456298528 [label=AddBackward0]
	5456310048 -> 5456298528
	5456310048 [label=NativeBatchNormBackward0]
	5456310240 -> 5456310048
	5456310240 [label=MmBackward0]
	5456310432 -> 5456310240
	5456310432 [label=ReluBackward0]
	5456310576 -> 5456310432
	5456310576 [label=NativeBatchNormBackward0]
	5456310672 -> 5456310576
	5456310672 [label=MmBackward0]
	5456309904 -> 5456310672
	5456309904 [label=ReluBackward0]
	5456310960 -> 5456309904
	5456310960 [label=AddBackward0]
	5456311056 -> 5456310960
	5456311056 [label=NativeBatchNormBackward0]
	5456311200 -> 5456311056
	5456311200 [label=MmBackward0]
	5456311392 -> 5456311200
	5456311392 [label=ReluBackward0]
	5456311536 -> 5456311392
	5456311536 [label=NativeBatchNormBackward0]
	5456311632 -> 5456311536
	5456311632 [label=MmBackward0]
	5456311008 -> 5456311632
	5456311008 [label=AddmmBackward0]
	5456311920 -> 5456311008
	4385765360 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	4385765360 -> 5456311920
	5456311920 [label=AccumulateGrad]
	5456311872 -> 5456311008
	5456311872 [label=TBackward0]
	5190176432 -> 5456311872
	4385767360 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	4385767360 -> 5190176432
	5190176432 [label=AccumulateGrad]
	5456311824 -> 5456311632
	5456311824 [label=TBackward0]
	5456312112 -> 5456311824
	5204823008 [label="blocks.0.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5204823008 -> 5456312112
	5456312112 [label=AccumulateGrad]
	5456311584 -> 5456311536
	4845762544 [label="blocks.0.net.1.weight
 (64)" fillcolor=lightblue]
	4845762544 -> 5456311584
	5456311584 [label=AccumulateGrad]
	5456311440 -> 5456311536
	4385761680 [label="blocks.0.net.1.bias
 (64)" fillcolor=lightblue]
	4385761680 -> 5456311440
	5456311440 [label=AccumulateGrad]
	5456311344 -> 5456311200
	5456311344 [label=TBackward0]
	5456173888 -> 5456311344
	5209571648 [label="blocks.0.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5209571648 -> 5456173888
	5456173888 [label=AccumulateGrad]
	5456311152 -> 5456311056
	5209574128 [label="blocks.0.net.4.weight
 (64)" fillcolor=lightblue]
	5209574128 -> 5456311152
	5456311152 [label=AccumulateGrad]
	5456311104 -> 5456311056
	5209574048 [label="blocks.0.net.4.bias
 (64)" fillcolor=lightblue]
	5209574048 -> 5456311104
	5456311104 [label=AccumulateGrad]
	5456311008 -> 5456310960
	5456310864 -> 5456310672
	5456310864 [label=TBackward0]
	5456311248 -> 5456310864
	5209574528 [label="blocks.1.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5209574528 -> 5456311248
	5456311248 [label=AccumulateGrad]
	5456310624 -> 5456310576
	5209574608 [label="blocks.1.net.1.weight
 (64)" fillcolor=lightblue]
	5209574608 -> 5456310624
	5456310624 [label=AccumulateGrad]
	5456310480 -> 5456310576
	5209574688 [label="blocks.1.net.1.bias
 (64)" fillcolor=lightblue]
	5209574688 -> 5456310480
	5456310480 [label=AccumulateGrad]
	5456310384 -> 5456310240
	5456310384 [label=TBackward0]
	5456310768 -> 5456310384
	5209575008 [label="blocks.1.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5209575008 -> 5456310768
	5456310768 [label=AccumulateGrad]
	5456310192 -> 5456310048
	5209575088 [label="blocks.1.net.4.weight
 (64)" fillcolor=lightblue]
	5209575088 -> 5456310192
	5456310192 [label=AccumulateGrad]
	5456310144 -> 5456310048
	5209575168 [label="blocks.1.net.4.bias
 (64)" fillcolor=lightblue]
	5209575168 -> 5456310144
	5456310144 [label=AccumulateGrad]
	5456309904 -> 5456298528
	5456309856 -> 5210341776
	5456309856 [label=TBackward0]
	5456310288 -> 5456309856
	5209575568 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5209575568 -> 5456310288
	5456310288 [label=AccumulateGrad]
	5210341776 -> 5209576048
}
