digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	4386877472 [label="
 (2, 1)" fillcolor=darkolivegreen1]
	5208791248 [label=AddmmBackward0]
	5206598864 -> 5208791248
	5206570752 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5206570752 -> 5206598864
	5206598864 [label=AccumulateGrad]
	5187216544 -> 5208791248
	5187216544 [label=ReluBackward0]
	5218616224 -> 5187216544
	5218616224 [label=AddBackward0]
	5218616176 -> 5218616224
	5218616176 [label=NativeBatchNormBackward0]
	5206470960 -> 5218616176
	5206470960 [label=MmBackward0]
	5218616608 -> 5206470960
	5218616608 [label=ReluBackward0]
	5218616752 -> 5218616608
	5218616752 [label=NativeBatchNormBackward0]
	5218616848 -> 5218616752
	5218616848 [label=MmBackward0]
	5218616272 -> 5218616848
	5218616272 [label=ReluBackward0]
	5218617136 -> 5218616272
	5218617136 [label=AddBackward0]
	5218617232 -> 5218617136
	5218617232 [label=NativeBatchNormBackward0]
	5218617376 -> 5218617232
	5218617376 [label=MmBackward0]
	5218617568 -> 5218617376
	5218617568 [label=ReluBackward0]
	5218617712 -> 5218617568
	5218617712 [label=NativeBatchNormBackward0]
	5218617808 -> 5218617712
	5218617808 [label=MmBackward0]
	5218617184 -> 5218617808
	5218617184 [label=AddmmBackward0]
	5218618096 -> 5218617184
	5206568992 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5206568992 -> 5218618096
	5218618096 [label=AccumulateGrad]
	5218618048 -> 5218617184
	5218618048 [label=TBackward0]
	5208568160 -> 5218618048
	5206566832 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5206566832 -> 5208568160
	5208568160 [label=AccumulateGrad]
	5218618000 -> 5218617808
	5218618000 [label=TBackward0]
	5218618288 -> 5218618000
	4392745504 [label="blocks.0.net.0.weight
 (64, 64)" fillcolor=lightblue]
	4392745504 -> 5218618288
	5218618288 [label=AccumulateGrad]
	5218617760 -> 5218617712
	4526267264 [label="blocks.0.net.1.weight
 (64)" fillcolor=lightblue]
	4526267264 -> 5218617760
	5218617760 [label=AccumulateGrad]
	5218617616 -> 5218617712
	5187297248 [label="blocks.0.net.1.bias
 (64)" fillcolor=lightblue]
	5187297248 -> 5218617616
	5218617616 [label=AccumulateGrad]
	5218617520 -> 5218617376
	5218617520 [label=TBackward0]
	5218618192 -> 5218617520
	5206569152 [label="blocks.0.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5206569152 -> 5218618192
	5218618192 [label=AccumulateGrad]
	5218617328 -> 5218617232
	5206567792 [label="blocks.0.net.4.weight
 (64)" fillcolor=lightblue]
	5206567792 -> 5218617328
	5218617328 [label=AccumulateGrad]
	5218617280 -> 5218617232
	5206567872 [label="blocks.0.net.4.bias
 (64)" fillcolor=lightblue]
	5206567872 -> 5218617280
	5218617280 [label=AccumulateGrad]
	5218617184 -> 5218617136
	5218617040 -> 5218616848
	5218617040 [label=TBackward0]
	5218617424 -> 5218617040
	5206569552 [label="blocks.1.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5206569552 -> 5218617424
	5218617424 [label=AccumulateGrad]
	5218616800 -> 5218616752
	5206569632 [label="blocks.1.net.1.weight
 (64)" fillcolor=lightblue]
	5206569632 -> 5218616800
	5218616800 [label=AccumulateGrad]
	5218616656 -> 5218616752
	5206569712 [label="blocks.1.net.1.bias
 (64)" fillcolor=lightblue]
	5206569712 -> 5218616656
	5218616656 [label=AccumulateGrad]
	5218616560 -> 5206470960
	5218616560 [label=TBackward0]
	5218616944 -> 5218616560
	5206570112 [label="blocks.1.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5206570112 -> 5218616944
	5218616944 [label=AccumulateGrad]
	5218616512 -> 5218616176
	5206570192 [label="blocks.1.net.4.weight
 (64)" fillcolor=lightblue]
	5206570192 -> 5218616512
	5218616512 [label=AccumulateGrad]
	5218616320 -> 5218616176
	5206570272 [label="blocks.1.net.4.bias
 (64)" fillcolor=lightblue]
	5206570272 -> 5218616320
	5218616320 [label=AccumulateGrad]
	5218616272 -> 5218616224
	5208565712 -> 5208791248
	5208565712 [label=TBackward0]
	5218616464 -> 5208565712
	5206570672 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5206570672 -> 5218616464
	5218616464 [label=AccumulateGrad]
	5208791248 -> 4386877472
}
