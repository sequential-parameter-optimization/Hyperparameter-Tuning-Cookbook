digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5825412416 [label="
 (2, 1)" fillcolor=darkolivegreen1]
	4555874096 [label=AddmmBackward0]
	4554060368 -> 4555874096
	5810697424 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	5810697424 -> 4554060368
	4554060368 [label=AccumulateGrad]
	5825461504 -> 4555874096
	5825461504 [label=ReluBackward0]
	5825461600 -> 5825461504
	5825461600 [label=AddBackward0]
	5825461552 -> 5825461600
	5825461552 [label=NativeBatchNormBackward0]
	5825461888 -> 5825461552
	5825461888 [label=MmBackward0]
	5825462080 -> 5825461888
	5825462080 [label=ReluBackward0]
	5825462224 -> 5825462080
	5825462224 [label=NativeBatchNormBackward0]
	5825462128 -> 5825462224
	5825462128 [label=MmBackward0]
	5825461696 -> 5825462128
	5825461696 [label=ReluBackward0]
	5825610128 -> 5825461696
	5825610128 [label=AddBackward0]
	5825610224 -> 5825610128
	5825610224 [label=NativeBatchNormBackward0]
	5825610368 -> 5825610224
	5825610368 [label=MmBackward0]
	5825610560 -> 5825610368
	5825610560 [label=ReluBackward0]
	5825610704 -> 5825610560
	5825610704 [label=NativeBatchNormBackward0]
	5825610800 -> 5825610704
	5825610800 [label=MmBackward0]
	5825610176 -> 5825610800
	5825610176 [label=AddmmBackward0]
	5825611088 -> 5825610176
	5810693824 [label="input_layer.bias
 (64)" fillcolor=lightblue]
	5810693824 -> 5825611088
	5825611088 [label=AccumulateGrad]
	5825611040 -> 5825610176
	5825611040 [label=TBackward0]
	5825611184 -> 5825611040
	5810689344 [label="input_layer.weight
 (64, 10)" fillcolor=lightblue]
	5810689344 -> 5825611184
	5825611184 [label=AccumulateGrad]
	5825610992 -> 5825610800
	5825610992 [label=TBackward0]
	5825611328 -> 5825610992
	4393500608 [label="blocks.0.net.0.weight
 (64, 64)" fillcolor=lightblue]
	4393500608 -> 5825611328
	5825611328 [label=AccumulateGrad]
	5825610752 -> 5825610704
	5810694384 [label="blocks.0.net.1.weight
 (64)" fillcolor=lightblue]
	5810694384 -> 5825610752
	5825610752 [label=AccumulateGrad]
	5825610608 -> 5825610704
	5810694224 [label="blocks.0.net.1.bias
 (64)" fillcolor=lightblue]
	5810694224 -> 5825610608
	5825610608 [label=AccumulateGrad]
	5825610512 -> 5825610368
	5825610512 [label=TBackward0]
	5825611136 -> 5825610512
	5810695264 [label="blocks.0.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5810695264 -> 5825611136
	5825611136 [label=AccumulateGrad]
	5825610320 -> 5825610224
	5810695344 [label="blocks.0.net.4.weight
 (64)" fillcolor=lightblue]
	5810695344 -> 5825610320
	5825610320 [label=AccumulateGrad]
	5825610272 -> 5825610224
	5810695424 [label="blocks.0.net.4.bias
 (64)" fillcolor=lightblue]
	5810695424 -> 5825610272
	5825610272 [label=AccumulateGrad]
	5825610176 -> 5825610128
	5825610032 -> 5825462128
	5825610032 [label=TBackward0]
	5825610416 -> 5825610032
	5810695984 [label="blocks.1.net.0.weight
 (64, 64)" fillcolor=lightblue]
	5810695984 -> 5825610416
	5825610416 [label=AccumulateGrad]
	5825609840 -> 5825462224
	5810696064 [label="blocks.1.net.1.weight
 (64)" fillcolor=lightblue]
	5810696064 -> 5825609840
	5825609840 [label=AccumulateGrad]
	5825609792 -> 5825462224
	5810696144 [label="blocks.1.net.1.bias
 (64)" fillcolor=lightblue]
	5810696144 -> 5825609792
	5825609792 [label=AccumulateGrad]
	5825462032 -> 5825461888
	5825462032 [label=TBackward0]
	5825462176 -> 5825462032
	5810696544 [label="blocks.1.net.3.weight
 (64, 64)" fillcolor=lightblue]
	5810696544 -> 5825462176
	5825462176 [label=AccumulateGrad]
	5825461840 -> 5825461552
	5810696624 [label="blocks.1.net.4.weight
 (64)" fillcolor=lightblue]
	5810696624 -> 5825461840
	5825461840 [label=AccumulateGrad]
	5825461648 -> 5825461552
	5810696704 [label="blocks.1.net.4.bias
 (64)" fillcolor=lightblue]
	5810696704 -> 5825461648
	5825461648 [label=AccumulateGrad]
	5825461696 -> 5825461600
	5825461456 -> 4555874096
	5825461456 [label=TBackward0]
	5825461936 -> 5825461456
	5810697344 [label="output_layer.weight
 (1, 64)" fillcolor=lightblue]
	5810697344 -> 5825461936
	5825461936 [label=AccumulateGrad]
	4555874096 -> 5825412416
}
