{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---"
      ],
      "id": "d5f7e15c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to `scipy.optimize`\n",
        "\n",
        "[SciPy](https://scipy.org) provides algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, differential equations, statistics and many other classes of problems.\n",
        "SciPy is a collection of mathematical algorithms and convenience functions built on NumPy. It adds significant power to Python by providing the user with high-level commands and classes for manipulating and visualizing data.\n",
        "\n",
        "[SciPy optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize) provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programing, constrained and nonlinear least-squares, root finding, and curve fitting.\n",
        "\n",
        "\n",
        "In this notebook, we will learn how to use the `scipy.optimize` module to solve optimization problems.\n",
        "See: [https://docs.scipy.org/doc/scipy/tutorial/optimize.html](https://docs.scipy.org/doc/scipy/tutorial/optimize.html)\n",
        "\n",
        "::: {.callout-note}\n",
        "* This content is based on information from the scipy.optimize package.\n",
        "* The `scipy.optimize` package provides several commonly used optimization algorithms. A detailed listing is available in `scipy.optimize` (can also be found by `help(scipy.optimize)`).\n",
        ":::\n",
        "\n",
        "Common functions and objects, shared across different SciPy optimize solvers, are shown in @tbl-shared-functions.\n",
        "\n",
        "| Function or Object | Description |\n",
        "| --- | --- |\n",
        "| [show_options([solver, method, disp])](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html#scipy.optimize.show_options) | Show documentation for additional options of optimization solvers. |\n",
        "| [OptimizeResult](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult) | Represents the optimization result. |\n",
        "| [OptimizeWarning](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeWarning.html#scipy.optimize.OptimizeWarning) | Warning issued by solvers. |\n",
        ": Common functions and objects, shared across different SciPy optimize solvers {#tbl-shared-functions}\n",
        "\n",
        "We will introduce unconstrained minimization of multivariate scalar functions in this chapter.\n",
        "The [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in `scipy.optimize`. To demonstrate the minimization function, consider the problem of minimizing the Rosenbrock function of *N* variables:\n",
        "\n",
        "$$\n",
        "f(J) = \\sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2)^2 + (1 - x_i)^2\n",
        "$$\n",
        "\n",
        "The minimum value of this function is 0, which is achieved when \\(x_i = 1\\).\n",
        "\n",
        "Note that the Rosenbrock function and its derivatives are included in `scipy.optimize`. The implementations shown in the following sections provide examples of how to define an objective function as well as its Jacobian and Hessian functions. Objective functions in `scipy.optimize` expect a numpy array as their first parameter, which is to be optimized and must return a float value. The exact calling signature must be `f(x, *args)`, where `x` represents a numpy array, and `args` is a tuple of additional arguments supplied to the objective function.\n",
        "\n",
        "##  Derivative-free Optimization Algorithms\n",
        "\n",
        "@sec-nelder-mead-simplex-algorithm and @sec-powells-method present two approaches that do not need gradient information to find the minimum. They use function evaluations to find the minimum.\n",
        "\n",
        "### Nelder-Mead Simplex Algorithm {#sec-nelder-mead-simplex-algorithm}\n",
        "\n",
        "`method='Nelder-Mead'`: In the example below, the `minimize` routine is used with the *Nelder-Mead* simplex algorithm (selected through the `method` parameter):\n"
      ],
      "id": "4ecf412e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def rosen(x):\n",
        "    \"\"\"The Rosenbrock function\"\"\"\n",
        "    return sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
        "\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "res = minimize(rosen, x0, method='nelder-mead',\n",
        "               options={'xatol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "id": "adb1b7c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The simplex algorithm is probably the simplest way to minimize a well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum.\n",
        "\n",
        "### Powell's Method {#sec-powells-method}\n",
        "\n",
        "Another optimization algorithm that needs only function calls to find the minimum is *Powell*'s method, which can be selected by setting the `method` parameter to `'powell'` in the `minimize` function.\n",
        "\n",
        "To demonstrate how to supply additional arguments to an objective function, let's consider minimizing the Rosenbrock function with an additional scaling factor $a$ and an offset $b$:\n",
        "\n",
        "$$\n",
        "f(J, a, b) = \\sum_{i=1}^{N-1} a (x_{i+1} - x_i^2)^2 + (1 - x_i)^2 + b\n",
        "$$\n",
        "\n",
        "You can achieve this using the `minimize` routine with the example parameters $a=0.5$ and $b=1$:\n"
      ],
      "id": "96942be3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rosen_with_args(x, a, b):\n",
        "    \"\"\"The Rosenbrock function with additional arguments\"\"\"\n",
        "    return sum(a * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0) + b\n",
        "\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "res = minimize(rosen_with_args, x0, method='nelder-mead',\n",
        "               args=(0.5, 1.), options={'xatol': 1e-8, 'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "id": "239c7eda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an alternative to using the `args` parameter of `minimize`, you can wrap the objective function in a new function that accepts only `x`. This approach is also useful when it is necessary to pass additional parameters to the objective function as keyword arguments.\n"
      ],
      "id": "58813a58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rosen_with_args(x, a, *, b):  # b is a keyword-only argument\n",
        "    return sum(a * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0) + b\n",
        "\n",
        "def wrapped_rosen_without_args(x):\n",
        "    return rosen_with_args(x, 0.5, b=1.)  # pass in `a` and `b`\n",
        "\n",
        "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
        "res = minimize(wrapped_rosen_without_args, x0, method='nelder-mead',\n",
        "               options={'xatol': 1e-8,})\n",
        "\n",
        "print(res.x)"
      ],
      "id": "9cab2a75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another alternative is to use `functools.partial`.\n"
      ],
      "id": "1382ba14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functools import partial\n",
        "\n",
        "partial_rosen = partial(rosen_with_args, a=0.5, b=1.)\n",
        "res = minimize(partial_rosen, x0, method='nelder-mead',\n",
        "               options={'xatol': 1e-8,})\n",
        "\n",
        "print(res.x)"
      ],
      "id": "6785e670",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient-based optimization algorithms\n",
        "\n",
        "### An Introductory Example: Broyden-Fletcher-Goldfarb-Shanno Algorithm (BFGS) {#sec-bfgs-intro}\n",
        "\n",
        "This section introduces an optimization algorithm that uses gradient information to find the minimum.\n",
        "The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (selected by setting `method='BFGS'`) is an optimization algorithm that aims to converge quickly to the solution.\n",
        "This algorithm uses the gradient of the objective function.\n",
        "If the gradient is not provided by the user, it is estimated using first-differences. The BFGS method typically requires fewer function calls compared to the simplex algorithm, even when the gradient needs to be estimated.\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Example: BFGS\n",
        "To demonstrate the BFGS algorithm, let's use the Rosenbrock function again.\n",
        "The gradient of the Rosenbrock function is a vector described by the following mathematical expression:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial f}{\\partial x_j} = \\sum_{i=1}^{N} 200(x_i - x_{i-1}^2)(\\delta_{i,j} - 2x_{i-1}\\delta_{i-1,j}) - 2(1 - x_{i-1})\\delta_{i-1,j} \\\\\n",
        "= 200(x_j - x_{j-1}^2) - 400x_j(x_{j+1} - x_j^2) - 2(1 - x_j)\n",
        "\\end{align}\n",
        "\n",
        "This expression is valid for interior derivatives, but special cases are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x_0} = -400x_0(x_1 - x_0^2) - 2(1 - x_0)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x_{N-1}} = 200(x_{N-1} - x_{N-2}^2)\n",
        "$$\n",
        "\n",
        "Here's a Python function that computes this gradient:\n"
      ],
      "id": "c8b4f78c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rosen_der(x):\n",
        "    xm = x[1:-1]\n",
        "    xm_m1 = x[:-2]\n",
        "    xm_p1 = x[2:]\n",
        "    der = np.zeros_like(x)\n",
        "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
        "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
        "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
        "    return der"
      ],
      "id": "e54fcaf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can specify this gradient information in the minimize function using the jac parameter as illustrated below:\n"
      ],
      "id": "61e4ff62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
        "               options={'disp': True})\n",
        "\n",
        "print(res.x)"
      ],
      "id": "74184ebe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### Background and Basics for Gradient-based Optimization\n",
        "\n",
        "### Gradient\n",
        "\n",
        "The gradient $\\nabla f(J)$ for a scalar function $f(J)$ with $n$ different variables is defined by its partial derivatives:\n",
        "\n",
        "$$\\nabla f(J) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right]$$\n",
        "\n",
        "### Jacobian Matrix\n",
        "\n",
        "The Jacobian matrix $J(J)$ for a vector-valued function $F(J) = [f_1(J), f_2(J), \\ldots, f_m(J)]$ is defined as:\n",
        "\n",
        "$J(J) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\ldots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\ldots & \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\ldots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$\n",
        "\n",
        "It consists of the first order partial derivatives and gives therefore an overview about the gradients of a vector valued function.\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Example: Jacobian matrix\n",
        "\n",
        "Consider a vector-valued function $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ defined as follows:\n",
        "$$f(J) = \\begin{bmatrix} x_1^2 + 2x_2 \\\\ 3x_1 - \\sin(x_2) \\\\ e^{x_1 + x_2} \\end{bmatrix}$$\n",
        "\n",
        "Let's compute the partial derivatives and construct the Jacobian matrix:\n",
        "\n",
        "$\\frac{\\partial f_1}{\\partial x_1} = 2x_1, \\quad \\frac{\\partial f_1}{\\partial x_2} = 2$\n",
        "\n",
        "$\\frac{\\partial f_2}{\\partial x_1} = 3, \\quad \\frac{\\partial f_2}{\\partial x_2} = -\\cos(x_2)$\n",
        "\n",
        "$\\frac{\\partial f_3}{\\partial x_1} = e^{x_1 + x_2}, \\quad \\frac{\\partial f_3}{\\partial x_2} = e^{x_1 + x_2}$\n",
        "\n",
        "So, the Jacobian matrix is:\n",
        "\n",
        "$$J(J) = \\begin{bmatrix} 2x_1 & 2 \\\\ 3 & -\\cos(x_2) \\\\ e^{x_1 + x_2} & e^{x_1 + x_2} \\end{bmatrix}$$\n",
        "\n",
        "This Jacobian matrix provides information about how small changes in the input variables $x_1$ and $x_2$ affect the corresponding changes in each component of the output vector.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "### Hessian Matrix\n",
        "\n",
        "The Hessian matrix $H(J)$ for a scalar function $f(J)$ is defined as:\n",
        "\n",
        "$H(J) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}$\n",
        "\n",
        "So, the Hessian matrix consists of the second order dervatives of the function. It provides information about the local curvature of the function with respect to changes in the input variables.\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Example: Hessian matrix\n",
        "\n",
        "Consider a scalar-valued function:\n",
        "$$f(J) = x_1^2 + 2x_2^2 + \\sin(x_1   x_2)$$\n",
        "\n",
        "The Hessian matrix of this scalar-valued function is the matrix of its second-order partial derivatives with respect to the input variables:\n",
        "$$H(J) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix}$$\n",
        "\n",
        "Let's compute the second-order partial derivatives and construct the Hessian matrix:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial^2 f}{\\partial x_1^2} &= 2 + \\cos(x_1 x_2) x_2^2\\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &= 2x_1  x_2 \\cos(x_1 x_2) - \\sin(x_1  x_2)\\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &= 2x_1  x_2  \\cos(x_1  x_2) - \\sin(x_1  x_2)\\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_2^2} &= 4x_2^2 + \\cos(x_1  x_2) x_1^2\n",
        "\\end{align}\n",
        "\n",
        "So, the Hessian matrix is:\n",
        "\n",
        "$$H(J) = \\begin{bmatrix} 2 + \\cos(x_1   x_2)   x_2^2 & 2x_1   x_2   \\cos(x_1   x_2) - \\sin(x_1   x_2) \\\\ 2x_1   x_2   \\cos(x_1   x_2) - \\sin(x_1   x_2) & 4x_2^2 + \\cos(x_1   x_2)   x_1^2 \\end{bmatrix}$$\n",
        "\n",
        ":::\n",
        "\n",
        "### Gradient for Optimization\n",
        "\n",
        "In optimization, the goal is to find the minimum or maximum of a function. Gradient-based optimization methods utilize information about the gradient (or derivative) of the function to guide the search for the optimal solution. This is particularly useful when dealing with complex, high-dimensional functions where an exhaustive search is impractical.\n",
        "\n",
        "The gradient descent method can be divided in the following steps:\n",
        "\n",
        "+ **Initialize:** start with an initial guess for the parameters of the function to be optimized.\n",
        "+ **Compute Gradient:** Calculate the gradient (partial derivatives) of the function with respect to each parameter at the current point. The gradient indicates the direction of the steepest increase in the function.\n",
        "+ **Update Parameters:** Adjust the parameters in the opposite direction of the gradient, scaled by a learning rate. This step aims to move towards the minimum of the function: \n",
        "    * $x_{k+1} = x_k - \\alpha \\times \\nabla f(x_{k})$\n",
        "    * $x_{x}$ is current parameter vector or point in the parameter space. \n",
        "    * $\\alpha$ is the learning rate, a positive scalar that determines the step size in each iteration.\n",
        "    * $\\nabla f(x)$ is the gradient of the objective function.\n",
        "+ **Iterate:** Repeat the above steps until convergence or a predefined number of iterations. Convergence is typically determined when the change in the function value or parameters becomes negligible.\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Example: Gradient Descent\n",
        "\n",
        "Let's consider a simple quadratic function as an example:\n",
        "$$f(x) = x^2 + 4x + y^2 + 2y + 4.$$\n",
        "\n",
        "We'll use gradient descent to find the minimum of this function.\n"
      ],
      "id": "b9b3fe61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Define the quadratic function\n",
        "def quadratic_function(x, y):\n",
        "    return x**2 + 4*x + y**2 + 2*y + 4\n",
        "\n",
        "# Define the gradient of the quadratic function\n",
        "def gradient_quadratic_function(x, y):\n",
        "    grad_x = 2*x + 4\n",
        "    grad_y = 2*y + 2\n",
        "    return np.array([grad_x, grad_y])\n",
        "\n",
        "# Gradient Descent for optimization in 2D\n",
        "def gradient_descent(initial_point, learning_rate, num_iterations):\n",
        "    points = [np.array(initial_point)]\n",
        "    \n",
        "    for _ in range(num_iterations):\n",
        "        current_point = points[-1]\n",
        "        gradient = gradient_quadratic_function(*current_point)\n",
        "        new_point = current_point - learning_rate * gradient\n",
        "        \n",
        "        points.append(new_point)\n",
        "        \n",
        "    return points\n",
        "\n",
        "# Visualization of optimization process with 3D surface and consistent arrow sizes\n",
        "def plot_optimization_process_3d_consistent_arrows(points):\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    x_vals = np.linspace(-10, 2, 100)\n",
        "    y_vals = np.linspace(-10, 2, 100)\n",
        "    X, Y = np.meshgrid(x_vals, y_vals)\n",
        "    Z = quadratic_function(X, Y)\n",
        "\n",
        "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n",
        "    ax.scatter(*zip(*points), [quadratic_function(*p) for p in points], c='red', label='Optimization Trajectory')\n",
        "\n",
        "    for i in range(len(points) - 1):  \n",
        "        x, y = points[i]\n",
        "        dx, dy = points[i + 1] - points[i]\n",
        "        dz = quadratic_function(*(points[i + 1])) - quadratic_function(*points[i])\n",
        "        gradient_length = 0.5\n",
        "\n",
        "        ax.quiver(x, y, quadratic_function(*points[i]), dx, dy, dz, color='blue', length=gradient_length, normalize=False, arrow_length_ratio=0.1)\n",
        "\n",
        "    ax.set_title('Gradient-Based Optimization with 2D Quadratic Function')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('f(x, y)')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Initial guess and parameters\n",
        "initial_guess = [-9.0, -9.0]\n",
        "learning_rate = 0.2\n",
        "num_iterations = 10\n",
        "\n",
        "# Run gradient descent in 2D and visualize the optimization process with 3D surface and consistent arrow sizes\n",
        "trajectory = gradient_descent(initial_guess, learning_rate, num_iterations)\n",
        "plot_optimization_process_3d_consistent_arrows(trajectory)"
      ],
      "id": "543f5302",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Newton Method\n",
        "\n",
        "**Initialization:** Start with an initial guess for the optimal solution: $x_0$.\n",
        "\n",
        "**Iteration:** Repeat the following three steps until convergence or a predefined stopping criterion is met:\n",
        "\n",
        "1) Calculate the gradient ($\\nabla$) and the Hessian matrix ($\\nabla^2$) of the objective function at the current point:\n",
        "    $$\\nabla f(x_k) \\quad \\text{and} \\quad \\nabla^2 f(x_k)$$\n",
        "\n",
        "2) Update the current solution using the Newton-Raphson update formula\n",
        "    $$x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k),$$ where\n",
        "\n",
        "* $\\nabla f(x_k)$ is the gradient (first derivative) of the objective function with respect to the variable $x$, evaluated at the current solution $x_k$.\n",
        "* $\\nabla^2 f(x_k)$: The Hessian matrix (second derivative) of the objective function with respect to $x$, evaluated at the current solution $x_k$.\n",
        "* $x_k$: The current solution or point in the optimization process.\n",
        "* $\\nabla^2 f(x_k)]^{-1}$: The inverse of the Hessian matrix at the current point, representing the approximation of the curvature of the objective function.\n",
        "* $x_{k+1}$: The updated solution or point after applying the Newton-Raphson update.\n",
        "\n",
        "3) Check for convergence.\n",
        "  \n",
        "::: {.callout-note}\n",
        "#### Example: Newton Method\n",
        "\n",
        "We want to optimize the Rosenbrock function and use the Hessian and the Jacobian (which is equal to the gradient vector for scalar objective function) to the `minimize` function. \n"
      ],
      "id": "6ef787ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rosenbrock(x):\n",
        "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "def rosenbrock_gradient(x):\n",
        "    dfdx0 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "    dfdx1 = 200 * (x[1] - x[0]**2)\n",
        "    return np.array([dfdx0, dfdx1])\n",
        "\n",
        "def rosenbrock_hessian(x):\n",
        "    d2fdx0 = 1200 * x[0]**2 - 400 * x[1] + 2\n",
        "    d2fdx1 = -400 * x[0]\n",
        "    return np.array([[d2fdx0, d2fdx1], [d2fdx1, 200]])\n",
        "\n",
        "def classical_newton_optimization_2d(initial_guess, tol=1e-6, max_iter=100):\n",
        "    x = initial_guess.copy()\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        gradient = rosenbrock_gradient(x)\n",
        "        hessian = rosenbrock_hessian(x)\n",
        "\n",
        "        # Solve the linear system H * d = -g for d\n",
        "        d = np.linalg.solve(hessian, -gradient)\n",
        "\n",
        "        # Update x\n",
        "        x += d\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(gradient, ord=np.inf) < tol:\n",
        "            break\n",
        "\n",
        "    return x\n",
        "\n",
        "# Initial guess\n",
        "initial_guess_2d = np.array([0.0, 0.0])\n",
        "\n",
        "# Run classical Newton optimization for the 2D Rosenbrock function\n",
        "result_2d = classical_newton_optimization_2d(initial_guess_2d)\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal solution:\", result_2d)\n",
        "print(\"Objective value:\", rosenbrock(result_2d))"
      ],
      "id": "f49d0178",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### BFGS-Algorithm\n",
        "\n",
        "BFGS is an optimization algorithm designed for unconstrained optimization problems. It belongs to the class of quasi-Newton methods and is known for its efficiency in finding the minimum of a smooth, unconstrained objective function.\n",
        "\n",
        "### Procedure:\n",
        "\n",
        "1. **Initialization:**\n",
        "    * Start with an initial guess for the parameters of the objective function.\n",
        "    * Initialize an approximation of the Hessian matrix (inverse) denoted by $H$.  \n",
        "2. **Iterative Update:**\n",
        "    * At each iteration, compute the gradient vector at the current point.\n",
        "    * Update the parameters using the BFGS update formula, which involves the inverse Hessian matrix approximation, the gradient, and the difference in parameter vectors between successive iterations: $$x_{k+1} = x_k - H_k^{-1} \\nabla f(x_k).$$\n",
        "    * Update the inverse Hessian approximation using the BFGS update formula for the inverse Hessian.\n",
        "    $$H_{k+1} = H_k + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta x_k^T \\Delta g_k} - \\frac{H_k g_k g_k^T H_k}{g_k^T H_k g_k},$$ where:\n",
        "    * $x_k$ and $x_{k+1}$ are the parameter vectors at the current and updated iterations, respectively.\n",
        "    * $\\nabla f(x_k)$ is the gradient vector at the current iteration.\n",
        "    * $\\Delta x_k = x_{k+1} - x_k$ is the change in parameter vectors.\n",
        "    * $\\Delta g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in gradient vectors.\n",
        "3. **Convergence:**\n",
        "    * Repeat the iterative update until the optimization converges. Convergence is typically determined by reaching a sufficiently low gradient or parameter change.\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Example: BFGS for Rosenbrock\n"
      ],
      "id": "c91f7026"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the 2D Rosenbrock function\n",
        "def rosenbrock(x):\n",
        "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
        "\n",
        "# Initial guess\n",
        "initial_guess = np.array([0.0, 0.0])\n",
        "\n",
        "# Minimize the Rosenbrock function using BFGS\n",
        "minimize(rosenbrock, initial_guess, method='BFGS')"
      ],
      "id": "fcbddab0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### Visualization BFGS for Rosenbrock\n",
        "\n",
        "A visualization of the BFGS search process on Rosenbrock's function can be found here: [https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif](https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif)\n",
        "\n",
        "<!-- \n",
        "![BFGS search process on Rosenbrock's function. Figure taken from: [https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif](https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif)](./figures_static/Rosenbrock-bfgs-animation.gif)\n",
        " -->\n",
        "\n",
        "::: {.callout-note}\n",
        "### Tasks\n",
        "\n",
        "* In which situations is it possible to use algorithms like BFGS, but not the classical Newton method?\n",
        "* Investigate the Newton-CG method\n",
        "* Use an objective function of your choice and apply Newton-CG\n",
        "* Compare the Newton-CG method with the BFGS. What are the similarities and differences between the two algorithms?\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Gradient- and Hessian-based Optimization Algorithms\n",
        "\n",
        "@sec-newton-cg presents an optimization algorithm that uses gradient and Hessian information to find the minimum.\n",
        "@sec-trust-region-newton presents an optimization algorithm that uses gradient and Hessian information to find the minimum.\n",
        "@sec-trust-region-truncated presents an optimization algorithm that uses gradient and Hessian information to find the minimum.\n",
        "\n",
        "The methods Newton-CG, trust-ncg and trust-krylov are suitable for dealing with large-scale problems (problems with thousands of variables). That is because the conjugate gradient algorithm approximately solve the trust-region subproblem (or invert the Hessian) by iterations without the explicit Hessian factorization. Since only the product of the Hessian with an arbitrary vector is needed, the algorithm is specially suited for dealing with sparse Hessians, allowing low storage requirements and significant time savings for those sparse problems.\n",
        "\n",
        "\n",
        "### Newton-Conjugate-Gradient Algorithm {#sec-newton-cg}\n",
        "\n",
        "Newton-Conjugate Gradient algorithm is a modified Newton’s method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian.\n",
        "\n",
        "\n",
        "### Trust-Region Newton-Conjugate-Gradient Algorithm {#sec-trust-region-newton}\n",
        "\n",
        "\n",
        "### Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm {#sec-trust-region-truncated}\n",
        "\n",
        "## Global Optimization\n",
        "\n",
        "Global optimization aims to find the global minimum of a function within given bounds, in the presence of potentially many local minima. Typically, global minimizers efficiently search the parameter space, while using a local minimizer (e.g., minimize) under the hood.\n",
        "\n",
        "### Local vs Global Optimization\n",
        "\n",
        "#### Local Optimizater:\n",
        "\n",
        "* Seeks the optimum in a **specific region** of the search space\n",
        "* Tends to **exploit** the local environment, to find solutions in the immediate area\n",
        "* Highly **sensitive to initial conditions**; may converge to different local optima based on the starting point\n",
        "* Often **computationally efficient for low-dimensional problems** but may struggle with high-dimensional or complex search spaces\n",
        "* Commonly used in situations where the objective is to refine and improve existing solutions\n",
        "\n",
        "\n",
        "\n",
        "#### Global Optimizer:\n",
        "\n",
        "* Explores the **entire search space** to find the global optimum\n",
        "* Emphasize **exploration over exploitation**, aiming to search broadly and avoid premature convergence to local optima\n",
        "* Aim to **mitigate the risk of premature convergence** to local optima by employing strategies for global exploration\n",
        "* **Less sensitive to initial conditions**, designed to navigate diverse regions of the search space\n",
        "* Equipped to handle **high-dimensional** and **complex** problems, though computational demands may vary depending on the specific algorithm\n",
        "* Preferred for applications where a comprehensive search of the solution space is crucial, such as in parameter tuning, machine learning, and complex engineering design\n",
        "\n",
        "<img src=\"./figures_static/globalvslocal.png\" alt=\"Local vs Global Optimum\" width=\"700\"/>\n",
        "\n",
        "\n",
        "\n",
        "### Global Optimizers in SciPy\n",
        "\n",
        "SciPy contains a number of good global optimizers. Here, we’ll use those on the same objective function, namely the (aptly named) eggholder function:\n"
      ],
      "id": "71bef172"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def eggholder(x):\n",
        "    return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1]  + 47))))\n",
        "            -x[0] * np.sin(np.sqrt(abs(x[0] - (x[1]  + 47)))))\n",
        "\n",
        "bounds = [(-512, 512), (-512, 512)]"
      ],
      "id": "8bb01cd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = np.arange(-512, 513)\n",
        "y = np.arange(-512, 513)\n",
        "xgrid, ygrid = np.meshgrid(x, y)\n",
        "xy = np.stack([xgrid, ygrid])\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.view_init(45, -45)\n",
        "ax.plot_surface(xgrid, ygrid, eggholder(xy), cmap='terrain')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('eggholder(x, y)')\n",
        "plt.show()"
      ],
      "id": "236a8dcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use the global optimizers to obtain the minimum and the function value at the minimum. We’ll store the results in a dictionary so we can compare different optimization results later.\n"
      ],
      "id": "7442a913"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy import optimize\n",
        "results = dict()\n",
        "results['shgo'] = optimize.shgo(eggholder, bounds)\n",
        "results['shgo']"
      ],
      "id": "030a1230",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results['DA'] = optimize.dual_annealing(eggholder, bounds)\n",
        "results['DA']"
      ],
      "id": "d9b685fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All optimizers return an `OptimizeResult`,\n",
        "which in addition to the solution contains information on the number of function evaluations, whether the optimization was successful, and more. For brevity, we won’t show the full output of the other optimizers:\n"
      ],
      "id": "7c3cf409"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results['DE'] = optimize.differential_evolution(eggholder, bounds)\n",
        "results['DE']"
      ],
      "id": "1665ab0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`shgo` has a second method, which returns all local minima rather than only what it thinks is the global minimum:\n"
      ],
      "id": "d6b4c375"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results['shgo_sobol'] = optimize.shgo(eggholder, bounds, n=200, iters=5,\n",
        "                                      sampling_method='sobol')\n",
        "results['shgo_sobol']"
      ],
      "id": "54f52b4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’ll now plot all found minima on a heatmap of the function:\n"
      ],
      "id": "2eb7310b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "im = ax.imshow(eggholder(xy), interpolation='bilinear', origin='lower',\n",
        "               cmap='gray')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "\n",
        "def plot_point(res, marker='o', color=None):\n",
        "    ax.plot(512+res.x[0], 512+res.x[1], marker=marker, color=color, ms=10)\n",
        "\n",
        "plot_point(results['DE'], color='c')  # differential_evolution - cyan\n",
        "plot_point(results['DA'], color='w')  # dual_annealing.        - white\n",
        "\n",
        "# SHGO produces multiple minima, plot them all (with a smaller marker size)\n",
        "plot_point(results['shgo'], color='r', marker='+')\n",
        "plot_point(results['shgo_sobol'], color='r', marker='x')\n",
        "for i in range(results['shgo_sobol'].xl.shape[0]):\n",
        "    ax.plot(512 + results['shgo_sobol'].xl[i, 0],\n",
        "            512 + results['shgo_sobol'].xl[i, 1],\n",
        "            'ro', ms=2)\n",
        "\n",
        "ax.set_xlim([-4, 514*2])\n",
        "ax.set_ylim([-4, 514*2])\n",
        "plt.show()"
      ],
      "id": "8c3a2f8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dual Annealing Optimization\n",
        "\n",
        "This function implements the Dual-Annealing optimization, which is a variant of the famous simulated annealing optimization.\n",
        "\n",
        "Simulated Annealing is a **probabilistic** optimization algorithm inspired by the annealing process in metallurgy. The algorithm is designed to find a good or optimal **global** solution to a problem by exploring the solution space in a controlled and adaptive manner.\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Annealing in Metallurgy\n",
        "Simulated Annealing draws inspiration from the physical process of annealing in metallurgy.\n",
        "Just as metals are gradually cooled to achieve a more stable state, Simulated Annealing uses a similar approach to explore solution spaces in the digital world.\n",
        ":::\n",
        "\n",
        "**Heating Phase**:\n",
        "In metallurgy, a metal is initially heated to a high temperature. At this elevated temperature, the atoms or molecules in the material become more energetic and chaotic, allowing the material to overcome energy barriers and defects.\n",
        "\n",
        "**Analogy Simulated Annealing (Exploration Phase):** In Simulated Annealing, the algorithm starts with a high \"temperature,\" which encourages exploration of the solution space. At this stage, the algorithm is more likely to accept solutions that are worse than the current one, allowing it to escape local optima and explore a broader region of the solution space.\n",
        "\n",
        "**Cooling Phase:** The material is then gradually cooled at a controlled rate. As the temperature decreases, the atoms or molecules start to settle into more ordered and stable arrangements. The slow cooling rate is crucial to avoid the formation of defects and to ensure the material reaches a well-organized state.  \n",
        "\n",
        "**Analogy Simulated Annealing (Exploitation Phase):** As the algorithm progresses, the temperature is gradually reduced over time according to a cooling schedule. This reduction simulates the cooling process in metallurgy. With lower temperatures, the algorithm becomes more selective and tends to accept only better solutions, focusing on refining and exploiting the promising regions discovered during the exploration phase.\n",
        "\n",
        "#### Key Concepts\n",
        "\n",
        "**Temperature:** The temperature is a parameter that controls the likelihood of accepting worse solutions. We start with a high temperature, allowing the algorithm to explore the solution space braodly. The temperature decreases with the iterations of the algorithm.\n",
        "\n",
        "**Cooling Schedule:** The temperature parameter is reduced according to this schedule. The analogy to the annealing of metals: a slower cooling rate allows the material to reach a more stable state.\n",
        "\n",
        "**Neighborhood Exploration:** At each iteration, the algorithm explores the neighborhood of the current solution. The neighborhood is defined by small perturbations or changes to the current solution.\n",
        "\n",
        "**Acceptance Probability:** The algorithm evaluates the objective function for the new solution in the neighborhood. If the new solution is better, it is accepted. If the new solution is worse, it may still be accepted with a certain probability. This probability is determined by both the difference in objective function values and the current temperature.\n",
        "\n",
        "\n",
        "**For minimization:**  \n",
        "If:\n",
        "\n",
        "$$f(x_{t}) > f(x_{t+1})$$\n",
        "Then:\n",
        "$$P(accept\\_new\\_point) = 1$$\n",
        "\n",
        "If:\n",
        "\n",
        "$$f(x_{t}) < f(x_{t+1})$$\n",
        "Then:\n",
        "$$P(accept\\_new\\_point) = e^{-(\\frac{f(x_{t+1}) - f(x_{t})}{Tt})}$$\n",
        "\n",
        "**Termination Criterion:** The algorithm continues iterations until a termination condition is met. This could be a fixed number of iterations, reaching a specific temperature threshold, or achieving a satisfactory solution.\n",
        "\n",
        "#### Steps\n",
        "\n",
        "**1. Initialization:** Set an initial temperature ($T_{0}$) and an initial solution ($f(x_{0})$). The temperature is typically set high initially to encourage exploration.\n",
        "\n",
        "**2. Generate a Neighbor:** Perturb the current solution to generate a neighboring solution. The perturbation can be random or follow a specific strategy.\n",
        "\n",
        "**3. Evaluate the Neighbor:** Evaluate the objective function for the new solution in the neighborhood.\n",
        "\n",
        "**4. Accept or Reject the Neighbor:** \n",
        "+ If the new solution is better (lower cost for minimization problems or higher for maximization problems), accept it as the new current solution.\n",
        "+ If the new solution is worse, accept it with a probability determined by an acceptance probability function as mentioned above. The probability is influenced by the difference in objective function values and the current temperature.\n",
        "\n",
        "**5. Cooling:** Reduce the temperature according to a cooling schedule. The cooling schedule defines how fast the temperature decreases over time. Common cooling schedules include exponential or linear decay.\n",
        "\n",
        "**6. Termination Criterion:** Repeat the iterations (2-5) until a termination condition is met. This could be a fixed number of iterations, reaching a specific temperature threshold, or achieving a satisfactory solution.\n",
        "\n",
        "#### Scipy Implementation of the Dual Annealing Algorithm\n",
        "\n",
        "In Scipy, we utilize the Dual Annealing optimizer, an extension of the simulated annealing algorithm that is versatile for both discrete and continuous problems.\n"
      ],
      "id": "01835b87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import dual_annealing\n",
        "\n",
        "def rastrigin_function(x):\n",
        "    return 20 + x[0]**2 - 10 * np.cos(2 * np.pi * x[0]) + x[1]**2 - 10 * np.cos(2 * np.pi * x[1])\n",
        "\n",
        "# Define the Rastrigin function for visualization\n",
        "def rastrigin_visualization(x, y):\n",
        "    return 20 + x**2 - 10 * np.cos(2 * np.pi * x) + y**2 - 10 * np.cos(2 * np.pi * y)\n",
        "\n",
        "# Create a meshgrid for visualization\n",
        "x_vals = np.linspace(-10, 10, 100)\n",
        "y_vals = np.linspace(-10, 10, 100)\n",
        "x_mesh, y_mesh = np.meshgrid(x_vals, y_vals)\n",
        "z_mesh = rastrigin_visualization(x_mesh, y_mesh)\n",
        "\n",
        "# Visualize the Rastrigin function\n",
        "plt.figure(figsize=(10, 8))\n",
        "contour = plt.contour(x_mesh, y_mesh, z_mesh, levels=50, cmap='viridis')\n",
        "plt.colorbar(contour, label='Rastrigin Function Value')\n",
        "plt.title('Visualization of the 2D Rastrigin Function')\n",
        "\n",
        "# Optimize the Rastrigin function using dual annealing\n",
        "result = dual_annealing(func = rastrigin_function,\n",
        "                        x0=[5.0,3.0],                       #Initial Guess\n",
        "                        bounds= [(-10, 10), (-10, 10)],\n",
        "                        initial_temp = 5230,                #Intial Value for temperature\n",
        "                        restart_temp_ratio = 2e-05,         #Temperature schedule\n",
        "                        seed=42)\n",
        "\n",
        "# Plot the optimized point\n",
        "optimal_x, optimal_y = result.x\n",
        "plt.plot(optimal_x, optimal_y, 'ro', label='Optimal Point')\n",
        "\n",
        "# Set labels and legend\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Display the optimization result\n",
        "print(\"Optimal parameters:\", result.x)\n",
        "print(\"Minimum value of the Rastrigin function:\", result.fun)"
      ],
      "id": "798da280",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result"
      ],
      "id": "951c858c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example of Comparison Table:\n",
        "\n",
        "| Function | Algorithm | Best Objective Function Value | Number of Iterations | Number of Evaluations | Local/Global Optimizer | Gradient-Based/Free Algorithm | Other Comments|\n",
        "|----------|-----------|---------------|-----------------------|-----------------------|------------------------|--------------------------------|--------------------------------------|\n",
        "| Rastrigin| Algorithm1| 1.23          | 1000                  | 5000                  | Global                 | Free                           |     ?                              |\n",
        "| Rastrigin| Algorithm2| 0.95          | 800                   | 4000                  | Local                  | Gradient-Based                 |      ?                                |                      \n",
        "| Rastrigin| Algorithm3| 1.45          | 1200                  | 6000                  | Global                 | Free                           |         ?                             |\n",
        "| Rosenbrock| Algorithm1| 2.56          | 1500                  | 7500                  | Local                  | Gradient-Based                 |         ?                            |\n",
        "| Rosenbrock| Algorithm2| 2.10          | 1200                  | 6000                  | Global                 | Free                           |         ?                            |\n",
        "| Rosenbrock| Algorithm3| 2.75          | 1800                  | 9000                  | Local                  | Gradient-Based                 |         ?                            |\n",
        "\n",
        "\n",
        "### Differential Evolution\n",
        "\n",
        "Differential Evolution is an algorithm used for finding the global minimum of multivariate functions. It is stochastic in nature (does not use gradient methods), and can search large areas of candidate space, but often requires larger numbers of function evaluations than conventional gradient based techniques.\n",
        "\n",
        "Differential Evolution (DE) is a versatile and global optimization algorithm inspired by natural selection and evolutionary processes. Introduced by Storn and Price in 1997, DE mimics the survival-of-the-fittest principle by evolving a population of candidate solutions through iterative mutation, crossover, and selection operations. This nature-inspired approach enables DE to efficiently explore complex and non-linear solution spaces, making it a widely adopted optimization technique in diverse fields such as engineering, finance, and machine learning.\n",
        "\n",
        "### Procedure\n",
        "\n",
        "The procedure boils down to the following steps:\n",
        "\n",
        "1. **Initialization:**\n",
        "   - Create a population of candidate solutions randomly within the specified search space.\n",
        "\n",
        "2. **Mutation:**\n",
        "   - For each individual in the population, select three distinct individuals (vectors) randomly.\n",
        "   - Generate a mutant vector `V` by combining these three vectors with a scaling factor.\n",
        "\n",
        "3. **Crossover:**\n",
        "   - Perform the crossover operation between the target vector `U` and the mutant vector `V`. Information from both vectors is used to create a trial vector `U´`\n",
        "\n",
        "::: {.callout-note}\n",
        "#### Cross-Over Strategies in DE\n",
        "\n",
        "* There are several crossover strategies in the literature. Two examples are:\n",
        "\n",
        "**Binominal Crossover:**\n",
        "\n",
        "In this strategy, each component of the trial vector is selected from the mutant vector with a probability equal to the crossover rate ($CR$). This means that each element of the trial vector has an independent probability of being replaced by the corresponding element of the mutant vector.\n",
        "\n",
        "$$U'_i =\n",
        "      \\begin{cases}\n",
        "      V_i, & \\text{if a random number} \\ \\sim U(0, 1) \\leq CR \\ \\text{(Crossover Rate)} \\\\\n",
        "      U_i, & \\text{otherwise}\n",
        "      \\end{cases}\n",
        "$$\n",
        "\n",
        "**Exponential Crossover:**\n",
        "\n",
        "In exponential crossover, the trial vector is constructed by selecting a random starting point and copying elements from the mutant vector with a certain probability. The probability decreases exponentially with the distance from the starting point. This strategy introduces a correlation between neighboring elements in the trial vector.\n",
        "\n",
        ":::\n",
        "\n",
        "4. **Selection:**\n",
        "   - Evaluate the fitness of the trial vector obtained from the crossover.\n",
        "   - Replace the target vector with the trial vector if its fitness is better.\n",
        "\n",
        "5. **Termination:**\n",
        "   - Repeat the mutation, crossover, and selection steps for a predefined number of generations or until convergence criteria are met.\n",
        "\n",
        "6. **Result:**\n",
        "   - The algorithm returns the best-found solution after the specified number of iterations.\n",
        "\n",
        "The key parameters in DE include the population size, crossover probability, and the scaling factor. Tweak these parameters based on the characteristics of the optimization problem for optimal performance.\n"
      ],
      "id": "1d37ca95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the Rastrigin function\n",
        "def rastrigin(x):\n",
        "    A = 10\n",
        "    return A * len(x) + sum([(xi**2 - A * np.cos(2 * np.pi * xi)) for xi in x])\n",
        "\n",
        "# Create a grid for visualization\n",
        "x_vals = np.linspace(-5.12, 5.12, 100)\n",
        "y_vals = np.linspace(-5.12, 5.12, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = rastrigin(np.vstack([X.ravel(), Y.ravel()]))\n",
        "\n",
        "# Reshape Z to match the shape of X and Y\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "# Plot the Rastrigin function\n",
        "plt.contour(X, Y, Z, levels=50, cmap='viridis', label='Rastrigin Function')\n",
        "\n",
        "# Initial guess (starting point for the optimization)\n",
        "initial_guess = (4,3,4,2)\n",
        "\n",
        "# Define the bounds for each variable in the Rastrigin function\n",
        "bounds = [(-5.12, 5.12)] * 4  # 4D problem, each variable has bounds (-5.12, 5.12)\n",
        "\n",
        "# Run the minimize function\n",
        "result = minimize(rastrigin, initial_guess, bounds=bounds, method='L-BFGS-B')\n",
        "\n",
        "# Extract the optimal solution\n",
        "optimal_solution = result.x\n",
        "\n",
        "# Plot the optimal solution\n",
        "plt.scatter(optimal_solution[0], optimal_solution[1], color='red', marker='x', label='Optimal Solution')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.title('Optimization of Rastrigin Function with Minimize')\n",
        "plt.xlabel('Variable 1')\n",
        "plt.ylabel('Variable 2')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the optimization result\n",
        "print(\"Optimal Solution:\", optimal_solution)\n",
        "print(\"Optimal Objective Value:\", result.fun)"
      ],
      "id": "12cb9fc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DIRECT\n",
        "\n",
        "DIviding RECTangles (DIRECT) is a deterministic global optimization algorithm capable of minimizing a black box function with its variables subject to lower and upper bound constraints by sampling potential solutions in the search space\n",
        "\n",
        "### SHGO\n",
        "\n",
        "SHGO stands for \"simplicial homology global optimization\".\n",
        "It is considered appropriate for solving general purpose NLP and blackbox optimization problems to global optimality (low-dimensional problems).\n",
        "\n",
        "\n",
        "### Basin-hopping\n",
        "\n",
        "Basin-hopping is a two-phase method that combines a global stepping algorithm with local minimization at each step. Designed to mimic the natural process of energy minimization of clusters of atoms, it works well for similar problems with “funnel-like, but rugged” energy landscapes\n",
        "\n",
        "## Project: One-Mass Oscillator Optimization\n"
      ],
      "id": "d276a7d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize"
      ],
      "id": "e7a53feb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction\n",
        "\n",
        "In this project, you will apply various optimization algorithms to fit a one-mass oscillator model to real-world data. The objective is to minimize the sum of the squared residuals between the model predictions and the observed amplitudes of a one-mass oscillator system across different frequencies.\n",
        "\n",
        "### One-Mass Oscillator Model\n",
        "\n",
        "The one-mass oscillator is characterized by the following equation, representing the amplitudes of the system:\n",
        "\n",
        "$$\n",
        "V(\\omega) = \\frac{F}{\\sqrt{(1 - \\nu^2)^2 + 4D^2\\nu^2}}\n",
        "$$\n",
        "\n",
        "Here,\n",
        "$\\omega$ represents the angular frequency of the system, $\\nu$ is the ratio of the excitation frequency to the natural frequency, i.e.,\n",
        "$$\n",
        "\\nu = \\frac{\\omega_{\\text{err}}}{\\omega_{\\text{eig}}},\n",
        "$$\n",
        "$D$ is the damping ratio,\n",
        "and $F$ is the force applied to the system.\n",
        "\n",
        "The goal of the project is to determine the optimal values for the parameters $\\omega_{\\text{eig}}$, $D$, and $F$\n",
        "that result in the best fit of the one-mass oscillator model to the observed amplitudes.\n",
        "\n",
        "\n",
        "### The Real-World Data\n",
        "\n",
        "There are two different measurements. `J` represents the measured frequencies, and `N` represents the measured amplitudes.\n"
      ],
      "id": "41141cd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df1 = pd.read_pickle(\"./data/Hcf.d/df1.pkl\")\n",
        "df2 = pd.read_pickle(\"./data/Hcf.d/df2.pkl\")\n",
        "df1.describe()"
      ],
      "id": "e3c45c22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df1.head()"
      ],
      "id": "cba14aba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot the data, i.e., the measured amplitudes as a function of the measured frequencies\n",
        "plt.scatter(df1[\"J\"], df1[\"N\"], color=\"black\", label=\"Spektralpunkte\", zorder=5, s=10)\n",
        "plt.xlabel(\"Frequency [Hz]\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ],
      "id": "dddcd3e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Low amplitudes distort the fit and are negligible therefore we define a lower threshold for `N`.\n"
      ],
      "id": "3bac433a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "threshold = 0.4\n",
        "df1.sort_values(\"N\")\n",
        "max_N = max(df1[\"N\"])\n",
        "df1 = df1[df1[\"N\"]>=threshold*max_N]"
      ],
      "id": "1a29eba9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract the frequency value for maximum value of the amplitude. This serves as the initial value for one decision variable.\n"
      ],
      "id": "123dc2b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_max=df1[df1[\"N\"]==max(df1[\"N\"])]\n",
        "initial_Oeig = df_max[\"J\"].values[0]\n",
        "max_N = df_max[\"N\"].values[0]"
      ],
      "id": "c8b51dbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have to define the other two initial guesses for the damping ratio and the force, e.g.,\n"
      ],
      "id": "63f1f90b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "initial_D = 0.006\n",
        "initial_F = 0.120\n",
        "initial_values = [initial_Oeig, initial_D, initial_F]"
      ],
      "id": "aa9c1ea2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, we define the bounds for the decision variables:\n"
      ],
      "id": "59f3db99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "min_Oerr = min(df1[\"J\"])\n",
        "max_Oerr = max(df1[\"J\"])"
      ],
      "id": "e3332243",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bounds = [(min_Oerr, max_Oerr), (0, 0.03), (0, 1)]"
      ],
      "id": "371d7f3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objective Function\n",
        "\n",
        "Then we define the objective function:\n"
      ],
      "id": "debde705"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def one_mass_oscillator(params, Oerr) -> np.ndarray:\n",
        "    # returns amplitudes of the system\n",
        "    # Defines the model of a one mass oscilator \n",
        "    Oeig, D, F = params\n",
        "    nue = Oerr / Oeig\n",
        "    V = F / (np.sqrt((1 - nue**2) ** 2 + (4 * D**2 * nue**2)))\n",
        "    return V"
      ],
      "id": "316f17ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def objective_function(params, Oerr, amplitudes) -> np.ndarray:\n",
        "    # objective function to compare calculated and real amplitudes\n",
        "    return np.sum((amplitudes - one_mass_oscillator(params, Oerr)) ** 2)"
      ],
      "id": "bd0d0d60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the options for the optimzer and start the optimization process:\n"
      ],
      "id": "79493d6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "options = {\n",
        "    \"maxfun\": 100000,\n",
        "    \"ftol\": 1e-9,\n",
        "    \"xtol\": 1e-9,\n",
        "    \"stepmx\": 10,\n",
        "    \"eta\": 0.25,\n",
        "    \"gtol\": 1e-5}"
      ],
      "id": "c5756c78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "J = np.array(df1[\"J\"]) # measured frequency\n",
        "N = np.array(df1[\"N\"]) # measured amplitude"
      ],
      "id": "d3303ea6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = minimize(\n",
        "    objective_function,\n",
        "    initial_values,\n",
        "    args=(J, N),\n",
        "    method='Nelder-Mead',\n",
        "    bounds=bounds,\n",
        "    options=options)"
      ],
      "id": "b3b914b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "We can observe the results:\n"
      ],
      "id": "6f899fb8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# map optimized values to variables\n",
        "resonant_frequency = result.x[0]\n",
        "D = result.x[1]\n",
        "F = result.x[2]\n",
        "# predict the resonant amplitude with the fitted one mass oscillator.\n",
        "X_pred = np.linspace(min_Oerr, max_Oerr, 1000)\n",
        "ypred_one_mass_oscillator = one_mass_oscillator(result.x, X_pred)\n",
        "resonant_amplitude = max(ypred_one_mass_oscillator)\n",
        "print(f\"result: {result}\")"
      ],
      "id": "eb6e20a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can plot the optimized fit and the real values:\n"
      ],
      "id": "6c808377"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.scatter(\n",
        "    df1[\"J\"],\n",
        "    df1[\"N\"],\n",
        "    color=\"black\",\n",
        "    label=\"Spektralpunkte filtered\",\n",
        "    zorder=5,\n",
        "    s=10,\n",
        ")\n",
        "# color the max amplitude point red\n",
        "plt.scatter(\n",
        "    initial_Oeig,\n",
        "    max_N,\n",
        "    color=\"red\",\n",
        "    label=\"Max Amplitude\",\n",
        "    zorder=5,\n",
        "    s=10,\n",
        ")\n",
        "\n",
        "plt.plot(\n",
        "        X_pred,\n",
        "        ypred_one_mass_oscillator,\n",
        "        label=\"Alpha\",\n",
        "        color=\"blue\",\n",
        "        linewidth=1,\n",
        "    )\n",
        "plt.scatter(\n",
        "    resonant_frequency,\n",
        "    resonant_amplitude,\n",
        "    color=\"blue\",\n",
        "    label=\"Max Curve Fit\",\n",
        "    zorder=10,\n",
        "    s=20,\n",
        ")"
      ],
      "id": "145338a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tasks\n",
        "\n",
        "+ Investigate the trust region Conjugate Gradient method\n",
        "+ Investigate the Differential Evolution Algorithm\n",
        "+ Compare all the methods we introduced for the Rosenbrock (4d) and Rastrigin (2d) function\n",
        "\n",
        "\n",
        "### Task for the Project Work\n",
        "\n",
        "::: {.callout-note}\n",
        "### Task\n",
        "\n",
        "* Experiment with various optimizers to identify the optimal parameter setup for the one-mass oscillator model and both data frames.\n",
        "* Please explain your thoughts and ideas, and subsequently compare the results obtained from different optimizers.\n",
        ":::\n",
        "\n",
        "\n",
        "## Jupyter Notebook\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "* The Jupyter-Notebook of this lecture is available on GitHub in the [Hyperparameter-Tuning-Cookbook Repository](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/003_scipy_optimize_intro.ipynb)\n",
        "\n",
        ":::"
      ],
      "id": "5442d13d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}