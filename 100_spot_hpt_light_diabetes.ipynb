{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Pytorch Lightning: Hyperparameter Tuning with Diabetes Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| label: 100_imports\n",
        "import numpy as np\n",
        "import os\n",
        "from math import inf\n",
        "import numpy as np\n",
        "import warnings\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will show how `spotPython` can be integrated into the `PyTorch` Lightning\n",
        "training workflow for a regression task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Diabetes Data Set {#sec-the-diabetes-data-set-100}\n",
        "\n",
        "This chapter describes the hyperparameter tuning of a `PyTorch Lightning` network on the Diabetes data set. This is a PyTorch Dataset for regression. A toy data set from scikit-learn. Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients,  as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Exploration of the sklearn Diabetes Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from spotPython.plot.xy import plot_y_vs_X\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(20, 15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "* Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of n_samples (i.e. the sum of squares of each column totals 1).\n",
        "\n",
        "* HDL (high-density lipoprotein) cholesterol, sometimes called “good” cholesterol, absorbs cholesterol in the blood and carries it back to the liver.\n",
        "* The liver then flushes it from the body.\n",
        "* High levels of HDL cholesterol can lower your risk for heart disease and stroke.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating the PyTorch data set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.diabetes import Diabetes\n",
        "data_set = Diabetes()\n",
        "print(len(data_set))\n",
        "print(data_set.names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup{#sec-setup-24}\n",
        "\n",
        "### General Experiment Setup{#sec-general-experiment-setup-24}\n",
        "\n",
        "To keep track of the different experiments, we use a `PREFIX` for the experiment name. The `PREFIX` is used to create a unique experiment name. The `PREFIX` is also used to create a unique TensorBoard folder, which is used to store the TensorBoard log files.\n",
        "\n",
        "`spotpython` allows the specification of two different types of stopping criteria: first, the number of function evaluations (`fun_evals`), and second, the maximum run time in seconds (`max_time`). Here, we will set the number of function evaluations to infinity and the maximum run time to one minute.\n",
        "\n",
        "Furthermore, we set the initial design size (`init_size`) to 10. The initial design is used to train the surrogate model. The surrogate model is used to predict the performance of the hyperparameter configurations. The initial design is also used to train the first model. Since the `init_size` belongs to the experimental design, it is set in the `design_control` dictionary, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.design_control_init).\n",
        "\n",
        "`max_time` is set to one minute for demonstration purposes and `init_size` is set to 10 for demonstration purposes. For real experiments, these values should be increased.\n",
        "Note,  the total run time may exceed the specified `max_time`, because the initial design is always evaluated, even if this takes longer than `max_time`.\n",
        "\n",
        "The following parameters are used to specify the general experiment setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 024_sum_exp\n",
        "PREFIX = \"100\"\n",
        "fun_evals = inf\n",
        "max_time = 1\n",
        "init_size = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preprocessing {#sec-preprocessing-100}\n",
        "\n",
        "Preprocessing is handled by `Lightning` and `PyTorch`. It is described in the [LIGHTNINGDATAMODULE](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) documentation. Here you can find information about the `transforms` methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Data Splitting\n",
        "\n",
        "The data splitting is handled by `Lightning`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function {#sec-loss-function-100}\n",
        "\n",
        "The loss function is specified in the configurable network class [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/regression/nn_linear_regressor.py).\n",
        "We will use the mean squared error (MSE) as the loss function. i.e., from `torchmetrics` the function ` mean_squared_error`, see [[SOURCE]](https://github.com/Lightning-AI/torchmetrics/blob/master/src/torchmetrics/functional/regression/mse.py).\n",
        "The user can specify any of the loss functions from `torchmetrics` via the argument `_torchmetric` in the `fun_control` dictionary, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.fun_control_init).\n",
        "\n",
        "A detailed description of the loss functions (torchmetrics) is presented in @sec-torchmetrics-100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Objective Function `fun` {#sec-the-objective-function-31}\n",
        "\n",
        "The objective function `fun` from the class `HyperLight` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/hyperlight.py) is selected next. It implements an interface from `PyTorch`'s training, validation, and testing methods to `spotPython`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.fun.hyperlight import HyperLight\n",
        "fun = HyperLight().fun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Setup\n",
        "\n",
        "By using `core_model_name = \"light.regression.NNLinearRegressor\"`, the `spotPython` model class `NetLightRegression` [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/light/regression/netlightregression/) from the `light.regression` module is selected.\n",
        "For a given `core_model_name`, the corresponding hyperparameters are automatically loaded from the associated dictionary, which is stored as a JSON file. The JSON file contains hyperparameter type information, names, and bounds. For `spotPython` models, the hyperparameters are stored in the `LightHyperDict`, see [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperdict/light_hyper_dict.json)\n",
        "Alternatively, you can load a local hyper_dict.\n",
        "The `hyperdict`  uses the default hyperparameter settings. These can be modified as described in @sec-modifying-hyperparameter-levels.\n",
        "\n",
        "The following parameters are used for the model setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 100_sum_model-setup\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "core_model_name = \"light.regression.NNLinearRegressor\"\n",
        "hyperdict = LightHyperDict\n",
        "_L_in=10\n",
        "_L_out=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Surrogate Model Setup\n",
        "\n",
        "The default surrogate model is the `Kriging` model, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/build/kriging/). We specify `noise` as `True` to include noise in the model. An `anisotropic` kernel is used, which allows different length scales for each dimension, by setting `n_theta = 2`. Furthermore, the interval for the `Lambda` value is set to `[1e-3, 1e2]`.\n",
        "\n",
        "These parameters are set in the `surrogate_control` dictionary and therefore passed  to the `surrogate_control_init` function [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.surrogate_control_init)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 100_surrogate_control_setup\n",
        "noise = True\n",
        "n_theta = 2\n",
        "min_Lambda = 1e-3\n",
        "max_Lambda = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary: Setting up the Experiment {#sec-summary-setting-up-the-experiment-24}\n",
        "\n",
        "At this stage, all required information is available to set up the dictionaries for the hyperparameter tuning.\n",
        "The class `Spot` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py) is the hyperparameter tuning workhorse. It is initialized with the following parameters, which were specified above.\n",
        "\n",
        "* `fun`: the objective function\n",
        "* `fun_control`: the dictionary with the control parameters for the objective function\n",
        "* `design_control`: the dictionary with the control parameters for the experimental design\n",
        "* `surrogate_control`: the dictionary with the control parameters for the surrogate model\n",
        "* `optimizer_control`: the dictionary with the control parameters for the optimizer\n",
        "\n",
        "`spotpython` allows maximum flexibility in the definition of the hyperparameter tuning setup. Alternative surrogate models, optimizers, and experimental designs can be used. Thus, interfaces for the `surrogate` model, experimental `design`, and `optimizer` are provided. The default surrogate model is the kriging model, the default optimizer is the differential evolution, and default experimental design is the Latin hypercube design.\n",
        "\n",
        "The information discussed so far can be summarized as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 100_summary_control\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.utils.init import (fun_control_init, surrogate_control_init)\n",
        "\n",
        "\n",
        "fun = HyperLight().fun\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=\"100\",\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = Diabetes(),\n",
        "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
        "    hyperdict=LightHyperDict,\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    _torchmetric=\"mean_squared_error\",\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "   )\n",
        "surrogate_control = surrogate_control_init(n_theta=2, noise=True, min_Lambda=1e-3, max_Lambda=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checking the Experimental Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import set_hyperparameter\n",
        "set_hyperparameter(fun_control, \"initialization\", [\"Default\"])\n",
        "print(gen_design_table(fun_control))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensorboard {#sec-tensorboard-100}\n",
        "\n",
        "The textual output shown in the console (or code cell) can be visualized with Tensorboard.\n",
        "\n",
        "\n",
        "```{raw}\n",
        "tensorboard --logdir=\"runs/\"\n",
        "```\n",
        "\n",
        "Further information can be found in the [PyTorch Lightning documentation](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html) for Tensorboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Hyperparameter Tuning Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 100_spot_run\n",
        "from spotPython.spot import spot\n",
        "spot_tuner = spot.Spot(\n",
        "    fun=fun,\n",
        "    fun_control=fun_control,\n",
        "    surrogate_control=surrogate_control,\n",
        ")\n",
        "res = spot_tuner.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the `spotgui`\n",
        "\n",
        "The `spotgui` [[github]](https://github.com/sequential-parameter-optimization/spotGUI) provides a convenient way to interact with the hyperparameter tuning process.\n",
        "To obtain the settings from @sec-summary-setting-up-the-experiment-24, the `spotgui` can be started as shown in @fig-spotgui.\n",
        "\n",
        "![spotgui](./figures_static/024_gui.png){width=100% #fig-spotgui}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Progress Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "After the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized with `spotpython`'s method `plot_progress`. The black points represent the performace values (score or metric) of  hyperparameter configurations from the initial design, whereas the red points represents the  hyperparameter configurations found by the surrogate model based optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_progress(log_y=True, filename=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuned Hyperparameters and Their Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results can be printed in tabular form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A histogram can be used to visualize the most important hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_importance(threshold=10.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Hyperparameter Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 024_plot_important_hyperparameter_contour\n",
        "spot_tuner.plot_important_hyperparameter_contour(max_imp=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parallel Coordinates Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: 024_parallel_plot\n",
        "spot_tuner.parallel_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the saved experiment and get the hyperparameters (tuned architecture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.file import get_experiment_from_PREFIX\n",
        "config = get_experiment_from_PREFIX(\"100\")[\"config\"]\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross Validation With Lightning\n",
        "\n",
        "* The `KFold` class from `sklearn.model_selection` is used to generate the folds for cross-validation.\n",
        "* These mechanism is used to generate the folds for the final evaluation of the model.\n",
        "* The `CrossValidationDataModule` class [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/data/lightcrossvalidationdatamodule.py) is used to generate the folds for the hyperparameter tuning process.\n",
        "* It is called from the `cv_model` function [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/cvmodel.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.cvmodel import cv_model\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"k_folds\",\n",
        "                        value=2,\n",
        "                        replace=True)\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"test_size\",\n",
        "                        value=0.6,\n",
        "                        replace=True)\n",
        "cv_model(config, fun_control)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on the full data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.testmodel import test_model\n",
        "test_model(config, fun_control)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.loadmodel import load_light_from_checkpoint\n",
        "model_loaded = load_light_from_checkpoint(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import get_feature_names\n",
        "get_feature_names(fun_control)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Integrated Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.plot.xai import get_attributions, plot_attributions\n",
        "df = get_attributions(spot_tuner, fun_control, attr_method=\"IntegratedGradients\")\n",
        "print(df)\n",
        "plot_attributions(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Lift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = get_attributions(spot_tuner, fun_control, attr_method=\"DeepLift\")\n",
        "print(df)\n",
        "plot_attributions(df,  attr_method=\"DeepLift\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Ablation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = get_attributions(spot_tuner, fun_control, attr_method=\"FeatureAblation\")\n",
        "print(df)\n",
        "plot_attributions(df, attr_method=\"FeatureAblation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Activations, Weights, and Gradients\n",
        "\n",
        "In neural networks, activations, weights, and gradients are fundamental concepts that play different.\n",
        "\n",
        "1. Activations:\n",
        "\n",
        "   Activations refer to the outputs of neurons after applying an activation function. In neural networks, the input passes through each neuron of the network layers, where each unit calculates a weighted sum of its inputs and then applies a non-linear activation function (such as ReLU, Sigmoid, or Tanh). These activation functions help introduce non-linearity into the model, enabling the neural network to learn complex relationships between the input data and the predictions. In short, activations are the outputs that are forwarded by the neurons after applying the activation function.\n",
        "\n",
        "2. Weights:\n",
        "\n",
        "   Weights are parameters within a neural network that control the strength of the connection between two neurons in successive layers. They are adjusted during the training process to enable the neural network to perform the desired task as well as possible. Each input is multiplied by a weight, and the neural network learns by adjusting these weights based on the error between the predictions and the actual values. Adjusting the weights allows the network to recognize patterns and relationships in the input data and use them for predictions or classifications.\n",
        "\n",
        "3. Gradients:\n",
        "\n",
        "   In the context of machine learning and specifically in neural networks, gradients are a measure of the rate of change or the slope of the loss function (a function that measures how well the network performs in predicting the desired output) with respect to the weights. During the training process, the goal is to minimize the value of the loss function to improve the model’s performance. The gradients indicate the direction and size of the steps that need to be taken to adjust the weights in a way that minimizes the loss (known as gradient descent). By repeatedly adjusting the weights in the opposite direction of the gradient, the network can be effectively trained to improve its prediction accuracy.\n",
        "\n",
        "::: {.callout-note}\n",
        "### Reference:\n",
        "\n",
        "* The following code is based on [[PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS]](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/02-activation-functions.html), Author: Phillip Lippe, License: [[CC BY-SA]](https://creativecommons.org/licenses/by-sa/3.0/), Generated: 2023-03-15T09:52:39.179933.\n",
        "\n",
        ":::\n",
        "\n",
        "After we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.plot.xai import (get_activations, get_gradients, get_weights, plot_nn_values_hist, plot_nn_values_scatter, visualize_weights, visualize_gradients, visualize_activations, visualize_activations_distributions, visualize_gradient_distributions, visualize_weights_distributions)\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.file import get_experiment_from_PREFIX\n",
        "get_experiment_from_PREFIX(\"100\", return_dict=False)\n",
        "pprint.pprint(config)\n",
        "batch_size = config[\"batch_size\"]\n",
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.loadmodel import load_light_from_checkpoint\n",
        "model_loaded = load_light_from_checkpoint(config, fun_control)\n",
        "model = model_loaded.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights, index = get_weights(model, return_index=True)\n",
        "print(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_weights(model, absolute=True, cmap=\"gray\", figsize=(6, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_weights_distributions(model, color=f\"C{0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activations = get_activations(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_activations(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\", cmap=\"BlueWhiteRed\", absolute=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Absolute values of the activations are plotted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_activations(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\", absolute=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_activations_distributions(net=model, fun_control=fun_control, batch_size=batch_size, device=\"cpu\", color=\"C0\", columns=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gradients = get_gradients(model, fun_control, batch_size, device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_gradients(model, fun_control, batch_size, absolute=True, cmap=\"BlueWhiteRed\", figsize=(6, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_gradient_distributions(model, fun_control, batch_size=batch_size, color=f\"C{0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Layer Conductance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.plot.xai import get_weights_conductance_last_layer, plot_conductance_last_layer\n",
        "w, c = get_weights_conductance_last_layer(spot_tuner, fun_control)\n",
        "plot_conductance_last_layer(w,c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Details of the Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function {#sec-loss-function-100}\n",
        "\n",
        "\n",
        "The parameter `_torchmetric` is used to specify the metric. Any metric from the `torchmetrics` library can be used.\n",
        "Currently, the following metrics are provided, see [[LINK]](https://lightning.ai/docs/torchmetrics/stable/all-metrics.html):\n",
        "\n",
        "* `concordance_corrcoef`\n",
        "* `cosine_similarity`\n",
        "* `critical_success_index`\n",
        "* `explained_variance`\n",
        "* `kendall_rank_corrcoef`\n",
        "* `kl_divergence`\n",
        "* `log_cosh_error`\n",
        "* `mean_squared_log_error`\n",
        "* `mean_absolute_error`\n",
        "* `mean_squared_error`\n",
        "* `pearson_corrcoef`\n",
        "* `mean_absolute_percentage_error`\n",
        "* `mean_absolute_percentage_error`\n",
        "* `minkowski_distance`\n",
        "* `r2_score`\n",
        "* `relative_squared_error`\n",
        "* `spearman_corrcoef`\n",
        "* `symmetric_mean_absolute_percentage_error`\n",
        "* `tweedie_deviance_score`\n",
        "* `weighted_mean_absolute_percentage_error`\n",
        "\n",
        "\n",
        "\n",
        " The metric is used in the training, validation, and testing steps of the model.\n",
        "\n",
        "\n",
        "`NNLightRegressor` implements a loss function as follows:\n",
        "\n",
        "```python\n",
        "    def _calculate_loss(self, batch):\n",
        "        \"\"\"\n",
        "        Calculate the loss for the given batch.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            mode (str, optional): The mode of the model. Defaults to \"train\".\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y = y.view(len(y), 1)\n",
        "        y_hat = self(x)\n",
        "        loss = self.metric(y_hat, y)\n",
        "        return loss\n",
        "```\n",
        "\n",
        "The loss function is used in the training, validation, and testing steps of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Handling {#sec-data-handling-100}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown below, a DataLoader from `torch.utils.data` can be used to check the data. `spotPython` implements a `DataModule` class, which is used to load the data. The `DataModule` class is called from the `fun` function. The `DataModule` class is used to load the data and to split the data into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.diabetes import Diabetes\n",
        "import torch\n",
        "data_set = Diabetes(target_type=torch.float)\n",
        "print(len(data_set))\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(data_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Information related to the data is set in the `fun_control` dictionary, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.fun_control_init).\n",
        "Specifically, the following parameters are set:\n",
        "\n",
        "* `data_set`: the data set\n",
        "* `num_workers`: the number of workers\n",
        "* `test_size`: the size of the test set\n",
        "* `test_seed`: the seed for the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "data_set = Diabetes(target_type=torch.float)\n",
        "fun_control = fun_control_init(\n",
        "    data_set=data_set,\n",
        "    device=\"cpu\",\n",
        "    enable_progress_bar=False,\n",
        "    num_workers=0,\n",
        "    show_progress=True,\n",
        "    test_size=0.4,\n",
        "    test_seed=42,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `spotPython`'s  `LightDataModule` Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The values from the `fun_control` dictionary are used to set up the `LightDataModule` class, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/data/lightdatamodule/).\n",
        "The parameter `batch_size` is a hyperparameter that can be tuned and therefore not set in the `fun_control` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "dm = LightDataModule(\n",
        "    dataset=fun_control[\"data_set\"],\n",
        "    batch_size=5,\n",
        "    num_workers=fun_control[\"num_workers\"],\n",
        "    test_size=fun_control[\"test_size\"],\n",
        "    test_seed=fun_control[\"test_seed\"],\n",
        ")\n",
        "dm.setup()\n",
        "print(f\"train_model(): Test set size: {len(dm.data_test)}\")\n",
        "print(f\"train_model(): Train set size: {len(dm.data_train)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The steps described above are handled by the `LightDataModule` class. This class is used to create the data loaders for the training, validation, and test sets. The `LightDataModule` class is part of the `spotPython` package.\n",
        "The `LightDataModule` class provides the following methods:\n",
        "\n",
        "* `prepare_data()`: This method is used to prepare the data set.\n",
        "* `setup()`: This method is used to create the data loaders for the training, validation, and test sets.\n",
        "* `train_dataloader()`: This method is used to return the data loader for the training set.\n",
        "* `val_dataloader()`: This method is used to return the data loader for the validation set.\n",
        "* `test_dataloader()`: This method is used to return the data loader for the test set.\n",
        "* `predict_dataloader()`: This method is used to return the data loader for the prediction set.\n",
        "\n",
        "Details of the implementation can be found in @sec-hpt-pytorch-lightning-data-30."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HyperLight: Interfacing the Optimizer `spotPython`  with the Data, the Loss Function, and the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The `HyperLight` Class Method `fun()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class `Hyperlight` implements the method `fun()`, which passes the hyperparameter tuning parametrizations to the neural network and the configurations to the DataModule. The `fun()` method passes one configuration to the `train_model()` method, which trains the model and returns the performance value. The performance value is then returned to the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "df_eval = train_model(config, fun_control)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Method `train_model`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`train_model` trains a model using the given configuration and function control parameters.\n",
        "It extracts the information from the configuration and the function control parameters and trains the model using the `Lightning` framework.\n",
        "A simplified version of the `train_model` method is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(config: dict, fun_control: dict, timestamp: bool = True) -> float:\n",
        "    _L_in = fun_control[\"_L_in\"]\n",
        "    _L_out = fun_control[\"_L_out\"]\n",
        "    _torchmetric = fun_control[\"_torchmetric\"]\n",
        "    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
        "    dm = LightDataModule(\n",
        "        dataset=fun_control[\"data_set\"],\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        num_workers=fun_control[\"num_workers\"],\n",
        "        test_size=fun_control[\"test_size\"],\n",
        "        test_seed=fun_control[\"test_seed\"],\n",
        "        scaler=fun_control[\"scaler\"],\n",
        "    )\n",
        "    trainer = L.Trainer()\n",
        "    trainer.fit(model=model, datamodule=dm)\n",
        "    # Test best model on validation set\n",
        "    result = trainer.validate(model=model, datamodule=dm)\n",
        "    result = result[0]\n",
        "    return result[\"val_loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of the Lightning Training Process in `spotPython`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code exemplifies the workflow and shows how to combine the elements discussed so far:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* First, results from the tuning experiment (\"100\") are loaded.\n",
        "* A `LightDataModule` object is created and the `setup()` method is called.\n",
        "* Then, the `Trainer` is initialized.\n",
        "* Load the NN model from the last checkpoint.\n",
        "* Next, the `fit()` method is called to train the model.\n",
        "* Finally, the `validate()` method is called to validate the model. The `validate()` method returns the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load results from the experiment 100\n",
        "from spotPython.utils.file import get_experiment_from_PREFIX\n",
        "config, fun_control, design_control, surrogate_control, optimizer_control = get_experiment_from_PREFIX(\"100\",return_dict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a LightDataModule object\n",
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "dm = LightDataModule(\n",
        "    dataset=fun_control[\"data_set\"],\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    num_workers=fun_control[\"num_workers\"],\n",
        "    test_size=fun_control[\"test_size\"],\n",
        "    test_seed=fun_control[\"test_seed\"],\n",
        ")\n",
        "dm.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "import lightning as L\n",
        "trainer = L.Trainer(enable_progress_bar=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model from the checkpoint\n",
        "from spotPython.light.loadmodel import load_light_from_checkpoint\n",
        "model_loaded = load_light_from_checkpoint(config, fun_control)\n",
        "model = model_loaded.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit (train) the model\n",
        "trainer.fit(model=model, datamodule=dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation: Test best model on validation set\n",
        "result = trainer.validate(model=model, datamodule=dm)\n",
        "# unlist the result (from a list of one dict)\n",
        "result = result[0]\n",
        "print(result[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading a User Specified Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a user-specified data set is straightforward. The user simply needs to provide a data set and loads is as a  `spotPython`  `CVSDataset()` class by specifying the path, filename, and target column as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "data_set = CSVDataset(directory=\"./userData/\",\n",
        "                     filename=\"data.csv\",\n",
        "                     target_column='target',\n",
        "                     feature_type=torch.float32,\n",
        "                     target_type=torch.float32,\n",
        "                     rmNA=True)\n",
        "print(len(data_set))\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(data_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using a User Specified Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As templates, we provide the following three files that allow the user to specify a model in the `/userModel` directory:\n",
        "    * `my_regressor.py`\n",
        "    * `my_hyperdict.json`\n",
        "    * `my_hyperdict.py`\n",
        "\n",
        "The `my_regressor.py` file contains the model class, which is a subclass of `nn.Module`. The `my_hyperdict.json` file contains the hyperparameter settings as a dictionary, which are loaded via the `my_hyperdict.py` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "import sys\n",
        "sys.path.insert(0, './userModel')\n",
        "import my_regressor\n",
        "import my_hyper_dict\n",
        "add_core_model_to_fun_control(fun_control=fun_control,\n",
        "                              core_model=my_regressor.MyRegressor,\n",
        "                              hyper_dict=my_hyper_dict.MyHyperDict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "/Users/bartz/miniforge3/envs/spotCondaEnv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
