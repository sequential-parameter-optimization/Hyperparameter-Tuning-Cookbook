{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "title: Hyperparameter Tuning with `spotpython` and `PyTorch` Lightning for the Diabetes Data Set Using a User Specified ResNet Model\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 606_user-user-imports\n",
        "#| echo: false\n",
        "import os\n",
        "from math import inf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing the necessary libraries, the `fun_control` dictionary is set up via the `fun_control_init` function.\n",
        "The `fun_control` dictionary contains\n",
        "\n",
        "* `PREFIX`: a unique identifier for the experiment\n",
        "* `fun_evals`: the number of function evaluations\n",
        "* `max_time`: the maximum run time in minutes\n",
        "* `data_set`: the data set. Here we use the `Diabetes` data set that is provided by `spotpython`.\n",
        "* `core_model_name`: the class name of the neural network model. This neural network model is provided by `spotpython`.\n",
        "* `hyperdict`: the hyperparameter dictionary. This dictionary is used to define the hyperparameters of the neural network model. It is also provided by `spotpython`.\n",
        "* `_L_in`: the number of input features. Since the `Diabetes` data set has 10 features, `_L_in` is set to 10.\n",
        "* `_L_out`: the number of output features. Since we want to predict a single value, `_L_out` is set to 1.\n",
        "\n",
        "The `HyperLight` class is used to define the objective function `fun`.\n",
        "It connects the `PyTorch` and the `spotpython` methods and is provided by `spotpython`.\n",
        "\n",
        "To access the user specified ResNet model, the path to the user model must be added to the Python path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 606_user-user-path_setup\n",
        "import sys\n",
        "sys.path.insert(0, './userModel')\n",
        "import my_resnet\n",
        "import my_hyper_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code, we do not specify the ResNet model in the `fun_control` dictionary. It will be added in a second step as the user specified model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 606_user-user-spotpython_setup\n",
        "\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.file import get_experiment_filename\n",
        "\n",
        "PREFIX=\"606-user-resnet\"\n",
        "\n",
        "data_set = Diabetes()\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    fun_evals=inf,\n",
        "    max_time=1,\n",
        "    data_set = data_set,\n",
        "    _L_in=10,\n",
        "    _L_out=1)\n",
        "\n",
        "fun = HyperLight().fun"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a second step, we can add the user specified ResNet model to the `fun_control` dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "add_core_model_to_fun_control(fun_control=fun_control,\n",
        "                              core_model=my_resnet.MyResNet,\n",
        "                              hyper_dict=my_hyper_dict.MyHyperDict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The method `set_hyperparameter` allows the user to modify default hyperparameter settings.\n",
        "Here we modify some hyperparameters to keep the model small and to decrease the tuning time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.hyperparameters.values import set_hyperparameter\n",
        "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
        "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
        "set_hyperparameter(fun_control, \"epochs\", [3,7])\n",
        "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
        "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
        "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
        "set_hyperparameter(fun_control, \"lr_mult\", [0.1, 20.0])\n",
        "\n",
        "design_control = design_control_init(init_size=10)\n",
        "\n",
        "print(gen_design_table(fun_control))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, a `Spot` object is created.\n",
        "Calling the method `run()` starts the hyperparameter tuning process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 606_user-user-run\n",
        "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
        "res = spot_tuner.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looking at the Results\n",
        "\n",
        "### Tuning Progress\n",
        "\n",
        "After the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized with `spotpython`'s method `plot_progress`. The black points represent the performace values (score or metric) of  hyperparameter configurations from the initial design, whereas the red points represents the  hyperparameter configurations found by the surrogate model based optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_tuner.plot_progress()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuned Hyperparameters and Their Importance\n",
        "\n",
        "Results can be printed in tabular form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A histogram can be used to visualize the most important hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_tuner.plot_importance(threshold=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_tuner.plot_important_hyperparameter_contour(max_imp=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the Tuned Architecture {#sec-get-spot-results-605}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pprint\n",
        "from spotpython.hyperparameters.values import get_tuned_architecture\n",
        "config = get_tuned_architecture(spot_tuner, fun_control)\n",
        "pprint.pprint(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Details of the User-Specified ResNet Model\n",
        "\n",
        "The specification of a user model requires three files:\n",
        "\n",
        "* `my_resnet.py`: the Python file containing the user specified ResNet model\n",
        "* `my_hyperdict.py`: the Python file for loading the hyperparameter dictionary `my_hyperdict.json` for the user specified ResNet model\n",
        "* `my_hyperdict.json`: the JSON file containing the hyperparameter dictionary for the user specified ResNet model\n",
        "\n",
        "\n",
        "\n",
        "### `my_resnet.py`\n",
        "\n",
        "```python\n",
        "import lightning as L\n",
        "import torch\n",
        "from torch import nn\n",
        "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
        "import torchmetrics.functional.regression\n",
        "import torch.optim as optim\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, act_fn, dropout_prob):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(output_dim)\n",
        "        self.ln1 = nn.LayerNorm(output_dim)  \n",
        "        self.fc2 = nn.Linear(output_dim, output_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
        "        self.ln2 = nn.LayerNorm(output_dim)\n",
        "        self.act_fn = act_fn\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if input_dim != output_dim:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Linear(input_dim, output_dim),\n",
        "                nn.BatchNorm1d(output_dim)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        \n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.ln1(out)\n",
        "        out = self.act_fn(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.ln2(out)\n",
        "        out += identity  # Residual connection\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "class MyResNet(L.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        l1: int,\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        initialization: str,\n",
        "        act_fn: nn.Module,\n",
        "        optimizer: str,\n",
        "        dropout_prob: float,\n",
        "        lr_mult: float,\n",
        "        patience: int,\n",
        "        _L_in: int,\n",
        "        _L_out: int,\n",
        "        _torchmetric: str,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._L_in = _L_in\n",
        "        self._L_out = _L_out\n",
        "        if _torchmetric is None:\n",
        "            _torchmetric = \"mean_squared_error\"\n",
        "        self._torchmetric = _torchmetric\n",
        "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
        "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
        "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
        "        \n",
        "        if self.hparams.l1 < 4:\n",
        "            raise ValueError(\"l1 must be at least 4\")\n",
        "        \n",
        "        # Get hidden sizes\n",
        "        hidden_sizes = self._get_hidden_sizes()\n",
        "        layer_sizes = [self._L_in] + hidden_sizes\n",
        "\n",
        "        # Construct the layers with Residual Blocks and Linear Layer at the end\n",
        "        layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            layers.append(\n",
        "                ResidualBlock(\n",
        "                    layer_sizes[i], \n",
        "                    layer_sizes[i + 1], \n",
        "                    self.hparams.act_fn, \n",
        "                    self.hparams.dropout_prob\n",
        "                )\n",
        "            )\n",
        "        layers.append(nn.Linear(layer_sizes[-1], self._L_out))\n",
        "        \n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Initialization (Xavier, Kaiming, or Default)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):        \n",
        "        if isinstance(module, nn.Linear):\n",
        "            if self.hparams.initialization == \"xavier_uniform\":\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "            elif self.hparams.initialization == \"xavier_normal\":\n",
        "                nn.init.xavier_normal_(module.weight)\n",
        "            elif self.hparams.initialization == \"kaiming_uniform\":\n",
        "                nn.init.kaiming_uniform_(module.weight)\n",
        "            elif self.hparams.initialization == \"kaiming_normal\":\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "            else: # \"Default\"\n",
        "                nn.init.uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def _generate_div2_list(self, n, n_min) -> list:\n",
        "        result = []\n",
        "        current = n\n",
        "        repeats = 1\n",
        "        max_repeats = 4\n",
        "        while current >= n_min:\n",
        "            result.extend([current] * min(repeats, max_repeats))\n",
        "            current = current // 2\n",
        "            repeats = repeats + 1\n",
        "        return result\n",
        "\n",
        "    def _get_hidden_sizes(self):\n",
        "        n_low = max(2, int(self._L_in / 4))  # Ensure minimum reasonable size\n",
        "        n_high = max(self.hparams.l1, 2 * n_low)\n",
        "        hidden_sizes = self._generate_div2_list(n_high, n_low)\n",
        "        return hidden_sizes\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "    def _calculate_loss(self, batch):\n",
        "        x, y = batch\n",
        "        y = y.view(len(y), 1)\n",
        "        y_hat = self(x)\n",
        "        loss = self.metric(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        return val_loss\n",
        "\n",
        "    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
        "        return val_loss\n",
        "\n",
        "    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
        "        return val_loss\n",
        "\n",
        "    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        x, y = batch\n",
        "        yhat = self(x)\n",
        "        y = y.view(len(y), 1)\n",
        "        yhat = yhat.view(len(yhat), 1)\n",
        "        return (x, y, yhat)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optimizer_handler(\n",
        "            optimizer_name=self.hparams.optimizer,\n",
        "            params=self.parameters(),\n",
        "            lr_mult=self.hparams.lr_mult\n",
        "        )\n",
        "\n",
        "        # Dynamic creation of milestones based on the number of epochs.\n",
        "        num_milestones = 3  # Number of milestones to divide the epochs\n",
        "        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n",
        "\n",
        "        # Print milestones for debug purposes\n",
        "        print(f\"Milestones: {milestones}\")\n",
        "\n",
        "        # Create MultiStepLR scheduler with dynamic milestones and learning rate multiplier.\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, \n",
        "            milestones=milestones, \n",
        "            gamma=0.1  # Decay factor\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler configuration\n",
        "        lr_scheduler_config = {\n",
        "            \"scheduler\": scheduler,\n",
        "            \"interval\": \"epoch\",  # Adjust learning rate per epoch\n",
        "            \"frequency\": 1,      # Apply the scheduler at every epoch\n",
        "        }\n",
        "        \n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n",
        "```\n",
        "\n",
        "### `my_hyperdict.py`\n",
        "\n",
        "```python\n",
        "import json\n",
        "from spotpython.data import base\n",
        "import pathlib\n",
        "\n",
        "\n",
        "class MyHyperDict(base.FileConfig):\n",
        "    \"\"\"User specified hyperparameter dictionary.\n",
        "\n",
        "    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n",
        "\n",
        "    Attributes:\n",
        "        filename (str):\n",
        "            The name of the file where the hyperparameters are stored.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        filename: str = \"my_hyper_dict.json\",\n",
        "        directory: None = None,\n",
        "    ) -> None:\n",
        "        super().__init__(filename=filename, directory=directory)\n",
        "        self.filename = filename\n",
        "        self.directory = directory\n",
        "        self.hyper_dict = self.load()\n",
        "\n",
        "    @property\n",
        "    def path(self):\n",
        "        if self.directory:\n",
        "            return pathlib.Path(self.directory).joinpath(self.filename)\n",
        "        return pathlib.Path(__file__).parent.joinpath(self.filename)\n",
        "\n",
        "    def load(self) -> dict:\n",
        "        \"\"\"Load the hyperparameters from the file.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the hyperparameters.\n",
        "\n",
        "        Examples:\n",
        "            # Assume the user specified file `my_hyper_dict.json` is in the `./hyperdict/` directory.\n",
        "            >>> user_lhd = MyHyperDict(filename='my_hyper_dict.json', directory='./hyperdict/')\n",
        "        \"\"\"\n",
        "        with open(self.path, \"r\") as f:\n",
        "            d = json.load(f)\n",
        "        return d\n",
        "```\n",
        "\n",
        "### `my_hyperdict.json`\n",
        "\n",
        "```python\n",
        " \"MyResNet\": {\n",
        "        \"l1\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 3,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 3,\n",
        "            \"upper\": 10\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 4,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 4,\n",
        "            \"upper\": 9\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 4,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 1,\n",
        "            \"upper\": 6\n",
        "        },\n",
        "        \"act_fn\": {\n",
        "            \"levels\": [\n",
        "                \"Sigmoid\",\n",
        "                \"Tanh\",\n",
        "                \"ReLU\",\n",
        "                \"LeakyReLU\",\n",
        "                \"ELU\",\n",
        "                \"Swish\"\n",
        "            ],\n",
        "            \"type\": \"factor\",\n",
        "            \"default\": \"ReLU\",\n",
        "            \"transform\": \"None\",\n",
        "            \"class_name\": \"spotpython.torch.activation\",\n",
        "            \"core_model_parameter_type\": \"instance()\",\n",
        "            \"lower\": 0,\n",
        "            \"upper\": 5\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"levels\": [\n",
        "                \"Adadelta\",\n",
        "                \"Adagrad\",\n",
        "                \"Adam\",\n",
        "                \"AdamW\",\n",
        "                \"SparseAdam\",\n",
        "                \"Adamax\",\n",
        "                \"ASGD\",\n",
        "                \"NAdam\",\n",
        "                \"RAdam\",\n",
        "                \"RMSprop\",\n",
        "                \"Rprop\",\n",
        "                \"SGD\"\n",
        "            ],\n",
        "            \"type\": \"factor\",\n",
        "            \"default\": \"SGD\",\n",
        "            \"transform\": \"None\",\n",
        "            \"class_name\": \"torch.optim\",\n",
        "            \"core_model_parameter_type\": \"str\",\n",
        "            \"lower\": 0,\n",
        "            \"upper\": 11\n",
        "        },\n",
        "        \"dropout_prob\": {\n",
        "            \"type\": \"float\",\n",
        "            \"default\": 0.01,\n",
        "            \"transform\": \"None\",\n",
        "            \"lower\": 0.0,\n",
        "            \"upper\": 0.25\n",
        "        },\n",
        "        \"lr_mult\": {\n",
        "            \"type\": \"float\",\n",
        "            \"default\": 1.0,\n",
        "            \"transform\": \"None\",\n",
        "            \"lower\": 0.1,\n",
        "            \"upper\": 10.0\n",
        "        },\n",
        "        \"patience\": {\n",
        "            \"type\": \"int\",\n",
        "            \"default\": 2,\n",
        "            \"transform\": \"transform_power_2_int\",\n",
        "            \"lower\": 2,\n",
        "            \"upper\": 6\n",
        "        },\n",
        "        \"initialization\": {\n",
        "            \"levels\": [\n",
        "                \"Default\",\n",
        "                \"kaiming_uniform\",\n",
        "                \"kaiming_normal\",\n",
        "                \"xavier_uniform\",\n",
        "                \"xavier_normal\"\n",
        "            ],\n",
        "            \"type\": \"factor\",\n",
        "            \"default\": \"Default\",\n",
        "            \"transform\": \"None\",\n",
        "            \"core_model_parameter_type\": \"str\",\n",
        "            \"lower\": 0,\n",
        "            \"upper\": 4\n",
        "        }\n",
        "    }\n",
        "```\n",
        "\n",
        "## Summary\n",
        "\n",
        "This section presented an introduction to the basic setup of hyperparameter tuning with `spotpython` and `PyTorch` Lightning using a ResNet model for the Diabetes data set.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}