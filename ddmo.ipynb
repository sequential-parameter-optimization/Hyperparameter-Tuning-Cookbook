{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---"
      ],
      "id": "86a37769"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data-Driven Modeling and Optimization\n",
        "\n",
        "\n",
        "## StatQuest Videos\n",
        "\n",
        "### June, 11th 2024\n",
        "\n",
        "#### [Histograms, Clearly Explained](https://youtu.be/qBigTkBLU6g)\n",
        "\n",
        "::: {#exr-histograms}\n",
        "###  Histograms\n",
        "Problems with histograms?\n",
        ":::\n",
        "\n",
        "#### [The Main Ideas behind Probability Distributions](https://youtu.be/oI3hZJqXJuc)\n",
        "\n",
        "::: {#exr-small-bins}\n",
        "###  Smaller Bins\n",
        "What happens when we use smaller bins in a histogram?\n",
        ":::\n",
        "\n",
        "::: {#exr-curve}\n",
        "###  Density Curve\n",
        "Why plot a curve to approximate a histogram?\n",
        ":::\n",
        "\n",
        "::: {#sol-curve}\n",
        "###  Density Curve\n",
        "- We can calculate propabilities.\n",
        "- We only need two parameters (the mean and the sd) to form the curve -> Store data more efficently\n",
        "- Blanks can be filled\n",
        ":::\n",
        "\n",
        "#### [The Normal Distribution, Clearly Explained!!!](https://youtu.be/rzFX5NWojp0)\n",
        "\n",
        "::: {#exr-2SD}\n",
        "###  TwoSDQuestion\n",
        "How many samples are plus/minus two SD around the mean?\n",
        ":::\n",
        "\n",
        "::: {#sol-2SD}\n",
        "###  TwoSDAnswer\n",
        "95%\n",
        ":::\n",
        "\n",
        "::: {#exr-2SD1}\n",
        "###  OneSDQuestion\n",
        "How many samples are plus/minus one SD around the mean?\n",
        ":::\n",
        "\n",
        "::: {#sol-2SD1}\n",
        "###  OneSDAnswer\n",
        "68%\n",
        ":::\n",
        "\n",
        "::: {#exr-2SD2}\n",
        "###  ThreeSDQuestion\n",
        "How many samples are plus/minus three SD around the mean?\n",
        ":::\n",
        "\n",
        "::: {#sol-2SD2}\n",
        "###  ThreeSDAnswer\n",
        "99,7%\n",
        ":::\n",
        "\n",
        "::: {#exr-2SD3}\n",
        "###  DataRangeQuestion\n",
        "You have a mean at 100 and a SD of 10. Where are 95% of the data?\n",
        ":::\n",
        "\n",
        "::: {#sol-2SD3}\n",
        "###  DataRangeAnswer\n",
        "80 - 120\n",
        ":::\n",
        "\n",
        "::: {#exr-2SD4}\n",
        "###  PeakHeightQuestion\n",
        "If the peak is very high, is the SD low or high?\n",
        ":::\n",
        "\n",
        "::: {#sol-2SD4}\n",
        "###  PeakHeightAnswer\n",
        "low\n",
        ":::\n",
        "\n",
        "#### [The mean, the media, and the mode]\n",
        "\n",
        "#### [The exponential distribution]\n",
        "\n",
        "#### [Population and Estimated Parameters, Clearly Explained!!!](https://youtu.be/vikkiwjQqfU)\n",
        "\n",
        "::: {#exr-POP1}\n",
        "### ProbabilityQuestion\n",
        "If we have a certain curve and want to calculate the probability of values equal to 20 if the mean is 20.\n",
        ":::\n",
        "\n",
        "::: {#sol-POP1}\n",
        "### ProbabilityAnswer\n",
        "50%\n",
        ":::\n",
        "\n",
        "\n",
        "#### [Calculating the Mean, Variance and Standard Deviation, Clearly Explained!!!](https://youtu.be/SzZ6GpcfoQY)\n",
        "\n",
        "::: {#exr-CAL1}\n",
        "### MeanDifferenceQuestion\n",
        "The difference between μ and x-bar?\n",
        ":::\n",
        "\n",
        "::: {#sol-CAL1}\n",
        "### MeanDifferenceAnswer\n",
        "If we have all the data, μ is the population mean and x-bar is the sample mean. We don't have the full information.\n",
        ":::\n",
        "\n",
        "::: {#exr-CAL2}\n",
        "### EstimateMeanQuestion\n",
        "How do you calculate the sample mean?\n",
        ":::\n",
        "\n",
        "::: {#sol-CAL2}\n",
        "### EstimateMeanAnswer\n",
        "Sum of the values divided by n.\n",
        ":::\n",
        "\n",
        "::: {#exr-CAL3}\n",
        "### SigmaSquaredQuestion\n",
        "What is sigma squared?\n",
        ":::\n",
        "\n",
        "::: {#sol-CAL3}\n",
        "### SigmaSquaredAnswer\n",
        "Variance\n",
        ":::\n",
        "\n",
        "::: {#exr-CAL4}\n",
        "### EstimatedSDQuestion\n",
        "What is the formula for the estimated standard deviation?\n",
        ":::\n",
        "\n",
        "::: {#sol-CAL4}\n",
        "### EstimatedSDAnswer\n",
        "The same as the normal standard deviation, but using n-1.\n",
        ":::\n",
        "\n",
        "::: {#exr-CAL5}\n",
        "### VarianceDifferenceQuestion\n",
        "Difference between the variance and the estimated variance?\n",
        ":::\n",
        "\n",
        "::: {#sol-CAL5}\n",
        "### VarianceDifferenceAnswer\n",
        "-\n",
        ":::\n",
        "\n",
        "#### [What is a mathematical model?]\n",
        "::: {#exr-MAT1}\n",
        "### ModelBenefitsQuestion\n",
        "What are the benefits of using models?\n",
        ":::\n",
        "\n",
        "::: {#sol-MAT1}\n",
        "### ModelBenefitsAnswer\n",
        "- Approximation\n",
        "- Prediction\n",
        "- Understanding\n",
        ":::\n",
        "\n",
        "#### [Sampling from a Distribution, Clearly Explained!!!](https://youtu.be/XLCWeSVzHUU)\n",
        "::: {#exr-SAM1}\n",
        "### SampleDefinitionQuestion\n",
        "What is a sample in statistics?\n",
        ":::\n",
        "\n",
        "::: {#sol-SAM1}\n",
        "### SampleDefinitionAnswer\n",
        "It's a subset of the data.\n",
        ":::\n",
        "\n",
        "### Hypothesis Testing and the Null-Hypothesis, Clearly Explained!!!\n",
        "\n",
        "::: {#exr-Hyp1}\n",
        "### RejectHypothesisQuestion\n",
        "What does it mean to reject a hypothesis?\n",
        ":::\n",
        "\n",
        "::: {#sol-Hyp1}\n",
        "### RejectHypothesisAnswer\n",
        "It means the evidence supports the alternative hypothesis, indicating that the null hypothesis is unlikely to be true.\n",
        ":::\n",
        "\n",
        "::: {#exr-Hyp2}\n",
        "### NullHypothesisQuestion\n",
        "What is a null hypothesis?\n",
        ":::\n",
        "\n",
        "::: {#sol-Hyp2}\n",
        "### NullHypothesisAnswer\n",
        "It's a statement that there is no effect or no difference, and it serves as the default or starting assumption in hypothesis testing.\n",
        ":::\n",
        "\n",
        "::: {#exr-Hyp3}\n",
        "### BetterDrugQuestion\n",
        "How can you show that you have found a better drug?\n",
        ":::\n",
        "\n",
        "::: {#sol-Hyp3}\n",
        "### BetterDrugAnswer\n",
        "By conducting experiments and statistical tests to compare the new drug's effectiveness against the current standard and demonstrating a significant improvement.\n",
        ":::\n",
        "\n",
        "#### [Alternative Hypotheses, Main Ideas]\n",
        "\n",
        "#### [p-values: What they are and how to interpret them]\n",
        "\n",
        "::: {#exr-PVal1}\n",
        "### PValueIntroductionQuestion\n",
        "What is the reason for introducing the p-value?\n",
        ":::\n",
        "\n",
        "::: {#sol-PVal1}\n",
        "### PValueIntroductionAnswer\n",
        "We can reject the null hypothesis. We can make a decision.\n",
        ":::\n",
        "\n",
        "::: {#exr-PVal2}\n",
        "### PValueRangeQuestion\n",
        "Is there any range for p-values? Can it be negative?\n",
        ":::\n",
        "\n",
        "::: {#sol-PVal2}\n",
        "### PValueRangeAnswer\n",
        "It can only be between 0 and 1.\n",
        ":::\n",
        "\n",
        "::: {#exr-PVal3}\n",
        "### PValueRangeQuestion\n",
        "Is there any range for p-values? Can it be negative?\n",
        ":::\n",
        "\n",
        "::: {#sol-PVal3}\n",
        "### PValueRangeAnswer\n",
        "It can only be between 0 and 1.\n",
        ":::\n",
        "\n",
        "::: {#exr-PVal4}\n",
        "### TypicalPValueQuestion\n",
        "What are typical values of the p-value and what does it mean? 5%?\n",
        ":::\n",
        "\n",
        "::: {#sol-PVal4}\n",
        "### TypicalPValueAnswer\n",
        "The chance that we wrongly reject the null hypothesis.\n",
        ":::\n",
        "\n",
        "::: {#exr-PVal5}\n",
        "### FalsePositiveQuestion\n",
        "What is a false-positive?\n",
        ":::\n",
        "\n",
        "::: {#sol-PVal5}\n",
        "### FalsePositiveAnswer\n",
        "If we have a false-positive, we succeed in rejecting the null hypothesis. But in fact/reality, this is false -> False positive.\n",
        ":::\n",
        "\n",
        "#### [How to calculate p-values]\n",
        "\n",
        "::: {#exr-Calc1}\n",
        "### CalculatePValueQuestion\n",
        "How to calculate p-value?\n",
        ":::\n",
        "\n",
        "::: {#sol-Calc1}\n",
        "### CalculatePValueAnswer\n",
        "Probability of specific result, probability of outcome with the same probability, and probability of events with smaller probability.\n",
        ":::\n",
        "\n",
        "::: {#exr-Calc2}\n",
        "### SDCalculationQuestion\n",
        "What is the SD if the mean is 155 and in the range from 142 - 169 there are 95% of the data?\n",
        ":::\n",
        "\n",
        "::: {#sol-Calc2}\n",
        "### SDCalculationAnswer\n",
        "7 is the SD.\n",
        ":::\n",
        "\n",
        "::: {#exr-Calc3}\n",
        "### SidedPValueQuestion\n",
        "When do we need the two-sided p-value and when the one-sided?\n",
        ":::\n",
        "\n",
        "::: {#sol-Calc3}\n",
        "### SidedPValueAnswer\n",
        "If we are not interested in the direction of the change, we use the two-sided. If we want to know about the direction, the one-sided.\n",
        ":::\n",
        "\n",
        "::: {#exr-Calc4}\n",
        "### CoinTestQuestion\n",
        "Test a coin with Tail-Head-Head. What is the p-value?\n",
        ":::\n",
        "\n",
        "::: {#sol-Calc4}\n",
        "### CoinTestAnswer\n",
        "-\n",
        ":::\n",
        "\n",
        "::: {#exr-Calc5}\n",
        "### BorderPValueQuestion\n",
        "If you get exactly the 0.05 border value, can you reject?\n",
        ":::\n",
        "\n",
        "::: {#sol-Calc5}\n",
        "### BorderPValueAnswer\n",
        "-\n",
        ":::\n",
        "\n",
        "::: {#exr-Calc6}\n",
        "### OneSidedPValueCautionQuestion\n",
        "Why should you be careful with a one-sided p-test?\n",
        ":::\n",
        "\n",
        "::: {#sol-Calc6}\n",
        "### OneSidedPValueCautionAnswer\n",
        "If you look in the wrong direction, there is no change.\n",
        ":::\n",
        "\n",
        "::: {#exr-Calc7}\n",
        "### BinomialDistributionQuestion\n",
        "What is the binomial distribution?\n",
        ":::\n",
        "\n",
        "::: {#sol-Calc7}\n",
        "### BinomialDistributionAnswer\n",
        "-\n",
        ":::\n",
        "\n",
        "#### [p-hacking: What it is and how to avoid it]\n",
        "\n",
        "::: {#exr-Hack1}\n",
        "### PHackingWaysQuestion\n",
        "Name two typical ways of p-hacking.\n",
        ":::\n",
        "\n",
        "::: {#sol-Hack1}\n",
        "### PHackingWaysAnswer\n",
        "- Performing repeats until you find one result with a small p-value -> false positive result.\n",
        "- Increasing the sample size within one experiment when it is close to the threshold.\n",
        ":::\n",
        "\n",
        "::: {#exr-Hack2}\n",
        "### AvoidPHackingQuestion\n",
        "How can p-hacking be avoided?\n",
        ":::\n",
        "\n",
        "::: {#sol-Hack2}\n",
        "### AvoidPHackingAnswer\n",
        "Specify the number of repeats and the sample sizes at the beginning.\n",
        ":::\n",
        "\n",
        "::: {#exr-Hack3}\n",
        "### MultipleTestingProblemQuestion\n",
        "What is the multiple testing problem?\n",
        ":::\n",
        "\n",
        "::: {#sol-Hack3}\n",
        "### MultipleTestingProblemAnswer\n",
        "-\n",
        ":::\n",
        "\n",
        "#### [Covariance, Clearly Explained!!!]\n",
        "\n",
        "::: {#exr-Cov1}\n",
        "### CovarianceDefinitionQuestion\n",
        "What is covariance?\n",
        ":::\n",
        "\n",
        "::: {#sol-Cov1}\n",
        "### CovarianceDefinitionAnswer\n",
        "Formula\n",
        ":::\n",
        "\n",
        "::: {#exr-Cov2}\n",
        "### CovarianceMeaningQuestion\n",
        "What is the meaning of covariance?\n",
        ":::\n",
        "\n",
        "::: {#sol-Cov2}\n",
        "### CovarianceMeaningAnswer\n",
        "Large values in the first variable result in large values in the second variable.\n",
        ":::\n",
        "\n",
        "::: {#exr-Cov3}\n",
        "### CovarianceVarianceRelationshipQuestion\n",
        "What is the relationship between covariance and variance?\n",
        ":::\n",
        "\n",
        "::: {#sol-Cov3}\n",
        "### CovarianceVarianceRelationshipAnswer\n",
        "Formula\n",
        ":::\n",
        "\n",
        "::: {#exr-Cov4}\n",
        "### HighCovarianceQuestion\n",
        "If covariance is high, is there a strong relationship?\n",
        ":::\n",
        "\n",
        "::: {#sol-Cov4}\n",
        "### HighCovarianceAnswer\n",
        "No, size doesn't matter.\n",
        ":::\n",
        "\n",
        "::: {#exr-Cov5}\n",
        "### ZeroCovarianceQuestion\n",
        "What if the covariance is zero?\n",
        ":::\n",
        "\n",
        "::: {#sol-Cov5}\n",
        "### ZeroCovarianceAnswer\n",
        "No relationship\n",
        ":::\n",
        "\n",
        "::: {#exr-Cov6}\n",
        "### NegativeCovarianceQuestion\n",
        "Can covariance be negative?\n",
        ":::\n",
        "\n",
        "::: {#sol-Cov6}\n",
        "### NegativeCovarianceAnswer\n",
        "Yes\n",
        ":::\n",
        "\n",
        "::: {#exr-Cov7}\n",
        "### NegativeVarianceQuestion\n",
        "Can variance be negative?\n",
        ":::\n",
        "\n",
        "::: {#sol-Cov7}\n",
        "### NegativeVarianceAnswer\n",
        "No\n",
        ":::\n",
        "\n",
        "#### [Pearson's Correlation, Clearly Explained]\n",
        "\n",
        "::: {#exr-Corr1}\n",
        "### CorrelationValueQuestion\n",
        "What do you do if the correlation value is 10?\n",
        ":::\n",
        "\n",
        "::: {#sol-Corr1}\n",
        "### CorrelationValueAnswer\n",
        "Recalculate\n",
        ":::\n",
        "\n",
        "::: {#exr-Corr2}\n",
        "### CorrelationRangeQuestion\n",
        "What is the possible range of correlation values?\n",
        ":::\n",
        "\n",
        "::: {#sol-Corr2}\n",
        "### CorrelationRangeAnswer\n",
        "-1 to 1\n",
        ":::\n",
        "\n",
        "::: {#exr-Corr3}\n",
        "### CorrelationFormulaQuestion\n",
        "What is the formula for correlation?\n",
        ":::\n",
        "\n",
        "::: {#sol-Corr3}\n",
        "### CorrelationFormulaAnswer\n",
        "Formula\n",
        ":::\n",
        "\n",
        "#### [Boxplots are Awesome]\n",
        "\n",
        "\n",
        "### June, 18th 2024\n",
        "\n",
        "#### [Statistical Power, Clearly Explained](https://youtu.be/Rsc5znwR5FA?si=Ca4e-EopumAtgl8Q)\n",
        "\n",
        "#### [Power Analysis, Clearly Explained!!!](https://youtu.be/VX_M3tIyiYk?si=Vb6Fr1aJWQU5Ujjp)\n",
        "\n",
        "#### [The Central Limit Theorem, Clearly Explained!!!](https://youtu.be/YAlJCEDH2uY?si=NRYvP7Y0Mow32jV2)\n",
        "\n",
        "#### [Boxplots are Awesome](https://youtu.be/fHLhBnmwUM0?si=QB5ccKIxL1FaIc0M)\n",
        "\n",
        "#### [R-squared, Clearly Explained](https://youtu.be/2AQKmw14mHM)\n",
        "\n",
        "Note: When I first made this video, I was thinking about how R-squared relates to Linear Regression, which will not fit a line worse than the mean of the y-axis values. This is because if the values along the x-axis are truly useless in terms of predicting y-axis values, then the slope of the line used to make predictions will be 0, and the intercept will equal the mean. However, it is possible to simply draw a line that fits the data worse than the mean and get a negative R^2. \n",
        "\n",
        "#### [The main ideas of fitting a line to data (The main ideas of least squares and linear regression.)](https://www.youtube.com/embed/PaFPbb66DxQ)\n",
        "\n",
        "#### [Linear Regression, Clearly Explained](https://youtu.be/nk2CQITm_eo)\n",
        "\n",
        "#### [Multiple Regression, Clearly Explained]\n",
        "\n",
        "#### [A Gentle Introduction to Machine Learning](https://youtu.be/Gv9_4yMHFhI?si=BPtw5Rekl37bJ9V1)\n",
        "\n",
        "#### [Maximum Likelihood, clearly explained!!!](https://youtu.be/XepXtl9YKwc?si=ADMYC10DscaxSTvk)\n",
        "\n",
        "#### [Probability is not Likelihood. Find out why!!!](https://youtu.be/pYxNSUDSFH4?si=eEan9lAUp1NNGEjY)\n",
        "\n",
        "#### [Machine Learning Fundamentals: Cross Validation](https://youtu.be/fSytzGwwBVw?si=a8U5yCIEhwAw4AyU)\n",
        "\n",
        "#### [Machine Learning Fundamentals: The Confusion Matrix](https://youtu.be/Kdsp6soqA7o?si=pOEUeyk1Crt9heg1)\n",
        "\n",
        "#### [Machine Learning Fundamentals: Sensitivity and Specificity](https://youtu.be/vP06aMoz4v8?si=9O6FfcKtOWSdx84t)\n",
        "\n",
        "#### [Machine Learning Fundamentals: Bias and Variance](https://youtu.be/EuBBz3bI-aA?si=7MVv_J1HbzMSQS4K)\n",
        "\n",
        "#### [Mutual Information, Clearly Explained](https://youtu.be/eJIp_mgVLwE?si=KaeiRN0st1gqkj4c)\n",
        "\n",
        "\n",
        "\n",
        "### June, 25th 2024\n",
        "\n",
        "#### [Principal Component Analysis (PCA), Step-by-Step](https://youtu.be/FgakZw6K1QQ?si=lmXhc-bpOqb7RmDP)\n",
        "\n",
        "#### [PCA main ideas in only 5 minutes!!!](https://youtu.be/HMOI_lkzW08?si=ZDqiJZNAjRVPx70H)\n",
        "\n",
        "#### [PCA - Practical Tips](https://youtu.be/oRvgq966yZg?si=TIUsxNItfyYOjTLt)\n",
        "\n",
        "#### [PCA in Python](https://youtu.be/Lsue2gEM9D0?si=_fV_RzK8j1jwcb-e)\n",
        "\n",
        "#### [t-SNE, Clearly Explained](https://youtu.be/NEaUSP4YerM?si=f8-6ewwv5TMD7gdL)\n",
        "\n",
        "#### [K-means clustering](https://youtu.be/4b5d3muPQmA?si=O9-s32Kw676wXCQF)\n",
        "\n",
        "#### [Clustering with DBSCAN, Clearly Explained!!!](https://youtu.be/RDZUdRSDOok?si=C7SzTAQC8BmD8AZy)\n",
        "\n",
        "#### [StatQuest: K-nearest neighbors, Clearly Explained](https://youtu.be/HVXime0nQeI?si=wTGGkn_6vIshrTk0)\n",
        "\n",
        "#### [Naive Bayes, Clearly Explained!!!](https://youtu.be/O2L2Uv9pdDA?si=CTRhu0XXwTZuxxwS)\n",
        "\n",
        "#### [Gaussian Naive Bayes, Clearly Explained!!!](https://youtu.be/H3EjCKtlVog?si=cXWTWaQ1cw5wbFXr)\n",
        "\n",
        "\n",
        "\n",
        "### July, 2nd 2024\n",
        "\n",
        "#### [Decision and Classification Trees, Clearly Explained](https://youtu.be/_L39rN6gz7Y?si=KtY-CsLGeAbIJN-f)\n",
        "\n",
        "#### [StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data](https://youtu.be/wpNl-JwwplA?si=R7qiQ4rVzsrW1GAI)\n",
        "\n",
        "#### [Regression Trees, Clearly Explained!!!](https://youtu.be/g9c66TUylZ4?si=aXOCqkDl-fGAFRx2)\n",
        "\n",
        "#### [How to Prune Regression Trees, Clearly Explained!!!](https://youtu.be/D0efHEJsfHo?si=OKizIPtcrWDOSCRF)\n",
        "\n",
        "\n",
        "### Additional Videos\n",
        "\n",
        "* [Odds and Log(Odds), Clearly Explained!!!](https://youtu.be/ARfXDSkQf1Y?si=E4TjFoloRjbQYbzQ)\n",
        "* [One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!](https://youtu.be/589nCGeWG1w?si=YN9vO0HQlnll1wb6)\n",
        "* [Maximum Likelihood for the Exponential Distribution, Clearly Explained!!!](https://youtu.be/p3T-_LMrvBc?si=3Jcjueue1otXzS1r)\n",
        "* [ROC and AUC, Clearly Explained!](https://youtu.be/4jRBRDbJemM?si=hJXxjRV7_Ib2ckVe)\n",
        "* [Entropy (for data science) Clearly Explained!!!](https://youtu.be/YtebGVx-Fxw?si=xbMzfX2oqAsE6MGK)\n",
        "* [Classification Trees in Python from Start to Finish](https://youtu.be/q90UDEgYqeI?si=teRC5oYkHcXXgUSU): Long live video! \n",
        "  0:00 Awesome song and introduction\n",
        "  5:23 Import Modules\n",
        "  7:40 Import Data\n",
        "  11:18 Missing Data Part 1: Identifying\n",
        "  15:57 Missing Data Part 2: Dealing with it\n",
        "  21:16 Format Data Part 1: X and y\n",
        "  23:33 Format Data Part 2: One-Hot Encoding\n",
        "  37:29 Build Preliminary Tree\n",
        "  46:31 Pruning Part 1: Visualize Alpha\n",
        "  51:22 Pruning Part 2: Cross Validation\n",
        "  56:46 Build and Draw Final Tree\n",
        "\n",
        "\n",
        "\n",
        "## Introduction to Statistical Learning\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "Parts of this course are based on the book **An Introduction to Statistical Learning**, @Jame14a.\n",
        "Some of the figures in this presentation are taken from **An Introduction to Statistical Learning** (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani.\n",
        ":::\n",
        "\n",
        "### Opening Remarks and Examples\n",
        "\n",
        "* Artificial Intelligence (AI)\n",
        "* Machine learning (ML)\n",
        "* Deep Learning (DL)\n",
        "\n",
        "![AI, ML, and DL. Taken fron @chol18b](./figures_static/aimldl.png)\n",
        "\n",
        "\n",
        "* 1980's neural networks.\n",
        "* Statistical learning.\n",
        "* IBM Watson supercomputer.\n",
        "\n",
        "\n",
        "Statistical learning problems include:\n",
        "\n",
        "1. Identification of prostate cancer through PSA and other measurements such as age, Gleason score, etc. Scatter plots help reveal the nature of the data and its correlations. Using transformed data (log scale) can highlight typos in the data; for example, a patient with a 449-gram prostate. Recommendation: Always examine the data before conducting any sophisticated analysis.\n",
        "\n",
        "2. Classification of phonemes, specifically between \"aa\" and \"ao.\"\n",
        "\n",
        "3. Prediction of heart attacks, which can be visualized through colored scatter plots.\n",
        "\n",
        "4. Detection of email spam, based on the frequency of words within the messages, using 57 features.\n",
        "\n",
        "5. Identification of numbers in handwritten zip codes, which involves pattern recognition.\n",
        "\n",
        "6. Classification of tissue samples into cancer classes based on gene expression profiles, utilizing heat maps for visualization.\n",
        "\n",
        "7. Establishing the relationship between salary and demographic variables like income (wage) versus age, year, and education level, employing regression models.\n",
        "\n",
        "8. Classification of pixels in LANDSAT images by their usage, using nearest neighbor methods.\n",
        "\n",
        "\n",
        "\n",
        "#### Supervised and Unsupervised Learning\n",
        "\n",
        "Two important types: supervised and unsupervised learning.\n",
        "There is even more, e.g., semi-supervised learning.\n",
        "\n",
        "##### Starting point\n",
        "\n",
        "* Outcome measurement $Y$ (dependent variable, response, target).\n",
        "* Vector of $p$ predictor measurements $X$ (inputs, regressors, covariates, features, independent variables).\n",
        "* Training data $(x_1, y1), \\ldots ,(x_N, y_N)$. These are observations (examples, instances) of these measurements.\n",
        "\n",
        "In the *regression* problem, $Y$ is quantitative (e.g., price, blood pressure).\n",
        "In the *classification* problem, $Y$ takes values in a finite, unordered set (e.g., survived/died, digit 0-9, cancer class of tissue sample).\n",
        "\n",
        "##### Philosophy\n",
        "\n",
        "It is important to understand the ideas behind the various techniques,\n",
        "in order to know how and when to use them.\n",
        "One has to understand the simpler methods first, in order to grasp the more sophisticated ones.\n",
        "It is important to accurately assess the performance of a method,\n",
        "to know how well or how badly it is working (simpler methods often perform as well as fancier ones!)\n",
        "This is an exciting research area, having important applications in science, industry and finance.\n",
        "Statistical learning is a fundamental ingredient in the training of a modern data scientist.\n",
        "\n",
        "\n",
        "## Basics\n",
        "\n",
        "### Histograms\n",
        "\n",
        "Creating a histogram and calculating the probabilities from a dataset can be approached with scientific precision\n",
        "\n",
        "1. Data Collection: Obtain the dataset you wish to analyze. This dataset could represent any quantitative measure, such to examine its distribution.\n",
        "\n",
        "2. Decide on the Number of Bins: The number of bins influences the histogram's granularity. There are several statistical rules to determine an optimal number of bins:\n",
        "   * Square-root rule: suggests using the square root of the number of data points as the number of bins.\n",
        "   * Sturges' formula: $k = 1 + 3.322 \\log_{10}(n)$, where $n$ is the number of data points and $k$ is the suggested number of bins.\n",
        "   * Freedman-Diaconis rule: uses the interquartile range (IQR) and the cube root of the number of data points $n$ to calculate bin width as $2 \\dfrac{IQR}{n^{1/3}}$.\n",
        "\n",
        "3. Determine Range and Bin Width: Calculate the range of data by subtracting the minimum data point value from the maximum. Divide this range by the number of bins to determine the width of each bin.\n",
        "\n",
        "4. Allocate Data Points to Bins: Iterate through the data, sorting each data point into the appropriate bin based on its value.\n",
        "\n",
        "5. Draw the Histogram: Use a histogram to visualize the frequency or relative frequency (probability) of data points within each bin.\n",
        "\n",
        "6. Calculate Probabilities: The relative frequency of data within each bin represents the probability of a randomly selected data point falling within that bin's range.\n",
        "\n",
        "Below is a Python script that demonstrates how to generate a histogram and compute probabilities using the `matplotlib` library for visualization and `numpy` for data manipulation.\n"
      ],
      "id": "55f80289"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-histogram\n",
        "#| fig-cap: Histogram with Probability Density\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Sample data: Randomly generated for demonstration\n",
        "data = np.random.normal(0, 1, 1000)  # 1000 data points with a normal distribution\n",
        "\n",
        "# Step 2: Decide on the number of bins\n",
        "num_bins = int(np.ceil(1 + 3.322 * np.log10(len(data))))  # Sturges' formula\n",
        "\n",
        "# Step 3: Determine range and bin width -- handled internally by matplotlib\n",
        "\n",
        "# Steps 4 & 5: Sort data into bins and draw the histogram\n",
        "fig, ax = plt.subplots()\n",
        "n, bins, patches = ax.hist(data, bins=num_bins, density=True, alpha=0.75, edgecolor='black')\n",
        "\n",
        "# Calculate probabilities (relative frequencies) manually, if needed\n",
        "bin_width = np.diff(bins)  # np.diff finds the difference between adjacent bin boundaries\n",
        "probabilities = n * bin_width  # n is already normalized to form a probability density if `density=True`\n",
        "\n",
        "# Adding labels and title for clarity\n",
        "ax.set_xlabel('Data Value')\n",
        "ax.set_ylabel('Probability Density')\n",
        "ax.set_title('Histogram with Probability Density')"
      ],
      "id": "fig-histogram",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i, prob in enumerate(probabilities):\n",
        "    print(f\"Bin {i+1} Probability: {prob:.4f}\")\n",
        "\n",
        "# Ensure probabilities sum to 1 (or very close, due to floating-point arithmetic)\n",
        "print(f\"Sum of probabilities: {np.sum(probabilities)}\")"
      ],
      "id": "e0931811",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code segment goes through the necessary steps to generate a histogram and calculate probabilities for a synthetic dataset. It demonstrates important scientific and computational practices including binning, visualization, and probability calculation in Python.\n",
        "\n",
        "Key Points:\n",
        "- The histogram represents the distribution of data, with the histogram's bins outlining the data's spread and density.\n",
        "- The option `density=True` in `ax.hist()` normalizes the histogram so that the total area under the histogram sums to 1, thereby converting frequencies to probability densities.\n",
        "- The choice of bin number and width has a significant influence on the histogram's shape and the insights that can be drawn from it, highlighting the importance of selecting appropriate binning strategies based on the dataset's characteristics and the analysis objectives.\n",
        "\n",
        "\n",
        "### Probability Distributions\n",
        "\n",
        "What happens when we use smaller bins in a histogram? The histogram becomes more detailed, revealing the distribution of data points with greater precision. However, as the bin size decreases, the number of data points within each bin may decrease, leading to sparse or empty bins. This sparsity can make it challenging to estimate probabilities accurately, especially for data points that fall within these empty bins.\n",
        "\n",
        "Advantages, when using a probability distribution, include:\n",
        "\n",
        "  * Blanks can be filled\n",
        "  * Probabilities can be calculated\n",
        "  * Parameters are sufficiemnt to describe the distribution, e.g., mean and variance for the normal distribution\n",
        "\n",
        "Probability distributions offer a powerful solution to the challenges posed by limited data in estimating probabilities. When data is scarce, constructing a histogram to determine the probability of certain outcomes can lead to inaccurate or unreliable results due to the lack of detail in the dataset. However, collecting vast amounts of data to populate a histogram for more precise estimates can often be impractical, time-consuming, and expensive.\n",
        "\n",
        "A probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes for an experiment. It is a more efficient approach to understanding the likelihood of various outcomes than relying solely on extensive data collection. For continuous data, this is often represented graphically by a smooth curve.\n",
        "\n",
        "#### The Normal Distribution: A Common Example\n",
        "\n",
        "A commonly encountered probability distribution is the normal distribution, known for its characteristic bell-shaped curve. This curve represents how the values of a variable are distributed: most of the observations cluster around the mean (or center) of the distribution, with frequencies gradually decreasing as values move away from the mean.\n",
        "\n",
        "The normal distribution is particularly useful because of its defined mathematical properties. It is determined entirely by its mean (mu, $\\mu$) and its standard deviation (sigma, $\\sigma$). The area under the curve represents probability, making it possible to calculate the likelihood of a random variable falling within a specific range.\n",
        "\n",
        "#### Practical Example: Estimating Probabilities\n",
        "\n",
        "Consider we are interested in the heights of adults in a population. Instead of measuring the height of every adult (which would be impractical), we can use the normal distribution to estimate the probability of adults' heights falling within certain intervals, assuming we know the mean and standard deviation of the heights.\n"
      ],
      "id": "92a58a95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-normal-distribution\n",
        "#| fig-cap: Normal Distribution Curve with Highlighted Probability Area. 95 percent of the data falls within two standard deviations of the mean.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "mu = 170  # e.g., mu height of adults in cm\n",
        "sd = 10  # e.g., standard deviation of heights in cm\n",
        "heights = np.linspace(mu - 3*sd, mu + 3*sd, 1000)\n",
        "# Calculate the probability density function for the normal distribution\n",
        "pdf = norm.pdf(heights, mu, sd)\n",
        "# Plot the normal distribution curve\n",
        "plt.plot(heights, pdf, color='blue', linewidth=2)\n",
        "plt.fill_between(heights, pdf, where=(heights >= mu - 2 * sd) & (heights <= mu + 2*sd), color='grey', alpha=0.5)\n",
        "plt.xlabel('Height (cm)')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.show()"
      ],
      "id": "fig-normal-distribution",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python code snippet generates a plot of the normal distribution for adult heights, with a mean of 170 cm and a standard deviation of 10 cm. It visually approximates a histogram with a blue bell-shaped curve, and highlights (in grey) the area under the curve between $\\mu \\pm 2 \\times \\sigma$.\n",
        "This area corresponds to the probability of randomly selecting an individual whose height falls within this range. \n",
        "\n",
        "By using the area under the curve, we can efficiently estimate probabilities without needing to collect and analyze a vast amount of data. This method not only saves time and resources but also provides a clear and intuitive way to understand and communicate statistical probabilities.\n",
        "\n",
        "### Discrete Distributions\n",
        "\n",
        "Discrete probability distributions are essential tools in statistics, providing a mathematical foundation to model and analyze situations with discrete outcomes. \n",
        "Histograms, which can be seen as discrete distributions with data organized into bins, offer a way to visualize and estimate probabilities based on the collected data. However, they come with limitations, especially when data is scarce or when we encounter gaps in the data (blank spaces in histograms). These gaps can make it challenging to accurately estimate probabilities.\n",
        "\n",
        "A more efficient approach, especially for discrete data, is to use mathematical equations---particularly those defining discrete probability distributions---to calculate probabilities directly, thus bypassing the intricacies of data collection and histogram interpretation.\n",
        "\n",
        "#### Bernoulli Distribution\n",
        "\n",
        "The Bernoulli distribution, named after Swiss scientist Jacob Bernoulli, is a discrete probability distribution, which takes value $1$ with success probability $p$ and value $0$ with failure probability $q = 1-p$. So if $X$ is a random variable with this distribution, we have:\n",
        "$$\n",
        "P(X=1) = 1-P(X=0) = p = 1-q.\n",
        "$$\n",
        "\n",
        "#### Binomial Distribution\n",
        "\n",
        "The Binomial Distribution is a prime example of a discrete probability distribution that is particularly useful for binary outcomes (e.g., success/failure, yes/no, pumpkin pie/blueberry pie). It leverages simple mathematical principles to calculate the probability of observing a specific number of successes (preferred outcomes) in a fixed number of trials, given the probability of success in each trial.\n",
        "\n",
        "#### An Illustrative Example: Pie Preference\n",
        "\n",
        "Consider a scenario from \"StatLand\" where 70% of people prefer pumpkin pie over blueberry pie. The question is: What is the probability that, out of three people asked, the first two prefer pumpkin pie and the third prefers blueberry pie?\n",
        "\n",
        "Using the concept of the Binomial Distribution, the probability of such an outcome can be calculated without the need to layout every possible combination by hand. This process not only simplifies calculations but also provides a clear and precise method to determine probabilities in scenarios involving discrete choices.\n",
        "We will use Python to calculate the probability of observing exactly two out of three people prefer pumpkin pie, given the 70% preference rate:\n"
      ],
      "id": "8d6e71ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import binom\n",
        "n = 3  # Number of trials (people asked)\n",
        "p = 0.7  # Probability of success (preferring pumpkin pie)\n",
        "x = 2  # Number of successes (people preferring pumpkin pie)\n",
        "# Probability calculation using Binomial Distribution\n",
        "prob = binom.pmf(x, n, p)\n",
        "print(f\"The probability that exactly 2 out of 3 people prefer pumpkin pie is: {prob:.3f}\")"
      ],
      "id": "ea7c32ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code uses the `binom.pmf()` function from `scipy.stats` to calculate the probability mass function (PMF) of observing exactly `x` successes in `n` trials, where each trial has a success probability of `p`.\n",
        "\n",
        "A Binomial random variable is the sum of $n$ independent, identically distributed Bernoulli random variables, each with probability $p$ of success. \n",
        "We may indicate a random variable $X$ with Bernoulli distribution using the notation $X \\sim \\mathrm{Bi}(1,\\theta)$. Then, the notation for the Binomial is\n",
        "$X \\sim \\mathrm{Bi}(n,\\theta)$. Its probability and distribution functions are, respectively,\n",
        "$$\n",
        "p_X(x) = {n\\choose x}\\theta^x(1-\\theta)^{n-x}, \\qquad F_X(x) = \\Pr\\{X \\le x\\} = \\sum_{i=0}^{x} {n\\choose i}\\theta^i(1-\\theta)^{n-i}.\n",
        "$$\n",
        "\n",
        "The mean of the binomial distribution is $\\text{E}[X] = n\\theta$. \n",
        "The variance of the distribution is $\\text{Var}[X] = n\\theta(1-\\theta)$ (see next section).\n",
        "\n",
        "A process consists of a sequence of $n$ independent trials, i.e., the outcome of each trial does not depend on the outcome of previous trials.\n",
        "The outcome of each trial is either a success or a failure. The probability of success is denoted as $p$, and $p$ is constant for\n",
        "each trial. Coin tossing is a classical example for this setting.\n",
        "\n",
        "The binomial distribution is a statistical distribution giving the probability of obtaining a specified number of successes in a binomial experiment;\n",
        "written Binomial(n, p), where $n$ is the number of trials, and $p$ the probability of success in each.\n",
        "\n",
        "::: {#def-binom}\n",
        "\n",
        "#### Binomial Distribution\n",
        "\n",
        "The binomial distribution with parameters $n$ and $p$,\n",
        "where $n$ is the number of trials, and $p$ the probability of success in each, is\n",
        "\\begin{equation}\n",
        "p(x) = { n \\choose k } p^x(1-p)^{n-x} \\qquad x = 0,1, \\ldots, n.\n",
        "\\end{equation}\n",
        "The mean $\\mu$ and the variance $\\sigma^2$ of the binomial distribution are\n",
        "\\begin{equation}\n",
        "\\mu = np\n",
        "\\end{equation}\n",
        "and\n",
        "\\begin{equation}\n",
        "\\sigma^2 = np(1-p).\n",
        "\\end{equation}\n",
        "\n",
        ":::\n",
        "\n",
        "Note, the Bernoulli distribution is simply Binomial(1,p).\n",
        "\n",
        "##  Continuous Distributions\n",
        "Our considerations regarding probability distributions, expectations, and standard deviations will be extended from discrete distributions to  continuous distributions. One simple example of a continuous distribution is the uniform distribution. Continuous distributions are defined by probability\n",
        " density functions.\n",
        "\n",
        "### Distribution functions: PDFs and CDFs\n",
        "\n",
        "The density for a continuous distribution is a measure of the relative probability of \"getting a value close to $x$.\" \n",
        "Probability density functions $f$ and cumulative distribution function $F$  are related as follows.\n",
        "\\begin{equation}\n",
        "f(x) = \\frac{d}{dx} F(x)\n",
        "\\end{equation}\n",
        "\n",
        "### Expectation (Continuous)\n",
        "\n",
        "::: {#def-expectation}\n",
        "\n",
        "#### Expectation (Continuous)\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{E}(X) = \\int_{-\\infty}^\\infty x f(x) \\, dx\n",
        "  \\end{equation}\n",
        ":::\n",
        "\n",
        "### Variance and Standard Deviation (Continuous)\n",
        "\n",
        "\n",
        "::: {#def-variance}\n",
        "\n",
        "#### Variance (Continuous)\n",
        "\n",
        "Variance can be calculated with $\\text{E}(X)$ and\n",
        "\\begin{equation}\n",
        "  \\text{E}(X^2) = \\int_{-\\infty}^\\infty x^2 f(x) \\, dx\n",
        "\\end{equation}\n",
        "  as\n",
        "\\begin{equation*}\n",
        "  \\text{Var}(X) = \\text{E}(X^2) - [ E(X)]^2.\n",
        "  \\end{equation*}\n",
        "  \\hfill $\\Box$\n",
        ":::\n",
        "\n",
        "::: {#def-standard-deviation}\n",
        "\n",
        "#### Standard Deviation (Continuous)\n",
        "\n",
        "Standard deviation can be calculated \n",
        "  as\n",
        "  \\begin{equation*}\n",
        "  \\text{sd}(X) = \\sqrt{\\text{Var}(X)}.\n",
        "  \\end{equation*}\n",
        "  \\hfill $\\Box$\n",
        ":::\n",
        "\n",
        "### Uniform Distribution\n",
        "\n",
        "This variable is defined in the interval $[a,b]$. We write it as $X \\sim U[a,b]$. Its\n",
        "density and cumulative distribution functions are, respectively,\n",
        "$$\n",
        "f_X(x) = \\frac{I_{[a,b]}(x)}{b-a}, \t\\quad\\quad F_X(x) = \\frac{1}{b-a}\\int\\limits_{-\\infty}\\limits^x I_{[a,b]}(t) \\mathrm{d}t = \\frac{x-a}{b-a},\n",
        "$$\n",
        "where $I_{[a,b]}(\\cdot)$ is the indicator function of the interval\n",
        "$[a,b]$. \n",
        "Note that, if we set $a=0$ and $b=1$,\n",
        "we obtain $F_X(x) = x$, $x$ $\\in$ $[0,1]$.\n",
        "\n",
        "A typical example is the following: the cdf of a continuous r.v. is uniformly distributed in\n",
        "$[0,1]$. The proof of this statement is as follows: For $u$ $\\in$\n",
        "$[0,1]$, we have\n",
        "\\begin{eqnarray*}\n",
        "\\Pr\\{F_X(X) \\leq u\\} &=& \\Pr\\{F_X^{-1}(F_X(X)) \\leq F_X^{-1}(u)\\} = \\Pr\\{X \\leq F_X^{-1}(u)\\} \\\\\n",
        "\t                  &=& F_X(F_X^{-1}(u)) = u.\t\t\n",
        "\\end{eqnarray*}\n",
        "This means that, when $X$ is continuous, there is a one-to-one relationship (given by the cdf) between $x$ $\\in$ $D_X$ and $u$ $\\in$ $[0,1]$.\n",
        "\n",
        "The \\emph{uniform distribution} has a constant density over a specified interval, say $[a,b]$.\n",
        " The uniform $U(a,b)$ distribution has density\n",
        " \\begin{equation}\n",
        " f(x) = \n",
        " \\left\\{\n",
        "  \\begin{array}{ll}\n",
        "  1/(b-a) & \\textrm{ if } a < x < b,\\\\\n",
        "  0 & \\textrm{ otherwise}\n",
        "  \\end{array}\n",
        "  \\right.\n",
        "  \\end{equation}\n",
        "\n",
        "### Normal Distribution\n",
        "  \n",
        "::: {#def-normal}\n",
        "\n",
        "#### Normal Distribution\n",
        "\n",
        "This variable is defined on the support $D_X = \\mathbb{R}$ and its density\n",
        "function is given by\n",
        "$$\n",
        "f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left \\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right \\}.\n",
        "$$\n",
        "The density function is identified by the pair of parameters\n",
        "$(\\mu,\\sigma^2)$, where $\\mu$ $\\in$ $\\mathbb{R}$ is the mean (or location\n",
        "parameter) and $\\sigma^2 > 0$ is the variance (or dispersion parameter)\n",
        "of $X$. \n",
        "\\hfill $\\Box$\n",
        ":::\n",
        "\n",
        "The density function is symmetric around $\\mu$. The normal distribution belongs to the location-scale family\n",
        "distributions. This means that, if $Z \\sim N(0,1)$ (read, $Z$ has a standard normal distribution; i.e., with $\\mu=0$ and $\\sigma^2=1$), and we \n",
        "consider the linear transformation $X = \\mu + \\sigma Z$, then $X \\sim N(\\mu,\\sigma^2)$ (read, $X$ has a normal distribution with mean\n",
        "$\\mu$ and variance $\\sigma^2$). This means that one can obtain the probability of any interval $(-\\infty,x]$, $x$ $\\in$ $R$ for any normal distribution (i.e., for any pair of the parameters $\\mu$ and $\\sigma$) once the quantiles of the standard normal distribution are known. Indeed\n",
        "\\begin{eqnarray*}\n",
        "F_X(x) &=& \\Pr\\left\\{X \\leq x \\right\\} = \\Pr\\left\\{\\frac{X-\\mu}{\\sigma} \\leq \\frac{x-\\mu}{\\sigma} \\right\\} \\\\\n",
        "           &=& \\Pr\\left\\{Z \\leq \\frac{x-\\mu}{\\sigma}\\right\\}  = F_Z\\left(\\frac{x-\\mu}{\\sigma}\\right)\t\\qquad x \\in \\mathbb{R}.\n",
        "\\end{eqnarray*}\n",
        "The quantiles of the standard normal\n",
        "distribution are available in any statistical program. The density and\n",
        "cumulative distribution function of the standard normal r.v.~at point\n",
        "$x$ are usually denoted by the symbols $\\phi(x)$ and $\\Phi(x)$.\n",
        "\n",
        "The standard normal distribution is based on the \n",
        "\\emph{standard normal density function}\n",
        "$$\n",
        " \\varphi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left(- \\frac{z^2}{2} \\right).\n",
        "$$ {#eq-standardization}\n",
        "\n",
        "An important application of the standardization introduced in @eq-standardization reads as follows.\n",
        "In case the distribution of $X$ is approximately normal, the distribution of X^{*} is approximately standard normal.\n",
        "That is\n",
        "\\begin{equation*}\n",
        "  P(X\\leq b) = P( \\frac{X-\\mu}{\\sigma} \\leq \\frac{b-\\mu}{\\sigma}) = P(X^{*} \\leq \\frac{b-\\mu}{\\sigma})\n",
        "\\end{equation*}\n",
        "The probability $P(X\\leq b)$ can be approximated by $\\Phi(\\frac{b-\\mu}{\\sigma})$,\n",
        "where $\\Phi$ is the standard normal cumulative distribution function. \n",
        "  \n",
        "If $X$ is a normal random variable with mean $\\mu$ and variance $\\sigma^2$,\n",
        "  i.e., $X \\sim \\cal{N} (\\mu, \\sigma^2)$, then\n",
        "  \\begin{equation}\n",
        "  X = \\mu + \\sigma Z \\textrm{ where } Z \\sim \\cal{N}(0,1).\n",
        "  \\end{equation}\n",
        "  \n",
        "  \n",
        "If $Z \\sim \\cal{N}(0,1)$ and $X\\sim \\cal{N}(\\mu, \\sigma^2)$, then \n",
        "  \\begin{equation*}\n",
        "  X = \\mu + \\sigma Z. \n",
        "\\end{equation*}\n",
        "  \n",
        " The probability of getting a value in a particular interval is the area\n",
        " under the corresponding part of the curve.\n",
        " Consider the density function of the normal distribution. It can \n",
        " be plotted using the following commands.\n",
        " The result is shown in @fig-normal-density.\n"
      ],
      "id": "4fac9f15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-normal-density\n",
        "#| fig-cap: Normal Distribution Density Function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "x = np.arange(-4, 4, 0.1)\n",
        "# Calculating the normal distribution's density function values for each point in x\n",
        "y = norm.pdf(x, 0, 1)\n",
        "plt.plot(x, y, linestyle='-', linewidth=2)\n",
        "plt.title('Normal Distribution')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "fig-normal-density",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The \\emph{cumulative distribution function} (CDF) describes the probability\n",
        "of \"hitting\" $x$ or less in a given distribution.\n",
        "We consider the CDF function of the normal distribution. It can be plotted using the following commands.\n",
        "The result is shown in @fig-normal-cdf.\n"
      ],
      "id": "99ccc14e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-normal-cdf\n",
        "#| fig-cap: Normal Distribution Cumulative Distribution Function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Generating a sequence of numbers from -4 to 4 with 0.1 intervals\n",
        "x = np.arange(-4, 4, 0.1)\n",
        "\n",
        "# Calculating the cumulative distribution function value of the normal distribution for each point in x\n",
        "y = norm.cdf(x, 0, 1)  # mean=0, stddev=1\n",
        "\n",
        "# Plotting the results. The equivalent of 'type=\"l\"' in R (line plot) becomes the default plot type in matplotlib.\n",
        "plt.plot(x, y, linestyle='-', linewidth=2)\n",
        "plt.title('Normal Distribution CDF')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "fig-normal-cdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Mean, the Median, and the Mode\n",
        "\n",
        "### The Exponential Distribution\n",
        "\n",
        "The exponential distribution is a continuous probability distribution that describes the time between events in a Poisson process, where events occur continuously and independently at a constant average rate. It is characterized by a single parameter, the rate parameter $\\lambda$, which represents the average number of events per unit time.\n",
        "\n",
        "### Population and Estimated Parameters\n",
        "\n",
        "### Calculating the Mean, Variance, and Standard Deviation\n",
        "\n",
        "### What is a Mathematical Model?\n",
        "\n",
        "### Sampling from a Distribution\n",
        "\n",
        "### Hypothesis Testing and the Null Hypothesis\n",
        "\n",
        "### Alternative Hypotheses\n",
        "\n",
        "### p-values: What They Are and How to Interpret Them\n",
        "\n",
        "### How to Calculate p-values\n",
        "\n",
        "### p-hacking: What It Is and How to Avoid It\n",
        "\n",
        "### Covariance\n",
        "\n",
        "### Pearson's Correlation\n",
        "\n",
        "### Boxplots\n",
        "\n",
        "### R-squared\n",
        "\n",
        "### The Main Ideas of Fitting a Line to Data\n",
        "\n",
        "### Linear Regression\n",
        "\n",
        "### Multiple Regression\n",
        "\n",
        "\n",
        "## Supervised Learning\n",
        "\n",
        "Objectives of supervised learning: On the basis of the training data we would like to:\n",
        "\n",
        "* Accurately predict unseen test cases.\n",
        "* Understand which inputs affect the outcome, and how.\n",
        "* Assess the quality of our predictions and inferences.\n",
        "\n",
        "Note: Supervised means $Y$ is known.\n",
        "\n",
        "::: {#exr-starting-point}\n",
        "\n",
        "* Do children learn supervised?\n",
        "* When do you learn supervised?\n",
        "* Can learning be unsupervised?\n",
        ":::\n",
        "\n",
        "##### Unsupervised Learning\n",
        "\n",
        "No outcome variable, just a set of predictors (features) measured on a set of samples.\n",
        "The objective is more fuzzy---find groups of samples that behave similarly, \n",
        "find features that behave similarly, find linear combinations of features with the most variation.\n",
        "It is difficult to know how well your are doing.\n",
        "Unsupervised learning different from supervised learning, \n",
        "but can be useful as a pre-processing step for supervised learning.\n",
        "Clustering and principle component analysis are important techniques.\n",
        "\n",
        "Unsupervised: $Y$ is unknown, there is no $Y$, no trainer, no teacher, but:\n",
        "distances between the inputs values (features).\n",
        "A distance (or similarity) measure is necessary.\n",
        "\n",
        "##### Statistical Learning\n",
        "\n",
        "We consider supervised learning first.\n",
        "\n",
        "![Sales as a function of TV, radio and newspaper. Taken from @Jame14a ](./figures_static/0201.png){width=100% #fig-0201}\n",
        "\n",
        "\n",
        "Sales figures from a marketing campaign, see @fig-0201.\n",
        "Trend shown using regression. First seems to be stronger than the third.\n",
        "\n",
        "Can we predict $Y$ = Sales using these three?\n",
        "Perhaps we can do better using a model\n",
        "$$\n",
        "Y = Sales \\approx  f(X_1 = TV,  X_2 = Radio, X_3= Newspaper)\n",
        "$$\n",
        "modeling the joint relationsship.\n",
        "\n",
        "Here Sales is a response or target that we wish to predict.\n",
        "We generically refer to the response as $Y$.\n",
        "TV is a feature, or input, or predictor; we name it $X_1$.\n",
        "Likewise name Radio as $X_2$, and so on.\n",
        "We can refer to the input vector collectively as\n",
        "$$\n",
        "X =\n",
        "\\begin{pmatrix}\n",
        "X_1\\\\\n",
        "X_2\\\\\n",
        "X_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Now we write our model as\n",
        "$$\n",
        "Y = f(X) + \\epsilon\n",
        "$$\n",
        "where $\\epsilon$ captures measurement errors and other discrepancies.\n",
        "\n",
        "What is $f$ good for? With a good $f$ we can make predictions of $Y$ at new points $X = x$.\n",
        "We can understand which components of\n",
        "$X = (X_1, X_2, \\ldots X_p)$ are important in explaining $Y$,\n",
        "and which are irrelevant.\n",
        "\n",
        "For example, Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n",
        "Depending on the complexity of $f$,\n",
        "we may be able to understand how each component $X_j$ of $X$ affects $Y$.\n",
        "\n",
        "\n",
        "### Statistical Learning and Regression\n",
        "\n",
        "#### Regression Function\n",
        "\n",
        "![Scatter plot of 2000 points (population). What is a good function $f$? There are many function values at $X=4$. A function can return only one value. We can take the mean from these values as a return value. Taken from @Jame14a ](./figures_static/0202a.png){width=100% #fig-0202a}\n",
        "\n",
        "Consider @fig-0202a. Is there an ideal $f(X)$? In particular, what is a good value for $f(X)$ at any selected value of $X$, say $X = 4$? There can be many $Y$ values at $X=4$.\n",
        "A good value is\n",
        "$$\n",
        "f(4) = E(Y |X = 4).\n",
        "$$\n",
        "\n",
        "$E(Y |X = 4)$  means **expected value** (average) of $Y$ given $X = 4$.\n",
        "\n",
        "\n",
        "The ideal $f(x) = E(Y |X = x)$ is called the **regression function**.\n",
        "Read: The regression function gives the conditional expectation of $Y$ given $X$.\n",
        "\n",
        "The regression function $f(x)$ is also defined for the vector $X$;\n",
        "e.g.,\n",
        "$f(x) = f(x_1, x_2, x_3) = E(Y | X_1 =x_1, X_2 =x_2, X_3 =x_3).$\n",
        "\n",
        "::: {def-optimal-predictor}\n",
        "\n",
        "### Optimal Predictor\n",
        "\n",
        "The regression function is the **ideal** or **optimal predictor** of $Y$ with regard to mean-squared prediction error: It means that\n",
        "$f(x) = E(Y | X = x)$ is the function that minimizes\n",
        "$$\n",
        "E[(Y - g(X))^2|X = x]\n",
        "$$\n",
        "over all functions $g$ at all points $X = x$.\n",
        "\n",
        ":::\n",
        "\n",
        "#### Residuals, Reducible and Irreducible Error\n",
        "\n",
        "At each point $X$ we make mistakes:\n",
        "$$\n",
        "\\epsilon = Y-f(x)\n",
        "$$\n",
        "is the **residual**.\n",
        "Even if we knew $f(x)$, we would still make errors in prediction,\n",
        "since at each $X=x$ there is typically a distribution of possible $Y$ values as\n",
        "is illustrated in @fig-0202a.\n",
        "\n",
        "For any estimate $\\hat{f}(x)$ of $f(x)$, we have\n",
        "$$\n",
        "E\\left[ ( Y - \\hat{f}(X))^2 | X = x\\right] = \\left[ f(x) - \\hat{f}(x) \\right]^2 + \\text{var}(\\epsilon),\n",
        "$$\n",
        "and\n",
        "$\\left[ f(x) - \\hat{f}(x) \\right]^2$ is the **reducible** error, because it depends on the model (changing the model $f$ might reduce this error), and\n",
        "$\\text{var}(\\epsilon)$ is the **irreducible** error.\n",
        "\n",
        "#### Local Regression (Smoothing)\n",
        "\n",
        " Typically we have few if any data points with $X = 4$ exactly. So we cannot compute $E(Y |X = x)$!\n",
        " Idea: Relax the definition and let\n",
        " $$\n",
        " \\hat{f}(x)=  Ave(Y|X \\in  \\cal{N}(x)),\n",
        " $$\n",
        "where $\\cal{N} (x)$ is some neighborhood of $x$, see @fig-0203a.\n",
        "\n",
        "![Relaxing the definition. There is no $Y$ value at $X=4$. Taken from @Jame14a ](./figures_static/0203a.png){width=70% #fig-0203a}\n",
        "\n",
        "Nearest neighbor averaging can be pretty good for small $p$, i.e.,  $p \\leq 4$ and large-ish $N$.\n",
        "We will discuss smoother versions, such as kernel and spline smoothing later in the course.\n",
        "\n",
        "### Curse of Dimensionality and Parametric Models\n",
        "\n",
        "![A 10\\% neighborhood in high dimensions need no longer be local. Left: Values of two variables $x_1$ and $x_2$, uniformly distributed. Form two 10\\% neighborhoods: (a) the first is just involving $x_1$ ignoring $x_2$. (b) is the neighborhood in two dimension. Notice that the radius of the circle is much larger than the lenght of the interval in one dimension. Right: radius plotted against fraction of the volume. In 10 dim, you have to break out the interval $[-1;+1]$ to get 10\\% of the data. Taken from @Jame14a ](./figures_static/0204a.png){width=100% #fig-0204a}\n",
        "\n",
        "Local, e.g., nearest neighbor, methods can be lousy when $p$ is large.\n",
        "Reason: **the curse of dimensionality**, i.e., nearest neighbors tend to be far away in high dimensions.\n",
        "We need to get a reasonable fraction of the $N$ values of $y_i$ to average to bring the variance down---e.g., 10\\%.\n",
        "A 10\\% neighborhood in high dimensions need no longer be local,\n",
        "so we lose the spirit of estimating $E(Y |X = x)$ by local averaging, \n",
        "see @fig-0204a.\n",
        "If the curse of dimensionality does not exist, nearest neighbor models would be\n",
        "perfect prediction models.\n",
        "\n",
        "We will use structured (parametric) models to deal with the curse of dimensionality.\n",
        "The linear model is an important example of a parametric model:\n",
        "$$\n",
        "f_L(X) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p.\n",
        "$$\n",
        "A linear model is specified in terms of $p + 1$ parameters\n",
        "$ \\beta_1, \\beta_2, \\ldots, \\beta_p$. We estimate the parameters by fitting the model to \\emph{training data}. Although it is almost never correct,\n",
        "a linear model often serves as a good and interpretable approximation to the\n",
        "unknown true function $f(X)$.\n",
        "\n",
        "\n",
        "The linear model is avoiding the curse of dimensionality, because it is not relying on any local properties.\n",
        "Linear models belong to the class of \\emph{model-based} approaches: they replace\n",
        "the problem of estimating $f$ with estimating a fixed set of coefficients $\\beta_i$, with $i=1,2, \\ldots, p$.\n",
        "\n",
        "![A linear model $\\hat{f}_L$ gives a reasonable fit. Taken from @Jame14a ](./figures_static/0301a.png){width=70% #fig-0301a}\n",
        "\n",
        "![A quadratic model $\\hat{f}_Q$ fits slightly better. Taken from @Jame14a ](./figures_static/0302a.png){width=70% #fig-0302a}\n",
        "\n",
        "\n",
        "A linear model\n",
        "$$\n",
        "\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\n",
        "$$\n",
        "gives a reasonable fit, see @fig-0301a.\n",
        "A quadratic  model\n",
        "$$\n",
        "\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\hat{\\beta}_2 X^2\n",
        "$$\n",
        "gives a slightly improved fit, see @fig-0302a.\n",
        "\n",
        "@fig-0203 shows a simulated example.\n",
        "Red points are simulated values for income from the model\n",
        "$$\n",
        "income = f(education, seniority) + \\epsilon\n",
        "$$\n",
        "$f$ is the blue surface.\n",
        "\n",
        "![The true model. Red points are simulated values for income from the model, $f$ is the blue surface. Taken from @Jame14a ](./figures_static/0203.png){width=70% #fig-0203}\n",
        "\n",
        "![Linear regression fit to the simulated data (red points). Taken from @Jame14a ](./figures_static/0204.png){width=70% #fig-0204}\n",
        "\n",
        "The linear regression model \n",
        "$$\n",
        "\\hat{f}(education, seniority) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times education +\n",
        " \\hat{\\beta}_2 \\times seniority\n",
        "$$\n",
        "captures the important information. But it does not capture everything.\n",
        "More flexible regression model\n",
        "$$\n",
        "\\hat{f}_S (education, seniority)\n",
        "$$\n",
        "fit to the simulated data.\n",
        "Here we use a technique called a **thin-plate spline** to fit a flexible surface.\n",
        "Even more flexible spline regression model\n",
        "$$\n",
        "\\hat{f}_S (education, seniority)\n",
        "$$\n",
        "fit to the simulated data. Here the\n",
        "fitted model makes no errors on the training data! Also known as **overfitting**.\n",
        "\n",
        "![Thin-plate spline models $\\hat{f}_S (education, seniority)$ fitted to the model from @fig-0203. Taken from @Jame14a ](./figures_static/0205.png){width=70% #fig-0205}\n",
        "\n",
        "![Thin-plate spline models $\\hat{f}_S (education, seniority)$ fitted to the model from @fig-0203. The model makes no errors on the training data (overfitting). Taken from @Jame14a ](./figures_static/0206.png){width=70% #fig-0206}\n",
        "\n",
        "#### Trade-offs\n",
        "\n",
        "* Prediction accuracy versus interpretability: Linear models are easy to interpret; thin-plate splines are not.\n",
        "* Good fit versus over-fit or under-fit: How do we know when the fit is just right?\n",
        "* Parsimony (Occam's razor) versus black-box: We often prefer a simpler model involving fewer variables over a black-box predictor involving them all.\n",
        "\n",
        "The trad-offs are visualized in @fig-0207.\n",
        "\n",
        "![Interpretability versus flexibility. Flexibility corresponds with the number of model parameters. Taken from @Jame14a ](./figures_static/0207.png){width=70% #fig-0207}\n",
        "\n",
        "### Assessing Model Accuracy and Bias-Variance Trade-off\n",
        "\n",
        "![Black curve is truth. Red curve on right is $MSETe$, grey curve is $MSETr$. Orange, blue and green curves/squares correspond to fits of different flexibility. The dotted line represents the irreducible error, i.e., $var(\\epsilon)$. Taken from @Jame14a ](./figures_static/0303a.png){width=100% #fig-0303a}\n",
        "\n",
        "![Here, the truth is smoother. Black curve is truth. Red curve on right is $MSETe$, grey curve is $MSETr$. Orange, blue and green curves/squares correspond to fits of different flexibility. The dotted line represents the irreducible error, i.e., $var(\\epsilon)$. Taken from @Jame14a ](./figures_static/0210.png){width=100% #fig-0210}\n",
        "\n",
        "![Here the truth is wiggly and the noise is low, so the more flexible fits do the best. Black curve is truth. Red curve on right is $MSETe$, grey curve is $MSETr$. Orange, blue and green curves/squares correspond to fits of different flexibility. The dotted line represents the irreducible error, i.e., $var(\\epsilon)$. Taken from @Jame14a ](./figures_static/0211.png){width=100% #fig-0211}\n",
        "\n",
        "\n",
        "Suppose we fit a model $f(x)$ to some training data $Tr = \\{x_i, y_i \\}^N_1$, and we wish to see how well it performs.\n",
        "We could compute the average squared prediction error\n",
        "over $Tr$:\n",
        "$$\n",
        "MSE_{Tr} = Ave_{i \\in Tr}[y_i - \\hat{f}(x_i)]^2.\n",
        "$$\n",
        "This may be biased toward more overfit models.\n",
        "Instead we should, if possible, compute it using fresh **test data** $Te== \\{x_i, y_i \\}^N_1$:\n",
        "$$\n",
        "MSE_{Te} = Ave_{i \\in Te}[y_i - \\hat{f}(x_i)]^2.\n",
        "$$\n",
        "The red curve, which illustrated the test error, can be estimated by holding out some data to get the test-data set.\n",
        "\n",
        "#### Bias-Variance Trade-off\n",
        "\n",
        "Suppose we have fit a model $f(x)$ to some training data $Tr$, and let $(x_0, y_0)$ be a test observation drawn from the population.\n",
        "If the true model is\n",
        "$$\n",
        "Y = f(X) + \\epsilon  \\qquad \\text{ with } f(x) = E(Y|X=x),\n",
        "$$\n",
        "then\n",
        "$$\n",
        "E \\left( y_0 - \\hat{f}(x_0) \\right)^2 = \\text{var} (\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + \\text{var}(\\epsilon).\n",
        "$$ {#eq-biasvariance}\n",
        "\n",
        "\n",
        "Here, $\\text{var}(\\epsilon)$ is the irreducible error.\n",
        "The reducible error consists of two components:\n",
        "\n",
        "* $\\text{var} (\\hat{f}(x_0))$ is the variance that comes from different training sets. Different training sets result in different functions $\\hat{f}$.\n",
        "* $Bias(\\hat{f}(x_0)) = E[\\hat{f}(x_0)] - f(x_0)$.\n",
        "\n",
        "The expectation averages over the variability of $y_0$ as well as the variability in $Tr$.\n",
        "Note that \n",
        "$$\n",
        "Bias(\\hat{f}(x_0)) = E[\\hat{f}(x_0)] - f(x_0).\n",
        "$$\n",
        "Typically as the flexibility of $\\hat{f}$ increases,\n",
        "its variance increases (because the fits differ from training set to trainig set), and its bias decreases.\n",
        "So choosing the flexibility based on average test error amounts to a bias-variance trade-off, see @fig-0212.\n",
        "\n",
        "![Bias-variance trade-off for the three examples. Taken from @Jame14a ](./figures_static/0212.png){width=100% #fig-0212}\n",
        "\n",
        "If we add the two components (reducible and irreducible error), we get the MSE in @fig-0212 as can be seen in @eq-biasvariance.\n",
        "\n",
        "### Classification Problems and K-Nearest Neighbors\n",
        "\n",
        "\n",
        "In classification we have a qualitative response variable.\n",
        "\n",
        "![Classification. Taken from @Jame14a ](./figures_static/0218a.png){width=100% #fig-0218a}\n",
        "\n",
        "Here the response variable $Y$ is qualitative, e.g., email is one of $\\cal{C} = (spam, ham)$, where ham is good email,\n",
        "digit class is one of $\\cal{C} = \\{ 0, 1, \\ldots, 9 \\}$.\n",
        "Our goals are to:\n",
        "\n",
        "* Build a classifier $C(X)$ that assigns a class label from $\\cal{C}$ to a future unlabeled observation $X$.\n",
        "* Assess the uncertainty in each classification\n",
        "* Understand the roles of the different predictors among $X = (X_1,X_2, \\ldots, X_p)$.\n",
        "\n",
        "Simulation example depicted in@fig-0218a.\n",
        "$Y$ takes two values, zero and one, and $X$ has only one value.\n",
        "Big sample: each single vertical bar indicates an occurrance of a zero (orange) or one (blue) as a function of the $X$s.\n",
        "Black curve generated the data: it is the probability of generating a one. For high values of $X$, the probability of ones is increasing.\n",
        "What is an ideal classifier $C(X)$?\n",
        "\n",
        "Suppose the $K$ elements in $\\cal{C}$ are numbered $1,2,\\ldots, K$. Let\n",
        "$$\n",
        "p_k(x) = Pr(Y = k|X = x), k = 1,2,\\ldots,K.\n",
        "$$\n",
        "\n",
        "These are the **conditional class probabilities** at $x$; e.g. see little barplot at $x = 5$.\n",
        "Then the  **Bayes optimal classifier** at $x$ is \n",
        "$$\n",
        "C(x) = j \\qquad \\text{ if }  p_j(x) = \\max \\{p_1(x),p_2(x),\\ldots, p_K(x)\\}.\n",
        "$$\n",
        "At $x=5$ there is an 80\\% probability of one, and an 20\\% probability of a zero.\n",
        "So, we classify this point to the class with the highest probability, the majority\n",
        "class.\n",
        "\n",
        "Nearest-neighbor averaging can be used as before. This is illustrated in Fig.~\\ref{fig:0219a}.\n",
        "Here, we consider 100 points only.\n",
        "Nearest-neighbor averaging also breaks down as dimension grows. \n",
        "However, the impact on $\\hat{C}(x)$ is less than on $\\hat{p}_k (x)$, \n",
        "$k = 1, \\ldots, K$.\n",
        "\n",
        "![Classification. Taken from @Jame14a ](./figures_static/0219a.png){width=100% #fig-0219a}\n",
        "\n",
        "\n",
        "#### Classification: Some Details\n",
        "\n",
        "Average number of errors made to measure the performance. Typically we measure the performance of $\\hat{C}(x)$ using the **misclassification error rate**:\n",
        "$$\n",
        "Err_{Te} = Ave_{i\\in Te} I[y_i \\neq \\hat{C} (x_i) ].\n",
        "$$\n",
        "The Bayes classifier (using the true $p_k(x)$) has smallest error (in the population).\n",
        "\n",
        "\n",
        "### k-Nearest Neighbor Classification\n",
        "\n",
        "Consider k-nearest neighbors in two dimensions. Orange and blue dots label the true class memberships of the underlying  points in the 2-dim plane.\n",
        "Dotted line is the decision boundary, that is the contour with equal probability for both classes.\n",
        "\n",
        "Nearest-neighbor averaging in 2-dim. At any given point we want to classify, we spread out a little neighborhood,\n",
        "say $K=10$ points from the neighborhood and calulated the percentage of blue and orange. We assign the color with the highest probability to this point.\n",
        "If this is done for every point in the plane, we obtain the solid black curve as the esitmated decsion boundary.\n",
        "\n",
        "We can use $K=1$. This is the **nearest-neighbor classifier**.\n",
        "The decision boundary is piecewise linear. Islands occur. Approximation is rather noisy.\n",
        "\n",
        "$K=100$ leads to a smooth decision boundary. But gets uninteresting.\n",
        "\n",
        "![K-nearest neighbors in two dimensions. Taken from @Jame14a ](./figures_static/0213.png){width=70% #fig-0213}\n",
        "\n",
        "![K-nearest neighbors in two dimensions. Taken from @Jame14a ](./figures_static/0215.png){width=70% #fig-0215}\n",
        "\n",
        "![K-nearest neighbors in two dimensions. Taken from @Jame14a ](./figures_static/0216.png){width=70% #fig-0216}\n",
        "\n",
        "$K$ large means higher bias, so $1/K$ is chosen, because we go from low to high complexity on the $x$-error, see @fig-0217.\n",
        "Horizontal dotted line is the base error.\n",
        "\n",
        "![K-nearest neighbors classification error. Taken from @Jame14a ](./figures_static/0217.png){width=70% #fig-0217}\n",
        "\n",
        "\n",
        "::: {def-minkowski-distance}\n",
        "\n",
        "### Minkowski Distance\n",
        "\n",
        "The Minkowski distance of order $p$ (where $p$ is an integer) between two points\n",
        "$X=(x_1,x_2,\\ldots,x_n)\\text{ and }Y=(y_1,y_2,\\ldots,y_n) \\in \\mathbb{R}^n$\n",
        "is defined as:\n",
        "$$\n",
        "D \\left( X,Y \\right) = \\left( \\sum_{i=1}^n |x_i-y_i|^p \\right)^\\frac{1}{p}.\n",
        "$$\n",
        ":::\n",
        "\n",
        "### Unsuperivsed Learning: Classification\n",
        "\n",
        "#### k-Means Algorithm\n",
        "\n",
        "The $k$-means algorithm is an unsupervised learning algorithm that has a loose relationship to the $k$-nearest neighbor classifier.\n",
        "The $k$-means algorithm works as follows:\n",
        "\n",
        "* Step 1: Randomly choose $k$ centers. Assign points to cluster.\n",
        "* Step 2: Determine the distances of each data point to the centroids and re-assign each point to the closest cluster centroid based upon minimum distance\n",
        "* Step 3: Calculate cluster centroids again\n",
        "* Step 4: Repeat steps 2 and 3 until we reach global optima where no improvements are possible and no switching of data points from one cluster to other.\n",
        "\n",
        "The basic principle of the $k$-means algorithm is illustrated in @fig-kmeans1, @fig-kmeans2, @fig-kmeans3, and @fig-kmeans4.\n",
        "\n",
        "![k-means algorithm. Step 1. Randomly choose $k$ centers. Assign points to cluster. $k$ initial \\lq means\\rq (in this case $k=3$) are randomly generated within the data domain (shown in color). Attribution: I, Weston.pace, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons](./figures_static/kmeans1.png){width=70% #fig-kmeans1}\n",
        "\n",
        "![k-means algorithm. Step 2. $k$ clusters are created by associating every observation with the nearest mean. The partitions here represent the Voronoi diagram generated by the means. Attribution: I, Weston.pace, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons](./figures_static/kmeans2.png){width=70% #fig-kmeans2}\n",
        "\n",
        "![k-means algorithm. Step 3. The centroid of each of the $k$ clusters becomes the new mean. Attribution: I, Weston.pace, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons](./figures_static/kmeans3.png){width=70% #fig-kmeans3}\n",
        "\n",
        "![k-means algorithm. Step 4. Steps 2 and 3 are repeated until convergence has been reached. Attribution: I, Weston.pace, CC BY-SA 3.0 <http://creativecommons.org/licenses/by-sa/3.0/>, via Wikimedia Commons](./figures_static/kmeans4.png){width=70% #fig-kmeans4}"
      ],
      "id": "5ff5745e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}