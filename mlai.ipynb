{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---"
      ],
      "id": "cb307d69"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning and Artificial Intelligence\n",
        "\n",
        "\n",
        "## Videos\n",
        "\n",
        "\n",
        "###  June, 11th 2024\n",
        "* [Happy Halloween (Neural Networks Are Not Scary)](https://youtu.be/zxagGtF9MeU?si=4klFloENih3Pw7Ix)\n",
        "* [The Essential Main Ideas of Neural Networks](https://youtu.be/CqOfi41LfDw?si=tGfuObKzWonsNLZ1)\n",
        "\n",
        "### June, 18th 2024\n",
        "* [The Chain Rule](https://youtu.be/wl1myxrtQHQ?si=jcGIAhXkBLUvSqeV)\n",
        "* [Gradient Descent, Step-by-Step](https://youtu.be/sDv4f4s2SB8?si=V3XzPVbJNsbZSbNw)\n",
        "* [Neural Networks Pt. 2: Backpropagation Main Ideas](https://youtu.be/IN2XmBhILt4?si=Ldx6rk6mPplQjZZv)\n",
        "* [Backpropagation Details Pt. 1: Optimizing 3 parameters simultaneously.](https://youtu.be/iyn2zdALii8?si=yBsgPec1R1O55f9q)\n",
        "* [Backpropagation Details Pt. 2: Going bonkers with The Chain Rule](https://youtu.be/GKZoOHXGcLo?si=Ypv_EDEMMC--8Flj)\n",
        "* [Neural Networks Pt. 3: ReLU In Action!!!](https://youtu.be/68BZ5f7P94E?si=3hPUkdicWLwFzOGZ)\n",
        "* [Neural Networks Pt. 4: Multiple Inputs and Outputs](https://youtu.be/83LYR-1IcjA?si=kePw0yRCj-A6MsOH)\n",
        "* [Neural Networks Part 5: ArgMax and SoftMax](https://youtu.be/KpKog-L9veg?si=gqXLSbOxwJwYs0hu)\n",
        "* [Tensors for Neural Networks, Clearly Explained!!!](https://youtu.be/L35fFDpwIM4?si=Q-oglIUJb8wVO9nd)\n",
        "* [Essential Matrix Algebra for Neural Networks, Clearly Explained!!!](https://youtu.be/ZTt9gsGcdDo?si=sKDLZ8nbj4vVi9aj)\n",
        "* [The StatQuest Introduction to PyTorch](https://youtu.be/FHdlXe1bSe4?si=Yh5gfWsnjDd2WqxN)\n",
        "\n",
        "#### PyTorch Links\n",
        "\n",
        "* [StatQuest: Introduction to Coding Neural Networks with PyTorch](https://lightning.ai/lightning-ai/studios/statquest-introduction-to-coding-neural-networks-with-pytorch?view=public&section=all)\n",
        "* [ML-AI Pytorch Introduction](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html)\n",
        "\n",
        "\n",
        "### June, 25th 2024\n",
        "* [Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)](https://youtu.be/HGwBXDKFk9I?si=3yBfpZQ0dXw7s6j9)\n",
        "* [Recurrent Neural Networks (RNNs), Clearly Explained!!!](https://youtu.be/AsNTP8Kwu80?si=-JiRYXyOpu-gnhnk)\n",
        "* [Long Short-Term Memory (LSTM), Clearly Explained](https://youtu.be/YCzL96nL7j0?si=DphYdoYgx23Twgz6)\n",
        "* [Introduction to Coding Neural Networks with PyTorch and Lightning](https://youtu.be/khMzi6xPbuM?si=6aqbmYIIaefKQnWX)\n",
        "* [Long Short-Term Memory with PyTorch + Lightning](https://youtu.be/RHGiXPuo_pI?si=VZOt3gRvARWD_CtL)\n",
        "\n",
        "\n",
        "### July, 2nd 2024\n",
        "* [Word Embedding and Word2Vec, Clearly Explained!!!](https://youtu.be/viZrOnJclY0?si=B0gvlx4_ppegZAB-)\n",
        "* [Sequence-to-Sequence (seq2seq) Encoder-Decoder Neural Networks, Clearly Explained!!!](https://youtu.be/L8HKweZIOmg?si=LzC6wjlC2yE9ZekP)\n",
        "* [Attention for Neural Networks, Clearly Explained!!!](https://youtu.be/PSs6nxngL6k?si=jajDsVYk4FQgCgNA)\n",
        "\n",
        "\n",
        "### Additional Lecture (July, 9th 2024)?\n",
        "* [Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!](https://youtu.be/zxQyTK8quyY?si=LGe6J13PJ4s0qKbr)\n",
        "* [Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly Explained!!!](https://youtu.be/bQ5BoolX9Ag?si=cojnYPck8CK6NK8p)\n",
        "* [The matrix math behind transformer neural networks, one step at a time!!!](https://youtu.be/KphmOJnLAdI?si=JwIK3MhmoHxnuI3G)\n",
        "* [Word Embedding in PyTorch + Lightning](https://youtu.be/Qf06XDYXCXI?si=gIKMOQ0xjAxLo_7_)\n",
        "\n",
        "\n",
        "### Additional Videos\n",
        "* [The SoftMax Derivative, Step-by-Step!!!](https://youtu.be/M59JElEPgIg?si=KoZGFEZWVc-PclSU)\n",
        "* [Neural Networks Part 6: Cross Entropy](https://youtu.be/6ArSys5qHAU?si=TxyJi22ELyYl0m3L)\n",
        "* [Neural Networks Part 7: Cross Entropy Derivatives and Backpropagation](https://youtu.be/xBEh66V9gZo?si=kUco4zKdH8CNW23k)\n",
        " \n",
        "\n",
        "### All Videos in a Playlist\n",
        "* Full Playlist [ML-AI](https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1)\n",
        "\n",
        "\n",
        "## The StatQuest Introduction to PyTorch\n",
        "\n",
        "The following code is taken from [The StatQuest Introduction to PyTorch](https://lightning.ai/lightning-ai/studios/statquest-introduction-to-coding-neural-networks-with-pytorch?view=public&section=all&tab=files&layout=column&path=cloudspaces%2F01hf54c4fhjc8wwadsd037kjjm&y=3&x=0). Attribution goes to Josh Starmer, the creator of StatQuest, see [Josh Starmer](https://lightning.ai/josh-starmer).\n"
      ],
      "id": "2579da98"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch # torch provides basic functions, from setting a random seed (for reproducability) to creating tensors.\n",
        "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
        "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
        "from torch.optim import SGD # optim contains many optimizers. Here, we're using SGD, stochastic gradient descent.\n",
        "\n",
        "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
        "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "faf961de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building a neural network in PyTorch means creating a new class with two methods: init() and forward(). The init() method defines and initializes all of the parameters that we want to use, and the forward() method tells PyTorch what should happen during a forward pass through the neural network.\n",
        "\n",
        "### Build a Simple Neural Network in PyTorch\n"
      ],
      "id": "a15bf3cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## create a neural network class by creating a class that inherits from nn.Module.\n",
        "class BasicNN(nn.Module):\n",
        "\n",
        "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
        "        \n",
        "        super().__init__() # initialize an instance of the parent class, nn.Model.\n",
        "        \n",
        "        ## Now create the weights and biases that we need for our neural network.\n",
        "        ## Each weight or bias is an nn.Parameter, which gives us the option to optimize the parameter by setting\n",
        "        ## requires_grad, which is short for \"requires gradient\", to True. Since we don't need to optimize any of these\n",
        "        ## parameters now, we set requires_grad=False.\n",
        "        ##\n",
        "        ## NOTE: Because our neural network is already fit to the data, we will input specific values\n",
        "        ## for each weight and bias. In contrast, if we had not already fit the neural network to the data,\n",
        "        ## we might start with a random initalization of the weights and biases.\n",
        "        self.w00 = nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
        "        self.b00 = nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
        "        self.w01 = nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
        "        \n",
        "        self.w10 = nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
        "        self.b10 = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
        "        self.w11 = nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
        "\n",
        "        self.final_bias = nn.Parameter(torch.tensor(-16.), requires_grad=False)\n",
        "        \n",
        "        \n",
        "    def forward(self, input): ## forward() takes an input value and runs it though the neural network \n",
        "                              ## illustrated at the top of this notebook. \n",
        "        \n",
        "        ## the next three lines implement the top of the neural network (using the top node in the hidden layer).\n",
        "        input_to_top_relu = input * self.w00 + self.b00\n",
        "        top_relu_output = F.relu(input_to_top_relu)\n",
        "        scaled_top_relu_output = top_relu_output * self.w01\n",
        "        \n",
        "        ## the next three lines implement the bottom of the neural network (using the bottom node in the hidden layer).\n",
        "        input_to_bottom_relu = input * self.w10 + self.b10\n",
        "        bottom_relu_output = F.relu(input_to_bottom_relu)\n",
        "        scaled_bottom_relu_output = bottom_relu_output * self.w11\n",
        "        \n",
        "        ## here, we combine both the top and bottom nodes from the hidden layer with the final bias.\n",
        "        input_to_final_relu = scaled_top_relu_output + scaled_bottom_relu_output + self.final_bias\n",
        "        \n",
        "        output = F.relu(input_to_final_relu)\n",
        "    \n",
        "        return output # output is the predicted effectiveness for a drug dose."
      ],
      "id": "99528dfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have created the class that defines the neural network, we can create an actual neural network and print out its parameters, just to make sure things are what we expect.\n"
      ],
      "id": "42d6a017"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## create the neural network. \n",
        "model = BasicNN()\n",
        "\n",
        "## print out the name and value for each parameter\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)"
      ],
      "id": "369deab8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the Neural Network and Graph the Output\n",
        "\n",
        "Now that we have a neural network, we can use it on a variety of doses to determine which will be effective. Then we can make a graph of these data, and this graph should match the green bent shape fit to the training data that's shown at the top of this document. So, let's start by making a sequence of input doses...\n"
      ],
      "id": "d91117ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## now create the different doses we want to run through the neural network.\n",
        "## torch.linspace() creates the sequence of numbers between, and including, 0 and 1.\n",
        "input_doses = torch.linspace(start=0, end=1, steps=11)\n",
        "\n",
        "# now print out the doses to make sure they are what we expect...\n",
        "input_doses"
      ],
      "id": "848dc539",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have input_doses, let's run them through the neural network and graph the output...\n"
      ],
      "id": "b353580e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## create the neural network. \n",
        "model = BasicNN() \n",
        "\n",
        "## now run the different doses through the neural network.\n",
        "output_values = model(input_doses)\n",
        "\n",
        "## Now draw a graph that shows the effectiveness for each dose.\n",
        "##\n",
        "## First, set the style for seaborn so that the graph looks cool.\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "## create the graph (you might not see it at this point, but you will after we save it as a PDF).\n",
        "sns.lineplot(x=input_doses, \n",
        "             y=output_values, \n",
        "             color='green', \n",
        "             linewidth=2.5)\n",
        "\n",
        "## now label the y- and x-axes.\n",
        "plt.ylabel('Effectiveness')\n",
        "plt.xlabel('Dose')\n",
        "\n",
        "## optionally, save the graph as a PDF.\n",
        "# plt.savefig('BasicNN.pdf')"
      ],
      "id": "b5888946",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph shows that the neural network fits the training data. In other words, so far, we don't have any bugs in our code.\n",
        "\n",
        "### Optimize (Train) a Parameter in the Neural Network and Graph the Output\n",
        "\n",
        "Now that we know how to create and use a simple neural network, and we can graph the output relative to the input, let's see how to train a neural network. The first thing we need to do is tell PyTorch which parameter (or parameters) we want to train, and we do that by setting requiresgrad=True. In this example, we'll train finalbias.\n",
        "\n",
        "Now we create a neural network by creating a class that inherits from nn.Module.\n",
        "\n",
        "NOTE: This code is the same as before, except we changed the class name to BasicNN_train and we modified  final_bias in two ways:\n",
        "\n",
        "    1) we set the value of the tensor to 0, and\n",
        "    2) we set \"requires_grad=True\".\n"
      ],
      "id": "69a29bbf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BasicNN_train(nn.Module):\n",
        "\n",
        "    def __init__(self): # __init__ is the class constructor function, and we use it to initialize the weights and biases.\n",
        "        \n",
        "        super().__init__() # initialize an instance of the parent class, nn.Module.\n",
        "        \n",
        "        self.w00 = nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
        "        self.b00 = nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
        "        self.w01 = nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
        "        \n",
        "        self.w10 = nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
        "        self.b10 = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
        "        self.w11 = nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
        "\n",
        "        ## we want to modify final_bias to demonstrate how to optimize it with backpropagation.\n",
        "        ## The optimal value for final_bias is -16...\n",
        "#         self.final_bias = nn.Parameter(torch.tensor(-16.), requires_grad=False)\n",
        "        ## ...so we set it to 0 and tell Pytorch that it now needs to calculate the gradient for this parameter.\n",
        "        self.final_bias = nn.Parameter(torch.tensor(0.), requires_grad=True) \n",
        "        \n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_to_top_relu = input * self.w00 + self.b00\n",
        "        top_relu_output = F.relu(input_to_top_relu)\n",
        "        scaled_top_relu_output = top_relu_output * self.w01\n",
        "        \n",
        "        input_to_bottom_relu = input * self.w10 + self.b10\n",
        "        bottom_relu_output = F.relu(input_to_bottom_relu)\n",
        "        scaled_bottom_relu_output = bottom_relu_output * self.w11\n",
        "    \n",
        "        input_to_final_relu = scaled_top_relu_output + scaled_bottom_relu_output + self.final_bias\n",
        "        \n",
        "        output = F.relu(input_to_final_relu)\n",
        "        \n",
        "        return output"
      ],
      "id": "894d7d6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's graph the output of BasicNN_train, which is currently not optimized, and compare it to the graph we drew earlier of the optimized neural network.\n"
      ],
      "id": "de487b8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## create the neural network. \n",
        "model = BasicNN_train() \n",
        "\n",
        "## now run the different doses through the neural network.\n",
        "output_values = model(input_doses)\n",
        "\n",
        "## Now draw a graph that shows the effectiveness for each dose.\n",
        "##\n",
        "## set the style for seaborn so that the graph looks cool.\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "## create the graph (you might not see it at this point, but you will after we save it as a PDF).\n",
        "sns.lineplot(x=input_doses, \n",
        "             y=output_values.detach(), ## NOTE: because final_bias has a gradident, we call detach() \n",
        "                                       ## to return a new tensor that only has the value and not the gradient.\n",
        "             color='green', \n",
        "             linewidth=2.5)\n",
        "\n",
        "## now label the y- and x-axes.\n",
        "plt.ylabel('Effectiveness')\n",
        "plt.xlabel('Dose')\n",
        "\n",
        "## lastly, save the graph as a PDF.\n",
        "# plt.savefig('BasicNN_train.pdf')"
      ],
      "id": "80278b2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph shows that when the dose is 0.5, the output from the unoptimized neural network is 17, which is wrong, since the output value should be 1. So, now that we have a parameter we can optimize, let's create some training data that we can use to optimize it.\n"
      ],
      "id": "2a6d0302"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## create the training data for the neural network.\n",
        "inputs = torch.tensor([0., 0.5, 1.])\n",
        "labels = torch.tensor([0., 1., 0.])"
      ],
      "id": "029575f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "..and now let's use that training data to train (or optimize) final_bias.\n"
      ],
      "id": "50e39815"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## create the neural network we want to train.\n",
        "model = BasicNN_train()\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.1) ## here we're creating an optimizer to train the neural network.\n",
        "                                            ## NOTE: There are a bunch of different ways to optimize a neural network.\n",
        "                                            ## In this example, we'll use Stochastic Gradient Descent (SGD). However,\n",
        "                                            ## another popular algortihm is Adam (which will be covered in a StatQuest).\n",
        "\n",
        "print(\"Final bias, before optimization: \" + str(model.final_bias.data) + \"\\n\")\n",
        "\n",
        "## this is the optimization loop. Each time the optimizer sees all of the training data is called an \"epoch\".\n",
        "for epoch in range(100):\n",
        "        \n",
        "    ## we create and initialize total_loss for each epoch so that we can evaluate how well model fits the\n",
        "    ## training data. At first, when the model doesn't fit the training data very well, total_loss\n",
        "    ## will be large. However, as gradient descent improves the fit, total_loss will get smaller and smaller.\n",
        "    ## If total_loss gets really small, we can decide that the model fits the data well enough and stop\n",
        "    ## optimizing the fit. Otherwise, we can just keep optimizing until we reach the maximum number of epochs. \n",
        "    total_loss = 0\n",
        "    \n",
        "    ## this internal loop is where the optimizer sees all of the training data and where we \n",
        "    ## calculate the total_loss for all of the training data.\n",
        "    for iteration in range(len(inputs)):\n",
        "        \n",
        "        input_i = inputs[iteration] ## extract a single input value (a single dose)...\n",
        "        label_i = labels[iteration] ## ...and its corresponding label (the effectiveness for the dose).\n",
        "        \n",
        "        output_i = model(input_i) ## calculate the neural network output for the input (the single dose).\n",
        "        \n",
        "        loss = (output_i - label_i)**2 ## calculate the loss for the single value.\n",
        "                                       ## NOTE: Because output_i = model(input_i), \"loss\" has a connection to \"model\"\n",
        "                                       ## and the derivative (calculated in the next step) is kept and accumulated\n",
        "                                       ## in \"model\".\n",
        "        \n",
        "        loss.backward() # backward() calculates the derivative for that single value and adds it to the previous one.\n",
        "        \n",
        "        total_loss += float(loss) # accumulate the total loss for this epoch.\n",
        "        \n",
        "        \n",
        "    if (total_loss < 0.0001):\n",
        "        print(\"Num steps: \" + str(epoch))\n",
        "        break\n",
        "      \n",
        "    optimizer.step() ## take a step toward the optimal value.\n",
        "    optimizer.zero_grad() ## This zeroes out the gradient stored in \"model\". \n",
        "                          ## Remember, by default, gradients are added to the previous step (the gradients are accumulated),\n",
        "                          ## and we took advantage of this process to calculate the derivative one data point at a time.\n",
        "                          ## NOTE: \"optimizer\" has access to \"model\" because of how it was created with the call \n",
        "                          ## (made earlier): optimizer = SGD(model.parameters(), lr=0.1).\n",
        "                          ## ALSO NOTE: Alternatively, we can zero out the gradient with model.zero_grad().\n",
        "    \n",
        "    print(\"Step: \" + str(epoch) + \" Final Bias: \" + str(model.final_bias.data) + \"\\n\")\n",
        "    ## now go back to the start of the loop and go through another epoch.\n",
        "\n",
        "print(\"Total loss: \" + str(total_loss))\n",
        "print(\"Final bias, after optimization: \" + str(model.final_bias.data))"
      ],
      "id": "a4b9806d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, if everything worked correctly, the optimizer should have converged on final_bias = 16.0019 after 34 steps, or epochs. BAM!\n",
        "\n",
        "Lastly, let's graph the output from the optimized neural network and see if it's the same as what we started with. If so, then the optimization worked.\n"
      ],
      "id": "4916085a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## run the different doses through the neural network\n",
        "output_values = model(input_doses)\n",
        "\n",
        "## set the style for seaborn so that the graph looks cool.\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "## create the graph (you might not see it at this point, but you will after we save it as a PDF).\n",
        "sns.lineplot(x=input_doses, \n",
        "             y=output_values.detach(), ## NOTE: we call detach() because final_bias has a gradient\n",
        "             color='green', \n",
        "             linewidth=2.5)\n",
        "\n",
        "## now label the y- and x-axes.\n",
        "plt.ylabel('Effectiveness')\n",
        "plt.xlabel('Dose')\n",
        "\n",
        "## lastly, save the graph as a PDF.\n",
        "# plt.savefig('BascNN_optimized.pdf')"
      ],
      "id": "bbc48c26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we see that the optimized model results in the same graph that we started with, so the optimization worked as expected."
      ],
      "id": "b0e2b7eb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}