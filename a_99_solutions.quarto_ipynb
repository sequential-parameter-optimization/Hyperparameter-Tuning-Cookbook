{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0268cbf2",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "jupyter: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Solutions to Selected Exercises\n",
    "\n",
    "::: {.callout-warning}\n",
    "* Solutions are incomplete and need to be corrected!\n",
    "* They serve as a starting point for the final solution.\n",
    ":::\n",
    "\n",
    "\n",
    "## Data-Driven Modeling and Optimization\n",
    "\n",
    "\n",
    "### Histograms\n",
    "\n",
    "::: {#sol-curve}\n",
    "###  Density Curve\n",
    "\n",
    "* We can calculate propabilities.\n",
    "*  We only need two parameters (the mean and the sd) to form the curve -> Store data more efficently\n",
    "*  Blanks can be filled\n",
    ":::\n",
    "\n",
    "### The Normal Distribution\n",
    "\n",
    "::: {#sol-2SD}\n",
    "###  TwoSDAnswer\n",
    "95%\n",
    ":::\n",
    "\n",
    "::: {#sol-2SD1}\n",
    "###  OneSDAnswer\n",
    "68%\n",
    ":::\n",
    "\n",
    "::: {#sol-2SD2}\n",
    "###  ThreeSDAnswer\n",
    "99,7%\n",
    ":::\n",
    "\n",
    "::: {#sol-2SD3}\n",
    "###  DataRangeAnswer\n",
    "80 - 120\n",
    ":::\n",
    "\n",
    "::: {#sol-2SD4}\n",
    "###  PeakHeightAnswer\n",
    "low\n",
    ":::\n",
    "\n",
    "### The mean, the media, and the mode\n",
    "\n",
    "### The exponential distribution\n",
    "\n",
    "### Population and Estimated Parameters\n",
    "\n",
    "::: {#sol-POP1}\n",
    "\n",
    "### ProbabilityAnswer\n",
    "50%\n",
    ":::\n",
    "\n",
    "\n",
    "### Calculating the Mean, Variance and Standard Deviation\n",
    "\n",
    "::: {#sol-CAL1}\n",
    "### MeanDifferenceAnswer\n",
    "If we have all the data, $\\mu$ is the population mean and x-bar is the sample mean. We don't have the full information.\n",
    ":::\n",
    "\n",
    "::: {#sol-CAL2}\n",
    "### EstimateMeanAnswer\n",
    "Sum of the values divided by n.\n",
    ":::\n",
    "\n",
    "::: {#sol-CAL3}\n",
    "### SigmaSquaredAnswer\n",
    "Variance\n",
    ":::\n",
    "\n",
    "::: {#sol-CAL4}\n",
    "### EstimatedSDAnswer\n",
    "The same as the normal standard deviation, but using n-1.\n",
    ":::\n",
    "\n",
    "::: {#sol-CAL5}\n",
    "### VarianceDifferenceAnswer\n",
    "$n$ and $n-1$\n",
    ":::\n",
    "\n",
    "::: {#sol-MAT1}\n",
    "### ModelBenefitsAnswer\n",
    "- Approximation\n",
    "- Prediction\n",
    "- Understanding\n",
    ":::\n",
    "\n",
    "::: {#sol-SAM1}\n",
    "### SampleDefinitionAnswer\n",
    "It's a subset of the data.\n",
    ":::\n",
    "\n",
    "### Hypothesis Testing and the Null-Hypothesis\n",
    "\n",
    "::: {#sol-Hyp1}\n",
    "### RejectHypothesisAnswer\n",
    "It means the evidence supports the alternative hypothesis, indicating that the null hypothesis is unlikely to be true.\n",
    ":::\n",
    "\n",
    "::: {#sol-Hyp2}\n",
    "### NullHypothesisAnswer\n",
    "It's a statement that there is no effect or no difference, and it serves as the default or starting assumption in hypothesis testing.\n",
    ":::\n",
    "\n",
    "::: {#sol-Hyp3}\n",
    "### BetterDrugAnswer\n",
    "By conducting experiments and statistical tests to compare the new drug's effectiveness against the current standard and demonstrating a significant improvement.\n",
    ":::\n",
    "\n",
    "### Alternative Hypotheses, Main Ideas\n",
    "\n",
    "### p-values: What they are and how to interpret them\n",
    "\n",
    "::: {#sol-PVal1}\n",
    "### PValueIntroductionAnswer\n",
    "We can reject the null hypothesis. We can make a decision.\n",
    ":::\n",
    "\n",
    "::: {#sol-PVal2}\n",
    "### PValueRangeAnswer\n",
    "It can only be between 0 and 1.\n",
    ":::\n",
    "\n",
    "::: {#sol-PVal3}\n",
    "### PValueRangeAnswer\n",
    "It can only be between 0 and 1.\n",
    ":::\n",
    "\n",
    "::: {#sol-PVal4}\n",
    "### TypicalPValueAnswer\n",
    "The chance that we wrongly reject the null hypothesis.\n",
    ":::\n",
    "\n",
    "::: {#sol-PVal5}\n",
    "### FalsePositiveAnswer\n",
    "If we have a false-positive, we succeed in rejecting the null hypothesis. But in fact/reality, this is false -> False positive.\n",
    ":::\n",
    "\n",
    "### How to calculate p-values\n",
    "\n",
    "::: {#sol-Calc1}\n",
    "### CalculatePValueAnswer\n",
    "Probability of specific result, probability of outcome with the same probability, and probability of events with smaller probability.\n",
    ":::\n",
    "\n",
    "::: {#sol-Calc2}\n",
    "### SDCalculationAnswer\n",
    "7 is the SD.\n",
    ":::\n",
    "\n",
    "::: {#sol-Calc3}\n",
    "### SidedPValueAnswer\n",
    "If we are not interested in the direction of the change, we use the two-sided. If we want to know about the direction, the one-sided.\n",
    ":::\n",
    "\n",
    "::: {#sol-Calc4}\n",
    "### CoinTestAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Calc5}\n",
    "### BorderPValueAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "::: {#sol-Calc6}\n",
    "### OneSidedPValueCautionAnswer\n",
    "If you look in the wrong direction, there is no change.\n",
    ":::\n",
    "\n",
    "::: {#sol-Calc7}\n",
    "### BinomialDistributionAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "### p-hacking: What it is and how to avoid it\n",
    "\n",
    "::: {#sol-Hack1}\n",
    "### PHackingWaysAnswer\n",
    "\n",
    "* Performing repeats until you find one result with a small p-value -> false positive result.\n",
    "* Increasing the sample size within one experiment when it is close to the threshold.\n",
    ":::\n",
    "\n",
    "::: {#sol-Hack2}\n",
    "### AvoidPHackingAnswer\n",
    "Specify the number of repeats and the sample sizes at the beginning.\n",
    ":::\n",
    "\n",
    "::: {#sol-Hack3}\n",
    "### MultipleTestingProblemAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "### Covariance\n",
    "\n",
    "\n",
    "::: {#sol-Cov1}\n",
    "### CovarianceDefinitionAnswer\n",
    "Formula\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Cov2}\n",
    "### CovarianceMeaningAnswer\n",
    "Large values in the first variable result in large values in the second variable.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Cov3}\n",
    "### CovarianceVarianceRelationshipAnswer\n",
    "Formula\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Cov4}\n",
    "### HighCovarianceAnswer\n",
    "No, size doesn't matter.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Cov5}\n",
    "### ZeroCovarianceAnswer\n",
    "No relationship\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Cov6}\n",
    "### NegativeCovarianceAnswer\n",
    "Yes\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Cov7}\n",
    "### NegativeVarianceAnswer\n",
    "No\n",
    ":::\n",
    "\n",
    "### Pearson's Correlation\n",
    "\n",
    "\n",
    "::: {#sol-Corr1}\n",
    "### CorrelationValueAnswer\n",
    "Recalculate\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Corr2}\n",
    "### CorrelationRangeAnswer\n",
    "From -1 to 1\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Corr3}\n",
    "### CorrelationFormulaAnswer\n",
    "Formula\n",
    ":::\n",
    "\n",
    "### Boxplots\n",
    "\n",
    "\n",
    "::: {#sol-StatPow1}\n",
    "### UnderstandingStatisticalPower\n",
    "It is the probability of correctly rejecting the null hypothesis.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-StatPow2}\n",
    "### DistributionEffectOnPower\n",
    "Power analysis is not applicable.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-StatPow3}\n",
    "### IncreasingPower\n",
    "By taking more samples.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-StatPow4}\n",
    "### PreventingPHacking\n",
    "TBD\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-StatPow5}\n",
    "### SampleSizeAndPower\n",
    "The power will be low.\n",
    ":::\n",
    "\n",
    "### Power Analysis\n",
    "\n",
    "::: {#sol-PowAn1}\n",
    "### MainFactorsAffectingPower\n",
    "The overlap (distance of the two means) and sample sizes.\n",
    ":::\n",
    "\n",
    "::: {#sol-PowAn2}\n",
    "### PowerAnalysisOutcome\n",
    "The sample size needed.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-PowAn3}\n",
    "### RisksInExperiments\n",
    "Few experiments lead to very low power, and many experiments might result in p-hacking.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-PowAn4}\n",
    "### StepsToPerformPowerAnalysis\n",
    "\n",
    "1. Select power\n",
    "2. Select threshold for significance (alpha)\n",
    "3. Estimate the overlap (done by the effect size)\n",
    ":::\n",
    "\n",
    "### The Central Limit Theorem\n",
    "\n",
    "::: {#sol-CenLi1}\n",
    "### CentralLimitTheoremAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "### Boxplots\n",
    "\n",
    "::: {#sol-BoxPlo1}\n",
    "### MedianAnswer\n",
    "The median.\n",
    ":::\n",
    "\n",
    "::: {#sol-BoxPlo2}\n",
    "### BoxContentAnswer\n",
    "50% of the data.\n",
    ":::\n",
    "\n",
    "### R-squared\n",
    "\n",
    "::: {#sol-RSqu1}\n",
    "### RSquaredFormulaAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "::: {#sol-RSqu2}\n",
    "### NegativeRSquaredAnswer\n",
    "If you fit a line, no, but there are cases where it could be negative. However, these are usually considered useless.\n",
    ":::\n",
    "\n",
    "::: {#sol-RSqu3}\n",
    "### RSquaredCalculationAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "#### The main ideas of fitting a line to data (The main ideas of least squares and linear regression.)\n",
    "\n",
    "::: {#sol-FitLin1}\n",
    "### LeastSquaresAnswer\n",
    "It is the calculation of the smallest sum of residuals when you fit a model to data.\n",
    ":::\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "### Multiple Regression\n",
    "\n",
    "### A Gentle Introduction to Machine Learning\n",
    "\n",
    "\n",
    "::: {#sol-ML1}\n",
    "### RegressionVsClassificationAnswer\n",
    "Regression involves predicting continuous values (e.g., temperature, size), while classification involves predicting discrete values (e.g., categories like cat, dog).\n",
    ":::\n",
    "\n",
    "### Maximum Likelihood\n",
    "\n",
    "::: {#sol-MaxLike1}\n",
    "### LikelihoodConceptAnswer\n",
    "The distribution that fits the data best.\n",
    ":::\n",
    "\n",
    "### Probability is not Likelihood\n",
    "\n",
    "::: {#sol-Prob1}\n",
    "### ProbabilityVsLikelihoodAnswer\n",
    "Likelihood: Finding the curve that best fits the data. Probability: Calculating the probability of an event given a specific curve.\n",
    ":::\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "\n",
    "::: {#sol-CroVal1}\n",
    "### TrainVsTestDataAnswer\n",
    "Training data is used to fit the model, while testing data is used to evaluate how well the model fits.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-CroVal2}\n",
    "### SingleValidationIssueAnswer\n",
    "The performance might not be representative because the data may not be equally distributed between training and testing sets.\n",
    ":::\n",
    "\n",
    "::: {#sol-CroVal3}\n",
    "### FoldDefinitionAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "::: {#sol-CroVal4}\n",
    "### LeaveOneOutValidationAnswer\n",
    "Only one data point is used as the test set, and the rest are used as the training set.\n",
    ":::\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "::: {#sol-ConMat1}\n",
    "### ConfusionMatrixAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "### Sensitivity and Specificity\n",
    "\n",
    "\n",
    "::: {#sol-SenSpe1}\n",
    "### SensitivitySpecificityAnswer1\n",
    "TBD\n",
    ":::\n",
    "\n",
    "::: {#sol-SenSpe2}\n",
    "### SensitivitySpecificityAnswer2\n",
    "TBD\n",
    ":::\n",
    "\n",
    "### Bias and Variance\n",
    "\n",
    "\n",
    "::: {#sol-MalLea1}\n",
    "### BiasAndVarianceAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "### Mutual Information\n",
    "\n",
    "::: {#sol-MutInf1}\n",
    "### MutualInformationExampleAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "::: {#sol-PCA1}\n",
    "### WhatIsPCAAnswer\n",
    "A dimension reduction technique that helps discover important variables.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-PCA2}\n",
    "### screePlotAnswer\n",
    "It shows how much variation is defined by the data.\n",
    ":::\n",
    "\n",
    "::: {#sol-PCA3}\n",
    "### LeastSquaresInPCAAnswer\n",
    "No, in the first step it tries to maximize distances.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-PCA4}\n",
    "### PCAStepsAnswer\n",
    "1. Calculate mean\n",
    "2. Shift the data to the center of the coordinate system\n",
    "3. Fit a line by maximizing the distances\n",
    "4. Calculate the sum of squared distances\n",
    "5. Calculate the slope\n",
    "6. Rotate\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-PCA5}\n",
    "### EigenvaluePC1Answer\n",
    "Formula (to be specified).\n",
    ":::\n",
    "\n",
    "::: {#sol-PCA6}\n",
    "### DifferencesBetweenPointsAnswer\n",
    "No, because the first difference is measured on the PC1 scale and it is more important.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-PCA7}\n",
    "### ScalingInPCAAnswer\n",
    "Scaling by dividing by the standard deviation (SD).\n",
    ":::\n",
    "\n",
    "::: {#sol-PCA8}\n",
    "### DetermineNumberOfComponentsAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-PCA9}\n",
    "### LimitingNumberOfComponentsAnswer\n",
    "\n",
    "1. The dimension of the problem\n",
    "2. Number of samples\n",
    ":::\n",
    "\n",
    "### t-SNE\n",
    "\n",
    "\n",
    "::: {#sol-tSNE1}\n",
    "### WhyUseTSNEAnswer\n",
    "For dimension reduction and picking out the relevant clusters.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-tSNE2}\n",
    "### MainIdeaOfTSNEAnswer\n",
    "To reduce the dimensions of the data by reconstructing the relationships in a lower-dimensional space.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-tSNE3}\n",
    "### BasicConceptOfTSNEAnswer\n",
    "\n",
    "1. First, randomly arrange the points in a lower dimension\n",
    "2. Decide whether to move points left or right, depending on distances in the original dimension\n",
    "3. Finally, arrange points in the lower dimension similarly to the original dimension\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-tSNE4}\n",
    "### TSNEStepsAnswer\n",
    "\n",
    "1. Project data to get random points\n",
    "2. Set up a matrix of distances\n",
    "3. Calculate the inner variances of the clusters and the Gaussian distribution\n",
    "4. Do the same with the projected points\n",
    "5. Move projected points so the second matrix gets more similar to the first matrix\n",
    ":::\n",
    "\n",
    "### K-means clustering\n",
    "\n",
    "\n",
    "::: {#sol-KMeans1}\n",
    "### HowKMeansWorksAnswer\n",
    "\n",
    "1. Select the number of clusters\n",
    "2. Randomly select distinct data points as initial cluster centers\n",
    "3. Measure the distance between each point and the cluster centers\n",
    "4. Assign each point to the nearest cluster\n",
    "5. Repeat the process\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-KMeans2}\n",
    "### QualityOfClustersAnswer\n",
    "Calculate the within-cluster variation.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-KMeans3}\n",
    "### IncreasingKAnswer\n",
    "If k is too high, each point would be its own cluster. If k is too low, you cannot see the structures.\n",
    ":::\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "\n",
    "::: {#sol-DBSCAN1}\n",
    "### CorePointInDBSCANAnswer\n",
    "A point that is close to at least k other points.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-DBSCAN2}\n",
    "### AddingVsExtendingAnswer\n",
    "Adding means we add a point and then stop. Extending means we add a point and then look for other neighbors from that point.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-DBSCAN3}\n",
    "### OutliersInDBSCANAnswer\n",
    "Points that are not core points and do not belong to existing clusters.\n",
    ":::\n",
    "\n",
    "### K-nearest neighbors\n",
    "\n",
    "\n",
    "::: {#sol-KNN1}\n",
    "### AdvantagesAndDisadvantagesOfKAnswer\n",
    "\n",
    "* k = 1: Noise can disturb the process because of possibly incorrect measurements of points.\n",
    "* k = 100: The majority can be wrong for some groups. It is smoother, but there is less chance to discover the structure of the data.\n",
    "\n",
    ":::\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "\n",
    "::: {#sol-NaiveBayes1}\n",
    "### NaiveBayesFormulaAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-NaiveBayes2}\n",
    "### CalculateProbabilitiesAnswer\n",
    "TBD\n",
    ":::\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "::: {#sol-GaussianNB1}\n",
    "### UnderflowProblemAnswer\n",
    "Small values multiplied together can become smaller than the limits of computer memory, resulting in zero. Using logarithms (e.g., log(1/2) -> -1, log(1/4) -> -2) helps prevent underflow.\n",
    ":::\n",
    "\n",
    "\n",
    "### Trees\n",
    "\n",
    "::: {#sol-Tree1}\n",
    "### Tree Usage\n",
    "Classication, Regression, Clustering\n",
    ":::\n",
    "\n",
    "::: {#sol-DTree1}\n",
    "### Tree Usage\n",
    "TBD\n",
    ":::\n",
    "\n",
    "::: {#sol-DTree2}\n",
    "### Tree Feature Importance\n",
    "The most important feature.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-RTree1}\n",
    "### Regression Tree Limitations\n",
    "High dimensions \n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-RTree2}\n",
    "### Regression Tree Score\n",
    "SSR + alpha * T \n",
    ":::\n",
    "\n",
    "::: {#sol-RTree3}\n",
    "### Regression Tree Alpha Value Small\n",
    "The tree is more complex.\n",
    ":::\n",
    "\n",
    "::: {#sol-RTree4}\n",
    "###  Regression Tree Increase Alpha Value \n",
    "We get smaller trees \n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-RTree5}\n",
    "### Regression Tree Pruning\n",
    "Decreases the complexity of the tree to enhance performance and reduce overfitting \n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Machine Learning and Artificial Intelligence\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "::: {#sol-BacPro1}\n",
    "### ChainRuleAndGradientDescentAnswer\n",
    "Combination of the chain rule and gradient descent.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-BacPro2}\n",
    "### BackpropagationNamingAnswer\n",
    "Because you start at the end and go backwards.\n",
    ":::\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "::: {#sol-GradDesc1}\n",
    "### GradDescStepSize\n",
    "learning rate x slope \n",
    ":::\n",
    "\n",
    "::: {#sol-GradDesc2}\n",
    "### GradDescIntercept\n",
    " Old intercept - step size \n",
    ":::\n",
    "\n",
    "::: {#sol-GradDesc3}\n",
    "### GradDescIntercept\n",
    "When the step size is small or after a certain number of steps \n",
    ":::\n",
    "\n",
    "\n",
    "### ReLU\n",
    "\n",
    "\n",
    "::: {#sol-Relu1}\n",
    "### Graph ReLU\n",
    "Graph of ReLU function: f(x) = max(0, x)\n",
    ":::\n",
    "\n",
    "\n",
    "### CNNs\n",
    "\n",
    "::: {#sol-CNN1}\n",
    "### CNNImageRecognitionAnswer\n",
    "- too many features for input layer -> high memory consumption\n",
    "- always shift in data\n",
    "- it learns local informations and local correlations \n",
    ":::\n",
    "\n",
    "::: {#sol-CNN2}\n",
    "### CNNFiltersInitializationAnswer\n",
    "The filter values in CNNs are randomly initialized and then trained and optimized through the process of backpropagation.\n",
    ":::\n",
    "\n",
    "::: {#sol-CNN3}\n",
    "### CNNFilterInitializationAnswer\n",
    "The filter values in CNNs are initially set by random initialization. These filters undergo training via backpropagation, where gradients are computed and used to adjust the filter values to optimize performance.\n",
    ":::\n",
    "\n",
    "::: {#sol-CNN4}\n",
    "### GenNNStockPredictionAnswer\n",
    "A limitation of using classical neural networks for stock market prediction is their reliance on fixed inputs. Stock market data is dynamic and requires models that can adapt to changing conditions over time.\n",
    ":::\n",
    "\n",
    "### RNN\n",
    "\n",
    "::: {#sol-RNN1}\n",
    "### RNNUnrollingAnswer\n",
    "In the unrolling process of RNNs, the network is copied and the output from the inner loop is fed into the second layer of the copied network.\n",
    ":::\n",
    "\n",
    "::: {#sol-RNN2}\n",
    "### RNNReliabilityAnswer\n",
    "RNNs sometimes fail to work reliably due to the vanishing gradient problem (where gradients are less than 1) and the exploding gradient problem (where gradients are greater than 1). Additionally, reliability issues arise because the network and the weights are copied during the unrolling process.\n",
    ":::\n",
    "\n",
    "### LSTM\n",
    "\n",
    "::: {#sol-LSTM1}\n",
    "### LSTMSigmoidTanhAnswer\n",
    "The sigmoid activation function outputs values between 0 and 1, making it suitable for probability determination, whereas the tanh activation function outputs values between -1 and 1.\n",
    ":::\n",
    "\n",
    "::: {#sol-LSTM11}\n",
    "### LSTMSigmoidTanhAnswer\n",
    "State how much of the long term memory should be used.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-LSTM2}\n",
    "### LSTMGatesAnswer\n",
    "An LSTM network has three types of gates: the forget gate, the input gate, and the output gate. The forget gate decides what information to discard from the cell state, the input gate updates the cell state with new information, and the output gate determines what part of the cell state should be output.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-LSTM3}\n",
    "### LSTMLongTermInfoAnswer\n",
    "Long-term information is used in the output gate of an LSTM network.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-LSTM4}\n",
    "### LSTMUpdateGatesAnswer\n",
    "In the input and forget gates.\n",
    ":::\n",
    "\n",
    "### Pytorch/Lightning\n",
    "\n",
    "\n",
    "::: {#sol-PyTorch1}\n",
    "### PyTorchRequiresGradAnswer\n",
    "In PyTorch, `requires_grad` indicates whether a tensor should be trained. If set to False, the tensor will not be trained.\n",
    ":::\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "::: {#sol-Embedding1}\n",
    "### NN STrings\n",
    "No, they process numerical values.\n",
    ":::\n",
    "\n",
    "::: {#sol-Embedding2}\n",
    "### Embedding Definition\n",
    "Representation of a word as a vector.\n",
    ":::\n",
    "\n",
    "::: {#sol-Embedding3}\n",
    "### Embedding Dimensions\n",
    "We can model similarities.\n",
    ":::\n",
    "\n",
    "\n",
    "### Sequence to Sequence Models\n",
    "\n",
    "::: {#sol-S2S1}\n",
    "### LSTM\n",
    "Because they are able to consider “far away” information.\n",
    ":::\n",
    "\n",
    "::: {#sol-S2S2}\n",
    "### Teacher Forcing\n",
    "We need to force the correct words for the training.\n",
    ":::\n",
    "\n",
    "::: {#sol-S2S3}\n",
    "### Attention\n",
    "Attention scores compute similarities for one input to the others.\n",
    ":::\n",
    "\n",
    "### Transformers\n",
    "\n",
    "::: {#sol-Transformer1}\n",
    "### ChatGPT\n",
    "Decoder only.\n",
    ":::\n",
    "\n",
    "::: {#sol-Transformer2}\n",
    "### Translation\n",
    "Encoder-Decoder structure.\n",
    ":::\n",
    "\n",
    "::: {#sol-Transformer3}\n",
    "### Difference Encoder-Decoder and Decoder Only.\n",
    "* Encoder-Decoder: self-attention.\n",
    "* Decoder only: masked self-attention.\n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Transformer4}\n",
    "### Weights\n",
    "* a: Randomly\n",
    "* b: Backpropagation\n",
    ":::\n",
    "\n",
    "::: {#sol-Transformer5}\n",
    "### Order of Words\n",
    "Positional Encoding\n",
    ":::\n",
    "\n",
    "::: {#sol-Transformer6}\n",
    "### Relationship Between Words\n",
    "Masked self-attention which looks at the previous tokens.\n",
    ":::\n",
    "\n",
    "::: {#sol-Transformer7}\n",
    "### Masked Self Attention\n",
    "It works by investigating how similar each word is to itself and all of the proceeding words in the sentence. \n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Transformer8}\n",
    "### Softmax\n",
    "Transformation to values between 0 and 1.\n",
    ":::\n",
    "\n",
    "::: {#sol-Transformer9}\n",
    "### Softmax Output\n",
    "We create two new numbers: Values – like K and Q with different weights. We scale these values by the percentage. -> we get the scaled V´s \n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Transformer10}\n",
    "### V´s\n",
    "Lastly, we sum these values together, which combine separate encodings for both words relative to their similarities to “is”, are the masked-self-attention values for “is”. \n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Transformer11}\n",
    "### Residual Connections\n",
    "They are bypasses, which combine the position encoded values with masked-self-attention values. \n",
    ":::\n",
    "\n",
    "::: {#sol-Transformer12}\n",
    "### Generate Known Word in Sequence\n",
    "* Training \n",
    "* Because it is a Decoder-Only transformer used for prediction and the calculations that you need.  \n",
    ":::\n",
    "\n",
    "\n",
    "::: {#sol-Transformer13}\n",
    "### Masked-Self-Attention Values and Bypass\n",
    "We use a simple neural network with two inputs and five outputs for the vocabulary. \n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
