{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---"
      ],
      "id": "d87224c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Documentation of the Sequential Parameter Optimization\n",
        "\n",
        "This document describes the `Spot` features. The official `spotPython` documentation can be found here: [https://sequential-parameter-optimization.github.io/spotPython/](https://sequential-parameter-optimization.github.io/spotPython/).\n",
        "\n",
        "## Example: spot\n"
      ],
      "id": "569b2b58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "from scipy.optimize import shgo\n",
        "from scipy.optimize import direct\n",
        "from scipy.optimize import differential_evolution\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "44413e71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Objective Function\n",
        "\n",
        "The `spotPython` package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula:\n",
        "$$f(x) = x^2$$\n"
      ],
      "id": "2dff7fb4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fun = analytical().fun_sphere"
      ],
      "id": "99be5dcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = np.linspace(-1,1,100).reshape(-1,1)\n",
        "y = fun(x)\n",
        "plt.figure()\n",
        "plt.plot(x,y, \"k\")\n",
        "plt.show()"
      ],
      "id": "ba4c5985",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
        "spot_1 = spot.Spot(fun=fun,\n",
        "                   fun_control=fun_control_init(\n",
        "                        lower = np.array([-10]),\n",
        "                        upper = np.array([100]),\n",
        "                        fun_evals = 7,\n",
        "                        fun_repeats = 1,\n",
        "                        max_time = inf,\n",
        "                        noise = False,\n",
        "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                        var_type=[\"num\"],\n",
        "                        infill_criterion = \"y\",\n",
        "                        n_points = 1,\n",
        "                        seed=123,\n",
        "                        log_level = 50),\n",
        "                   design_control=design_control_init(\n",
        "                        init_size=5,\n",
        "                        repeats=1),\n",
        "                   surrogate_control=surrogate_control_init(\n",
        "                        noise=False,\n",
        "                        min_theta=-4,\n",
        "                        max_theta=3,\n",
        "                        n_theta=1,\n",
        "                        model_optimizer=differential_evolution,\n",
        "                        model_fun_evals=10000))"
      ],
      "id": "cd885f58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`spot`'s  `__init__` method sets the control parameters. There are two parameter groups:\n",
        "\n",
        "1. external parameters can be specified by the user\n",
        "2. internal parameters, which are handled by `spot`.\n",
        "\n",
        "\n",
        "### External Parameters\n",
        "\n",
        "|external parameter| type | description | default | mandatory |\n",
        "| -- | -- | -- |-- |-- |\n",
        "| `fun` | object |objective function | | yes |\n",
        "| `lower` | array | lower bound | | yes |\n",
        "| `upper` | array | upper bound | | yes |\n",
        "| `fun_evals`| int | number of function evaluations | 15 | no |\n",
        "| `fun_evals`| int | number of function evaluations | 15 | no |\n",
        "| `fun_control` | dict | noise etc. | {} | n |\n",
        "| `max_time` | int | max run time budget | `inf` | no |\n",
        "| `noise`| bool | if repeated evaluations of `fun` results in different values, then `noise` should be set to `True`. | `False`| no|\n",
        "| `tolerance_x`| float | tolerance for new x solutions. Minimum distance of new solutions, generated by `suggest_new_X`, to already existing solutions. If zero (which is the default), every new solution is accepted. | `0` | no|\n",
        "| `var_type` | list | list of type information, can be either `\"num\"` or `\"factor\"` | `[\"num\"]` | no |\n",
        "| `infill_criterion`| string | Can be `\"y\"`, `\"s\"`, `\"ei\"` (negative expected improvement), or `\"all\"`| `\"y\"` | no|\n",
        "| `n_points`| int | number of infill points | 1 | no |\n",
        "| `seed` | int | initial seed. If `Spot.run()` is called twice, different results will be generated. To reproduce results, the `seed` can be used. | `123` | no |\n",
        "| `log_level`| int | log level with the following settings: `NOTSET` (`0`), `DEBUG` (`10`: Detailed information, typically of interest only when diagnosing problems.), `INFO` (`20`: Confirmation that things are working as expected.), `WARNING` (`30`: An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected.), `ERROR` (`40`: Due to a more serious problem, the software has not been able to perform some function.), and `CRITICAL` (`50`: A serious error, indicating that the program itself may be unable to continue running.)| `50` | no |\n",
        "| `show_models` | bool | Plot model. Currently only 1-dim functions are supported | `False` | no|\n",
        "| `design`| object | experimental design | `None` | no |\n",
        "| `design_control` | dict | control parameters | see below | no|\n",
        "| `surrogate` | | surrogate model | `kriging` | no |\n",
        "| `surrogate_control` | dict | control parameters | see below | no|\n",
        "| `optimizer` | object | optimizer | see below | no|\n",
        "| `optimizer_control` | dict | control parameters | see below | no|\n",
        "\n",
        "* Besides these single parameters, the following parameter dictionaries can be specified by the user:\n",
        "  * `fun_control`\n",
        "  * `design_control`\n",
        "  * `surrogate_control`\n",
        "  * `optimizer_control`\n",
        "\n",
        "## The `fun_control` Dictionary\n",
        "\n",
        "|external parameter| type | description | default | mandatory |\n",
        "| -- | -- | -- |-- |-- |\n",
        "| `sigma` | float | noise: standard deviation | `0` | yes |\n",
        "| `seed` | int | seed for rng | `124` | yes |\n",
        "\n",
        "## The `design_control` Dictionary\n",
        "\n",
        "|external parameter| type | description | default | mandatory |\n",
        "| -- | -- | -- |-- |-- |\n",
        "| `init_size` | int | initial sample size | `10` | yes |\n",
        "| `repeats` | int | number of repeats of the initial sammples | `1` | yes |\n",
        "\n",
        "## The `surrogate_control` Dictionary\n",
        "\n",
        "|external parameter| type | description | default | mandatory |\n",
        "| -- | -- | -- |-- |-- |\n",
        "| `noise`\n",
        "| `model_optimizer` | object | optimizer | `differential_evolution` | no |\n",
        "| `model_fun_evals` | | | | |\n",
        "| `min_theta` | | | `-3.`| |\n",
        "| `max_theta` | | | `3.` | |\n",
        "| `n_theta`   | | | `1` | |\n",
        "| `n_p`       | | | `1` | | \n",
        "| `optim_p`   | | | `False` | |\n",
        "| `cod_type`  | | | `\"norm\"` | |\n",
        "| `var_type`  | | | | |\n",
        "| `use_cod_y` | bool | |`False`| |\n",
        "\n",
        "## The `optimizer_control` Dictionary\n",
        "\n",
        "|external parameter| type | description | default | mandatory |\n",
        "| -- | -- | -- |-- |-- |\n",
        "| `max_iter` | int | max number of iterations. Note: these are the cheap evaluations on the surrogate. | `1000` | no |\n",
        "\n",
        "## Run\n"
      ],
      "id": "01abac1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_1.run()"
      ],
      "id": "2d272c0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print the Results\n"
      ],
      "id": "03ed1ede"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_1.print_results()"
      ],
      "id": "4a6e44cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the Progress\n"
      ],
      "id": "d692741c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_1.plot_progress()"
      ],
      "id": "dc4a500e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Surrogate\n",
        "\n",
        "* The plot method of the `kriging` surrogate is used.\n",
        "* Note: the plot uses the interval defined by the ranges of the natural variables.\n"
      ],
      "id": "36c5d884"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_1.surrogate.plot()"
      ],
      "id": "568edc25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run With a Specific Start Design\n"
      ],
      "id": "19eb915a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_x0 = spot.Spot(fun=fun,\n",
        "                    fun_control=fun_control_init(\n",
        "                        lower = np.array([-10]),\n",
        "                        upper = np.array([100]),\n",
        "                        fun_evals = 7,\n",
        "                        fun_repeats = 1,\n",
        "                        max_time = inf,\n",
        "                        noise = False,\n",
        "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                        var_type=[\"num\"],\n",
        "                        infill_criterion = \"y\",\n",
        "                        n_points = 1,\n",
        "                        seed=123,\n",
        "                        log_level = 50),\n",
        "                    design_control=design_control_init(\n",
        "                        init_size=5,\n",
        "                        repeats=1),\n",
        "                    surrogate_control=surrogate_control_init(\n",
        "                        noise=False,\n",
        "                        min_theta=-4,\n",
        "                        max_theta=3,\n",
        "                        n_theta=1,\n",
        "                        model_optimizer=differential_evolution,\n",
        "                        model_fun_evals=10000))\n",
        "spot_x0.run(X_start=np.array([0.5, -0.5]))\n",
        "spot_x0.plot_progress()"
      ],
      "id": "b19a181f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Init: Build Initial Design\n"
      ],
      "id": "2e50ebbd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotPython.design.spacefilling import spacefilling\n",
        "from spotPython.build.kriging import Kriging\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "gen = spacefilling(2)\n",
        "rng = np.random.RandomState(1)\n",
        "lower = np.array([-5,-0])\n",
        "upper = np.array([10,15])\n",
        "fun = analytical().fun_branin\n",
        "fun_control = {\"sigma\": 0,\n",
        "               \"seed\": 123}\n",
        "\n",
        "X = gen.scipy_lhd(10, lower=lower, upper = upper)\n",
        "print(X)\n",
        "y = fun(X, fun_control=fun_control)\n",
        "print(y)"
      ],
      "id": "b45808eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replicability\n",
        "\n",
        "Seed\n"
      ],
      "id": "973d50da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gen = spacefilling(2, seed=123)\n",
        "X0 = gen.scipy_lhd(3)\n",
        "gen = spacefilling(2, seed=345)\n",
        "X1 = gen.scipy_lhd(3)\n",
        "X2 = gen.scipy_lhd(3)\n",
        "gen = spacefilling(2, seed=123)\n",
        "X3 = gen.scipy_lhd(3)\n",
        "X0, X1, X2, X3"
      ],
      "id": "e44eab35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Surrogates\n",
        "\n",
        "### A Simple Predictor\n",
        "\n",
        "The code below shows how to use a simple model for prediction. Assume that only two (very costly) measurements are available:\n",
        "  \n",
        "  1. f(0) = 0.5\n",
        "  2. f(2) = 2.5\n",
        "\n",
        "We are interested in the value at $x_0 = 1$, i.e., $f(x_0 = 1)$, but cannot run an additional, third experiment.\n"
      ],
      "id": "a250b6fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import linear_model\n",
        "X = np.array([[0], [2]])\n",
        "y = np.array([0.5, 2.5])\n",
        "S_lm = linear_model.LinearRegression()\n",
        "S_lm = S_lm.fit(X, y)\n",
        "X0 = np.array([[1]])\n",
        "y0 = S_lm.predict(X0)\n",
        "print(y0)"
      ],
      "id": "07dbb1ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Central Idea: Evaluation of the surrogate model `S_lm` is much cheaper (or / and much faster) than running the real-world experiment $f$.\n",
        "\n",
        "\n",
        "## Demo/Test: Objective Function Fails\n",
        "\n",
        "SPOT expects `np.nan` values from failed objective function values. These are handled. Note: SPOT's counter considers only successful executions of the objective function.\n"
      ],
      "id": "5599eac4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "import numpy as np\n",
        "from math import inf\n",
        "# number of initial points:\n",
        "ni = 20\n",
        "# number of points\n",
        "n = 30\n",
        "\n",
        "fun = analytical().fun_random_error\n",
        "fun_control=fun_control_init(\n",
        "    lower = np.array([-1]),\n",
        "    upper= np.array([1]),\n",
        "    fun_evals = n,\n",
        "    show_progress=False)\n",
        "design_control=design_control_init(init_size=ni)\n",
        "\n",
        "spot_1 = spot.Spot(fun=fun,\n",
        "                     fun_control=fun_control,\n",
        "                     design_control=design_control)\n",
        "spot_1.run()\n",
        "# To check whether the run was successfully completed,\n",
        "# we compare the number of evaluated points to the specified\n",
        "# number of points.\n",
        "assert spot_1.y.shape[0] == n"
      ],
      "id": "e571d0da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## PyTorch: Detailed Description of the Data Splitting {#sec-detailed-data-splitting}\n",
        "\n",
        "### Description of the `\"train_hold_out\"` Setting\n",
        "\n",
        "The `\"train_hold_out\"` setting is used by default. \n",
        "It uses the loss function specfied in `fun_control` and the metric specified in `fun_control`.\n",
        "\n",
        "1. First, the method `HyperTorch().fun_torch` is called. \n",
        "2. `fun_torc()`, which is implemented in the file `hypertorch.py`, calls `evaluate_hold_out()` as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "df_eval, _ = evaluate_hold_out(\n",
        "    model,\n",
        "    train_dataset=fun_control[\"train\"],\n",
        "    shuffle=self.fun_control[\"shuffle\"],\n",
        "    loss_function=self.fun_control[\"loss_function\"],\n",
        "    metric=self.fun_control[\"metric_torch\"],\n",
        "    device=self.fun_control[\"device\"],\n",
        "    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n",
        "    path=self.fun_control[\"path\"],\n",
        "    task=self.fun_control[\"task\"],\n",
        "    writer=self.fun_control[\"writer\"],\n",
        "    writerId=config_id,\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Note: Only the data set `fun_control[\"train\"]` is used for training and validation.\n",
        "It is used in `evaluate_hold_out` as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "trainloader, valloader = create_train_val_data_loaders(\n",
        "                dataset=train_dataset, batch_size=batch_size_instance, shuffle=shuffle\n",
        "            )\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "`create_train_val_data_loaders()` splits the `train_dataset` into `trainloader` and `valloader` using `torch.utils.data.random_split()` as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "def create_train_val_data_loaders(dataset, batch_size, shuffle, num_workers=0):\n",
        "    test_abs = int(len(dataset) * 0.6)\n",
        "    train_subset, val_subset = random_split(dataset, [test_abs, len(dataset) - test_abs])\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n",
        "    )\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n",
        "    )\n",
        "    return trainloader, valloader\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The optimizer is set up as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "optimizer_instance = net.optimizer\n",
        "lr_mult_instance = net.lr_mult\n",
        "sgd_momentum_instance = net.sgd_momentum\n",
        "optimizer = optimizer_handler(\n",
        "    optimizer_name=optimizer_instance,\n",
        "    params=net.parameters(),\n",
        "    lr_mult=lr_mult_instance,\n",
        "    sgd_momentum=sgd_momentum_instance,\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "3. `evaluate_hold_out()` sets the `net` attributes such as `epochs`, `batch_size`, `optimizer`, and `patience`. For each epoch, \n",
        "the methods `train_one_epoch()` and `validate_one_epoch()` are called, the former for training and the latter for validation and early stopping. The validation loss from the last epoch (not the best validation loss) is returned from `evaluate_hold_out`.\n",
        "4. The method `train_one_epoch()` is implemented as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "def train_one_epoch(\n",
        "    net,\n",
        "    trainloader,\n",
        "    batch_size,\n",
        "    loss_function,\n",
        "    optimizer,\n",
        "    device,\n",
        "    show_batch_interval=10_000,\n",
        "    task=None,\n",
        "):\n",
        "    running_loss = 0.0\n",
        "    epoch_steps = 0\n",
        "    for batch_nr, data in enumerate(trainloader, 0):\n",
        "        input, target = data\n",
        "        input, target = input.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = net(input)\n",
        "        if task == \"regression\":\n",
        "            target = target.unsqueeze(1)\n",
        "            if target.shape == output.shape:\n",
        "                loss = loss_function(output, target)\n",
        "            else:\n",
        "                raise ValueError(f\"Shapes of target and output do not match:\n",
        "                 {target.shape} vs {output.shape}\")\n",
        "        elif task == \"classification\":\n",
        "            loss = loss_function(output, target)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        epoch_steps += 1\n",
        "        if batch_nr % show_batch_interval == (show_batch_interval - 1):  \n",
        "            print(\n",
        "                \"Batch: %5d. Batch Size: %d. Training Loss (running): %.3f\"\n",
        "                % (batch_nr + 1, int(batch_size), running_loss / epoch_steps)\n",
        "            )\n",
        "            running_loss = 0.0\n",
        "    return loss.item()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "5. The method `validate_one_epoch()` is implemented as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "def validate_one_epoch(net, valloader, loss_function, metric, device, task):\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    metric.reset()\n",
        "    for i, data in enumerate(valloader, 0):\n",
        "        # get batches\n",
        "        with torch.no_grad():\n",
        "            input, target = data\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            output = net(input)\n",
        "            # print(f\"target: {target}\")\n",
        "            # print(f\"output: {output}\")\n",
        "            if task == \"regression\":\n",
        "                target = target.unsqueeze(1)\n",
        "                if target.shape == output.shape:\n",
        "                    loss = loss_function(output, target)\n",
        "                else:\n",
        "                    raise ValueError(f\"Shapes of target and output \n",
        "                        do not match: {target.shape} vs {output.shape}\")\n",
        "                metric_value = metric.update(output, target)\n",
        "            elif task == \"classification\":\n",
        "                loss = loss_function(output, target)\n",
        "                metric_value = metric.update(output, target)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown task: {task}\")\n",
        "            val_loss += loss.cpu().numpy()\n",
        "            val_steps += 1\n",
        "    loss = val_loss / val_steps\n",
        "    print(f\"Loss on hold-out set: {loss}\")\n",
        "    if task == \"classification\":\n",
        "        accuracy = correct / total\n",
        "        print(f\"Accuracy on hold-out set: {accuracy}\")\n",
        "    # metric on all batches using custom accumulation\n",
        "    metric_value = metric.compute()\n",
        "    metric_name = type(metric).__name__\n",
        "    print(f\"{metric_name} value on hold-out data: {metric_value}\")\n",
        "    return metric_value, loss\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### Description of the `\"test_hold_out\"` Setting\n",
        "\n",
        "It uses the loss function specfied in `fun_control` and the metric specified in `fun_control`.\n",
        "\n",
        "1. First, the method `HyperTorch().fun_torch` is called. \n",
        "2. `fun_torc()` calls `spotPython.torch.traintest.evaluate_hold_out()` similar to the `\"train_hold_out\"` setting with one exception:\n",
        "It passes an additional `test` data set to `evaluate_hold_out()` as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "test_dataset=fun_control[\"test\"]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "`evaluate_hold_out()` calls `create_train_test_data_loaders` instead of \n",
        "`create_train_val_data_loaders`:\n",
        "The two data sets are used in `create_train_test_data_loaders` as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "def create_train_test_data_loaders(dataset, batch_size, shuffle, test_dataset, \n",
        "        num_workers=0):\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=int(batch_size), shuffle=shuffle, \n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=int(batch_size), shuffle=shuffle, \n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return trainloader, testloader\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "3. The following steps are identical to the `\"train_hold_out\"` setting. Only a different data loader is used for testing.\n",
        "\n",
        "\n",
        "#### Detailed Description of the `\"train_cv\"` Setting\n",
        "\n",
        "It uses the loss function specfied in `fun_control` and the metric specified in `fun_control`.\n",
        "\n",
        "1. First, the method `HyperTorch().fun_torch` is called. \n",
        "2. `fun_torc()` calls `spotPython.torch.traintest.evaluate_cv()` as follows (Note: Only the data set `fun_control[\"train\"]` is used for CV.):\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "df_eval, _ = evaluate_cv(\n",
        "    model,\n",
        "    dataset=fun_control[\"train\"],\n",
        "    shuffle=self.fun_control[\"shuffle\"],\n",
        "    device=self.fun_control[\"device\"],\n",
        "    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n",
        "    task=self.fun_control[\"task\"],\n",
        "    writer=self.fun_control[\"writer\"],\n",
        "    writerId=config_id,\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "3. In `evaluate_cv(), the following steps are performed:\n",
        "The optimizer is set up as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "optimizer_instance = net.optimizer\n",
        "lr_instance = net.lr\n",
        "sgd_momentum_instance = net.sgd_momentum\n",
        "optimizer = optimizer_handler(optimizer_name=optimizer_instance,\n",
        "     params=net.parameters(), lr_mult=lr_mult_instance)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "`evaluate_cv()` sets the `net` attributes such as `epochs`, `batch_size`, `optimizer`, and `patience`. \n",
        "CV is implemented as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "def evaluate_cv(\n",
        "    net,\n",
        "    dataset,\n",
        "    shuffle=False,\n",
        "    loss_function=None,\n",
        "    num_workers=0,\n",
        "    device=None,\n",
        "    show_batch_interval=10_000,\n",
        "    metric=None,\n",
        "    path=None,\n",
        "    task=None,\n",
        "    writer=None,\n",
        "    writerId=None,\n",
        "):\n",
        "    lr_mult_instance = net.lr_mult\n",
        "    epochs_instance = net.epochs\n",
        "    batch_size_instance = net.batch_size\n",
        "    k_folds_instance = net.k_folds\n",
        "    optimizer_instance = net.optimizer\n",
        "    patience_instance = net.patience\n",
        "    sgd_momentum_instance = net.sgd_momentum\n",
        "    removed_attributes, net = get_removed_attributes_and_base_net(net)\n",
        "    metric_values = {}\n",
        "    loss_values = {}\n",
        "    try:\n",
        "        device = getDevice(device=device)\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda:0\"\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                print(\"We will use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "                net = nn.DataParallel(net)\n",
        "        net.to(device)\n",
        "        optimizer = optimizer_handler(\n",
        "            optimizer_name=optimizer_instance,\n",
        "            params=net.parameters(),\n",
        "            lr_mult=lr_mult_instance,\n",
        "            sgd_momentum=sgd_momentum_instance,\n",
        "        )\n",
        "        kfold = KFold(n_splits=k_folds_instance, shuffle=shuffle)\n",
        "        for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "            print(f\"Fold: {fold + 1}\")\n",
        "            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "            val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
        "            trainloader = torch.utils.data.DataLoader(\n",
        "                dataset, batch_size=batch_size_instance, \n",
        "                sampler=train_subsampler, num_workers=num_workers\n",
        "            )\n",
        "            valloader = torch.utils.data.DataLoader(\n",
        "                dataset, batch_size=batch_size_instance, \n",
        "                sampler=val_subsampler, num_workers=num_workers\n",
        "            )\n",
        "            # each fold starts with new weights:\n",
        "            reset_weights(net)\n",
        "            # Early stopping parameters\n",
        "            best_val_loss = float(\"inf\")\n",
        "            counter = 0\n",
        "            for epoch in range(epochs_instance):\n",
        "                print(f\"Epoch: {epoch + 1}\")\n",
        "                # training loss from one epoch:\n",
        "                training_loss = train_one_epoch(\n",
        "                    net=net,\n",
        "                    trainloader=trainloader,\n",
        "                    batch_size=batch_size_instance,\n",
        "                    loss_function=loss_function,\n",
        "                    optimizer=optimizer,\n",
        "                    device=device,\n",
        "                    show_batch_interval=show_batch_interval,\n",
        "                    task=task,\n",
        "                )\n",
        "                # Early stopping check. Calculate validation loss from one epoch:\n",
        "                metric_values[fold], loss_values[fold] = validate_one_epoch(\n",
        "                    net, valloader=valloader, loss_function=loss_function, \n",
        "                    metric=metric, device=device, task=task\n",
        "                )\n",
        "                # Log the running loss averaged per batch\n",
        "                metric_name = \"Metric\"\n",
        "                if metric is None:\n",
        "                    metric_name = type(metric).__name__\n",
        "                    print(f\"{metric_name} value on hold-out data: \n",
        "                        {metric_values[fold]}\")\n",
        "                if writer is not None:\n",
        "                    writer.add_scalars(\n",
        "                        \"evaluate_cv fold:\" + str(fold + 1) + \n",
        "                        \". Train & Val Loss and Val Metric\" + writerId,\n",
        "                        {\"Train loss\": training_loss, \"Val loss\": \n",
        "                        loss_values[fold], metric_name: metric_values[fold]},\n",
        "                        epoch + 1,\n",
        "                    )\n",
        "                    writer.flush()\n",
        "                if loss_values[fold] < best_val_loss:\n",
        "                    best_val_loss = loss_values[fold]\n",
        "                    counter = 0\n",
        "                    # save model:\n",
        "                    if path is not None:\n",
        "                        torch.save(net.state_dict(), path)\n",
        "                else:\n",
        "                    counter += 1\n",
        "                    if counter >= patience_instance:\n",
        "                        print(f\"Early stopping at epoch {epoch}\")\n",
        "                        break\n",
        "        df_eval = sum(loss_values.values()) / len(loss_values.values())\n",
        "        df_metrics = sum(metric_values.values()) / len(metric_values.values())\n",
        "        df_preds = np.nan\n",
        "    except Exception as err:\n",
        "        print(f\"Error in Net_Core. Call to evaluate_cv() failed. {err=}, \n",
        "            {type(err)=}\")\n",
        "        df_eval = np.nan\n",
        "        df_preds = np.nan\n",
        "    add_attributes(net, removed_attributes)\n",
        "    if writer is not None:\n",
        "        metric_name = \"Metric\"\n",
        "        if metric is None:\n",
        "            metric_name = type(metric).__name__\n",
        "        writer.add_scalars(\n",
        "            \"CV: Val Loss and Val Metric\" + writerId,\n",
        "            {\"CV-loss\": df_eval, metric_name: df_metrics},\n",
        "            epoch + 1,\n",
        "        )\n",
        "        writer.flush()\n",
        "    return df_eval, df_preds, df_metrics\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "4. The method `train_fold()` is implemented as shown above.\n",
        "\n",
        "5. The method `validate_one_epoch()` is implemented as shown above. In contrast to the hold-out setting, it is called for each of the $k$ folds. The results are stored in a dictionaries `metric_values`  and `loss_values`. \n",
        "The results are averaged over the $k$ folds and returned as `df_eval`.\n",
        "\n",
        "\n",
        "#### Detailed Description of the `\"test_cv\"` Setting\n",
        "\n",
        "It uses the loss function specfied in `fun_control` and the metric specified in `fun_control`.\n",
        "\n",
        "1. First, the method `HyperTorch().fun_torch` is called. \n",
        "2. `fun_torc()` calls `spotPython.torch.traintest.evaluate_cv()` as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "df_eval, _ = evaluate_cv(\n",
        "    model,\n",
        "    dataset=fun_control[\"test\"],\n",
        "    shuffle=self.fun_control[\"shuffle\"],\n",
        "    device=self.fun_control[\"device\"],\n",
        "    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n",
        "    task=self.fun_control[\"task\"],\n",
        "    writer=self.fun_control[\"writer\"],\n",
        "    writerId=config_id,\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Note: The data set `fun_control[\"test\"]` is used for CV. The rest is the same as for the `\"train_cv\"` setting.\n",
        "\n",
        "#### Detailed Description of the Final Model Training and Evaluation {#sec-final-model-evaluation}\n",
        "\n",
        "There are two methods that can be used for the final evaluation of a Pytorch model:\n",
        "\n",
        "  1. `\"train_tuned` and\n",
        "  2. `\"test_tuned\"`.\n",
        "\n",
        "`train_tuned()` is just a wrapper to `evaluate_hold_out` using the `train` data set. It is implemented as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "def train_tuned(\n",
        "    net,\n",
        "    train_dataset,\n",
        "    shuffle,\n",
        "    loss_function,\n",
        "    metric,\n",
        "    device=None,\n",
        "    show_batch_interval=10_000,\n",
        "    path=None,\n",
        "    task=None,\n",
        "    writer=None,\n",
        "):\n",
        "    evaluate_hold_out(\n",
        "        net=net,\n",
        "        train_dataset=train_dataset,\n",
        "        shuffle=shuffle,\n",
        "        test_dataset=None,\n",
        "        loss_function=loss_function,\n",
        "        metric=metric,\n",
        "        device=device,\n",
        "        show_batch_interval=show_batch_interval,\n",
        "        path=path,\n",
        "        task=task,\n",
        "        writer=writer,\n",
        "    )\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The `test_tuned()` procedure is implemented as follows:\n",
        "\n",
        "\n",
        "\n",
        "```{raw}\n",
        "def test_tuned(net, shuffle, test_dataset=None, loss_function=None,\n",
        "    metric=None, device=None, path=None, task=None):\n",
        "    batch_size_instance = net.batch_size\n",
        "    removed_attributes, net = get_removed_attributes_and_base_net(net)\n",
        "    if path is not None:\n",
        "        net.load_state_dict(torch.load(path))\n",
        "        net.eval()\n",
        "    try:\n",
        "        device = getDevice(device=device)\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda:0\"\n",
        "            if torch.cuda.device_count() > 1:\n",
        "                print(\"We will use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "                net = nn.DataParallel(net)\n",
        "        net.to(device)\n",
        "        valloader = torch.utils.data.DataLoader(\n",
        "            test_dataset, batch_size=int(batch_size_instance),\n",
        "            shuffle=shuffle, \n",
        "            num_workers=0\n",
        "        )\n",
        "        metric_value, loss = validate_one_epoch(\n",
        "            net, valloader=valloader, loss_function=loss_function,\n",
        "            metric=metric, device=device, task=task\n",
        "        )\n",
        "        df_eval = loss\n",
        "        df_metric = metric_value\n",
        "        df_preds = np.nan\n",
        "    except Exception as err:\n",
        "        print(f\"Error in Net_Core. Call to test_tuned() failed. {err=}, \n",
        "            {type(err)=}\")\n",
        "        df_eval = np.nan\n",
        "        df_metric = np.nan\n",
        "        df_preds = np.nan\n",
        "    add_attributes(net, removed_attributes)\n",
        "    print(f\"Final evaluation: Validation loss: {df_eval}\")\n",
        "    print(f\"Final evaluation: Validation metric: {df_metric}\")\n",
        "    print(\"----------------------------------------------\")\n",
        "    return df_eval, df_preds, df_metric\n",
        "```"
      ],
      "id": "9a27ed8e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}