{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "\n",
        "# Sampling Plans\n",
        "\n",
        "::: {.callout-note}\n",
        "### Note\n",
        "This section is based on chapter 1 in @Forr08a.\n",
        ":::\n",
        "\n",
        "## Ideas and Concepts\n",
        "\n",
        "\n",
        "::: {#def-sampling-plan}\n",
        "\n",
        "#### Sampling Plan\n",
        "In the context of computer experiments, the term sampling plan refers to the set of input values, say $X$,at which the computer code is evaluated.\n",
        "\n",
        ":::\n",
        "\n",
        "The goal of a sampling plan is to efficiently explore the input space to understand the behavior of the computer code and build a surrogate model that accurately represents the code's behavior.\n",
        "Traditionally, Response Surface Methodology (RSM) has been used to design sampling plans for computer experiments.\n",
        "These sampling plans are based on procedures that generate points by means of a rectangular grid or a factorial design.\n",
        "\n",
        "However, more recently, Design and Analysis of Computer Experiments (DACE) has emerged as a more flexible and powerful approach for designing sampling plans.\n",
        "\n",
        "Engineering design often requires the construction of a surrogate model $\\hat{f}$ to approximate the expensive response of a black-box function $f$. The function $f(x)$ represents a continuous metric (e.g., quality, cost, or performance) defined over a design space $D \\subset \\mathbb{R}^k$, where $x$ is a $k$-dimensional vector of design variables. Since evaluating $f$ is costly, only a sparse set of samples is used to construct $\\hat{f}$, which can then provide inexpensive predictions for any $x \\in D$.\n",
        "\n",
        "The process involves:\n",
        "\n",
        "* Sampling discrete observations:\n",
        "* Using these samples to construct an approximation $\\hat{f}$.\n",
        "* Ensuring the surrogate model is well-posed, meaning it is mathematically valid and can generalize predictions effectively.\n",
        "\n",
        "A sampling plan \n",
        "\n",
        "$$\n",
        "X =\n",
        "\\left\\{\n",
        "  x^{(i)} \\in D | i = 1, \\ldots, n \n",
        "\\right\\}\n",
        "$$\n",
        "\n",
        "determines the spatial arrangement of observations. While some models require a minimum number of data points $n$, once this threshold is met, a surrogate model can be constructed to approximate $f$ efficiently.\n",
        "\n",
        "A well-posed model does not always perform well because its ability to generalize depends heavily on the sampling plan used to collect data. If the sampling plan is poorly designed, the model may fail to capture critical behaviors in the design space. For example:\n",
        "\n",
        "* Extreme Sampling: Measuring performance only at the extreme values of parameters may miss important behaviors in the center of the design space, leading to incomplete understanding.\n",
        "* Uneven Sampling: Concentrating samples in certain regions while neglecting others forces the model to extrapolate over unsampled areas, potentially resulting in inaccurate or misleading predictions.\n",
        "Additionally, in some cases, the data may come from external sources or be limited in scope, leaving little control over the sampling plan. This can further restrict the model's ability to generalize effectively.\n",
        "\n",
        "\n",
        "### The 'Curse of Dimensionality' and How to Avoid It\n",
        "\n",
        "\n",
        "The \"curse of dimensionality\" refers to the exponential increase in computational complexity and data requirements as the number of dimensions (variables) in a problem grows.\n",
        "For a one-dimensional space, sampling $n$ locations may suffice for accurate predictions.\n",
        "In high-dimensional spaces, the amount of data needed to maintain the same level of accuracy or coverage increases dramatically. For example, if a one-dimensional space requires $n$ samples for a certain accuracy, a $k$-dimensional space would require $n^k$ samples. This makes tasks like optimization, sampling, and modeling computationally expensive and often impractical in high-dimensional settings.\n",
        "\n",
        "\n",
        "\n",
        "::: {#exm-curse-of-dim}\n",
        "#### Example: Curse of Dimensionality\n",
        "Consider a simple example where we want to model the cost of a car tire based on its wheel diameter. If we have one variable (wheel diameter), we might need 10 simulations to get a good estimate of the cost.\n",
        "Now, if we add 8 more variables (e.g., tread pattern, rubber type, etc.), the number of simulations required increases to $10^8$ (10 million). This is because the number of combinations of design variables grows exponentially with the number of dimensions.\n",
        "This means that the computational budget required to evaluate all combinations of design variables becomes infeasible. In this case, it would take 11,416 years to complete the simulations, making it impractical to explore the design space fully.\n",
        "\n",
        ":::\n",
        "\n",
        "### Physical versus Computational Experiments\n",
        "\n",
        "Physical experiments are prone to experimental errors from three main sources:\n",
        "\n",
        "* Human error: Mistakes made by the experimenter.\n",
        "* Random error: Measurement inaccuracies that vary unpredictably.\n",
        "* Systematic error: Consistent bias due to flaws in the experimental setup.\n",
        "\n",
        "The key distinction is repeatability: systematic errors remain constant across repetitions, while random errors vary.\n",
        "\n",
        "Computational experiments, on the other hand, are deterministic and free from random errors. However, they are still affected by:\n",
        "\n",
        "* Human error: Bugs in code or incorrect boundary conditions.\n",
        "* Systematic error: Biases from model simplifications (e.g., inviscid flow approximations) or finite resolution (e.g., insufficient mesh resolution).\n",
        "\n",
        "The term \"noise\" is used differently in physical and computational contexts. In physical experiments, it refers to random errors, while in computational experiments, it often refers to systematic errors.\n",
        "\n",
        "Understanding these differences is crucial for designing experiments and applying techniques like Gaussian process-based approximations. For physical experiments, replication mitigates random errors, but this is unnecessary for deterministic computational experiments.\n",
        "\n",
        "### Designing Preliminary Experiments (Screening)\n",
        "\n",
        "Minimizing the number of design variables $x_1, x_2, \\dots, x_k$ is crucial before modeling the objective function $f$. This process, called screening, aims to reduce dimensionality without compromising the analysis. If $f$ is at least once differentiable over the design domain $D$, the partial derivative $\\frac{\\partial f}{\\partial x_i}$ can be used to classify variables:\n",
        "\n",
        "* Negligible Variables:\n",
        "If $\\frac{\\partial f}{\\partial x_i} = 0, \\, \\forall x \\in D$, the variable $x_i$ can be safely neglected.\n",
        "* Linear Additive Variables:\n",
        "If $\\frac{\\partial f}{\\partial x_i} = \\text{constant} \\neq 0, \\, \\forall x \\in D$, the effect of $x_i$ is linear and additive.\n",
        "* Nonlinear Variables:\n",
        "If $\\frac{\\partial f}{\\partial x_i} = g(x_i), \\, \\forall x \\in D$, where $g(x_i)$ is a non-constant function, $f$ is nonlinear in $x_i$.\n",
        "* Interactive Nonlinear Variables:\n",
        "If $\\frac{\\partial f}{\\partial x_i} = g(x_i, x_j, \\dots), /, \\forall x \\in D$, where $g(x_i, x_j, \\dots)$ is a function involving interactions with other variables, $f$ is nonlinear in $x_i$ and interacts with $x_j$.\n",
        "\n",
        "\n",
        "Measuring $\\frac{\\partial f}{\\partial x_i}$ across the entire design space is often infeasible due to limited budgets.\n",
        "The percentage of time allocated to screening depends on the problem:\n",
        "If many variables are expected to be inactive, thorough screening can significantly improve model accuracy by reducing dimensionality.\n",
        "If most variables are believed to impact the objective, focus should shift to modeling instead.\n",
        "Screening is a trade-off between computational cost and model accuracy, and its effectiveness depends on the specific problem context.\n",
        "\n",
        "#### Estimating the Distribution of Elementary Effects\n",
        "\n",
        "In order to simplify the presentation of what follows, we make, without loss of generality, the assumption that the design space $D = [0, 1]^k$; that is, we normalize all variables into the unit cube. We shall adhere to this convention for the rest of the book and strongly urge the reader to do likewise when implementing any algorithms described here, as this step not only yields clearer mathematics in some cases but also safeguards against scaling issues.\n",
        "\n",
        "Before proceeding with the description of the Morris algorithm, we need to define an important statistical concept. Let us restrict our design space $D$ to a $k$-dimensional, $p$-level full factorial grid, that is, \n",
        "\n",
        "$$\n",
        "x_i \\in \\{0, \\frac{1}{p-1}, \\frac{2}{p-1}, \\dots, 1\\}, \\quad \\text{ for } i = 1, \\dots, k.\n",
        "$$\n",
        "\n",
        "For a given baseline value $x \\in D$, let $d_i(x)$ denote the elementary effect of $x_i$, where:\n",
        "\n",
        "$$\n",
        "d_i(x) = \\frac{f(x_1, \\dots, x_i + \\Delta, \\dots, x_k) - f(x_1, \\dots, x_i - \\Delta, \\dots, x_k)}{2\\Delta}, \\quad i = 1, \\dots, k,\n",
        "$$ {#eq-eleffect}\n",
        "where $\\Delta$ is the step size, which is defined as the distance between two adjacent levels in the grid. In other words, we have:\n",
        "\n",
        "with \n",
        "$$\\Delta = \\frac{\\xi}{p-1}, \\quad \\xi \\in \\mathbb{N}^*, \\quad \\text{and} \\quad x \\in D , \\text{ such that its components } x_i \\leq 1 - \\Delta.\n",
        "$$\n",
        "\n",
        "\n",
        "$\\Delta$ is the step size. The elementary effect $d_i(x)$ measures the sensitivity of the function $f$ to changes in the variable $x_i$ at the point $x$.\n",
        "\n",
        "\n",
        "Morris's method aims to estimate the parameters of the distribution of elementary effects associated with each variable. A large measure of central tendency indicates that a variable has a significant influence on the objective function across the design space, while a large measure of spread suggests that the variable is involved in interactions or contributes to the nonlinearity of $f$. In practice, the sample mean and standard deviation of a set of $d_i(x)$ values, calculated in different parts of the design space, are used for this estimation.\n",
        "\n",
        "To ensure efficiency, the preliminary sampling plan $X$ should be designed so that each evaluation of the objective function $f$ contributes to the calculation of two elementary effects, rather than just one (as would occur with a naive random spread of baseline $x$ values and adding $\\Delta$ to one variable). Additionally, the sampling plan should provide a specified number (e.g., $r$) of elementary effects for each variable, independently drawn with replacement. For a detailed discussion on constructing such a sampling plan, readers are encouraged to consult Morris's original paper (Morris, 1991). Here, we focus on describing the process itself.\n",
        "\n",
        "The random orientation of the sampling plan $B$ can be constructed as follows:\n",
        "* Let $B$ be a $(k+1) \\times k$ matrix of 0s and 1s, where for each column $i$, two rows differ only in their $i$-th entries.\n",
        "* Compute a random orientation of $B$, denoted $B^*$: \n",
        "\n",
        "$$\n",
        "B^* =\n",
        "\\left(\n",
        "1_{k+1,k} x^* + (\\Delta/2) \n",
        "\\left[\n",
        "(2B-1_{k+1,k})\n",
        "D^* +\n",
        "1_{k+1,k}\n",
        "\\right]\n",
        "\\right)\n",
        "P^*,\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $D^*$ is a $k$-dimensional diagonal matrix with diagonal elements $\\pm 1$ (equal probability),\n",
        "* $\\mathbf{1}$ is a matrix of 1s,\n",
        "* $x^*$ is a randomly chosen point in the $p$-level design space (limited by $\\Delta$),\n",
        "* $P^*$ is a $k \\times k$ random permutation matrix with one 1 per column and row.\n",
        "\n",
        "`spotpython` provides a `Python` implementation to compute $B^*$, see [https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py).\n",
        "\n",
        "Here is the corresponding code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def randorient(k, p, xi, seed=None):\n",
        "    # Initialize random number generator with the provided seed\n",
        "    if seed is not None:\n",
        "        rng = np.random.default_rng(seed)\n",
        "    else:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    # Step length\n",
        "    Delta = xi / (p - 1)\n",
        "\n",
        "    m = k + 1\n",
        "\n",
        "    # A truncated p-level grid in one dimension\n",
        "    xs = np.arange(0, 1 - Delta, 1 / (p - 1))\n",
        "    xsl = len(xs)\n",
        "    if xsl < 1:\n",
        "        print(f\"xi = {xi}.\")\n",
        "        print(f\"p = {p}.\")\n",
        "        print(f\"Delta = {Delta}.\")\n",
        "        print(f\"p - 1 = {p - 1}.\")\n",
        "        raise ValueError(f\"The number of levels xsl is {xsl}, but it must be greater than 0.\")\n",
        "\n",
        "    # Basic sampling matrix\n",
        "    B = np.vstack((np.zeros((1, k)), np.tril(np.ones((k, k)))))\n",
        "\n",
        "    # Randomization\n",
        "\n",
        "    # Matrix with +1s and -1s on the diagonal with equal probability\n",
        "    Dstar = np.diag(2 * rng.integers(0, 2, size=k) - 1)\n",
        "\n",
        "    # Random base value\n",
        "    xstar = xs[rng.integers(0, xsl, size=k)]\n",
        "\n",
        "    # Permutation matrix\n",
        "    Pstar = np.zeros((k, k))\n",
        "    rp = rng.permutation(k)\n",
        "    for i in range(k):\n",
        "        Pstar[i, rp[i]] = 1\n",
        "\n",
        "    # A random orientation of the sampling matrix\n",
        "    Bstar = (np.ones((m, 1)) @ xstar.reshape(1, -1) + (Delta / 2) * ((2 * B - np.ones((m, k))) @ Dstar + np.ones((m, k)))) @ Pstar\n",
        "\n",
        "    return Bstar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code following snippet generates a random orientation of a sampling matrix `Bstar` using the `randorient()` function. The input parameters are:\n",
        "\n",
        "* k = 3: The number of design variables (dimensions).\n",
        "* p = 3: The number of levels in the grid for each variable.\n",
        "* xi = 1: A parameter used to calculate the step size Delta.\n",
        "\n",
        "Step-size calculation is performed as follows:\n",
        "`Delta = xi / (p - 1) = 1 / (3 - 1) = 0.5`, which determines the spacing between levels in the grid.\n",
        "\n",
        "Next, random sampling matrix construction is computed:\n",
        "\n",
        "* A truncated grid is created with levels `[0, 0.5]` (based on Delta).\n",
        "* A basic sampling matrix B is constructed, which is a lower triangular matrix with 0s and 1s.\n",
        "\n",
        "Then, randomization is applied:\n",
        "\n",
        "* Dstar: A diagonal matrix with random entries of +1 or -1.\n",
        "* xstar: A random starting point from the grid.\n",
        "* Pstar: A random permutation matrix.\n",
        "\n",
        "Random orientation is applied to the basic sampling matrix B to create Bstar. This involves scaling, shifting, and permuting the rows and columns of B.\n",
        "\n",
        "The final output is the matrix `Bstar`, which represents a random orientation of the sampling plan. Each row corresponds to a sampled point in the design space, and each column corresponds to a design variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example usage\n",
        "k = 3\n",
        "p = 3\n",
        "xi = 1\n",
        "Bstar = randorient(k, p, xi)\n",
        "print(f\"Random orientation of the sampling matrix:\\n{Bstar}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To obtain $r$ elementary effects for each variable, the screening plan is built from $r$ random orientations:\n",
        "\n",
        "$$\n",
        "X = \n",
        "\\begin{pmatrix}\n",
        "B^*_1 \\\\\n",
        "B^*_2 \\\\\n",
        "\\vdots \\\\\n",
        "B^*_r\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "The screening plan implementation in `Python` is as follows (see [https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def screeningplan(k, p, xi, r):\n",
        "    # Empty list to accumulate screening plan rows\n",
        "    X = []\n",
        "\n",
        "    for i in range(r):\n",
        "        X.append(randorient(k, p, xi))\n",
        "\n",
        "    # Concatenate list of arrays into a single array\n",
        "    X = np.vstack(X)\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function `screeningplan()` generates a screening plan by calling the `randorient()` function `r` times. It creates a list of random orientations and then concatenates them into a single array, which represents the screening plan.\n",
        "It works like follows:\n",
        "\n",
        "* The value of the objective function $f$ is computed for each row of the screening plan matrix $X$.\n",
        "These values are stored in a column vector $t$ of size $(r * (k + 1)) \\times 1$, where:\n",
        "\n",
        "* r is the number of random orientations.\n",
        "* k is the number of design variables.\n",
        "\n",
        "\n",
        "The elementary effects are calculated using the following formula:\n",
        "\n",
        "* For each random orientation, adjacent rows of the screening plan matrix X and their corresponding function values from t are used.\n",
        "* These values are inserted into @eq-eleffect to compute elementary effects for each variable. An elementary effect measures the sensitivity of the objective function to changes in a specific variable.\n",
        "\n",
        "Results can be used for a statistical analysis.  After collecting a sample of $r$ elementary effects for each variable:\n",
        "\n",
        "* The sample mean (central tendency) is computed to indicate the overall influence of the variable.\n",
        "* The sample standard deviation (spread) is computed to capture variability, which may indicate interactions or nonlinearity.\n",
        "\n",
        "The results (sample means and standard deviations) are plotted on a chart for comparison.\n",
        "This helps identify which variables have the most significant impact on the objective function and whether their effects are linear or involve interactions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-forre08a-1-2\n",
        "#| fig-cap: \"Estimated means and standard deviations of the elementary effects for the 10 design variables of the wing weight function. Example based on @Forr08a.\"\n",
        "import numpy as np\n",
        "from spotpython.utils.effects import screening_plot, screeningplan\n",
        "from spotpython.fun.objectivefunctions import Analytical\n",
        "fun = Analytical()\n",
        "k = 10\n",
        "p = 10\n",
        "xi = 1\n",
        "r = 25\n",
        "X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n",
        "value_range = np.array([\n",
        "    [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n",
        "    [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n",
        "])\n",
        "labels = [\n",
        "    \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n",
        "    \"q\",   \"lambda\", \"tc\", \"N_z\",\n",
        "    \"W_dg\", \"W_p\"\n",
        "]\n",
        "screening_plot(\n",
        "    X=X,\n",
        "    fun=fun.fun_wingwt,\n",
        "    bounds=value_range,\n",
        "    xi=xi,\n",
        "    p=p,\n",
        "    labels=labels,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Designing a Sampling Plan\n",
        "\n",
        "### Stratification\n",
        "\n",
        "A feature shared by all of the approximation models discussed in @Forr08a is that they are more accurate in the vicinity of the points where we have evaluated the objective function. \n",
        "In later chapters we will delve into the laws that quantify our decaying trust in the model as we move away from a known, sampled point, but for the purposes of the present discussion we shall merely draw the intuitive conclusion that a uniform level of model accuracy throughout the design space requires a uniform spread of points. A sampling plan possessing this feature is said to be space-filling .\n",
        "\n",
        "The most straightforward way of sampling a design space in a uniform fashion is by means of a rectangular grid of points. This is the full factorial sampling technique.\n",
        "\n",
        "Here is the simplified version of a `Python` function that will sample the unit hypercube at all levels in all dimensions, with the $k$-vector $q$ containing the number of points required along each dimension, see \n",
        "[https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/sampling.py](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/sampling.py).\n",
        "\n",
        "The variable `Edges` specifies whether we want the points to be equally spaced from edge to edge (`Edges=1`)  or we want them to be in the centres of $n = q_1 \\times q_2 \\times \\ldots \\times q_k$ bins filling the unit hypercube (for any other value of `Edges`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from typing import Tuple, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def fullfactorial(q, Edges=1) -> np.ndarray:\n",
        "    \"\"\"Generates a full factorial sampling plan in the unit cube.\n",
        "\n",
        "    Args:\n",
        "        q (list or np.ndarray):\n",
        "            A list or array containing the number of points along each dimension (k-vector).\n",
        "        Edges (int, optional):\n",
        "            Determines spacing of points. If `Edges=1`, points are equally spaced from edge to edge (default).\n",
        "            Otherwise, points will be in the centers of n = q[0]*q[1]*...*q[k-1] bins filling the unit cube.\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray): Full factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any dimension in `q` is less than 2.\n",
        "\n",
        "    Examples:\n",
        "        >>> from spotpython.utils.sampling import fullfactorial\n",
        "        >>> q = [3, 2]\n",
        "        >>> X = fullfactorial(q, Edges=0)\n",
        "        >>> print(X)\n",
        "                [[0.         0.        ]\n",
        "                [0.         0.75      ]\n",
        "                [0.41666667 0.        ]\n",
        "                [0.41666667 0.75      ]\n",
        "                [0.83333333 0.        ]\n",
        "                [0.83333333 0.75      ]]\n",
        "        >>> X = fullfactorial(q, Edges=1)\n",
        "        >>> print(X)\n",
        "                [[0.  0. ]\n",
        "                [0.  1. ]\n",
        "                [0.5 0. ]\n",
        "                [0.5 1. ]\n",
        "                [1.  0. ]\n",
        "                [1.  1. ]]\n",
        "\n",
        "    \"\"\"\n",
        "    q = np.array(q)\n",
        "    if np.min(q) < 2:\n",
        "        raise ValueError(\"You must have at least two points per dimension.\")\n",
        "\n",
        "    # Total number of points in the sampling plan\n",
        "    n = np.prod(q)\n",
        "\n",
        "    # Number of dimensions\n",
        "    k = len(q)\n",
        "\n",
        "    # Pre-allocate memory for the sampling plan\n",
        "    X = np.zeros((n, k))\n",
        "\n",
        "    # Additional phantom element\n",
        "    q = np.append(q, 1)\n",
        "\n",
        "    for j in range(k):\n",
        "        if Edges == 1:\n",
        "            one_d_slice = np.linspace(0, 1, q[j])\n",
        "        else:\n",
        "            one_d_slice = np.linspace(1 / (2 * q[j]), 1, q[j]) - 1 / (2 * q[j])\n",
        "\n",
        "        column = np.array([])\n",
        "\n",
        "        while len(column) < n:\n",
        "            for ll in range(q[j]):\n",
        "                column = np.append(column, np.ones(np.prod(q[j + 1 : k])) * one_d_slice[ll])\n",
        "\n",
        "        X[:, j] = column\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.utils.sampling import fullfactorial\n",
        "q = [3, 2]\n",
        "X = fullfactorial(q, Edges=0)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = fullfactorial(q, Edges=1)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The full factorial sampling plan method generates a uniform sampling design by creating a grid of points across all dimensions. For example, calling fullfactorial([3, 4, 5], 1) produces a three-dimensional sampling plan with 3, 4, and 5 levels along each dimension, respectively. While this approach satisfies the uniformity criterion, it has two significant limitations:\n",
        "\n",
        "* Restricted Design Sizes: The method only works for designs where the total number of points $n$ can be expressed as the product of the number of levels in each dimension, i.e., $n = q_1 \\times q_2 \\times \\cdots \\times q_k$.\n",
        "\n",
        "* Overlapping Projections: When the sampling points are projected onto individual axes, sets of points may overlap, reducing the effectiveness of the sampling plan. This can lead to non-uniform coverage in the projections, which may not fully represent the design space.\n",
        "\n",
        "###  Latin Squares and Random Latin Hypercubes\n",
        "\n",
        "\n",
        "To improve the uniformity of projections for any individual variable, the range of that variable can be divided into a large number of equal-sized bins, and random subsamples of equal size can be generated within these bins. This method is called stratified random sampling. Extending this idea to all dimensions results in a stratified sampling plan, commonly implemented using Latin hypercube sampling.\n",
        "\n",
        "For two-dimensional discrete variables, a Latin square ensures uniform projections. An $(n \\times n)$ Latin square is constructed by filling each row and column with a permutation of $\\{1, 2, \\dots, n\\}$, ensuring each number appears only once per row and column. For example, for $n = 4 $, a Latin square might look like this:\n",
        "\n",
        "\n",
        "```{raw}\n",
        "2\t1\t3\t4\n",
        "3\t2\t4\t1\n",
        "1\t4\t2\t3\n",
        "4\t3\t1\t2\n",
        "```\n",
        "\n",
        "\n",
        "Latin Hypercubes are the multidimensional extension of Latin squares. The design space is divided into equal-sized hypercubes (bins), and one point is placed in each bin. The placement ensures that moving along any axis from an occupied bin does not encounter another occupied bin. This guarantees uniform projections across all dimensions.\n",
        "To construct a Latin hypercube:\n",
        "\n",
        "Represent the sampling plan as an ( n \\times k ) matrix ( X ), where ( n ) is the number of points and ( k ) is the number of dimensions.\n",
        "Fill each column of ( X ) with random permutations of ( {1, 2, \\dots, n} ).\n",
        "Normalize the plan into the unit hypercube ([0, 1]^k).\n",
        "This approach ensures multidimensional stratification and uniformity in projections.\n",
        "Here is the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rlh(n: int, k: int, edges: int = 0) -> np.ndarray:\n",
        "    # Initialize array\n",
        "    X = np.zeros((n, k), dtype=float)\n",
        "\n",
        "    # Fill with random permutations\n",
        "    for i in range(k):\n",
        "        X[:, i] = np.random.permutation(n)\n",
        "\n",
        "    # Adjust normalization based on the edges flag\n",
        "    if edges == 1:\n",
        "        # [X=0..n-1] -> [0..1]\n",
        "        X = X / (n - 1)\n",
        "    else:\n",
        "        # Points at true midpoints\n",
        "        # [X=0..n-1] -> [0.5/n..(n-0.5)/n]\n",
        "        X = (X + 0.5) / n\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import rlh\n",
        "# Generate a 2D Latin hypercube with 5 points and edges=0\n",
        "X = rlh(n=5, k=2, edges=0)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import rlh\n",
        "# Generate a 2D Latin hypercube with 5 points and edges=1\n",
        "X = rlh(n=5, k=2, edges=1)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from spotpython.utils.sampling import rlh\n",
        "\n",
        "# Generate a 2D Latin hypercube with 5 points and edges=0\n",
        "X = rlh(n=5, k=2, edges=0)\n",
        "\n",
        "# Plot the points\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], color='blue', s=50, label='Hypercube Points')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.title('2D Latin Hypercube Sampling (5 Points, Edges=0)')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Space-filling Latin Hypercubes\n",
        "\n",
        "\n",
        "#### Die Funktion `jd(X,p)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def jd(X: np.ndarray, p: float = 1.0) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes and counts the distinct p-norm distances between all pairs of points in X.\n",
        "    It returns:\n",
        "    1) A list of distinct distances (sorted), and\n",
        "    2) A corresponding multiplicity array that indicates how often each distance occurs.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray):\n",
        "            A 2D array of shape (n, d) representing n points in d-dimensional space.\n",
        "        p (float, optional):\n",
        "            The distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the\n",
        "            Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray, np.ndarray):\n",
        "            A tuple (J, distinct_d), where:\n",
        "            - distinct_d is a 1D float array of unique, sorted distances between points.\n",
        "            - J is a 1D integer array that provides the multiplicity (occurrence count)\n",
        "              of each distance in distinct_d.\n",
        "    \"\"\"\n",
        "    n = X.shape[0]\n",
        "\n",
        "    # Allocate enough space for all pairwise distances\n",
        "    # (n*(n-1))/2 pairs for an n-point set\n",
        "    pair_count = n * (n - 1) // 2\n",
        "    d = np.zeros(pair_count, dtype=float)\n",
        "\n",
        "    # Fill the distance array\n",
        "    idx = 0\n",
        "    for i in range(n - 1):\n",
        "        for j in range(i + 1, n):\n",
        "            # Compute the p-norm distance\n",
        "            d[idx] = np.linalg.norm(X[i] - X[j], ord=p)\n",
        "            idx += 1\n",
        "\n",
        "    # Find unique distances and their multiplicities\n",
        "    distinct_d = np.unique(d)\n",
        "    J = np.zeros_like(distinct_d, dtype=int)\n",
        "    for i, val in enumerate(distinct_d):\n",
        "        J[i] = np.sum(d == val)\n",
        "\n",
        "    return J, distinct_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Die Funktion `mm(X1, X2,p)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import jd\n",
        "# A small 3-point set in 2D\n",
        "X = np.array([[0.0, 0.0],[1.0, 1.0],[2.0, 2.0]])\n",
        "J, distinct_d = jd(X, p=2.0)\n",
        "print(\"Distinct distances (d_i):\", distinct_d)\n",
        "print(\"Occurrences (J_i):\", J)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def mm(X1: np.ndarray, X2: np.ndarray, p: Optional[float] = 1.0) -> int:\n",
        "    \"\"\"\n",
        "    Determines which of two sampling plans has better space-filling properties\n",
        "    according to the Morris-Mitchell criterion.\n",
        "\n",
        "    Args:\n",
        "        X1 (np.ndarray): A 2D array representing the first sampling plan.\n",
        "        X2 (np.ndarray): A 2D array representing the second sampling plan.\n",
        "        p (float, optional): The distance metric. p=1 uses Manhattan (L1) distance,\n",
        "            while p=2 uses Euclidean (L2). Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        int:\n",
        "            - 0 if both plans are identical or equally space-filling\n",
        "            - 1 if X1 is more space-filling\n",
        "            - 2 if X2 is more space-filling\n",
        "    \"\"\"\n",
        "    X1_sorted = X1[np.lexsort(np.rot90(X1))]\n",
        "    X2_sorted = X2[np.lexsort(np.rot90(X2))]\n",
        "    if np.array_equal(X1_sorted, X2_sorted):\n",
        "        return 0  # Identical sampling plans\n",
        "\n",
        "    # Compute distance multiplicities for each plan\n",
        "    J1, d1 = jd(X1, p)\n",
        "    J2, d2 = jd(X2, p)\n",
        "    m1, m2 = len(d1), len(d2)\n",
        "\n",
        "    # Construct V1 and V2: alternate distance and negative multiplicity\n",
        "    V1 = np.zeros(2 * m1)\n",
        "    V1[0::2] = d1\n",
        "    V1[1::2] = -J1\n",
        "\n",
        "    V2 = np.zeros(2 * m2)\n",
        "    V2[0::2] = d2\n",
        "    V2[1::2] = -J2\n",
        "\n",
        "    # Trim the longer vector to match the size of the shorter\n",
        "    m = min(m1, m2)\n",
        "    V1 = V1[:m]\n",
        "    V2 = V2[:m]\n",
        "\n",
        "    # Compare element-by-element:\n",
        "    # c[i] = 1 if V1[i] > V2[i], 2 if V1[i] < V2[i], 0 otherwise.\n",
        "    c = (V1 > V2).astype(int) + 2 * (V1 < V2).astype(int)\n",
        "\n",
        "    if np.sum(c) == 0:\n",
        "        # Equally space-filling\n",
        "        return 0\n",
        "    else:\n",
        "        # The first non-zero entry indicates which plan is better\n",
        "        idx = np.argmax(c != 0)\n",
        "        return c[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from spotpython.utils.sampling import mm\n",
        "# Create two 3-point sampling plans in 2D\n",
        "X1 = np.array([[0.0, 0.0],[0.5, 0.5],[0.0, 1.0], [1.0, 1.0]])\n",
        "X2 = np.array([[0.1, 0.1],[0.4, 0.6],[0.1, 0.9], [0.9, 0.9]])\n",
        "# plot the two plans\n",
        "plt.scatter(X1[:, 0], X1[:, 1], color='blue', label='Plan 1')\n",
        "plt.scatter(X2[:, 0], X2[:, 1], color='red', label='Plan 2')\n",
        "plt.title('Comparison of Two Sampling Plans')\n",
        "plt.grid()\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Compare which plan has better space-filling (Morris-Mitchell)\n",
        "better = mm(X1, X2, p=2.0)\n",
        "print(better)\n",
        "# Prints either 0, 1, or 2 depending on which plan is more space-filling."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Die Funktion `mmphi(X,q,p)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def mmphi(X: np.ndarray, q: Optional[float] = 2.0, p: Optional[float] = 1.0) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Morris-Mitchell sampling plan quality criterion.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray):\n",
        "            A 2D array representing the sampling plan, where each row is a point in\n",
        "            d-dimensional space (shape: (n, d)).\n",
        "        q (float, optional):\n",
        "            Exponent used in the computation of the metric. Defaults to 2.0.\n",
        "        p (float, optional):\n",
        "            The distance norm to use. For example, p=1 is Manhattan (L1),\n",
        "            p=2 is Euclidean (L2). Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        float:\n",
        "            The space-fillingness metric Phiq. Larger values typically indicate a more\n",
        "            space-filling plan according to the Morris-Mitchell criterion.\n",
        "    \"\"\"\n",
        "    # Compute the distance multiplicities: J, and unique distances: d\n",
        "    J, d = jd(X, p)\n",
        "\n",
        "    # Summation of J[i] * d[i]^(-q), then raised to 1/q\n",
        "    # This follows the Morris-Mitchell definition.\n",
        "    Phiq = np.sum(J * (d ** (-q))) ** (1.0 / q)\n",
        "    return Phiq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import mmphi\n",
        "# Two simple sampling plans from above\n",
        "quality1 = mmphi(X1, q=2, p=2)\n",
        "quality2 = mmphi(X2, q=2, p=2)\n",
        "print(f\"Quality of sampling plan X1:  {quality1}\")\n",
        "print(f\"Quality of sampling plan X2:  {quality2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Die Funktion `mmsort(X3D,p)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def mmsort(X3D: np.ndarray, p: Optional[float] = 1.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Ranks multiple sampling plans stored in a 3D array according to the\n",
        "    Morris-Mitchell criterion, using a simple bubble sort.\n",
        "\n",
        "    Args:\n",
        "        X3D (np.ndarray):\n",
        "            A 3D NumPy array of shape (n, d, m), where m is the number of\n",
        "            sampling plans, and each plan is an (n, d) matrix of points.\n",
        "        p (float, optional):\n",
        "            The distance metric to use. p=1 for Manhattan (L1), p=2 for\n",
        "            Euclidean (L2). Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            A 1D integer array of length m that holds the plan indices in\n",
        "            ascending order of space-filling quality. The first index in the\n",
        "            returned array corresponds to the most space-filling plan.\n",
        "\n",
        "    Notes:\n",
        "        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n",
        "        \"This program is free software: you can redistribute it and/or modify  it\n",
        "        under the terms of the GNU Lesser General Public License as published by\n",
        "        the Free Software Foundation, either version 3 of the License, or any\n",
        "        later version.\n",
        "        This program is distributed in the hope that it will be useful, but\n",
        "        WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n",
        "        General Public License for more details.\n",
        "        You should have received a copy of the GNU General Public License and GNU\n",
        "        Lesser General Public License along with this program. If not, see\n",
        "        <http://www.gnu.org/licenses/>.\"\n",
        "    \"\"\"\n",
        "    # Number of plans (m)\n",
        "    m = X3D.shape[2]\n",
        "\n",
        "    # Create index array (1-based to match original MATLAB convention)\n",
        "    Index = np.arange(1, m + 1)\n",
        "\n",
        "    swap_flag = True\n",
        "    while swap_flag:\n",
        "        swap_flag = False\n",
        "        i = 0\n",
        "        while i < m - 1:\n",
        "            # Compare plan at Index[i] vs. Index[i+1] using mm()\n",
        "            # Note: subtract 1 from each index to convert to 0-based array indexing\n",
        "            if mm(X3D[:, :, Index[i] - 1], X3D[:, :, Index[i + 1] - 1], p) == 2:\n",
        "                # Swap indices if the second plan is more space-filling\n",
        "                Index[i], Index[i + 1] = Index[i + 1], Index[i]\n",
        "                swap_flag = True\n",
        "            i += 1\n",
        "\n",
        "    return Index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import mmsort\n",
        "# Suppose we have two 3-point sampling plans X1 and X1 from above\n",
        "X3D = np.stack([X1, X2], axis=2)\n",
        "# Sort them using the Morris-Mitchell criterion with p=2\n",
        "ranking = mmsort(X3D, p=2.0)\n",
        "print(ranking)\n",
        "# It prints [1, 2] indicating that X1 is more space-filling than X2."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Die Funktion `perturb(X,PertNum=1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def perturb(X: np.ndarray, PertNum: Optional[int] = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs a specified number of random element swaps on a sampling plan.\n",
        "    If the plan is a Latin hypercube, the result remains a valid Latin hypercube.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray):\n",
        "            A 2D array (sampling plan) of shape (n, k), where each row is a point\n",
        "            and each column is a dimension.\n",
        "        PertNum (int, optional):\n",
        "            The number of element swaps (perturbations) to perform. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            The perturbed sampling plan, identical in shape to the input, with\n",
        "            one or more random column swaps executed.\n",
        "    \"\"\"\n",
        "    # Get dimensions of the plan\n",
        "    n, k = X.shape\n",
        "    if n < 2 or k < 2:\n",
        "        raise ValueError(\"Latin hypercubes require at least 2 points and 2 dimensions\")\n",
        "\n",
        "    for _ in range(PertNum):\n",
        "        # Pick a random column\n",
        "        col = int(np.floor(np.random.rand() * k))\n",
        "\n",
        "        # Pick two distinct row indices\n",
        "        el1, el2 = 0, 0\n",
        "        while el1 == el2:\n",
        "            el1 = int(np.floor(np.random.rand() * n))\n",
        "            el2 = int(np.floor(np.random.rand() * n))\n",
        "\n",
        "        # Swap the two selected elements in the chosen column\n",
        "        X[el1, col], X[el2, col] = X[el2, col], X[el1, col]\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import perturb\n",
        "# Create a simple 4x2 sampling plan\n",
        "X_original = np.array([[1, 3],[2, 4],[3, 1],[4, 2]])\n",
        "print(\"Original Sampling Plan:\")\n",
        "print(X_original)\n",
        "print(\"Perturbed Sampling Plan:\")\n",
        "X_perturbed = perturb(X_original, PertNum=1)\n",
        "print(X_perturbed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The function `mmlhs(X_start, population, iterations, q=2.0, plot=False)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def mmlhs(X_start: np.ndarray, population: int, iterations: int, q: Optional[float] = 2.0, plot=False) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs an evolutionary search (using perturbations) to find a Morris-Mitchell\n",
        "    optimal Latin hypercube, starting from an initial plan X_start.\n",
        "\n",
        "    This function does the following:\n",
        "      1. Initializes a \"best\" Latin hypercube (X_best) from the provided X_start.\n",
        "      2. Iteratively perturbs X_best to create offspring.\n",
        "      3. Evaluates the space-fillingness of each offspring via the Morris-Mitchell\n",
        "         metric (using mmphi).\n",
        "      4. Updates the best plan whenever a better offspring is found.\n",
        "\n",
        "    Args:\n",
        "        X_start (np.ndarray):\n",
        "            A 2D array of shape (n, k) providing the initial Latin hypercube\n",
        "            (n points in k dimensions).\n",
        "        population (int):\n",
        "            Number of offspring to create in each generation.\n",
        "        iterations (int):\n",
        "            Total number of generations to run the evolutionary search.\n",
        "        q (float, optional):\n",
        "            The exponent used by the Morris-Mitchell space-filling criterion.\n",
        "            Defaults to 2.0.\n",
        "        plot (bool, optional):\n",
        "            If True, a simple scatter plot of the first two dimensions will be\n",
        "            displayed at each iteration. Only if k >= 2. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            A 2D array representing the most space-filling Latin hypercube found\n",
        "            after all iterations, of the same shape as X_start.\n",
        "    \"\"\"\n",
        "    n = X_start.shape[0]\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Latin hypercubes require at least 2 points\")\n",
        "    k = X_start.shape[1]\n",
        "    if k < 2:\n",
        "        raise ValueError(\"Latin hypercubes are not defined for dim k < 2\")\n",
        "\n",
        "    # Initialize best plan and its metric\n",
        "    X_best = X_start.copy()\n",
        "    Phi_best = mmphi(X_best, q=q)\n",
        "\n",
        "    # After 85% of iterations, reduce the mutation rate to 1\n",
        "    leveloff = int(np.floor(0.85 * iterations))\n",
        "\n",
        "    for it in range(1, iterations + 1):\n",
        "        # Decrease number of mutations over time\n",
        "        if it < leveloff:\n",
        "            mutations = int(round(1 + (0.5 * n - 1) * (leveloff - it) / (leveloff - 1)))\n",
        "        else:\n",
        "            mutations = 1\n",
        "\n",
        "        X_improved = X_best.copy()\n",
        "        Phi_improved = Phi_best\n",
        "\n",
        "        # Create offspring, evaluate, and keep the best\n",
        "        for _ in range(population):\n",
        "            X_try = perturb(X_best.copy(), mutations)\n",
        "            Phi_try = mmphi(X_try, q=q)\n",
        "\n",
        "            if Phi_try < Phi_improved:\n",
        "                X_improved = X_try\n",
        "                Phi_improved = Phi_try\n",
        "\n",
        "        # Update the global best if we found a better plan\n",
        "        if Phi_improved < Phi_best:\n",
        "            X_best = X_improved\n",
        "            Phi_best = Phi_improved\n",
        "\n",
        "        # Simple visualization of the first two dimensions\n",
        "        if plot and (X_best.shape[1] >= 2):\n",
        "            plt.clf()\n",
        "            plt.scatter(X_best[:, 0], X_best[:, 1], marker=\"o\")\n",
        "            plt.grid(True)\n",
        "            plt.title(f\"Iteration {it} - Current Best Plan\")\n",
        "            plt.pause(0.01)\n",
        "\n",
        "    return X_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import mmlhs\n",
        "# Suppose we have an initial 4x2 plan\n",
        "X_start = np.array([[0.1, 0.3],[.1, .4],[.2, .9],[.9, .2]])\n",
        "print(\"Initial plan:\")\n",
        "print(X_start)\n",
        "# Search for a more space-filling plan\n",
        "X_opt = mmlhs(X_start, population=10, iterations=100, q=2)\n",
        "print(\"Optimized plan:\")\n",
        "print(X_opt)\n",
        "# Plot the initial and optimized plans\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X_start[:, 0], X_start[:, 1], color='blue', label='Initial Plan')\n",
        "plt.scatter(X_opt[:, 0], X_opt[:, 1], color='red', label='Optimized Plan')\n",
        "plt.title('Comparison of Initial and Optimized Plans')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.grid()\n",
        "# plot legend outside the plot\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Die Funktion `bestlh()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def bestlh(n: int, k: int, population: int, iterations: int, p=1, plot=False, verbosity=0, edges=0, q_list=[1, 2, 5, 10, 20, 50, 100]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates an optimized Latin hypercube by evolving the Morris-Mitchell\n",
        "    criterion across multiple exponents (q values) and selecting the best plan.\n",
        "\n",
        "    Args:\n",
        "        n (int):\n",
        "            Number of points required in the Latin hypercube.\n",
        "        k (int):\n",
        "            Number of design variables (dimensions).\n",
        "        population (int):\n",
        "            Number of offspring in each generation of the evolutionary search.\n",
        "        iterations (int):\n",
        "            Number of generations for the evolutionary search.\n",
        "        p (int, optional):\n",
        "            The distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2).\n",
        "            Defaults to 1 (faster than 2).\n",
        "        plot (bool, optional):\n",
        "            If True, a scatter plot of the optimized plan in the first two dimensions\n",
        "            will be displayed. Only if k>=2.  Defaults to False.\n",
        "        verbosity (int, optional):\n",
        "            Verbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.\n",
        "        edges (int, optional):\n",
        "            If 1, places centers of the extreme bins at the domain edges ([0,1]).\n",
        "            Otherwise, bins are fully contained within the domain, i.e. midpoints.\n",
        "            Defaults to 0.\n",
        "        q_list (list, optional):\n",
        "            A list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100].\n",
        "            These values are used to evaluate the space-fillingness of the Latin\n",
        "            hypercube. The best plan is selected based on the lowest mmphi value.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            A 2D array of shape (n, k) representing an optimized Latin hypercube.\n",
        "\n",
        "    Notes:\n",
        "        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n",
        "        \"This program is free software: you can redistribute it and/or modify  it\n",
        "        under the terms of the GNU Lesser General Public License as published by\n",
        "        the Free Software Foundation, either version 3 of the License, or any\n",
        "        later version.\n",
        "        This program is distributed in the hope that it will be useful, but\n",
        "        WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n",
        "        General Public License for more details.\n",
        "        You should have received a copy of the GNU General Public License and GNU\n",
        "        Lesser General Public License along with this program. If not, see\n",
        "        <http://www.gnu.org/licenses/>.\"\n",
        "    \"\"\"\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Latin hypercubes require at least 2 points\")\n",
        "    if k < 2:\n",
        "        raise ValueError(\"Latin hypercubes are not defined for dim k < 2\")\n",
        "\n",
        "    # A list of exponents (q) to optimize\n",
        "\n",
        "    # Start with a random Latin hypercube\n",
        "    X_start = rlh(n, k, edges=edges)\n",
        "\n",
        "    # Allocate a 3D array to store the results for each q\n",
        "    # (shape: (n, k, number_of_q_values))\n",
        "    X3D = np.zeros((n, k, len(q_list)))\n",
        "\n",
        "    # Evolve the plan for each q in q_list\n",
        "    for i, q_val in enumerate(q_list):\n",
        "        if verbosity > 0:\n",
        "            print(f\"Now optimizing for q={q_val}...\")\n",
        "        X3D[:, :, i] = mmlhs(X_start, population, iterations, q_val)\n",
        "\n",
        "    # Sort the set of evolved plans according to the Morris-Mitchell criterion\n",
        "    index_order = mmsort(X3D, p=p)\n",
        "\n",
        "    # index_order is a 1-based array of plan indices; the first element is the best\n",
        "    best_idx = index_order[0] - 1\n",
        "    if verbosity > 0:\n",
        "        print(f\"Best lh found using q={q_list[best_idx]}...\")\n",
        "\n",
        "    # The best plan in 3D array order\n",
        "    X = X3D[:, :, best_idx]\n",
        "\n",
        "    # Plot the first two dimensions\n",
        "    if plot and (k >= 2):\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=\"r\", marker=\"o\")\n",
        "        plt.title(f\"Morris-Mitchell optimum plan found using q={q_list[best_idx]}\")\n",
        "        plt.xlabel(\"x_1\")\n",
        "        plt.ylabel(\"x_2\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import bestlh\n",
        "Xbestlh= bestlh(n=5, k=2, population=5, iterations=10)\n",
        "# plot the bestlh\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(Xbestlh[:, 0], Xbestlh[:, 1], color='red', label='Best Latin Hypercube')\n",
        "plt.title('Best Latin Hypercube Sampling')\n",
        "plt.grid()\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "# plot legend outside the plot\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Die Funktion `phisort(X3D,q,p)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def phisort(X3D: np.ndarray, q: Optional[float] = 2.0, p: Optional[float] = 1.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Ranks multiple sampling plans stored in a 3D array by the Morris-Mitchell\n",
        "    numerical quality metric (mmphi). Uses a simple bubble-sort:\n",
        "    sampling plans with smaller mmphi values are placed first in the index array.\n",
        "\n",
        "    Args:\n",
        "        X3D (np.ndarray):\n",
        "            A 3D array of shape (n, d, m), where m is the number of sampling plans.\n",
        "        q (float, optional):\n",
        "            Exponent for the mmphi metric. Defaults to 2.0.\n",
        "        p (float, optional):\n",
        "            Distance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            A 1D integer array of length m, giving the plan indices in ascending\n",
        "            order of mmphi. The first index in the returned array corresponds\n",
        "            to the numerically lowest mmphi value.\n",
        "    \"\"\"\n",
        "    # Number of 2D sampling plans\n",
        "    m = X3D.shape[2]\n",
        "\n",
        "    # Create a 1-based index array\n",
        "    Index = np.arange(1, m + 1)\n",
        "\n",
        "    # Bubble-sort: plan with lower mmphi() climbs toward the front\n",
        "    swap_flag = True\n",
        "    while swap_flag:\n",
        "        swap_flag = False\n",
        "        for i in range(m - 1):\n",
        "            # Retrieve mmphi values for consecutive plans\n",
        "            val_i = mmphi(X3D[:, :, Index[i] - 1], q=q, p=p)\n",
        "            val_j = mmphi(X3D[:, :, Index[i + 1] - 1], q=q, p=p)\n",
        "\n",
        "            # Swap if the left plan's mmphi is larger (i.e. 'worse')\n",
        "            if val_i > val_j:\n",
        "                Index[i], Index[i + 1] = Index[i + 1], Index[i]\n",
        "                swap_flag = True\n",
        "\n",
        "    return Index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.sampling import phisort\n",
        "X1 = bestlh(n=5, k=2, population=5, iterations=10)\n",
        "X2 = bestlh(n=5, k=2, population=15, iterations=20)\n",
        "X3 = bestlh(n=5, k=2, population=25, iterations=30)\n",
        "# Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n",
        "X3D = np.array([X1, X2])\n",
        "print(phisort(X3D))\n",
        "X3D = np.array([X3, X2])\n",
        "print(phisort(X3D))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-important}\n",
        "### Goals\n",
        "\n",
        "* How to choose models and optimizers for solving real-world problems\n",
        "* How to use simulation to understand and improve processes\n",
        ":::\n",
        "\n",
        "## Jupyter Notebook\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "* The Jupyter-Notebook of this lecture is available on GitHub in the [Hyperparameter-Tuning-Cookbook Repository](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/001_optimization_surrogate.ipynb)\n",
        "\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}