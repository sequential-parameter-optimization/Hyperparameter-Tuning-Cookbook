{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: false\n",
        "  echo: true\n",
        "  warning: false\n",
        "title: User Specified Basic Lightning Module With spotpython\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This chapter implements a user-defined DataModule and a user-defined neural network.\n",
        "Remember, that a `LightningModule` organizes your `PyTorch` code into six sections:\n",
        "\n",
        "* Initialization (`__init__` and `setup()`).\n",
        "* Train Loop (`training_step()`)\n",
        "* Validation Loop (`validation_step()`)\n",
        "* Test Loop (`test_step()`)\n",
        "* Prediction Loop (`predict_step()`)\n",
        "* Optimizers and LR Schedulers (`configure_optimizers()`)\n",
        "\n",
        "The `Trainer` automates every required step in a clear and reproducible way. It is the most important part of PyTorch Lightning. It is responsible for training, testing, and validating the model.\n",
        "The `Lightning` core structure looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "df = pd.read_pickle(\"./userData/Turbo_Charger_Data.pkl\")\n",
        "df = df.drop(columns=[\"M\", \"R\"])\n",
        "print(f\"Features des DataFrames: {df.columns}\")\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from lightning import LightningDataModule\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class UserDataset(Dataset):\n",
        "    def __init__(self, data, y_varname=\"N\", x_varnames=None, dtype=torch.float32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (pd.DataFrame):\n",
        "                The user data. for example,\n",
        "                generated by the `preprocess_data` function.\n",
        "            y_varname (str):\n",
        "                The name of the target variable.\n",
        "                Default is \"N\".\n",
        "            x_varnames (list):\n",
        "                The names of the input variables.\n",
        "                Default is `None`, which means all columns\n",
        "                except the target variable are used.\n",
        "            dtype (torch.dtype):\n",
        "                The data type for the tensors.\n",
        "                Default is `torch.float32`.\n",
        "\n",
        "        Examples:\n",
        "            >>> dataset = UserDataset(data)\n",
        "            >>> x, y = dataset[0]\n",
        "        \"\"\"\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        if x_varnames is not None:\n",
        "            self.x_varnames = x_varnames\n",
        "        else:\n",
        "            self.x_varnames = [col for col in self.data.columns if col != y_varname]\n",
        "        print(f\"X variables: {self.x_varnames}\")\n",
        "        print(f\"Y variable: {y_varname}\")\n",
        "        self.y_varname = y_varname\n",
        "        self.dtype = dtype\n",
        "        self.encoders = {}\n",
        "\n",
        "        for var in self.x_varnames:\n",
        "            if self.data[var].dtype == \"object\" or isinstance(self.data[var][0], str):\n",
        "                le = LabelEncoder()\n",
        "                self.data[var] = le.fit_transform(self.data[var])\n",
        "                self.encoders[var] = le\n",
        "\n",
        "        if self.data[self.y_varname].dtype == \"object\" or isinstance(self.data[self.y_varname][0], str):\n",
        "            le = LabelEncoder()\n",
        "            self.data[self.y_varname] = le.fit_transform(self.data[self.y_varname])\n",
        "            self.encoders[self.y_varname] = le\n",
        "\n",
        "        # Convert entire dataset to tensors\n",
        "        self.features = torch.tensor(self.data[self.x_varnames].values, dtype=self.dtype)\n",
        "        self.targets = torch.tensor(self.data[self.y_varname].values, dtype=self.dtype)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = UserDataset(df)\n",
        "x, y = dataset[0]\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataModule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import lightning as L\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from typing import Optional\n",
        "from math import floor\n",
        "\n",
        "\n",
        "class LightDataModule(L.LightningDataModule):\n",
        "    \"\"\"\n",
        "    A LightningDataModule for handling data.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int):\n",
        "            The batch size. Required.\n",
        "        dataset (torch.utils.data.Dataset, optional):\n",
        "            The dataset from the torch.utils.data Dataset class.\n",
        "            It must implement three functions: __init__, __len__, and __getitem__.\n",
        "        data_full_train (torch.utils.data.Dataset, optional):\n",
        "            The full training dataset from which training and validation sets will be derived.\n",
        "        data_test (torch.utils.data.Dataset, optional):\n",
        "            The separate test dataset that will be used for testing.\n",
        "        test_size (float, optional):\n",
        "            The test size. If test_size is float, then train_size is 1 - test_size.\n",
        "            If test_size is int, then train_size is len(data_full) - test_size.\n",
        "        test_seed (int):\n",
        "            The test seed. Defaults to 42.\n",
        "        num_workers (int):\n",
        "            The number of workers. Defaults to 0.\n",
        "        scaler (object, optional):\n",
        "            The spot scaler object (e.g. TorchStandardScaler). Defaults to None.\n",
        "        verbosity (int):\n",
        "            The verbosity level. Defaults to 0.\n",
        "\n",
        "    Examples:\n",
        "        >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "            from spotpython.data.csvdataset import CSVDataset\n",
        "            from spotpython.utils.scaler import TorchStandardScaler\n",
        "            import torch\n",
        "            # data.csv is simple csv file with 11 samples\n",
        "            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "            scaler = TorchStandardScaler()\n",
        "            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
        "            data_module.setup()\n",
        "            print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "            print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "            print(f\"Test set size: {len(data_module.data_test)}\")\n",
        "            full_train_size: 0.5\n",
        "            val_size: 0.25\n",
        "            train_size: 0.25\n",
        "            test_size: 0.5\n",
        "            Training set size: 3\n",
        "            Validation set size: 3\n",
        "            Test set size: 6\n",
        "\n",
        "    References:\n",
        "        See https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size: int,\n",
        "        dataset: Optional[object] = None,\n",
        "        data_full_train: Optional[object] = None,\n",
        "        data_test: Optional[object] = None,\n",
        "        test_size: Optional[float] = None,\n",
        "        test_seed: int = 42,\n",
        "        num_workers: int = 0,\n",
        "        scaler: Optional[object] = None,\n",
        "        verbosity: int = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_full = dataset\n",
        "        self.data_full_train = data_full_train\n",
        "        self.data_test = data_test\n",
        "        self.test_size = test_size\n",
        "        self.test_seed = test_seed\n",
        "        self.num_workers = num_workers\n",
        "        self.scaler = scaler\n",
        "        self.verbosity = verbosity\n",
        "\n",
        "    def transform_dataset(self, dataset) -> TensorDataset:\n",
        "        \"\"\"Applies the scaler transformation to the dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset (List[Tuple[torch.Tensor, Any]]): The dataset to transform, consisting of data and target pairs.\n",
        "\n",
        "        Returns:\n",
        "            TensorDataset: A PyTorch TensorDataset containing the transformed and cloned data and targets.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the input data is not correctly formatted for transformation.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Perform transformations on the data in a single iteration\n",
        "            transformed_data = [(self.scaler.transform(data), target) for data, target in dataset]\n",
        "            # Clone and detach data tensors\n",
        "            data_tensors = [data.clone().detach() for data, _ in transformed_data]\n",
        "            target_tensors = [target.clone().detach() for _, target in transformed_data]\n",
        "            # Create a TensorDataset from the processed data\n",
        "            return TensorDataset(torch.stack(data_tensors).squeeze(1), torch.stack(target_tensors))\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error transforming dataset: {e}\")\n",
        "\n",
        "    def handle_scaling_and_transform(self) -> None:\n",
        "        \"\"\"\n",
        "        Fits the scaler on the training data and transforms both training and validation datasets.\n",
        "        This function is only called when self.scaler is not None.\n",
        "        \"\"\"\n",
        "        # Ensure self.scaler is not None before proceeding\n",
        "        if self.scaler is None:\n",
        "            raise ValueError(\"Scaler object is required to perform scaling and transformation.\")\n",
        "        # Fit the scaler on training data\n",
        "        scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n",
        "        if self.verbosity > 0:\n",
        "            print(scaler_train_data.shape)\n",
        "        self.scaler.fit(scaler_train_data)\n",
        "        # Transform the training data\n",
        "        self.data_train = self.transform_dataset(self.data_train)\n",
        "        # Transform the validation data\n",
        "        self.data_val = self.transform_dataset(self.data_val)\n",
        "\n",
        "    def prepare_data(self) -> None:\n",
        "        \"\"\"Prepares the data for use.\"\"\"\n",
        "        # download\n",
        "        pass\n",
        "\n",
        "    def _setup_full_data_provided(self, stage) -> None:\n",
        "        full_size = len(self.data_full)\n",
        "        test_size = self.test_size\n",
        "\n",
        "        # consider the case when test_size is a float\n",
        "        if isinstance(self.test_size, float):\n",
        "            full_train_size = 1.0 - self.test_size\n",
        "            val_size = full_train_size * self.test_size\n",
        "            train_size = full_train_size - val_size\n",
        "        else:\n",
        "            # test_size is an int, training size calculation directly based on it\n",
        "            full_train_size = full_size - self.test_size\n",
        "            val_size = floor(full_train_size * self.test_size / full_size)\n",
        "            train_size = full_size - val_size - test_size\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            generator_fit = torch.Generator().manual_seed(self.test_seed)\n",
        "            self.data_train, self.data_val, _ = random_split(self.data_full, [train_size, val_size, test_size], generator=generator_fit)\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"train_size: {train_size}, val_size: {val_size}, test_sie: {test_size} for splitting train & val data.\")\n",
        "                print(f\"train samples: {len(self.data_train)}, val samples: {len(self.data_val)} generated for train & val data.\")\n",
        "            # Handle scaling and transformation if scaler is provided\n",
        "            if self.scaler is not None:\n",
        "                self.handle_scaling_and_transform()\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            generator_test = torch.Generator().manual_seed(self.test_seed)\n",
        "            self.data_test, _, _ = random_split(self.data_full, [test_size, train_size, val_size], generator=generator_test)\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"train_size: {train_size}, val_size: {val_size}, test_sie: {test_size} for splitting test data.\")\n",
        "                print(f\"test samples: {len(self.data_test)} generated for test data.\")\n",
        "            if self.scaler is not None:\n",
        "                # Transform the test data\n",
        "                self.data_test = self.transform_dataset(self.data_test)\n",
        "\n",
        "        # Assign pred dataset for use in dataloader(s)\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            generator_predict = torch.Generator().manual_seed(self.test_seed)\n",
        "            self.data_predict, _, _ = random_split(self.data_full, [test_size, train_size, val_size], generator=generator_predict)\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"train_size: {train_size}, val_size: {val_size}, test_size (= predict_size): {test_size} for splitting predict data.\")\n",
        "                print(f\"predict samples: {len(self.data_predict)} generated for train & val data.\")\n",
        "            if self.scaler is not None:\n",
        "                # Transform the predict data\n",
        "                self.data_predict = self.transform_dataset(self.data_predict)\n",
        "\n",
        "    def _setup_test_data_provided(self, stage) -> None:\n",
        "        # New functionality with separate full_train and test datasets. Use these datasets directly.\n",
        "        full_train_size = len(self.data_full_train)\n",
        "        test_size = self.test_size\n",
        "        # consider the case when test_size is a float\n",
        "        if isinstance(self.test_size, float):\n",
        "            val_size = self.test_size\n",
        "            train_size = 1 - self.test_size\n",
        "        else:\n",
        "            # test_size is an int, training size calculation directly based on it\n",
        "            full_size = len(self.data_full_train) + len(self.data_test)\n",
        "            full_train_size = len(self.data_full_train)\n",
        "            val_size = floor(full_train_size * self.test_size / full_size)\n",
        "            train_size = full_train_size - val_size\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"train_size: {train_size}, val_size: {val_size} used for train & val data.\")\n",
        "            generator_fit = torch.Generator().manual_seed(self.test_seed)\n",
        "            self.data_train, self.data_val = random_split(self.data_full_train, [train_size, val_size], generator=generator_fit)\n",
        "            # Handle scaling and transformation if scaler is provided\n",
        "            if self.scaler is not None:\n",
        "                self.handle_scaling_and_transform()\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"test_size: {test_size} used for test dataset.\")\n",
        "            self.data_test = self.data_test\n",
        "            if self.scaler is not None:\n",
        "                # Transform the test data\n",
        "                self.data_test = self.transform_dataset(self.data_test)\n",
        "\n",
        "        # Assign pred dataset for use in dataloader(s)\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"test_size: {test_size} used for predict dataset.\")\n",
        "            self.data_predict = self.data_test\n",
        "            if self.scaler is not None:\n",
        "                # Transform the predict data\n",
        "                self.data_predict = self.transform_dataset(self.data_predict)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        \"\"\"\n",
        "        Splits the data for use in training, validation, and testing.\n",
        "        Uses torch.utils.data.random_split() to split the data.\n",
        "        Splitting is based on the test_size and test_seed.\n",
        "        The test_size can be a float or an int.\n",
        "        If a spotpython scaler object is defined, the data will be scaled.\n",
        "\n",
        "        Args:\n",
        "            stage (Optional[str]):\n",
        "                The current stage. Can be \"fit\" (for training and validation), \"test\" (testing),\n",
        "                or None (for all three stages). Defaults to None.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "                Training set size: 3\n",
        "\n",
        "        \"\"\"\n",
        "        if self.data_full is not None:\n",
        "            self._setup_full_data_provided(stage)\n",
        "        else:\n",
        "            self._setup_test_data_provided(stage)\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the training dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the training dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The training dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "                Training set size: 3\n",
        "\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.train_dataloader(). data_train size: {len(self.data_train)}\")\n",
        "        # print(f\"LightDataModule: train_dataloader(). batch_size: {self.batch_size}\")\n",
        "        # print(f\"LightDataModule: train_dataloader(). num_workers: {self.num_workers}\")\n",
        "        return DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the validation dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the validation dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The validation dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Training set size: {len(data_module.data_val)}\")\n",
        "                Training set size: 3\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.val_dataloader(). Val. set size: {len(self.data_val)}\")\n",
        "        # print(f\"LightDataModule: val_dataloader(). batch_size: {self.batch_size}\")\n",
        "        # print(f\"LightDataModule: val_dataloader(). num_workers: {self.num_workers}\")\n",
        "        return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the test dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the test dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The test dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Test set size: {len(data_module.data_test)}\")\n",
        "                Test set size: 6\n",
        "\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.test_dataloader(). Test set size: {len(self.data_test)}\")\n",
        "        # print(f\"LightDataModule: test_dataloader(). batch_size: {self.batch_size}\")\n",
        "        # print(f\"LightDataModule: test_dataloader(). num_workers: {self.num_workers}\")\n",
        "        return DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def predict_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the predict dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the predict dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The predict dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
        "                Predict set size: 6\n",
        "\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.predict_dataloader(). Predict set size: {len(self.data_predict)}\")\n",
        "        # print(f\"LightDataModule: predict_dataloader(). batch_size: {self.batch_size}\")\n",
        "        # print(f\"LightDataModule: predict_dataloader(). num_workers: {self.num_workers}\")\n",
        "        return DataLoader(self.data_predict, batch_size=len(self.data_predict), num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_module = LightDataModule(batch_size=2, dataset=dataset, test_size=0.2)\n",
        "data_module.setup()\n",
        "for batch in data_module.train_dataloader():\n",
        "    print(batch)\n",
        "    print(f\"Anzahl Eingabefeaturews: {batch[0][1].shape}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "PREFIX=\"702_lightning_user_datamodule\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.insert(0, './userModel')\n",
        "import my_regressor\n",
        "import my_hyper_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.hyperparameters.values import set_hyperparameter\n",
        "from spotpython.spot import spot\n",
        "from math import inf\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    fun_evals=inf,\n",
        "    fun_repeats=2,\n",
        "    max_time=1,\n",
        "    accelerator=\"cpu\",\n",
        "    data_module=data_module,\n",
        "    _L_in=dataset[0][0].shape[0],\n",
        "    _L_out=1,\n",
        "    noise=True,\n",
        "    ocba_delta=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    tensorboard_log=True,\n",
        "    _torchmetric=\"mean_squared_error\",\n",
        "    log_level=50,\n",
        "    save_experiment=True,\n",
        "    verbosity=1)\n",
        "\n",
        "add_core_model_to_fun_control(fun_control=fun_control,\n",
        "                              core_model=my_regressor.MyRegressor,\n",
        "                              hyper_dict=my_hyper_dict.MyHyperDict)\n",
        "\n",
        "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
        "set_hyperparameter(fun_control, \"act_fn\", [ \"ReLU\", \"Swish\", \"LeakyReLU\"])\n",
        "set_hyperparameter(fun_control, \"l1\", [3,10])\n",
        "set_hyperparameter(fun_control, \"epochs\", [10,14])\n",
        "set_hyperparameter(fun_control, \"batch_size\", [1,5])\n",
        "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
        "set_hyperparameter(fun_control, \"patience\", [2,10])\n",
        "# set_hyperparameter(fun_control, \"initialization\", [\"Default\"])\n",
        "\n",
        "design_control = design_control_init(init_size=5, repeats=2)\n",
        "surrogate_control = surrogate_control_init(log_level=50, noise=True)\n",
        "\n",
        "fun = HyperLight().fun\n",
        "\n",
        "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control, surrogate_control=surrogate_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from spotpython.utils.file import load_experiment\n",
        "if os.path.exists(\"spot_\" + PREFIX + \"_experiment.pickle\"):\n",
        "    (spot_tuner, fun_control, design_control,\n",
        "        surrogate_control, optimizer_control) = load_experiment(PREFIX=PREFIX)\n",
        "else:\n",
        "    res = spot_tuner.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}