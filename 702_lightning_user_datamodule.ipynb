{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: false\n",
        "  echo: true\n",
        "  warning: false\n",
        "title: User Specified Basic Lightning Module With spotpython\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This chapter implements a user-defined DataModule and a user-defined neural network.\n",
        "Remember, that a `LightningModule` organizes your `PyTorch` code into six sections:\n",
        "\n",
        "* Initialization (`__init__` and `setup()`).\n",
        "* Train Loop (`training_step()`)\n",
        "* Validation Loop (`validation_step()`)\n",
        "* Test Loop (`test_step()`)\n",
        "* Prediction Loop (`predict_step()`)\n",
        "* Optimizers and LR Schedulers (`configure_optimizers()`)\n",
        "\n",
        "The `Trainer` automates every required step in a clear and reproducible way. It is the most important part of PyTorch Lightning. It is responsible for training, testing, and validating the model.\n",
        "The `Lightning` core structure looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "df = pd.read_pickle(\"./userData/Turbo_Charger_Data.pkl\")\n",
        "df = df.drop(columns=[\"M\", \"R\"])\n",
        "print(f\"Features des DataFrames: {df.columns}\")\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from lightning import LightningDataModule\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class UserDataset(Dataset):\n",
        "    def __init__(self, data, y_varname=\"N\", x_varnames=None, dtype=torch.float32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (pd.DataFrame):\n",
        "                The user data. for example,\n",
        "                generated by the `preprocess_data` function.\n",
        "            y_varname (str):\n",
        "                The name of the target variable.\n",
        "                Default is \"N\".\n",
        "            x_varnames (list):\n",
        "                The names of the input variables.\n",
        "                Default is `None`, which means all columns\n",
        "                except the target variable are used.\n",
        "            dtype (torch.dtype):\n",
        "                The data type for the tensors.\n",
        "                Default is `torch.float32`.\n",
        "\n",
        "        Examples:\n",
        "            >>> dataset = UserDataset(data)\n",
        "            >>> x, y = dataset[0]\n",
        "        \"\"\"\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        if x_varnames is not None:\n",
        "            self.x_varnames = x_varnames\n",
        "        else:\n",
        "            self.x_varnames = [col for col in self.data.columns if col != y_varname]\n",
        "        print(f\"X variables: {self.x_varnames}\")\n",
        "        print(f\"Y variable: {y_varname}\")\n",
        "        self.y_varname = y_varname\n",
        "        self.dtype = dtype\n",
        "        self.encoders = {}\n",
        "\n",
        "        for var in self.x_varnames:\n",
        "            if self.data[var].dtype == \"object\" or isinstance(self.data[var][0], str):\n",
        "                le = LabelEncoder()\n",
        "                self.data[var] = le.fit_transform(self.data[var])\n",
        "                self.encoders[var] = le\n",
        "\n",
        "        if self.data[self.y_varname].dtype == \"object\" or isinstance(self.data[self.y_varname][0], str):\n",
        "            le = LabelEncoder()\n",
        "            self.data[self.y_varname] = le.fit_transform(self.data[self.y_varname])\n",
        "            self.encoders[self.y_varname] = le\n",
        "\n",
        "        # Convert entire dataset to tensors\n",
        "        self.features = torch.tensor(self.data[self.x_varnames].values, dtype=self.dtype)\n",
        "        self.targets = torch.tensor(self.data[self.y_varname].values, dtype=self.dtype)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = UserDataset(df)\n",
        "x, y = dataset[0]\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataModule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import lightning as L\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from typing import Optional\n",
        "from math import floor\n",
        "\n",
        "\n",
        "class LightDataModule(L.LightningDataModule):\n",
        "    \"\"\"\n",
        "    A LightningDataModule for handling data.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int):\n",
        "            The batch size. Required.\n",
        "        dataset (torch.utils.data.Dataset, optional):\n",
        "            The dataset from the torch.utils.data Dataset class.\n",
        "            It must implement three functions: __init__, __len__, and __getitem__.\n",
        "        test_size (float, optional):\n",
        "            The test size. If test_size is float, then train_size is 1 - test_size.\n",
        "            If test_size is int, then train_size is len(data_full) - test_size.\n",
        "        test_seed (int):\n",
        "            The test seed. Defaults to 42.\n",
        "        num_workers (int):\n",
        "            The number of workers. Defaults to 0.\n",
        "        verbosity (int):\n",
        "            The verbosity level. Defaults to 0.\n",
        "\n",
        "    Examples:\n",
        "        >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "            from spotpython.data.csvdataset import CSVDataset\n",
        "            from spotpython.utils.scaler import TorchStandardScaler\n",
        "            import torch\n",
        "            # data.csv is simple csv file with 11 samples\n",
        "            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "            scaler = TorchStandardScaler()\n",
        "            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
        "            data_module.setup()\n",
        "            print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "            print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "            print(f\"Test set size: {len(data_module.data_test)}\")\n",
        "            full_train_size: 0.5\n",
        "            val_size: 0.25\n",
        "            train_size: 0.25\n",
        "            test_size: 0.5\n",
        "            Training set size: 3\n",
        "            Validation set size: 3\n",
        "            Test set size: 6\n",
        "\n",
        "    References:\n",
        "        See https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size: int,\n",
        "        dataset: Optional[object] = None,\n",
        "        test_size: Optional[float] = None,\n",
        "        test_seed: int = 42,\n",
        "        num_workers: int = 0,\n",
        "        verbosity: int = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_full = dataset\n",
        "        self.test_size = test_size\n",
        "        self.test_seed = test_seed\n",
        "        self.num_workers = num_workers\n",
        "        self.verbosity = verbosity\n",
        "\n",
        "    def prepare_data(self) -> None:\n",
        "        \"\"\"Prepares the data for use.\"\"\"\n",
        "        # download\n",
        "        pass\n",
        "\n",
        "    def _setup_full_data_provided(self, stage) -> None:\n",
        "        full_size = len(self.data_full)\n",
        "        test_size = self.test_size\n",
        "\n",
        "        # consider the case when test_size is a float\n",
        "        if isinstance(self.test_size, float):\n",
        "            full_train_size = 1.0 - self.test_size\n",
        "            val_size = full_train_size * self.test_size\n",
        "            train_size = full_train_size - val_size\n",
        "        else:\n",
        "            # test_size is an int, training size calculation directly based on it\n",
        "            full_train_size = full_size - self.test_size\n",
        "            val_size = floor(full_train_size * self.test_size / full_size)\n",
        "            train_size = full_size - val_size - test_size\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            generator_fit = torch.Generator().manual_seed(self.test_seed)\n",
        "            self.data_train, self.data_val, _ = random_split(self.data_full, [train_size, val_size, test_size], generator=generator_fit)\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"train_size: {train_size}, val_size: {val_size}, test_sie: {test_size} for splitting train & val data.\")\n",
        "                print(f\"train samples: {len(self.data_train)}, val samples: {len(self.data_val)} generated for train & val data.\")\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            generator_test = torch.Generator().manual_seed(self.test_seed)\n",
        "            self.data_test, _, _ = random_split(self.data_full, [test_size, train_size, val_size], generator=generator_test)\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"train_size: {train_size}, val_size: {val_size}, test_sie: {test_size} for splitting test data.\")\n",
        "                print(f\"test samples: {len(self.data_test)} generated for test data.\")\n",
        "\n",
        "        # Assign pred dataset for use in dataloader(s)\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            generator_predict = torch.Generator().manual_seed(self.test_seed)\n",
        "            self.data_predict, _, _ = random_split(self.data_full, [test_size, train_size, val_size], generator=generator_predict)\n",
        "            if self.verbosity > 0:\n",
        "                print(f\"train_size: {train_size}, val_size: {val_size}, test_size (= predict_size): {test_size} for splitting predict data.\")\n",
        "                print(f\"predict samples: {len(self.data_predict)} generated for train & val data.\")\n",
        "    \n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        \"\"\"\n",
        "        Splits the data for use in training, validation, and testing.\n",
        "        Uses torch.utils.data.random_split() to split the data.\n",
        "        Splitting is based on the test_size and test_seed.\n",
        "        The test_size can be a float or an int.\n",
        "        If a spotpython scaler object is defined, the data will be scaled.\n",
        "\n",
        "        Args:\n",
        "            stage (Optional[str]):\n",
        "                The current stage. Can be \"fit\" (for training and validation), \"test\" (testing),\n",
        "                or None (for all three stages). Defaults to None.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "                Training set size: 3\n",
        "\n",
        "        \"\"\"\n",
        "        self._setup_full_data_provided(stage)\n",
        "\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the training dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the training dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The training dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "                Training set size: 3\n",
        "\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.train_dataloader(). data_train size: {len(self.data_train)}\")\n",
        "        return DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the validation dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the validation dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The validation dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Training set size: {len(data_module.data_val)}\")\n",
        "                Training set size: 3\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.val_dataloader(). Val. set size: {len(self.data_val)}\")\n",
        "        return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the test dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the test dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The test dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Test set size: {len(data_module.data_test)}\")\n",
        "                Test set size: 6\n",
        "\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.test_dataloader(). Test set size: {len(self.data_test)}\")\n",
        "        return DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def predict_dataloader(self) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Returns the predict dataloader, i.e., a pytorch DataLoader instance\n",
        "        using the predict dataset.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: The predict dataloader.\n",
        "\n",
        "        Examples:\n",
        "            >>> from spotpython.data.lightdatamodule import LightDataModule\n",
        "                from spotpython.data.csvdataset import CSVDataset\n",
        "                import torch\n",
        "                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "                data_module.setup()\n",
        "                print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
        "                Predict set size: 6\n",
        "\n",
        "        \"\"\"\n",
        "        if self.verbosity > 0:\n",
        "            print(f\"LightDataModule.predict_dataloader(). Predict set size: {len(self.data_predict)}\")\n",
        "        return DataLoader(self.data_predict, batch_size=len(self.data_predict), num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_module = LightDataModule(batch_size=2, dataset=dataset, test_size=0.2)\n",
        "data_module.setup()\n",
        "for batch in data_module.train_dataloader():\n",
        "    print(batch)\n",
        "    print(f\"Number of input features: {batch[0][1].shape}\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Neural Network: MyRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: MyRegressor\n",
        "#| eval: false\n",
        "import lightning as L\n",
        "import torch\n",
        "from torch import nn\n",
        "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
        "import torchmetrics.functional.regression\n",
        "from math import ceil\n",
        "\n",
        "class MyRegressor(L.LightningModule):\n",
        "    \"\"\"\n",
        "    A LightningModule class for a regression neural network model.\n",
        "\n",
        "    Attributes:\n",
        "        l1 (int):\n",
        "            The number of neurons in the first hidden layer.\n",
        "        epochs (int):\n",
        "            The number of epochs to train the model for.\n",
        "        batch_size (int):\n",
        "            The batch size to use during training.\n",
        "        initialization (str):\n",
        "            The initialization method to use for the weights.\n",
        "        act_fn (nn.Module):\n",
        "            The activation function to use in the hidden layers.\n",
        "        optimizer (str):\n",
        "            The optimizer to use during training.\n",
        "        dropout_prob (float):\n",
        "            The probability of dropping out a neuron during training.\n",
        "        lr_mult (float):\n",
        "            The learning rate multiplier for the optimizer.\n",
        "        patience (int):\n",
        "            The number of epochs to wait before early stopping.\n",
        "        _L_in (int):\n",
        "            The number of input features.\n",
        "        _L_out (int):\n",
        "            The number of output classes.\n",
        "        _torchmetric (str):\n",
        "            The metric to use for the loss function. If `None`,\n",
        "            then \"mean_squared_error\" is used.\n",
        "        layers (nn.Sequential):\n",
        "            The neural network model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        l1: int,\n",
        "        epochs: int,\n",
        "        batch_size: int,\n",
        "        initialization: str,\n",
        "        act_fn: nn.Module,\n",
        "        optimizer: str,\n",
        "        dropout_prob: float,\n",
        "        lr_mult: float,\n",
        "        patience: int,\n",
        "        _L_in: int,\n",
        "        _L_out: int,\n",
        "        _torchmetric: str,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the MyRegressor object.\n",
        "\n",
        "        Args:\n",
        "            l1 (int):\n",
        "                The number of neurons in the first hidden layer.\n",
        "            epochs (int):\n",
        "                The number of epochs to train the model for.\n",
        "            batch_size (int):\n",
        "                The batch size to use during training.\n",
        "            initialization (str):\n",
        "                The initialization method to use for the weights.\n",
        "            act_fn (nn.Module):\n",
        "                The activation function to use in the hidden layers.\n",
        "            optimizer (str):\n",
        "                The optimizer to use during training.\n",
        "            dropout_prob (float):\n",
        "                The probability of dropping out a neuron during training.\n",
        "            lr_mult (float):\n",
        "                The learning rate multiplier for the optimizer.\n",
        "            patience (int):\n",
        "                The number of epochs to wait before early stopping.\n",
        "            _L_in (int):\n",
        "                The number of input features. Not a hyperparameter, but needed to create the network.\n",
        "            _L_out (int):\n",
        "                The number of output classes. Not a hyperparameter, but needed to create the network.\n",
        "            _torchmetric (str):\n",
        "                The metric to use for the loss function. If `None`,\n",
        "                then \"mean_squared_error\" is used.\n",
        "\n",
        "        Returns:\n",
        "            (NoneType): None\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If l1 is less than 4.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._L_in = _L_in\n",
        "        self._L_out = _L_out\n",
        "        if _torchmetric is None:\n",
        "            _torchmetric = \"mean_squared_error\"\n",
        "        self._torchmetric = _torchmetric\n",
        "        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n",
        "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
        "        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n",
        "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n",
        "        # set dummy input array for Tensorboard Graphs\n",
        "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
        "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
        "        if self.hparams.l1 < 4:\n",
        "            raise ValueError(\"l1 must be at least 4\")\n",
        "        hidden_sizes = [l1 * 2, l1, ceil(l1/2)]\n",
        "        # Create the network based on the specified hidden sizes\n",
        "        layers = []\n",
        "        layer_sizes = [self._L_in] + hidden_sizes\n",
        "        layer_size_last = layer_sizes[0]\n",
        "        for layer_size in layer_sizes[1:]:\n",
        "            layers += [\n",
        "                nn.Linear(layer_size_last, layer_size),\n",
        "                self.hparams.act_fn,\n",
        "                nn.Dropout(self.hparams.dropout_prob),\n",
        "            ]\n",
        "            layer_size_last = layer_size\n",
        "        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor containing a batch of input data.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the output of the model.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "    def _calculate_loss(self, batch):\n",
        "        \"\"\"\n",
        "        Calculate the loss for the given batch.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y = y.view(len(y), 1)\n",
        "        y_hat = self(x)\n",
        "        loss = self.metric(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single training step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        return val_loss\n",
        "\n",
        "    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single validation step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "\n",
        "        \"\"\"\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
        "        return val_loss\n",
        "\n",
        "    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single test step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing the loss for this batch.\n",
        "        \"\"\"\n",
        "        val_loss = self._calculate_loss(batch)\n",
        "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
        "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
        "        return val_loss\n",
        "\n",
        "    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a single prediction step.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A tuple containing a batch of input data and labels.\n",
        "            batch_idx (int): The index of the current batch.\n",
        "            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the input data, the true labels, and the predicted values.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        yhat = self(x)\n",
        "        y = y.view(len(y), 1)\n",
        "        yhat = yhat.view(len(yhat), 1)\n",
        "        print(f\"Predict step x: {x}\")\n",
        "        print(f\"Predict step y: {y}\")\n",
        "        print(f\"Predict step y_hat: {yhat}\")\n",
        "        return (x, y, yhat)\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        \"\"\"\n",
        "        Configures the optimizer for the model.\n",
        "        Simple examples use the following code here:\n",
        "        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`\n",
        "\n",
        "        Notes:\n",
        "            The default Lightning way is to define an optimizer as\n",
        "            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n",
        "            spotpython uses an optimizer handler to create the optimizer, which\n",
        "            adapts the learning rate according to the lr_mult hyperparameter as\n",
        "            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n",
        "\n",
        "        Returns:\n",
        "            torch.optim.Optimizer: The optimizer to use during training.\n",
        "\n",
        "        \"\"\"\n",
        "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        optimizer = optimizer_handler(\n",
        "            optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n",
        "        )\n",
        "        return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calling the Neural Network With spotpython"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: prefix_setup\n",
        "PREFIX=\"702_lightning_user_datamodule\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: spotpython_setup\n",
        "import sys\n",
        "sys.path.insert(0, './userModel')\n",
        "import my_regressor\n",
        "import my_hyper_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
        "from spotpython.hyperparameters.values import set_hyperparameter\n",
        "from spotpython.spot import Spot\n",
        "from math import inf\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    fun_evals=inf,\n",
        "    fun_repeats=1,\n",
        "    max_time=5,\n",
        "    accelerator=\"cpu\",\n",
        "    data_module=data_module,\n",
        "    _L_in=dataset[0][0].shape[0],\n",
        "    _L_out=1,\n",
        "    noise=False,\n",
        "    ocba_delta=0,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    tensorboard_log=True,\n",
        "    _torchmetric=\"mean_squared_error\",\n",
        "    log_level=50,\n",
        "    save_experiment=True,\n",
        "    verbosity=1)\n",
        "\n",
        "add_core_model_to_fun_control(fun_control=fun_control,\n",
        "                              core_model=my_regressor.MyRegressor,\n",
        "                              hyper_dict=my_hyper_dict.MyHyperDict)\n",
        "\n",
        "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
        "set_hyperparameter(fun_control, \"act_fn\", [ \"ReLU\", \"Swish\", \"LeakyReLU\"])\n",
        "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
        "set_hyperparameter(fun_control, \"epochs\", [3,5])\n",
        "set_hyperparameter(fun_control, \"batch_size\", [1,5])\n",
        "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
        "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
        "# set_hyperparameter(fun_control, \"initialization\", [\"Default\"])\n",
        "\n",
        "design_control = design_control_init(init_size=5, repeats=1)\n",
        "surrogate_control = surrogate_control_init(noise=True)\n",
        "\n",
        "fun = HyperLight().fun\n",
        "\n",
        "spot_tuner = Spot(fun=fun,fun_control=fun_control, design_control=design_control, surrogate_control=surrogate_control)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: spot_tuner_run\n",
        "import os\n",
        "from spotpython.utils.file import load_experiment\n",
        "if os.path.exists(\"spot_\" + PREFIX + \"_experiment.pickle\"):\n",
        "    (spot_tuner, fun_control, design_control,\n",
        "        surrogate_control, optimizer_control) = load_experiment(PREFIX=PREFIX)\n",
        "else:\n",
        "    res = spot_tuner.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looking at the Results\n",
        "\n",
        "### Tuning Progress\n",
        "\n",
        "After the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized with `spotpython`'s method `plot_progress`. The black points represent the performace values (score or metric) of  hyperparameter configurations from the initial design, whereas the red points represents the  hyperparameter configurations found by the surrogate model based optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 601_plot_progress_1\n",
        "spot_tuner.plot_progress()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuned Hyperparameters and Their Importance\n",
        "\n",
        "Results can be printed in tabular form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.utils.eda import print_res_table\n",
        "print_res_table(spot_tuner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A histogram can be used to visualize the most important hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_tuner.plot_importance(threshold=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_tuner.plot_important_hyperparameter_contour(max_imp=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the Tuned Architecture {#sec-get-spot-results-702}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pprint\n",
        "from spotpython.hyperparameters.values import get_tuned_architecture\n",
        "config = get_tuned_architecture(spot_tuner)\n",
        "pprint.pprint(config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}