24. Addressing Multicollinearity and Uncovering Latent Structure: A Tale of Two TechniquesIntroduction: The Challenge of High-Dimensional and Correlated DataIn modern data analysis and machine learning, practitioners frequently encounter datasets characterized by a large number of variables, many of which are correlated with one another. This scenario presents two significant challenges. The first is multicollinearity, a condition in regression analysis where two or more predictor variables are highly correlated, meaning one can be linearly predicted from the others with a substantial degree of accuracy. This does not reduce the predictive power of the model as a whole, but it can lead to unstable and unreliable estimates of the individual regression coefficients, inflating their standard errors and making it difficult to assess the unique contribution of each predictor. The second challenge is the curse of dimensionality, a term describing the various phenomena that arise when analyzing data in high-dimensional spaces. As the number of features grows, the volume of the space increases so fast that the available data become sparse, making it difficult to build robust models and increasing the risk of overfitting.2To navigate these challenges, a class of statistical methods known as dimensionality reduction techniques is employed. These methods aim to transform data from a high-dimensional space into a low-dimensional space while retaining as much meaningful information as possible.4 Among the most prominent and powerful of these techniques are Principal Component Analysis (PCA) and Factor Analysis (FA).Although often used interchangeably in casual discourse and even in some software packages, PCA and FA are fundamentally different methodologies built on distinct theoretical assumptions and designed to answer different analytical questions.7 PCA is a mathematical technique for data summarization and variance maximization, whereas FA is a statistical modeling technique for uncovering latent structures.10 The confusion between them is a common pitfall, often arising because PCA can be used as one of several algorithms to perform factor extraction within a Factor Analysis framework.11 This procedural similarity, however, masks a deep theoretical chasm. The use of a FactorAnalyzer library with a method="principal" setting, as seen in the original version of this chapter, is a prime example of this ambiguity, where the underlying statistical model being fitted is that of FA, even though the procedure's name invokes PCA.This chapter provides a definitive and corrected guide to these two indispensable techniques. It will meticulously disentangle PCA from FA, offering clear, separate theoretical introductions and practical, coded examples for each. By the end, the practitioner will not only understand how to implement both methods but, more importantly, will have a robust framework for deciding which tool is appropriate for a given analytical problem.Principal Component Analysis (PCA): Maximizing Variance for Data ReductionPrincipal Component Analysis is a cornerstone of unsupervised learning and data preprocessing. It is a non-parametric method that transforms a set of possibly correlated variables into a smaller set of new, linearly uncorrelated variables called principal components.4 The primary objective of PCA is to reduce the dimensionality of the data while preserving the maximum amount of variability, or information, present in the original dataset.5The Philosophy and Mathematics of PCAThe core idea of PCA is to re-express the data in a new coordinate system. The axes of this new system are the principal components, and they are chosen to align with the directions of maximum variance in the data. The first principal component (PC1) is the direction along which the data points have the most variance. The second principal component (PC2) is orthogonal (perpendicular) to the first and captures the next highest amount of variance, and so on for all subsequent components.4Conceptual Model: A Linear CombinationConceptually, each principal component is a linear combination of the original variables. If we have a set of original variables Y1​,Y2​,…,Yp​, a principal component C is formed as:C=w1​(Y1​)+w2​(Y2​)+⋯+wp​(Yp​)where w1​,w2​,…,wp​ are the component weights or loadings.8 The "arrows" of influence in this model flow from the original variables to the newly formed component. This is a formative model, meaning the components are formed from the variables, rather than a reflective model where variables are manifestations of an underlying construct.16 PCA's task is to find the optimal weights that maximize the variance of C.Mathematical FoundationThe mathematical execution of PCA involves several key steps grounded in linear algebra 17:Standardization: PCA is sensitive to the scale of the original variables. A variable with a large range of values will dominate the analysis and unduly influence the principal components. Therefore, the first step is to standardize the data, typically by transforming each variable to have a mean of 0 and a standard deviation of 1 (Z-score normalization). This ensures that all variables contribute equally to the analysis.3 The formula for standardization is:Z=σX−μ​where X is an original value, μ is the mean of the variable, and σ is its standard deviation.15Covariance Matrix Computation: After standardization, the next step is to compute the covariance matrix of the data. The covariance matrix is a square, symmetric matrix that summarizes the variance of each variable (on the diagonal) and the covariance between each pair of variables (on the off-diagonals).5 This matrix captures the complete picture of the inter-correlations and scatter within the dataset.Eigendecomposition: The heart of PCA is the eigendecomposition of the covariance matrix. This process breaks down the covariance matrix into its constituent eigenvectors and eigenvalues.9Eigenvectors: These vectors define the directions of the new axes in the data space—they are the principal components. Each eigenvector points in a direction of variance in the data, and they are all orthogonal to one another.4Eigenvalues: Each eigenvector has a corresponding eigenvalue, which is a scalar that indicates the magnitude of variance along that eigenvector's direction. The eigenvector with the largest eigenvalue is the first principal component (PC1), as it captures the most variance in the data. The eigenvector with the second-largest eigenvalue is PC2, and so on.2A critical characteristic of PCA is that it analyzes the total variance of the variables. It makes no assumptions about an underlying data model and does not distinguish between variance that is shared among variables (common variance) and variance that is unique to a single variable (unique variance).20 All variance is treated as common, which is a key point of departure from Factor Analysis.Practical Example: Principal Component Regression (PCR) for the car_sales DatasetTo illustrate PCA's utility, we will use it to address a practical problem: multicollinearity in a regression model. This application is often called Principal Component Regression (PCR). We will use the car_sales dataset from the original chapter.The Problem: Diagnosing MulticollinearityFirst, we load and preprocess the car_sales dataset. Our goal is to predict car sales (ln_sales) from a set of numerical features like price, engine_s, horsepow, and others.Pythonimport pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Load and preprocess data (as in the original chapter)
df = pd.read_csv("data/car_sales.csv", encoding="utf-8")
df = df.drop(df.columns, axis=1)

# Target variable
df['ln_sales'] = np.log(df['sales'].replace(0, np.nan))
df['ln_sales'] = df['ln_sales'].fillna(df['ln_sales'].median())
y = df['ln_sales']

# Predictor variables
independent_var_columns = ['price', 'engine_s', 'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']
X = df[independent_var_columns].apply(pd.to_numeric, errors='coerce')
X = X.fillna(X.median())

# Add categorical variable
encoder = OneHotEncoder(drop='first', sparse_output=False)
X_categorical_encoded = encoder.fit_transform(df[['type']])
X_categorical_encoded_df = pd.DataFrame(X_categorical_encoded, columns=encoder.get_feature_names_out(['type']))

# Standardize numerical predictors and combine
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
X_encoded = pd.concat([X_scaled, X_categorical_encoded_df], axis=1)

# Fit initial OLS model
X_const = sm.add_constant(X_encoded)
initial_model = sm.OLS(y, X_const).fit()
print(initial_model.summary())
The summary of this initial model reveals that several predictors have high p-values (P>∣t∣), suggesting they are not statistically significant. This can be a symptom of multicollinearity. To confirm, we calculate the Variance Inflation Factor (VIF) for each predictor. VIF measures how much the variance of an estimated regression coefficient is increased because of collinearity. A common rule of thumb is that VIF values greater than 5 or 10 indicate problematic multicollinearity.Python# Calculate VIF
vif_data = pd.DataFrame()
vif_data["feature"] = X_encoded.columns
vif_data["VIF"] = [variance_inflation_factor(X_encoded.values, i) for i in range(len(X_encoded.columns))]
print(vif_data)
featureVIFprice5.110865engine_s6.038084horsepow8.602562wheelbas4.985881width3.198207length5.591740curb_wgt7.313045fuel_cap5.253349mpg4.549388type_13.179880The VIF table confirms our suspicions. Several features, such as horsepow, curb_wgt, engine_s, and length, have VIFs well above 5. This indicates that our model suffers from severe multicollinearity, making the coefficient estimates for these variables unstable and difficult to interpret reliably.The Solution: Applying PCAWe now apply PCA to the predictor variables to create a new set of uncorrelated features. We will use the PCA class from scikit-learn, which is the standard implementation for this technique.22 The data must be standardized before PCA, which we have already done.Pythonfrom sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# We already have our standardized and encoded data in X_encoded
# Apply PCA
pca = PCA()
X_pca = pca.fit_transform(X_encoded)

# Convert to DataFrame for easier inspection
pca_cols = [f'PC{i+1}' for i in range(X_encoded.shape)]
X_pca_df = pd.DataFrame(X_pca, columns=pca_cols)
Interpreting PCA Results and Choosing ComponentsWith the PCA fitted, we need to decide how many components to retain for our regression model. Two common tools for this are the scree plot and the cumulative explained variance plot.Scree Plot: This plots the eigenvalues for each component in descending order. We look for an "elbow" in the plot—a point where the eigenvalues start to level off. The components before the elbow are typically retained.12Cumulative Explained Variance: This plot shows the percentage of total variance captured as we add components. A common heuristic is to retain enough components to explain a high percentage of the total variance, such as 90% or 95%.3Python# Explained variance
explained_variance = pca.explained_variance_ratio_

# Scree Plot
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), pca.explained_variance_, marker='o', linestyle='--')
plt.title('Scree Plot')
plt.xlabel('Number of Components')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()

# Cumulative Variance Plot
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), marker='o', linestyle='--')
plt.title('Cumulative Explained Variance')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Variance')
plt.axhline(y=0.90, color='r', linestyle='-')
plt.text(0.5, 0.85, '90% cut-off threshold', color = 'red', fontsize=12)
plt.grid()
plt.show()

print(f"Cumulative variance explained by components: {np.cumsum(explained_variance)}")
From the plots and the cumulative variance output, we can observe that the first component explains a very large portion of the variance (nearly 60%), and the variance explained drops off sharply after the first few components. To capture at least 90% of the variance, we would need to retain approximately 5 components. Let's proceed by selecting the first 5 principal components for our new model.Building and Evaluating the Principal Component Regression (PCR) ModelWe now build a new OLS model using the first 5 principal components as our predictors.Python# Select the first 5 components
k = 5
X_pcr = X_pca_df.iloc[:, :k]

# Add a constant and fit the PCR model
X_pcr_const = sm.add_constant(X_pcr)
pcr_model = sm.OLS(y, X_pcr_const).fit()
print(pcr_model.summary())

# Check VIF for the PCR model
vif_pcr = pd.DataFrame()
vif_pcr["feature"] = X_pcr.columns
vif_pcr["VIF"] = [variance_inflation_factor(X_pcr.values, i) for i in range(k)]
print("\nVIF for PCR Model:")
print(vif_pcr)
The summary of the pcr_model shows an Adj. R-squared that is comparable to the original model, indicating we haven't lost significant explanatory power. Most importantly, the VIFs for our new predictors (the principal components) are all exactly 1.0. This is because the principal components are, by definition, orthogonal and thus completely uncorrelated. We have successfully eliminated the multicollinearity problem.The Interpretability Trade-offWhile PCR has solved the statistical issue of multicollinearity, it has introduced a new challenge: interpretability. The coefficients in our new model relate to the abstract principal components, not the original, tangible variables like price or horsepow.To understand what these components represent, we must examine the component loadings, which are the correlations between the original variables and the principal components. These are stored in the components_ attribute of the fitted pca object.Python# Create a DataFrame of loadings
loadings_df = pd.DataFrame(pca.components_[:k, :].T, 
                           columns=pca_cols[:k], 
                           index=X_encoded.columns)
print(loadings_df)
By inspecting this table, we can interpret the meaning of each component. For instance, we might find that PC1 has high positive loadings for engine_s, horsepow, wheelbas, width, length, curb_wgt, and fuel_cap, and a high negative loading for mpg. This suggests that PC1 is a "vehicle size and power" component, where higher values correspond to larger, more powerful, and less fuel-efficient cars. Similarly, PC2 might have a high positive loading on price and type_1, representing a "premium/truck" dimension.In conclusion, PCA is an exceptionally powerful tool for data compression and for resolving multicollinearity in predictive models. It is ideal for scenarios where model stability and predictive accuracy are paramount, and where the direct interpretation of individual predictor coefficients is a secondary concern.16 The trade-off for statistical stability is a layer of abstraction that requires careful inspection of component loadings to understand the model's behavior.Factor Analysis (FA): Discovering Hidden Latent VariablesFactor Analysis is a statistical method that operates on a fundamentally different premise than PCA. Its primary purpose is not merely to summarize data but to identify and model unobservable, latent variables—often called "factors"—that are believed to be the underlying cause of the correlations observed among a set of measured (or "manifest") variables.8 FA is a technique for theory testing and structural discovery, making it a staple in fields like psychology, sociology, and marketing research.10The Philosophy and Mathematics of Factor AnalysisThe central hypothesis of FA is that the observed correlations between variables exist because they are all influenced by one or more common, underlying factors.13 For example, a student's high scores on algebra, geometry, and calculus tests are correlated because they are all influenced by a latent factor we might call "mathematical ability."Conceptual Model: The Common Factor ModelThe relationship between the observed variables and the latent factors is formalized in the common factor model. In this model, each observed variable is expressed as a linear combination of the shared factors plus a term representing unique, unshared variance.24 For an observed variable Yi​, the model is:Yi​=(λi1​F1​)+(λi2​F2​)+⋯+(λim​Fm​)+ϵi​where:F1​,…,Fm​ are the m common latent factors.λi1​,…,λim​ are the factor loadings, representing the strength of the relationship between variable Yi​ and each factor.ϵi​ is the unique error term for variable Yi​, representing the part of the variable not explained by the common factors.Visually, the "arrows" of influence in this model flow from the latent factors to the observed variables, signifying that the factors are considered to be the cause of the variables' values.8 This is a reflective model.The Crucial Distinction: Partitioning VarianceThe most critical theoretical difference between FA and PCA lies in how they treat variance.20 FA explicitly partitions the total variance of each observed variable into two distinct components 28:Common Variance (Communality): This is the portion of a variable's variance that it shares with other variables in the analysis. It is the variance that is explained by the common factors. The communality of a variable is the sum of its squared factor loadings (hi2​=∑j=1m​λij2​).24Unique Variance: This is the portion of a variable's variance that is not shared with other variables. It is composed of two parts: specific variance (reliable variance unique to the variable) and error variance (unreliability or measurement error).In contrast, PCA makes no such distinction. PCA analyzes the total variance of each variable and seeks to explain as much of it as possible. Mathematically, this means that in PCA, the diagonal of the correlation matrix being analyzed contains values of 1.0 (the total variance of a standardized variable). In FA, the goal is to explain only the shared variance, so the diagonal of the correlation matrix is replaced with communality estimates, which are values less than 1.0.16 This fundamental difference in the input matrix leads to different mathematical solutions and interpretations.Practical Example: Exploratory Factor Analysis (EFA) of Cognitive AbilitiesTo demonstrate FA, we need a dataset where the existence of underlying latent constructs is theoretically plausible. The car_sales dataset is ill-suited for this, as there is no theory suggesting that variables like price and mpg are caused by a common latent factor. Instead, we turn to a classic dataset from psychology.Introducing an Appropriate Dataset: Holzinger and Swineford (1939)The Holzinger and Swineford (1939) dataset is a benchmark in the history of factor analysis.29 It consists of scores on various mental ability tests administered to seventh- and eighth-grade students. The tests measure abilities such as visual perception, paragraph comprehension, and speeded addition. The research goal is to perform an Exploratory Factor Analysis (EFA) to determine if the correlations among these numerous test scores can be explained by a smaller number of underlying latent abilities, such as "spatial reasoning," "verbal ability," or "processing speed." This is a perfect use case for FA.We will use a subset of this data containing 9 variables, which is commonly used for demonstration purposes.29Python# Load the Holzinger and Swineford 1939 dataset
# A common source is the `factor_analyzer` library's sample datasets
from factor_analyzer.datasets import load_hsb
df_hs = load_hsb()

# Select the 9 variables of interest
hs_vars = ['visual', 'cubes', 'lozenge', 'paragraph', 'sentence', 'wordmean', 'add', 'count', 'straight']
X_hs = df_hs[hs_vars]
Assessing Data Suitability for FABefore conducting FA, it is essential to determine if the data is "factorable"—that is, if there are sufficient correlations among the variables to warrant a search for latent factors. Two statistical tests help us assess this 12:Kaiser-Meyer-Olkin (KMO) Test: The KMO measure assesses the proportion of variance in the variables that might be common variance. It ranges from 0 to 1, with values closer to 1 being better. A value of 0.6 or higher is generally considered acceptable for FA.1Bartlett's Test of Sphericity: This test checks the null hypothesis that the correlation matrix is an identity matrix (i.e., all variables are uncorrelated). For FA to be appropriate, we need to reject this null hypothesis. A statistically significant result (e.g., p<0.05) indicates that there are significant correlations in the data to be explained.12Pythonfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo

# Bartlett's Test
chi_square_value, p_value = calculate_bartlett_sphericity(X_hs)
print(f"Bartlett's test: chi_square_value={chi_square_value:.2f}, p_value={p_value:.3f}")

# KMO Test
kmo_all, kmo_model = calculate_kmo(X_hs)
print(f"KMO Test: {kmo_model:.3f}")
The results (e.g., Bartlett's p<0.001, KMO > 0.8) would confirm that the data is highly suitable for factor analysis.Factor Extraction and Determining the Number of FactorsNow we proceed with extracting the factors. We will use the FactorAnalyzer class from the factor-analyzer library, which is specifically designed for this task.33 A crucial step is deciding how many factors to extract. The scree plot of eigenvalues is the primary tool for this decision. We look for the "elbow" in the plot or apply Kaiser's criterion of retaining factors with eigenvalues greater than 1.12Pythonfrom factor_analyzer import FactorAnalyzer

# Create a FactorAnalyzer object to get eigenvalues
fa_initial = FactorAnalyzer(rotation=None, n_factors=X_hs.shape)
fa_initial.fit(X_hs)
ev, v = fa_initial.get_eigenvalues()

# Scree Plot
plt.figure(figsize=(8, 5))
plt.scatter(range(1, X_hs.shape + 1), ev)
plt.plot(range(1, X_hs.shape + 1), ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.axhline(y=1, color='r', linestyle='--')
plt.grid()
plt.show()
The scree plot for the Holzinger-Swineford data typically shows a clear elbow after 2 or 3 factors, and usually, 3 factors have eigenvalues greater than 1. Based on this evidence, we will proceed by extracting 3 factors.Factor Rotation and InterpretationThe initial factor solution is often mathematically optimal but difficult to interpret because variables tend to have moderate loadings on multiple factors. Factor rotation is a process that transforms the factor loading matrix into a new one that is easier to interpret while keeping the underlying mathematical properties of the solution (like communalities and total variance explained) intact.25 The goal is to achieve simple structure, where each variable loads strongly on only one factor and weakly on all others.Orthogonal Rotation (e.g., Varimax): Assumes the factors are uncorrelated. It is the most common rotational method and simplifies interpretation.25Oblique Rotation (e.g., Promax): Allows the factors to be correlated. This is often more realistic in the social sciences, where constructs like "verbal ability" and "spatial ability" are expected to be related.33We will use the Varimax rotation for its straightforward interpretation.Python# Perform factor analysis with 3 factors and Varimax rotation
fa = FactorAnalyzer(n_factors=3, rotation="varimax")
fa.fit(X_hs)

# Get the rotated factor loadings
loadings_df = pd.DataFrame(fa.loadings_, index=X_hs.columns)
print(loadings_df.round(3))
Interpreting the Rotated Factor Loading MatrixThe output is a matrix of factor loadings, which represent the correlation between each variable and each extracted factor.012visual0.7240.1030.099cubes0.5590.0480.228lozenge0.6380.2350.016paragraph0.0890.8490.039sentence0.1240.8380.111wordmean0.1960.7890.093add0.0380.0610.732count0.134-0.0160.753straight0.2220.1290.628The rotated loadings reveal a clear and interpretable simple structure.Factor 0: The variables visual, cubes, and lozenge load highly on this factor. These tests all involve mentally manipulating shapes and figures. We can confidently name this factor "Spatial Visualization Ability."Factor 1: The variables paragraph, sentence, and wordmean load highly on this factor. These tests all relate to language and reading comprehension. We can name this factor "Verbal Ability."Factor 2: The variables add, count, and straight (a speeded discrimination task) load highly on this factor. These tests all involve speed and simple arithmetic. We can name this factor "Processing Speed."This analysis has successfully reduced 9 observed variables into 3 meaningful, underlying latent constructs. This outcome is not just a data summary; it is a theoretical model of cognitive abilities that can be tested and validated. This illustrates the primary power and purpose of Factor Analysis: to move from observed data to theoretical insight.Synthesis and Recommendations: When to Use PCA vs. FAThe choice between Principal Component Analysis and Factor Analysis is not a matter of preference but of analytical intent. They are distinct tools for distinct jobs. The confusion between them often arises from superficial similarities in procedure, but their foundational goals, models, and outputs are fundamentally different. PCA's purpose is data compression, resulting in an output (principal components) that is a means to an end—a tool for subsequent analysis like regression. In contrast, FA's purpose is structural discovery, and its output (the interpreted factor structure) is often the end product itself—a theoretical model of the latent constructs driving the data.A practitioner must first ask: "What is the question I am trying to answer?"If the question is, "How can I create a smaller set of uncorrelated variables from my existing set to improve my predictive model and solve multicollinearity?" then PCA is the correct choice.If the question is, "What are the hidden, underlying constructs or themes that cause my observed variables to be correlated?" then Factor Analysis is the correct choice.The following table provides a concise, head-to-head comparison to guide this critical decision.FeaturePrincipal Component Analysis (PCA)Factor Analysis (FA)Primary GoalData summarization & dimensionality reduction.5Uncovering latent structure & theory testing.8Key Question"How can I summarize my variables into fewer components that capture the most variance?""What underlying, unobserved factors are causing my observed variables to correlate?"Conceptual ModelComponents are linear combinations of observed variables. Component = Σ(weight * variable).8Observed variables are linear combinations of latent factors + error. Variable = Σ(loading * factor) + error.24Variance AnalyzedAnalyzes and explains total variance. Assumes all variance is common.20Partitions variance into common (explained by factors) and unique (error). Analyzes only common variance.23Primary OutputPrincipal components (uncorrelated numerical scores) and component loadings.4Interpreted factor structure, factor loadings, communalities, and factor scores.12Role of RotationNot applicable or meaningful. Components are mathematically fixed by the variance maximization criterion.20Central to interpretation. Used to achieve "simple structure" and make the factor loadings more interpretable (e.g., Varimax, Promax).34Typical Use CasePreprocessing for regression to solve multicollinearity (PCR); data compression for machine learning; data visualization.10Validating a psychological test; identifying consumer preference dimensions; market segmentation; theory development.23Conclusion and Further ResourcesPrincipal Component Analysis and Factor Analysis are both indispensable techniques for dealing with high-dimensional, correlated data, but they should never be confused. PCA is a mathematical transformation for optimal data compression, while FA is a statistical model for discovering latent structure. PCA creates composite variables that explain maximum variance; FA posits that latent factors cause observed variables to covary. PCA analyzes total variance; FA analyzes only shared variance.The original chapter's conflation of these methods is a common but serious error. By using a Factor Analysis library and methodology while labeling the output as PCA, it misleads the reader about the purpose, assumptions, and interpretation of the results. This corrected chapter has sought to rectify this by providing distinct theoretical and practical guides for each technique. PCA was used to solve the tangible problem of multicollinearity in a regression context, producing a more stable predictive model. Factor Analysis was used to explore a psychological dataset, successfully uncovering a theoretically meaningful latent structure of cognitive abilities.Ultimately, the choice of method hinges on the research question. For the data scientist focused on building robust predictive models, PCA is a vital preprocessing tool. For the researcher seeking to understand the hidden forces shaping their data, Factor Analysis is the appropriate instrument for discovery. Choosing the right tool for the job is the hallmark of an expert practitioner.Jupyter NotebookA complete, runnable Jupyter Notebook containing the code for both the Principal Component Regression and Exploratory Factor Analysis examples presented in this chapter is available on GitHub in the(https://github.com/Digital-Media-Lab/Hyperparameter-Tuning-Cookbook).Further Reading and ResourcesPrincipal Component Analysis:scikit-learn documentation for PCA: sklearn.decomposition.PCA 22A step-by-step visual explanation of PCA:(https://www.youtube.com/watch?v=FgakZw6K1QQ)Factor Analysis:factor-analyzer library documentation: FactorAnalyzer on PyPI 33An introductory guide to FA:(https://stats.oarc.ucla.edu/spss/seminars/efa-spss/) 21