{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "# Sequential Parameter Optimization: Using `scipy` Optimizers {#sec-scipy-optimizers}\n",
        "\n",
        "As a default optimizer, `spotpython` uses `differential_evolution` from the `scipy.optimize` package. Alternatively, any other optimizer from the `scipy.optimize` package can be used. This chapter describes how different optimizers from the `scipy optimize` package can be used on the surrogate.\n",
        "The optimization algorithms are available from [https://docs.scipy.org/doc/scipy/reference/optimize.html](https://docs.scipy.org/doc/scipy/reference/optimize.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 04_imports\n",
        "import numpy as np\n",
        "from math import inf\n",
        "from spotpython.fun.objectivefunctions import Analytical\n",
        "from spotpython.spot import Spot\n",
        "from scipy.optimize import shgo\n",
        "from scipy.optimize import direct\n",
        "from scipy.optimize import differential_evolution\n",
        "from scipy.optimize import dual_annealing\n",
        "from scipy.optimize import basinhopping\n",
        "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Objective Function Branin\n",
        "\n",
        "The `spotpython` package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula. Here we will use the Branin function. The 2-dim Branin function is\n",
        "$$\n",
        "y = a  (x_2 - b  x_1^2 + c  x_1 - r) ^2 + s  (1 - t)  \\cos(x_1) + s,\n",
        "$$\n",
        "where values of $a$, $b$, $c$, $r$, $s$ and $t$ are: $a = 1$, $b = 5.1 / (4\\pi^2)$, $c = 5 / \\pi$, $r = 6$, $s = 10$ and $t = 1 / (8\\pi)$.\n",
        "\n",
        "It has three global minima: $f(x) = 0.397887$ at $(-\\pi, 12.275)$, $(\\pi, 2.275)$, and $(9.42478, 2.475)$.\n",
        "\n",
        "Input Domain: This function is usually evaluated on the square  $x_1 \\in  [-5, 10] \\times x_2 \\in  [0, 15]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 04_objective_function\n",
        "from spotpython.fun.objectivefunctions import Analytical\n",
        "lower = np.array([-5,-0])\n",
        "upper = np.array([10,15])\n",
        "fun = Analytical(seed=123).fun_branin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Optimizer{#sec-optimizer}\n",
        "\n",
        "Differential Evolution (DE) from the `scikit.optimize` package, see [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution) is the default optimizer for the search on the surrogate.\n",
        "Other optimiers that are available in `spotpython`, see [https://docs.scipy.org/doc/scipy/reference/optimize.html#global-optimization](https://docs.scipy.org/doc/scipy/reference/optimize.html#global-optimization).\n",
        "\n",
        "  * `dual_annealing`\n",
        "  * `direct`\n",
        "  * `shgo`\n",
        "  * `basinhopping`\n",
        "\n",
        "These optimizers can be selected as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: 04_optimizer_control\n",
        "#| eval: false\n",
        "from scipy.optimize import differential_evolution\n",
        "optimizer = differential_evolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As noted above, we will use `differential_evolution`. The optimizer can use `1000` evaluations. This value will be passed to the `differential_evolution` method, which has the argument `maxiter` (int). It defines the maximum number of generations over which the entire differential evolution population is evolved, see [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution)\n",
        "\n",
        "\n",
        ":::{.callout-note}\n",
        "#### TensorBoard\n",
        "\n",
        "Similar to the one-dimensional case, which is discussed in @sec-visualizing-tensorboard-01, we can use TensorBoard to monitor the progress of the optimization. We will use a similar code, only the prefix is different:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fun_control=fun_control_init(\n",
        "                    lower = lower,\n",
        "                    upper = upper,\n",
        "                    fun_evals = 20,\n",
        "                    PREFIX = \"04_DE_\"\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_de = Spot(fun=fun,\n",
        "                    fun_control=fun_control)\n",
        "spot_de.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorBoard\n",
        "\n",
        "If the `prefix` argument in `fun_control_init()`is not `None` (as above, where the `prefix` was set to `04_DE_`) , we can start TensorBoard in the background with the following command:\n",
        "\n",
        "```{raw}\n",
        "tensorboard --logdir=\"./runs\"\n",
        "```\n",
        "\n",
        "We can access the TensorBoard web server with the following URL:\n",
        "\n",
        "```{raw}\n",
        "http://localhost:6006/\n",
        "```\n",
        "\n",
        "The TensorBoard plot illustrates how `spotpython` can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate $\\theta$ of the Kriging surrogate is plotted against the number of optimization steps.\n",
        "\n",
        "![TensorBoard visualization of the spotpython optimization process and the surrogate model.](figures_static/05_tensorboard_01.png){width=\"100%\"}\n",
        "\n",
        "\n",
        "## Print the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_de.print_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the Progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_de.plot_progress(log_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_de.surrogate.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "\n",
        "### `dual_annealing`\n",
        "\n",
        "* Describe the optimization algorithm, see [scipy.optimize.dual_annealing](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html).\n",
        "* Use the algorithm as an optimizer on the surrogate.\n",
        "\n",
        ":::{.callout-tip}\n",
        "##### Tip: Selecting the Optimizer for the Surrogate\n",
        "\n",
        "We can run spotpython with the `dual_annealing` optimizer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_da = Spot(fun=fun,\n",
        "                    fun_control=fun_control,\n",
        "                    optimizer=dual_annealing)\n",
        "spot_da.run()\n",
        "spot_da.print_results()\n",
        "spot_da.plot_progress(log_y=True)\n",
        "spot_da.surrogate.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### `direct`\n",
        "\n",
        "* Describe the optimization algorithm\n",
        "* Use the algorithm as an optimizer on the surrogate\n",
        "\n",
        ":::{.callout-tip}\n",
        "##### Tip: Selecting the Optimizer for the Surrogate\n",
        "\n",
        "We can run spotpython with the `direct` optimizer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_di = Spot(fun=fun,\n",
        "                    fun_control=fun_control,\n",
        "                    optimizer=direct)\n",
        "spot_di.run()\n",
        "spot_di.print_results()\n",
        "spot_di.plot_progress(log_y=True)\n",
        "spot_di.surrogate.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "### `shgo`\n",
        "\n",
        "* Describe the optimization algorithm\n",
        "* Use the algorithm as an optimizer on the surrogate\n",
        "\n",
        ":::{.callout-tip}\n",
        "##### Tip: Selecting the Optimizer for the Surrogate\n",
        "\n",
        "We can run spotpython with the `direct` optimizer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_sh = Spot(fun=fun,\n",
        "                    fun_control=fun_control,\n",
        "                    optimizer=shgo)\n",
        "spot_sh.run()\n",
        "spot_sh.print_results()\n",
        "spot_sh.plot_progress(log_y=True)\n",
        "spot_sh.surrogate.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "\n",
        "### `basinhopping`\n",
        "\n",
        "* Describe the optimization algorithm\n",
        "* Use the algorithm as an optimizer on the surrogate\n",
        "\n",
        ":::{.callout-tip}\n",
        "##### Tip: Selecting the Optimizer for the Surrogate\n",
        "\n",
        "We can run spotpython with the `direct` optimizer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spot_bh = Spot(fun=fun,\n",
        "                    fun_control=fun_control,\n",
        "                    optimizer=basinhopping)\n",
        "spot_bh.run()\n",
        "spot_bh.print_results()\n",
        "spot_bh.plot_progress(log_y=True)\n",
        "spot_bh.surrogate.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "### Performance Comparison\n",
        "\n",
        "Compare the performance and run time of the 5 different optimizers:\n",
        "\n",
        "  * `differential_evolution`\n",
        "  * `dual_annealing`\n",
        "  *  `direct`\n",
        "  * `shgo`\n",
        "  * `basinhopping`.\n",
        "\n",
        "The Branin function has three global minima:\n",
        "\n",
        "* $f(x) = 0.397887$  at \n",
        "  * $(-\\pi, 12.275)$, \n",
        "  * $(\\pi, 2.275)$, and \n",
        "  * $(9.42478, 2.475)$.    \n",
        "* Which optima are found by the optimizers?\n",
        "* Does the `seed` argument in `fun = Analytical(seed=123).fun_branin` change this behavior?\n",
        "\n",
        "## Jupyter Notebook\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "* The Jupyter-Notebook of this chapter is available on GitHub in the [Hyperparameter-Tuning-Cookbook Repository](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/004_spot_sklearn_optimization.ipynb)\n",
        "\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}