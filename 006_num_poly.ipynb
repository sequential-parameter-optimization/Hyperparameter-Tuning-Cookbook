{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "# Polynomial Models\n",
        "\n",
        "::: {.callout-note}\n",
        "### Note\n",
        "This section is based on chapter 2.2 in @Forr08a.\n",
        ":::\n",
        "\n",
        "## Fitting a Polynomial\n",
        "\n",
        "Let us consider the scalar-valued function $f$ observed according to the sampling plan\n",
        "$X = \\{x^{(1)}, x^{(2)} \\dots, x^{(n)}\\}^T$,\n",
        "yielding the responses \n",
        "$\\vec{y} = \\{y^{(1)}, y^{(2)}, \\dots, y^{(n)}\\}^T$.\n",
        "\n",
        "A polynomial approximation of $f$ of order $m$ can be written as:\n",
        "\n",
        "$$\n",
        "\\hat{f}(x, m, \\vec{w}) = \\sum_{i=0}^m w_i x^i.\n",
        "$$\n",
        "\n",
        "\n",
        "In the spirit of the earlier discussion of maximum likelihood parameter estimation, we seek to estimate $w = {w_0, w_1, \\dots, w_m}^T$ through a least squares solution of:\n",
        "\n",
        "$$\n",
        "\\Phi \\vec{w} = \\vec{y}\n",
        "$$\n",
        "where $\\Phi$ is the Vandermonde matrix:\n",
        "\n",
        "$$\n",
        "\\Phi =\n",
        "\\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & \\dots & x_1^m \\\\\n",
        "1 & x_2 & x_2^2 & \\dots & x_2^m \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_n & x_n^2 & \\dots & x_n^m\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "The maximum likelihood estimate of $w$ is given by:\n",
        "\n",
        "$$\n",
        "\\vec{w} = (\\Phi^T \\Phi)^{-1} \\Phi^T y,\n",
        "$$\n",
        "\n",
        "where $\\Phi^+ = (\\Phi^T \\Phi)^{-1} \\Phi^T$ is the Moore-Penrose pseudo-inverse of $\\Phi$ (see @sec-matrix-pseudoinverse).\n",
        "\n",
        "The polynomial approximation of order $m$ is essentially a truncated Taylor series expansion.\n",
        "While higher values of $m$ yield more accurate approximations, they risk overfitting the noise in the data.\n",
        "\n",
        "To prevent this, we estimate $m$ using cross-validation. This involves minimizing the cross-validation error over a discrete set of possible orders $m$ (e.g., $m \\in {1, 2, \\dots, 15}$).\n",
        "\n",
        "For each $m$, the data is split into $q$ subsets. The model is trained on $q-1$ subsets, and the error is computed on the left-out subset. This process is repeated for all subsets, and the cross-validation error is summed. The order $m$ with the smallest cross-validation error is chosen.\n",
        "\n",
        "## Polynomial Fitting in Python\n",
        "\n",
        "### Fitting the Polynomial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: polynomial_fit\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def polynomial_fit(X, Y, max_order=15, q=5):\n",
        "    \"\"\"\n",
        "    Fits a one-variable polynomial to one-dimensional data using cross-validation.\n",
        "\n",
        "    Args:\n",
        "        X (array-like): Training data vector (independent variable).\n",
        "        Y (array-like): Training data vector (dependent variable).\n",
        "        max_order (int): Maximum polynomial order to consider. Default is 15.\n",
        "        q (int): Number of cross-validation folds. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        best_order (int): The optimal polynomial order.\n",
        "        coeff (array): Coefficients of the best-fit polynomial.\n",
        "        mu (tuple): Normalization parameters (mean, std) for X.\n",
        "    \"\"\"\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    n = len(X)\n",
        "\n",
        "    # Normalize X\n",
        "    mu = (np.mean(X), np.std(X))\n",
        "    X_norm = (X - mu[0]) / mu[1]\n",
        "    # Cross-validation setup\n",
        "    kf = KFold(n_splits=q, shuffle=True, random_state=42)\n",
        "    cross_val_errors = np.zeros(max_order)\n",
        "    for order in range(1, max_order + 1):\n",
        "        fold_errors = []\n",
        "\n",
        "        for train_idx, val_idx in kf.split(X_norm):\n",
        "            X_train, X_val = X_norm[train_idx], X_norm[val_idx]\n",
        "            Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
        "\n",
        "            # Fit polynomial\n",
        "            coeff = np.polyfit(X_train, Y_train, order)\n",
        "\n",
        "            # Predict on validation set\n",
        "            Y_pred = np.polyval(coeff, X_val)\n",
        "\n",
        "            # Compute mean squared error\n",
        "            mse = np.mean((Y_val - Y_pred) ** 2)\n",
        "            fold_errors.append(mse)\n",
        "\n",
        "        cross_val_errors[order - 1] = np.mean(fold_errors)\n",
        "\n",
        "    # Find the best order\n",
        "    best_order = np.argmin(cross_val_errors) + 1\n",
        "\n",
        "    # Fit the best polynomial on the entire dataset\n",
        "    best_coeff = np.polyfit(X_norm, Y, best_order)\n",
        "\n",
        "    return best_order, best_coeff, mu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making Predictions\n",
        "\n",
        "To make predictions, we can use the coefficients. The data is standardized around its mean in the polynomial function, which is why the vector `mnstd` is required. The coefficient vector is computed based on the normalized data, and this must be taken into account if further analytical calculations are performed on the fitted model.\n",
        "\n",
        "The polynomial approximation of $C_D$ is:\n",
        "\n",
        "$$\n",
        "C_D(x) = w_8 x^8 + w_7 x^7 + \\dots + w_1 x + w_0,\n",
        "$$\n",
        "\n",
        "where $x$ is normalized as:\n",
        "\n",
        "$$\n",
        "\\bar{x} = \\frac{x - \\mu(X)}{\\sigma(X)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict_polynomial_fit(X, coeff, mu):\n",
        "    \"\"\"\n",
        "    Generates predictions for the polynomial fit.\n",
        "\n",
        "    Args:\n",
        "        X (array-like): Original independent variable data.\n",
        "        coeff (array): Coefficients of the best-fit polynomial.\n",
        "        mu (tuple): Normalization parameters (mean, std) for X.\n",
        "\n",
        "    Returns:\n",
        "        tuple: De-normalized predicted X values and corresponding Y predictions.\n",
        "    \"\"\"\n",
        "    # Normalize X\n",
        "    X_norm = (X - mu[0]) / mu[1]\n",
        "\n",
        "    # Generate predictions\n",
        "    X_pred = np.linspace(min(X_norm), max(X_norm), 100)\n",
        "    Y_pred = np.polyval(coeff, X_pred)\n",
        "\n",
        "    # De-normalize X for plotting\n",
        "    X_pred_original = X_pred * mu[1] + mu[0]\n",
        "\n",
        "    return X_pred_original, Y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_polynomial_fit(X, Y, X_pred_original, Y_pred, best_order, y_true=None):\n",
        "    \"\"\"\n",
        "    Visualizes the polynomial fit.\n",
        "\n",
        "    Args:\n",
        "        X (array-like): Original independent variable data.\n",
        "        Y (array-like): Original dependent variable data.\n",
        "        X_pred_original (array): De-normalized predicted X values.\n",
        "        Y_pred (array): Predicted Y values.\n",
        "        y_true (array): True Y values.\n",
        "        best_order (int): The optimal polynomial order.\n",
        "    \"\"\"\n",
        "    plt.scatter(X, Y, label=\"Training Data\", color=\"grey\", marker=\"o\")\n",
        "    plt.plot(X_pred_original, Y_pred, label=f\"Order {best_order} Polynomial\", color=\"red\")\n",
        "    if y_true is not None:\n",
        "        plt.plot(X, y_true, label=\"True Function\", color=\"blue\", linestyle=\"--\")\n",
        "    plt.title(f\"Polynomial Fit (Order {best_order})\")\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example One: Aerofoil Drag\n",
        "\n",
        "The circles in @fig-aerofoil-drag represent 101 drag coefficient values obtained through a numerical simulation by iterating each member of a family of aerofoils towards a target lift value (see the Appendix, Section A.3 in @Forr08a). The members of the family have different shapes, as determined by the sampling plan:\n",
        "\n",
        "$$\n",
        "X = {x_1, x_2, \\dots, x_{101}}\n",
        "$$\n",
        "\n",
        "The responses are:\n",
        "\n",
        "$$\n",
        "C_D = \\{C_{D}^{(1)}, C_{D}^{(2)}, \\dots, C_{D}^{(101)}\\}\n",
        "$$\n",
        "\n",
        "These responses are corrupted by \"noise,\" which are deviations of the systematic variety caused by small changes in the computational mesh from one design to the next.\n",
        "\n",
        "The original data is measured in natural units, i.e., from $-0.3$ untion to $0.1$ unit. The data is normalized to the range of $0$ to $1$ for the computation with the `aerofoilcd` function. The data is then fitted with a polynomial of order $m$.\n",
        "To obtain the best polynomial through this data, the following Python code can be used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotpython.surrogate.functions.forr08a import aerofoilcd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X = np.linspace(-0.3, 0.1, 101)\n",
        "# normalize the data so that it will be in the range of 0 to 1\n",
        "a = np.min(X)\n",
        "b = np.max(X)\n",
        "X_cod = (X - a) / (b - a)\n",
        "y = aerofoilcd(X_cod)\n",
        "best_order, best_coeff, mnstd = polynomial_fit(X, y)\n",
        "X_pred_original, Y_pred = predict_polynomial_fit(X, best_coeff, mnstd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-aerofoil-drag\n",
        "#| fig-cap: Aerofoil drag data\n",
        "plot_polynomial_fit(X, y, X_pred_original, Y_pred, best_order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "@fig-aerofoil-drag shows an eighth-order polynomial fitted through the aerofoil drag data. The order was selected via cross-validation, and the coefficients were determined through likelihood maximization.\n",
        "Results, i.e, the best polynomial order and coefficients, are printed in the console. The coefficients are stored in the vector `best_coeff`, which contains the coefficients of the polynomial in descending order. The first element is the coefficient of $x^8$, and the last element is the constant term.\n",
        "The vector `mnstd`, containing the mean and standard deviation of $X$, is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: aerofoil-coefficients\n",
        "print(f\"Best polynomial order: {best_order}\\n\")\n",
        "print(f\"Coefficients (starting with w0):\\n {best_coeff}\\n\")\n",
        "print(f\"Normalization parameters (mean, std):\\n {mnstd}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Example Two: A Multimodal Test Case\n",
        "\n",
        "Let us consider the one-variable test function:\n",
        "\n",
        "$$\n",
        "f(x) = (6x - 2)^2 \\sin(12x - 4).\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from spotpython.surrogate.functions.forr08a import onevar\n",
        "X = np.linspace(0, 1, 51)\n",
        "y_true = onevar(X)\n",
        "# initialize random seed\n",
        "np.random.seed(42)\n",
        "y = y_true + np.random.normal(0, 1, len(X))*1.1\n",
        "best_order, best_coeff, mnstd = polynomial_fit(X, y)\n",
        "X_pred_original, Y_pred = predict_polynomial_fit(X, best_coeff, mnstd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-onevar\n",
        "#| fig-cap: Onevar function\n",
        "plot_polynomial_fit(X, y, X_pred_original, Y_pred, best_order, y_true=y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function, depicted by the dotted line in @fig-onevar, has local minima of different depths, which can be deceptive to some surrogate-based optimization procedures. Here, we use it as an example of a multimodal function for polynomial fitting.\n",
        "\n",
        "We generate the training data (depicted by circles in @fig-onevar) by adding normally distributed noise to the function.\n",
        "@fig-onevar shows a seventh-order polynomial fitted through the noisy data. This polynomial was selected as it minimizes the cross-validation metric.\n",
        "\n",
        "## Extending to Multivariable Polynomial Models\n",
        "\n",
        "While the examples above focus on the one-variable case, real-world engineering problems typically involve multiple input variables. For $k$-variable problems, polynomial approximation becomes significantly more complex but follows the same fundamental principles.\n",
        "For a $k$-dimensional input space, the polynomial approximation can be expressed as:\n",
        "\n",
        "$$\n",
        "\\hat{f}(\\vec{x}) = \\sum_{i=1}^N w_i \\phi_i(\\vec{x}),\n",
        "$$\n",
        "where $\\phi_i(\\vec{x})$ represents multivariate basis functions, and $N$ is the total number of terms in the polynomial. Unlike the univariate case, these basis functions include all possible combinations of variables up to the selected polynomial order $m$, which might result in a  \"basis function explosion\" as the number of variables increases.\n",
        "\n",
        "For a third-order polynomial ($m = 3$) with three variables ($k = 3$), the complete set of basis functions would include 20 terms:\n",
        "\n",
        "\\begin{align} \n",
        "\\text{Constant term: } & {1} \\\\\n",
        "\\text{First-order terms: } & {x_1, x_2, x_3} \\\\\n",
        "\\text{Second-order terms: } & {x_1^2, x_2^2, x_3^2, x_1x_2, x_1x_3, x_2x_3} \\\\\n",
        "\\text{Third-order terms: } & {x_1^3, x_2^3, x_3^3, x_1^2x_2, x_1^2x_3, x_2^2x_1, x_2^2x_3, x_3^2x_1, x_3^2x_2, x_1x_2x_3}\n",
        "\\end{align} \n",
        "\n",
        "The total number of terms grows combinatorially as $N = \\binom{k+m}{m}$, which quickly becomes prohibitive as dimensionality increases.\n",
        "For example, a 10-variable cubic polynomial requires $\\binom{13}{3} = 286$ coefficients!\n",
        "This exponential growth creates three interrelated challenges:\n",
        "\n",
        "* Model Selection: Determining the appropriate polynomial order $m$ that balances complexity with generalization ability\n",
        "* Coefficient Estimation: Computing the potentially large number of weights $\\vec{w}$ while avoiding numerical instability\n",
        "* Term Selection: Identifying which specific basis functions should be included, as many may be irrelevant to the response\n",
        "\n",
        "Several techniques have been developed to address these challenges:\n",
        "\n",
        "* Regularization methods (LASSO, ridge regression) that penalize model complexity\n",
        "* Stepwise regression algorithms that incrementally add or remove terms\n",
        "* Dimension reduction techniques that project the input space to lower dimensions\n",
        "* Orthogonal polynomials that improve numerical stability for higher-order models\n",
        "\n",
        "These limitations of polynomial models in higher dimensions motivate the exploration of more flexible surrogate modeling approaches like Radial Basis Functions and Kriging, which we'll examine in subsequent sections.\n",
        "\n",
        "## Jupyter Notebook\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "* The Jupyter-Notebook of this lecture is available on GitHub in the [Hyperparameter-Tuning-Cookbook Repository](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_poly.ipynb)\n",
        "\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/miniforge3/envs/spot312/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}