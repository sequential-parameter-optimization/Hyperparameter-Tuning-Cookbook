% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Hyperparameter Tuning Cookbook},
  pdfauthor={Thomas Bartz-Beielstein},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Hyperparameter Tuning Cookbook}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A guide for scikit-learn, PyTorch, river, and spotPython}
\author{Thomas Bartz-Beielstein}
\date{Nov 8, 2023}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, interior hidden, sharp corners, enhanced, boxrule=0pt, breakable, borderline west={3pt}{0pt}{shadecolor}]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\begin{quote}
This document provides a comprehensive guide to hyperparameter tuning
using spotPython for scikit-learn, scipy-optimize, River, and PyTorch.
The first part introduces fundamental ideas from optimization. The
second part discusses numerical issues and introduces spotPython's
surrogate model-based optimization process. The thirs part focuses on
hyperparameter tuning. Several case studies are presented, including
hyperparameter tuning for sklearn models such as Support Vector
Classification, Random Forests, Gradient Boosting (XGB), and K-nearest
neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from
river. The integration of spotPython into the PyTorch and PyTorch
Lightning training workflow is also discussed. With a hands-on approach
and step-by-step explanations, this cookbook serves as a practical
starting point for anyone interested in hyperparameter tuning with
Python. Highlights include the interplay between Tensorboard, PyTorch
Lightning, spotPython, spotRiver, and River. This publication is under
development, with updates available on the corresponding webpage.
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important: This book is still under development.}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The most recent version of this book is available at
\url{https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/}

\end{tcolorbox}

\hypertarget{book-structure}{%
\section*{Book Structure}\label{book-structure}}
\addcontentsline{toc}{section}{Book Structure}

\markright{Book Structure}

This document is structured in three parts. The first part presents an
introduction to optimization. The second part describes numerical
methods, and the third part presents hyperparameter tuning.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Hyperparameter Tuning Reference}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, colframe=quarto-callout-tip-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  The open access book Bartz et al. (2022) provides a comprehensive
  overview of hyperparameter tuning. It can be downloaded from
  \url{https://link.springer.com/book/10.1007/978-981-19-5170-1}.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The \texttt{.ipynb} notebook (Bartz-Beielstein 2023) is updated
regularly and reflects updates and changes in the \texttt{spotPython}
package. It can be downloaded from
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb}.

\end{tcolorbox}

\hypertarget{software-used-in-this-book}{%
\section*{Software Used in this Book}\label{software-used-in-this-book}}
\addcontentsline{toc}{section}{Software Used in this Book}

\markright{Software Used in this Book}

\href{https://scikit-learn.org}{scikit-learn} is a Python module for
machine learning built on top of SciPy and is distributed under the
3-Clause BSD license. The project was started in 2007 by David
Cournapeau as a Google Summer of Code project, and since then many
volunteers have contributed.

\href{https://pytorch.org}{PyTorch} is an optimized tensor library for
deep learning using GPUs and CPUs.
\href{https://lightning.ai/docs/pytorch/latest/}{Lightning} is a
lightweight PyTorch wrapper for high-performance AI research. It allows
you to decouple the research from the engineering.

\href{https://riverml.xyz}{River} is a Python library for online machine
learning. It is designed to be used in real-world environments, where
not all data is available at once, but streaming in.

\href{https://github.com/sequential-parameter-optimization/spotPython}{spotPython}
(``Sequential Parameter Optimization Toolbox in Python'') is the Python
version of the well-known hyperparameter tuner SPOT, which has been
developed in the R programming environment for statistical analysis for
over a decade. The related open-access book is available here:
\href{https://link.springer.com/book/10.1007/978-981-19-5170-1}{Hyperparameter
Tuning for Machine and Deep Learning with R---A Practical Guide}.

\href{https://github.com/sequential-parameter-optimization/spotRiver}{spotRiver}
provides an interface between
\href{https://github.com/sequential-parameter-optimization/spotPython}{spotPython}
and \href{https://riverml.xyz}{River}.

\hypertarget{citation}{%
\section*{Citation}\label{citation}}
\addcontentsline{toc}{section}{Citation}

\markright{Citation}

If this document has been useful to you and you wish to cite it in a
scientific publication, please refer to the following paper, which can
be found on arXiv: \url{https://arxiv.org/abs/2307.10262}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{@ARTICLE\{bart23iArXiv,}
\NormalTok{      author = \{\{Bartz{-}Beielstein\}, Thomas\},}
\NormalTok{      title = "\{Hyperparameter Tuning Cookbook:}
\NormalTok{          A guide for scikit{-}learn, PyTorch, river, and spotPython\}",}
\NormalTok{     journal = \{arXiv e{-}prints\},}
\NormalTok{    keywords = \{Computer Science {-} Machine Learning,}
\NormalTok{      Computer Science {-} Artificial Intelligence, 90C26, I.2.6, G.1.6\},}
\NormalTok{         year = 2023,}
\NormalTok{        month = jul,}
\NormalTok{          eid = \{arXiv:2307.10262\},}
\NormalTok{        pages = \{arXiv:2307.10262\},}
\NormalTok{          doi = \{10.48550/arXiv.2307.10262\},}
\NormalTok{archivePrefix = \{arXiv\},}
\NormalTok{       eprint = \{2307.10262\},}
\NormalTok{ primaryClass = \{cs.LG\},}
\NormalTok{       adsurl = \{https://ui.adsabs.harvard.edu/abs/2023arXiv230710262B\},}
\NormalTok{      adsnote = \{Provided by the SAO/NASA Astrophysics Data System\}}
\NormalTok{\}}

\end{Highlighting}
\end{Shaded}

\part{Optimization}

\hypertarget{introduction-optimization}{%
\chapter{Introduction: Optimization}\label{introduction-optimization}}

\hypertarget{optimization-simulation-and-surrogate-modeling}{%
\section{Optimization, Simulation, and Surrogate
Modeling}\label{optimization-simulation-and-surrogate-modeling}}

\begin{itemize}
\tightlist
\item
  We will consider the interplay between

  \begin{itemize}
  \tightlist
  \item
    mathematical models,
  \item
    numerical approximation,
  \item
    simulation,
  \item
    computer experiments, and
  \item
    field data
  \end{itemize}
\item
  Experimental design will play a key role in our developments, but not
  in the classical regression and response surface methodology sense
\item
  Challenging real-data/real-simulation examples benefiting from modern
  surrogate modeling methodology
\item
  We will consider the classical, response surface methodology (RSM)
  approach, and then move on to more modern approaches
\item
  All approaches are based on surrogates
\end{itemize}

\hypertarget{surrogates}{%
\section{Surrogates}\label{surrogates}}

\begin{itemize}
\tightlist
\item
  Gathering data is \textbf{expensive}, and sometimes getting exactly
  the data you want is impossible or unethical
\item
  \textbf{Surrogate}: substitute for the real thing
\item
  In statistics, draws from predictive equations derived from a fitted
  model can act as a surrogate for the data-generating mechanism
\item
  Benefits of the surrogate approach:

  \begin{itemize}
  \tightlist
  \item
    Surrogate could represent a cheaper way to explore relationships,
    and entertain ``what ifs?''
  \item
    Surrogates favor faithful yet pragmatic reproduction of dynamics:

    \begin{itemize}
    \tightlist
    \item
      interpretation,
    \item
      establishing causality, or
    \item
      identification
    \end{itemize}
  \item
    Many numerical simulators are \textbf{deterministic}, whereas field
    observations are noisy or have measurement error
  \end{itemize}
\end{itemize}

\hypertarget{costs-of-simulation}{%
\subsection{Costs of Simulation}\label{costs-of-simulation}}

\begin{itemize}
\tightlist
\item
  Computer simulations are generally cheaper (but not always!) than
  physical observation
\item
  Some computer simulations can be just as expensive as field
  experimentation, but computer modeling is regarded as easier because:

  \begin{itemize}
  \tightlist
  \item
    the experimental apparatus is better understood
  \item
    more aspects may be controlled.
  \end{itemize}
\end{itemize}

\hypertarget{mathematical-models-and-meta-models}{%
\subsection{Mathematical Models and
Meta-Models}\label{mathematical-models-and-meta-models}}

\begin{itemize}
\tightlist
\item
  Use of mathematical models leveraging numerical solvers has been
  commonplace for some time
\item
  Mathematical models became more complex, requiring more resources to
  simulate/solve numerically
\item
  Practitioners increasingly relied on \textbf{meta-models} built off of
  limited simulation campaigns
\end{itemize}

\hypertarget{surrogates-trained-meta-models}{%
\subsection{Surrogates = Trained
Meta-models}\label{surrogates-trained-meta-models}}

\begin{itemize}
\tightlist
\item
  Data collected via expensive computer evaluations tuned flexible
  functional forms that could be used in lieu of further simulation to

  \begin{itemize}
  \tightlist
  \item
    save money or computational resources;
  \item
    cope with an inability to perform future runs (expired licenses,
    off-line or over-impacted supercomputers)
  \end{itemize}
\item
  Trained meta-models became known as \textbf{surrogates}
\end{itemize}

\hypertarget{computer-experiments}{%
\subsection{Computer Experiments}\label{computer-experiments}}

\begin{itemize}
\tightlist
\item
  \textbf{Computer experiment}: design, running, and fitting
  meta-models.

  \begin{itemize}
  \tightlist
  \item
    Like an ordinary statistical experiment, except the data are
    generated by computer codes rather than physical or field
    observations, or surveys
  \end{itemize}
\item
  \textbf{Surrogate modeling} is statistical modeling of computer
  experiments
\end{itemize}

\hypertarget{limits-of-mathematical-modeling}{%
\subsection{Limits of Mathematical
Modeling}\label{limits-of-mathematical-modeling}}

\begin{itemize}
\tightlist
\item
  Mathematical biologists, economists and others had reached the limit
  of equilibrium-based mathematical modeling with cute closed-form
  solutions
\item
  \textbf{Stochastic simulations replace deterministic solvers} based on
  FEM, Navier--Stokes or Euler methods
\item
  Agent-based simulation models are used to explore predator-prey
  (Lotka--Voltera) dynamics, spread of disease, management of inventory
  or patients in health insurance markets
\item
  Consequence: the distinction between surrogate and statistical model
  is all but gone
\end{itemize}

\hypertarget{example-why-computer-simulations-are-necessary}{%
\subsection{Example: Why Computer Simulations are
Necessary}\label{example-why-computer-simulations-are-necessary}}

\begin{itemize}
\tightlist
\item
  You can't seed a real community with Ebola and watch what happens
\item
  If there's (real) field data, say on a historical epidemic, further
  experimentation may be almost entirely limited to the mathematical and
  computer modeling side
\item
  Classical statistical methods offer little guidance
\end{itemize}

\hypertarget{simulation-requirements}{%
\subsection{Simulation Requirements}\label{simulation-requirements}}

\begin{itemize}
\tightlist
\item
  Simulation should

  \begin{itemize}
  \tightlist
  \item
    enable rich \textbf{diagnostics} to help criticize that models
  \item
    \textbf{understanding} its sensitivity to inputs and other
    configurations
  \item
    providing the ability to \textbf{optimize} and
  \item
    refine both \textbf{automatically} and with expert intervention
  \end{itemize}
\item
  And it has to do all that while remaining \textbf{computationally
  tractable}
\item
  One perspective is so-called \textbf{response surface methods} (RSMs):
\item
  a poster child from industrial statistics' heyday, well before
  information technology became a dominant industry
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Goals}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  How to choose models and optimizers for solving real-world problems
\item
  How to use simulation to understand and improve processes
\end{itemize}

\end{tcolorbox}

\hypertarget{aircraft-wing-weight-example}{%
\chapter{Aircraft Wing Weight
Example}\label{aircraft-wing-weight-example}}

\hypertarget{awwe-equation}{%
\section{AWWE Equation}\label{awwe-equation}}

\begin{itemize}
\tightlist
\item
  Example from Forrester et al.~
\item
  Understand the \textbf{weight} of an unpainted light aircraft wing as
  a function of nine design and operational parameters:
\end{itemize}

\[ W = 0.036 S_W^{0.758} \times W_{fw}^{0.0035} \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times  q^{0.006}  \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49}\]

\hypertarget{awwe-parameters-and-equations-part-1}{%
\section{AWWE Parameters and Equations (Part
1)}\label{awwe-parameters-and-equations-part-1}}

\hypertarget{tbl-awwe}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5063}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1266}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1139}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1139}}@{}}
\caption{\label{tbl-awwe}Aircraft Wing Weight Parameters}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(S_W\) & Wing area (\(ft^2\)) & 174 & 150 & 200 \\
\(W_{fw}\) & Weight of fuel in wing (lb) & 252 & 220 & 300 \\
\(A\) & Aspect ratio & 7.52 & 6 & 10 \\
\(\Lambda\) & Quarter-chord sweep (deg) & 0 & -10 & 10 \\
\(q\) & Dynamic pressure at cruise (\(lb/ft^2\)) & 34 & 16 & 45 \\
\(\lambda\) & Taper ratio & 0.672 & 0.5 & 1 \\
\(R_{tc}\) & Aerofoil thickness to chord ratio & 0.12 & 0.08 & 0.18 \\
\(N_z\) & Ultimate load factor & 3.8 & 2.5 & 6 \\
\(W_{dg}\) & Flight design gross weight (lb) & 2000 & 1700 & 2500 \\
\(W_p\) & paint weight (lb/ft\^{}2) & 0.064 & 0.025 & 0.08 \\
\end{longtable}

The study begins with a baseline Cessna C172 Skyhawk Aircraft as its
reference point. It aims to investigate the impact of wing area and fuel
weight on the overall weight of the aircraft. Two crucial parameters in
this analysis are the aspect ratio (\(A\)), defined as the ratio of the
wing's length to the average chord (thickness of the airfoil), and the
taper ratio (\(\lambda\)), which represents the ratio of the maximum to
the minimum thickness of the airfoil or the maximum to minimum chord.

It's important to note that the equation used in this context is not a
computer simulation but will be treated as one for the purpose of
illustration. This approach involves employing a true mathematical
equation, even if it's considered unknown, as a useful tool for
generating realistic settings to test the methodology. The functional
form of this equation was derived by ``calibrating'' known physical
relationships to curves obtained from existing aircraft data, as
referenced in Raymer 2012. Essentially, it acts as a surrogate for
actual measurements of aircraft weight.

Examining the mathematical properties of the AWWE (Aircraft Weight With
Wing Area and Fuel Weight Equation), it is evident that the response is
highly nonlinear concerning its inputs. While it's common to apply the
logarithm to simplify equations with complex exponents, even when
modeling the logarithm, which transforms powers into slope coefficients
and products into sums, the response remains nonlinear due to the
presence of trigonometric terms. Given the combination of nonlinearity
and high input dimension, simple linear and quadratic response surface
approximations are likely to be inadequate for this analysis.

\hypertarget{goals-understanding-and-optimization}{%
\section{Goals: Understanding and
Optimization}\label{goals-understanding-and-optimization}}

The primary goals of this study revolve around understanding and
optimization:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Understanding}: One of the straightforward objectives is to
  gain a deep understanding of the input-output relationships in this
  context. Given the global perspective implied by this setting, it
  becomes evident that a more sophisticated model is almost necessary.
  At this stage, let's focus on this specific scenario to establish a
  clear understanding.
\item
  \textbf{Optimization}: Another application of this analysis could be
  optimization. There may be an interest in minimizing the weight of the
  aircraft, but it's likely that there will be constraints in place. For
  example, the presence of wings with a nonzero area is essential for
  the aircraft to be capable of flying. In situations involving
  (constrained) optimization, a global perspective and, consequently,
  the use of flexible modeling are vital.
\end{enumerate}

The provided Python code serves as a genuine computer implementation
that ``solves'' a mathematical model. It accepts arguments encoded in
the unit cube, with defaults used to represent baseline settings, as
detailed in the table labeled as Table~\ref{tbl-awwe}. To map values
from the interval \([a, b]\) to the interval \([0, 1]\), the following
formula can be employed:

\[y = f(x) = \frac{x - a}{b - a}.\]

To reverse this mapping and obtain the original values, the formula
\[g(y) = a + (b - a) y\] can be used.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ wingwt(Sw}\OperatorTok{=}\FloatTok{0.48}\NormalTok{, Wfw}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, A}\OperatorTok{=}\FloatTok{0.38}\NormalTok{, L}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, q}\OperatorTok{=}\FloatTok{0.62}\NormalTok{, l}\OperatorTok{=}\FloatTok{0.344}\NormalTok{,  Rtc}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, Nz}\OperatorTok{=}\FloatTok{0.37}\NormalTok{, Wdg}\OperatorTok{=}\FloatTok{0.38}\NormalTok{):}
    \CommentTok{\# put coded inputs back on natural scale}
\NormalTok{    Sw }\OperatorTok{=}\NormalTok{ Sw }\OperatorTok{*}\NormalTok{ (}\DecValTok{200} \OperatorTok{{-}} \DecValTok{150}\NormalTok{) }\OperatorTok{+} \DecValTok{150} 
\NormalTok{    Wfw }\OperatorTok{=}\NormalTok{ Wfw }\OperatorTok{*}\NormalTok{ (}\DecValTok{300} \OperatorTok{{-}} \DecValTok{220}\NormalTok{) }\OperatorTok{+} \DecValTok{220} 
\NormalTok{    A }\OperatorTok{=}\NormalTok{ A }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}} \DecValTok{6}\NormalTok{) }\OperatorTok{+} \DecValTok{6} 
\NormalTok{    L }\OperatorTok{=}\NormalTok{ (L }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}}\NormalTok{ (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{)) }\OperatorTok{{-}} \DecValTok{10}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.pi}\OperatorTok{/}\DecValTok{180}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ q }\OperatorTok{*}\NormalTok{ (}\DecValTok{45} \OperatorTok{{-}} \DecValTok{16}\NormalTok{) }\OperatorTok{+} \DecValTok{16} 
\NormalTok{    l }\OperatorTok{=}\NormalTok{ l }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+} \FloatTok{0.5}  
\NormalTok{    Rtc }\OperatorTok{=}\NormalTok{ Rtc }\OperatorTok{*}\NormalTok{ (}\FloatTok{0.18} \OperatorTok{{-}} \FloatTok{0.08}\NormalTok{) }\OperatorTok{+} \FloatTok{0.08}
\NormalTok{    Nz }\OperatorTok{=}\NormalTok{ Nz }\OperatorTok{*}\NormalTok{ (}\DecValTok{6} \OperatorTok{{-}} \FloatTok{2.5}\NormalTok{) }\OperatorTok{+} \FloatTok{2.5}
\NormalTok{    Wdg }\OperatorTok{=}\NormalTok{ Wdg}\OperatorTok{*}\NormalTok{(}\DecValTok{2500} \OperatorTok{{-}} \DecValTok{1700}\NormalTok{) }\OperatorTok{+} \DecValTok{1700}
    \CommentTok{\# calculation on natural scale}
\NormalTok{    W }\OperatorTok{=} \FloatTok{0.036} \OperatorTok{*}\NormalTok{ Sw}\OperatorTok{**}\FloatTok{0.758} \OperatorTok{*}\NormalTok{ Wfw}\OperatorTok{**}\FloatTok{0.0035} \OperatorTok{*}\NormalTok{ (A}\OperatorTok{/}\NormalTok{np.cos(L)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\FloatTok{0.6} \OperatorTok{*}\NormalTok{ q}\OperatorTok{**}\FloatTok{0.006} 
\NormalTok{    W }\OperatorTok{=}\NormalTok{ W }\OperatorTok{*}\NormalTok{ l}\OperatorTok{**}\FloatTok{0.04} \OperatorTok{*}\NormalTok{ (}\DecValTok{100}\OperatorTok{*}\NormalTok{Rtc}\OperatorTok{/}\NormalTok{np.cos(L))}\OperatorTok{**}\NormalTok{(}\OperatorTok{{-}}\FloatTok{0.3}\NormalTok{) }\OperatorTok{*}\NormalTok{ (Nz}\OperatorTok{*}\NormalTok{Wdg)}\OperatorTok{**}\NormalTok{(}\FloatTok{0.49}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{(W)}
\end{Highlighting}
\end{Shaded}

\hypertarget{properties-of-the-python-solver}{%
\section{Properties of the Python
``Solver''}\label{properties-of-the-python-solver}}

The compute time required by the ``wingwt'' solver is extremely short
and can be considered trivial in terms of computational resources. The
approximation error is exceptionally small, effectively approaching
machine precision, which indicates the high accuracy of the solver's
results.

To simulate time-consuming evaluations, a deliberate delay is introduced
by incorporating a \texttt{sleep(3600)} command, which effectively
synthesizes a one-hour execution time for a particular evaluation.

Moving on to the AWWE visualization, plotting in two dimensions is
considerably simpler than dealing with nine dimensions. To aid in
creating visual representations, the code provided below establishes a
grid within the unit square to facilitate the generation of sliced
visuals. This involves generating a ``meshgrid'' as outlined in the
code.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zp }\OperatorTok{=} \BuiltInTok{zip}\NormalTok{(np.ravel(X), np.ravel(Y))}
\BuiltInTok{list}\NormalTok{(zp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[(0.0, 0.0),
 (0.5, 0.0),
 (1.0, 0.0),
 (0.0, 0.5),
 (0.5, 0.5),
 (1.0, 0.5),
 (0.0, 1.0),
 (0.5, 1.0),
 (1.0, 1.0)]
\end{verbatim}

The coding used to transform inputs from natural units is largely a
matter of taste, so long as it's easy to undo for reporting back on
original scales

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{matplotlib inline}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\CommentTok{\# plt.style.use(\textquotesingle{}seaborn{-}white\textquotesingle{})}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-1-load-factor-n_z-and-aspect-ratio-a}{%
\section{\texorpdfstring{Plot 1: Load Factor (\(N_z\)) and Aspect Ratio
(\(A\))}{Plot 1: Load Factor (N\_z) and Aspect Ratio (A)}}\label{plot-1-load-factor-n_z-and-aspect-ratio-a}}

We will vary \(N_z\) and \(A\), with other inputs fixed at their
baseline values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(A }\OperatorTok{=}\NormalTok{ X, Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{7.}\NormalTok{, }\FloatTok{5.}\NormalTok{))}
\NormalTok{plt.contourf(X, Y, z, }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Load factor (Nz) vs. Aspect Ratio (A)"}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{002_awwe_files/figure-pdf/cell-5-output-1.pdf}

}

\end{figure}

Contour plots can be refined, e.g., by adding explicit contour lines as
shown in the following figure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<matplotlib.colorbar.Colorbar at 0x16d6870d0>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{002_awwe_files/figure-pdf/cell-6-output-2.pdf}

}

\end{figure}

The interpretation of the AWWE plot can be summarized as follows:

\begin{itemize}
\tightlist
\item
  The figure displays the weight response as a function of two
  variables, \(N_z\) and \(A\), using an image-contour plot.
\item
  The slight curvature observed in the contours suggests an interaction
  between these two variables.
\item
  Notably, the range of outputs depicted in the figure, spanning from
  approximately 160 to 320, nearly encompasses the entire range of
  outputs observed from various input settings within the full
  9-dimensional input space.
\item
  The plot indicates that aircraft wings tend to be heavier when the
  aspect ratios (\(A\)) are high.
\item
  This observation aligns with the idea that wings are designed to
  withstand and accommodate high gravitational forces (\(g\)-forces,
  large \(N_z\)), and there may be a compounding effect where larger
  values of \(N_z\) contribute to increased wing weight.
\item
  It's plausible that this phenomenon is related to the design
  considerations of fighter jets, which cannot have the efficient and
  lightweight glider-like wings typically found in other types of
  aircraft.
\end{itemize}

\hypertarget{plot-2-taper-ratio-and-fuel-weight}{%
\section{Plot 2: Taper Ratio and Fuel
Weight}\label{plot-2-taper-ratio-and-fuel-weight}}

\begin{itemize}
\tightlist
\item
  The same experiment for two other inputs, e.g., taper ratio
  \(\lambda\) and fuel weight \(W_{fw}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(Wfw }\OperatorTok{=}\NormalTok{ X,  Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"WfW"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"l"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{002_awwe_files/figure-pdf/cell-7-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  Interpretation of Taper Ratio (\(l\)) and Fuel Weight (\(W_{fw}\))

  \begin{itemize}
  \tightlist
  \item
    Apparently, neither input has much effect on wing weight:

    \begin{itemize}
    \tightlist
    \item
      with \(\lambda\) having a marginally greater effect, covering less
      than 4 percent of the span of weights observed in the
      \(A \times N_z\) plane
    \end{itemize}
  \item
    There's no interaction evident in \(\lambda \times W_{fw}\)
  \end{itemize}
\end{itemize}

\hypertarget{the-big-picture-combining-all-variables}{%
\section{The Big Picture: Combining all
Variables}\label{the-big-picture-combining-all-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pl }\OperatorTok{=}\NormalTok{ [}\StringTok{"Sw"}\NormalTok{, }\StringTok{"Wfw"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"q"}\NormalTok{, }\StringTok{"l"}\NormalTok{,  }\StringTok{"Rtc"}\NormalTok{, }\StringTok{"Nz"}\NormalTok{, }\StringTok{"Wdg"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\NormalTok{Z }\OperatorTok{=}\NormalTok{ []}
\NormalTok{Zlab }\OperatorTok{=}\NormalTok{ []}
\NormalTok{l }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(pl)}
\CommentTok{\# lc = math.comb(l,2)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(l):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, l):}
    \CommentTok{\# for j in range(l):}
        \CommentTok{\# print(pl[i], pl[j])}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ \{pl[i]: X, pl[j]: Y\}}
\NormalTok{        Z.append(wingwt(}\OperatorTok{**}\NormalTok{d))}
\NormalTok{        Zlab.append([pl[i],pl[j]])}
\end{Highlighting}
\end{Shaded}

Now we can generate all 36 combinations, e.g., our first example is
combination \texttt{p\ =\ 19}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{19}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['A', 'Nz']
\end{verbatim}

To help interpret outputs from experiments such as this one---to level
the playing field when comparing outputs from other pairs of
inputs---code below sets up a color palette that can be re-used from one
experiment to the next. We use the arguments \texttt{vmin=180} and
\texttt{vmax\ =360} to implement comparibility

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<matplotlib.colorbar.Colorbar at 0x16d8c3010>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{002_awwe_files/figure-pdf/cell-11-output-2.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  Let's plot the second example, taper ratio \(\lambda\) and fuel weight
  \(W_{fw}\)
\item
  This is combination \texttt{11}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{11}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['Wfw', 'l']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<matplotlib.colorbar.Colorbar at 0x16d8ef0d0>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{002_awwe_files/figure-pdf/cell-13-output-2.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  Using a global colormap indicates that these variables have minor
  effects on the wing weight.
\item
  Important factors can be detected by visual inspection
\item
  Plotting the Big Picture: we can plot all 36 combinations in one
  figure.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.axes\_grid1 }\ImportTok{import}\NormalTok{ ImageGrid}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{20.}\NormalTok{, }\FloatTok{20.}\NormalTok{))}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ ImageGrid(fig, }\DecValTok{111}\NormalTok{,  }\CommentTok{\# similar to subplot(111)}
\NormalTok{                 nrows\_ncols}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{),  }\CommentTok{\# creates 2x2 grid of axes}
\NormalTok{                 axes\_pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,  }\CommentTok{\# pad between axes in inch.}
\NormalTok{                 share\_all}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                 label\_mode}\OperatorTok{=}\StringTok{"0"}\NormalTok{,}
\NormalTok{                 ) }
\NormalTok{i }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ ax, im }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(grid, Z):}
    \CommentTok{\# Iterating over the grid returns the Axes.}
\NormalTok{    ax.set\_xlabel(Zlab[i][}\DecValTok{0}\NormalTok{])}
\NormalTok{    ax.set\_ylabel(Zlab[i][}\DecValTok{1}\NormalTok{])}
    \CommentTok{\# ax.set\_title(Zlab[i][1] + " vs. " + Zlab[i][0])}
\NormalTok{    ax.contourf(X, Y, im, }\DecValTok{30}\NormalTok{, cmap }\OperatorTok{=} \StringTok{"jet"}\NormalTok{,  vmin }\OperatorTok{=} \DecValTok{180}\NormalTok{, vmax }\OperatorTok{=} \DecValTok{360}\NormalTok{)}
\NormalTok{    i }\OperatorTok{=}\NormalTok{ i }\OperatorTok{+} \DecValTok{1}
       
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{002_awwe_files/figure-pdf/cell-14-output-1.pdf}

}

\end{figure}

\hypertarget{awwe-landscape}{%
\section{AWWE Landscape}\label{awwe-landscape}}

\begin{itemize}
\tightlist
\item
  Our Observations

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The load factor \(N_z\), which determines the magnitude of the
    maximum aerodynamic load on the wing, is very active and involved in
    interactions with other variables.
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Classic example: the interaction of \(N_z\) with the aspect ratio
    \(A\) indicates a heavy wing for high aspect ratios and large
    \(g\)-forces
  \item
    This is the reaon why highly manoeuvrable fighter jets cannot have
    very efficient, glider wings)
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Aspect ratio \(A\) and airfoil thickness to chord ratio \(R_{tc}\)
    have nonlinear interactions.
  \item
    Most important variables:
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Ultimate load factor \(N_z\), wing area \(S_w\), and flight design
    gross weight\(W_{dg}\).
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Little impact: dynamic pressure \(q\), taper ratio \(l\), and
    quarter-chord sweep \(L\).
  \end{enumerate}
\item
  Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Aircraft designers know that the overall weight of the aircraft and
    the wing area must be kept to a minimum
  \item
    the latter usually dictated by constraints such as required stall
    speed, landing distance, turn rate, etc.
  \end{itemize}
\end{itemize}

\hypertarget{summary-of-the-first-experiments}{%
\section{Summary of the First
Experiments}\label{summary-of-the-first-experiments}}

\begin{itemize}
\tightlist
\item
  First, we considered two pairs of inputs, out of 36 total pairs
\item
  Then, the ``Big Picture'':

  \begin{itemize}
  \tightlist
  \item
    For each pair we evaluated \texttt{wingwt} 10,000 times
  \end{itemize}
\item
  Doing the same for all pairs would require 360K evaluations:

  \begin{itemize}
  \tightlist
  \item
    not a reasonable number with a real computer simulation that takes
    any non-trivial amount of time to evaluate
  \item
    Only 1s per evaluation: \(>100\) hours
  \end{itemize}
\item
  Many solvers take minutes/hours/days to execute a single run
\item
  And: three-way interactions?
\item
  Consequence: a different strategy is needed
\end{itemize}

\hypertarget{exercise}{%
\section{Exercise}\label{exercise}}

\hypertarget{adding-paint-weight}{%
\subsection{Adding Paint Weight}\label{adding-paint-weight}}

\begin{itemize}
\tightlist
\item
  Paint weight is not considered.
\item
  Add Paint Weight \(W_p\) to formula (the updated formula is shown
  below) and update the functions and plots in the notebook.
\end{itemize}

\[ W = 0.036S_W^{0.758} \times W_{fw}^{0.0035} \times \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times q^{0.006} \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49} + S_w W_p\]

\hypertarget{introduction-to-scipy.optimize}{%
\chapter{\texorpdfstring{Introduction to
\texttt{scipy.optimize}}{Introduction to scipy.optimize}}\label{introduction-to-scipy.optimize}}

\href{https://scipy.org}{SciPy} provides algorithms for optimization,
integration, interpolation, eigenvalue problems, algebraic equations,
differential equations, statistics and many other classes of problems.
SciPy is a collection of mathematical algorithms and convenience
functions built on NumPy. It adds significant power to Python by
providing the user with high-level commands and classes for manipulating
and visualizing data.

\href{https://docs.scipy.org/doc/scipy/reference/optimize.html\#module-scipy.optimize}{SciPy
optimize} provides functions for minimizing (or maximizing) objective
functions, possibly subject to constraints. It includes solvers for
nonlinear problems (with support for both local and global optimization
algorithms), linear programing, constrained and nonlinear least-squares,
root finding, and curve fitting.

In this notebook, we will learn how to use the \texttt{scipy.optimize}
module to solve optimization problems. See:
\url{https://docs.scipy.org/doc/scipy/tutorial/optimize.html}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  This content is based on information from the scipy.optimize package.
\item
  The \texttt{scipy.optimize} package provides several commonly used
  optimization algorithms. A detailed listing is available in
  \texttt{scipy.optimize} (can also be found by
  \texttt{help(scipy.optimize)}).
\end{itemize}

\end{tcolorbox}

Common functions and objects, shared across different SciPy optimize
solvers, are shown in Table~\ref{tbl-shared-functions}.

\hypertarget{tbl-shared-functions}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\caption{\label{tbl-shared-functions}Common functions and objects,
shared across different SciPy optimize solvers}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function or Object
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function or Object
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html\#scipy.optimize.show_options}{show\_options({[}solver,
method, disp{]})} & Show documentation for additional options of
optimization solvers. \\
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html\#scipy.optimize.OptimizeResult}{OptimizeResult}
& Represents the optimization result. \\
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeWarning.html\#scipy.optimize.OptimizeWarning}{OptimizeWarning}
& Warning issued by solvers. \\
\end{longtable}

We will introduce unconstrained minimization of multivariate scalar
functions in this chapter. The
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\#scipy.optimize.minimize}{minimize}
function provides a common interface to unconstrained and constrained
minimization algorithms for multivariate scalar functions in
\texttt{scipy.optimize}. To demonstrate the minimization function,
consider the problem of minimizing the Rosenbrock function of \emph{N}
variables:

\[
f(\mathbf{x}) = \sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2)^2 + (1 - x_i)^2
\]

The minimum value of this function is 0, which is achieved when (x\_i =
1).

Note that the Rosenbrock function and its derivatives are included in
\texttt{scipy.optimize}. The implementations shown in the following
sections provide examples of how to define an objective function as well
as its Jacobian and Hessian functions. Objective functions in
\texttt{scipy.optimize} expect a numpy array as their first parameter,
which is to be optimized and must return a float value. The exact
calling signature must be \texttt{f(x,\ *args)}, where \texttt{x}
represents a numpy array, and \texttt{args} is a tuple of additional
arguments supplied to the objective function.

\hypertarget{derivative-free-optimization-algorithms}{%
\section{Derivative-free Optimization
Algorithms}\label{derivative-free-optimization-algorithms}}

Section~\ref{sec-nelder-mead-simplex-algorithm} and
Section~\ref{sec-powells-method} present two approaches that do not need
gradient information to find the minimum. They use function evaluations
to find the minimum.

\hypertarget{sec-nelder-mead-simplex-algorithm}{%
\subsection{Nelder-Mead Simplex
Algorithm}\label{sec-nelder-mead-simplex-algorithm}}

\texttt{method=\textquotesingle{}Nelder-Mead\textquotesingle{}}: In the
example below, the \texttt{minimize} routine is used with the
\emph{Nelder-Mead} simplex algorithm (selected through the
\texttt{method} parameter):

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\KeywordTok{def}\NormalTok{ rosen(x):}
    \CommentTok{"""The Rosenbrock function"""}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(}\FloatTok{100.0} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{, }\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 339
         Function evaluations: 571
[1. 1. 1. 1. 1.]
\end{verbatim}

The simplex algorithm is probably the simplest way to minimize a
well-behaved function. It requires only function evaluations and is a
good choice for simple minimization problems. However, because it does
not use any gradient evaluations, it may take longer to find the
minimum.

\hypertarget{sec-powells-method}{%
\subsection{Powell's Method}\label{sec-powells-method}}

Another optimization algorithm that needs only function calls to find
the minimum is \emph{Powell}'s method, which can be selected by setting
the \texttt{method} parameter to
\texttt{\textquotesingle{}powell\textquotesingle{}} in the
\texttt{minimize} function.

To demonstrate how to supply additional arguments to an objective
function, let's consider minimizing the Rosenbrock function with an
additional scaling factor \emph{a} and an offset \emph{b}:

\[
f(\mathbf{x}, a, b) = \sum_{i=1}^{N-1} a (x_{i+1} - x_i^2)^2 + (1 - x_i)^2 + b
\]

You can achieve this using the \texttt{minimize} routine with the
example parameters \emph{a=0.5} and \emph{b=1}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_with\_args(x, a, b):}
    \CommentTok{"""The Rosenbrock function with additional arguments"""}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(a }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ b}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen\_with\_args, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               args}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.}\NormalTok{), options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{, }\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 1.000000
         Iterations: 319
         Function evaluations: 525
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

As an alternative to using the \texttt{args} parameter of
\texttt{minimize}, you can wrap the objective function in a new function
that accepts only \texttt{x}. This approach is also useful when it is
necessary to pass additional parameters to the objective function as
keyword arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_with\_args(x, a, }\OperatorTok{*}\NormalTok{, b):  }\CommentTok{\# b is a keyword{-}only argument}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(a }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ b}

\KeywordTok{def}\NormalTok{ wrapped\_rosen\_without\_args(x):}
    \ControlFlowTok{return}\NormalTok{ rosen\_with\_args(x, }\FloatTok{0.5}\NormalTok{, b}\OperatorTok{=}\FloatTok{1.}\NormalTok{)  }\CommentTok{\# pass in \textasciigrave{}a\textasciigrave{} and \textasciigrave{}b\textasciigrave{}}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(wrapped\_rosen\_without\_args, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{,\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

Another alternative is to use \texttt{functools.partial}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ functools }\ImportTok{import}\NormalTok{ partial}

\NormalTok{partial\_rosen }\OperatorTok{=}\NormalTok{ partial(rosen\_with\_args, a}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, b}\OperatorTok{=}\FloatTok{1.}\NormalTok{)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(partial\_rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{,\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

\hypertarget{gradient-based-optimization-algorithms}{%
\section{Gradient-based optimization
algorithms}\label{gradient-based-optimization-algorithms}}

Section~\ref{sec-bfgs} presents an optimization algorithm that uses
gradient information to find the minimum.

\hypertarget{sec-bfgs}{%
\subsection{Broyden-Fletcher-Goldfarb-Shanno Algorithm
(BFGS)}\label{sec-bfgs}}

The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (selected by
setting \texttt{method=\textquotesingle{}BFGS\textquotesingle{}}) is an
optimization algorithm that aims to converge quickly to the solution.
This algorithm uses the gradient of the objective function. If the
gradient is not provided by the user, it is estimated using
first-differences. The BFGS method typically requires fewer function
calls compared to the simplex algorithm, even when the gradient needs to
be estimated.

\hypertarget{using-bfgs-with-the-rosenbrock-function}{%
\subsubsection{Using BFGS with the Rosenbrock
Function}\label{using-bfgs-with-the-rosenbrock-function}}

To demonstrate the BFGS algorithm, let's use the Rosenbrock function
again. The gradient of the Rosenbrock function is a vector described by
the following mathematical expression:

\begin{align}
\frac{\partial f}{\partial x_j} = \sum_{i=1}^{N} 200(x_i - x_{i-1}^2)(\delta_{i,j} - 2x_{i-1}\delta_{i-1,j}) - 2(1 - x_{i-1})\delta_{i-1,j} \\
= 200(x_j - x_{j-1}^2) - 400x_j(x_{j+1} - x_j^2) - 2(1 - x_j)
\end{align}

This expression is valid for interior derivatives, but special cases
are:

\[
\frac{\partial f}{\partial x_0} = -400x_0(x_1 - x_0^2) - 2(1 - x_0)
\]

\[
\frac{\partial f}{\partial x_{N-1}} = 200(x_{N-1} - x_{N-2}^2)
\]

Here's a Python function that computes this gradient:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_der(x):}
\NormalTok{    xm }\OperatorTok{=}\NormalTok{ x[}\DecValTok{1}\NormalTok{:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    xm\_m1 }\OperatorTok{=}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}
\NormalTok{    xm\_p1 }\OperatorTok{=}\NormalTok{ x[}\DecValTok{2}\NormalTok{:]}
\NormalTok{    der }\OperatorTok{=}\NormalTok{ np.zeros\_like(x)}
\NormalTok{    der[}\DecValTok{1}\NormalTok{:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{200}\OperatorTok{*}\NormalTok{(xm}\OperatorTok{{-}}\NormalTok{xm\_m1}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{400}\OperatorTok{*}\NormalTok{(xm\_p1 }\OperatorTok{{-}}\NormalTok{ xm}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{xm }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{xm)}
\NormalTok{    der[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{400}\OperatorTok{*}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\OperatorTok{*}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{])}
\NormalTok{    der[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{200}\OperatorTok{*}\NormalTok{(x[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{x[}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ der}
\end{Highlighting}
\end{Shaded}

You can specify this gradient information in the minimize function using
the jac parameter as illustrated below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}BFGS\textquotesingle{}}\NormalTok{, jac}\OperatorTok{=}\NormalTok{rosen\_der,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 25
         Function evaluations: 30
         Gradient evaluations: 30
[1.00000004 1.0000001  1.00000021 1.00000044 1.00000092]
\end{verbatim}

\hypertarget{gradient--and-hessian-based-optimization-algorithms}{%
\section{Gradient- and Hessian-based optimization
algorithms}\label{gradient--and-hessian-based-optimization-algorithms}}

Section~\ref{sec-newton-cg} presents an optimization algorithm that uses
gradient and Hessian information to find the minimum.
Section~\ref{sec-trust-region-newton} presents an optimization algorithm
that uses gradient and Hessian information to find the minimum.
Section~\ref{sec-trust-region-truncated} presents an optimization
algorithm that uses gradient and Hessian information to find the
minimum.

The methods Newton-CG, trust-ncg and trust-krylov are suitable for
dealing with large-scale problems (problems with thousands of
variables). That is because the conjugate gradient algorithm
approximately solve the trust-region subproblem (or invert the Hessian)
by iterations without the explicit Hessian factorization. Since only the
product of the Hessian with an arbitrary vector is needed, the algorithm
is specially suited for dealing with sparse Hessians, allowing low
storage requirements and significant time savings for those sparse
problems.

\hypertarget{sec-newton-cg}{%
\subsection{Newton-Conjugate-Gradient Algorithm}\label{sec-newton-cg}}

Newton-Conjugate Gradient algorithm is a modified Newton's method and
uses a conjugate gradient algorithm to (approximately) invert the local
Hessian.

\hypertarget{sec-trust-region-newton}{%
\subsection{Trust-Region Newton-Conjugate-Gradient
Algorithm}\label{sec-trust-region-newton}}

\hypertarget{sec-trust-region-truncated}{%
\subsection{Trust-Region Truncated Generalized Lanczos / Conjugate
Gradient Algorithm}\label{sec-trust-region-truncated}}

\hypertarget{global-optimization}{%
\section{Global Optimization}\label{global-optimization}}

Global optimization aims to find the global minimum of a function within
given bounds, in the presence of potentially many local minima.
Typically, global minimizers efficiently search the parameter space,
while using a local minimizer (e.g., minimize) under the hood. SciPy
contains a number of good global optimizers. Here, we'll use those on
the same objective function, namely the (aptly named) eggholder
function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ eggholder(x):}
    \ControlFlowTok{return}\NormalTok{ (}\OperatorTok{{-}}\NormalTok{(x[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{47}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.sin(np.sqrt(}\BuiltInTok{abs}\NormalTok{(x[}\DecValTok{0}\NormalTok{]}\OperatorTok{/}\DecValTok{2} \OperatorTok{+}\NormalTok{ (x[}\DecValTok{1}\NormalTok{]  }\OperatorTok{+} \DecValTok{47}\NormalTok{))))}
            \OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ np.sin(np.sqrt(}\BuiltInTok{abs}\NormalTok{(x[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ (x[}\DecValTok{1}\NormalTok{]  }\OperatorTok{+} \DecValTok{47}\NormalTok{)))))}

\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{512}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{512}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{513}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{513}\NormalTok{)}
\NormalTok{xgrid, ygrid }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{xy }\OperatorTok{=}\NormalTok{ np.stack([xgrid, ygrid])}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.view\_init(}\DecValTok{45}\NormalTok{, }\OperatorTok{{-}}\DecValTok{45}\NormalTok{)}
\NormalTok{ax.plot\_surface(xgrid, ygrid, eggholder(xy), cmap}\OperatorTok{=}\StringTok{\textquotesingle{}terrain\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}eggholder(x, y)\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{003_scipy_optimize_intro_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

We now use the global optimizers to obtain the minimum and the function
value at the minimum. We'll store the results in a dictionary so we can
compare different optimization results later.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ optimize}
\NormalTok{results }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{()}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.shgo(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -935.3379515605789
    funl: [-9.353e+02]
       x: [ 4.395e+02  4.540e+02]
      xl: [[ 4.395e+02  4.540e+02]]
     nit: 1
    nfev: 45
   nlfev: 40
   nljev: 10
   nlhev: 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.dual\_annealing(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: ['Maximum number of iteration reached']
 success: True
  status: 0
     fun: -956.9182316245582
       x: [ 4.824e+02  4.329e+02]
     nit: 1000
    nfev: 4160
    njev: 53
    nhev: 0
\end{verbatim}

All optimizers return an \texttt{OptimizeResult}, which in addition to
the solution contains information on the number of function evaluations,
whether the optimization was successful, and more. For brevity, we won't
show the full output of the other optimizers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.differential\_evolution(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -959.6406627208189
       x: [ 5.120e+02  4.042e+02]
     nit: 46
    nfev: 1425
     jac: [-3.386e+00  0.000e+00]
\end{verbatim}

\texttt{shgo} has a second method, which returns all local minima rather
than only what it thinks is the global minimum:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.shgo(eggholder, bounds, n}\OperatorTok{=}\DecValTok{200}\NormalTok{, iters}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                                      sampling\_method}\OperatorTok{=}\StringTok{\textquotesingle{}sobol\textquotesingle{}}\NormalTok{)}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -959.640662720831
    funl: [-9.596e+02 -9.353e+02 ... -6.591e+01 -6.387e+01]
       x: [ 5.120e+02  4.042e+02]
      xl: [[ 5.120e+02  4.042e+02]
           [ 4.395e+02  4.540e+02]
           ...
           [ 3.165e+01 -8.523e+01]
           [ 5.865e+01 -5.441e+01]]
     nit: 5
    nfev: 3529
   nlfev: 2327
   nljev: 634
   nlhev: 0
\end{verbatim}

We'll now plot all found minima on a heatmap of the function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(eggholder(xy), interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}bilinear\textquotesingle{}}\NormalTok{, origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{               cmap}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}

\KeywordTok{def}\NormalTok{ plot\_point(res, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
\NormalTok{    ax.plot(}\DecValTok{512}\OperatorTok{+}\NormalTok{res.x[}\DecValTok{0}\NormalTok{], }\DecValTok{512}\OperatorTok{+}\NormalTok{res.x[}\DecValTok{1}\NormalTok{], marker}\OperatorTok{=}\NormalTok{marker, color}\OperatorTok{=}\NormalTok{color, ms}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{)  }\CommentTok{\# differential\_evolution {-} cyan}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{)  }\CommentTok{\# dual\_annealing.        {-} white}

\CommentTok{\# SHGO produces multiple minima, plot them all (with a smaller marker size)}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{)}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl.shape[}\DecValTok{0}\NormalTok{]):}
\NormalTok{    ax.plot(}\DecValTok{512} \OperatorTok{+}\NormalTok{ results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl[i, }\DecValTok{0}\NormalTok{],}
            \DecValTok{512} \OperatorTok{+}\NormalTok{ results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl[i, }\DecValTok{1}\NormalTok{],}
            \StringTok{\textquotesingle{}ro\textquotesingle{}}\NormalTok{, ms}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax.set\_xlim([}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{514}\OperatorTok{*}\DecValTok{2}\NormalTok{])}
\NormalTok{ax.set\_ylim([}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{514}\OperatorTok{*}\DecValTok{2}\NormalTok{])}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{003_scipy_optimize_intro_files/figure-pdf/cell-14-output-1.pdf}

}

\end{figure}

\hypertarget{dual-annealing-optimization}{%
\subsection{Dual Annealing
Optimization}\label{dual-annealing-optimization}}

This function implements the Dual Annealing optimization.

\hypertarget{differential-evolution}{%
\subsection{Differential Evolution}\label{differential-evolution}}

Differential Evolution is an algorithm used for finding the global
minimum of multivariate functions. It is stochastic in nature (does not
use gradient methods), and can search large areas of candidate space,
but often requires larger numbers of function evaluations than
conventional gradient based techniques.

\hypertarget{direct}{%
\subsection{DIRECT}\label{direct}}

DIviding RECTangles (DIRECT) is a deterministic global optimization
algorithm capable of minimizing a black box function with its variables
subject to lower and upper bound constraints by sampling potential
solutions in the search space

\hypertarget{shgo}{%
\subsection{SHGO}\label{shgo}}

SHGO stands for ``simplicial homology global optimization''. It is
considered appropriate for solving general purpose NLP and blackbox
optimization problems to global optimality (low-dimensional problems).

\hypertarget{basin-hopping}{%
\subsection{Basin-hopping}\label{basin-hopping}}

Basin-hopping is a two-phase method that combines a global stepping
algorithm with local minimization at each step. Designed to mimic the
natural process of energy minimization of clusters of atoms, it works
well for similar problems with ``funnel-like, but rugged'' energy
landscapes

\hypertarget{sec-scipy-optimizers}{%
\chapter{\texorpdfstring{Sequential Parameter Optimization: Using
\texttt{scipy}
Optimizers}{Sequential Parameter Optimization: Using scipy Optimizers}}\label{sec-scipy-optimizers}}

As a default optimizer, \texttt{spotPython} uses
\texttt{differential\_evolution} from the \texttt{scipy.optimize}
package. Alternatively, any other optimizer from the
\texttt{scipy.optimize} package can be used. This chapter describes how
different optimizers from the \texttt{scipy\ optimize} package can be
used on the surrogate. The optimization algorithms are available from
\url{https://docs.scipy.org/doc/scipy/reference/optimize.html}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ dual\_annealing}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ basinhopping}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-objective-function-branin}{%
\section{The Objective Function
Branin}\label{the-objective-function-branin}}

\begin{itemize}
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula.
\item
  Here we will use the Branin function. The 2-dim Branin function is

  \[y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * cos(x1) + s,\]
  where values of a, b, c, r, s and t are:
  \(a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10\) and
  \(t = 1 / (8*pi)\).
\item
  It has three global minima:

  \(f(x) = 0.397887\) at \((-\pi, 12.275)\), \((\pi, 2.275)\), and
  \((9.42478, 2.475)\).
\item
  Input Domain: This function is usually evaluated on the square x1 in
  {[}-5, 10{]} x x2 in {[}0, 15{]}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-optimizer}{%
\section{The Optimizer}\label{the-optimizer}}

\begin{itemize}
\item
  Differential Evalution from the \texttt{scikit.optimize} package, see
  \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html\#scipy.optimize.differential_evolution}
  is the default optimizer for the search on the surrogate.
\item
  Other optimiers that are available in \texttt{spotPython}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{dual\_annealing}
  \item
    \texttt{direct}
  \item
    \texttt{shgo}
  \item
    \texttt{basinhopping}, see
    \url{https://docs.scipy.org/doc/scipy/reference/optimize.html\#global-optimization}.
  \end{itemize}
\item
  These can be selected as follows:

  \texttt{surrogate\_control\ =\ "model\_optimizer":\ differential\_evolution}
\item
  We will use \texttt{differential\_evolution}.
\item
  The optimizer can use \texttt{1000} evaluations. This value will be
  passed to the \texttt{differential\_evolution} method, which has the
  argument \texttt{maxiter} (int). It defines the maximum number of
  generations over which the entire differential evolution population is
  evolved, see
  \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html\#scipy.optimize.differential_evolution}
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"05\_DE\_"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
05_DE__maans14_2023-11-08_10-06-40
\end{verbatim}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{125}\NormalTok{,}
\NormalTok{                   noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{10}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(lower),}
                                      \StringTok{"model\_optimizer"}\NormalTok{: differential\_evolution,}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{1000}\NormalTok{,}
\NormalTok{                                      \},}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{spot\_de.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 5.213735995388665 [######----] 55.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.213735995388665 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.5179173635657266 [######----] 65.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.016872525620073 [#######---] 70.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.4160579721446034 [########--] 75.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.4096599475220657 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.4096599475220657 [########--] 85.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.3999223417849329 [#########-] 90.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.39969164980122684 [##########] 95.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.39969164980122684 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x3255f9b10>
\end{verbatim}

\hypertarget{tensorboard-1}{%
\subsection{TensorBoard}\label{tensorboard-1}}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/05_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}

\hypertarget{print-the-results}{%
\section{Print the Results}\label{print-the-results}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.39969164980122684
x0: -3.158224446089584
x1: 12.293182279400076
\end{verbatim}

\begin{verbatim}
[['x0', -3.158224446089584], ['x1', 12.293182279400076]]
\end{verbatim}

\hypertarget{show-the-progress}{%
\section{Show the Progress}\label{show-the-progress}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\hypertarget{dual_annealing}{%
\subsection{\texorpdfstring{\texttt{dual\_annealing}}{dual\_annealing}}\label{dual_annealing}}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\hypertarget{direct-1}{%
\subsection{\texorpdfstring{\texttt{direct}}{direct}}\label{direct-1}}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\hypertarget{shgo-1}{%
\subsection{\texorpdfstring{\texttt{shgo}}{shgo}}\label{shgo-1}}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\hypertarget{basinhopping}{%
\subsection{\texorpdfstring{\texttt{basinhopping}}{basinhopping}}\label{basinhopping}}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\hypertarget{performance-comparison}{%
\subsection{Performance Comparison}\label{performance-comparison}}

Compare the performance and run time of the 5 different optimizers:

\begin{verbatim}
* `differential_evolution`
* `dual_annealing`
*  `direct`
* `shgo`
* `basinhopping`.
\end{verbatim}

The Branin function has three global minima:

\begin{itemize}
\tightlist
\item
  \(f(x) = 0.397887\) at

  \begin{itemize}
  \tightlist
  \item
    \((-\pi, 12.275)\),
  \item
    \((\pi, 2.275)\), and
  \item
    \((9.42478, 2.475)\).\\
  \end{itemize}
\item
  Which optima are found by the optimizers? Does the \texttt{seed}
  change this behavior?
\end{itemize}

\part{Numerical Methods}

\hypertarget{introduction-numerical-methods}{%
\chapter{Introduction: Numerical
Methods}\label{introduction-numerical-methods}}

This part deals with numerical implementations of optimization methods.
The goal is to understand the implementation of optimization methods and
to solve real-world problems numerically and efficiently. We will focus
on the implementation of surrogate models, because they are the most
efficient way to solve real-world problems.

Starting point is the well-established response surface methodology. It
will be extended to the design and analysis of computer experiments
(DACE). The DACE methodology is a modern extension of the response
surface methodology. It is based on the use of surrogate models, which
are used to replace the real-world problem with a simpler problem. The
simpler problem is then solved numerically. The solution of the simpler
problem is then used to solve the real-world problem.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Numerical methods: Goals}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Understand implementation of optimization methods
\item
  Solve real-world problems numerically and efficiently
\end{itemize}

\end{tcolorbox}

\hypertarget{response-surface-methods-what-is-rsm}{%
\section{Response Surface Methods: What is
RSM?}\label{response-surface-methods-what-is-rsm}}

Response Surface Methods (RSM) refer to a collection of statistical and
mathematical tools that are valuable for developing, improving, and
optimizing processes. The overarching theme of RSM involves studying how
input variables that control a product or process can potentially
influence a response that measures performance or quality
characteristics.

The advantages of RSM include a rich literature, well-established
methods often used in manufacturing, the importance of careful
experimental design combined with a well-understood model, and the
potential to add significant value to scientific inquiry, process
refinement, optimization, and more. However, there are also drawbacks to
RSM, such as the use of simple and crude surrogates, the hands-on nature
of the methods, and the limitation of local methods.

RSM is related to various fields, including Design of Experiments (DoE),
quality management, reliability, and productivity. Its applications are
widespread in industry and manufacturing, focusing on designing,
developing, and formulating new products and improving existing ones, as
well as from laboratory research. RSM is commonly applied in domains
such as materials science, manufacturing, applied chemistry, climate
science, and many others.

An example of RSM involves studying the relationship between a response
variable, such as yield (\(y\)) in a chemical process, and two process
variables: reaction time (\(\xi_1\)) and reaction temperature
(\(\xi_2\)). The provided code illustrates this scenario, following a
variation of the so-called ``banana function.''

In the context of visualization, RSM offers the choice between 3D plots
and contour plots. In a 3D plot, the independent variables \(\xi_1\) and
\(\xi_2\) are represented, with \(y\) as the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\KeywordTok{def}\NormalTok{ fun\_rosen(x1, x2):}
\NormalTok{    b }\OperatorTok{=} \DecValTok{10}
    \ControlFlowTok{return}\NormalTok{ (x1}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ b}\OperatorTok{*}\NormalTok{(x2}\OperatorTok{{-}}\NormalTok{x1}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zs }\OperatorTok{=}\NormalTok{ np.array(fun\_rosen(np.ravel(X), np.ravel(Y)))}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ zs.reshape(X.shape)}

\NormalTok{ax.plot\_surface(X, Y, Z)}

\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}X1\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}X2\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-2-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  contour plot example:

  \begin{itemize}
  \tightlist
  \item
    \(x_1\) and \(x_2\) are the independent variables
  \item
    \(y\) is the dependent variable
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_rosen(X1, X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y , }\DecValTok{50}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Rosenbrock\textquotesingle{}s Banana Function"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, "Rosenbrock's Banana Function")
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-3-output-2.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  Visual inspection: yield is optimized near \((\xi_1. \xi_2)\)
\end{itemize}

\hypertarget{visualization-problems-in-practice}{%
\subsection{Visualization: Problems in
Practice}\label{visualization-problems-in-practice}}

\begin{itemize}
\tightlist
\item
  True response surface is unknown in practice
\item
  When yield evaluation is not as simple as a toy banana function, but a
  process requiring care to monitor, reconfigure and run, it's far too
  expensive to observe over a dense grid
\item
  And, measuring yield may be a noisy/inexact process
\item
  That's where stats (RSM) comes in
\end{itemize}

\hypertarget{rsm-strategies}{%
\subsection{RSM: Strategies}\label{rsm-strategies}}

\begin{itemize}
\item
  RSMs consist of experimental strategies for
\item
  \textbf{exploring} the space of the process (i.e., independent/input)
  variables (above \(\xi_1\) and \(\xi2)\)
\item
  empirical statistical \textbf{modeling} targeted toward development of
  an appropriate approximating relationship between the response (yield)
  and process variables local to a study region of interest
\item
  \textbf{optimization} methods for sequential refinement in search of
  the levels or values of process variables that produce desirable
  responses (e.g., that maximize yield or explain variation)
\item
  RSM used for fitting an Empirical Model
\item
  True response surface driven by an unknown physical mechanism
\item
  Observations corrupted by noise
\item
  Helpful: fit an empirical model to output collected under different
  process configurations
\item
  Consider response \(Y\) that depends on controllable input variables
  \(\xi_1, \xi_2, \ldots, \xi_m\)
\item
  RSM: Equations of the Empirical Model

  \begin{itemize}
  \tightlist
  \item
    \(Y=f(\xi_1, \xi_2, \ldots, \xi_m) + \epsilon\)
  \item
    \(\mathbb{E}\{Y\} = \eta = f(\xi1_1, \xi_2, \ldots, \xi_m)\)
  \item
    \(\epsilon\) is treated as zero mean idiosyncratic noise possibly
    representing

    \begin{itemize}
    \tightlist
    \item
      inherent variation, or
    \item
      the effect of other systems or
    \item
      variables not under our purview at this time
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{rsm-noise-in-the-empirical-model}{%
\subsection{RSM: Noise in the Empirical
Model}\label{rsm-noise-in-the-empirical-model}}

\begin{itemize}
\tightlist
\item
  Typical simplifying assumption: \(\epsilon \sim N(0,\sigma^2)\)
\item
  We seek estimates for \(f\) and \(\sigma^2\) from noisy observations
  \(Y\) at inputs \(\xi\)
\end{itemize}

\hypertarget{rsm-natural-and-coded-variables}{%
\subsection{RSM: Natural and Coded
Variables}\label{rsm-natural-and-coded-variables}}

\begin{itemize}
\tightlist
\item
  Inputs \(\xi_1, \xi_2, \ldots, \xi_m\) called \textbf{natural
  variables}:

  \begin{itemize}
  \tightlist
  \item
    expressed in natural units of measurement, e.g., degrees Celsius,
    pounds per square inch (psi), etc.
  \end{itemize}
\item
  Transformed to \textbf{coded variables} \(x_1, x_2, \ldots, x_m\):

  \begin{itemize}
  \tightlist
  \item
    to mitigate hassles and confusion that can arise when working with a
    multitude of scales of measurement
  \end{itemize}
\item
  Typical \textbf{Transformations} offering dimensionless inputs
  \(x_1, x_2, \ldots, x_m\)

  \begin{itemize}
  \tightlist
  \item
    in the unit cube, or
  \item
    scaled to have a mean of zero and standard deviation of one, are
    common choices.
  \end{itemize}
\item
  Empirical model becomes \(\eta = f(x_1, x_2, \ldots, x_m)\)
\end{itemize}

\hypertarget{rsm-low-order-polynomials}{%
\subsection{RSM Low-order Polynomials}\label{rsm-low-order-polynomials}}

\begin{itemize}
\tightlist
\item
  Low-order polynomial make the following simplifying Assumptions

  \begin{itemize}
  \tightlist
  \item
    Learning about \(f\) is lots easier if we make some simplifying
    approximations
  \item
    Appealing to \textbf{Taylor's theorem}, a low-order polynomial in a
    small, localized region of the input (\(x\)) space is one way
    forward
  \item
    Classical RSM:

    \begin{itemize}
    \tightlist
    \item
      disciplined application of \textbf{local analysis} and
    \item
      \textbf{sequential refinement} of locality through conservative
      extrapolation
    \end{itemize}
  \item
    Inherently a \textbf{hands-on process}
  \end{itemize}
\end{itemize}

\hypertarget{first-order-models-main-effects-model}{%
\section{First-Order Models (Main Effects
Model)}\label{first-order-models-main-effects-model}}

\begin{itemize}
\tightlist
\item
  \textbf{First-order model} (sometimes called main effects model)
  useful in parts of the input space where it's believed that there's
  little curvature in \(f\):
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \]
\item
  For example: \[\eta = 50 + 8 x_1 + 3x_2\]
\item
  In practice, such a surface would be obtained by fitting a model to
  the outcome of a designed experiment
\item
  First-Order Model in python Evaluated on a Grid
\item
  Evaluate model on a grid in a double-unit square centered at the
  origin
\item
  Coded units are chosen arbitrarily, although one can imagine deploying
  this approximating function nearby \(x^{(0)} = (0,0)\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_1(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_1(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model: $50 + 8x\_1 + 3x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model: $50 + 8x_1 + 3x_2$')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-5-output-2.pdf}

}

\end{figure}

\hypertarget{first-order-model-properties}{%
\subsection{First-Order Model
Properties}\label{first-order-model-properties}}

\begin{itemize}
\tightlist
\item
  First-order model in 2d traces out a \textbf{plane} in
  \(y \times (x_1, x_2)\) space
\item
  Only be appropriate for the most trivial of response surfaces, even
  when applied in a highly localized part of the input space
\item
  Adding \textbf{curvature} is key to most applications:

  \begin{itemize}
  \tightlist
  \item
    First-order model with \textbf{interactions} induces limited degree
    of curvature via different rates of change of \(y\) as \(x_1\) is
    varied for fixed \(x_2\), and vice versa:
    \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_{12} \]
  \end{itemize}
\item
  For example \(\eta = 50+8x_1+3x_2-4x_1x_2\)
\end{itemize}

\hypertarget{first-order-model-with-interactions-in-python}{%
\subsection{First-order Model with Interactions in
python}\label{first-order-model-with-interactions-in-python}}

\begin{itemize}
\tightlist
\item
  Code below facilitates evaluations for pairs \((x_1, x_2)\)
\item
  Responses may be observed over a mesh in the same double-unit square
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_11(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_11(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model with Interactions\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model with Interactions')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-7-output-2.pdf}

}

\end{figure}

\hypertarget{observations-first-order-model-with-interactions}{%
\subsection{Observations: First-Order Model with
Interactions}\label{observations-first-order-model-with-interactions}}

\begin{itemize}
\tightlist
\item
  Mean response \(\eta\) is increasing marginally in both \(x_1\) and
  \(x_2\), or conditional on a fixed value of the other until \(x_1\) is
  0.75
\item
  Rate of increase slows as both coordinates grow simultaneously since
  the coefficient in front of the interaction term \(x_1 x_2\) is
  negative
\item
  Compared to the first-order model (without interactions): surface is
  far more useful locally
\item
  Least squares regressions often flag up significant interactions when
  fit to data collected on a design far from local optima
\end{itemize}

\hypertarget{second-order-models}{%
\section{Second-Order Models}\label{second-order-models}}

\begin{itemize}
\item
  Second-order model may be appropriate near local optima where \(f\)
  would have substantial curvature:
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \beta_{11}x_1^2 + \beta_{22}x^2 + \beta_{12} x_1 x_2\]
\item
  For example \[\eta = 50 + 8 x_1 + 3x_2 - 7x_1^2 - 3 x_2^2 - 4x_1x_2\]
\item
  Implementation of the Second-Order Model as \texttt{fun\_2()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_2(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{7} \OperatorTok{*}\NormalTok{ x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_2(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Second Order Model with Interactions. Maximum near about $(0.6,0.2)$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-9-output-2.pdf}

}

\end{figure}

\hypertarget{second-order-models-properties}{%
\subsection{Second-Order Models:
Properties}\label{second-order-models-properties}}

\begin{itemize}
\tightlist
\item
  Not all second-order models would have a single stationary point (in
  RSM jargon called ``a simple maximum'')
\item
  In ``yield maximizing'' setting we're presuming response surface is
  \textbf{concave} down from a global viewpoint

  \begin{itemize}
  \tightlist
  \item
    even though local dynamics may be more nuanced
  \end{itemize}
\item
  Exact criteria depend upon the eigenvalues of a certain matrix built
  from those coefficients
\item
  Box and Draper (2007) provide a diagram categorizing all of the kinds
  of second-order surfaces in RSM analysis, where finding local maxima
  is the goal
\end{itemize}

\hypertarget{example-stationary-ridge}{%
\subsection{Example: Stationary Ridge}\label{example-stationary-ridge}}

\begin{itemize}
\tightlist
\item
  Example set of coefficients describing what's called a
  \textbf{stationary ridge} is provided by the code below
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Example of a stationary ridge\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Example of a stationary ridge')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-11-output-2.pdf}

}

\end{figure}

\hypertarget{observations-second-order-model-ridge}{%
\subsection{Observations: Second-Order Model
(Ridge)}\label{observations-second-order-model-ridge}}

\begin{itemize}
\tightlist
\item
  \textbf{Ridge}: a whole line of stationary points corresponding to
  maxima
\item
  Situation means that the practitioner has some flexibility when it
  comes to optimizing:

  \begin{itemize}
  \tightlist
  \item
    can choose the precise setting of \((x_1, x_2)\) either arbitrarily
    or (more commonly) by consulting some tertiary criteria
  \end{itemize}
\end{itemize}

\hypertarget{example-rising-ridge}{%
\subsection{Example: Rising Ridge}\label{example-rising-ridge}}

\begin{itemize}
\tightlist
\item
  An example of a rising ridge is implemented by the code below.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge\_rise(x1, x2):}
     \ControlFlowTok{return} \DecValTok{80} \OperatorTok{{-}} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{12}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge\_rise(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Rising ridge: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 3x\_1\^{}2 {-} 12x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Rising ridge: $\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-13-output-2.pdf}

}

\end{figure}

\hypertarget{summary-rising-ridge}{%
\subsection{Summary: Rising Ridge}\label{summary-rising-ridge}}

\begin{itemize}
\tightlist
\item
  The stationary point is remote to the study region
\item
  Ccontinuum of (local) stationary points along any line going through
  the 2d space, excepting one that lies directly on the ridge
\item
  Although estimated response will increase while moving along the axis
  of symmetry toward its stationary point, this situation indicates

  \begin{itemize}
  \tightlist
  \item
    either a poor fit by the approximating second-order function, or
  \item
    that the study region is not yet precisely in the vicinity of a
    local optima---often both.
  \end{itemize}
\end{itemize}

\hypertarget{falling-ridge}{%
\subsection{Falling Ridge}\label{falling-ridge}}

\begin{itemize}
\tightlist
\item
  Inversion of a rising ridge is a falling ridge
\item
  Similarly indicating one is far from local optima, except that the
  response decreases as you move toward the stationary point
\item
  Finding a falling ridge system can be a back-to-the-drawing-board
  affair.
\end{itemize}

\hypertarget{saddle-point}{%
\subsection{Saddle Point}\label{saddle-point}}

\begin{itemize}
\tightlist
\item
  Finally, we can get what's called a saddle or minimax system.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_saddle(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_saddle(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Saddle Point: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 2x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Saddle Point: $\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{005_num_rsm_files/figure-pdf/cell-15-output-2.pdf}

}

\end{figure}

\hypertarget{interpretation-saddle-points}{%
\subsection{Interpretation: Saddle
Points}\label{interpretation-saddle-points}}

\begin{itemize}
\tightlist
\item
  Likely further data collection, and/or outside expertise, is needed
  before determining a course of action in this situation
\end{itemize}

\hypertarget{summary-ridge-analysis}{%
\subsection{Summary: Ridge Analysis}\label{summary-ridge-analysis}}

\begin{itemize}
\tightlist
\item
  Finding a simple maximum, or stationary ridge, represents ideals in
  the spectrum of second-order approximating functions
\item
  But getting there can be a bit of a slog
\item
  Using models fitted from data means uncertainty due to noise, and
  therefore uncertainty in the type of fitted second-order model
\item
  A ridge analysis attempts to offer a principled approach to navigating
  uncertainties when one is seeking local maxima
\item
  The two-dimensional setting exemplified above is convenient for
  visualization, but rare in practice
\item
  Complications compound when studying the effect of more than two
  process variables
\end{itemize}

\hypertarget{general-rsm-models}{%
\section{General RSM Models}\label{general-rsm-models}}

\begin{itemize}
\tightlist
\item
  General \textbf{first-order model} on \(m\) process variables
  \(x_1, x_2, \cdots, x_m\) is
  \[\eta = \beta_0 + \beta_1x_1 + \cdots + \beta_m x_m\]
\item
  General \textbf{second-order model} on \(m\) process variables \[
  \eta= \beta_0 + \sum_{j=1}^m + \sum_{j=1}^m x_j^2 + \sum_{j=2}^m \sum_{k=1}^j \beta_{kj}x_k x_j.
  \]
\end{itemize}

\hypertarget{ordinary-least-squares}{%
\subsection{Ordinary Least Squares}\label{ordinary-least-squares}}

\begin{itemize}
\tightlist
\item
  Inference from data is carried out by \textbf{ordinary least squares}
  (OLS)
\item
  For an excellent review including R examples, see Sheather (2009)
\item
  OLS and maximum likelihood estimators (MLEs) are in the typical
  Gaussian linear modeling setup basically equivalent
\end{itemize}

\hypertarget{designs}{%
\section{Designs}\label{designs}}

\begin{itemize}
\tightlist
\item
  Important: Organize the data collection phase of a response surface
  study carefully
\item
  \textbf{Design}: choice of \(x\)'s where we plan to observe \(y\)'s,
  for the purpose of approximating \(f\)
\item
  Analyses and designs need to be carefully matched
\item
  When using a first-order model, some designs are preferred over others
\item
  When using a second-order model to capture curvature, a different sort
  of design is appropriate
\item
  Design choices often contain features enabling modeling assumptions to
  be challenged

  \begin{itemize}
  \tightlist
  \item
    e.g., to check if initial impressions are supported by the data
    ultimately collected
  \end{itemize}
\end{itemize}

\hypertarget{different-designs}{%
\subsection{Different Designs}\label{different-designs}}

\begin{itemize}
\tightlist
\item
  \textbf{Screening desings}: determine which variables matter so that
  subsequent experiments may be smaller and/or more focused
\item
  Then there are designs tailored to the form of model (first- or
  second-order, say) in the screened variables
\item
  And then there are more designs still
\end{itemize}

\hypertarget{rsm-experimentation}{%
\section{RSM Experimentation}\label{rsm-experimentation}}

\hypertarget{first-step}{%
\subsection{First Step}\label{first-step}}

\begin{itemize}
\tightlist
\item
  RSM-based experimentation begins with a \textbf{first-order model},
  possibly with interactions
\item
  Presumption: current process operating \textbf{far from optimal}
  conditions
\item
  Collect data and apply \textbf{method of steepest ascent} (gradient)
  on fitted surfaces to move to the optimum
\end{itemize}

\hypertarget{second-step}{%
\subsection{Second Step}\label{second-step}}

\begin{itemize}
\tightlist
\item
  Eventually, if all goes well after several such carefully iterated
  refinements, \textbf{second-order models} are used on appropriate
  designs in order to zero-in on ideal operating conditions
\item
  Careful analysis of the fitted surface:

  \begin{itemize}
  \tightlist
  \item
    Ridge analysis with further refinement using gradients of, and
  \item
    standard errors associated with, the fitted surfaces, and so on
  \end{itemize}
\end{itemize}

\hypertarget{third-step}{%
\subsection{Third Step}\label{third-step}}

\begin{itemize}
\tightlist
\item
  Once the practitioner is satisfied with the full arc of

  \begin{itemize}
  \tightlist
  \item
    design(s),
  \item
    fit(s), and
  \item
    decision(s):
  \end{itemize}
\item
  A small experiment called \textbf{confirmation test} may be performed
  to check if the predicted optimal settings are realizable in practice
\end{itemize}

\hypertarget{rsm-review-and-general-considerations}{%
\section{RSM: Review and General
Considerations}\label{rsm-review-and-general-considerations}}

\begin{itemize}
\item
  First Glimpse, RSM seems sensible, and pretty straightforward as
  quantitative statistics-based analysis goes
\item
  But: RSM can get complicated, especially when input dimensions are not
  very low
\item
  Design considerations are particularly nuanced, since the goal is to
  obtain reliable estimates of main effects, interaction, and curvature
  while minimizing sampling effort/expense
\item
  RSM Downside: Inefficiency

  \begin{itemize}
  \tightlist
  \item
    Despite intuitive appeal, several RSM downsides become apparent upon
    reflection
  \item
    Problems in practice
  \item
    Stepwise nature of sequential decision making is inefficient:

    \begin{itemize}
    \tightlist
    \item
      Not obvious how to re-use or update analysis from earlier phases,
      or couple with data from other sources/related experiments
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Locality

  \begin{itemize}
  \tightlist
  \item
    In addition to being local in experiment-time (stepwise approach),
    it's local in experiment-space
  \item
    Balance between

    \begin{itemize}
    \tightlist
    \item
      exploration (maybe we're barking up the wrong tree) and
    \item
      exploitation (let's make things a little better) is modest at best
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Interjection of expert knowledge is limited to hunches about
    relevant variables (i.e., the screening phase), where to initialize
    search, how to design the experiments
  \item
    Yet at the same time classical RSMs rely heavily on constant
    examination throughout stages of modeling and design and on the
    instincts of seasoned practitioners
  \end{itemize}
\item
  RSM Downside: Replicability

  \begin{itemize}
  \tightlist
  \item
    Parallel analyses, conducted according to the same best intentions,
    rarely lead to the same designs, model fits and so on
  \item
    Sometimes that means they lead to different conclusions, which can
    be cause for concern
  \end{itemize}
\end{itemize}

\hypertarget{historical-considerations-about-rsm}{%
\subsection{Historical Considerations about
RSM}\label{historical-considerations-about-rsm}}

\begin{itemize}
\tightlist
\item
  In spite of those criticisms, however, there was historically little
  impetus to revise the status quo
\item
  Classical RSM was comfortable in its skin, consistently led to
  improvements or compelling evidence that none can reasonably be
  expected
\item
  But then in the late 20th century came an explosive expansion in
  computational capability, and with it a means of addressing many of
  those downsides
\end{itemize}

\hypertarget{status-quo}{%
\subsection{Status Quo}\label{status-quo}}

\begin{itemize}
\tightlist
\item
  Nowadays, field experiments and statistical models, designs and
  optimizations are coupled with with mathematical models
\item
  Simple equations are not regarded as sufficient to describe real-world
  systems anymore
\item
  Physicists figured that out fifty years ago; industrial engineers
  followed, biologists, social scientists, climate scientists and
  weather forecasters, etc.
\item
  Systems of equations are required, solved over meshes (e.g., finite
  elements), or stochastically interacting agents
\item
  Goals for those simulation experiments are as diverse as their
  underlying dynamics
\item
  Optimization of systems is common, e.g., to identify worst-case
  scenarios
\end{itemize}

\hypertarget{the-role-of-statistics}{%
\subsection{The Role of Statistics}\label{the-role-of-statistics}}

\begin{itemize}
\tightlist
\item
  Solving systems of equations, or interacting agents, requires
  computing
\item
  Statistics involved at various stages:

  \begin{itemize}
  \tightlist
  \item
    choosing the mathematical model
  \item
    solving by stochastic simulation (Monte Carlo)
  \item
    designing the computer experiment
  \item
    smoothing over idiosyncrasies or noise
  \item
    finding optimal conditions, or
  \item
    calibrating mathematical/computer models to data from field
    experiments
  \end{itemize}
\end{itemize}

\hypertarget{new-rsm-is-needed-dace}{%
\subsection{New RSM is needed: DACE}\label{new-rsm-is-needed-dace}}

\begin{itemize}
\tightlist
\item
  Classical RSMs are not well-suited to any of those tasks, because

  \begin{itemize}
  \tightlist
  \item
    they lack the fidelity required to model these data
  \item
    their intended application is too local
  \item
    they're also too hands-on.
  \end{itemize}
\item
  Once computers are involved, a natural inclination is to automate---to
  remove humans from the loop and set the computer running on the
  analysis in order to maximize computing throughput, or minimize idle
  time
\item
  \textbf{Design and Analysis of Computer Experiments} as a modern
  extension of RSM
\item
  Experimentation is changing due to advances in machine learning
\item
  \textbf{Gaussian process} (GP) regression is the canonical surrogate
  model
\item
  Origins in geostatistics (gold mining)
\item
  Wide applicability in contexts where prediction is king
\item
  Machine learners exposed GPs as powerful predictors for all sorts of
  tasks:
\item
  from regression to classification,
\item
  active learning/sequential design,
\item
  reinforcement learning and optimization,
\item
  latent variable modeling, and so on
\end{itemize}

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate 3d Plots for the Contour Plots in this notebook.
\item
  Write a \texttt{plot\_3d} function, that takes the objective function
  \texttt{fun} as an argument.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface: \texttt{plot\_3d(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Write a \texttt{plot\_contour} function, that takes the objective
  function \texttt{fun} as an argument:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface:
  \texttt{plot\_contour(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Consider further arguments that might be useful for both function,
  e.g., ranges, size, etc.
\end{enumerate}

\hypertarget{kriging-gaussian-process-regression}{%
\chapter{Kriging (Gaussian Process
Regression)}\label{kriging-gaussian-process-regression}}

\hypertarget{dace-and-rsm}{%
\section{DACE and RSM}\label{dace-and-rsm}}

Mathematical models implemented in computer codes are used to circumvent
the need for expensive field data collection. These models are
particularly useful when dealing with highly nonlinear response
surfaces, high signal-to-noise ratios (which often involve deterministic
evaluations), and a global scope. As a result, a new approach is
required in comparison to Response Surface Methodology (RSM).

With the improvement in computing power and simulation fidelity,
researchers gain higher confidence and a better understanding of the
dynamics in physical, biological, and social systems. However, the
expansion of configuration spaces and increasing input dimensions
necessitates more extensive designs. High-performance computing (HPC)
allows for thousands of runs, whereas previously only tens were
possible. This shift towards larger models and training data presents
new computational challenges.

Research questions for DACE (Design and Analysis of Computer
Experiments) include how to design computer experiments that make
efficient use of computation and how to meta-model computer codes to
save on simulation effort. The choice of surrogate model for computer
codes significantly impacts the optimal experiment design, and the
preferred model-design pairs can vary depending on the specific goal.

The combination of computer simulation, design, and modeling with field
data from similar real-world experiments introduces a new category of
computer model tuning problems. The ultimate goal is to automate these
processes to the greatest extent possible, allowing for the deployment
of HPC with minimal human intervention.

One of the remaining differences between RSM and DACE lies in how they
handle noise. DACE employs replication, a technique that would not be
used in a deterministic setting, to separate signal from noise.
Traditional RSM is best suited for situations where a substantial
proportion of the variability in the data is due to noise, and where the
acquisition of data values can be severely limited. Consequently, RSM is
better suited for a different class of problems, aligning with its
intended purposes.

\hypertarget{dace-literature}{%
\subsection{DACE Literature}\label{dace-literature}}

\begin{itemize}
\tightlist
\item
  Two very good texts on computer experiments and surrogate modeling:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The Design and Analysis of Computer Experiments, by Santner,
    Williams, and Notz (2018) is the canonical reference in the
    statistics literature
  \item
    Engineering Design via Surrogate Modeling by Forrester, Sobester,
    and Keane (2008) is perhaps more popular in engineering
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    We will analyze an example from the latter.
  \end{itemize}
\end{itemize}

\hypertarget{introduction-to-gaussian-processes}{%
\section{Introduction to Gaussian
Processes}\label{introduction-to-gaussian-processes}}

The concept of GP (Gaussian Process) regression can be understood as a
simple extension of linear modeling. It is worth noting that this
approach goes by various names and acronyms, including ``kriging,'' a
term derived from geostatistics, as introduced by Matheron in 1963.
Additionally, it is referred to as Gaussian spatial modeling or a
Gaussian stochastic process, and machine learning (ML) researchers often
use the term Gaussian process regression (GPR). In all of these
instances, the central focus is on regression. This involves training on
both inputs and outputs, with the ultimate objective of making
predictions and quantifying uncertainty (referred to as uncertainty
quantification or UQ).

However, it's important to emphasize that GPs are not a universal
solution for every problem. Specialized tools may outperform GPs in
specific, non-generic contexts, and GPs have their own set of
limitations that need to be considered.

\hypertarget{gaussian-process-prior}{%
\subsection{Gaussian Process Prior}\label{gaussian-process-prior}}

In this context, any finite collection of realizations, which is
represented by \(n\) observations, is modeled as having a multivariate
normal (MVN) distribution. The characteristics of these realizations can
be fully described by two key parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Their mean, denoted as an \(n\)-vector \(\mu\).
\item
  The covariance matrix, denoted as an \(n \times n\) matrix \(\Sigma\).
  This covariance matrix encapsulates the relationships and variability
  between the individual realizations within the collection.
\end{enumerate}

\hypertarget{covariance-function}{%
\subsection{Covariance Function}\label{covariance-function}}

The covariance function is defined by inverse exponentiated squared
Euclidean distance: \[\Sigma(x, x') = \exp\{ - || x - x '||^2 \}.\]

Covariance decays exponentially fast as \(x\) and \(x'\) become farther
apart. Observe that \[\Sigma(x,x) = 1\] and \[\Sigma(x, x') < 1\] for
\(x \neq x'\) The function \(\Sigma(x,x')\) must be positive definite.

Positive definiteness in the context of the covariance matrix
\(\Sigma_n\) is a fundamental requirement. It is determined by
evaluating \(\Sigma(x_i, x_j)\) at pairs of \(n\) \(x\)-values, denoted
as \(x_1, x_2, \ldots, x_n\). The condition for positive definiteness is
that for all \(x\) vectors that are not equal to zero, the expression
\(x^\top \Sigma_n x\) must be greater than zero. This property is
essential when intending to use \(\Sigma_n\) as a covariance matrix in
multivariate normal (MVN) analysis. It is analogous to the requirement
in univariate Gaussian distributions where the variance parameter,
\(\sigma^2\), must be positive.

Gaussian Processes (GPs) can be effectively utilized to generate random
data that follows a smooth functional relationship. The process involves
the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select a set of \(x\)-values, denoted as \(x_1, x_2, \ldots, x_n\).
\item
  Define the covariance matrix \(\Sigma_n\) by evaluating
  \(\Sigma_n^{ij} = \Sigma(x_i, x_j)\) for \(i, j = 1, 2, \ldots, n\).
\item
  Generate an \(n\)-variate realization \(Y\) that follows a
  multivariate normal distribution with a mean of zero and a covariance
  matrix \(\Sigma_n\), expressed as
  \(Y \sim \mathcal{N}_n(0, \Sigma_n)\).
\item
  Visualize the result by plotting it in the \(x\)-\(y\) plane.
\end{enumerate}

\hypertarget{construction-of-the-covariance-matrix}{%
\subsection{Construction of the Covariance
Matrix}\label{construction-of-the-covariance-matrix}}

Here is an one-dimensional example. The process begins by creating an
input grid using \(x\)-values. This grid consists of 100 elements,
providing the basis for further analysis and visualization.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{n }\OperatorTok{=} \DecValTok{100}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the context of this discussion, the construction of the covariance
matrix, denoted as \(\Sigma_n\), relies on the concept of inverse
exponentiated squared Euclidean distances. However, it's important to
note that a modification is introduced later in the process.
Specifically, the diagonal of the covariance matrix is augmented with a
small value, represented as ``eps'' or \(\epsilon\).

The reason for this augmentation is that while inverse exponentiated
distances theoretically ensure the covariance matrix's positive
definiteness, in practical applications, the matrix can sometimes become
numerically ill-conditioned. By adding a small value to the diagonal,
such as \(\epsilon\), this ill-conditioning issue is mitigated. In this
context, \(\epsilon\) is often referred to as ``jitter.''

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ multivariate\_normal}
\KeywordTok{def}\NormalTok{ build\_Psi(X, theta):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=}\NormalTok{ theta[l]}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\NormalTok{np.}\BuiltInTok{round}\NormalTok{(Psi, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.   , 0.99 , 0.961, ..., 0.   , 0.   , 0.   ],
       [0.99 , 1.   , 0.99 , ..., 0.   , 0.   , 0.   ],
       [0.961, 0.99 , 1.   , ..., 0.   , 0.   , 0.   ],
       ...,
       [0.   , 0.   , 0.   , ..., 1.   , 0.99 , 0.961],
       [0.   , 0.   , 0.   , ..., 0.99 , 1.   , 0.99 ],
       [0.   , 0.   , 0.   , ..., 0.961, 0.99 , 1.   ]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.imshow(Psi, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}hot\textquotesingle{}}\NormalTok{, interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}nearest\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-5-output-1.pdf}

}

\end{figure}

\hypertarget{generation-of-random-samples-and-plotting-the-realizations-of-the-random-function}{%
\subsection{Generation of Random Samples and Plotting the Realizations
of the Random
Function}\label{generation-of-random-samples-and-plotting-the-realizations-of-the-random-function}}

In the context of the multivariate normal distribution, the next step is
to utilize the previously constructed covariance matrix denoted as
\texttt{Psi}. It is used as an essential component in generating random
samples from the multivariate normal distribution.

The function \texttt{multivariate\_normal} is employed for this purpose.
It serves as a random number generator specifically designed for the
multivariate normal distribution. In this case, the mean of the
distribution is set equal to \texttt{mean}, and the covariance matrix is
provided as \texttt{Psi}. The argument \texttt{size} specifies the
number of realizations, which, in this specific scenario, is set to one.

By default, the mean vector is initialized to zero. To match the number
of samples, which is equivalent to the number of rows in the \texttt{X}
and \texttt{Psi} matrices, the argument \texttt{zeros(n)} is used, where
\texttt{n} represents the number of samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y }\OperatorTok{=}\NormalTok{ multivariate\_normal(zeros(Psi.shape[}\DecValTok{0}\NormalTok{]), Psi, size }\OperatorTok{=} \DecValTok{1}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Y.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(100, 1)
\end{verbatim}

Now we can plot the results, i.e., a finite realization of the random
function \(Y()\) under a GP prior with a particular covariance
structure. We will plot those \texttt{X} and \texttt{Y} pairs as
connected points on an \(x\)-\(y\) plane.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(X, Y)}
\NormalTok{plt.title(}\StringTok{"Realization of Random Functions under a GP prior.}\CharTok{\textbackslash{}n}\StringTok{ theta: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(theta[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-7-output-1.pdf}

}

\end{figure}

\hypertarget{properties-of-the-1d-example}{%
\subsection{Properties of the 1d
Example}\label{properties-of-the-1d-example}}

\hypertarget{several-bumps}{%
\subsubsection{Several Bumps:}\label{several-bumps}}

In this analysis, we observe several bumps in the \(x\)-range of
\([0,10]\). These bumps in the function occur because shorter distances
exhibit high correlation, while longer distances tend to be essentially
uncorrelated. This leads to variations in the function's behavior.

\hypertarget{smoothness}{%
\subsubsection{Smoothness:}\label{smoothness}}

The function plotted in the above figure represents only a finite
realization, which means that we have data for a limited number of
pairs, specifically 100 points. These points appear smooth in a tactile
sense because they are closely spaced, and the plot function connects
the dots with lines to create the appearance of smoothness. The complete
surface, which can be conceptually extended to an infinite realization
over a compact domain, is exceptionally smooth in a calculus sense due
to the covariance function's property of being infinitely
differentiable.

\hypertarget{scale-of-two}{%
\subsubsection{Scale of Two:}\label{scale-of-two}}

Regarding the scale of the \(Y\) values, they have a range of
approximately \([-2,2]\), with a 95\% probability of falling within this
range. This range is influenced by the scale of the covariance, which is
set to 1, without considering the small value, \(\epsilon\), added to
the diagonal. In standard statistical terms, 95\% of the data points
typically fall within two standard deviations of the mean, which is a
common measure of the spread or range of data.

\hypertarget{background-expectation-mean}{%
\section{Background: Expectation,
Mean}\label{background-expectation-mean}}

The distribution of a random vector is characterized by some indexes.
One of them is the expected value, which is defined as \[
E[X] = \sum_{x \in D_X} xp_X(x)  \qquad \text{if $X$ is discrete}
\] \[
E[X] = \int\limits_{x \in D_X} xf_X(x)\mathrm{d}x  \quad  \text{if $X$ is continuous.}
\]

The mean, \(\mu\), of a probability distribution is a measure of its
central tendency or location. That is, \(E(X)\) is defined as the
average of all possible values of \(X\), weighted by their
probabilities.

\hypertarget{example}{%
\subsection{Example}\label{example}}

Let \(X\) denote the number produced by rolling a fair die. Then \[
E(X) = 1 \times 1/6 + 2 \times 1/6 + 3 \times 1/6 + 4 \times 1/6 + 5 \times 1/6 + 6\times 1/6 = 3.5 
\]

\hypertarget{sample-mean}{%
\subsection{Sample Mean}\label{sample-mean}}

The sample mean is an important estimate of the population mean. The
sample mean of a sample \(\{x_i\}\) (\(i=1,2,\ldots,n\)) is defined as
\[\overline{x}  = \frac{1}{n} \sum_i x_i.\]

\hypertarget{variance-and-standard-deviation}{%
\subsection{Variance and Standard
Deviation}\label{variance-and-standard-deviation}}

If we are trying to predict the value of a random variable \(X\) by its
mean \(\mu = E(X)\), the error will be \(X-\mu\). In many situations it
is useful to have an idea how large this deviation or error is. Since
\(E(X-\mu) = E(X) -\mu = 0\), it is necessary to use the absolute value
or the square of (\(X-\mu\)). The squared error is the first choice,
because the derivatives are easier to calculate. These considerations
motivate the definition of the variance:

The variance of a random variable \(X\) is the mean squared deviation of
\(X\) from its expected value \(\mu = E(X)\). \begin{equation}
Var(X) = E[ (X-\mu)^2].
\end{equation}

\hypertarget{standard-deviation}{%
\subsection{Standard Deviation}\label{standard-deviation}}

Taking the square root of the variance to get back to the same scale of
units as \(X\) gives the standard deviation. The standard deviation of
\(X\) is the square root of the variance of \(X\). \begin{equation}
sd(X) = \sqrt{Var(X)}.
\end{equation}

\hypertarget{calculation-of-the-standard-deviation-with-python}{%
\subsection{Calculation of the Standard Deviation with
Python}\label{calculation-of-the-standard-deviation-with-python}}

The function \texttt{numpy.std} returns the standard deviation, a
measure of the spread of a distribution, of the array elements. The
standard deviation is computed for the flattened array by default,
otherwise over the specified axis. The argument \texttt{ddof} specifies
the Delta Degrees of Freedom. The divisor used in calculations is
\texttt{N\ -\ ddof}, where \texttt{N} represents the number of elements.
By default \texttt{ddof} is zero, i.e., \texttt{std} uses the formula
\begin{equation}  \sqrt{  \frac{1}{N} \sum_i \left( x_i - \bar{x} \right)^2  } \qquad \text{with } \quad \bar{x} = \sum_{i=1}^N x_i /N. \end{equation}

If no \texttt{axis} is specified, \texttt{std} uses the flattened array
(which is the default.) Since \(\bar{x} = 2\), the following value is
computed:
\[ \sqrt{1/3 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/3}.\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]])}
\NormalTok{np.std(a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.816496580927726
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, axis }\OperatorTok{=} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0., 0., 0.])
\end{verbatim}

The empirical standard deviation (which uses N-1),
\(\sqrt{1/2 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2 \right)} = \sqrt{2/2}\),
can be calculated as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, axis }\OperatorTok{=} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.81649658])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]])}
\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1, 2],
       [3, 4]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.118033988749895
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1., 1.])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.5, 0.5])
\end{verbatim}

\hypertarget{single-versus-double-precision}{%
\subsection{Single Versus Double
Precision}\label{single-versus-double-precision}}

In single precision, \texttt{std()} can be inaccurate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.zeros((}\DecValTok{2}\NormalTok{, }\DecValTok{4}\OperatorTok{*}\DecValTok{4}\NormalTok{), dtype}\OperatorTok{=}\NormalTok{np.float32)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a[}\DecValTok{0}\NormalTok{, :] }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{a[}\DecValTok{1}\NormalTok{, :] }\OperatorTok{=} \FloatTok{0.1}
\NormalTok{a }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,
        1. , 1. , 1. ],
       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
        0.1, 0.1, 0.1]], dtype=float32)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.45000002
\end{verbatim}

Computing the standard deviation in float64 is more accurate (result may
vary):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, dtype}\OperatorTok{=}\NormalTok{np.float64)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.44999999925494194
\end{verbatim}

\hypertarget{visualization-of-the-standard-deviation}{%
\subsection{Visualization of the Standard
Deviation}\label{visualization-of-the-standard-deviation}}

The standard deviation of normal distributed can be visualized in terms
of the histogram of \(X\):

\begin{itemize}
\tightlist
\item
  about 68\% of the values will lie in the interval within one standard
  deviation of the mean
\item
  95\% lie within two standard deviation of the mean
\item
  and 99.9\% lie within 3 standard deviations of the mean.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{np.random.seed(}\DecValTok{1}\NormalTok{)}

\CommentTok{\# example data}
\NormalTok{mu }\OperatorTok{=} \FloatTok{0.0}  \CommentTok{\# mean of distribution}
\NormalTok{sigma }\OperatorTok{=} \DecValTok{1}  \CommentTok{\# standard deviation of distribution}
\NormalTok{x }\OperatorTok{=}\NormalTok{ mu }\OperatorTok{+}\NormalTok{ sigma }\OperatorTok{*}\NormalTok{ np.random.randn(}\DecValTok{2000}\NormalTok{)}
\NormalTok{num\_bins }\OperatorTok{=} \DecValTok{33}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\CommentTok{\# the histogram of the data}
\NormalTok{n, bins, patches }\OperatorTok{=}\NormalTok{ ax.hist(x, num\_bins, density}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\CommentTok{\# add a \textquotesingle{}best fit\textquotesingle{} line}
\NormalTok{y }\OperatorTok{=}\NormalTok{ ((}\DecValTok{1} \OperatorTok{/}\NormalTok{ (np.sqrt(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi) }\OperatorTok{*}\NormalTok{ sigma)) }\OperatorTok{*}
\NormalTok{     np.exp(}\OperatorTok{{-}}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{/}\NormalTok{ sigma }\OperatorTok{*}\NormalTok{ (bins }\OperatorTok{{-}}\NormalTok{ mu))}\OperatorTok{**}\DecValTok{2}\NormalTok{))}
\NormalTok{ax.plot(bins, y, }\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}Probability density\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_title(}\VerbatimStringTok{r\textquotesingle{}Histogram: $\textbackslash{}mu=0$, $\textbackslash{}sigma=1$\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.vlines(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, ymin}\OperatorTok{=}\DecValTok{0}\NormalTok{, ymax }\OperatorTok{=}\NormalTok{ ((}\DecValTok{1} \OperatorTok{/}\NormalTok{ (np.sqrt(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi) }\OperatorTok{*}\NormalTok{ sigma)) }\OperatorTok{*}
\NormalTok{     np.exp(}\OperatorTok{{-}}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{/}\NormalTok{ sigma }\OperatorTok{*}\NormalTok{ (}\OperatorTok{{-}}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ mu))}\OperatorTok{**}\DecValTok{2}\NormalTok{)), colors}\OperatorTok{=}\StringTok{"orange"}\NormalTok{, linestyles}\OperatorTok{=}\StringTok{"{-}."}\NormalTok{)}
\NormalTok{ax.vlines(}\DecValTok{1}\NormalTok{, ymin}\OperatorTok{=}\DecValTok{0}\NormalTok{, ymax }\OperatorTok{=}\NormalTok{ ((}\DecValTok{1} \OperatorTok{/}\NormalTok{ (np.sqrt(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi) }\OperatorTok{*}\NormalTok{ sigma)) }\OperatorTok{*}
\NormalTok{     np.exp(}\OperatorTok{{-}}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{/}\NormalTok{ sigma }\OperatorTok{*}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ mu))}\OperatorTok{**}\DecValTok{2}\NormalTok{)), colors}\OperatorTok{=}\StringTok{"orange"}\NormalTok{, linestyles}\OperatorTok{=}\StringTok{"{-}."}\NormalTok{)}
\CommentTok{\# Tweak spacing to prevent clipping of ylabel}
\NormalTok{fig.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-20-output-1.pdf}

}

\end{figure}

\hypertarget{standardization}{%
\section{Standardization}\label{standardization}}

To compare statistical properties of random variables which use
different units, it is a common practice to transform these random
variables into standardized variables. If a random variable \(X\) has
expectation \(E(X) = \mu\) and standard deviation \(sd(X) = \sigma >0\),
the random variable \[
X^{\ast} = (X-\mu)/\sigma
\] is called \(X\) in standard units. It has \(E(X^{\ast}) = 0\) and
\(sd(X^{\ast}) =1\).

\hypertarget{random-numbers-in-python}{%
\section{Random Numbers in Python}\label{random-numbers-in-python}}

Results from computers are deterministic, so it sounds like a
contradiction in terms to generate random numbers on a computer.
Standard computers generate pseudo-randomnumbers, i.e., numbers that
behave as if they were drawn randomly. To generate ten random numbers
from a normal distribution, the following command can be used.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}
\NormalTok{n }\OperatorTok{=} \DecValTok{10}
\NormalTok{mu, sigma }\OperatorTok{=} \DecValTok{2}\NormalTok{, }\FloatTok{0.1}
\NormalTok{x }\OperatorTok{=}\NormalTok{ rng.normal(mu, sigma, n)}
\end{Highlighting}
\end{Shaded}

Verify the mean:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(mu }\OperatorTok{{-}}\NormalTok{ np.mean(x))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.008943346416482978
\end{verbatim}

Note: To verify the standard deviation, we use \texttt{ddof\ =\ 1}
(empirical standard deviation):

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(sigma }\OperatorTok{{-}}\NormalTok{ np.std(x, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.012307297550095839
\end{verbatim}

\hypertarget{the-multivariate-normal-distribution}{%
\section{The Multivariate Normal
Distribution}\label{the-multivariate-normal-distribution}}

The multivariate normal, multinormal, or Gaussian distribution serves as
a generalization of the one-dimensional normal distribution to higher
dimensions. To fully define this distribution, it is necessary to
specify its mean and covariance matrix. These parameters are analogous
to the mean, which represents the central location, and the variance
(squared standard deviation) of the one-dimensional normal distribution.

In the context of the multivariate normal distribution, the mean takes
the form of a coordinate within an n-dimensional space. This coordinate
represents the location where samples are most likely to be generated,
akin to the peak of the bell curve in a one-dimensional or univariate
normal distribution.

On the other hand, the covariance within the multivariate normal
distribution denotes the extent to which two variables vary together.
When drawing n-dimensional samples from this distribution, it results in
a set of values represented as \(X = [x_1, x_2, \ldots, x_n]\). The
elements of the covariance matrix, such as \(C_{ij}\), represent the
covariances between the variables \(x_i\) and \(x_j\). These covariances
describe how the different variables in the distribution are related to
each other in terms of their variability.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{mean }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]}
\NormalTok{cov }\OperatorTok{=}\NormalTok{ [[}\DecValTok{9}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{]]  }\CommentTok{\# diagonal covariance}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(mean, cov, }\DecValTok{1000}\NormalTok{).T}
\NormalTok{plt.plot(x, y, }\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.axis(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\NormalTok{plt.title(}\StringTok{"Bivariate Normal. Mean zero and covariance: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(cov))}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-24-output-1.pdf}

}

\end{figure}

\hypertarget{kriging}{%
\section{Kriging}\label{kriging}}

\hypertarget{the-kriging-covariance-matrix}{%
\subsection{The Kriging Covariance
Matrix}\label{the-kriging-covariance-matrix}}

\begin{itemize}
\tightlist
\item
  Basis functions of the form
  \begin{equation} \psi^{(i)} = \exp \left( - \sum_{l=1}^k \theta_l | x_{l}^{(i)} - x_{l} | ^{p_l} \right) \end{equation}
  are used in a method known as Kriging.
\item
  Although the Kriging basis function is related to the Gaussian basis
  function, there are some differences:

  \begin{itemize}
  \tightlist
  \item
    Where the Gaussian basis function has \(1/\sigma^2\), the Kriging
    basis has a vector
    \(\theta = [\theta_1, \theta_2, \ldots, \theta_k]^T\).
  \item
    The \(\theta\) vector allows the width of the basis function to vary
    from dimension to dimension.
  \item
    In the Gaussian basis function, the exponent is fixed at 2, Kriging
    allows this exponent to vary (typically from 1 to 2).
  \end{itemize}
\end{itemize}

\hypertarget{building-the-kriging-model}{%
\subsection{Building the Kriging
Model}\label{building-the-kriging-model}}

\begin{itemize}
\tightlist
\item
  Consider a set of \(k\)-dimensional sample data
  \[X = [x_1, x_2, \ldots, x_n],\] i.e., \(X\) is a \((n,k)\)-matrix,
  with related responses \[y = [y_1, y_2, \ldots, y_n].\]
\item
  We are interested in finding an expression for the predicted value at
  a new point \(x\).
\item
  We are going to interpret the observed responses as if they are
  generated by a stochastic process, which will be denoted as follows:
  \begin{equation} Y = [Y(x_1), \ldots, Y(x_n) ]^T. \end{equation}
\item
  The random process has a mean of \(\mu\) (which is a (n,1)-dimensional
  vector). The random variables are correlated with each other using the
  basis function expression
  \begin{equation} \text{cor} [Y(x_i), Y(x_j)] =  \exp \left( - \sum_{l=1}^k \theta_l | x_{il} - x_{jl} | ^{p_l} \right). \end{equation}
\item
  From this, the \((n,n)\) correlation matrix \(\Psi\) of the observed
  data can be computed:
  \begin{equation} \Psi = \begin{pmatrix} \text{cor} [Y(x_1), Y(x_1)] & \ldots & \text{cor} [Y(x_1), Y(x_n)] \\  \vdots & \ddots & \vdots \\ \text{cor} [Y(x_n), Y(x_1)] &  \ldots & \text{cor} [Y(x_n), Y(x_n)] \end{pmatrix} .\end{equation}
\end{itemize}

\hypertarget{example-correlation-matrix}{%
\subsection{Example: Correlation
Matrix}\label{example-correlation-matrix}}

\begin{itemize}
\item
  Let \(n=4\) and \(k=3\). The sample plan is represented by the
  following matrix \(X\):
  \[X = \begin{pmatrix} x_{11} & x_{12} & x_{13}\\ x_{21} & x_{22} & x_{23}\\ x_{31} & x_{32} & x_{33}\\ x_{41} & x_{42} & x_{43}\\ \end{pmatrix}\]
\item
  To compute the elements of the matrix \(\Psi\), the following \(k\)
  \((n,n)\)-matrices have to be computed:
  \[ D_1 = \begin{pmatrix} x_{11} - x_{11} & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  x_{21} - x_{11} & x_{21} - x_{21} & x_{21} -x_{31} & x_{21} - x_{41} \\ x_{31} - x_{11} & x_{31} - x_{21} & x_{31} -x_{31} & x_{31} - x_{41} \\ x_{41} - x_{11} & x_{41} - x_{21} & x_{41} -x_{31} & x_{41} - x_{41} \\\end{pmatrix}\]
  \[ D_2 = \begin{pmatrix} x_{12} - x_{12} & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  x_{22} - x_{12} & x_{22} - x_{22} & x_{22} -x_{32} & x_{22} - x_{42} \\ x_{32} - x_{12} & x_{32} - x_{22} & x_{32} -x_{32} & x_{32} - x_{42} \\ x_{42} - x_{12} & x_{42} - x_{22} & x_{42} -x_{32} & x_{42} - x_{42} \\\end{pmatrix}\]
  \[ D_3 = \begin{pmatrix} x_{13} - x_{13} & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  x_{23} - x_{13} & x_{23} - x_{23} & x_{23} -x_{33} & x_{23} - x_{43} \\ x_{33} - x_{13} & x_{33} - x_{23} & x_{33} -x_{33} & x_{33} - x_{43} \\ x_{43} - x_{13} & x_{43} - x_{23} & x_{43} -x_{33} & x_{43} - x_{43} \\\end{pmatrix}\]
\item
  Since the matrices are symmetric and the main diagonals are zero, it
  is sufficient to compute the following matrices:
  \[D_1 = \begin{pmatrix} 0 & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  0 &  0 & x_{21} -x_{31} & x_{21} - x_{41} \\ 0 & 0 & 0 & x_{31} - x_{41} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}\]
  \[ D_2 = \begin{pmatrix} 0 & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  0 & 0 & x_{22} -x_{32} & x_{22} - x_{42} \\ 0 & 0 & 0 & x_{32} - x_{42} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}\]
  \[ D_3 = \begin{pmatrix} 0 & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  0 & 0 & x_{23} -x_{33} & x_{23} - x_{43} \\ 0 & 0 & 0 & x_{33} - x_{43} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}\]
\item
  We will consider \(p_l=2\).
\item
  The differences will be squared and multiplied by \(\theta_i\), i.e.:
  \[ D_1 = \theta_1 \begin{pmatrix} 0 & (x_{11} - x_{21})^2 & (x_{11} -x_{31})^2 & (x_{11} - x_{41})^2 \\  0 &  0 & (x_{21} -x_{31})^2 & (x_{21} - x_{41})^2 \\ 0 & 0 & 0 & (x_{31} - x_{41})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}\]
  \[ D_2 = \theta_2 \begin{pmatrix} 0 & (x_{12} - x_{22})^2 & (x_{12} -x_{32})^2 & (x_{12} - x_{42})^2 \\  0 & 0 & (x_{22} -x_{32})^2 & (x_{22} - x_{42})^2 \\ 0 & 0 & 0 & (x_{32} - x_{42})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}\]
  \[ D_3 = \theta_3 \begin{pmatrix} 0 & (x_{13} - x_{23})^2 & (x_{13} -x_{33})^2 & (x_{13} - x_{43})^2 \\  0 & 0 & (x_{23} -x_{33})^2 & (x_{23} - x_{43})^2 \\ 0 & 0 & 0 & (x_{33} - x_{43})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}\]
\item
  The sum of the three matrices \(D=D_1+ D_2 + D_3\) will be calculated
  next: \[ \begin{pmatrix} 0 & 
  \theta_1  (x_{11} - x_{21})^2 + \theta_2 (x_{12} - x_{22})^2 + \theta_3  (x_{13} - x_{23})^2  &
  \theta_1 (x_{11} -x_{31})^2 + \theta_2  (x_{12} -x_{32})^2 + \theta_3  (x_{13} -x_{33})^2 &
  \theta_1  (x_{11} - x_{41})^2 + \theta_2  (x_{12} - x_{42})^2 + \theta_3 (x_{13} - x_{43})^2
  \\  0 &  0 & 
  \theta_1  (x_{21} -x_{31})^2 + \theta_2 (x_{22} -x_{32})^2 + \theta_3  (x_{23} -x_{33})^2 &
  \theta_1  x_{21} - x_{41})^2 + \theta_2  (x_{22} - x_{42})^2 + \theta_3 (x_{23} - x_{43})^2
  \\ 0 & 0 & 0 & 
  \theta_1 (x_{31} - x_{41})^2 + \theta_2 (x_{32} - x_{42})^2 + \theta_3 (x_{33} - x_{43})^2
  \\ 0 & 0 & 0 & 0 \\\end{pmatrix}\]
\item
  Finally, \[ \Psi = \exp(-D)\] is computed.
\item
  Next, we will demonstrate how this computation can be implemented in
  Python.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([ [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{]])}
\NormalTok{X}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1, 0, 0],
       [0, 1, 0],
       [0, 0, 1],
       [0, 0, 0]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_Psi(X, theta):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=}\NormalTok{ theta[l]}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.01831564, 0.36787944],
       [0.04978707, 1.        , 0.00673795, 0.13533528],
       [0.01831564, 0.00673795, 1.        , 0.04978707],
       [0.36787944, 0.13533528, 0.04978707, 1.        ]])
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The same result can be obtained with existing python functions, e.g.,
  from the package \texttt{scipy}.

  \begin{itemize}
  \tightlist
  \item
    Note: A small value, \texttt{eps}, can be passed to the function
    \texttt{build\_Psi} to improve the condition number.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ squareform}
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ pdist}

\KeywordTok{def}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{ squareform(pdist(X, metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{, out}\OperatorTok{=}\VariableTok{None}\NormalTok{, w}\OperatorTok{=}\NormalTok{theta))) }\OperatorTok{+}\NormalTok{  multiply(eye(X.shape[}\DecValTok{0}\NormalTok{]), eps)}

\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\FloatTok{.0}\NormalTok{)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.01831564, 0.36787944],
       [0.04978707, 1.        , 0.00673795, 0.13533528],
       [0.01831564, 0.00673795, 1.        , 0.04978707],
       [0.36787944, 0.13533528, 0.04978707, 1.        ]])
\end{verbatim}

\hypertarget{generate-sample-from-nmu-psi}{%
\subsection{\texorpdfstring{Generate sample from N(\(\mu\),
\(\Psi\))}{Generate sample from N(\textbackslash mu, \textbackslash Psi)}}\label{generate-sample-from-nmu-psi}}

\begin{itemize}
\tightlist
\item
  The following block of code generates 5 draws from a bivariate normal
  distribution with zero mean and covariance matrix \(\Psi\).
\item
  Because our input space is three dimensional, the results cannot be
  visualized in a simple way.
\item
  The next examples will use a one-dimensional input space, so that our
  results can be easily visualized in 2d.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ multivariate\_normal}
\NormalTok{m }\OperatorTok{=} \DecValTok{5}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ multivariate\_normal(zeros(Psi.shape[}\DecValTok{0}\NormalTok{]), Psi, size}\OperatorTok{=}\NormalTok{m)}
\end{Highlighting}
\end{Shaded}

\hypertarget{kriging-in-a-nutshell}{%
\chapter{Kriging in a Nutshell}\label{kriging-in-a-nutshell}}

\begin{itemize}
\tightlist
\item
  We will use the Kriging correlation \(\Psi\) to predict new values
  based on the observed data.
\item
  The matrix algebra involved i calculating the likelihood is the most
  computationally intensive part of the Kriging process

  \begin{itemize}
  \tightlist
  \item
    Care must be taken that the computer code is as efficient as
    possible.
  \end{itemize}
\item
  Basic elements of the Kriging based surrogate optimization such as
  interpolation, expected improvement, and regression are presented.
\item
  The presentation follows the approach described in Forr08a and
  Bart21i.
\end{itemize}

\hypertarget{the-kriging-model}{%
\subsection{The Kriging Model}\label{the-kriging-model}}

\begin{itemize}
\item
  Consider sample data \(\vec{X}\) and \(\vec{y}\) from \(n\) locations
  that are available in matrix form: \(\vec{X}\) is a \((n \times k)\)
  matrix, where \(k\) denotes the problem dimension and \(\vec{y}\) is a
  \((n\times 1)\) vector.
\item
  The observed responses \(\vec{y}\) are considered as if they are from
  a stochastic process, which will be denoted as \begin{equation*}
  \begin{pmatrix}
  \vec{Y}(\vec{x}^{(1)})\\
  \vdots\\
  \vec{Y}(\vec{x}^{(n)})\\
  \end{pmatrix}.
  \end{equation*}
\item
  The set of random vectors (also referred to as a \emph{random field})
  has a mean of \(\vec{1} \mu\), which is a \((n\times 1)\) vector.
\end{itemize}

\hypertarget{correlations}{%
\subsection{Correlations}\label{correlations}}

\begin{itemize}
\item
  The random vectors are correlated with each other using the basis
  function expression
  \[\text{cor} \left(\vec{Y}(\vec{x}^{(i)}),\vec{Y}(\vec{x}^{(l)}) \right) = \exp\left\{ - \sum_{j=1}^k \theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\right\}.\]
\item
  The \((n \times n)\) correlation matrix of the observed sample data is
  \[\vec{\Psi} = \begin{pmatrix}
  \text{cor}\left(
  \vec{Y}(\vec{x}^{(i)}),
  \vec{Y}(\vec{x}^{(l)}) 
  \right) & \ldots &
  \text{cor}\left(
  \vec{Y}(\vec{x}^{(i)}),
  \vec{Y}(\vec{x}^{(l)}) 
  \right)\\
  \vdots  & \vdots &  \vdots\\
   \text{cor}\left(
  \vec{Y}(\vec{x}^{(i)}),
  \vec{Y}(\vec{x}^{(l)}) 
  \right)&
  \ldots &
  \text{cor}\left(
  \vec{Y}(\vec{x}^{(i)}),
  \vec{Y}(\vec{x}^{(l)}) 
  \right)
  \end{pmatrix}.\]
\item
  Note: correlations depend on the absolute distances between sample
  points \(|x_j^{(n)} - x_j^{(n)}|\) and the parameters \(p_j\) and
  \(\theta_j\).
\item
  Correlation is intuitive, because when two points move close together,
  then \(|x_l^{(i)} - x_l| \to 0\) and
  \(\exp(-|x_l^{(i)} - x_l| \to 1\), points show very close correlation
  and \(Y(x_l^{(i)}) = Y(x_l)\).
\item
  \(\theta\) can be seen as a width parameter:

  \begin{itemize}
  \tightlist
  \item
    low \(\theta_j\) means that all points will have a high correlation,
    with \(Y(x_j)\) being similar across the sample
  \item
    high \(\theta_j\) means that there is a significant difference
    between the \(Y(x_j)\)'s
  \end{itemize}
\item
  \(\theta_j\) is a measure of how active the function we are
  approximating is
\item
  High \(\theta_j\) indicate important parameters
\end{itemize}

\hypertarget{mle-to-estimate-theta-and-p}{%
\subsection{\texorpdfstring{MLE to estimate \(\theta\) and
\(p\)}{MLE to estimate \textbackslash theta and p}}\label{mle-to-estimate-theta-and-p}}

\begin{itemize}
\tightlist
\item
  We know what the correlations mean, but how do we estimate the values
  of \(\theta_j\) amd where does our observed data \(y\) come in?
\item
  To estimate the values of \(\vec{\theta}\) and \(\vec{p}\), they are
  chosen to maximize the likelihood of \(\vec{y}\), which can be
  expressed in terms of the sample data
  \[L\left(\vec{Y}(\vec{x}^{(1)}), \ldots, \vec{Y}(\vec{x}^{(n)}) | \mu, \sigma \right) = \frac{1}{(2\pi \sigma)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left\{ \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}\right\},\]
  and formulated as the log-likelihood:
  \[\ln(L) = - \frac{n}{2} \ln(2\pi \sigma) - \frac{1}{2} \ln |\vec{\Psi}| \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}.\]
\item
  Optimization of the log-likelihood by taking derivatives with respect
  to \(\mu\) and \(\sigma\) results in
  \[\hat{\mu} = \frac{\vec{1}^T \vec{\Psi}^{-1} \vec{y}^T}{\vec{1}^T \vec{\Psi}^{-1} \vec{1}^T}\]
  and
  \[\hat{\sigma} = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{n}.\]
\item
  Combining the equations leads to the concentrated log-likelihood:
  \[\ln(L) = - \frac{n}{2} \ln(\hat{\sigma}) - \frac{1}{2} \ln |\vec{\Psi}|.\]
\item
  Note:

  \begin{itemize}
  \tightlist
  \item
    The first term requires information about the measured point
    (observations) \(y_i\).
  \item
    To maximize \(\ln(L)\), optimal values of \(\vec{\theta}\) and
    \(\vec{p}\) are determined numerically, because the equation is not
    differentiable.
  \end{itemize}
\end{itemize}

\hypertarget{tuning-theta-and-p}{%
\section{\texorpdfstring{Tuning \(\theta\) and
\(p\)}{Tuning \textbackslash theta and p}}\label{tuning-theta-and-p}}

\begin{itemize}
\tightlist
\item
  Optimizers such as Nelder-Mead, Conjugate Gradient, or Simulated
  Annealing can be used to determine optimal values for \(\theta\) and
  \(p\).
\item
  After the optimization, the correlation matrix is build with the
  optimized \(\theta\) and \(p\) values. This is best (most likely)
  Kriging model for the given data \(y\).
\item
  We will skip the optimization step here and use \(\theta = 1\) and
  \(p=2\).
\end{itemize}

\hypertarget{kriging-prediction}{%
\section{Kriging Prediction}\label{kriging-prediction}}

\begin{itemize}
\item
  Main idea for prediction:

  \begin{itemize}
  \tightlist
  \item
    The new \(Y(x)\) should be consistent with the old sample data
    \(X\).
  \end{itemize}
\item
  For a new prediction \(\hat{y}\) at \(\vec{x}\), the value of
  \(\hat{y}\) is chosen so that it maximizes the likelihood of the
  sample data \(\vec{X}\) and the prediction, given the (optimized)
  correlation parameter \(\vec{\theta}\) and \(\vec{p}\) from above.
\item
  The observed data \(\vec{y}\) is augmented with the new prediction
  \(\hat{y}\) which results in the augmented vector
  \(\vec{\tilde{y}} = ( \vec{y}^T, \hat{y})^T\).
\item
  A vector of correlations between the observed data and the new
  prediction is defined as \begin{equation*}
  \vec{\psi} = 
  \begin{pmatrix}
  \text{cor}\left(
  \vec{Y}(\vec{x}^{(1)}),
  \vec{Y}(\vec{x}) 
  \right) \\
  \vdots  \\
  \text{cor}\left(
  \vec{Y}(\vec{x}^{(n)}),
  \vec{Y}(\vec{x}) 
  \right)
  \end{pmatrix}
  =
  \begin{pmatrix}
  \vec{\psi}^{(1)}\\
  \vdots\\
  \vec{\psi}^{(n)}
  \end{pmatrix}.
  \end{equation*}
\item
  The augmented correlation matrix is constructed as \begin{equation*}
  \tilde{\vec{\Psi}} =
  \begin{pmatrix}
  \vec{\Psi} & \vec{\psi} \\
  \vec{\psi}^T & 1
  \end{pmatrix}.
  \end{equation*}
\item
  The log-likelihood of the augmented data is
  \[\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\hat{\sigma}^2) - \frac{1}{2} \ln |\vec{\hat{\Psi}}| -  \frac{(\vec{\tilde{y}} - \vec{1}\hat{\mu})^T \vec{\tilde{\Psi}}^{-1}(\vec{\tilde{y}} - \vec{1}\hat{\mu})}{2 \hat{\sigma}^2}.\]
\item
  The MLE for \(\hat{y}\) can be calculated as
  \[\hat{y}(\vec{x}) = \hat{\mu} + \vec{\psi}^T \vec{\tilde{\Psi}}^{-1} (\vec{y} - \vec{1}\hat{\mu}).\]
\end{itemize}

\hypertarget{properties-of-the-predictor}{%
\subsection{Properties of the
Predictor}\label{properties-of-the-predictor}}

\begin{itemize}
\tightlist
\item
  This Equation reveals two important properties of the Kriging
  predictor:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Basis functions}: The basis function impacts the vector
    \(\vec{\psi}\), which contains the \(n\) correlations between the
    new point \(\vec{x}\) and the observed locations. Values from the
    \(n\) basis functions are added to a mean base term \(\mu\) with
    weightings
    \(\vec{w} = \vec{\tilde{\Psi}}^{(-1)} (\vec{y} - \vec{1}\hat{\mu})\).
  \item
    \textbf{Interpolation}: The predictions interpolate the sample data.
    When calculating the prediction at the \(i\)th sample point,
    \(\vec{x}^{(i)}\), the \(i\)th column of \(\vec{\Psi}^{-1}\) is
    \(\vec{\psi}\), and \(\vec{\psi} \vec{\Psi}^{-1}\) is the \(i\)th
    unit vector. Hence, \(\hat{y}(\vec{x}^{(i)}) = y^{(i)}\).
  \end{itemize}
\end{itemize}

\hypertarget{example-sinusoid-function}{%
\section{Example: Sinusoid Function}\label{example-sinusoid-function}}

\begin{itemize}
\tightlist
\item
  Toy example in 1d where the response is a simple sinusoid measured at
  eight equally spaced \(x\)-locations in the span of a single period of
  oscillation.
\end{itemize}

\hypertarget{calculating-the-correlation-matrix-psi}{%
\section{\texorpdfstring{Calculating the Correlation Matrix
\(\Psi\)}{Calculating the Correlation Matrix \textbackslash Psi}}\label{calculating-the-correlation-matrix-psi}}

\begin{itemize}
\tightlist
\item
  The correlation matrix \(\Psi\) is based on the pairwise squared
  distances between the input locations
\item
  Here we will use \(n=8\) sample locations
\item
  \(\theta\) is set to 1.0
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{8}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\CommentTok{\# theta should be an array (of one value, for the moment, will be changed later)}
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Evaluate at sample points
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(X)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Sin(x) evaluated at }\SpecialCharTok{\{\}}\StringTok{ points"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(n))}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-32-output-1.pdf}

}

\end{figure}

\hypertarget{computing-the-psi-vector}{%
\section{\texorpdfstring{Computing the \(\psi\)
Vector}{Computing the \textbackslash psi Vector}}\label{computing-the-psi-vector}}

\hypertarget{based-on-distances-between-testing-and-training-data-locations}{%
\subsection{Based on Distances Between Testing and Training Data
Locations}\label{based-on-distances-between-testing-and-training-data-locations}}

\begin{itemize}
\tightlist
\item
  Distances between testing locations \(x\) and training data locations
  \(X\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ cdist}

\KeywordTok{def}\NormalTok{ build\_psi(X, x, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ x.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    theta }\OperatorTok{=}\NormalTok{ theta }\OperatorTok{*}\NormalTok{ ones(k)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ cdist(x.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{,}
\NormalTok{              out}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{              w}\OperatorTok{=}\NormalTok{theta)}
    \BuiltInTok{print}\NormalTok{(D.shape)}
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}
    \CommentTok{\# return psi transpose to be consistent with the literature}
    \ControlFlowTok{return}\NormalTok{(psi.T)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We would like to predict at \(m = 100\) new locations:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=} \DecValTok{100}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, m, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{psi }\OperatorTok{=}\NormalTok{ build\_psi(X, x, theta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(100, 8)
\end{verbatim}

\hypertarget{predictive-equations}{%
\section{Predictive Equations}\label{predictive-equations}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U }\OperatorTok{=}\NormalTok{ cholesky(Psi).T}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one }\OperatorTok{=}\NormalTok{ np.ones(n).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ (one.T.dot(solve(U, solve(U.T, y)))) }\OperatorTok{/}\NormalTok{ one.T.dot(solve(U, solve(U.T, one)))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OperatorTok{=}\NormalTok{ mu }\OperatorTok{*}\NormalTok{ ones(m).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ psi.T.dot(solve(U, solve(U.T, y }\OperatorTok{{-}}\NormalTok{ one }\OperatorTok{*}\NormalTok{ mu)))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(x, f, color }\OperatorTok{=} \StringTok{"orange"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Fitted"}\NormalTok{)}
\NormalTok{plt.plot(x, np.sin(x), color }\OperatorTok{=} \StringTok{"grey"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Original"}\NormalTok{)}
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Measurements"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Kriging prediction of sin(x) with }\SpecialCharTok{\{\}}\StringTok{ points.}\CharTok{\textbackslash{}n}\StringTok{ theta: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(n, theta[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper right\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-38-output-1.pdf}

}

\end{figure}

\hypertarget{exercises-2}{%
\chapter{Exercises}\label{exercises-2}}

\hypertarget{number-of-sample-points}{%
\section{1 Number of Sample Points}\label{number-of-sample-points}}

\begin{itemize}
\tightlist
\item
  The example uses \(n=8\) sample points to fit the sin function.

  \begin{itemize}
  \tightlist
  \item
    What happens, if less than 8 samples are available?
  \end{itemize}
\end{itemize}

\hypertarget{modified-theta-values}{%
\section{\texorpdfstring{2 Modified \(\theta\)
values}{2 Modified \textbackslash theta values}}\label{modified-theta-values}}

\begin{itemize}
\tightlist
\item
  The example uses a \(\theta\) value of \(1.0\).

  \begin{itemize}
  \tightlist
  \item
    What happens if \(\theta\) is modified?
  \item
    Can get better predictions with smaller or larger \(\theta\) values?
  \end{itemize}
\end{itemize}

\hypertarget{prediction-interval}{%
\section{3 Prediction Interval}\label{prediction-interval}}

\begin{itemize}
\tightlist
\item
  The prediction interval was identical to the measurement interval,
  i.e., in the range from \(0\) to \(2\pi\). This is referred to as
  ``interpolation''.

  \begin{itemize}
  \tightlist
  \item
    What happens if this interval is increased (which is referred to as
    ``extrapolation'')?
  \end{itemize}
\end{itemize}

\hypertarget{exercise-rbf}{%
\section{Exercise RBF}\label{exercise-rbf}}

\hypertarget{package-loading}{%
\subsection{Package Loading}\label{package-loading}}

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{matplotlib inline}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy.matlib }\ImportTok{import}\NormalTok{ eye}
\ImportTok{import}\NormalTok{ scipy.linalg}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ linalg }\ImportTok{as}\NormalTok{ LA}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\hypertarget{define-a-small-number}{%
\subsection{Define a small number}\label{define-a-small-number}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eps }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-sampling-plan-x}{%
\subsection{The Sampling Plan (X)}\label{the-sampling-plan-x}}

\begin{itemize}
\tightlist
\item
  We will use 256 points.
\item
  The first 10 points are shown below.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{256}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{X[}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 0.67319891, -0.11153561],
       [ 0.4979386 , -0.10717298],
       [-0.9991017 ,  1.66863389],
       [-0.423669  , -0.97527218],
       [-0.57241762,  1.77447307],
       [ 1.34580967,  0.62640122],
       [ 0.18662343,  0.18793039],
       [ 1.7664757 ,  1.65743858],
       [ 0.98282275,  0.42361525]])
\end{verbatim}

\hypertarget{the-objective-function}{%
\subsection{The Objective Function}\label{the-objective-function}}

\begin{itemize}
\tightlist
\item
  Here we use \(\sum_{i=1}^n (x_i-1)^2\).
\item
  \texttt{f\_map()} is a helper function that maps \(f\) to the entries
  (points) in the matrix \(X\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ f(x):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((x}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}

\KeywordTok{def}\NormalTok{ f\_map(x):}
    \ControlFlowTok{return}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(f, x)))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ f\_map(X)}
\NormalTok{y[}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1.34231036, 1.47789766, 4.44347889, 5.9285336 , 3.07230572,
       0.25916038, 1.32103849, 1.01971047, 0.33251443])
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Alternatively, we can use pre-defined functions from the
  \texttt{pyspot} package:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fun = analytical(sigma=0).fun\_branin}
\CommentTok{\# fun = analytical(sigma=0).fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{XX, YY }\OperatorTok{=}\NormalTok{ np.meshgrid(np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{128}\NormalTok{), np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{128}\NormalTok{))}
\NormalTok{zz }\OperatorTok{=}\NormalTok{ np.array([f\_map(np.array([xi, yi]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\ControlFlowTok{for}\NormalTok{ xi, yi }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(np.ravel(XX), np.ravel(YY))]).reshape(}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{2.7}\NormalTok{), layout}\OperatorTok{=}\StringTok{\textquotesingle{}constrained\textquotesingle{}}\NormalTok{)}
\NormalTok{co }\OperatorTok{=}\NormalTok{ ax.pcolormesh(XX, YY, zz, vmin}\OperatorTok{={-}}\DecValTok{1}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{1}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}RdBu\_r\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-46-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{2.7}\NormalTok{), layout}\OperatorTok{=}\StringTok{\textquotesingle{}constrained\textquotesingle{}}\NormalTok{)}
\NormalTok{co }\OperatorTok{=}\NormalTok{ ax.contourf(XX, YY, zz, levels}\OperatorTok{=}\NormalTok{np.linspace(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-47-output-1.pdf}

}

\end{figure}

\hypertarget{the-gram-matrix}{%
\subsection{The Gram Matrix}\label{the-gram-matrix}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_Gram(X):}
        \CommentTok{"""}
\CommentTok{        Construction of the Gram matrix.}
\CommentTok{        """}
\NormalTok{        n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{        G }\OperatorTok{=}\NormalTok{ np.zeros((n, n))}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                G[i, j] }\OperatorTok{=}\NormalTok{ np.linalg.norm(X[i] }\OperatorTok{{-}}\NormalTok{ X[j])}
\NormalTok{        G }\OperatorTok{=}\NormalTok{ G }\OperatorTok{+}\NormalTok{ G.T    }
        \ControlFlowTok{return}\NormalTok{ G}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{G }\OperatorTok{=}\NormalTok{ build\_Gram(X)}
\NormalTok{np.}\BuiltInTok{round}\NormalTok{(G,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[0.  , 0.51, 0.57, ..., 0.99, 0.62, 1.39],
       [0.51, 0.  , 0.18, ..., 0.91, 0.94, 1.41],
       [0.57, 0.18, 0.  , ..., 0.75, 0.87, 1.26],
       ...,
       [0.99, 0.91, 0.75, ..., 0.  , 0.71, 0.52],
       [0.62, 0.94, 0.87, ..., 0.71, 0.  , 0.91],
       [1.39, 1.41, 1.26, ..., 0.52, 0.91, 0.  ]])
\end{verbatim}

\hypertarget{the-radial-basis-functions}{%
\subsection{The Radial Basis
Functions}\label{the-radial-basis-functions}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ basis\_linear(r):}
    \ControlFlowTok{return}\NormalTok{ r}\OperatorTok{*}\NormalTok{r}\OperatorTok{*}\NormalTok{r}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ basis\_gauss(r, sigma }\OperatorTok{=} \FloatTok{1e{-}1}\NormalTok{):}
    \ControlFlowTok{return}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{r}\OperatorTok{**}\DecValTok{2}\OperatorTok{/}\NormalTok{sigma)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We select the Gaussian basis function for the following examples:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{basis }\OperatorTok{=}\NormalTok{ basis\_gauss}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-psi-matrix}{%
\subsection{\texorpdfstring{The \(\Psi\)
Matrix}{The \textbackslash Psi Matrix}}\label{the-psi-matrix}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_Phi(G, basis, eps}\OperatorTok{=}\NormalTok{np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{))):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ G.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    Phi }\OperatorTok{=}\NormalTok{ np.zeros((n,n))}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{            Phi[i,j] }\OperatorTok{=}\NormalTok{ basis(G[i,j])}
\NormalTok{    Phi }\OperatorTok{=}\NormalTok{ Phi }\OperatorTok{+}\NormalTok{  np.multiply(np.mat(eye(n)), eps)}
    \ControlFlowTok{return}\NormalTok{ Phi}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Phi }\OperatorTok{=}\NormalTok{ build\_Phi(G, basis}\OperatorTok{=}\NormalTok{basis)}
\NormalTok{Phi[}\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
matrix([[1.00000001, 0.07413611, 0.03828587],
        [0.07413611, 1.00000001, 0.73539165],
        [0.03828587, 0.73539165, 1.00000001]])
\end{verbatim}

\hypertarget{inverting-psi-via-cholesky-factorization}{%
\subsection{\texorpdfstring{Inverting \(\Psi\) via Cholesky
Factorization}{Inverting \textbackslash Psi via Cholesky Factorization}}\label{inverting-psi-via-cholesky-factorization}}

\begin{itemize}
\tightlist
\item
  There a two different implementations of the Cholesky factorization
  oin Python:

  \begin{itemize}
  \tightlist
  \item
    \texttt{numpy}'s \texttt{linalg.cholesky()} and
  \item
    \texttt{scipy}'s \texttt{linalg.cholesky()}
  \end{itemize}
\item
  We will use \texttt{numpy}'s version.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ get\_rbf\_weights(Phi, y):}
    \CommentTok{""" }
\CommentTok{    Calculating the weights of the radial basis function surrogate.}
\CommentTok{    Cholesky factorization used.}
\CommentTok{    LU decomposition otherwise (not implemented yet).}
\CommentTok{    """}
    \CommentTok{\# U = scipy.linalg.cholesky(Phi, lower=True)}
\NormalTok{    U }\OperatorTok{=}\NormalTok{ np.linalg.cholesky(Phi)}
\NormalTok{    U }\OperatorTok{=}\NormalTok{ U.T}
    \CommentTok{\# w = U\textbackslash{}(U\textquotesingle{}\textbackslash{}ModelInfo.y)}
\NormalTok{    w }\OperatorTok{=}\NormalTok{ np.linalg.solve(U, np.linalg.solve(U.T, y))}
    \ControlFlowTok{return}\NormalTok{ w}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w }\OperatorTok{=}\NormalTok{ get\_rbf\_weights(Phi, y)}
\NormalTok{w[}\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-6.98432952,  3.06393352, -5.13738971])
\end{verbatim}

\hypertarget{predictions}{%
\subsection{Predictions}\label{predictions}}

\hypertarget{the-predictor}{%
\subsubsection{The Predictor}\label{the-predictor}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ pred\_rbf(x, X, basis, w):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    d }\OperatorTok{=}\NormalTok{ np.zeros((n))}
\NormalTok{    phi }\OperatorTok{=}\NormalTok{ np.zeros((n))}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        d[i] }\OperatorTok{=}\NormalTok{ np.linalg.norm(x }\OperatorTok{{-}}\NormalTok{ X[i])}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        phi[i] }\OperatorTok{=}\NormalTok{ basis(d[i])}
    \ControlFlowTok{return}\NormalTok{ w }\OperatorTok{@}\NormalTok{ phi    }
\end{Highlighting}
\end{Shaded}

\hypertarget{testing-some-example-points}{%
\subsubsection{Testing some Example
Points}\label{testing-some-example-points}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ X[}\DecValTok{0}\NormalTok{]}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([ 0.76153494, -0.61391197])
\end{verbatim}

\hypertarget{the-rbf-prediction-hatf}{%
\subsubsection{\texorpdfstring{The RBF Prediction
\(\hat{f}\)}{The RBF Prediction \textbackslash hat\{f\}}}\label{the-rbf-prediction-hatf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_rbf(x}\OperatorTok{=}\NormalTok{x, X}\OperatorTok{=}\NormalTok{X, basis}\OperatorTok{=}\NormalTok{basis, w}\OperatorTok{=}\NormalTok{w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2.6615775203204706
\end{verbatim}

\hypertarget{the-original-true-value-f}{%
\subsubsection{\texorpdfstring{The Original (True) Value
\(f\)}{The Original (True) Value f}}\label{the-original-true-value-f}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\_map(np.array(x).reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([2.66157742])
\end{verbatim}

\hypertarget{visualizations}{%
\subsubsection{Visualizations}\label{visualizations}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{XX, YY }\OperatorTok{=}\NormalTok{ np.meshgrid(np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{128}\NormalTok{), np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{128}\NormalTok{))}
\NormalTok{zz }\OperatorTok{=}\NormalTok{ np.array([pred\_rbf(x}\OperatorTok{=}\NormalTok{np.array([xi, yi]), X}\OperatorTok{=}\NormalTok{X, basis}\OperatorTok{=}\NormalTok{basis,w}\OperatorTok{=}\NormalTok{w) }\ControlFlowTok{for}\NormalTok{ xi, yi }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(np.ravel(XX), np.ravel(YY))]).reshape(}\DecValTok{128}\NormalTok{,}\DecValTok{128}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{2.7}\NormalTok{), layout}\OperatorTok{=}\StringTok{\textquotesingle{}constrained\textquotesingle{}}\NormalTok{)}
\NormalTok{co }\OperatorTok{=}\NormalTok{ ax.pcolormesh(XX, YY, zz, vmin}\OperatorTok{={-}}\DecValTok{1}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{1}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}RdBu\_r\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-62-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{2.7}\NormalTok{), layout}\OperatorTok{=}\StringTok{\textquotesingle{}constrained\textquotesingle{}}\NormalTok{)}
\NormalTok{co }\OperatorTok{=}\NormalTok{ ax.contourf(XX, YY, zz, levels}\OperatorTok{=}\NormalTok{np.linspace(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{006_num_gp_files/figure-pdf/cell-63-output-1.pdf}

}

\end{figure}

\hypertarget{note}{%
\subsubsection{Note}\label{note}}

The original function \(f\) is cheaper than the surrogate \(\hat{f}\) in
this example, because we have chosen a simple analytical function as the
ground truth. This is not the case in real-world settings.

\hypertarget{cholesky-factorization}{%
\subsection{Cholesky Factorization}\label{cholesky-factorization}}

\hypertarget{a-ut-u}{%
\paragraph{\texorpdfstring{\(A = U^T U\)}{A = U\^{}T U}}\label{a-ut-u}}

\begin{itemize}
\tightlist
\item
  \(U\) is an upper triangular matrix
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ cholesky\_U(A):}
\NormalTok{    N }\OperatorTok{=}\NormalTok{ A.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    U }\OperatorTok{=}\NormalTok{ np.zeros((N,N))}
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,N):}
         \CommentTok{\# compute diagonal entry}
\NormalTok{         U[k,k] }\OperatorTok{=}\NormalTok{ A[k,k]}
         \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,k):}
\NormalTok{             U[k,k] }\OperatorTok{=}\NormalTok{ U[k,k] }\OperatorTok{{-}}\NormalTok{ U[j,k]}\OperatorTok{*}\NormalTok{U[j,k]}
\NormalTok{         U[k,k] }\OperatorTok{=}\NormalTok{ np.sqrt(U[k,k])}
         \CommentTok{\# compute remaining column}
         \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k}\OperatorTok{+}\DecValTok{1}\NormalTok{,N):}
\NormalTok{             U[k,i] }\OperatorTok{=}\NormalTok{ A[k,i]}
             \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,k):}
\NormalTok{                 U[k,i] }\OperatorTok{=}\NormalTok{ U[k,i] }\OperatorTok{{-}}\NormalTok{ U[j,i]}\OperatorTok{*}\NormalTok{U[j,k]}
\NormalTok{             U[k,i] }\OperatorTok{=}\NormalTok{ U[k,i] }\OperatorTok{/}\NormalTok{ U[k,k]}
    \ControlFlowTok{return}\NormalTok{ U}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-l-lt}{%
\subsubsection{\texorpdfstring{\(A = L L^T\)}{A = L L\^{}T}}\label{a-l-lt}}

\(L\) is a lower triangular matrix

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ cholesky\_L(A):}
\NormalTok{    N }\OperatorTok{=}\NormalTok{ A.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    L }\OperatorTok{=}\NormalTok{ np.zeros((N,N))}
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,N):}
         \CommentTok{\# compute diagonal entry}
\NormalTok{         L[k,k] }\OperatorTok{=}\NormalTok{ A[k,k]}
         \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,k):}
\NormalTok{             L[k,k] }\OperatorTok{=}\NormalTok{ L[k,k] }\OperatorTok{{-}}\NormalTok{ L[k,j]}\OperatorTok{*}\NormalTok{L[k,j]}
\NormalTok{         L[k,k] }\OperatorTok{=}\NormalTok{ np.sqrt(L[k,k])}
         \CommentTok{\# compute remaining column}
         \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k}\OperatorTok{+}\DecValTok{1}\NormalTok{,N):}
\NormalTok{             L[i,k] }\OperatorTok{=}\NormalTok{ A[i,k]}
             \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,k):}
\NormalTok{                 L[i,k] }\OperatorTok{=}\NormalTok{ L[i,k] }\OperatorTok{{-}}\NormalTok{ L[i,j]}\OperatorTok{*}\NormalTok{L[k,j]}
\NormalTok{             L[i,k] }\OperatorTok{=}\NormalTok{ L[i,k] }\OperatorTok{/}\NormalTok{ L[k,k]}
    \ControlFlowTok{return}\NormalTok{ L}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-1}{%
\subsubsection{Example}\label{example-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{6}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 4,  2,  4,  4],
       [ 2, 10,  5,  2],
       [ 4,  5,  9,  6],
       [ 4,  2,  6,  9]])
\end{verbatim}

\hypertarget{check-is-a-positive-definite}{%
\subsubsection{\texorpdfstring{Check: Is \(A\) positive
definite?}{Check: Is A positive definite?}}\label{check-is-a-positive-definite}}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{assert}\NormalTok{(np.}\BuiltInTok{all}\NormalTok{(np.linalg.eigvals(A) }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-ut-u-1}{%
\subsubsection{\texorpdfstring{\(A = U^T U\)}{A = U\^{}T U}}\label{a-ut-u-1}}

Perform Cholesky Factorization

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U }\OperatorTok{=}\NormalTok{ cholesky\_U(A)}
\NormalTok{U}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[2., 1., 2., 2.],
       [0., 3., 1., 0.],
       [0., 0., 2., 1.],
       [0., 0., 0., 2.]])
\end{verbatim}

Test Result

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U.T }\OperatorTok{@}\NormalTok{ U}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 4.,  2.,  4.,  4.],
       [ 2., 10.,  5.,  2.],
       [ 4.,  5.,  9.,  6.],
       [ 4.,  2.,  6.,  9.]])
\end{verbatim}

\hypertarget{a-l-lt-1}{%
\subsubsection{\texorpdfstring{\(A = L L^T\)}{A = L L\^{}T}}\label{a-l-lt-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L }\OperatorTok{=}\NormalTok{ cholesky\_L(A)}
\NormalTok{L}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[2., 0., 0., 0.],
       [1., 3., 0., 0.],
       [2., 1., 2., 0.],
       [2., 0., 1., 2.]])
\end{verbatim}

Test Result

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L }\OperatorTok{@}\NormalTok{ L.T}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 4.,  2.,  4.,  4.],
       [ 2., 10.,  5.,  2.],
       [ 4.,  5.,  9.,  6.],
       [ 4.,  2.,  6.,  9.]])
\end{verbatim}

\hypertarget{exercises-3}{%
\section{Exercises}\label{exercises-3}}

\hypertarget{gaussian-basis-function}{%
\subsection{Gaussian Basis Function}\label{gaussian-basis-function}}

\begin{itemize}
\tightlist
\item
  Plot the Gaussian Basis Function \texttt{basis\_gauss} in the range
  from -2 to 2 using \texttt{matplotlib.pyplot}

  \begin{itemize}
  \tightlist
  \item
    Hint: Check the
    \href{https://matplotlib.org/stable/tutorials/introductory/pyplot.html}{matplotlib
    documentation} for examples.
  \item
    Generate a plot with several \texttt{sigma} values, e.g., 0.1, 1.0,
    and 10.
  \end{itemize}
\item
  What is the meaning of the \texttt{sigma} parameter: Can you explain
  its influence / effect on the model quality?

  \begin{itemize}
  \tightlist
  \item
    Is the \texttt{sigma} value important?
  \end{itemize}
\end{itemize}

\hypertarget{linear-basis-function}{%
\subsection{Linear Basis Function}\label{linear-basis-function}}

\begin{itemize}
\tightlist
\item
  Select the linear basis function?
\item
  What errors occur?
\item
  Do you have any ideas how to fix this error?
\end{itemize}

\hypertarget{sec-spot}{%
\chapter{\texorpdfstring{Introduction to
\texttt{spotPython}}{Introduction to spotPython}}\label{sec-spot}}

Surrogate model based optimization methods are common approaches in
simulation and optimization. SPOT was developed because there is a great
need for sound statistical analysis of simulation and optimization
algorithms. SPOT includes methods for tuning based on classical
regression and analysis of variance techniques. It presents tree-based
models such as classification and regression trees and random forests as
well as Bayesian optimization (Gaussian process models, also known as
Kriging). Combinations of different meta-modeling approaches are
possible. SPOT comes with a sophisticated surrogate model based
optimization method, that can handle discrete and continuous inputs.
Furthermore, any model implemented in \texttt{scikit-learn} can be used
out-of-the-box as a surrogate in \texttt{spotPython}.

SPOT implements key techniques such as exploratory fitness landscape
analysis and sensitivity analysis. It can be used to understand the
performance of various algorithms, while simultaneously giving insights
into their algorithmic behavior.

The \texttt{spot} loop consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Init: Build initial design \(X\)
\item
  Evaluate initial design on real objective \(f\): \(y = f(X)\)
\item
  Build surrogate: \(S = S(X,y)\)
\item
  Optimize on surrogate: \(X_0 = \text{optimize}(S)\)
\item
  Evaluate on real objective: \(y_0 = f(X_0)\)
\item
  Impute (Infill) new points: \(X = X \cup X_0\), \(y = y \cup y_0\).
\item
  Got 3.
\end{enumerate}

Central Idea: Evaluation of the surrogate model \texttt{S} is much
cheaper (or / and much faster) than running the real-world experiment
\(f\). We start with a small example.

\hypertarget{example-spot-and-the-sphere-function}{%
\section{\texorpdfstring{Example: \texttt{Spot} and the Sphere
Function}{Example: Spot and the Sphere Function}}\label{example-spot-and-the-sphere-function}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-objective-function-sphere}{%
\subsection{The Objective Function:
Sphere}\label{the-objective-function-sphere}}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[f(x) = x^2\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

We can apply the function \texttt{fun} to input values and plot the
result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x, y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{007_num_spot_intro_files/figure-pdf/cell-4-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [#######---] 73.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [#########-] 86.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [#########-] 93.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.897545259852824e-10 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x105e82fd0>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 4.897545259852824e-10
x0: 2.2130398233770724e-05
\end{verbatim}

\begin{verbatim}
[['x0', 2.2130398233770724e-05]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{007_num_spot_intro_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.plot\_model()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{007_num_spot_intro_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\hypertarget{spot-parameters-fun_evals-init_size-and-show_models}{%
\section{\texorpdfstring{\texttt{Spot} Parameters: \texttt{fun\_evals},
\texttt{init\_size} and
\texttt{show\_models}}{Spot Parameters: fun\_evals, init\_size and show\_models}}\label{spot-parameters-fun_evals-init_size-and-show_models}}

We will modify three parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The number of function evaluations (\texttt{fun\_evals})
\item
  The size of the initial design (\texttt{init\_size})
\item
  The parameter \texttt{show\_models}, which visualizes the search
  process for 1-dim functions.
\end{enumerate}

The full list of the \texttt{Spot} parameters is shown in the Help
System and in the notebook \texttt{spot\_doc.ipynb}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{]),}
\NormalTok{                   fun\_evals}\OperatorTok{=} \DecValTok{10}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{9}\NormalTok{\})}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{007_num_spot_intro_files/figure-pdf/cell-10-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{007_num_spot_intro_files/figure-pdf/cell-10-output-2.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 3.648984784366253e-07 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2efb45690>
\end{verbatim}

\hypertarget{print-the-results-1}{%
\section{Print the Results}\label{print-the-results-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 3.648984784366253e-07
x0: -0.0006040682729929005
\end{verbatim}

\begin{verbatim}
[['x0', -0.0006040682729929005]]
\end{verbatim}

\hypertarget{show-the-progress-1}{%
\section{Show the Progress}\label{show-the-progress-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{007_num_spot_intro_files/figure-pdf/cell-12-output-1.pdf}

}

\end{figure}

\hypertarget{sec-visualizing-tensorboard-01}{%
\section{Visualizing the Optimization and Hyperparameter Tuning Process
with TensorBoard}\label{sec-visualizing-tensorboard-01}}

\texttt{spotPython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotPython}.

First, we define an ``experiment name'' to identify the hyperparameter
tuning process. The experiment name is used to create a directory for
the TensorBoard files.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"01"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
01_maans14_2023-11-08_10-07-26
\end{verbatim}

Since the \texttt{spot\_tensorboard\_path} is defined,
\texttt{spotPython} will log the optimization process in the TensorBoard
files. The TensorBoard files are stored in the directory
\texttt{spot\_tensorboard\_path}. We can pass the TensorBoard
information to the \texttt{Spot} method via the \texttt{fun\_control}
dictionary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{]),}
\NormalTok{                   fun\_evals}\OperatorTok{=} \DecValTok{10}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{5}\NormalTok{\},}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,)}
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.760068954719313e-05 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 7.588618329369276e-07 [#######---] 70.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 7.546340185833067e-07 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.3653559447366466e-07 [#########-] 90.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 7.948275967360275e-11 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x32dd29e50>
\end{verbatim}

Now we can start TensorBoard in the background. The TensorBoard process
will read the TensorBoard files and visualize the hyperparameter tuning
process. From the terminal, we can start TensorBoard with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\texttt{logdir} is the directory where the TensorBoard files are stored.
In our case, the TensorBoard files are stored in the directory
\texttt{./runs}.

TensorBoard will start a web server on port 6006. We can access the
TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The first TensorBoard visualization shows the objective function values
plotted against the wall time. The wall time is the time that has passed
since the start of the hyperparameter tuning process. The five initial
design points are shown in the upper left region of the plot. The line
visualizes the optimization process.
\includegraphics{figures_static/01_tensorboard_01.png}

The second TensorBoard visualization shows the input values, i.e.,
\(x_0\), plotted against the wall time.
\includegraphics{figures_static/01_tensorboard_02.png}

The third TensorBoard plot illustrates how \texttt{spotPython} can be
used as a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures_static/01_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotPython process.}

\end{figure}

\hypertarget{sec-multi-dim}{%
\chapter{Multi-dimensional Functions}\label{sec-multi-dim}}

This chapter illustrates how high-dimensional functions can be optimzed
and analyzed.

\hypertarget{example-spot-and-the-3-dim-sphere-function}{%
\section{\texorpdfstring{Example: \texttt{Spot} and the 3-dim Sphere
Function}{Example: Spot and the 3-dim Sphere Function}}\label{example-spot-and-the-3-dim-sphere-function}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-objective-function-3-dim-sphere}{%
\subsection{The Objective Function: 3-dim
Sphere}\label{the-objective-function-3-dim-sphere}}

\begin{itemize}
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula: \[f(x) = \sum_i^n x_i^2 \]
\item
  Here we will use \(n=3\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  The size of the \texttt{lower} bound vector determines the problem
  dimension.
\item
  Here we will use \texttt{-1.0\ *\ np.ones(3)}, i.e., a three-dim
  function.
\item
  We will use three different \texttt{theta} values (one for each
  dimension), i.e., we set

  \texttt{surrogate\_control=\{"n\_theta":\ 3\}}.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"02"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
02_maans14_2023-11-08_10-07-36
\end{verbatim}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{1.0}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{                   var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{"Pressure"}\NormalTok{, }\StringTok{"Temp"}\NormalTok{, }\StringTok{"Lambda"}\NormalTok{],}
\NormalTok{                   show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"n\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{\},}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,)}

\NormalTok{spot\_3.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 0.03443367156190887 [#######---] 73.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.031348911082058686 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.0009629115535977041 [#########-] 86.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 8.600065786394651e-05 [#########-] 93.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.9908343748136085e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x3206fa750>
\end{verbatim}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\hypertarget{results}{%
\subsection{Results}\label{results}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 5.9908343748136085e-05
Pressure: 0.005157864627379999
Temp: 0.00195710957248863
Lambda: 0.005429042121316765
\end{verbatim}

\begin{verbatim}
[['Pressure', 0.005157864627379999],
 ['Temp', 0.00195710957248863],
 ['Lambda', 0.005429042121316765]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{008_num_spot_multidim_files/figure-pdf/cell-7-output-1.pdf}

}

\end{figure}

\hypertarget{a-contour-plot}{%
\subsection{A Contour Plot}\label{a-contour-plot}}

\begin{itemize}
\tightlist
\item
  We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
  contour plot as follows.

  \begin{itemize}
  \tightlist
  \item
    Note: We have specified identical \texttt{min\_z} and
    \texttt{max\_z} values to generate comparable plots!
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{008_num_spot_multidim_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  In a similar manner, we can plot dimension \(i=0\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{008_num_spot_multidim_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  The final combination is \(i=1\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{1}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{008_num_spot_multidim_files/figure-pdf/cell-10-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  The three plots look very similar, because the \texttt{fun\_sphere} is
  symmetric.
\item
  This can also be seen from the variable importance:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pressure:  100.0
Temp:  99.78247670817808
Lambda:  94.72233826625329
\end{verbatim}

\begin{verbatim}
[['Pressure', 100.0],
 ['Temp', 99.78247670817808],
 ['Lambda', 94.72233826625329]]
\end{verbatim}

\hypertarget{tensorboard-3}{%
\subsection{TensorBoard}\label{tensorboard-3}}

\begin{figure}

{\centering \includegraphics{figures_static/02_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython process. Objective
function values plotted against wall time.}

\end{figure}

The second TensorBoard visualization shows the input values, i.e.,
\(x_0, \ldots, x_2\), plotted against the wall time.
\includegraphics{figures_static/02_tensorboard_02.png}

The third TensorBoard plot illustrates how \texttt{spotPython} can be
used as a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/02_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotPython surrogate model.}

\end{figure}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Based on this quick analysis, we can conclude that all three dimensions
are equally important (as expected, because the analytical function is
known).

\hypertarget{exercises-4}{%
\section{Exercises}\label{exercises-4}}

\begin{itemize}
\tightlist
\item
  Important:

  \begin{itemize}
  \tightlist
  \item
    Results from these exercises should be added to this document, i.e.,
    you should submit an updated version of this notebook.
  \item
    Please combine your results using this notebook.
  \item
    Only one notebook from each group!
  \item
    Presentation is based on this notebook. No addtional slides are
    required!
  \item
    spotPython version \texttt{0.16.11} (or greater) is required
  \end{itemize}
\end{itemize}

\hypertarget{the-three-dimensional-fun_cubed}{%
\subsection{\texorpdfstring{The Three Dimensional
\texttt{fun\_cubed}}{The Three Dimensional fun\_cubed}}\label{the-three-dimensional-fun_cubed}}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-1 \leq x \leq 1\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{the-ten-dimensional-fun_wing_wt}{%
\subsection{\texorpdfstring{The Ten Dimensional
\texttt{fun\_wing\_wt}}{The Ten Dimensional fun\_wing\_wt}}\label{the-ten-dimensional-fun_wing_wt}}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{10}. The search range is
  \(0 \leq x \leq 1\) for all dimensions.
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \item
    Generate contour plots for the three most important variables. Do
    they confirm your selection?
  \end{itemize}
\end{itemize}

\hypertarget{the-three-dimensional-fun_runge}{%
\subsection{\texorpdfstring{The Three Dimensional
\texttt{fun\_runge}}{The Three Dimensional fun\_runge}}\label{the-three-dimensional-fun_runge}}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-5 \leq x \leq 5\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{the-three-dimensional-fun_linear}{%
\subsection{\texorpdfstring{The Three Dimensional
\texttt{fun\_linear}}{The Three Dimensional fun\_linear}}\label{the-three-dimensional-fun_linear}}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-5 \leq x \leq 5\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{sec-iso-aniso-kriging}{%
\chapter{Isotropic and Anisotropic
Kriging}\label{sec-iso-aniso-kriging}}

This chapter illustrates the difference between isotropic and
anisotropic Kriging models. The difference is illustrated with the help
of the \texttt{spotPython} package. Isotropic Kriging models use the
same \texttt{theta} value for every dimension. Anisotropic Kriging
models use different \texttt{theta} values for each dimension.

\hypertarget{example-isotropic-spot-surrogate-and-the-2-dim-sphere-function}{%
\section{\texorpdfstring{Example: Isotropic \texttt{Spot} Surrogate and
the 2-dim Sphere
Function}{Example: Isotropic Spot Surrogate and the 2-dim Sphere Function}}\label{example-isotropic-spot-surrogate-and-the-2-dim-sphere-function}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-objective-function-2-dim-sphere}{%
\subsection{The Objective Function: 2-dim
Sphere}\label{the-objective-function-2-dim-sphere}}

\begin{itemize}
\tightlist
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula: \[f(x, y) = x^2 + y^2\]
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\DecValTok{0}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The size of the \texttt{lower} bound vector determines the problem
  dimension.
\item
  Here we will use \texttt{np.array({[}-1,\ -1{]})}, i.e., a two-dim
  function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]))}

\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.8750731199649933e-05 [#######---] 73.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.8750731199649933e-05 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.8750731199649933e-05 [#########-] 86.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.8750731199649933e-05 [#########-] 93.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.8750731199649933e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x177f7e1d0>
\end{verbatim}

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.8750731199649933e-05
x0: 0.0015130475553084242
x1: 0.0040572673433020325
\end{verbatim}

\begin{verbatim}
[['x0', 0.0015130475553084242], ['x1', 0.0040572673433020325]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-6-output-1.pdf}

}

\end{figure}

\hypertarget{example-with-anisotropic-kriging}{%
\section{Example With Anisotropic
Kriging}\label{example-with-anisotropic-kriging}}

\begin{itemize}
\tightlist
\item
  The default parameter setting of \texttt{spotPython}'s Kriging
  surrogate uses the same \texttt{theta} value for every dimension.
\item
  This is referred to as ``using an isotropic kernel''.
\item
  If different \texttt{theta} values are used for each dimension, then
  an anisotropic kernel is used
\item
  To enable anisotropic models in \texttt{spotPython}, the number of
  \texttt{theta} values should be larger than one.
\item
  We can use \texttt{surrogate\_control=\{"n\_theta":\ 2\}} to enable
  this behavior (\texttt{2} is the problem dimension).
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"03"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
03_maans14_2023-11-08_10-07-51
\end{verbatim}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"n\_theta"}\NormalTok{: }\DecValTok{2}\NormalTok{\},}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{spot\_2\_anisotropic.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.7904944376943484e-05 [#######---] 73.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.7904944376943484e-05 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.7904944376943484e-05 [#########-] 86.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.7904944376943484e-05 [#########-] 93.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.68261952864018e-06 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x3116318d0>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The search progress of the optimization with the anisotropic model can
  be visualized:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 5.68261952864018e-06
x0: -0.002137037687426695
x1: -0.0010562620182313395
\end{verbatim}

\begin{verbatim}
[['x0', -0.002137037687426695], ['x1', -0.0010562620182313395]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-11-output-1.pdf}

}

\end{figure}

\hypertarget{taking-a-look-at-the-theta-values}{%
\subsection{\texorpdfstring{Taking a Look at the \texttt{theta}
Values}{Taking a Look at the theta Values}}\label{taking-a-look-at-the-theta-values}}

\hypertarget{theta-values-from-the-spot-model}{%
\subsubsection{\texorpdfstring{\texttt{theta} Values from the
\texttt{spot}
Model}{theta Values from the spot Model}}\label{theta-values-from-the-spot-model}}

\begin{itemize}
\tightlist
\item
  We can check, whether one or several \texttt{theta} values were used.
\item
  The \texttt{theta} values from the surrogate can be printed as
  follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.surrogate.theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.16545225, 0.28999215])
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the surrogate from the isotropic setting was stored as
  \texttt{spot\_2}, we can also take a look at the \texttt{theta} value
  from this model:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.26287446])
\end{verbatim}

\hypertarget{tensorboard-5}{%
\subsubsection{TensorBoard}\label{tensorboard-5}}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/03_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotPython surrogate model.}

\end{figure}

\hypertarget{exercises-5}{%
\section{Exercises}\label{exercises-5}}

\hypertarget{fun_branin}{%
\subsection{\texorpdfstring{\texttt{fun\_branin}}{fun\_branin}}\label{fun_branin}}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 10\) and \(0 \leq x_2 \leq 15\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion: instead of the number of evaluations
  (which is specified via \texttt{fun\_evals}), the time should be used
  as the termination criterion. This can be done as follows
  (\texttt{max\_time=1} specifies a run time of one minute):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_sin_cos}{%
\subsection{\texorpdfstring{\texttt{fun\_sin\_cos}}{fun\_sin\_cos}}\label{fun_sin_cos}}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-2\pi \leq x_1 \leq 2\pi\) and \(-2\pi \leq x_2 \leq 2\pi\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\hypertarget{fun_runge}{%
\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge}}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 5\) and \(-5 \leq x_2 \leq 5\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\hypertarget{fun_wingwt}{%
\subsection{\texorpdfstring{\texttt{fun\_wingwt}}{fun\_wingwt}}\label{fun_wingwt}}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{10}. The search ranges are between 0
    and 1 (values are mapped internally to their natural bounds).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\hypertarget{sec-sklearn-surrogates}{%
\chapter{\texorpdfstring{Using \texttt{sklearn} Surrogates in
\texttt{spotPython}}{Using sklearn Surrogates in spotPython}}\label{sec-sklearn-surrogates}}

Besides the internal kriging surrogate, which is used as a default py
\texttt{spotPython}, any surrogate model from \texttt{scikit-learn} can
be used as a surrogate in \texttt{spotPython}. This chapter explains how
to use \texttt{scikit-learn} surrogates in \texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-branin-function-with-spotpythons-internal-kriging-surrogate}{%
\section{\texorpdfstring{Example: Branin Function with
\texttt{spotPython}'s Internal Kriging
Surrogate}{Example: Branin Function with spotPython's Internal Kriging Surrogate}}\label{example-branin-function-with-spotpythons-internal-kriging-surrogate}}

\hypertarget{the-objective-function-branin-1}{%
\subsection{The Objective Function
Branin}\label{the-objective-function-branin-1}}

\begin{itemize}
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula.
\item
  Here we will use the Branin function:

\begin{verbatim}
  y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,
  where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2),
  c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).
\end{verbatim}
\item
  It has three global minima:

\begin{verbatim}
  f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).
\end{verbatim}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"04"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
04_maans14_2023-11-08_10-08-12
\end{verbatim}

\end{tcolorbox}

\hypertarget{running-the-surrogate-model-based-optimizer-spot}{%
\subsection{\texorpdfstring{Running the surrogate model based optimizer
\texttt{Spot}:}{Running the surrogate model based optimizer Spot:}}\label{running-the-surrogate-model-based-optimizer-spot}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{10}\NormalTok{\},}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.447460568213552 [######----] 55.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.447460568213552 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.0394923470341615 [######----] 65.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.0394923470341615 [#######---] 70.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.1632551812894665 [########--] 75.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.612453922191154 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.4576355201245761 [########--] 85.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.3983178342401956 [#########-] 90.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.3983178342401956 [##########] 95.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.3983178342401956 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x31aff9b90>
\end{verbatim}

\hypertarget{tensorboard-7}{%
\subsection{TensorBoard}\label{tensorboard-7}}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/04_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}

\hypertarget{print-the-results-2}{%
\subsection{Print the Results}\label{print-the-results-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.3983178342401956
x0: 3.135416996435963
x1: 2.2955490975636685
\end{verbatim}

\begin{verbatim}
[['x0', 3.135416996435963], ['x1', 2.2955490975636685]]
\end{verbatim}

\hypertarget{show-the-progress-and-the-surrogate}{%
\subsection{Show the Progress and the
Surrogate}\label{show-the-progress-and-the-surrogate}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-10-output-1.pdf}

}

\end{figure}

\hypertarget{example-using-surrogates-from-scikit-learn}{%
\section{Example: Using Surrogates From
scikit-learn}\label{example-using-surrogates-from-scikit-learn}}

\begin{itemize}
\tightlist
\item
  Default is the \texttt{spotPython} (i.e., the internal)
  \texttt{kriging} surrogate.
\item
  It can be called explicitely and passed to \texttt{Spot}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\NormalTok{S\_0 }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Alternatively, models from \texttt{scikit-learn} can be selected,
  e.g., Gaussian Process, RBFs, Regression Trees, etc.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Here are some additional models that might be useful later:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_Tree }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{S\_LM }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_Ridge }\OperatorTok{=}\NormalTok{ linear\_model.Ridge()}
\NormalTok{S\_RF }\OperatorTok{=}\NormalTok{ RandomForestRegressor(max\_depth}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{gaussianprocessregressor-as-a-surrogate}{%
\subsection{GaussianProcessRegressor as a
Surrogate}\label{gaussianprocessregressor-as-a-surrogate}}

\begin{itemize}
\tightlist
\item
  To use a Gaussian Process model from \texttt{sklearn}, that is similar
  to \texttt{spotPython}'s \texttt{Kriging}, we can proceed as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  The scikit-learn GP model \texttt{S\_GP} is selected for \texttt{Spot}
  as follows:

  \texttt{surrogate\ =\ S\_GP}
\item
  We can check the kind of surogate model with the command
  \texttt{isinstance}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S\_GP, GaussianProcessRegressor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S\_0, Kriging)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Similar to the \texttt{Spot} run with the internal \texttt{Kriging}
  model, we can call the run with the \texttt{scikit-learn} surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\NormalTok{spot\_2\_GP }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{10}\NormalTok{\},}
\NormalTok{                   surrogate }\OperatorTok{=}\NormalTok{ S\_GP)}
\NormalTok{spot\_2\_GP.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 18.865102092040814 [######----] 55.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.067063943633956 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.461921636551292 [######----] 65.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.461921636551292 [#######---] 70.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.3283243814960493 [########--] 75.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.9549503108510411 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.9352250003678275 [########--] 85.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.39960822331589974 [#########-] 90.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.3981631812933486 [##########] 95.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.3981631812933486 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x321acac90>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_GP.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-18-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.3981631812933486
x0: 3.1491652274330053
x1: 2.2698186003445153
\end{verbatim}

\begin{verbatim}
[['x0', 3.1491652274330053], ['x1', 2.2698186003445153]]
\end{verbatim}

\hypertarget{example-one-dimensional-sphere-function-with-spotpythons-kriging}{%
\section{\texorpdfstring{Example: One-dimensional Sphere Function With
\texttt{spotPython}'s
Kriging}{Example: One-dimensional Sphere Function With spotPython's Kriging}}\label{example-one-dimensional-sphere-function-with-spotpythons-kriging}}

\begin{itemize}
\tightlist
\item
  In this example, we will use an one-dimensional function, which allows
  us to visualize the optimization process.

  \begin{itemize}
  \tightlist
  \item
    \texttt{show\_models=\ True} is added to the argument list.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{10}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{3}\NormalTok{\},)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-2.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.03475493366922229 [####------] 40.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-4.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.03475493366922229 [#####-----] 50.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-6.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.014642358641673271 [######----] 60.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-8.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.00018032497380230452 [#######---] 70.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-10.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 2.1786524623022742e-08 [########--] 80.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-12.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 2.1786524623022742e-08 [#########-] 90.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-21-output-14.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 2.1786524623022742e-08 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x31c8e4510>
\end{verbatim}

\hypertarget{results-2}{%
\subsection{Results}\label{results-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.1786524623022742e-08
x0: -0.00014760259016366462
\end{verbatim}

\begin{verbatim}
[['x0', -0.00014760259016366462]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-23-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  The method \texttt{plot\_model} plots the final surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_model()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-1.pdf}

}

\end{figure}

\hypertarget{example-sklearn-model-gaussianprocess}{%
\section{\texorpdfstring{Example: \texttt{Sklearn} Model
GaussianProcess}{Example: Sklearn Model GaussianProcess}}\label{example-sklearn-model-gaussianprocess}}

\begin{itemize}
\tightlist
\item
  This example visualizes the search process on the
  \texttt{GaussianProcessRegression} surrogate from \texttt{sklearn}.
\item
  Therefore \texttt{surrogate\ =\ S\_GP} is added to the argument list.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{spot\_1\_GP }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{10}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{3}\NormalTok{\},}
\NormalTok{                   surrogate }\OperatorTok{=}\NormalTok{ S\_GP)}
\NormalTok{spot\_1\_GP.run()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-2.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.0049257617153734565 [####------] 40.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-4.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.002612076484523571 [#####-----] 50.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-6.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 5.912935436232656e-07 [######----] 60.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-8.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 4.211659449449057e-08 [#######---] 70.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-10.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 4.211659449449057e-08 [########--] 80.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-12.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 1.6119389142446866e-09 [#########-] 90.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-25-output-14.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 1.6119389142446866e-09 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x321fcc390>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.6119389142446866e-09
x0: 4.0148959068009304e-05
\end{verbatim}

\begin{verbatim}
[['x0', 4.0148959068009304e-05]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-27-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.plot\_model()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-28-output-1.pdf}

}

\end{figure}

\hypertarget{exercises-6}{%
\section{Exercises}\label{exercises-6}}

\hypertarget{decisiontreeregressor}{%
\subsection{\texorpdfstring{\texttt{DecisionTreeRegressor}}{DecisionTreeRegressor}}\label{decisiontreeregressor}}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\hypertarget{randomforestregressor}{%
\subsection{\texorpdfstring{\texttt{RandomForestRegressor}}{RandomForestRegressor}}\label{randomforestregressor}}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\hypertarget{linear_model.linearregression}{%
\subsection{\texorpdfstring{\texttt{linear\_model.LinearRegression}}{linear\_model.LinearRegression}}\label{linear_model.linearregression}}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\hypertarget{linear_model.ridge}{%
\subsection{\texorpdfstring{\texttt{linear\_model.Ridge}}{linear\_model.Ridge}}\label{linear_model.ridge}}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\hypertarget{exercise-2}{%
\section{Exercise 2}\label{exercise-2}}

\begin{itemize}
\item
  Compare the performance of the five different surrogates on both
  objective functions:

  \begin{itemize}
  \tightlist
  \item
    \texttt{spotPython}'s internal Kriging
  \item
    \texttt{DecisionTreeRegressor}
  \item
    \texttt{RandomForestRegressor}
  \item
    \texttt{linear\_model.LinearRegression}
  \item
    \texttt{linear\_model.Ridge}
  \end{itemize}
\end{itemize}

\hypertarget{sec-gaussian-process-models}{%
\chapter{Sequential Parameter Optimization: Gaussian Process
Models}\label{sec-gaussian-process-models}}

This chapter analyzes differences between the \texttt{Kriging}
implementation in \texttt{spotPython} and the
\texttt{GaussianProcessRegressor} in \texttt{scikit-learn}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ m}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\end{Highlighting}
\end{Shaded}

\hypertarget{gaussian-processes-regression-basic-introductory-scikit-learn-example}{%
\section{\texorpdfstring{Gaussian Processes Regression: Basic
Introductory \texttt{scikit-learn}
Example}{Gaussian Processes Regression: Basic Introductory scikit-learn Example}}\label{gaussian-processes-regression-basic-introductory-scikit-learn-example}}

\begin{itemize}
\item
  This is the example from
  \href{https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html}{scikit-learn:
  https://scikit-learn.org/stable/auto\_examples/gaussian\_process/plot\_gpr\_noisy\_targets.html}
\item
  After fitting our model, we see that the hyperparameters of the kernel
  have been optimized.
\item
  Now, we will use our kernel to compute the mean prediction of the full
  dataset and plot the 95\% confidence interval.
\end{itemize}

\hypertarget{train-and-test-data}{%
\subsection{Train and Test Data}\label{train-and-test-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}
\end{Highlighting}
\end{Shaded}

\hypertarget{building-the-surrogate-with-sklearn}{%
\subsection{\texorpdfstring{Building the Surrogate With
\texttt{Sklearn}}{Building the Surrogate With Sklearn}}\label{building-the-surrogate-with-sklearn}}

\begin{itemize}
\tightlist
\item
  The model building with \texttt{sklearn} consisits of three steps:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Instantiating the model, then
  \item
    fitting the model (using \texttt{fit}), and
  \item
    making predictions (using \texttt{predict})
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{gaussian\_process }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{gaussian\_process.fit(X\_train, y\_train)}
\NormalTok{mean\_prediction, std\_prediction }\OperatorTok{=}\NormalTok{ gaussian\_process.predict(X, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plotting-the-sklearnmodel}{%
\subsection{\texorpdfstring{Plotting the
\texttt{Sklearn}Model}{Plotting the SklearnModel}}\label{plotting-the-sklearnmodel}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"sk{-}learn Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-5-output-1.pdf}

}

\end{figure}

\hypertarget{the-spotpython-version}{%
\subsection{\texorpdfstring{The \texttt{spotPython}
Version}{The spotPython Version}}\label{the-spotpython-version}}

\begin{itemize}
\tightlist
\item
  The \texttt{spotPython} version is very similar:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Instantiating the model, then
  \item
    fitting the model and
  \item
    making predictions (using \texttt{predict}).
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}
\NormalTok{S\_mean\_prediction, S\_std\_prediction, S\_ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, S\_mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    S\_mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ S\_std\_prediction,}
\NormalTok{    S\_mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ S\_std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"spotPython Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-7-output-1.pdf}

}

\end{figure}

\hypertarget{visualizing-the-differences-between-the-spotpython-and-the-sklearn-model-fits}{%
\subsection{\texorpdfstring{Visualizing the Differences Between the
\texttt{spotPython} and the \texttt{sklearn} Model
Fits}{Visualizing the Differences Between the spotPython and the sklearn Model Fits}}\label{visualizing-the-differences-between-the-spotpython-and-the-sklearn-model-fits}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, S\_mean\_prediction, label}\OperatorTok{=}\StringTok{"spotPython Mean prediction"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Sklearn Mean Prediction"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Comparing Mean Predictions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\hypertarget{exercises-7}{%
\section{Exercises}\label{exercises-7}}

\hypertarget{schonlau-example-function}{%
\subsection{\texorpdfstring{\texttt{Schonlau\ Example\ Function}}{Schonlau Example Function}}\label{schonlau-example-function}}

\begin{itemize}
\tightlist
\item
  The Schonlau Example Function is based on sample points only (there is
  no analytical function description available):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{, }\FloatTok{2.}\NormalTok{, }\FloatTok{3.}\NormalTok{, }\FloatTok{4.}\NormalTok{, }\FloatTok{12.}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.75}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Since there is no analytical function available, you might be
  interested in adding some points and describe the effects.
\end{itemize}

\hypertarget{forrester-example-function}{%
\subsection{\texorpdfstring{\texttt{Forrester\ Example\ Function}}{Forrester Example Function}}\label{forrester-example-function}}

\begin{itemize}
\item
  The Forrester Example Function is defined as follows:

  \texttt{f(x)\ =\ (6x-\ 2)\^{}2\ sin(12x-4)\ for\ x\ in\ {[}0,1{]}.}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\FloatTok{0.5}\NormalTok{, stop}\OperatorTok{=}\FloatTok{1.5}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.175}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.375}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\FloatTok{0.1}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X\_train, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.2}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\FloatTok{0.2}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_runge-function-1-dim}{%
\subsection{\texorpdfstring{\texttt{fun\_runge\ Function\ (1-dim)}}{fun\_runge Function (1-dim)}}\label{fun_runge-function-1-dim}}

\begin{itemize}
\item
  The Runge function is defined as follows:

  \texttt{f(x)\ =\ 1/\ (1\ +\ sum(x\_i))\^{}2}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\FloatTok{0.025}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.05}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\FloatTok{0.5}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_cubed-1-dim}{%
\subsection{\texorpdfstring{\texttt{fun\_cubed\ (1-dim)}}{fun\_cubed (1-dim)}}\label{fun_cubed-1-dim}}

\begin{itemize}
\item
  The Cubed function is defined as follows:

  \texttt{np.sum(X{[}i{]}**\ 3)}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\FloatTok{0.025}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.05}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\FloatTok{0.05}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-effect-of-noise}{%
\subsection{The Effect of Noise}\label{the-effect-of-noise}}

How does the behavior of the \texttt{spotPython} fit changes when the
argument \texttt{noise} is set to \texttt{True}, i.e.,

\texttt{S\ =\ Kriging(name=\textquotesingle{}kriging\textquotesingle{},\ \ seed=123,\ n\_theta=1,\ noise=True)}

is used?

\hypertarget{sec-expected-improvement}{%
\chapter{Expected Improvement}\label{sec-expected-improvement}}

This chapter describes, analyzes, and compares different infill
criterion. An infill criterion defines how the next point \(x_{n+1}\) is
selected from the surrogate model \(S\). Expected improvement is a
popular infill criterion in Bayesian optimization.

\hypertarget{example-spot-and-the-1-dim-sphere-function}{%
\section{\texorpdfstring{Example: \texttt{Spot} and the 1-dim Sphere
Function}{Example: Spot and the 1-dim Sphere Function}}\label{example-spot-and-the-1-dim-sphere-function}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-objective-function-1-dim-sphere}{%
\subsection{The Objective Function: 1-dim
Sphere}\label{the-objective-function-1-dim-sphere}}

\begin{itemize}
\tightlist
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula: \[f(x) = x^2 \]
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The size of the \texttt{lower} bound vector determines the problem
  dimension.
\item
  Here we will use \texttt{np.array({[}-1{]})}, i.e., a one-dim
  function.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_Y"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
07_Y_maans14_2023-11-08_10-08-58
\end{verbatim}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{10}\NormalTok{\},}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,)}

\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [####------] 44.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [#####-----] 48.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [#####-----] 52.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.2459257396367542e-08 [######----] 56.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.897545259852824e-10 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.897545259852824e-10 [######----] 64.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.897545259852824e-10 [#######---] 68.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.897545259852824e-10 [#######---] 72.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.897545259852824e-10 [########--] 76.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.897545259852824e-10 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.9335518024989866e-10 [########--] 84.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.9335518024989866e-10 [#########-] 88.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.9335518024989866e-10 [#########-] 92.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.9335518024989866e-10 [##########] 96.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.135607331180881e-12 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x3173c7fd0>
\end{verbatim}

\hypertarget{results-3}{%
\subsection{Results}\label{results-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.135607331180881e-12
x0: -1.4613717292943917e-06
\end{verbatim}

\begin{verbatim}
[['x0', -1.4613717292943917e-06]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/07_tensorboard_Y.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}

\hypertarget{same-but-with-ei-as-infill_criterion}{%
\section{Same, but with EI as
infill\_criterion}\label{same-but-with-ei-as-infill_criterion}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_EI\_ISO"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
07_EI_ISO_maans14_2023-11-08_10-09-00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"ei"}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{10}\NormalTok{\},}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,)}
\NormalTok{spot\_1\_ei.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 8.79000773789907e-08 [####------] 44.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.6197300077861015e-08 [#####-----] 48.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.6197300077861015e-08 [#####-----] 52.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.6197300077861015e-08 [######----] 56.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [######----] 64.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [#######---] 68.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [#######---] 72.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [########--] 76.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [########--] 84.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [#########-] 88.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [#########-] 92.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [##########] 96.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1963022660037201e-10 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x107a4ffd0>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-11-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.1963022660037201e-10
x0: 1.4819926673245452e-05
\end{verbatim}

\begin{verbatim}
[['x0', 1.4819926673245452e-05]]
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/07_tensorboard_EI_ISO.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model. Expected improvement, isotropic
Kriging.}

\end{figure}

\hypertarget{non-isotropic-kriging}{%
\section{Non-isotropic Kriging}\label{non-isotropic-kriging}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_EI\_NONISO"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
07_EI_NONISO_maans14_2023-11-08_10-09-02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"ei"}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{10}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\DecValTok{2}\NormalTok{,}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{1000}\NormalTok{,}
\NormalTok{                                      \},}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,)}
\NormalTok{spot\_2\_ei\_noniso.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.8247169797759505e-05 [####------] 44.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.8247169797759505e-05 [#####-----] 48.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.8247169797759505e-05 [#####-----] 52.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.0281222147432436e-05 [######----] 56.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.0281222147432436e-05 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [######----] 64.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [#######---] 68.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [#######---] 72.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [########--] 76.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [########--] 84.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [#########-] 88.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [#########-] 92.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [##########] 96.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.088759927339735e-07 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x317c43d10>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-15-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.088759927339735e-07
x0: -0.0002833471276146305
x1: 0.00016908695398081962
\end{verbatim}

\begin{verbatim}
[['x0', -0.0002833471276146305], ['x1', 0.00016908695398081962]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-17-output-1.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/07_tensorboard_EI_NONISO.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model. Expected improvement, isotropic
Kriging.}

\end{figure}

\hypertarget{using-sklearn-surrogates}{%
\section{\texorpdfstring{Using \texttt{sklearn}
Surrogates}{Using sklearn Surrogates}}\label{using-sklearn-surrogates}}

\hypertarget{the-spot-loop}{%
\subsection{The spot Loop}\label{the-spot-loop}}

The \texttt{spot} loop consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Init: Build initial design \(X\)
\item
  Evaluate initial design on real objective \(f\): \(y = f(X)\)
\item
  Build surrogate: \(S = S(X,y)\)
\item
  Optimize on surrogate: \(X_0 = \text{optimize}(S)\)
\item
  Evaluate on real objective: \(y_0 = f(X_0)\)
\item
  Impute (Infill) new points: \(X = X \cup X_0\), \(y = y \cup y_0\).
\item
  Got 3.
\end{enumerate}

The \texttt{spot} loop is implemented in \texttt{R} as follows:

\begin{figure}

{\centering \includegraphics{figures_static/spotModel.png}

}

\caption{Visual representation of the model based search with SPOT.
Taken from: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter
tuning approaches. In Hyperparameter Tuning for Machine and Deep
Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M.
Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch.~4, pp.~67--114.}

\end{figure}

\hypertarget{spot-the-initial-model}{%
\subsection{spot: The Initial Model}\label{spot-the-initial-model}}

\hypertarget{example-modifying-the-initial-design-size}{%
\subsubsection{Example: Modifying the initial design
size}\label{example-modifying-the-initial-design-size}}

This is the ``Example: Modifying the initial design size'' from Chapter
4.5.1 in {[}bart21i{]}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_ei }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{               lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{               upper}\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]),}
\NormalTok{               design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{5}\NormalTok{\})}
\NormalTok{spot\_ei.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 0.13881986540743513 [####------] 40.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.011157100173301121 [#####-----] 46.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.0010077722891862157 [#####-----] 53.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.0006326308401677749 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.0005880000745278913 [#######---] 66.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.0005853974252148365 [#######---] 73.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.0005615353015376504 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.0004470375728318479 [#########-] 86.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 6.506371306758665e-05 [#########-] 93.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.881581967484049e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x3174d1910>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_ei.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-19-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.}\BuiltInTok{min}\NormalTok{(spot\_1.y), np.}\BuiltInTok{min}\NormalTok{(spot\_ei.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(2.135607331180881e-12, 1.881581967484049e-05)
\end{verbatim}

\hypertarget{init-build-initial-design}{%
\subsection{Init: Build Initial
Design}\label{init-build-initial-design}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 8.97647221 13.41926847]
 [ 0.66946019  1.22344228]
 [ 5.23614115 13.78185824]
 [ 5.6149825  11.5851384 ]
 [-1.72963184  1.66516096]
 [-4.26945568  7.1325531 ]
 [ 1.26363761 10.17935555]
 [ 2.88779942  8.05508969]
 [-3.39111089  4.15213772]
 [ 7.30131231  5.22275244]]
[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975
  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{S.fit(X, y)}
\NormalTok{S.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-22-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{345}\NormalTok{)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X0, X1, X2, X3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]),
 array([[0.78373509, 0.86811887],
        [0.06692621, 0.6058029 ],
        [0.41374778, 0.00525456]]),
 array([[0.121357  , 0.69043832],
        [0.41906219, 0.32838498],
        [0.86742658, 0.52910374]]),
 array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]))
\end{verbatim}

\hypertarget{evaluate}{%
\subsection{Evaluate}\label{evaluate}}

\hypertarget{build-surrogate}{%
\subsection{Build Surrogate}\label{build-surrogate}}

\hypertarget{a-simple-predictor}{%
\subsection{A Simple Predictor}\label{a-simple-predictor}}

The code below shows how to use a simple model for prediction.

\begin{itemize}
\item
  Assume that only two (very costly) measurements are available:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    f(0) = 0.5
  \item
    f(2) = 2.5
  \end{enumerate}
\item
  We are interested in the value at \(x_0 = 1\), i.e., \(f(x_0 = 1)\),
  but cannot run an additional, third experiment.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{], [}\DecValTok{2}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{])}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ S\_lm.fit(X, y)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{]])}
\NormalTok{y0 }\OperatorTok{=}\NormalTok{ S\_lm.predict(X0)}
\BuiltInTok{print}\NormalTok{(y0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.5]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Central Idea:

  \begin{itemize}
  \tightlist
  \item
    Evaluation of the surrogate model \texttt{S\_lm} is much cheaper (or
    / and much faster) than running the real-world experiment \(f\).
  \end{itemize}
\end{itemize}

\hypertarget{gaussian-processes-regression-basic-introductory-example}{%
\section{Gaussian Processes regression: basic introductory
example}\label{gaussian-processes-regression-basic-introductory-example}}

This example was taken from
\href{https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html}{scikit-learn}.
After fitting our model, we see that the hyperparameters of the kernel
have been optimized. Now, we will use our kernel to compute the mean
prediction of the full dataset and plot the 95\% confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ m}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}

\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{gaussian\_process }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{gaussian\_process.fit(X\_train, y\_train)}
\NormalTok{gaussian\_process.kernel\_}

\NormalTok{mean\_prediction, std\_prediction }\OperatorTok{=}\NormalTok{ gaussian\_process.predict(X, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"sk{-}learn Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-25-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}


\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{std\_prediction}

\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"spotPython Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-26-output-1.pdf}

}

\end{figure}

\hypertarget{the-surrogate-using-scikit-learn-models}{%
\section{The Surrogate: Using scikit-learn
models}\label{the-surrogate-using-scikit-learn-models}}

Default is the internal \texttt{kriging} surrogate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_0 }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Models from \texttt{scikit-learn} can be selected, e.g., Gaussian
Process:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  and many more:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_Tree }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{S\_LM }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_Ridge }\OperatorTok{=}\NormalTok{ linear\_model.Ridge()}
\NormalTok{S\_RF }\OperatorTok{=}\NormalTok{ RandomForestRegressor(max\_depth}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The scikit-learn GP model \texttt{S\_GP} is selected.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ S\_GP}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S, GaussianProcessRegressor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{5}\NormalTok{\}}
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{\{}
            \StringTok{"infill\_criterion"}\NormalTok{: }\VariableTok{None}\NormalTok{,}
            \StringTok{"n\_points"}\NormalTok{: }\DecValTok{1}\NormalTok{,}
\NormalTok{        \}}
\NormalTok{spot\_GP }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun, lower }\OperatorTok{=}\NormalTok{ lower, upper}\OperatorTok{=}\NormalTok{ upper, surrogate}\OperatorTok{=}\NormalTok{S, }
\NormalTok{                    fun\_evals }\OperatorTok{=} \DecValTok{15}\NormalTok{, noise }\OperatorTok{=} \VariableTok{False}\NormalTok{, log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}

\NormalTok{spot\_GP.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 24.51465459019188 [####------] 40.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 11.003078163486554 [#####-----] 46.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 10.960665185123245 [#####-----] 53.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 10.960665185123245 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 10.960665185123245 [#######---] 66.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 4.0894841491438765 [#######---] 73.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.4230377508791392 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.4230377508791392 [#########-] 86.67% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.4230377508791392 [#########-] 93.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 0.6989341031319167 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x32d73dc90>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([ 69.32459936, 152.38491454, 107.92560483,  24.51465459,
        76.73500031,  86.304256  ,  11.00307816,  10.96066519,
        16.06668258,  24.08432082,   4.08948415,   1.42303775,
         1.47359526,  16.04703294,   0.6989341 ])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-35-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.6989341031319167
x0: 3.358292789592623
x1: 2.3886120108545597
\end{verbatim}

\begin{verbatim}
[['x0', 3.358292789592623], ['x1', 2.3886120108545597]]
\end{verbatim}

\hypertarget{additional-examples}{%
\section{Additional Examples}\label{additional-examples}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}

\NormalTok{S\_K }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{              seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{              log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{              infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{              n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{              noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{              cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}

\NormalTok{design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{10}\NormalTok{\}}
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{\{}
            \StringTok{"n\_points"}\NormalTok{: }\DecValTok{1}\NormalTok{,}
\NormalTok{        \}}
\NormalTok{spot\_S\_K }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                     upper}\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                     surrogate}\OperatorTok{=}\NormalTok{S\_K,}
\NormalTok{                     fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{                     noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                     log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                     surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}

\NormalTok{spot\_S\_K.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.1370719642847402e-05 [####------] 44.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1370719642847402e-05 [#####-----] 48.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1370719642847402e-05 [#####-----] 52.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1370719642847402e-05 [######----] 56.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.1370719642847402e-05 [######----] 60.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.2590483826517302e-05 [######----] 64.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.930538349742746e-06 [#######---] 68.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 3.3191760809461184e-06 [#######---] 72.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.4684282727935e-06 [########--] 76.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.8279736801432919e-06 [########--] 80.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.809224307539433e-06 [########--] 84.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.809224307539433e-06 [#########-] 88.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.809224307539433e-06 [#########-] 92.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.809224307539433e-06 [##########] 96.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.809224307539433e-06 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x32ec79d90>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-40-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-41-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.809224307539433e-06
x0: -0.001330101474082372
x1: 0.0002001358942901893
\end{verbatim}

\begin{verbatim}
[['x0', -0.001330101474082372], ['x1', 0.0002001358942901893]]
\end{verbatim}

\hypertarget{optimize-on-surrogate}{%
\subsection{Optimize on Surrogate}\label{optimize-on-surrogate}}

\hypertarget{evaluate-on-real-objective}{%
\subsection{Evaluate on Real
Objective}\label{evaluate-on-real-objective}}

\hypertarget{impute-infill-new-points}{%
\subsection{Impute / Infill new Points}\label{impute-infill-new-points}}

\hypertarget{tests}{%
\section{Tests}\label{tests}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}

\NormalTok{fun\_sphere }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{fun\_sphere,}
\NormalTok{    lower}\OperatorTok{=}\NormalTok{np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{    n\_points }\OperatorTok{=} \DecValTok{2}
\NormalTok{)}

\CommentTok{\# (S{-}2) Initial Design:}
\NormalTok{spot\_1.X }\OperatorTok{=}\NormalTok{ spot\_1.design.scipy\_lhd(}
\NormalTok{    spot\_1.design\_control[}\StringTok{"init\_size"}\NormalTok{], lower}\OperatorTok{=}\NormalTok{spot\_1.lower, upper}\OperatorTok{=}\NormalTok{spot\_1.upper}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(spot\_1.X)}

\CommentTok{\# (S{-}3): Eval initial design:}
\NormalTok{spot\_1.y }\OperatorTok{=}\NormalTok{ spot\_1.fun(spot\_1.X)}
\BuiltInTok{print}\NormalTok{(spot\_1.y)}

\NormalTok{spot\_1.surrogate.fit(spot\_1.X, spot\_1.y)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ spot\_1.suggest\_new\_X()}
\BuiltInTok{print}\NormalTok{(X0)}
\ControlFlowTok{assert}\NormalTok{ X0.size }\OperatorTok{==}\NormalTok{ spot\_1.n\_points }\OperatorTok{*}\NormalTok{ spot\_1.k}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.86352963  0.7892358 ]
 [-0.24407197 -0.83687436]
 [ 0.36481882  0.8375811 ]
 [ 0.415331    0.54468512]
 [-0.56395091 -0.77797854]
 [-0.90259409 -0.04899292]
 [-0.16484832  0.35724741]
 [ 0.05170659  0.07401196]
 [-0.78548145 -0.44638164]
 [ 0.64017497 -0.30363301]]
[1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764
 0.15480068 0.00815134 0.81623768 0.502017  ]
[[0.00151305 0.00405727]
 [0.00151305 0.00405727]]
\end{verbatim}

\hypertarget{ei-the-famous-schonlau-example}{%
\section{EI: The Famous Schonlau
Example}\label{ei-the-famous-schonlau-example}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_train0 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{12}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{5}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{, }\FloatTok{2.}\NormalTok{, }\FloatTok{3.}\NormalTok{, }\FloatTok{4.}\NormalTok{, }\FloatTok{12.}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.75}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.}\NormalTok{])}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\ControlFlowTok{if} \VariableTok{True}\NormalTok{:}
\NormalTok{    plt.fill\_between(}
\NormalTok{        X.ravel(),}
\NormalTok{        mean\_prediction }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        mean\_prediction }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-45-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\CommentTok{\# plt.scatter(X\_train, y\_train, label="Observations")}
\NormalTok{plt.plot(X, }\OperatorTok{{-}}\NormalTok{ei, label}\OperatorTok{=}\StringTok{"Expected Improvement"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-46-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'negLnLike': array([1.20788205]),
 'theta': array([1.09275997]),
 'p': [],
 'Lambda': []}
\end{verbatim}

\hypertarget{ei-the-forrester-example}{%
\section{EI: The Forrester Example}\label{ei-the-forrester-example}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}

\CommentTok{\# exact x locations are unknown:}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.175}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.375}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{1.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X\_train, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{1}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\ControlFlowTok{if} \VariableTok{True}\NormalTok{:}
\NormalTok{    plt.fill\_between(}
\NormalTok{        X.ravel(),}
\NormalTok{        mean\_prediction }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        mean\_prediction }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-48-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\CommentTok{\# plt.scatter(X\_train, y\_train, label="Observations")}
\NormalTok{plt.plot(X, }\OperatorTok{{-}}\NormalTok{ei, label}\OperatorTok{=}\StringTok{"Expected Improvement"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-49-output-1.pdf}

}

\end{figure}

\hypertarget{noise}{%
\section{Noise}\label{noise}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[-1.57464135 16.13714981  2.77008442 93.14904827 71.59322218 14.28895359
 15.9770567  12.96468767 39.82265329 59.88028242]
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-50-output-2.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'negLnLike': array([25.26601608]),
 'theta': array([-1.98024606]),
 'p': [],
 'Lambda': []}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-52-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'negLnLike': array([21.82530943]),
 'theta': array([-0.41935831]),
 'p': [],
 'Lambda': array([5.20850895e-05])}
\end{verbatim}

\hypertarget{cubic-function}{%
\section{Cubic Function}\label{cubic-function}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{10.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Cubed: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[  -9.63480707  -72.98497325   12.7936499   895.34567477 -573.35961837
  -41.83176425   65.27989461   46.37081417  254.1530734  -474.09587355]
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-54-output-2.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{0}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Cubed: Gaussian process with nugget regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-55-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[0.712453   0.05595118 0.83735691 0.0106654  0.01413372 0.07074765
 0.05479457 0.07763503 0.02412205 0.01625354]
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-56-output-2.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{012_num_spot_ei_files/figure-pdf/cell-57-output-1.pdf}

}

\end{figure}

\hypertarget{factors}{%
\section{Factors}\label{factors}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\StringTok{"num"}\NormalTok{] }\OperatorTok{*} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['num', 'num', 'num']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{n }\OperatorTok{=} \DecValTok{30}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin\_factor}
\CommentTok{\#fun = analytical(sigma=0).fun\_sphere}

\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(n, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ np.random.randint(low}\OperatorTok{=}\DecValTok{1}\NormalTok{, high}\OperatorTok{=}\DecValTok{3}\NormalTok{, size}\OperatorTok{=}\NormalTok{(n,))}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.c\_[X0, X1]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X)}
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{])}
\NormalTok{S.fit(X, y)}
\NormalTok{Sf }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{, }\StringTok{"factor"}\NormalTok{])}
\NormalTok{Sf.fit(X, y)}
\NormalTok{n }\OperatorTok{=} \DecValTok{50}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(n, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ np.random.randint(low}\OperatorTok{=}\DecValTok{1}\NormalTok{, high}\OperatorTok{=}\DecValTok{3}\NormalTok{, size}\OperatorTok{=}\NormalTok{(n,))}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.c\_[X0, X1]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X)}
\NormalTok{s}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(S.predict(X)[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ y))}
\NormalTok{sf}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(Sf.predict(X)[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ y))}
\NormalTok{sf }\OperatorTok{{-}}\NormalTok{ s}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
-40.513457642582125
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# vars(S)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# vars(Sf)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-noise}{%
\chapter{Handling Noise}\label{sec-noise}}

This chapter demonstrates how noisy functions can be handled by
\texttt{Spot}.

\hypertarget{example-spot-and-the-noisy-sphere-function}{%
\section{\texorpdfstring{Example: \texttt{Spot} and the Noisy Sphere
Function}{Example: Spot and the Noisy Sphere Function}}\label{example-spot-and-the-noisy-sphere-function}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"08"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
08_maans14_2023-11-08_10-09-29
\end{verbatim}

\hypertarget{the-objective-function-noisy-sphere}{%
\subsection{The Objective Function: Noisy
Sphere}\label{the-objective-function-noisy-sphere}}

\begin{itemize}
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function with noise, i.e., a
  function that can be described by a (closed) formula:
  \[f(x) = x^2 + \epsilon\]
\item
  Since \texttt{sigma} is set to \texttt{0.1}, noise is added to the
  function:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  A plot illustrates the noise:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-4-output-1.pdf}

}

\end{figure}

\texttt{Spot} is adopted as follows to cope with noisy functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_repeats} is set to a value larger than 1 (here: 2)
\item
  \texttt{noise} is set to \texttt{true}. Therefore, a nugget
  (\texttt{Lambda}) term is added to the correlation matrix
\item
  \texttt{init\ size} (of the \texttt{design\_control} dictionary) is
  set to a value larger than 1 (here: 2)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{2}\NormalTok{,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{2}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{\},}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.run()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-2.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.01497250376483504 [####------] 40.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-4.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.014972272755587455 [#####-----] 50.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-6.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.014966273462465166 [######----] 60.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-8.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.01480994923420837 [#######---] 70.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-10.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.011665893638594611 [########--] 80.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-12.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: -0.012922167154961792 [#########-] 90.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-6-output-14.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: -0.015064679263867698 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x105c3d510>
\end{verbatim}

\hypertarget{print-the-results-3}{%
\section{Print the Results}\label{print-the-results-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: -0.015064679263867698
x0: 0.0686858627600274
min mean y: -0.008851332275068022
x0: 0.0686858627600274
\end{verbatim}

\begin{verbatim}
[['x0', 0.0686858627600274], ['x0', 0.0686858627600274]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-8-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\hypertarget{noise-and-surrogates-the-nugget-effect}{%
\section{Noise and Surrogates: The Nugget
Effect}\label{noise-and-surrogates-the-nugget-effect}}

\hypertarget{the-noisy-sphere}{%
\subsection{The Noisy Sphere}\label{the-noisy-sphere}}

\hypertarget{the-data}{%
\subsubsection{The Data}\label{the-data}}

\begin{itemize}
\tightlist
\item
  We prepare some data first:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  A surrogate without nugget is fitted to these data:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  In comparison to the surrogate without nugget, we fit a surrogate with
  nugget to the data:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S\_nug.fit(X\_train, y\_train)}
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S\_nug.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-11-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  The value of the nugget term can be extracted from the model as
  follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
5.208508947162493e-05
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We see:

  \begin{itemize}
  \tightlist
  \item
    the first model \texttt{S} has no nugget,
  \item
    whereas the second model has a nugget value (\texttt{Lambda}) larger
    than zero.
  \end{itemize}
\end{itemize}

\hypertarget{exercises-8}{%
\section{Exercises}\label{exercises-8}}

\hypertarget{noisy-fun_cubed}{%
\subsection{\texorpdfstring{Noisy
\texttt{fun\_cubed}}{Noisy fun\_cubed}}\label{noisy-fun_cubed}}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_cubed} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_runge-1}{%
\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge-1}}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_runge} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_forrester}{%
\subsection{\texorpdfstring{\texttt{fun\_forrester}}{fun\_forrester}}\label{fun_forrester}}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_forrester} function
  with the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_xsin}{%
\subsection{\texorpdfstring{\texttt{fun\_xsin}}{fun\_xsin}}\label{fun_xsin}}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_xsin} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\FloatTok{1.}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_xsin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-ocba}{%
\chapter{\texorpdfstring{Optimal Computational Budget Allocation in
\texttt{Spot}}{Optimal Computational Budget Allocation in Spot}}\label{sec-ocba}}

This chapter demonstrates how noisy functions can be handled with
Optimal Computational Budget Allocation (OCBA) by \texttt{Spot}.

\hypertarget{example-spot-ocba-and-the-noisy-sphere-function}{%
\section{\texorpdfstring{Example: \texttt{Spot}, OCBA, and the Noisy
Sphere
Function}{Example: Spot, OCBA, and the Noisy Sphere Function}}\label{example-spot-ocba-and-the-noisy-sphere-function}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"09"}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
09_maans14_2023-11-08_10-09-41
\end{verbatim}

\hypertarget{the-objective-function-noisy-sphere-1}{%
\subsection{The Objective Function: Noisy
Sphere}\label{the-objective-function-noisy-sphere-1}}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function with noise,
i.e., a function that can be described by a (closed) formula:
\[f(x) = x^2 + \epsilon\]

Since \texttt{sigma} is set to \texttt{0.1}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

A plot illustrates the noise:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-4-output-1.pdf}

}

\end{figure}

\texttt{Spot} is adopted as follows to cope with noisy functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_repeats} is set to a value larger than 1 (here: 2)
\item
  \texttt{noise} is set to \texttt{true}. Therefore, a nugget
  (\texttt{Lambda}) term is added to the correlation matrix
\item
  \texttt{init\ size} (of the \texttt{design\_control} dictionary) is
  set to a value larger than 1 (here: 2)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{2}\NormalTok{,}
\NormalTok{                   infill\_criterion}\OperatorTok{=}\StringTok{"ei"}\NormalTok{,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   tolerance\_x}\OperatorTok{=}\FloatTok{0.0}\NormalTok{,}
\NormalTok{                   ocba\_delta }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{2}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.run()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-2.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: -0.0641572013655628 [####------] 45.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-4.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: -0.08106318979737473 [######----] 60.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-6.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: -0.08106318979737473 [########--] 75.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-8.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: -0.08106318979737473 [#########-] 90.00% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-10.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: -0.08106318979737473 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x352dd4110>
\end{verbatim}

\hypertarget{print-the-results-4}{%
\section{Print the Results}\label{print-the-results-4}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: -0.08106318979737473
x0: 0.13359994475077583
min mean y: -0.03275683462209028
x0: 0.13359994475077583
\end{verbatim}

\begin{verbatim}
[['x0', 0.13359994475077583], ['x0', 0.13359994475077583]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\hypertarget{noise-and-surrogates-the-nugget-effect-1}{%
\section{Noise and Surrogates: The Nugget
Effect}\label{noise-and-surrogates-the-nugget-effect-1}}

\hypertarget{the-noisy-sphere-1}{%
\subsection{The Noisy Sphere}\label{the-noisy-sphere-1}}

\hypertarget{the-data-1}{%
\subsubsection{The Data}\label{the-data-1}}

We prepare some data first:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{125}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

A surrogate without nugget is fitted to these data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-10-output-1.pdf}

}

\end{figure}

In comparison to the surrogate without nugget, we fit a surrogate with
nugget to the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S\_nug.fit(X\_train, y\_train)}
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S\_nug.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{014_num_spot_ocba_files/figure-pdf/cell-11-output-1.pdf}

}

\end{figure}

The value of the nugget term can be extracted from the model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
9.088149959982792e-05
\end{verbatim}

We see:

\begin{itemize}
\tightlist
\item
  the first model \texttt{S} has no nugget,
\item
  whereas the second model has a nugget value (\texttt{Lambda}) larger
  than zero.
\end{itemize}

\hypertarget{exercises-9}{%
\section{Exercises}\label{exercises-9}}

\hypertarget{noisy-fun_cubed-1}{%
\subsection{\texorpdfstring{Noisy
\texttt{fun\_cubed}}{Noisy fun\_cubed}}\label{noisy-fun_cubed-1}}

Analyse the effect of noise on the \texttt{fun\_cubed} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_runge-2}{%
\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge-2}}

Analyse the effect of noise on the \texttt{fun\_runge} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_forrester-1}{%
\subsection{\texorpdfstring{\texttt{fun\_forrester}}{fun\_forrester}}\label{fun_forrester-1}}

Analyse the effect of noise on the \texttt{fun\_forrester} function with
the following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\DecValTok{5}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{fun_xsin-1}{%
\subsection{\texorpdfstring{\texttt{fun\_xsin}}{fun\_xsin}}\label{fun_xsin-1}}

Analyse the effect of noise on the \texttt{fun\_xsin} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\FloatTok{1.}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_xsin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\part{Introduction to Hyperparameter Tuning}

\hypertarget{hyperparameter-tuning}{%
\chapter{Hyperparameter Tuning}\label{hyperparameter-tuning}}

\hypertarget{structure-of-the-hyperparameter-tuning-chapters}{%
\section{Structure of the Hyperparameter Tuning
Chapters}\label{structure-of-the-hyperparameter-tuning-chapters}}

The first part is structured as follows:

The concept of the hyperparameter tuning is described in
Section~\ref{sec-hyperparameter-tuning-goals}.

Hyperparameter tuning with sklearn in Python is described in
Chapter~\ref{sec-hpt-sklearn}.

Hyperparameter tuning with river in Python is described in
Chapter~\ref{sec-hpt-river}.

This part of the book is concluded with a description of the most recent
\texttt{PyTorch} hyperparameter tuning approach, which is the
integration of \texttt{spotPython} into the \texttt{PyTorch\ Lightning}
training workflow. Hyperparameter tuning with PyTorch Lightning in
Python is described in Chapter~\ref{sec-hpt-pytorch}. This is considered
as the most effective, efficient, and flexible way to integrate
\texttt{spotPython} into the \texttt{PyTorch} training workflow.

Figure~\ref{fig-spotGUI} shows the graphical user interface of
\texttt{spotPython} that is used in this book.

\begin{figure}

{\centering \includegraphics{./figures_static/spotGUI.png}

}

\caption{\label{fig-spotGUI}spot GUI}

\end{figure}

\hypertarget{sec-hyperparameter-tuning-goals}{%
\section{Goals of Hyperparameter
Tuning}\label{sec-hyperparameter-tuning-goals}}

The goal of hyperparameter tuning is to optimize the hyperparameters in
a way that improves the performance of the machine learning or deep
learning model. Hyperparameters are parameters that are not learned
during the training process, but are set before the training process
begins. Hyperparameter tuning is an important, but often difficult and
computationally intensive task. Changing the architecture of a neural
network or the learning rate of an optimizer can have a significant
impact on the performance.

Hyperparameter tuning is referred to as ``hyperparameter optimization''
(HPO) in the literature. However, since we do not consider the
optimization, but also the understanding of the hyperparameters, we use
the term ``hyperparameter tuning'' in this book. See also the discussion
in Chapter 2 of Bartz et al. (2022), which lays the groundwork and
presents an introduction to the process of tuning Machine Learning and
Deep Learning hyperparameters and the respective methodology. Since the
key elements such as the hyperparameter tuning process and measures of
tunability and performance are presented in Bartz et al. (2022), we
refer to this chapter for details.

The simplest, but also most computationally expensive, hyperparameter
tuning approach uses manual search (or trial-and-error (Meignan et al.
2015)). Commonly encountered is simple random search, i.e., random and
repeated selection of hyperparameters for evaluation, and lattice search
(``grid search''). In addition, methods that perform directed search and
other model-free algorithms, i.e., algorithms that do not explicitly
rely on a model, e.g., evolution strategies (Bartz-Beielstein et al.
2014) or pattern search (Lewis, Torczon, and Trosset 2000) play an
important role. Also, ``hyperband'', i.e., a multi-armed bandit strategy
that dynamically allocates resources to a set of random configurations
and uses successive bisections to stop configurations with poor
performance (Li et al. 2016), is very common in hyperparameter tuning.
The most sophisticated and efficient approaches are the Bayesian
optimization and surrogate model based optimization methods, which are
based on the optimization of cost functions determined by simulations or
experiments.

We consider a surrogate optimization based hyperparameter tuning
approach that uses the Python version of the SPOT (``Sequential
Parameter Optimization Toolbox'') (Bartz-Beielstein, Lasarczyk, and
Preuss 2005), which is suitable for situations where only limited
resources are available. This may be due to limited availability and
cost of hardware, or due to the fact that confidential data may only be
processed locally, e.g., due to legal requirements. Furthermore, in our
approach, the understanding of algorithms is seen as a key tool for
enabling transparency and explainability. This can be enabled, for
example, by quantifying the contribution of machine learning and deep
learning components (nodes, layers, split decisions, activation
functions, etc.). Understanding the importance of hyperparameters and
the interactions between multiple hyperparameters plays a major role in
the interpretability and explainability of machine learning models. SPOT
provides statistical tools for understanding hyperparameters and their
interactions. Last but not least, it should be noted that the SPOT
software code is available in the open source \texttt{spotPython}
package on github\footnote{\url{https://github.com/sequential-parameter-optimization}},
allowing replicability of the results. This tutorial describes the
Python variant of SPOT, which is called \texttt{spotPython}. The R
implementation is described in Bartz et al. (2022). SPOT is an
established open source software that has been maintained for more than
15 years (Bartz-Beielstein, Lasarczyk, and Preuss 2005) (Bartz et al.
2022).

\part{Hyperparameter Tuning with Sklearn}

\hypertarget{sec-hpt-sklearn}{%
\chapter{HPT: sklearn}\label{sec-hpt-sklearn}}

\hypertarget{sec-hpt-sklearn-intro}{%
\section{Introduction to sklearn}\label{sec-hpt-sklearn-intro}}

\hypertarget{sec-hpt-sklearn-svc}{%
\chapter{HPT: sklearn SVC on Moons Data}\label{sec-hpt-sklearn-svc}}

This chapter is a tutorial for the Hyperparameter Tuning (HPT) of a
\texttt{sklearn} SVC model on the Moons dataset.

\hypertarget{sec-setup-10}{%
\section{Step 1: Setup}\label{sec-setup-10}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time, initial design size and the device that
is used.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  MAX\_TIME is set to one minute for demonstration purposes. For real
  experiments, this should be increased to at least 1 hour.
\item
  INIT\_SIZE is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{10}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"10"}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-2-initialization-of-the-empty-fun_control-dictionary}{%
\section{\texorpdfstring{Step 2: Initialization of the Empty
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the Empty fun\_control Dictionary}}\label{step-2-initialization-of-the-empty-fun_control-dictionary}}

The \texttt{fun\_control} dictionary is the central data structure that
is used to control the optimization process. It is initialized as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}

\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    task}\OperatorTok{=}\StringTok{"classification"}\NormalTok{,}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-data-loading-10}{%
\section{Step 3: SKlearn Load Data
(Classification)}\label{sec-data-loading-10}}

Randomly generate classification data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ make\_moons, make\_circles, make\_classification}
\NormalTok{n\_features }\OperatorTok{=} \DecValTok{2}
\NormalTok{n\_samples }\OperatorTok{=} \DecValTok{500}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\NormalTok{ds }\OperatorTok{=}\NormalTok{  make\_moons(n\_samples, noise}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{X, y }\OperatorTok{=}\NormalTok{ ds}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{    X, y, test\_size}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{train }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_train, y\_train.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{test }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_test, y\_test.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{train.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& x1 & x2 & y \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1.960101 & 0.383172 & 0.0 \\
1 & 2.354420 & -0.536942 & 1.0 \\
2 & 1.682186 & -0.332108 & 0.0 \\
3 & 1.856507 & 0.687220 & 1.0 \\
4 & 1.925524 & 0.427413 & 1.0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ matplotlib.colors }\ImportTok{import}\NormalTok{ ListedColormap}

\NormalTok{x\_min, x\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{y\_min, y\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{cm }\OperatorTok{=}\NormalTok{ plt.cm.RdBu}
\NormalTok{cm\_bright }\OperatorTok{=}\NormalTok{ ListedColormap([}\StringTok{"\#FF0000"}\NormalTok{, }\StringTok{"\#0000FF"}\NormalTok{])}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ plt.subplot(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Input data"}\NormalTok{)}
\CommentTok{\# Plot the training points}
\NormalTok{ax.scatter(X\_train[:, }\DecValTok{0}\NormalTok{], X\_train[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\NormalTok{y\_train, cmap}\OperatorTok{=}\NormalTok{cm\_bright, edgecolors}\OperatorTok{=}\StringTok{"k"}\NormalTok{)}
\CommentTok{\# Plot the testing points}
\NormalTok{ax.scatter(}
\NormalTok{    X\_test[:, }\DecValTok{0}\NormalTok{], X\_test[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\NormalTok{y\_test, cmap}\OperatorTok{=}\NormalTok{cm\_bright, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{"k"}
\NormalTok{)}
\NormalTok{ax.set\_xlim(x\_min, x\_max)}
\NormalTok{ax.set\_ylim(y\_min, y\_max)}
\NormalTok{ax.set\_xticks(())}
\NormalTok{ax.set\_yticks(())}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{017_spot_hpt_sklearn_classification_files/figure-pdf/cell-5-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(train)}
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}\StringTok{"data"}\NormalTok{: }\VariableTok{None}\NormalTok{, }\CommentTok{\# dataset,}
               \StringTok{"train"}\NormalTok{: train,}
               \StringTok{"test"}\NormalTok{: test,}
               \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
               \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-specification-of-preprocessing-model-10}{%
\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-10}}

Data preprocesssing can be very simple, e.g., you can ignore it. Then
you would choose the \texttt{prep\_model} ``None'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

A default approach for numerical data is the \texttt{StandardScaler}
(mean 0, variance 1). This can be selected as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\NormalTok{prep\_model }\OperatorTok{=}\NormalTok{ StandardScaler()}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

Even more complicated pre-processing steps are possible, e.g., the
follwing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_columns = []}
\NormalTok{one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore", sparse\_output=False)}
\NormalTok{prep\_model = ColumnTransformer(}
\NormalTok{         transformers=[}
\NormalTok{             ("categorical", one\_hot\_encoder, categorical\_columns),}
\NormalTok{         ],}
\NormalTok{         remainder=StandardScaler(),}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-5-select-model-algorithm-and-core_model_hyper_dict}{%
\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{step-5-select-model-algorithm-and-core_model_hyper_dict}}

The selection of the algorithm (ML model) that should be tuned is done
by specifying the its name from the \texttt{sklearn} implementation. For
example, the \texttt{SVC} support vector machine classifier is selected
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotPython.data.sklearn\_hyper\_dict }\ImportTok{import}\NormalTok{ SklearnHyperDict}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{SVC,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{SklearnHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now \texttt{fun\_control} has the information from the JSON file. The
corresponding entries for the \texttt{core\_model} class are shown
below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control[}\StringTok{\textquotesingle{}core\_model\_hyper\_dict\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'C': {'type': 'float',
  'default': 1.0,
  'transform': 'None',
  'lower': 0.1,
  'upper': 10.0},
 'kernel': {'levels': ['linear', 'poly', 'rbf', 'sigmoid'],
  'type': 'factor',
  'default': 'rbf',
  'transform': 'None',
  'core_model_parameter_type': 'str',
  'lower': 0,
  'upper': 3},
 'degree': {'type': 'int',
  'default': 3,
  'transform': 'None',
  'lower': 3,
  'upper': 3},
 'gamma': {'levels': ['scale', 'auto'],
  'type': 'factor',
  'default': 'scale',
  'transform': 'None',
  'core_model_parameter_type': 'str',
  'lower': 0,
  'upper': 1},
 'coef0': {'type': 'float',
  'default': 0.0,
  'transform': 'None',
  'lower': 0.0,
  'upper': 0.0},
 'shrinking': {'levels': [0, 1],
  'type': 'factor',
  'default': 0,
  'transform': 'None',
  'core_model_parameter_type': 'bool',
  'lower': 0,
  'upper': 1},
 'probability': {'levels': [0, 1],
  'type': 'factor',
  'default': 0,
  'transform': 'None',
  'core_model_parameter_type': 'bool',
  'lower': 0,
  'upper': 1},
 'tol': {'type': 'float',
  'default': 0.001,
  'transform': 'None',
  'lower': 0.0001,
  'upper': 0.01},
 'cache_size': {'type': 'float',
  'default': 200,
  'transform': 'None',
  'lower': 100,
  'upper': 400},
 'break_ties': {'levels': [0, 1],
  'type': 'factor',
  'default': 0,
  'transform': 'None',
  'core_model_parameter_type': 'bool',
  'lower': 0,
  'upper': 1}}
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\texttt{sklearn\ Model} Selection}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The following \texttt{sklearn} models are supported by default:

\begin{itemize}
\tightlist
\item
  RidgeCV
\item
  RandomForestClassifier
\item
  SVC
\item
  LogisticRegression
\item
  KNeighborsClassifier
\item
  GradientBoostingClassifier
\item
  GradientBoostingRegressor
\item
  ElasticNet
\end{itemize}

They can be imported as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from sklearn.linear\_model import RidgeCV}
\NormalTok{from sklearn.ensemble import RandomForestClassifier}
\NormalTok{from sklearn.svm import SVC}
\NormalTok{from sklearn.linear\_model import LogisticRegression}
\NormalTok{from sklearn.neighbors import KNeighborsClassifier}
\NormalTok{from sklearn.ensemble import GradientBoostingClassifier}
\NormalTok{from sklearn.ensemble import GradientBoostingRegressor}
\NormalTok{from sklearn.linear\_model import ElasticNet}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model}{%
\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model}}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modification-of-hyperparameters-14}.

\hypertarget{modify-hyperparameter-of-type-numeric-and-integer-boolean}{%
\subsection{Modify hyperparameter of type numeric and integer
(boolean)}\label{modify-hyperparameter-of-type-numeric-and-integer-boolean}}

Numeric and boolean values can be modified using the
\texttt{modify\_hyper\_parameter\_bounds} method.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\texttt{sklearn\ Model} Hyperparameters}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The hyperparameters of the \texttt{sklearn} \texttt{SVC} model are
described in the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}{sklearn
documentation}.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  For example, to change the \texttt{tol} hyperparameter of the
  \texttt{SVC} model to the interval {[}1e-5, 1e-3{]}, the following
  code can be used:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"tol"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{1e{-}5}\NormalTok{, }\FloatTok{1e{-}3}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"probability"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{][}\StringTok{"tol"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'type': 'float',
 'default': 0.001,
 'transform': 'None',
 'lower': 1e-05,
 'upper': 0.001}
\end{verbatim}

\hypertarget{modify-hyperparameter-of-type-factor}{%
\subsection{Modify hyperparameter of type
factor}\label{modify-hyperparameter-of-type-factor}}

Factors can be modified with the
\texttt{modify\_hyper\_parameter\_levels} function. For example, to
exclude the \texttt{sigmoid} kernel from the tuning, the \texttt{kernel}
hyperparameter of the \texttt{SVC} model can be modified as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"kernel"}\NormalTok{, [}\StringTok{"poly"}\NormalTok{, }\StringTok{"rbf"}\NormalTok{])}
\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{][}\StringTok{"kernel"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'levels': ['poly', 'rbf'],
 'type': 'factor',
 'default': 'rbf',
 'transform': 'None',
 'core_model_parameter_type': 'str',
 'lower': 0,
 'upper': 1}
\end{verbatim}

\hypertarget{sec-optimizers-10}{%
\subsection{Optimizers}\label{sec-optimizers-10}}

Optimizers are described in Section~\ref{sec-optimizers-14}.

\hypertarget{step-7-selection-of-the-objective-loss-function}{%
\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function}}

There are two metrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{metric\_river} is used for the river based evaluation via
  \texttt{eval\_oml\_iter\_progressive}.
\item
  \texttt{metric\_sklearn} is used for the sklearn based evaluation.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_absolute\_error, accuracy\_score, roc\_curve, roc\_auc\_score, log\_loss, mean\_squared\_error}
\NormalTok{fun\_control.update(\{}
               \StringTok{"metric\_sklearn"}\NormalTok{: log\_loss,}
               \StringTok{"weights"}\NormalTok{: }\FloatTok{1.0}\NormalTok{,}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{\texttt{metric\_sklearn}: Minimization and Maximization}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-warning-color!10!white, toptitle=1mm, colframe=quarto-callout-warning-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Because the \texttt{metric\_sklearn} is used for the sklearn based
  evaluation, it is important to know whether the metric should be
  minimized or maximized.
\item
  The \texttt{weights} parameter is used to indicate whether the metric
  should be minimized or maximized.
\item
  If \texttt{weights} is set to \texttt{-1.0}, the metric is maximized.
\item
  If \texttt{weights} is set to \texttt{1.0}, the metric is minimized,
  e.g., \texttt{weights\ =\ 1.0} for \texttt{mean\_absolute\_error}, or
  \texttt{weights\ =\ -1.0} for \texttt{roc\_auc\_score}.
\end{itemize}

\end{tcolorbox}

\hypertarget{predict-classes-or-class-probabilities}{%
\subsection{Predict Classes or Class
Probabilities}\label{predict-classes-or-class-probabilities}}

If the key \texttt{"predict\_proba"} is set to \texttt{True}, the class
probabilities are predicted. \texttt{False} is the default, i.e., the
classes are predicted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
               \StringTok{"predict\_proba"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-8-calling-the-spot-function}{%
\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function}}

\hypertarget{sec-prepare-spot-call-10}{%
\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-10}}

The following code passes the information about the parameter ranges and
bounds to \texttt{spot}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extract the variable types, names, and bounds}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (    }
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_var\_type,}
\NormalTok{    get\_bound\_values}
\NormalTok{    )}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name        | type   | default   |   lower |   upper | transform   |
|-------------|--------|-----------|---------|---------|-------------|
| C           | float  | 1.0       |   0.1   |  10     | None        |
| kernel      | factor | rbf       |   0     |   1     | None        |
| degree      | int    | 3         |   3     |   3     | None        |
| gamma       | factor | scale     |   0     |   1     | None        |
| coef0       | float  | 0.0       |   0     |   0     | None        |
| shrinking   | factor | 0         |   0     |   1     | None        |
| probability | factor | 0         |   0     |   0     | None        |
| tol         | float  | 0.001     |   1e-05 |   0.001 | None        |
| cache_size  | float  | 200.0     | 100     | 400     | None        |
| break_ties  | factor | 0         |   0     |   1     | None        |
\end{verbatim}

\hypertarget{sec-the-objective-function-10}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-10}}

The objective function is selected next. It implements an interface from
\texttt{sklearn}'s training, validation, and testing methods to
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperSklearn().fun\_sklearn}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\CommentTok{\# X\_start = get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer}}

\begin{itemize}
\tightlist
\item
  Run SPOT for approx. x mins (\texttt{max\_time}).
\item
  Note: the run takes longer, because the evaluation time of initial
  design (here: \texttt{initi\_size}, 20 points) is not considered.
\end{itemize}

\hypertarget{sec-call-the-hyperparameter-tuner-10}{%
\subsection{Starting the Hyperparameter
Tuning}\label{sec-call-the-hyperparameter-tuner-10}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                   n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{1}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{,}
                                      \StringTok{"log\_level"}\NormalTok{: }\DecValTok{50}
\NormalTok{                                      \})}
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [----------] 1.85% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [----------] 4.30% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [#---------] 7.51% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [#---------] 10.45% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [#---------] 13.16% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [##--------] 15.53% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [##--------] 17.64% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [##--------] 22.96% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [###-------] 28.56% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [###-------] 34.99% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [####------] 42.14% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [#####-----] 49.53% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [######----] 55.65% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [######----] 59.17% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [######----] 63.27% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [########--] 76.79% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [#########-] 92.35% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 5.734217584632275 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x3148a95d0>
\end{verbatim}

\hypertarget{sec-results-tuning-10}{%
\section{Step 9: Results}\label{sec-results-tuning-10}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle}
\NormalTok{save\_pickle(spot\_tuner, experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_pickle}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ load\_pickle(experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Show the Progress of the hyperparameter tuning:
\end{itemize}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{017_spot_hpt_sklearn_classification_files/figure-pdf/cell-22-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\begin{itemize}
\tightlist
\item
  Print the results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{    spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name        | type   | default   |   lower |   upper |                tuned | transform   |   importance | stars   |
|-------------|--------|-----------|---------|---------|----------------------|-------------|--------------|---------|
| C           | float  | 1.0       |     0.1 |    10.0 |    2.394471655384338 | None        |         5.97 | *       |
| kernel      | factor | rbf       |     0.0 |     1.0 |                  1.0 | None        |       100.00 | ***     |
| degree      | int    | 3         |     3.0 |     3.0 |                  3.0 | None        |         0.00 |         |
| gamma       | factor | scale     |     0.0 |     1.0 |                  0.0 | None        |         0.00 |         |
| coef0       | float  | 0.0       |     0.0 |     0.0 |                  0.0 | None        |         0.00 |         |
| shrinking   | factor | 0         |     0.0 |     1.0 |                  0.0 | None        |         0.00 |         |
| probability | factor | 0         |     0.0 |     0.0 |                  0.0 | None        |         0.00 |         |
| tol         | float  | 0.001     |   1e-05 |   0.001 | 0.000982585315792582 | None        |         0.00 |         |
| cache_size  | float  | 200.0     |   100.0 |   400.0 |    375.6371648003268 | None        |         0.00 |         |
| break_ties  | factor | 0         |     0.0 |     1.0 |                  0.0 | None        |         0.00 |         |
\end{verbatim}

\hypertarget{show-variable-importance}{%
\subsection{Show variable importance}\label{show-variable-importance}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{017_spot_hpt_sklearn_classification_files/figure-pdf/cell-24-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}

\hypertarget{get-default-hyperparameters}{%
\subsection{Get Default
Hyperparameters}\label{get-default-hyperparameters}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_values, transform\_hyper\_parameter\_values}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ get\_default\_values(fun\_control)}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ transform\_hyper\_parameter\_values(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, hyper\_parameter\_values}\OperatorTok{=}\NormalTok{values\_default)}
\NormalTok{values\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'C': 1.0,
 'kernel': 'rbf',
 'degree': 3,
 'gamma': 'scale',
 'coef0': 0.0,
 'shrinking': 0,
 'probability': 0,
 'tol': 0.001,
 'cache_size': 200.0,
 'break_ties': 0}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ make\_pipeline}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ make\_pipeline(fun\_control[}\StringTok{"prep\_model"}\NormalTok{], fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{values\_default))}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svc',
                 SVC(break_ties=0, cache_size=200.0, probability=0,
                     shrinking=0))])
\end{verbatim}

\hypertarget{get-spot-results}{%
\subsection{Get SPOT Results}\label{get-spot-results}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[2.39447166e+00 1.00000000e+00 3.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.82585316e-04
  3.75637165e+02 0.00000000e+00]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, return\_conf\_list\_from\_var\_dict}
\NormalTok{v\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, fun\_control[}\StringTok{"var\_name"}\NormalTok{])}
\NormalTok{return\_conf\_list\_from\_var\_dict(var\_dict}\OperatorTok{=}\NormalTok{v\_dict, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[{'C': 2.394471655384338,
  'kernel': 'rbf',
  'degree': 3,
  'gamma': 'scale',
  'coef0': 0.0,
  'shrinking': 0,
  'probability': 0,
  'tol': 0.000982585315792582,
  'cache_size': 375.6371648003268,
  'break_ties': 0}]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_sklearn\_model\_from\_X}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_sklearn\_model\_from\_X(X, fun\_control)}
\NormalTok{model\_spot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svc',
                 SVC(C=2.394471655384338, break_ties=0,
                     cache_size=375.6371648003268, probability=0, shrinking=0,
                     tol=0.000982585315792582))])
\end{verbatim}

\hypertarget{plot-compare-predictions}{%
\subsection{Plot: Compare Predictions}\label{plot-compare-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_roc}
\NormalTok{plot\_roc(model\_list}\OperatorTok{=}\NormalTok{[model\_default, model\_spot], fun\_control}\OperatorTok{=}\NormalTok{ fun\_control, model\_names}\OperatorTok{=}\NormalTok{[}\StringTok{"Default"}\NormalTok{, }\StringTok{"Spot"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{017_spot_hpt_sklearn_classification_files/figure-pdf/cell-30-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_confusion\_matrix}
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_default, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title }\OperatorTok{=} \StringTok{"Default"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{017_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{017_spot_hpt_sklearn_classification_files/figure-pdf/cell-32-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{min}\NormalTok{(spot\_tuner.y), }\BuiltInTok{max}\NormalTok{(spot\_tuner.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(5.734217584632275, 7.782152436286657)
\end{verbatim}

\hypertarget{detailed-hyperparameter-plots}{%
\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
C:  5.974249844745921
kernel:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{017_spot_hpt_sklearn_classification_files/figure-pdf/cell-34-output-2.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plot}{%
\subsection{Parallel Coordinates Plot}\label{parallel-coordinates-plot}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters}{%
\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-hpt-random-forest-classifier}{%
\chapter{HPT: sklearn RandomForestClassifier VBDP
Data}\label{sec-hpt-random-forest-classifier}}

This chapter describes the hyperparameter tuning of a
\texttt{RandomForestClassifier} on the Vector Borne Disease Prediction
(VBDP) data set.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Vector Borne Disease Prediction Data Set}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

This chapter uses the Vector Borne Disease Prediction data set from
Kaggle. It is a categorical dataset for eleven Vector Borne Diseases
with associated symptoms.

\begin{quote}
The person who associated a work with this deed has dedicated the work
to the public domain by waiving all of his or her rights to the work
worldwide under copyright law, including all related and neighboring
rights, to the extent allowed by law.You can copy, modify, distribute
and perform the work, even for commercial purposes, all without asking
permission. See Other Information below, see
\url{https://creativecommons.org/publicdomain/zero/1.0/}.
\end{quote}

The data set is available at:
\url{https://www.kaggle.com/datasets/richardbernat/vector-borne-disease-prediction},

The data should be downloaded and stored in the \texttt{data/VBDP}
subfolder. The data set is not available as a part of the
\texttt{spotPython} package.

\end{tcolorbox}

\hypertarget{sec-setup-16}{%
\section{Step 1: Setup}\label{sec-setup-16}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time and the initial design size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{ORIGINAL }\OperatorTok{=} \VariableTok{True}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"16"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-2-initialization-of-the-empty-fun_control-dictionary-1}{%
\section{\texorpdfstring{Step 2: Initialization of the Empty
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the Empty fun\_control Dictionary}}\label{step-2-initialization-of-the-empty-fun_control-dictionary-1}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}

\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    task}\OperatorTok{=}\StringTok{"classification"}\NormalTok{,}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-3-pytorch-data-loading}{%
\section{Step 3: PyTorch Data
Loading}\label{step-3-pytorch-data-loading}}

\hypertarget{load-data-classification-vbdp}{%
\subsection{Load Data: Classification
VBDP}\label{load-data-classification-vbdp}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ControlFlowTok{if}\NormalTok{ ORIGINAL }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/trainn.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    test\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/testt.csv\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/train.csv\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# remove the id column}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ train\_df.drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OrdinalEncoder}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{n\_features }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"prognosis"}
\CommentTok{\# Encoder our prognosis labels as integers for easier decoding later}
\NormalTok{enc }\OperatorTok{=}\NormalTok{ OrdinalEncoder()}
\NormalTok{train\_df[target\_column] }\OperatorTok{=}\NormalTok{ enc.fit\_transform(train\_df[[target\_column]])}
\NormalTok{train\_df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train\_df.shape)}
\NormalTok{train\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(252, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
2 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
3 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
4 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
\end{longtable}

The full data set \texttt{train\_df} 64 features. The target column is
labeled as \texttt{prognosis}.

\hypertarget{holdout-train-and-test-data}{%
\subsection{Holdout Train and Test
Data}\label{holdout-train-and-test-data}}

We split out a hold-out test set (25\% of the data) so we can calculate
an example MAP@K

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(train\_df.drop(target\_column, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{), train\_df[target\_column],}
\NormalTok{                                                    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{                                                    test\_size}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{                                                    stratify}\OperatorTok{=}\NormalTok{train\_df[target\_column])}
\NormalTok{train }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_train, np.array(y\_train).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{test }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_test, np.array(y\_test).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train.shape)}
\BuiltInTok{print}\NormalTok{(test.shape)}
\NormalTok{train.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(189, 65)
(63, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 7.0 \\
1 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & ... &
0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 3.0 \\
2 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 10.0 \\
3 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & ... &
1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 3.0 \\
4 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 8.0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}\StringTok{"data"}\NormalTok{: train\_df, }\CommentTok{\# full dataset,}
               \StringTok{"train"}\NormalTok{: train,}
               \StringTok{"test"}\NormalTok{: test,}
               \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
               \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-specification-of-preprocessing-model-16}{%
\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-16}}

Data preprocesssing can be very simple, e.g., you can ignore it. Then
you would choose the \texttt{prep\_model} ``None'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

A default approach for numerical data is the \texttt{StandardScaler}
(mean 0, variance 1). This can be selected as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# prep\_model = StandardScaler()}
\CommentTok{\# fun\_control.update(\{"prep\_model": prep\_model\})}
\end{Highlighting}
\end{Shaded}

Even more complicated pre-processing steps are possible, e.g., the
follwing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# categorical\_columns = []}
\CommentTok{\# one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore", sparse\_output=False)}
\CommentTok{\# prep\_model = ColumnTransformer(}
\CommentTok{\#         transformers=[}
\CommentTok{\#             ("categorical", one\_hot\_encoder, categorical\_columns),}
\CommentTok{\#         ],}
\CommentTok{\#         remainder=StandardScaler(),}
\CommentTok{\#     )}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-5-select-model-algorithm-and-core_model_hyper_dict-1}{%
\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{step-5-select-model-algorithm-and-core_model_hyper_dict-1}}

The selection of the algorithm (ML model) that should be tuned is done
by specifying the its name from the \texttt{sklearn} implementation. For
example, the \texttt{SVC} support vector machine classifier is selected
as follows:

\texttt{add\_core\_model\_to\_fun\_control(SVC,\ fun\_control,\ SklearnHyperDict)}

Other core\_models are, e.g.,:

\begin{itemize}
\tightlist
\item
  RidgeCV
\item
  GradientBoostingRegressor
\item
  ElasticNet
\item
  RandomForestClassifier
\item
  LogisticRegression
\item
  KNeighborsClassifier
\item
  RandomForestClassifier
\item
  GradientBoostingClassifier
\item
  HistGradientBoostingClassifier
\end{itemize}

We will use the \texttt{RandomForestClassifier} classifier in this
example.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ RidgeCV}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingRegressor}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ ElasticNet}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotPython.data.sklearn\_hyper\_dict }\ImportTok{import}\NormalTok{ SklearnHyperDict}
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# core\_model  = RidgeCV}
\CommentTok{\# core\_model = GradientBoostingRegressor}
\CommentTok{\# core\_model = ElasticNet}
\NormalTok{core\_model }\OperatorTok{=}\NormalTok{ RandomForestClassifier}
\CommentTok{\# core\_model = SVC}
\CommentTok{\# core\_model = LogisticRegression}
\CommentTok{\# core\_model = KNeighborsClassifier}
\CommentTok{\# core\_model = GradientBoostingClassifier}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{core\_model,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{SklearnHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now \texttt{fun\_control} has the information from the JSON file. The
available hyperparameters are:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\OperatorTok{*}\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{].keys(), sep}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_estimators
criterion
max_depth
min_samples_split
min_samples_leaf
min_weight_fraction_leaf
max_features
max_leaf_nodes
min_impurity_decrease
bootstrap
oob_score
\end{verbatim}

\hypertarget{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-1}{%
\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-1}}

\hypertarget{modify-hyperparameter-of-type-numeric-and-integer-boolean-1}{%
\subsection{Modify hyperparameter of type numeric and integer
(boolean)}\label{modify-hyperparameter-of-type-numeric-and-integer-boolean-1}}

Numeric and boolean values can be modified using the
\texttt{modify\_hyper\_parameter\_bounds} method. For example, to change
the \texttt{tol} hyperparameter of the \texttt{SVC} model to the
interval {[}1e-3, 1e-2{]}, the following code can be used:

\texttt{modify\_hyper\_parameter\_bounds(fun\_control,\ "tol",\ bounds={[}1e-3,\ 1e-2{]})}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "tol", bounds=[1e{-}3, 1e{-}2])}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyperparameter-of-type-factor-1}{%
\subsection{Modify hyperparameter of type
factor}\label{modify-hyperparameter-of-type-factor-1}}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modification-of-hyperparameters-14}.

Factors can be modified with the
\texttt{modify\_hyper\_parameter\_levels} function. For example, to
exclude the \texttt{sigmoid} kernel from the tuning, the \texttt{kernel}
hyperparameter of the \texttt{SVC} model can be modified as follows:

\texttt{modify\_hyper\_parameter\_levels(fun\_control,\ "kernel",\ {[}"linear",\ "rbf"{]})}

The new setting can be controlled via:

\texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}{[}"kernel"{]}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\CommentTok{\# XGBoost:}
\CommentTok{\# modify\_hyper\_parameter\_levels(fun\_control, "loss", ["log\_loss"])}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: RandomForestClassifier and Out-of-bag Estimation}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Since \texttt{oob\_score} requires the \texttt{bootstrap} hyperparameter
to \texttt{True}, we set the \texttt{oob\_score} parameter to
\texttt{False}. The \texttt{oob\_score} is later discussed in
Section~\ref{sec-oob-score-16}.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"bootstrap"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"oob\_score"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-optimizers-16}{%
\subsection{Optimizers}\label{sec-optimizers-16}}

Optimizers are described in Section~\ref{sec-optimizers-14}.

\hypertarget{selection-of-the-objective-metric-and-loss-functions}{%
\subsection{Selection of the Objective: Metric and Loss
Functions}\label{selection-of-the-objective-metric-and-loss-functions}}

\begin{itemize}
\tightlist
\item
  Machine learning models are optimized with respect to a metric, for
  example, the \texttt{accuracy} function.
\item
  Deep learning, e.g., neural networks are optimized with respect to a
  loss function, for example, the \texttt{cross\_entropy} function and
  evaluated with respect to a metric, for example, the \texttt{accuracy}
  function.
\end{itemize}

\hypertarget{step-7-selection-of-the-objective-loss-function-1}{%
\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function-1}}

The loss function, that is usually used in deep learning for optimizing
the weights of the net, is stored in the \texttt{fun\_control}
dictionary as \texttt{"loss\_function"}.

\hypertarget{metric-function}{%
\subsection{Metric Function}\label{metric-function}}

There are two different types of metrics in \texttt{spotPython}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{"metric\_river"} is used for the river based evaluation via
  \texttt{eval\_oml\_iter\_progressive}.
\item
  \texttt{"metric\_sklearn"} is used for the sklearn based evaluation.
\end{enumerate}

We will consider multi-class classification metrics, e.g.,
\texttt{mapk\_score} and \texttt{top\_k\_accuracy\_score}.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Predict Probabilities}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

In this multi-class classification example the machine learning
algorithm should return the probabilities of the specific classes
(\texttt{"predict\_proba"}) instead of the predicted values.

\end{tcolorbox}

We set \texttt{"predict\_proba"} to \texttt{True} in the
\texttt{fun\_control} dictionary.

\hypertarget{the-mapk-metric}{%
\subsubsection{The MAPK Metric}\label{the-mapk-metric}}

To select the MAPK metric, the following two entries can be added to the
\texttt{fun\_control} dictionary:

\texttt{"metric\_sklearn":\ mapk\_score"}

\texttt{"metric\_params":\ \{"k":\ 3\}}.

\hypertarget{other-metrics}{%
\subsubsection{Other Metrics}\label{other-metrics}}

Alternatively, other metrics for multi-class classification can be used,
e.g.,: * top\_k\_accuracy\_score or * roc\_auc\_score

The metric \texttt{roc\_auc\_score} requires the parameter
\texttt{"multi\_class"}, e.g.,

\texttt{"multi\_class":\ "ovr"}.

This is set in the \texttt{fun\_control} dictionary.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Weights}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotPython} performs a minimization, therefore, metrics that
should be maximized have to be multiplied by -1. This is done by setting
\texttt{"weights"} to \texttt{-1}.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  The complete setup for the metric in our example is:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.metrics }\ImportTok{import}\NormalTok{ mapk\_score}
\NormalTok{fun\_control.update(\{}
               \StringTok{"weights"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}
               \StringTok{"metric\_sklearn"}\NormalTok{: mapk\_score,}
               \StringTok{"predict\_proba"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
               \StringTok{"metric\_params"}\NormalTok{: \{}\StringTok{"k"}\NormalTok{: }\DecValTok{3}\NormalTok{\},}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluation-on-hold-out-data}{%
\subsection{Evaluation on Hold-out
Data}\label{evaluation-on-hold-out-data}}

\begin{itemize}
\tightlist
\item
  The default method for computing the performance is
  \texttt{"eval\_holdout"}.
\item
  Alternatively, cross-validation can be used for every machine learning
  model.
\item
  Specifically for RandomForests, the OOB-score can be used.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
    \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_hold\_out"}\NormalTok{,}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-oob-score-16}{%
\subsection{OOB Score}\label{sec-oob-score-16}}

Using the OOB-Score is a very efficient way to estimate the performance
of a random forest classifier. The OOB-Score is calculated on the
training data and does not require a hold-out test set. If the OOB-Score
is used, the key ``eval'' in the \texttt{fun\_control} dictionary should
be set to \texttt{"oob\_score"} as shown below.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{OOB-Score}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

In addition to setting the key \texttt{"eval"} in the
\texttt{fun\_control} dictionary to \texttt{"oob\_score"}, the keys
\texttt{"oob\_score"} and \texttt{"bootstrap"} have to be set to
\texttt{True}, because the OOB-Score requires the bootstrap method.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  Uncomment the following lines to use the OOB-Score:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
    \StringTok{"eval"}\NormalTok{: }\StringTok{"eval\_oob\_score"}\NormalTok{,}
\NormalTok{\})}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"bootstrap"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"oob\_score"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation}{%
\subsubsection{Cross Validation}\label{cross-validation}}

Instead of using the OOB-score, the classical cross validation can be
used. The number of folds is set by the key \texttt{"k\_folds"}. For
example, to use 5-fold cross validation, the key \texttt{"k\_folds"} is
set to \texttt{5}. Uncomment the following line to use cross validation:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fun\_control.update(\{}
\CommentTok{\#      "eval": "train\_cv",}
\CommentTok{\#      "k\_folds": 10,}
\CommentTok{\# \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-8-calling-the-spot-function-1}{%
\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-1}}

\hypertarget{sec-prepare-spot-call-16}{%
\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-16}}

\begin{itemize}
\tightlist
\item
  Get types and variable names as well as lower and upper bounds for the
  hyperparameters.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extract the variable types, names, and bounds}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (get\_bound\_values,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_var\_type,)}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name                     | type   | default   |   lower |   upper | transform              |
|--------------------------|--------|-----------|---------|---------|------------------------|
| n_estimators             | int    | 7         |       5 |   10    | transform_power_2_int  |
| criterion                | factor | gini      |       0 |    2    | None                   |
| max_depth                | int    | 10        |       1 |   20    | transform_power_2_int  |
| min_samples_split        | int    | 2         |       2 |  100    | None                   |
| min_samples_leaf         | int    | 1         |       1 |   25    | None                   |
| min_weight_fraction_leaf | float  | 0.0       |       0 |    0.01 | None                   |
| max_features             | factor | sqrt      |       0 |    1    | transform_none_to_None |
| max_leaf_nodes           | int    | 10        |       7 |   12    | transform_power_2_int  |
| min_impurity_decrease    | float  | 0.0       |       0 |    0.01 | None                   |
| bootstrap                | factor | 1         |       1 |    1    | None                   |
| oob_score                | factor | 0         |       1 |    1    | None                   |
\end{verbatim}

\hypertarget{sec-the-objective-function-16}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-16}}

The objective function is selected next. It implements an interface from
\texttt{sklearn}'s training, validation, and testing methods to
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperSklearn().fun\_sklearn}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer-1}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-1}}

\begin{itemize}
\tightlist
\item
  Run SPOT for approx. x mins (\texttt{max\_time}).
\item
  Note: the run takes longer, because the evaluation time of initial
  design (here: \texttt{initi\_size}, 20 points) is not considered.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{X\_start}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 7.,  0., 10.,  2.,  1.,  0.,  0., 10.,  0.,  1.,  0.]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                   n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{1}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{,}
                                      \StringTok{"log\_level"}\NormalTok{: }\DecValTok{50}
\NormalTok{                                      \})}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [----------] 1.49% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [----------] 2.41% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [----------] 3.48% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [#---------] 5.03% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [#---------] 6.91% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [#---------] 8.85% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [#---------] 10.85% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [#---------] 13.34% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [##--------] 15.16% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [##--------] 17.93% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [##--------] 19.82% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [##--------] 21.78% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [##--------] 24.68% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [###-------] 27.07% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [###-------] 29.11% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [###-------] 34.82% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [####------] 39.82% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [####------] 43.50% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8544973544973545 [#####-----] 46.63% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [#####-----] 49.56% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [######----] 57.97% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [#######---] 65.26% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [#######---] 72.69% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [########--] 82.95% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [#########-] 91.62% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [##########] 95.65% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [##########] 98.91% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8580246913580246 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2fe4ada90>
\end{verbatim}

\hypertarget{sec-tensorboard-16}{%
\section{Step 9: Tensorboard}\label{sec-tensorboard-16}}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard as described in Section~\ref{sec-tensorboard-14}, see
also the description in the documentation:
\href{https://sequential-parameter-optimization.github.io/spotPython/14_spot_ray_hpt_torch_cifar10.html\#sec-tensorboard-14}{Tensorboard.}

\hypertarget{sec-results-tuning-16}{%
\section{Step 10: Results}\label{sec-results-tuning-16}}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The following code generates
the progress plot from \textbf{?@fig-progress}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-27-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\begin{itemize}
\tightlist
\item
  Print the results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{    spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name                     | type   | default   |   lower |   upper |                tuned | transform              |   importance | stars   |
|--------------------------|--------|-----------|---------|---------|----------------------|------------------------|--------------|---------|
| n_estimators             | int    | 7         |     5.0 |    10.0 |                  7.0 | transform_power_2_int  |         0.73 | .       |
| criterion                | factor | gini      |     0.0 |     2.0 |                  1.0 | None                   |       100.00 | ***     |
| max_depth                | int    | 10        |     1.0 |    20.0 |                  4.0 | transform_power_2_int  |         0.20 | .       |
| min_samples_split        | int    | 2         |     2.0 |   100.0 |                  9.0 | None                   |         2.83 | *       |
| min_samples_leaf         | int    | 1         |     1.0 |    25.0 |                  1.0 | None                   |         0.74 | .       |
| min_weight_fraction_leaf | float  | 0.0       |     0.0 |    0.01 |                  0.0 | None                   |         0.00 |         |
| max_features             | factor | sqrt      |     0.0 |     1.0 |                  0.0 | transform_none_to_None |         0.00 |         |
| max_leaf_nodes           | int    | 10        |     7.0 |    12.0 |                 12.0 | transform_power_2_int  |         0.00 |         |
| min_impurity_decrease    | float  | 0.0       |     0.0 |    0.01 | 0.003665798756399038 | None                   |         1.06 | *       |
| bootstrap                | factor | 1         |     1.0 |     1.0 |                  1.0 | None                   |         0.00 |         |
| oob_score                | factor | 0         |     1.0 |     1.0 |                  1.0 | None                   |         0.00 |         |
\end{verbatim}

\hypertarget{show-variable-importance-1}{%
\subsection{Show variable importance}\label{show-variable-importance-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-29-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}

\hypertarget{get-default-hyperparameters-1}{%
\subsection{Get Default
Hyperparameters}\label{get-default-hyperparameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_values, transform\_hyper\_parameter\_values}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ get\_default\_values(fun\_control)}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ transform\_hyper\_parameter\_values(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, hyper\_parameter\_values}\OperatorTok{=}\NormalTok{values\_default)}
\NormalTok{values\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'n_estimators': 128,
 'criterion': 'gini',
 'max_depth': 1024,
 'min_samples_split': 2,
 'min_samples_leaf': 1,
 'min_weight_fraction_leaf': 0.0,
 'max_features': 'sqrt',
 'max_leaf_nodes': 1024,
 'min_impurity_decrease': 0.0,
 'bootstrap': 1,
 'oob_score': 0}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ make\_pipeline}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ make\_pipeline(fun\_control[}\StringTok{"prep\_model"}\NormalTok{], fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{values\_default))}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pipeline(steps=[('nonetype', None),
                ('randomforestclassifier',
                 RandomForestClassifier(bootstrap=1, max_depth=1024,
                                        max_leaf_nodes=1024, n_estimators=128,
                                        oob_score=0))])
\end{verbatim}

\hypertarget{get-spot-results-1}{%
\subsection{Get SPOT Results}\label{get-spot-results-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[7.00000000e+00 1.00000000e+00 4.00000000e+00 9.00000000e+00
  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.20000000e+01
  3.66579876e-03 1.00000000e+00 1.00000000e+00]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, return\_conf\_list\_from\_var\_dict}
\NormalTok{v\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, fun\_control[}\StringTok{"var\_name"}\NormalTok{])}
\NormalTok{return\_conf\_list\_from\_var\_dict(var\_dict}\OperatorTok{=}\NormalTok{v\_dict, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[{'n_estimators': 128,
  'criterion': 'entropy',
  'max_depth': 16,
  'min_samples_split': 9,
  'min_samples_leaf': 1,
  'min_weight_fraction_leaf': 0.0,
  'max_features': 'sqrt',
  'max_leaf_nodes': 4096,
  'min_impurity_decrease': 0.003665798756399038,
  'bootstrap': 1,
  'oob_score': 1}]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_sklearn\_model\_from\_X}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_sklearn\_model\_from\_X(X, fun\_control)}
\NormalTok{model\_spot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RandomForestClassifier(bootstrap=1, criterion='entropy', max_depth=16,
                       max_leaf_nodes=4096,
                       min_impurity_decrease=0.003665798756399038,
                       min_samples_split=9, n_estimators=128, oob_score=1)
\end{verbatim}

\hypertarget{evaluate-spot-results}{%
\subsection{Evaluate SPOT Results}\label{evaluate-spot-results}}

\begin{itemize}
\tightlist
\item
  Fetch the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.convert }\ImportTok{import}\NormalTok{ get\_Xy\_from\_df}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"train"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test, y\_test }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"test"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test.shape, y\_test.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
((63, 64), (63,))
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Fit the model with the tuned hyperparameters. This gives one result:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_spot.fit(X\_train, y\_train)}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_spot.predict\_proba(X\_test)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.8333333333333334
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ repeated\_eval(n, model):}
\NormalTok{    res\_values }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        model.fit(X\_train, y\_train)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ model.predict\_proba(X\_test)}
\NormalTok{        res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{        res\_values.append(res)}
\NormalTok{    mean\_res }\OperatorTok{=}\NormalTok{ np.mean(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"mean\_res: }\SpecialCharTok{\{}\NormalTok{mean\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    std\_res }\OperatorTok{=}\NormalTok{ np.std(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"std\_res: }\SpecialCharTok{\{}\NormalTok{std\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    min\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"min\_res: }\SpecialCharTok{\{}\NormalTok{min\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    max\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"max\_res: }\SpecialCharTok{\{}\NormalTok{max\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    median\_res }\OperatorTok{=}\NormalTok{ np.median(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"median\_res: }\SpecialCharTok{\{}\NormalTok{median\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ mean\_res, std\_res, min\_res, max\_res, median\_res}
\end{Highlighting}
\end{Shaded}

\hypertarget{handling-non-deterministic-results}{%
\subsection{Handling Non-deterministic
Results}\label{handling-non-deterministic-results}}

\begin{itemize}
\tightlist
\item
  Because the model is non-determinstic, we perform \(n=30\) runs and
  calculate the mean and standard deviation of the performance metric.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_spot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.8455908289241622
std_res: 0.011316719728387954
min_res: 0.8174603174603174
max_res: 0.8650793650793651
median_res: 0.8439153439153438
\end{verbatim}

\hypertarget{evalution-of-the-default-hyperparameters}{%
\subsection{Evalution of the Default
Hyperparameters}\label{evalution-of-the-default-hyperparameters}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_default.fit(X\_train, y\_train)[}\StringTok{"randomforestclassifier"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RandomForestClassifier(bootstrap=1, max_depth=1024, max_leaf_nodes=1024,
                       n_estimators=128, oob_score=0)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  One evaluation of the default hyperparameters is performed on the
  hold-out test set.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_default.predict\_proba(X\_test)}
\NormalTok{mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.8544973544973544
\end{verbatim}

Since one single evaluation is not meaningful, we perform, similar to
the evaluation of the SPOT results, \(n=30\) runs of the default setting
and and calculate the mean and standard deviation of the performance
metric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_default)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.853968253968254
std_res: 0.010480109006522471
min_res: 0.8253968253968254
max_res: 0.8783068783068783
median_res: 0.8544973544973544
\end{verbatim}

\hypertarget{plot-compare-predictions-1}{%
\subsection{Plot: Compare
Predictions}\label{plot-compare-predictions-1}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_confusion\_matrix}
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_default, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title }\OperatorTok{=} \StringTok{"Default"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-42-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-43-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{min}\NormalTok{(spot\_tuner.y), }\BuiltInTok{max}\NormalTok{(spot\_tuner.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(-0.8580246913580246, -0.6155202821869489)
\end{verbatim}

\hypertarget{cross-validated-evaluations}{%
\subsection{Cross-validated
Evaluations}\label{cross-validated-evaluations}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.sklearn.traintest }\ImportTok{import}\NormalTok{ evaluate\_cv}
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.8587231968810916, None)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"test\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in fun_sklearn(). Call to evaluate_cv failed. err=ValueError('n_splits=10 cannot be greater than the number of members in each class.'), type(err)=<class 'ValueError'>
\end{verbatim}

\begin{verbatim}
(nan, None)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  This is the evaluation that will be used in the comparison:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"data\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.8727435897435898, None)
\end{verbatim}

\hypertarget{detailed-hyperparameter-plots-1}{%
\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_estimators:  0.7339484812410658
criterion:  100.0
max_depth:  0.20179703307353752
min_samples_split:  2.8315205448442713
min_samples_leaf:  0.7369569202425469
min_impurity_decrease:  1.0619648661823429
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-2.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-3.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-4.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-5.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-6.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-7.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-8.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-9.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-10.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-11.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-12.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-13.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-14.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-15.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{018_spot_hpt_sklearn_multiclass_classification_randomforest_files/figure-pdf/cell-48-output-16.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plot-1}{%
\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-1}{%
\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-1}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-hpt-sklearn-xgb-classifier-vbdp-data}{%
\chapter{HPT: sklearn XGB Classifier VBDP
Data}\label{sec-hpt-sklearn-xgb-classifier-vbdp-data}}

This chapter describes the hyperparameter tuning of a
\texttt{HistGradientBoostingClassifier} on the Vector Borne Disease
Prediction (VBDP) data set.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Vector Borne Disease Prediction Data Set}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

This chapter uses the Vector Borne Disease Prediction data set from
Kaggle. It is a categorical dataset for eleven Vector Borne Diseases
with associated symptoms.

\begin{quote}
The person who associated a work with this deed has dedicated the work
to the public domain by waiving all of his or her rights to the work
worldwide under copyright law, including all related and neighboring
rights, to the extent allowed by law.You can copy, modify, distribute
and perform the work, even for commercial purposes, all without asking
permission. See Other Information below, see
\url{https://creativecommons.org/publicdomain/zero/1.0/}.
\end{quote}

The data set is available at:
\url{https://www.kaggle.com/datasets/richardbernat/vector-borne-disease-prediction},

The data should be downloaded and stored in the \texttt{data/VBDP}
subfolder. The data set is not available as a part of the
\texttt{spotPython} package.

\end{tcolorbox}

\hypertarget{sec-setup-17}{%
\section{Step 1: Setup}\label{sec-setup-17}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time and the initial design size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{ORIGINAL }\OperatorTok{=} \VariableTok{True}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"17"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-2-initialization-of-the-empty-fun_control-dictionary-2}{%
\section{\texorpdfstring{Step 2: Initialization of the Empty
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the Empty fun\_control Dictionary}}\label{step-2-initialization-of-the-empty-fun_control-dictionary-2}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}

\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    task}\OperatorTok{=}\StringTok{"classification"}\NormalTok{,}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-data-loading-17}{%
\section{Step 3: PyTorch Data Loading}\label{sec-data-loading-17}}

\hypertarget{load-data-classification-vbdp-1}{%
\subsection{1. Load Data: Classification
VBDP}\label{load-data-classification-vbdp-1}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ControlFlowTok{if}\NormalTok{ ORIGINAL }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/trainn.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    test\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/testt.csv\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/train.csv\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# remove the id column}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ train\_df.drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OrdinalEncoder}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{n\_features }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"prognosis"}
\CommentTok{\# Encoder our prognosis labels as integers for easier decoding later}
\NormalTok{enc }\OperatorTok{=}\NormalTok{ OrdinalEncoder()}
\NormalTok{train\_df[target\_column] }\OperatorTok{=}\NormalTok{ enc.fit\_transform(train\_df[[target\_column]])}
\NormalTok{train\_df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train\_df.shape)}
\NormalTok{train\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(252, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
2 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
3 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
4 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
\end{longtable}

The full data set \texttt{train\_df} 64 features. The target column is
labeled as \texttt{prognosis}.

\hypertarget{holdout-train-and-test-data-1}{%
\subsection{Holdout Train and Test
Data}\label{holdout-train-and-test-data-1}}

We split out a hold-out test set (25\% of the data) so we can calculate
an example MAP@K

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(train\_df.drop(target\_column, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{), train\_df[target\_column],}
\NormalTok{                                                    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{                                                    test\_size}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{                                                    stratify}\OperatorTok{=}\NormalTok{train\_df[target\_column])}
\NormalTok{train }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_train, np.array(y\_train).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{test }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_test, np.array(y\_test).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train.shape)}
\BuiltInTok{print}\NormalTok{(test.shape)}
\NormalTok{train.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(189, 65)
(63, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 7.0 \\
1 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & ... &
0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 3.0 \\
2 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 10.0 \\
3 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & ... &
1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 3.0 \\
4 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 8.0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}\StringTok{"data"}\NormalTok{: train\_df, }\CommentTok{\# full dataset,}
               \StringTok{"train"}\NormalTok{: train,}
               \StringTok{"test"}\NormalTok{: test,}
               \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
               \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-specification-of-preprocessing-model-17}{%
\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-17}}

Data preprocesssing can be very simple, e.g., you can ignore it. Then
you would choose the \texttt{prep\_model} ``None'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

A default approach for numerical data is the \texttt{StandardScaler}
(mean 0, variance 1). This can be selected as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# prep\_model = StandardScaler()}
\CommentTok{\# fun\_control.update(\{"prep\_model": prep\_model\})}
\end{Highlighting}
\end{Shaded}

Even more complicated pre-processing steps are possible, e.g., the
follwing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# categorical\_columns = []}
\CommentTok{\# one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore", sparse\_output=False)}
\CommentTok{\# prep\_model = ColumnTransformer(}
\CommentTok{\#         transformers=[}
\CommentTok{\#             ("categorical", one\_hot\_encoder, categorical\_columns),}
\CommentTok{\#         ],}
\CommentTok{\#         remainder=StandardScaler(),}
\CommentTok{\#     )}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-5-select-model-algorithm-and-core_model_hyper_dict-2}{%
\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{step-5-select-model-algorithm-and-core_model_hyper_dict-2}}

The selection of the algorithm (ML model) that should be tuned is done
by specifying the its name from the \texttt{sklearn} implementation. For
example, the \texttt{SVC} support vector machine classifier is selected
as follows:

\texttt{add\_core\_model\_to\_fun\_control(SVC,\ fun\_control,\ SklearnHyperDict)}

Other core\_models are, e.g.,:

\begin{itemize}
\tightlist
\item
  RidgeCV
\item
  GradientBoostingRegressor
\item
  ElasticNet
\item
  RandomForestClassifier
\item
  LogisticRegression
\item
  KNeighborsClassifier
\item
  RandomForestClassifier
\item
  GradientBoostingClassifier
\item
  HistGradientBoostingClassifier
\end{itemize}

We will use the \texttt{RandomForestClassifier} classifier in this
example.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ RidgeCV}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ HistGradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ ElasticNet}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotPython.data.sklearn\_hyper\_dict }\ImportTok{import}\NormalTok{ SklearnHyperDict}
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# core\_model  = RidgeCV}
\CommentTok{\# core\_model = GradientBoostingRegressor}
\CommentTok{\# core\_model = ElasticNet}
\NormalTok{core\_model }\OperatorTok{=}\NormalTok{ RandomForestClassifier}
\CommentTok{\# core\_model = SVC}
\CommentTok{\# core\_model = LogisticRegression}
\CommentTok{\# core\_model = KNeighborsClassifier}
\CommentTok{\# core\_model = GradientBoostingClassifier}
\NormalTok{core\_model }\OperatorTok{=}\NormalTok{ HistGradientBoostingClassifier}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{core\_model,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{SklearnHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now \texttt{fun\_control} has the information from the JSON file. The
available hyperparameters are:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\OperatorTok{*}\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{].keys(), sep}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
loss
learning_rate
max_iter
max_leaf_nodes
max_depth
min_samples_leaf
l2_regularization
max_bins
early_stopping
n_iter_no_change
tol
\end{verbatim}

\hypertarget{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-2}{%
\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-2}}

\hypertarget{modify-hyperparameter-of-type-numeric-and-integer-boolean-2}{%
\subsection{Modify hyperparameter of type numeric and integer
(boolean)}\label{modify-hyperparameter-of-type-numeric-and-integer-boolean-2}}

Numeric and boolean values can be modified using the
\texttt{modify\_hyper\_parameter\_bounds} method. For example, to change
the \texttt{tol} hyperparameter of the \texttt{SVC} model to the
interval {[}1e-3, 1e-2{]}, the following code can be used:

\texttt{modify\_hyper\_parameter\_bounds(fun\_control,\ "tol",\ bounds={[}1e-3,\ 1e-2{]})}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "tol", bounds=[1e{-}3, 1e{-}2])}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "min\_samples\_split", bounds=[3, 20])}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "dual", bounds=[0, 0])}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "probability", bounds=[1, 1])}
\CommentTok{\# fun\_control["core\_model\_hyper\_dict"]["tol"]}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "min\_samples\_leaf", bounds=[1, 25])}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "n\_estimators", bounds=[5, 10])}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyperparameter-of-type-factor-2}{%
\subsection{Modify hyperparameter of type
factor}\label{modify-hyperparameter-of-type-factor-2}}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modification-of-hyperparameters-14}.

Factors can be modified with the
\texttt{modify\_hyper\_parameter\_levels} function. For example, to
exclude the \texttt{sigmoid} kernel from the tuning, the \texttt{kernel}
hyperparameter of the \texttt{SVC} model can be modified as follows:

\texttt{modify\_hyper\_parameter\_levels(fun\_control,\ "kernel",\ {[}"linear",\ "rbf"{]})}

The new setting can be controlled via:

\texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}{[}"kernel"{]}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\CommentTok{\# XGBoost:}
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"loss"}\NormalTok{, [}\StringTok{"log\_loss"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-optimizers-17}{%
\subsection{Optimizers}\label{sec-optimizers-17}}

Optimizers are described in Section~\ref{sec-optimizers-14}.

\hypertarget{step-7-selection-of-the-objective-loss-function-2}{%
\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function-2}}

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

The evaluation procedure requires the specification of two elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the way how the data is split into a train and a test set and
\item
  the loss function (and a metric).
\end{enumerate}

\hypertarget{selection-of-the-objective-metric-and-loss-functions-1}{%
\subsection{Selection of the Objective: Metric and Loss
Functions}\label{selection-of-the-objective-metric-and-loss-functions-1}}

\begin{itemize}
\tightlist
\item
  Machine learning models are optimized with respect to a metric, for
  example, the \texttt{accuracy} function.
\item
  Deep learning, e.g., neural networks are optimized with respect to a
  loss function, for example, the \texttt{cross\_entropy} function and
  evaluated with respect to a metric, for example, the \texttt{accuracy}
  function.
\end{itemize}

\hypertarget{loss-function}{%
\subsection{Loss Function}\label{loss-function}}

The loss function, that is usually used in deep learning for optimizing
the weights of the net, is stored in the \texttt{fun\_control}
dictionary as \texttt{"loss\_function"}.

\hypertarget{metric-function-1}{%
\subsection{Metric Function}\label{metric-function-1}}

There are two different types of metrics in \texttt{spotPython}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{"metric\_river"} is used for the river based evaluation via
  \texttt{eval\_oml\_iter\_progressive}.
\item
  \texttt{"metric\_sklearn"} is used for the sklearn based evaluation.
\end{enumerate}

We will consider multi-class classification metrics, e.g.,
\texttt{mapk\_score} and \texttt{top\_k\_accuracy\_score}.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Predict Probabilities}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

In this multi-class classification example the machine learning
algorithm should return the probabilities of the specific classes
(\texttt{"predict\_proba"}) instead of the predicted values.

\end{tcolorbox}

We set \texttt{"predict\_proba"} to \texttt{True} in the
\texttt{fun\_control} dictionary.

\hypertarget{the-mapk-metric-1}{%
\subsubsection{The MAPK Metric}\label{the-mapk-metric-1}}

To select the MAPK metric, the following two entries can be added to the
\texttt{fun\_control} dictionary:

\texttt{"metric\_sklearn":\ mapk\_score"}

\texttt{"metric\_params":\ \{"k":\ 3\}}.

\hypertarget{other-metrics-1}{%
\subsubsection{Other Metrics}\label{other-metrics-1}}

Alternatively, other metrics for multi-class classification can be used,
e.g.,: * top\_k\_accuracy\_score or * roc\_auc\_score

The metric \texttt{roc\_auc\_score} requires the parameter
\texttt{"multi\_class"}, e.g.,

\texttt{"multi\_class":\ "ovr"}.

This is set in the \texttt{fun\_control} dictionary.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Weights}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotPython} performs a minimization, therefore, metrics that
should be maximized have to be multiplied by -1. This is done by setting
\texttt{"weights"} to \texttt{-1}.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  The complete setup for the metric in our example is:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.metrics }\ImportTok{import}\NormalTok{ mapk\_score}
\NormalTok{fun\_control.update(\{}
               \StringTok{"weights"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}
               \StringTok{"metric\_sklearn"}\NormalTok{: mapk\_score,}
               \StringTok{"predict\_proba"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
               \StringTok{"metric\_params"}\NormalTok{: \{}\StringTok{"k"}\NormalTok{: }\DecValTok{3}\NormalTok{\},}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluation-on-hold-out-data-1}{%
\subsection{Evaluation on Hold-out
Data}\label{evaluation-on-hold-out-data-1}}

\begin{itemize}
\tightlist
\item
  The default method for computing the performance is
  \texttt{"eval\_holdout"}.
\item
  Alternatively, cross-validation can be used for every machine learning
  model.
\item
  Specifically for RandomForests, the OOB-score can be used.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
    \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_hold\_out"}\NormalTok{,}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation-1}{%
\subsubsection{Cross Validation}\label{cross-validation-1}}

Instead of using the OOB-score, the classical cross validation can be
used. The number of folds is set by the key \texttt{"k\_folds"}. For
example, to use 5-fold cross validation, the key \texttt{"k\_folds"} is
set to \texttt{5}. Uncomment the following line to use cross validation:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fun\_control.update(\{}
\CommentTok{\#      "eval": "train\_cv",}
\CommentTok{\#      "k\_folds": 10,}
\CommentTok{\# \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-8-calling-the-spot-function-2}{%
\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-2}}

\hypertarget{sec-prepare-spot-call-17}{%
\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-17}}

\begin{itemize}
\tightlist
\item
  Get types and variable names as well as lower and upper bounds for the
  hyperparameters.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extract the variable types, names, and bounds}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (get\_bound\_values,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_var\_type,)}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name              | type   | default   |   lower |   upper | transform             |
|-------------------|--------|-----------|---------|---------|-----------------------|
| loss              | factor | log_loss  |   0     |   0     | None                  |
| learning_rate     | float  | -1.0      |  -5     |   0     | transform_power_10    |
| max_iter          | int    | 7         |   3     |  10     | transform_power_2_int |
| max_leaf_nodes    | int    | 5         |   1     |  12     | transform_power_2_int |
| max_depth         | int    | 2         |   1     |  20     | transform_power_2_int |
| min_samples_leaf  | int    | 4         |   2     |  10     | transform_power_2_int |
| l2_regularization | float  | 0.0       |   0     |  10     | None                  |
| max_bins          | int    | 255       | 127     | 255     | None                  |
| early_stopping    | factor | 1         |   0     |   1     | None                  |
| n_iter_no_change  | int    | 10        |   5     |  20     | None                  |
| tol               | float  | 0.0001    |   1e-05 |   0.001 | None                  |
\end{verbatim}

\hypertarget{sec-the-objective-function-17}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-17}}

The objective function is selected next. It implements an interface from
\texttt{sklearn}'s training, validation, and testing methods to
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperSklearn().fun\_sklearn}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer-2}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-2}}

\begin{itemize}
\tightlist
\item
  Run SPOT for approx. x mins (\texttt{max\_time}).
\item
  Note: the run takes longer, because the evaluation time of initial
  design (here: \texttt{initi\_size}, 20 points) is not considered.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{X\_start}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 0.00e+00, -1.00e+00,  7.00e+00,  5.00e+00,  2.00e+00,  4.00e+00,
         0.00e+00,  2.55e+02,  1.00e+00,  1.00e+01,  1.00e-04]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                   n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{1}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{,}
                                      \StringTok{"log\_level"}\NormalTok{: }\DecValTok{50}
\NormalTok{                                      \})}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: -0.84375 [##--------] 19.95% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.84375 [####------] 38.63% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.84375 [####------] 40.63% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.84375 [#####-----] 50.14% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.84375 [#######---] 71.79% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8645833333333334 [########--] 84.07% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8645833333333334 [#########-] 87.72% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.8645833333333334 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2febcd250>
\end{verbatim}

\hypertarget{sec-tensorboard-17}{%
\section{Step 9: Tensorboard}\label{sec-tensorboard-17}}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard as described in Section~\ref{sec-tensorboard-14}, see
also the description in the documentation:
\href{https://sequential-parameter-optimization.github.io/spotPython/14_spot_ray_hpt_torch_cifar10.html\#sec-tensorboard-14}{Tensorboard.}

\hypertarget{sec-results-tuning-17}{%
\section{Step 10: Results}\label{sec-results-tuning-17}}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The following code generates
the progress plot from \textbf{?@fig-progress}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-25-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\begin{itemize}
\tightlist
\item
  Print the results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{    spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name              | type   | default   |   lower |   upper |              tuned | transform             |   importance | stars   |
|-------------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|
| loss              | factor | log_loss  |     0.0 |     0.0 |                0.0 | None                  |         0.00 |         |
| learning_rate     | float  | -1.0      |    -5.0 |     0.0 | -0.744606058195489 | transform_power_10    |        25.04 | *       |
| max_iter          | int    | 7         |     3.0 |    10.0 |                8.0 | transform_power_2_int |         2.01 | *       |
| max_leaf_nodes    | int    | 5         |     1.0 |    12.0 |               12.0 | transform_power_2_int |         0.00 |         |
| max_depth         | int    | 2         |     1.0 |    20.0 |                2.0 | transform_power_2_int |         0.00 |         |
| min_samples_leaf  | int    | 4         |     2.0 |    10.0 |                2.0 | transform_power_2_int |       100.00 | ***     |
| l2_regularization | float  | 0.0       |     0.0 |    10.0 |                0.0 | None                  |         0.09 |         |
| max_bins          | int    | 255       |   127.0 |   255.0 |              204.0 | None                  |         0.00 |         |
| early_stopping    | factor | 1         |     0.0 |     1.0 |                1.0 | None                  |         5.07 | *       |
| n_iter_no_change  | int    | 10        |     5.0 |    20.0 |               20.0 | None                  |         0.02 |         |
| tol               | float  | 0.0001    |   1e-05 |   0.001 |              0.001 | None                  |         0.01 |         |
\end{verbatim}

\hypertarget{show-variable-importance-2}{%
\subsection{Show variable importance}\label{show-variable-importance-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-27-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}

\hypertarget{get-default-hyperparameters-2}{%
\subsection{Get Default
Hyperparameters}\label{get-default-hyperparameters-2}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_values, transform\_hyper\_parameter\_values}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ get\_default\_values(fun\_control)}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ transform\_hyper\_parameter\_values(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, hyper\_parameter\_values}\OperatorTok{=}\NormalTok{values\_default)}
\NormalTok{values\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'loss': 'log_loss',
 'learning_rate': 0.1,
 'max_iter': 128,
 'max_leaf_nodes': 32,
 'max_depth': 4,
 'min_samples_leaf': 16,
 'l2_regularization': 0.0,
 'max_bins': 255,
 'early_stopping': 1,
 'n_iter_no_change': 10,
 'tol': 0.0001}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ make\_pipeline}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ make\_pipeline(fun\_control[}\StringTok{"prep\_model"}\NormalTok{], fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{values\_default))}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pipeline(steps=[('nonetype', None),
                ('histgradientboostingclassifier',
                 HistGradientBoostingClassifier(early_stopping=1, max_depth=4,
                                                max_iter=128, max_leaf_nodes=32,
                                                min_samples_leaf=16,
                                                tol=0.0001))])
\end{verbatim}

\hypertarget{get-spot-results-2}{%
\subsection{Get SPOT Results}\label{get-spot-results-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.00000000e+00 -7.44606058e-01  8.00000000e+00  1.20000000e+01
   2.00000000e+00  2.00000000e+00  0.00000000e+00  2.04000000e+02
   1.00000000e+00  2.00000000e+01  1.00000000e-03]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, return\_conf\_list\_from\_var\_dict}
\NormalTok{v\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, fun\_control[}\StringTok{"var\_name"}\NormalTok{])}
\NormalTok{return\_conf\_list\_from\_var\_dict(var\_dict}\OperatorTok{=}\NormalTok{v\_dict, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[{'loss': 'log_loss',
  'learning_rate': 0.1800503383382619,
  'max_iter': 256,
  'max_leaf_nodes': 4096,
  'max_depth': 4,
  'min_samples_leaf': 4,
  'l2_regularization': 0.0,
  'max_bins': 204,
  'early_stopping': 1,
  'n_iter_no_change': 20,
  'tol': 0.001}]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_sklearn\_model\_from\_X}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_sklearn\_model\_from\_X(X, fun\_control)}
\NormalTok{model\_spot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
HistGradientBoostingClassifier(early_stopping=1,
                               learning_rate=0.1800503383382619, max_bins=204,
                               max_depth=4, max_iter=256, max_leaf_nodes=4096,
                               min_samples_leaf=4, n_iter_no_change=20,
                               tol=0.001)
\end{verbatim}

\hypertarget{evaluate-spot-results-1}{%
\subsection{Evaluate SPOT Results}\label{evaluate-spot-results-1}}

\begin{itemize}
\tightlist
\item
  Fetch the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.convert }\ImportTok{import}\NormalTok{ get\_Xy\_from\_df}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"train"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test, y\_test }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"test"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test.shape, y\_test.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
((63, 64), (63,))
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Fit the model with the tuned hyperparameters. This gives one result:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_spot.fit(X\_train, y\_train)}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_spot.predict\_proba(X\_test)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.7883597883597884
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ repeated\_eval(n, model):}
\NormalTok{    res\_values }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        model.fit(X\_train, y\_train)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ model.predict\_proba(X\_test)}
\NormalTok{        res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{        res\_values.append(res)}
\NormalTok{    mean\_res }\OperatorTok{=}\NormalTok{ np.mean(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"mean\_res: }\SpecialCharTok{\{}\NormalTok{mean\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    std\_res }\OperatorTok{=}\NormalTok{ np.std(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"std\_res: }\SpecialCharTok{\{}\NormalTok{std\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    min\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"min\_res: }\SpecialCharTok{\{}\NormalTok{min\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    max\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"max\_res: }\SpecialCharTok{\{}\NormalTok{max\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    median\_res }\OperatorTok{=}\NormalTok{ np.median(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"median\_res: }\SpecialCharTok{\{}\NormalTok{median\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ mean\_res, std\_res, min\_res, max\_res, median\_res}
\end{Highlighting}
\end{Shaded}

\hypertarget{handling-non-deterministic-results-1}{%
\subsection{Handling Non-deterministic
Results}\label{handling-non-deterministic-results-1}}

\begin{itemize}
\tightlist
\item
  Because the model is non-determinstic, we perform \(n=30\) runs and
  calculate the mean and standard deviation of the performance metric.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_spot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.7916225749559083
std_res: 0.012918495037590494
min_res: 0.7671957671957672
max_res: 0.8148148148148149
median_res: 0.791005291005291
\end{verbatim}

\hypertarget{evalution-of-the-default-hyperparameters-1}{%
\subsection{Evalution of the Default
Hyperparameters}\label{evalution-of-the-default-hyperparameters-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_default.fit(X\_train, y\_train)[}\StringTok{"histgradientboostingclassifier"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
HistGradientBoostingClassifier(early_stopping=1, max_depth=4, max_iter=128,
                               max_leaf_nodes=32, min_samples_leaf=16,
                               tol=0.0001)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  One evaluation of the default hyperparameters is performed on the
  hold-out test set.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_default.predict\_proba(X\_test)}
\NormalTok{mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.783068783068783
\end{verbatim}

Since one single evaluation is not meaningful, we perform, similar to
the evaluation of the SPOT results, \(n=30\) runs of the default setting
and and calculate the mean and standard deviation of the performance
metric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_default)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.7910934744268077
std_res: 0.014188764587985373
min_res: 0.7592592592592592
max_res: 0.8121693121693121
median_res: 0.7936507936507936
\end{verbatim}

\hypertarget{plot-compare-predictions-2}{%
\subsection{Plot: Compare
Predictions}\label{plot-compare-predictions-2}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_confusion\_matrix}
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_default, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title }\OperatorTok{=} \StringTok{"Default"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-40-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-41-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{min}\NormalTok{(spot\_tuner.y), }\BuiltInTok{max}\NormalTok{(spot\_tuner.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(-0.8645833333333334, -0.06944444444444443)
\end{verbatim}

\hypertarget{cross-validated-evaluations-1}{%
\subsection{Cross-validated
Evaluations}\label{cross-validated-evaluations-1}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.sklearn.traintest }\ImportTok{import}\NormalTok{ evaluate\_cv}
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.8142787524366473, None)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"test\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in fun_sklearn(). Call to evaluate_cv failed. err=ValueError('n_splits=10 cannot be greater than the number of members in each class.'), type(err)=<class 'ValueError'>
\end{verbatim}

\begin{verbatim}
(nan, None)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  This is the evaluation that will be used in the comparison:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"data\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.8374615384615384, None)
\end{verbatim}

\hypertarget{detailed-hyperparameter-plots-2}{%
\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
learning_rate:  25.038151098863793
max_iter:  2.0061454393150147
min_samples_leaf:  99.99999999999999
l2_regularization:  0.08861356049567441
early_stopping:  5.07201008659328
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-2.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-3.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-4.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-5.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-6.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-7.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-8.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-9.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-10.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{019_spot_hpt_sklearn_multiclass_classification_xgb_files/figure-pdf/cell-46-output-11.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plot-2}{%
\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-2}{%
\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-2}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-hpt-sklearn-svc-vbdp-data}{%
\chapter{HPT: sklearn SVC VBDP
Data}\label{sec-hpt-sklearn-svc-vbdp-data}}

This chapter describes the hyperparameter tuning of a \texttt{SVC} on
the Vector Borne Disease Prediction (VBDP) data set.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Vector Borne Disease Prediction Data Set}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

This chapter uses the Vector Borne Disease Prediction data set from
Kaggle. It is a categorical dataset for eleven Vector Borne Diseases
with associated symptoms.

\begin{quote}
The person who associated a work with this deed has dedicated the work
to the public domain by waiving all of his or her rights to the work
worldwide under copyright law, including all related and neighboring
rights, to the extent allowed by law.You can copy, modify, distribute
and perform the work, even for commercial purposes, all without asking
permission. See Other Information below, see
\url{https://creativecommons.org/publicdomain/zero/1.0/}.
\end{quote}

The data set is available at:
\url{https://www.kaggle.com/datasets/richardbernat/vector-borne-disease-prediction},

The data should be downloaded and stored in the \texttt{data/VBDP}
subfolder. The data set is not available as a part of the
\texttt{spotPython} package.

\end{tcolorbox}

\hypertarget{sec-setup-18}{%
\section{Step 1: Setup}\label{sec-setup-18}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time and the initial design size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{ORIGINAL }\OperatorTok{=} \VariableTok{True}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"18"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-2-initialization-of-the-empty-fun_control-dictionary-3}{%
\section{\texorpdfstring{Step 2: Initialization of the Empty
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the Empty fun\_control Dictionary}}\label{step-2-initialization-of-the-empty-fun_control-dictionary-3}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}

\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    task}\OperatorTok{=}\StringTok{"classification"}\NormalTok{,}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-3-pytorch-data-loading-1}{%
\section{Step 3: PyTorch Data
Loading}\label{step-3-pytorch-data-loading-1}}

\hypertarget{load-data-classification-vbdp-2}{%
\subsection{1. Load Data: Classification
VBDP}\label{load-data-classification-vbdp-2}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ControlFlowTok{if}\NormalTok{ ORIGINAL }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/trainn.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    test\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/testt.csv\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/train.csv\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# remove the id column}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ train\_df.drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OrdinalEncoder}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{n\_features }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"prognosis"}
\CommentTok{\# Encoder our prognosis labels as integers for easier decoding later}
\NormalTok{enc }\OperatorTok{=}\NormalTok{ OrdinalEncoder()}
\NormalTok{train\_df[target\_column] }\OperatorTok{=}\NormalTok{ enc.fit\_transform(train\_df[[target\_column]])}
\NormalTok{train\_df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train\_df.shape)}
\NormalTok{train\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(252, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
2 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
3 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
4 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
\end{longtable}

The full data set \texttt{train\_df} 64 features. The target column is
labeled as \texttt{prognosis}.

\hypertarget{holdout-train-and-test-data-2}{%
\subsection{Holdout Train and Test
Data}\label{holdout-train-and-test-data-2}}

We split out a hold-out test set (25\% of the data) so we can calculate
an example MAP@K

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(train\_df.drop(target\_column, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{), train\_df[target\_column],}
\NormalTok{                                                    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{                                                    test\_size}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{                                                    stratify}\OperatorTok{=}\NormalTok{train\_df[target\_column])}
\NormalTok{train }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_train, np.array(y\_train).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{test }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_test, np.array(y\_test).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train.shape)}
\BuiltInTok{print}\NormalTok{(test.shape)}
\NormalTok{train.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(189, 65)
(63, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 7.0 \\
1 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & ... &
0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 3.0 \\
2 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 10.0 \\
3 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & ... &
1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 3.0 \\
4 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 8.0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}\StringTok{"data"}\NormalTok{: train\_df, }\CommentTok{\# full dataset,}
               \StringTok{"train"}\NormalTok{: train,}
               \StringTok{"test"}\NormalTok{: test,}
               \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
               \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-specification-of-preprocessing-model-18}{%
\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-18}}

Data preprocesssing can be very simple, e.g., you can ignore it. Then
you would choose the \texttt{prep\_model} ``None'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

A default approach for numerical data is the \texttt{StandardScaler}
(mean 0, variance 1). This can be selected as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# prep\_model = StandardScaler()}
\CommentTok{\# fun\_control.update(\{"prep\_model": prep\_model\})}
\end{Highlighting}
\end{Shaded}

Even more complicated pre-processing steps are possible, e.g., the
follwing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# categorical\_columns = []}
\CommentTok{\# one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore", sparse\_output=False)}
\CommentTok{\# prep\_model = ColumnTransformer(}
\CommentTok{\#         transformers=[}
\CommentTok{\#             ("categorical", one\_hot\_encoder, categorical\_columns),}
\CommentTok{\#         ],}
\CommentTok{\#         remainder=StandardScaler(),}
\CommentTok{\#     )}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-5-select-model-algorithm-and-core_model_hyper_dict-3}{%
\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{step-5-select-model-algorithm-and-core_model_hyper_dict-3}}

The selection of the algorithm (ML model) that should be tuned is done
by specifying the its name from the \texttt{sklearn} implementation. For
example, the \texttt{SVC} support vector machine classifier is selected
as follows:

\texttt{add\_core\_model\_to\_fun\_control(SVC,\ fun\_control,\ SklearnHyperDict)}

Other core\_models are, e.g.,:

\begin{itemize}
\tightlist
\item
  RidgeCV
\item
  GradientBoostingRegressor
\item
  ElasticNet
\item
  RandomForestClassifier
\item
  LogisticRegression
\item
  KNeighborsClassifier
\item
  RandomForestClassifier
\item
  GradientBoostingClassifier
\item
  HistGradientBoostingClassifier
\end{itemize}

We will use the \texttt{RandomForestClassifier} classifier in this
example.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ RidgeCV}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ HistGradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ ElasticNet}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotPython.data.sklearn\_hyper\_dict }\ImportTok{import}\NormalTok{ SklearnHyperDict}
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# core\_model  = RidgeCV}
\CommentTok{\# core\_model = GradientBoostingRegressor}
\CommentTok{\# core\_model = ElasticNet}
\CommentTok{\# core\_model = RandomForestClassifier}
\NormalTok{core\_model }\OperatorTok{=}\NormalTok{ SVC}
\CommentTok{\# core\_model = LogisticRegression}
\CommentTok{\# core\_model = KNeighborsClassifier}
\CommentTok{\# core\_model = GradientBoostingClassifier}
\CommentTok{\# core\_model = HistGradientBoostingClassifier}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{core\_model,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{SklearnHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now \texttt{fun\_control} has the information from the JSON file. The
available hyperparameters are:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\OperatorTok{*}\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{].keys(), sep}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
C
kernel
degree
gamma
coef0
shrinking
probability
tol
cache_size
break_ties
\end{verbatim}

\hypertarget{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-3}{%
\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-3}}

\hypertarget{modify-hyperparameter-of-type-numeric-and-integer-boolean-3}{%
\subsection{Modify hyperparameter of type numeric and integer
(boolean)}\label{modify-hyperparameter-of-type-numeric-and-integer-boolean-3}}

Numeric and boolean values can be modified using the
\texttt{modify\_hyper\_parameter\_bounds} method. For example, to change
the \texttt{tol} hyperparameter of the \texttt{SVC} model to the
interval {[}1e-3, 1e-2{]}, the following code can be used:

\texttt{modify\_hyper\_parameter\_bounds(fun\_control,\ "tol",\ bounds={[}1e-3,\ 1e-2{]})}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"probability"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyperparameter-of-type-factor-3}{%
\subsection{Modify hyperparameter of type
factor}\label{modify-hyperparameter-of-type-factor-3}}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modification-of-hyperparameters-14}.

Factors can be modified with the
\texttt{modify\_hyper\_parameter\_levels} function. For example, to
exclude the \texttt{sigmoid} kernel from the tuning, the \texttt{kernel}
hyperparameter of the \texttt{SVC} model can be modified as follows:

\texttt{modify\_hyper\_parameter\_levels(fun\_control,\ "kernel",\ {[}"linear",\ "rbf"{]})}

The new setting can be controlled via:

\texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}{[}"kernel"{]}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"kernel"}\NormalTok{, [}\StringTok{"rbf"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-optimizers-18}{%
\subsection{Optimizers}\label{sec-optimizers-18}}

Optimizers are described in Section~\ref{sec-optimizers-14}.

\hypertarget{selection-of-the-objective-metric-and-loss-functions-2}{%
\subsection{Selection of the Objective: Metric and Loss
Functions}\label{selection-of-the-objective-metric-and-loss-functions-2}}

\begin{itemize}
\tightlist
\item
  Machine learning models are optimized with respect to a metric, for
  example, the \texttt{accuracy} function.
\item
  Deep learning, e.g., neural networks are optimized with respect to a
  loss function, for example, the \texttt{cross\_entropy} function and
  evaluated with respect to a metric, for example, the \texttt{accuracy}
  function.
\end{itemize}

\hypertarget{step-7-selection-of-the-objective-loss-function-3}{%
\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function-3}}

The loss function, that is usually used in deep learning for optimizing
the weights of the net, is stored in the \texttt{fun\_control}
dictionary as \texttt{"loss\_function"}.

\hypertarget{metric-function-2}{%
\subsection{Metric Function}\label{metric-function-2}}

There are two different types of metrics in \texttt{spotPython}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{"metric\_river"} is used for the river based evaluation via
  \texttt{eval\_oml\_iter\_progressive}.
\item
  \texttt{"metric\_sklearn"} is used for the sklearn based evaluation.
\end{enumerate}

We will consider multi-class classification metrics, e.g.,
\texttt{mapk\_score} and \texttt{top\_k\_accuracy\_score}.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Predict Probabilities}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

In this multi-class classification example the machine learning
algorithm should return the probabilities of the specific classes
(\texttt{"predict\_proba"}) instead of the predicted values.

\end{tcolorbox}

We set \texttt{"predict\_proba"} to \texttt{True} in the
\texttt{fun\_control} dictionary.

\hypertarget{the-mapk-metric-2}{%
\subsubsection{The MAPK Metric}\label{the-mapk-metric-2}}

To select the MAPK metric, the following two entries can be added to the
\texttt{fun\_control} dictionary:

\texttt{"metric\_sklearn":\ mapk\_score"}

\texttt{"metric\_params":\ \{"k":\ 3\}}.

\hypertarget{other-metrics-2}{%
\subsubsection{Other Metrics}\label{other-metrics-2}}

Alternatively, other metrics for multi-class classification can be used,
e.g.,: * top\_k\_accuracy\_score or * roc\_auc\_score

The metric \texttt{roc\_auc\_score} requires the parameter
\texttt{"multi\_class"}, e.g.,

\texttt{"multi\_class":\ "ovr"}.

This is set in the \texttt{fun\_control} dictionary.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Weights}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotPython} performs a minimization, therefore, metrics that
should be maximized have to be multiplied by -1. This is done by setting
\texttt{"weights"} to \texttt{-1}.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  The complete setup for the metric in our example is:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.metrics }\ImportTok{import}\NormalTok{ mapk\_score}
\NormalTok{fun\_control.update(\{}
               \StringTok{"weights"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}
               \StringTok{"metric\_sklearn"}\NormalTok{: mapk\_score,}
               \StringTok{"predict\_proba"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
               \StringTok{"metric\_params"}\NormalTok{: \{}\StringTok{"k"}\NormalTok{: }\DecValTok{3}\NormalTok{\},}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluation-on-hold-out-data-2}{%
\subsection{Evaluation on Hold-out
Data}\label{evaluation-on-hold-out-data-2}}

\begin{itemize}
\tightlist
\item
  The default method for computing the performance is
  \texttt{"eval\_holdout"}.
\item
  Alternatively, cross-validation can be used for every machine learning
  model.
\item
  Specifically for RandomForests, the OOB-score can be used.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
    \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_hold\_out"}\NormalTok{,}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation-2}{%
\subsubsection{Cross Validation}\label{cross-validation-2}}

Instead of using the OOB-score, the classical cross validation can be
used. The number of folds is set by the key \texttt{"k\_folds"}. For
example, to use 5-fold cross validation, the key \texttt{"k\_folds"} is
set to \texttt{5}. Uncomment the following line to use cross validation:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fun\_control.update(\{}
\CommentTok{\#      "eval": "train\_cv",}
\CommentTok{\#      "k\_folds": 10,}
\CommentTok{\# \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-8-calling-the-spot-function-3}{%
\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-3}}

\hypertarget{sec-prepare-spot-call-18}{%
\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-18}}

\begin{itemize}
\tightlist
\item
  Get types and variable names as well as lower and upper bounds for the
  hyperparameters.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extract the variable types, names, and bounds}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (get\_bound\_values,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_var\_type,)}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name        | type   | default   |    lower |   upper | transform   |
|-------------|--------|-----------|----------|---------|-------------|
| C           | float  | 1.0       |   0.1    |   10    | None        |
| kernel      | factor | rbf       |   0      |    0    | None        |
| degree      | int    | 3         |   3      |    3    | None        |
| gamma       | factor | scale     |   0      |    1    | None        |
| coef0       | float  | 0.0       |   0      |    0    | None        |
| shrinking   | factor | 0         |   0      |    1    | None        |
| probability | factor | 0         |   1      |    1    | None        |
| tol         | float  | 0.001     |   0.0001 |    0.01 | None        |
| cache_size  | float  | 200.0     | 100      |  400    | None        |
| break_ties  | factor | 0         |   0      |    1    | None        |
\end{verbatim}

\hypertarget{sec-the-objective-function-18}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-18}}

The objective function is selected next. It implements an interface from
\texttt{sklearn}'s training, validation, and testing methods to
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperSklearn().fun\_sklearn}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer-3}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-3}}

\begin{itemize}
\tightlist
\item
  Run SPOT for approx. x mins (\texttt{max\_time}).
\item
  Note: the run takes longer, because the evaluation time of initial
  design (here: \texttt{initi\_size}, 20 points) is not considered.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{X\_start}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.e+00, 0.e+00, 3.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-03,
        2.e+02, 0.e+00]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                   n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{1}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{,}
                                      \StringTok{"log\_level"}\NormalTok{: }\DecValTok{50}
\NormalTok{                                      \})}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 0.52% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 1.03% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 1.45% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 1.81% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 2.59% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 3.09% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 3.89% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [----------] 4.69% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#---------] 5.64% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#---------] 7.12% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#---------] 8.68% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#---------] 9.40% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#---------] 11.09% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#---------] 12.99% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [##--------] 16.12% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [##--------] 19.65% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [##--------] 21.63% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [##--------] 23.45% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [##--------] 24.82% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [###-------] 26.16% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [###-------] 29.53% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [###-------] 33.05% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [####------] 37.03% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [####------] 40.35% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [####------] 44.11% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#####-----] 45.84% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#####-----] 47.73% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#####-----] 49.62% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [######----] 55.62% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [######----] 63.43% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#######---] 70.25% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [########--] 77.02% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [########--] 81.92% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [########--] 84.96% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#########-] 87.33% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [#########-] 90.08% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.875 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x3196d7e50>
\end{verbatim}

\hypertarget{sec-tensorboard-18}{%
\section{Step 9: Tensorboard}\label{sec-tensorboard-18}}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard as described in Section~\ref{sec-tensorboard-14}, see
also the description in the documentation:
\href{https://sequential-parameter-optimization.github.io/spotPython/14_spot_ray_hpt_torch_cifar10.html\#sec-tensorboard-14}{Tensorboard.}

\hypertarget{sec-results-tuning-18}{%
\section{Step 10: Results}\label{sec-results-tuning-18}}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The following code generates
the progress plot from \textbf{?@fig-progress}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{020_spot_hpt_sklearn_multiclass_classification_svc_files/figure-pdf/cell-25-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\begin{itemize}
\tightlist
\item
  Print the results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{    spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name        | type   | default   |   lower |   upper |                 tuned | transform   |   importance | stars   |
|-------------|--------|-----------|---------|---------|-----------------------|-------------|--------------|---------|
| C           | float  | 1.0       |     0.1 |    10.0 |     8.648943310768674 | None        |         0.02 |         |
| kernel      | factor | rbf       |     0.0 |     0.0 |                   0.0 | None        |         0.00 |         |
| degree      | int    | 3         |     3.0 |     3.0 |                   3.0 | None        |         0.00 |         |
| gamma       | factor | scale     |     0.0 |     1.0 |                   1.0 | None        |         0.05 |         |
| coef0       | float  | 0.0       |     0.0 |     0.0 |                   0.0 | None        |         0.00 |         |
| shrinking   | factor | 0         |     0.0 |     1.0 |                   0.0 | None        |         0.00 |         |
| probability | factor | 0         |     1.0 |     1.0 |                   1.0 | None        |         0.00 |         |
| tol         | float  | 0.001     |  0.0001 |    0.01 | 0.0036949438148166343 | None        |         0.18 | .       |
| cache_size  | float  | 200.0     |   100.0 |   400.0 |    389.44564593489815 | None        |         0.00 |         |
| break_ties  | factor | 0         |     0.0 |     1.0 |                   0.0 | None        |       100.00 | ***     |
\end{verbatim}

\hypertarget{show-variable-importance-3}{%
\subsection{Show variable importance}\label{show-variable-importance-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{020_spot_hpt_sklearn_multiclass_classification_svc_files/figure-pdf/cell-27-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}

\hypertarget{get-default-hyperparameters-3}{%
\subsection{Get Default
Hyperparameters}\label{get-default-hyperparameters-3}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_values, transform\_hyper\_parameter\_values}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ get\_default\_values(fun\_control)}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ transform\_hyper\_parameter\_values(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, hyper\_parameter\_values}\OperatorTok{=}\NormalTok{values\_default)}
\NormalTok{values\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'C': 1.0,
 'kernel': 'rbf',
 'degree': 3,
 'gamma': 'scale',
 'coef0': 0.0,
 'shrinking': 0,
 'probability': 0,
 'tol': 0.001,
 'cache_size': 200.0,
 'break_ties': 0}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ make\_pipeline}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ make\_pipeline(fun\_control[}\StringTok{"prep\_model"}\NormalTok{], fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{values\_default))}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pipeline(steps=[('nonetype', None),
                ('svc',
                 SVC(break_ties=0, cache_size=200.0, probability=0,
                     shrinking=0))])
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Default value for ``probability'' is False, but we need it to be True
  for the metric ``mapk\_score''.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{values\_default.update(\{}\StringTok{"probability"}\NormalTok{: }\DecValTok{1}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{get-spot-results-3}{%
\subsection{Get SPOT Results}\label{get-spot-results-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[8.64894331e+00 0.00000000e+00 3.00000000e+00 1.00000000e+00
  0.00000000e+00 0.00000000e+00 1.00000000e+00 3.69494381e-03
  3.89445646e+02 0.00000000e+00]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, return\_conf\_list\_from\_var\_dict}
\NormalTok{v\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, fun\_control[}\StringTok{"var\_name"}\NormalTok{])}
\NormalTok{return\_conf\_list\_from\_var\_dict(var\_dict}\OperatorTok{=}\NormalTok{v\_dict, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[{'C': 8.648943310768674,
  'kernel': 'rbf',
  'degree': 3,
  'gamma': 'auto',
  'coef0': 0.0,
  'shrinking': 0,
  'probability': 1,
  'tol': 0.0036949438148166343,
  'cache_size': 389.44564593489815,
  'break_ties': 0}]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_sklearn\_model\_from\_X}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_sklearn\_model\_from\_X(X, fun\_control)}
\NormalTok{model\_spot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
SVC(C=8.648943310768674, break_ties=0, cache_size=389.44564593489815,
    gamma='auto', probability=1, shrinking=0, tol=0.0036949438148166343)
\end{verbatim}

\hypertarget{evaluate-spot-results-2}{%
\subsection{Evaluate SPOT Results}\label{evaluate-spot-results-2}}

\begin{itemize}
\tightlist
\item
  Fetch the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.convert }\ImportTok{import}\NormalTok{ get\_Xy\_from\_df}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"train"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test, y\_test }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"test"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test.shape, y\_test.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
((63, 64), (63,))
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Fit the model with the tuned hyperparameters. This gives one result:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_spot.fit(X\_train, y\_train)}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_spot.predict\_proba(X\_test)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.8465608465608466
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ repeated\_eval(n, model):}
\NormalTok{    res\_values }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        model.fit(X\_train, y\_train)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ model.predict\_proba(X\_test)}
\NormalTok{        res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{        res\_values.append(res)}
\NormalTok{    mean\_res }\OperatorTok{=}\NormalTok{ np.mean(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"mean\_res: }\SpecialCharTok{\{}\NormalTok{mean\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    std\_res }\OperatorTok{=}\NormalTok{ np.std(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"std\_res: }\SpecialCharTok{\{}\NormalTok{std\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    min\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"min\_res: }\SpecialCharTok{\{}\NormalTok{min\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    max\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"max\_res: }\SpecialCharTok{\{}\NormalTok{max\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    median\_res }\OperatorTok{=}\NormalTok{ np.median(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"median\_res: }\SpecialCharTok{\{}\NormalTok{median\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ mean\_res, std\_res, min\_res, max\_res, median\_res}
\end{Highlighting}
\end{Shaded}

\hypertarget{handling-non-deterministic-results-2}{%
\subsection{Handling Non-deterministic
Results}\label{handling-non-deterministic-results-2}}

\begin{itemize}
\tightlist
\item
  Because the model is non-determinstic, we perform \(n=30\) runs and
  calculate the mean and standard deviation of the performance metric.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_spot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.8485890652557321
std_res: 0.0039050422972635584
min_res: 0.8386243386243387
max_res: 0.8571428571428571
median_res: 0.8465608465608466
\end{verbatim}

\hypertarget{evalution-of-the-default-hyperparameters-2}{%
\subsection{Evalution of the Default
Hyperparameters}\label{evalution-of-the-default-hyperparameters-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_default[}\StringTok{"svc"}\NormalTok{].probability }\OperatorTok{=} \VariableTok{True}
\NormalTok{model\_default.fit(X\_train, y\_train)[}\StringTok{"svc"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
SVC(break_ties=0, cache_size=200.0, probability=True, shrinking=0)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  One evaluation of the default hyperparameters is performed on the
  hold-out test set.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_default.predict\_proba(X\_test)}
\NormalTok{mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.8571428571428571
\end{verbatim}

Since one single evaluation is not meaningful, we perform, similar to
the evaluation of the SPOT results, \(n=30\) runs of the default setting
and and calculate the mean and standard deviation of the performance
metric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_default)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.854320987654321
std_res: 0.004207005446870034
min_res: 0.8492063492063492
max_res: 0.8650793650793651
median_res: 0.8571428571428571
\end{verbatim}

\hypertarget{plot-compare-predictions-3}{%
\subsection{Plot: Compare
Predictions}\label{plot-compare-predictions-3}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_confusion\_matrix}
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_default, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title }\OperatorTok{=} \StringTok{"Default"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{020_spot_hpt_sklearn_multiclass_classification_svc_files/figure-pdf/cell-41-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{020_spot_hpt_sklearn_multiclass_classification_svc_files/figure-pdf/cell-42-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{min}\NormalTok{(spot\_tuner.y), }\BuiltInTok{max}\NormalTok{(spot\_tuner.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(-0.875, -0.6631944444444444)
\end{verbatim}

\hypertarget{cross-validated-evaluations-2}{%
\subsection{Cross-validated
Evaluations}\label{cross-validated-evaluations-2}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.sklearn.traintest }\ImportTok{import}\NormalTok{ evaluate\_cv}
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.8513645224171539, None)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"test\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in fun_sklearn(). Call to evaluate_cv failed. err=ValueError('n_splits=10 cannot be greater than the number of members in each class.'), type(err)=<class 'ValueError'>
\end{verbatim}

\begin{verbatim}
(nan, None)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  This is the evaluation that will be used in the comparison:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"data\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.8807179487179487, None)
\end{verbatim}

\hypertarget{detailed-hyperparameter-plots-3}{%
\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
gamma:  0.047342720750464695
tol:  0.1821713271282166
break_ties:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{020_spot_hpt_sklearn_multiclass_classification_svc_files/figure-pdf/cell-47-output-2.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{020_spot_hpt_sklearn_multiclass_classification_svc_files/figure-pdf/cell-47-output-3.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{020_spot_hpt_sklearn_multiclass_classification_svc_files/figure-pdf/cell-47-output-4.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plot-3}{%
\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot(show}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-3}{%
\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-3}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-hpt-sklearn-knn-classifier-vbdp-data}{%
\chapter{HPT: sklearn KNN Classifier VBDP
Data}\label{sec-hpt-sklearn-knn-classifier-vbdp-data}}

This chapter describes the hyperparameter tuning of a
\texttt{KNeighborsClassifier} on the Vector Borne Disease Prediction
(VBDP) data set.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Vector Borne Disease Prediction Data Set}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

This chapter uses the Vector Borne Disease Prediction data set from
Kaggle. It is a categorical dataset for eleven Vector Borne Diseases
with associated symptoms.

\begin{quote}
The person who associated a work with this deed has dedicated the work
to the public domain by waiving all of his or her rights to the work
worldwide under copyright law, including all related and neighboring
rights, to the extent allowed by law.You can copy, modify, distribute
and perform the work, even for commercial purposes, all without asking
permission. See Other Information below, see
\url{https://creativecommons.org/publicdomain/zero/1.0/}.
\end{quote}

The data set is available at:
\url{https://www.kaggle.com/datasets/richardbernat/vector-borne-disease-prediction},

The data should be downloaded and stored in the \texttt{data/VBDP}
subfolder. The data set is not available as a part of the
\texttt{spotPython} package.

\end{tcolorbox}

\hypertarget{sec-setup-19}{%
\section{Step 1: Setup}\label{sec-setup-19}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time and the initial design size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{ORIGINAL }\OperatorTok{=} \VariableTok{True}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"19"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-2-initialization-of-the-empty-fun_control-dictionary-4}{%
\section{\texorpdfstring{Step 2: Initialization of the Empty
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the Empty fun\_control Dictionary}}\label{step-2-initialization-of-the-empty-fun_control-dictionary-4}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}

\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    task}\OperatorTok{=}\StringTok{"classification"}\NormalTok{,}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name))}
\end{Highlighting}
\end{Shaded}

\hypertarget{load-data-classification-vbdp-3}{%
\subsection{Load Data: Classification
VBDP}\label{load-data-classification-vbdp-3}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ControlFlowTok{if}\NormalTok{ ORIGINAL }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/trainn.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    test\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/testt.csv\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/train.csv\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# remove the id column}
\NormalTok{    train\_df }\OperatorTok{=}\NormalTok{ train\_df.drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OrdinalEncoder}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{n\_features }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"prognosis"}
\CommentTok{\# Encoder our prognosis labels as integers for easier decoding later}
\NormalTok{enc }\OperatorTok{=}\NormalTok{ OrdinalEncoder()}
\NormalTok{train\_df[target\_column] }\OperatorTok{=}\NormalTok{ enc.fit\_transform(train\_df[[target\_column]])}
\NormalTok{train\_df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train\_df.shape)}
\NormalTok{train\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(252, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
2 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
3 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
4 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & ... & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0.0 \\
\end{longtable}

The full data set \texttt{train\_df} 64 features. The target column is
labeled as \texttt{prognosis}.

\hypertarget{holdout-train-and-test-data-3}{%
\subsection{Holdout Train and Test
Data}\label{holdout-train-and-test-data-3}}

We split out a hold-out test set (25\% of the data) so we can calculate
an example MAP@K

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(train\_df.drop(target\_column, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{), train\_df[target\_column],}
\NormalTok{                                                    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{                                                    test\_size}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{                                                    stratify}\OperatorTok{=}\NormalTok{train\_df[target\_column])}
\NormalTok{train }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_train, np.array(y\_train).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{test }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_test, np.array(y\_test).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\BuiltInTok{print}\NormalTok{(train.shape)}
\BuiltInTok{print}\NormalTok{(test.shape)}
\NormalTok{train.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(189, 65)
(63, 65)
\end{verbatim}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x56 & x57 &
x58 & x59 & x60 & x61 & x62 & x63 & x64 & prognosis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 7.0 \\
1 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & ... &
0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 3.0 \\
2 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 10.0 \\
3 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & ... &
1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 3.0 \\
4 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 8.0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}\StringTok{"data"}\NormalTok{: train\_df, }\CommentTok{\# full dataset,}
               \StringTok{"train"}\NormalTok{: train,}
               \StringTok{"test"}\NormalTok{: test,}
               \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
               \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-specification-of-preprocessing-model-19}{%
\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-19}}

Data preprocesssing can be very simple, e.g., you can ignore it. Then
you would choose the \texttt{prep\_model} ``None'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

A default approach for numerical data is the \texttt{StandardScaler}
(mean 0, variance 1). This can be selected as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# prep\_model = StandardScaler()}
\CommentTok{\# fun\_control.update(\{"prep\_model": prep\_model\})}
\end{Highlighting}
\end{Shaded}

Even more complicated pre-processing steps are possible, e.g., the
follwing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# categorical\_columns = []}
\CommentTok{\# one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore", sparse\_output=False)}
\CommentTok{\# prep\_model = ColumnTransformer(}
\CommentTok{\#         transformers=[}
\CommentTok{\#             ("categorical", one\_hot\_encoder, categorical\_columns),}
\CommentTok{\#         ],}
\CommentTok{\#         remainder=StandardScaler(),}
\CommentTok{\#     )}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-5-select-model-algorithm-and-core_model_hyper_dict-4}{%
\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{step-5-select-model-algorithm-and-core_model_hyper_dict-4}}

The selection of the algorithm (ML model) that should be tuned is done
by specifying the its name from the \texttt{sklearn} implementation. For
example, the \texttt{SVC} support vector machine classifier is selected
as follows:

\texttt{add\_core\_model\_to\_fun\_control(SVC,\ fun\_control,\ SklearnHyperDict)}

Other core\_models are, e.g.,:

\begin{itemize}
\tightlist
\item
  RidgeCV
\item
  GradientBoostingRegressor
\item
  ElasticNet
\item
  RandomForestClassifier
\item
  LogisticRegression
\item
  KNeighborsClassifier
\item
  RandomForestClassifier
\item
  GradientBoostingClassifier
\item
  HistGradientBoostingClassifier
\end{itemize}

We will use the \texttt{RandomForestClassifier} classifier in this
example.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ RidgeCV}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ HistGradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ ElasticNet}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotPython.data.sklearn\_hyper\_dict }\ImportTok{import}\NormalTok{ SklearnHyperDict}
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# core\_model  = RidgeCV}
\CommentTok{\# core\_model = GradientBoostingRegressor}
\CommentTok{\# core\_model = ElasticNet}
\CommentTok{\# core\_model = RandomForestClassifier}
\NormalTok{core\_model }\OperatorTok{=}\NormalTok{ KNeighborsClassifier}
\CommentTok{\# core\_model = LogisticRegression}
\CommentTok{\# core\_model = KNeighborsClassifier}
\CommentTok{\# core\_model = GradientBoostingClassifier}
\CommentTok{\# core\_model = HistGradientBoostingClassifier}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{core\_model,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{SklearnHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now \texttt{fun\_control} has the information from the JSON file. The
available hyperparameters are:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\OperatorTok{*}\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{].keys(), sep}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_neighbors
weights
algorithm
leaf_size
p
\end{verbatim}

\hypertarget{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-4}{%
\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-4}}

\hypertarget{modify-hyperparameter-of-type-numeric-and-integer-boolean-4}{%
\subsection{Modify hyperparameter of type numeric and integer
(boolean)}\label{modify-hyperparameter-of-type-numeric-and-integer-boolean-4}}

Numeric and boolean values can be modified using the
\texttt{modify\_hyper\_parameter\_bounds} method. For example, to change
the \texttt{tol} hyperparameter of the \texttt{SVC} model to the
interval {[}1e-3, 1e-2{]}, the following code can be used:

\texttt{modify\_hyper\_parameter\_bounds(fun\_control,\ "tol",\ bounds={[}1e-3,\ 1e-2{]})}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from spotPython.hyperparameters.values import modify\_hyper\_parameter\_bounds}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "probability", bounds=[1, 1])}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyperparameter-of-type-factor-4}{%
\subsection{Modify hyperparameter of type
factor}\label{modify-hyperparameter-of-type-factor-4}}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modification-of-hyperparameters-14}.

Factors can be modified with the
\texttt{modify\_hyper\_parameter\_levels} function. For example, to
exclude the \texttt{sigmoid} kernel from the tuning, the \texttt{kernel}
hyperparameter of the \texttt{SVC} model can be modified as follows:

\texttt{modify\_hyper\_parameter\_levels(fun\_control,\ "kernel",\ {[}"linear",\ "rbf"{]})}

The new setting can be controlled via:

\texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}{[}"kernel"{]}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from spotPython.hyperparameters.values import modify\_hyper\_parameter\_levels}
\CommentTok{\# modify\_hyper\_parameter\_levels(fun\_control, "kernel", ["rbf"])}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-optimizers-19}{%
\subsection{Optimizers}\label{sec-optimizers-19}}

Optimizers are described in Section~\ref{sec-optimizers-14}.

\hypertarget{selection-of-the-objective-metric-and-loss-functions-3}{%
\subsection{Selection of the Objective: Metric and Loss
Functions}\label{selection-of-the-objective-metric-and-loss-functions-3}}

\begin{itemize}
\tightlist
\item
  Machine learning models are optimized with respect to a metric, for
  example, the \texttt{accuracy} function.
\item
  Deep learning, e.g., neural networks are optimized with respect to a
  loss function, for example, the \texttt{cross\_entropy} function and
  evaluated with respect to a metric, for example, the \texttt{accuracy}
  function.
\end{itemize}

\hypertarget{step-7-selection-of-the-objective-loss-function-4}{%
\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function-4}}

The loss function, that is usually used in deep learning for optimizing
the weights of the net, is stored in the \texttt{fun\_control}
dictionary as \texttt{"loss\_function"}.

\hypertarget{metric-function-3}{%
\subsection{Metric Function}\label{metric-function-3}}

There are two different types of metrics in \texttt{spotPython}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{"metric\_river"} is used for the river based evaluation via
  \texttt{eval\_oml\_iter\_progressive}.
\item
  \texttt{"metric\_sklearn"} is used for the sklearn based evaluation.
\end{enumerate}

We will consider multi-class classification metrics, e.g.,
\texttt{mapk\_score} and \texttt{top\_k\_accuracy\_score}.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Predict Probabilities}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

In this multi-class classification example the machine learning
algorithm should return the probabilities of the specific classes
(\texttt{"predict\_proba"}) instead of the predicted values.

\end{tcolorbox}

We set \texttt{"predict\_proba"} to \texttt{True} in the
\texttt{fun\_control} dictionary.

\hypertarget{the-mapk-metric-3}{%
\subsubsection{The MAPK Metric}\label{the-mapk-metric-3}}

To select the MAPK metric, the following two entries can be added to the
\texttt{fun\_control} dictionary:

\texttt{"metric\_sklearn":\ mapk\_score"}

\texttt{"metric\_params":\ \{"k":\ 3\}}.

\hypertarget{other-metrics-3}{%
\subsubsection{Other Metrics}\label{other-metrics-3}}

Alternatively, other metrics for multi-class classification can be used,
e.g.,: * top\_k\_accuracy\_score or * roc\_auc\_score

The metric \texttt{roc\_auc\_score} requires the parameter
\texttt{"multi\_class"}, e.g.,

\texttt{"multi\_class":\ "ovr"}.

This is set in the \texttt{fun\_control} dictionary.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Weights}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotPython} performs a minimization, therefore, metrics that
should be maximized have to be multiplied by -1. This is done by setting
\texttt{"weights"} to \texttt{-1}.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  The complete setup for the metric in our example is:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.metrics }\ImportTok{import}\NormalTok{ mapk\_score}
\NormalTok{fun\_control.update(\{}
               \StringTok{"weights"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}
               \StringTok{"metric\_sklearn"}\NormalTok{: mapk\_score,}
               \StringTok{"predict\_proba"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
               \StringTok{"metric\_params"}\NormalTok{: \{}\StringTok{"k"}\NormalTok{: }\DecValTok{3}\NormalTok{\},}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{evaluation-on-hold-out-data-3}{%
\subsection{Evaluation on Hold-out
Data}\label{evaluation-on-hold-out-data-3}}

\begin{itemize}
\tightlist
\item
  The default method for computing the performance is
  \texttt{"eval\_holdout"}.
\item
  Alternatively, cross-validation can be used for every machine learning
  model.
\item
  Specifically for RandomForests, the OOB-score can be used.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
    \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_hold\_out"}\NormalTok{,}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-validation-3}{%
\subsubsection{Cross Validation}\label{cross-validation-3}}

Instead of using the OOB-score, the classical cross validation can be
used. The number of folds is set by the key \texttt{"k\_folds"}. For
example, to use 5-fold cross validation, the key \texttt{"k\_folds"} is
set to \texttt{5}. Uncomment the following line to use cross validation:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fun\_control.update(\{}
\CommentTok{\#      "eval": "train\_cv",}
\CommentTok{\#      "k\_folds": 10,}
\CommentTok{\# \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-8-calling-the-spot-function-4}{%
\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-4}}

\hypertarget{sec-prepare-spot-call-19}{%
\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-19}}

\begin{itemize}
\tightlist
\item
  Get types and variable names as well as lower and upper bounds for the
  hyperparameters.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extract the variable types, names, and bounds}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (get\_bound\_values,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_var\_type,)}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name        | type   | default   |   lower |   upper | transform             |
|-------------|--------|-----------|---------|---------|-----------------------|
| n_neighbors | int    | 2         |       1 |       7 | transform_power_2_int |
| weights     | factor | uniform   |       0 |       1 | None                  |
| algorithm   | factor | auto      |       0 |       3 | None                  |
| leaf_size   | int    | 5         |       2 |       7 | transform_power_2_int |
| p           | int    | 2         |       1 |       2 | None                  |
\end{verbatim}

\hypertarget{sec-the-objective-function-19}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-19}}

The objective function is selected next. It implements an interface from
\texttt{sklearn}'s training, validation, and testing methods to
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperSklearn().fun\_sklearn}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer-4}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-4}}

\begin{itemize}
\tightlist
\item
  Run SPOT for approx. x mins (\texttt{max\_time}).
\item
  Note: the run takes longer, because the evaluation time of initial
  design (here: \texttt{initi\_size}, 20 points) is not considered.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{X\_start}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[2, 0, 0, 5, 2]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                   n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{1}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{,}
                                      \StringTok{"log\_level"}\NormalTok{: }\DecValTok{50}
\NormalTok{                                      \})}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: -0.71875 [----------] 0.71% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.71875 [----------] 1.41% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [----------] 2.19% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [----------] 2.61% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [----------] 3.02% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [----------] 3.50% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [----------] 4.03% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [----------] 4.77% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 5.32% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 5.83% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 6.34% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 7.38% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 8.57% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 9.66% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 10.71% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 12.10% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 13.22% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 14.01% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7326388888888888 [#---------] 15.00% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [##--------] 17.08% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [##--------] 19.63% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [##--------] 21.23% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [##--------] 23.89% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [###-------] 26.57% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [###-------] 27.90% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [###-------] 28.85% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [###-------] 29.88% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [###-------] 33.86% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [####------] 37.95% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [####------] 41.41% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [#####-----] 45.66% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [#####-----] 48.60% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [#####-----] 50.61% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [#####-----] 52.16% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [#####-----] 54.10% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [######----] 61.38% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [#######---] 73.15% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [#########-] 93.78% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -0.7465277777777777 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2f77b1350>
\end{verbatim}

\hypertarget{sec-tensorboard-19}{%
\section{Step 9: Tensorboard}\label{sec-tensorboard-19}}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard as described in Section~\ref{sec-tensorboard-14}, see
also the description in the documentation:
\href{https://sequential-parameter-optimization.github.io/spotPython/14_spot_ray_hpt_torch_cifar10.html\#sec-tensorboard-14}{Tensorboard.}

\hypertarget{sec-results-tuning-19}{%
\section{Step 10: Results}\label{sec-results-tuning-19}}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The following code generates
the progress plot from \textbf{?@fig-progress}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{021_spot_hpt_sklearn_multiclass_classification_knn_files/figure-pdf/cell-25-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\begin{itemize}
\tightlist
\item
  Print the results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{    spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name        | type   | default   |   lower |   upper |   tuned | transform             |   importance | stars   |
|-------------|--------|-----------|---------|---------|---------|-----------------------|--------------|---------|
| n_neighbors | int    | 2         |       1 |       7 |     3.0 | transform_power_2_int |         4.99 | *       |
| weights     | factor | uniform   |       0 |       1 |     0.0 | None                  |       100.00 | ***     |
| algorithm   | factor | auto      |       0 |       3 |     1.0 | None                  |         0.00 |         |
| leaf_size   | int    | 5         |       2 |       7 |     4.0 | transform_power_2_int |         0.00 |         |
| p           | int    | 2         |       1 |       2 |     2.0 | None                  |         0.01 |         |
\end{verbatim}

\hypertarget{show-variable-importance-4}{%
\subsection{Show variable importance}\label{show-variable-importance-4}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{021_spot_hpt_sklearn_multiclass_classification_knn_files/figure-pdf/cell-27-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}

\hypertarget{get-default-hyperparameters-4}{%
\subsection{Get Default
Hyperparameters}\label{get-default-hyperparameters-4}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_values, transform\_hyper\_parameter\_values}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ get\_default\_values(fun\_control)}
\NormalTok{values\_default }\OperatorTok{=}\NormalTok{ transform\_hyper\_parameter\_values(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, hyper\_parameter\_values}\OperatorTok{=}\NormalTok{values\_default)}
\NormalTok{values\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'n_neighbors': 4,
 'weights': 'uniform',
 'algorithm': 'auto',
 'leaf_size': 32,
 'p': 2}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ make\_pipeline}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ make\_pipeline(fun\_control[}\StringTok{"prep\_model"}\NormalTok{], fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{values\_default))}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pipeline(steps=[('nonetype', None),
                ('kneighborsclassifier',
                 KNeighborsClassifier(leaf_size=32, n_neighbors=4))])
\end{verbatim}

\hypertarget{get-spot-results-4}{%
\subsection{Get SPOT Results}\label{get-spot-results-4}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[3. 0. 1. 4. 2.]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, return\_conf\_list\_from\_var\_dict}
\NormalTok{v\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, fun\_control[}\StringTok{"var\_name"}\NormalTok{])}
\NormalTok{return\_conf\_list\_from\_var\_dict(var\_dict}\OperatorTok{=}\NormalTok{v\_dict, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[{'n_neighbors': 8,
  'weights': 'uniform',
  'algorithm': 'ball_tree',
  'leaf_size': 16,
  'p': 2}]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_sklearn\_model\_from\_X}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_sklearn\_model\_from\_X(X, fun\_control)}
\NormalTok{model\_spot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
KNeighborsClassifier(algorithm='ball_tree', leaf_size=16, n_neighbors=8)
\end{verbatim}

\hypertarget{evaluate-spot-results-3}{%
\subsection{Evaluate SPOT Results}\label{evaluate-spot-results-3}}

\begin{itemize}
\tightlist
\item
  Fetch the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.convert }\ImportTok{import}\NormalTok{ get\_Xy\_from\_df}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"train"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test, y\_test }\OperatorTok{=}\NormalTok{ get\_Xy\_from\_df(fun\_control[}\StringTok{"test"}\NormalTok{], fun\_control[}\StringTok{"target\_column"}\NormalTok{])}
\NormalTok{X\_test.shape, y\_test.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
((63, 64), (63,))
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Fit the model with the tuned hyperparameters. This gives one result:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_spot.fit(X\_train, y\_train)}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_spot.predict\_proba(X\_test)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.7010582010582012
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ repeated\_eval(n, model):}
\NormalTok{    res\_values }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        model.fit(X\_train, y\_train)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ model.predict\_proba(X\_test)}
\NormalTok{        res }\OperatorTok{=}\NormalTok{ mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{        res\_values.append(res)}
\NormalTok{    mean\_res }\OperatorTok{=}\NormalTok{ np.mean(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"mean\_res: }\SpecialCharTok{\{}\NormalTok{mean\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    std\_res }\OperatorTok{=}\NormalTok{ np.std(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"std\_res: }\SpecialCharTok{\{}\NormalTok{std\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    min\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"min\_res: }\SpecialCharTok{\{}\NormalTok{min\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    max\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"max\_res: }\SpecialCharTok{\{}\NormalTok{max\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    median\_res }\OperatorTok{=}\NormalTok{ np.median(res\_values)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"median\_res: }\SpecialCharTok{\{}\NormalTok{median\_res}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ mean\_res, std\_res, min\_res, max\_res, median\_res}
\end{Highlighting}
\end{Shaded}

\hypertarget{handling-non-deterministic-results-3}{%
\subsection{Handling Non-deterministic
Results}\label{handling-non-deterministic-results-3}}

\begin{itemize}
\tightlist
\item
  Because the model is non-determinstic, we perform \(n=30\) runs and
  calculate the mean and standard deviation of the performance metric.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_spot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.7010582010582015
std_res: 3.3306690738754696e-16
min_res: 0.7010582010582012
max_res: 0.7010582010582012
median_res: 0.7010582010582012
\end{verbatim}

\hypertarget{evalution-of-the-default-hyperparameters-3}{%
\subsection{Evalution of the Default
Hyperparameters}\label{evalution-of-the-default-hyperparameters-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_default.fit(X\_train, y\_train)[}\StringTok{"kneighborsclassifier"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
KNeighborsClassifier(leaf_size=32, n_neighbors=4)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  One evaluation of the default hyperparameters is performed on the
  hold-out test set.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model\_default.predict\_proba(X\_test)}
\NormalTok{mapk\_score(y\_true}\OperatorTok{=}\NormalTok{y\_test, y\_pred}\OperatorTok{=}\NormalTok{y\_pred, k}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.6878306878306879
\end{verbatim}

Since one single evaluation is not meaningful, we perform, similar to
the evaluation of the SPOT results, \(n=30\) runs of the default setting
and and calculate the mean and standard deviation of the performance
metric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ repeated\_eval(}\DecValTok{30}\NormalTok{, model\_default)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean_res: 0.6878306878306877
std_res: 2.220446049250313e-16
min_res: 0.6878306878306879
max_res: 0.6878306878306879
median_res: 0.6878306878306879
\end{verbatim}

\hypertarget{plot-compare-predictions-4}{%
\subsection{Plot: Compare
Predictions}\label{plot-compare-predictions-4}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_confusion\_matrix}
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_default, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title }\OperatorTok{=} \StringTok{"Default"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{021_spot_hpt_sklearn_multiclass_classification_knn_files/figure-pdf/cell-40-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{021_spot_hpt_sklearn_multiclass_classification_knn_files/figure-pdf/cell-41-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{min}\NormalTok{(spot\_tuner.y), }\BuiltInTok{max}\NormalTok{(spot\_tuner.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(-0.7465277777777777, -0.59375)
\end{verbatim}

\hypertarget{cross-validated-evaluations-3}{%
\subsection{Cross-validated
Evaluations}\label{cross-validated-evaluations-3}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.sklearn.traintest }\ImportTok{import}\NormalTok{ evaluate\_cv}
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.7156920077972708, None)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"test\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in fun_sklearn(). Call to evaluate_cv failed. err=ValueError('n_splits=10 cannot be greater than the number of members in each class.'), type(err)=<class 'ValueError'>
\end{verbatim}

\begin{verbatim}
(nan, None)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  This is the evaluation that will be used in the comparison:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
     \StringTok{"eval"}\NormalTok{: }\StringTok{"data\_cv"}\NormalTok{,}
     \StringTok{"k\_folds"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
\NormalTok{\})}
\NormalTok{evaluate\_cv(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, verbose}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.7089487179487179, None)
\end{verbatim}

\hypertarget{detailed-hyperparameter-plots-4}{%
\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-4}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_neighbors:  4.987092475888252
weights:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{021_spot_hpt_sklearn_multiclass_classification_knn_files/figure-pdf/cell-46-output-2.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plot-4}{%
\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-4}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-4}{%
\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-4}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\part{Hyperparameter Tuning with River}

\hypertarget{sec-hpt-river}{%
\chapter{HPT: River}\label{sec-hpt-river}}

\hypertarget{sec-hpt-river-intro}{%
\section{Introduction to River}\label{sec-hpt-river-intro}}

\hypertarget{the-spotriver-gui}{%
\chapter{The spotriver GUI}\label{the-spotriver-gui}}

\hypertarget{hyperparameter-tuning-1}{%
\section{Hyperparameter Tuning}\label{hyperparameter-tuning-1}}

Calls \texttt{run\_spot\_river\_experiment} from
\texttt{spotRiver.tuner.run.py} with the following parameters:

\begin{itemize}
\tightlist
\item
  MAX\_TIME,
\item
  INIT\_SIZE
\item
  PREFIX
\item
  horizon
\item
  n\_total
\item
  perc\_train
\item
  oml\_grace\_period
\item
  data\_set
\item
  prepmodel
\item
  coremodel
\end{itemize}

\hypertarget{the-run_spot_river_experiment-method}{%
\subsection{The run\_spot\_river\_experiment
Method}\label{the-run_spot_river_experiment-method}}

\texttt{run\_spot\_river\_experiment} calls the tuner spot after
processing the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate an experiment name.
\item
  Initialize the \texttt{fun\_control} dictionary.
\item
  Select the data set based on the \texttt{data\_set} parameter and
  generate a data frame.
\item
  Splits the data into training and test sets.
\item
  Sets the oml\_grace\_period parameter.
\item
  Select the preprocessing model based on the \texttt{prepmodel}
  parameter.
\item
  Sets the weights for the evaluation function and the weight coeffient.
\item
  Loads the coremodel based on the \texttt{coremodel} parameter with
  hyperparameters set to the values specified in the
  \texttt{RiverHyperDict} dictionary.
\item
  Determines the default hyperparameters.
\item
  Selects the evaluation function:
  \texttt{HyperRiver.fun\_oml\_horizon}.
\item
  Determines hyperparameter types, names, lower and upper bounds for the
  \texttt{spot} tuner.
\item
  Starts tensorboard as a background process.
\item
  Starts the \texttt{spot} tuner.
\end{enumerate}

When the tuner is finished, the following steps are performed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The tensorboard process is terminated.
\item
  The spot\_tuner object and the \texttt{fun\_control} dictionary are
  returned.
\end{enumerate}

After the tuner is finished, the following information is available:

\hypertarget{binary-classification}{%
\subsection{Binary Classification}\label{binary-classification}}

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

\hypertarget{sec-river-hpt}{%
\chapter{\texorpdfstring{\texttt{river} Hyperparameter Tuning: Hoeffding
Adaptive Tree Regressor with Friedman Drift
Data}{river Hyperparameter Tuning: Hoeffding Adaptive Tree Regressor with Friedman Drift Data}}\label{sec-river-hpt}}

This chapter demonstrates hyperparameter tuning for \texttt{river}'s
\texttt{Hoeffding\ Adaptive\ Tree\ Regressor} with the Friedman drift
data set
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
The \texttt{Hoeffding\ Adaptive\ Tree\ Regressor} is a decision tree
that uses the Hoeffding bound to limit the number of splits evaluated at
each node. The \texttt{Hoeffding\ Adaptive\ Tree\ Regressor} is a
regression tree, i.e., it predicts a real value for each sample. The
\texttt{Hoeffding\ Adaptive\ Tree\ Regressor} is a drift aware model,
i.e., it can handle concept drifts.

\hypertarget{sec-setup-13}{%
\section{Setup}\label{sec-setup-13}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time, initial design size, size of the data
set, and the experiment name.

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME}: The maximum run time in seconds for the
  hyperparameter tuning process.
\item
  \texttt{INIT\_SIZE}: The initial design size for the hyperparameter
  tuning process.
\item
  \texttt{PREFIX}: The prefix for the experiment name.
\item
  \texttt{K}: The factor that determines the number of samples in the
  data set.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{K} is the multiplier for the number of samples. If it is set
  to 1, then \texttt{100\_000}samples are taken. It is set to 0.1 for
  demonstration purposes. For real experiments, this should be increased
  to at least 1.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"13{-}river"}
\NormalTok{K }\OperatorTok{=} \FloatTok{0.1}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  This notebook exemplifies hyperparameter tuning with SPOT (spotPython
  and spotRiver).
\item
  The hyperparameter software SPOT is available in Python. It was
  developed in R (statistical programming language), see Open Access
  book ``Hyperparameter Tuning for Machine and Deep Learning with R - A
  Practical Guide'', available here:
  \url{https://link.springer.com/book/10.1007/978-981-19-5170-1}.
\item
  This notebook demonstrates hyperparameter tuning for \texttt{river}.
  It is based on the notebook ``Incremental decision trees in river: the
  Hoeffding Tree case'', see:
  \url{https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/\#42-regression-tree-splitters}.
\item
  Here we will use the river \texttt{HTR} and \texttt{HATR} functions as
  in ``Incremental decision trees in river: the Hoeffding Tree case'',
  see:
  \url{https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/\#42-regression-tree-splitters}.
\end{itemize}

\hypertarget{initialization-of-the-fun_control-dictionary}{%
\section{\texorpdfstring{Initialization of the \texttt{fun\_control}
Dictionary}{Initialization of the fun\_control Dictionary}}\label{initialization-of-the-fun_control-dictionary}}

\texttt{spotPython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotPython}.

First, we define an ``experiment name'' to identify the hyperparameter
tuning process. The experiment name is also used to create a directory
for the TensorBoard files.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
13-river_maans14_2023-11-08_10-42-50
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, colframe=quarto-callout-tip-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Since the \texttt{spot\_tensorboard\_path} argument is not
  \texttt{None}, which is the default, \texttt{spotPython} will log the
  optimization process in the TensorBoard folder.
\item
  Section~\ref{sec-tensorboard-10} describes how to start TensorBoard
  and access the TensorBoard dashboard.
\item
  The \texttt{TENSORBOARD\_CLEAN} argument is set to \texttt{True} to
  archive the TensorBoard folder if it already exists. This is useful if
  you want to start a hyperparameter tuning process from scratch. If you
  want to continue a hyperparameter tuning process, set
  \texttt{TENSORBOARD\_CLEAN} to \texttt{False}. Then the TensorBoard
  folder will not be archived and the old and new TensorBoard files will
  shown in the TensorBoard dashboard.
\end{itemize}

\end{tcolorbox}

\hypertarget{load-data-the-friedman-drift-data}{%
\section{Load Data: The Friedman Drift
Data}\label{load-data-the-friedman-drift-data}}

We will use the Friedman synthetic dataset with concept drifts
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
Each observation is composed of ten features. Each feature value is
sampled uniformly in {[}0, 1{]}. Only the first five features are
relevant. The target is defined by different functions depending on the
type of the drift. Global Recurring Abrupt drift will be used, i.e., the
concept drift appears over the whole instance space. There are two
points of concept drift. At the second point of drift the old concept
reoccurs.

The following parameters are used to generate and handle the data set:

\begin{itemize}
\tightlist
\item
  horizon: The prediction horizon in hours.
\item
  n\_samples: The number of samples in the data set.
\item
  p\_1: The position of the first concept drift.
\item
  p\_2: The position of the second concept drift.
\item
  position: The position of the concept drifts.
\item
  n\_train: The number of samples used for training.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{horizon }\OperatorTok{=} \DecValTok{7}\OperatorTok{*}\DecValTok{24}
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{100\_000}\NormalTok{)}
\NormalTok{p\_1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{25\_000}\NormalTok{)}
\NormalTok{p\_2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{50\_000}\NormalTok{)}
\NormalTok{position}\OperatorTok{=}\NormalTok{(p\_1, p\_2)}
\NormalTok{n\_train }\OperatorTok{=} \DecValTok{1\_000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.datasets }\ImportTok{import}\NormalTok{ synth}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{position,}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We will use \texttt{spotRiver}'s \texttt{convert\_to\_df} function
  \href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/utils/data_conversion.py}{{[}SOURCE{]}}
  to convert the \texttt{river} data set to a \texttt{pandas} data
  frame.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.utils.data\_conversion }\ImportTok{import}\NormalTok{ convert\_to\_df}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\NormalTok{df }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, target\_column}\OperatorTok{=}\NormalTok{target\_column, n\_total}\OperatorTok{=}\NormalTok{n\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Add column names x1 until x10 to the first 10 columns of the dataframe
  and the column name y to the last column of the dataframe.
\item
  Then split the data frame into a training and test data set. The train
  and test data sets are stored in the \texttt{fun\_control} dictionary.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\StringTok{"y"}\NormalTok{]}
\NormalTok{fun\_control.update(\{}\StringTok{"train"}\NormalTok{:  df[:n\_train],}
                    \StringTok{"test"}\NormalTok{:  df[n\_train:],}
                    \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
                    \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{specification-of-the-preprocessing-model}{%
\section{Specification of the Preprocessing
Model}\label{specification-of-the-preprocessing-model}}

\begin{itemize}
\tightlist
\item
  We use the \texttt{StandardScaler}
  \href{https://riverml.xyz/dev/api/preprocessing/StandardScaler/}{{[}SOURCE{]}}
  from \texttt{river} as the preprocessing model. The
  \texttt{StandardScaler} is used to standardize the data set, i.e., it
  has zero mean and unit variance.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river }\ImportTok{import}\NormalTok{ preprocessing}
\NormalTok{prep\_model }\OperatorTok{=}\NormalTok{ preprocessing.StandardScaler()}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{selectselect-model-algorithm-and-core_model_hyper_dict}{%
\section{\texorpdfstring{SelectSelect Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{SelectSelect Model (algorithm) and core\_model\_hyper\_dict}}\label{selectselect-model-algorithm-and-core_model_hyper_dict}}

\texttt{spotPython} hyperparameter tuning approach uses two components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a model (class) and
\item
  an associated hyperparameter dictionary.
\end{enumerate}

Here, the \texttt{river} model class
\texttt{HoeffdingAdaptiveTreeRegressor}
\href{https://riverml.xyz/dev/api/tree/HoeffdingAdaptiveTreeRegressor/}{{[}SOURCE{]}}
is selected.

The corresponding hyperparameters are loaded from the associated
dictionary, which is stored as a JSON file
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}.
The JSON file contains hyperparameter type information, names, and
bounds.

The method \texttt{add\_core\_model\_to\_fun\_control} adds the model
and the hyperparameter dictionary to the \texttt{fun\_control}
dictionary.

Alternatively, you can load a local hyper\_dict. Simply set
\texttt{river\_hyper\_dict.json} as the filename. If \texttt{filename}is
set to \texttt{None}, which is the default, the hyper\_dict
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}
is loaded from the \texttt{spotRiver} package.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.tree }\ImportTok{import}\NormalTok{ HoeffdingAdaptiveTreeRegressor}
\ImportTok{from}\NormalTok{ spotRiver.data.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{HoeffdingAdaptiveTreeRegressor,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model}{%
\section{\texorpdfstring{Modify \texttt{hyper\_dict} Hyperparameters for
the Selected Algorithm aka
\texttt{core\_model}}{Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model}}

After the \texttt{core\_model} and the \texttt{core\_model\_hyper\_dict}
are added to the \texttt{fun\_control} dictionary, the hyperparameter
tuning can be started. However, in some settings, the user wants to
modify the hyperparameters of the \texttt{core\_model\_hyper\_dict}.
This can be done with the \texttt{modify\_hyper\_parameter\_bounds} and
\texttt{modify\_hyper\_parameter\_levels} functions
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}}.

The following code shows how hyperparameter of type numeric and integer
(boolean) can be modified. The \texttt{modify\_hyper\_parameter\_bounds}
function is used to modify the bounds of the hyperparameter
\texttt{delta} and \texttt{merit\_preprune}. Similar option exists for
the \texttt{modify\_hyper\_parameter\_levels} function to modify the
levels of categorical hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"delta"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{1e{-}10}\NormalTok{, }\FloatTok{1e{-}6}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"merit\_preprune"}\NormalTok{, [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Active and Inactive Hyperparameters}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Hyperparameters can be excluded from the tuning procedure by selecting
identical values for the lower and upper bounds. For example, the
hyperparameter \texttt{merit\_preprune} is excluded from the tuning
procedure by setting the bounds to \texttt{{[}0,\ 0{]}}.

\end{tcolorbox}

\texttt{spotPython}'s method \texttt{gen\_design\_table} summarizes the
experimental design that is used for the hyperparameter tuning:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name                   | type   | default          |      lower |    upper | transform             |
|------------------------|--------|------------------|------------|----------|-----------------------|
| grace_period           | int    | 200              |     10     | 1000     | None                  |
| max_depth              | int    | 20               |      2     |   20     | transform_power_2_int |
| delta                  | float  | 1e-07            |      1e-10 |    1e-06 | None                  |
| tau                    | float  | 0.05             |      0.01  |    0.1   | None                  |
| leaf_prediction        | factor | mean             |      0     |    2     | None                  |
| leaf_model             | factor | LinearRegression |      0     |    2     | None                  |
| model_selector_decay   | float  | 0.95             |      0.9   |    0.99  | None                  |
| splitter               | factor | EBSTSplitter     |      0     |    2     | None                  |
| min_samples_split      | int    | 5                |      2     |   10     | None                  |
| bootstrap_sampling     | factor | 0                |      0     |    1     | None                  |
| drift_window_threshold | int    | 300              |    100     |  500     | None                  |
| switch_significance    | float  | 0.05             |      0.01  |    0.1   | None                  |
| binary_split           | factor | 0                |      0     |    1     | None                  |
| max_size               | float  | 500.0            |    100     | 1000     | None                  |
| memory_estimate_period | int    | 1000000          | 100000     |    1e+06 | None                  |
| stop_mem_management    | factor | 0                |      0     |    1     | None                  |
| remove_poor_attrs      | factor | 0                |      0     |    1     | None                  |
| merit_preprune         | factor | 0                |      0     |    0     | None                  |
\end{verbatim}

\hypertarget{selection-of-the-objective-loss-function}{%
\section{Selection of the Objective (Loss)
Function}\label{selection-of-the-objective-loss-function}}

The \texttt{metric\_sklearn} is used for the sklearn based evaluation
via \texttt{eval\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_bml.py}{{[}SOURCE{]}}.
Here we use the \texttt{mean\_absolute\_error}
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html}{{[}SOURCE{]}}
as the objective function.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Additional metrics}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotRiver} also supports additional metrics. For example, the
\texttt{metric\_river} is used for the river based evaluation via
\texttt{eval\_oml\_iter\_progressive}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_oml.py}{{[}SOURCE{]}}.
The \texttt{metric\_river} is implemented to simulate the behaviour of
the ``original'' \texttt{river} metrics.

\end{tcolorbox}

\texttt{spotRiver} provides information about the model' s score
(metric), memory, and time. The hyperparamter tuner requires a single
objective. Therefore, a weighted sum of the metric, memory, and time is
computed. The weights are defined in the \texttt{weights} array.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Weights}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The \texttt{weights} provide a flexible way to define specific
requirements, e.g., if the memory is more important than the time, the
weight for the memory can be increased.

\end{tcolorbox}

The \texttt{oml\_grace\_period} defines the number of observations that
are used for the initial training of the model. The \texttt{step}
defines the iteration number at which to yield results. This only takes
into account the predictions, and not the training steps. The
\texttt{weight\_coeff} defines a multiplier for the results: results are
multiplied by (step/n\_steps)**weight\_coeff, where n\_steps is the
total number of iterations. Results from the beginning have a lower
weight than results from the end if weight\_coeff \textgreater{} 1. If
weight\_coeff == 0, all results have equal weight. Note, that the
\texttt{weight\_coeff} is only used internally for the tuner and does
not affect the results that are used for the evaluation or comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_absolute\_error}

\NormalTok{weights }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{])}\OperatorTok{*}\FloatTok{10\_000.0}
\NormalTok{oml\_grace\_period }\OperatorTok{=} \DecValTok{2}
\NormalTok{step }\OperatorTok{=} \DecValTok{100}
\NormalTok{weight\_coeff }\OperatorTok{=} \FloatTok{1.0}

\NormalTok{fun\_control.update(\{}
               \StringTok{"horizon"}\NormalTok{: horizon,}
               \StringTok{"oml\_grace\_period"}\NormalTok{: oml\_grace\_period,}
               \StringTok{"weights"}\NormalTok{: weights,}
               \StringTok{"step"}\NormalTok{: step,}
               \StringTok{"weight\_coeff"}\NormalTok{: weight\_coeff,}
               \StringTok{"metric\_sklearn"}\NormalTok{: mean\_absolute\_error}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{calling-the-spot-function}{%
\section{Calling the SPOT Function}\label{calling-the-spot-function}}

\hypertarget{prepare-the-spot-parameters}{%
\subsection{Prepare the SPOT
Parameters}\label{prepare-the-spot-parameters}}

The hyperparameter tuning configuration is stored in the
\texttt{fun\_control} dictionary. Since \texttt{Spot} can be used as an
optimization algorithm with a similar interface as optimization
algorithms from \texttt{scipy.optimize}
\href{https://docs.scipy.org/doc/scipy/reference/optimize.html\#module-scipy.optimize}{{[}LINK{]}},
the bounds and variable types have to be specified explicitely. The
\texttt{get\_var\_type}, \texttt{get\_var\_name}, and
\texttt{get\_bound\_values} functions
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}}
implement the required functionality.

\begin{itemize}
\tightlist
\item
  Get types and variable names as well as lower and upper bounds for the
  hyperparameters, so that they can be passed to the \texttt{Spot}
  function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (}
\NormalTok{    get\_var\_type,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_bound\_values}
\NormalTok{    )}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-the-objective-function-13}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-13}}

The objective function \texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py}{{[}SOURCE{]}}
is selected next.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.fun.hyperriver }\ImportTok{import}\NormalTok{ HyperRiver}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperRiver().fun\_oml\_horizon}
\end{Highlighting}
\end{Shaded}

The following code snippet shows how to get the default hyperparameters
as an array, so that they can be passed to the \texttt{Spot} function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer-5}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-5}}

The class \texttt{Spot}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}
is the hyperparameter tuning workhorse. It is initialized with the
following parameters:

\begin{itemize}
\tightlist
\item
  \texttt{fun}: the objective function
\item
  \texttt{lower}: lower bounds of the hyperparameters
\item
  \texttt{upper}: upper bounds of the hyperparameters
\item
  \texttt{fun\_evals}: number of function evaluations
\item
  \texttt{max\_time}: maximum time in seconds
\item
  \texttt{tolerance\_x}: tolerance for the hyperparameters
\item
  \texttt{var\_type}: variable types of the hyperparameters
\item
  \texttt{var\_name}: variable names of the hyperparameters
\item
  \texttt{show\_progress}: show progress bar
\item
  \texttt{fun\_control}: dictionary with control parameters for the
  objective function
\item
  \texttt{design\_control}: dictionary with control parameters for the
  initial design
\item
  \texttt{surrogate\_control}: dictionary with control parameters for
  the surrogate model
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Total run time}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The total run time may exceed the specified \texttt{max\_time}, because
the initial design (here: \texttt{init\_size} = INIT\_SIZE as specified
above) is always evaluated, even if this takes longer than
\texttt{max\_time}.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{\})}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.0746566971551794 [#---------] 12.94% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.0746566971551794 [###-------] 27.25% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.0746566971551794 [####------] 43.24% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.0746566971551794 [######----] 55.83% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.0746566971551794 [#######---] 71.47% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.0746566971551794 [#########-] 90.77% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.0746566971551794 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x309c5c2d0>
\end{verbatim}

\hypertarget{sec-tensorboard-10}{%
\subsection{TensorBoard}\label{sec-tensorboard-10}}

Now we can start TensorBoard in the background with the following
command, where \texttt{./runs} is the default directory for the
TensorBoard log files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TENSORBOARD\_PATH}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, colframe=quarto-callout-tip-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The TensorBoard path can be printed with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_tensorboard\_path}
\NormalTok{get\_tensorboard\_path(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'runs/'
\end{verbatim}

\end{tcolorbox}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/build/kriging.py}{{[}SOURCE{]}}
is plotted against the number of optimization steps.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/13_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}

\hypertarget{results-4}{%
\subsection{Results}\label{results-4}}

After the hyperparameter tuning run is finished, the results can be
saved and reloaded with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle}
\NormalTok{save\_pickle(spot\_tuner, experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_pickle}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ load\_pickle(experiment\_name)}
\end{Highlighting}
\end{Shaded}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-21-output-1.pdf}

}

\end{figure}

Results can also be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name                   | type   | default          |    lower |     upper |              tuned | transform             |   importance | stars   |
|------------------------|--------|------------------|----------|-----------|--------------------|-----------------------|--------------|---------|
| grace_period           | int    | 200              |     10.0 |    1000.0 |              167.0 | None                  |         0.00 |         |
| max_depth              | int    | 20               |      2.0 |      20.0 |               18.0 | transform_power_2_int |         0.00 |         |
| delta                  | float  | 1e-07            |    1e-10 |     1e-06 |              1e-10 | None                  |         0.00 |         |
| tau                    | float  | 0.05             |     0.01 |       0.1 |                0.1 | None                  |         0.00 |         |
| leaf_prediction        | factor | mean             |      0.0 |       2.0 |                2.0 | None                  |         0.00 |         |
| leaf_model             | factor | LinearRegression |      0.0 |       2.0 |                0.0 | None                  |        17.15 | *       |
| model_selector_decay   | float  | 0.95             |      0.9 |      0.99 |               0.99 | None                  |         0.00 |         |
| splitter               | factor | EBSTSplitter     |      0.0 |       2.0 |                2.0 | None                  |       100.00 | ***     |
| min_samples_split      | int    | 5                |      2.0 |      10.0 |                4.0 | None                  |         0.00 |         |
| bootstrap_sampling     | factor | 0                |      0.0 |       1.0 |                0.0 | None                  |         0.00 |         |
| drift_window_threshold | int    | 300              |    100.0 |     500.0 |              150.0 | None                  |         0.00 |         |
| switch_significance    | float  | 0.05             |     0.01 |       0.1 |               0.01 | None                  |         0.00 |         |
| binary_split           | factor | 0                |      0.0 |       1.0 |                0.0 | None                  |         0.00 |         |
| max_size               | float  | 500.0            |    100.0 |    1000.0 | 247.17760214559334 | None                  |         0.00 |         |
| memory_estimate_period | int    | 1000000          | 100000.0 | 1000000.0 |           994945.0 | None                  |         0.00 |         |
| stop_mem_management    | factor | 0                |      0.0 |       1.0 |                0.0 | None                  |         0.04 |         |
| remove_poor_attrs      | factor | 0                |      0.0 |       1.0 |                0.0 | None                  |         0.00 |         |
| merit_preprune         | factor | 0                |      0.0 |       0.0 |                0.0 | None                  |         0.00 |         |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.0025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-23-output-1.pdf}

}

\end{figure}

\hypertarget{the-larger-data-set}{%
\section{The Larger Data Set}\label{the-larger-data-set}}

After the hyperparamter were tuned on a small data set, we can now apply
the hyperparameter configuration to a larger data set. The following
code snippet shows how to generate the larger data set.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Increased Friedman-Drift Data Set}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  The Friedman-Drift Data Set is increased by a factor of two to show
  the transferability of the hyperparameter tuning results.
\item
  Larger values of \texttt{K} lead to a longer run time.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{K }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{100\_000}\NormalTok{)}
\NormalTok{p\_1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{25\_000}\NormalTok{)}
\NormalTok{p\_2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{50\_000}\NormalTok{)}
\NormalTok{position}\OperatorTok{=}\NormalTok{(p\_1, p\_2)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{position,}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The larger data set is converted to a Pandas data frame and passed to
the \texttt{fun\_control} dictionary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, target\_column}\OperatorTok{=}\NormalTok{target\_column, n\_total}\OperatorTok{=}\NormalTok{n\_samples)}
\NormalTok{df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\StringTok{"y"}\NormalTok{]}
\NormalTok{fun\_control.update(\{}\StringTok{"train"}\NormalTok{: df[:n\_train],}
                    \StringTok{"test"}\NormalTok{: df[n\_train:],}
                    \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
                    \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-default-hyperparameters-5}{%
\section{Get Default
Hyperparameters}\label{get-default-hyperparameters-5}}

The default hyperparameters, whihc will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: \texttt{spotPython} tunes numpy arrays}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{spotPython} tunes numpy arrays, i.e., the hyperparameters are
  stored in a numpy array.
\end{itemize}

\end{tcolorbox}

The model with the default hyperparameters can be trained and evaluated
with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ eval\_oml\_horizon}

\NormalTok{df\_eval\_default, df\_true\_default }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_default,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

The three performance criteria, i.e., scaoe (metric), runtime, and
memory consumption, can be visualized with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ plot\_bml\_oml\_horizon\_metrics, plot\_bml\_oml\_horizon\_predictions}
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-29-output-1.pdf}

}

\end{figure}

\hypertarget{show-predictions}{%
\subsection{Show Predictions}\label{show-predictions}}

\begin{itemize}
\tightlist
\item
  Select a subset of the data set for the visualization of the
  predictions:

  \begin{itemize}
  \tightlist
  \item
    We use the mean, \(m\), of the data set as the center of the
    visualization.
  \item
    We use 100 data points, i.e., \(m \pm 50\) as the visualization
    window.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"test"}\NormalTok{].shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{a }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{{-}}\DecValTok{50}
\NormalTok{b }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-31-output-1.pdf}

}

\end{figure}

\hypertarget{get-spot-results-5}{%
\section{Get SPOT Results}\label{get-spot-results-5}}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval\_spot, df\_true\_spot }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_spot,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{, }\StringTok{"spot"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default, df\_eval\_spot], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{], filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_metrics.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-34-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b], df\_true\_spot[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_predictions.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-35-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_actual\_vs\_predicted}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_default[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_default[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"Default"}\NormalTok{)}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_spot[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_spot[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-36-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-36-output-2.pdf}

}

\end{figure}

\hypertarget{visualize-regression-trees}{%
\section{Visualize Regression Trees}\label{visualize-regression-trees}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_f }\OperatorTok{=}\NormalTok{ dataset.take(n\_samples)}
\ControlFlowTok{for}\NormalTok{ x, y }\KeywordTok{in}\NormalTok{ dataset\_f:}
\NormalTok{    model\_default.learn\_one(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Large Trees}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Since the trees are large, the visualization is suppressed by default.
\item
  To visualize the trees, uncomment the following line.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model\_default.draw()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_default.summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'n_nodes': 35,
 'n_branches': 17,
 'n_leaves': 18,
 'n_active_leaves': 96,
 'n_inactive_leaves': 0,
 'height': 6,
 'total_observed_weight': 39002.0,
 'n_alternate_trees': 21,
 'n_pruned_alternate_trees': 6,
 'n_switch_alternate_trees': 2}
\end{verbatim}

\hypertarget{spot-model}{%
\subsection{Spot Model}\label{spot-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_f }\OperatorTok{=}\NormalTok{ dataset.take(n\_samples)}
\ControlFlowTok{for}\NormalTok{ x, y }\KeywordTok{in}\NormalTok{ dataset\_f:}
\NormalTok{    model\_spot.learn\_one(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Large Trees}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Since the trees are large, the visualization is suppressed by default.
\item
  To visualize the trees, uncomment the following line.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model\_spot.draw()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_spot.summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'n_nodes': 57,
 'n_branches': 28,
 'n_leaves': 29,
 'n_active_leaves': 117,
 'n_inactive_leaves': 0,
 'height': 11,
 'total_observed_weight': 39002.0,
 'n_alternate_trees': 46,
 'n_pruned_alternate_trees': 30,
 'n_switch_alternate_trees': 0}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ compare\_two\_tree\_models}
\BuiltInTok{print}\NormalTok{(compare\_two\_tree\_models(model\_default, model\_spot))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| Parameter                |   Default |   Spot |
|--------------------------|-----------|--------|
| n_nodes                  |        35 |     57 |
| n_branches               |        17 |     28 |
| n_leaves                 |        18 |     29 |
| n_active_leaves          |        96 |    117 |
| n_inactive_leaves        |         0 |      0 |
| height                   |         6 |     11 |
| total_observed_weight    |     39002 |  39002 |
| n_alternate_trees        |        21 |     46 |
| n_pruned_alternate_trees |         6 |     30 |
| n_switch_alternate_trees |         2 |      0 |
\end{verbatim}

\hypertarget{detailed-hyperparameter-plots-5}{%
\section{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-5}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
leaf_model:  17.151231799791788
splitter:  100.0
stop_mem_management:  0.03726907041854541
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-44-output-2.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-44-output-3.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{024_spot_hpt_river_friedman_hatr_files/figure-pdf/cell-44-output-4.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plots}{%
\section{Parallel Coordinates Plots}\label{parallel-coordinates-plots}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-5}{%
\section{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-5}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-river-hpt-51}{%
\chapter{\texorpdfstring{\texttt{river} Hyperparameter Tuning: Mondrian
Tree Regressor with Friedman Drift
Data}{river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data}}\label{sec-river-hpt-51}}

This chapter demonstrates hyperparameter tuning for \texttt{river}'s
\texttt{Mondrian\ Tree\ Regressor} with the Friedman drift data set
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
The \texttt{Mondrian\ Tree\ Regressor} is a regression tree, i.e., it
predicts a real value for each sample.

\hypertarget{sec-setup-51}{%
\section{Setup}\label{sec-setup-51}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time, initial design size, size of the data
set, and the experiment name.

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME}: The maximum run time in seconds for the
  hyperparameter tuning process.
\item
  \texttt{INIT\_SIZE}: The initial design size for the hyperparameter
  tuning process.
\item
  \texttt{PREFIX}: The prefix for the experiment name.
\item
  \texttt{K}: The factor that determines the number of samples in the
  data set.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{K} is the multiplier for the number of samples. If it is set
  to 1, then \texttt{100\_000}samples are taken. It is set to 0.1 for
  demonstration purposes. For real experiments, this should be increased
  to at least 1.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"51{-}river"}
\NormalTok{K }\OperatorTok{=} \FloatTok{0.1}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  This notebook exemplifies hyperparameter tuning with SPOT (spotPython
  and spotRiver).
\item
  The hyperparameter software SPOT is available in Python. It was
  developed in R (statistical programming language), see Open Access
  book ``Hyperparameter Tuning for Machine and Deep Learning with R - A
  Practical Guide'', available here:
  \url{https://link.springer.com/book/10.1007/978-981-19-5170-1}.
\item
  This notebook demonstrates hyperparameter tuning for \texttt{river}.
  It is based on the notebook ``Incremental decision trees in river: the
  Hoeffding Tree case'', see:
  \url{https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/\#42-regression-tree-splitters}.
\item
  Here we will use the river \texttt{AMFRegressor} functions, see:
  \url{https://riverml.xyz/0.19.0/api/forest/AMFRegressor/}.
\end{itemize}

\hypertarget{initialization-of-the-fun_control-dictionary-1}{%
\section{\texorpdfstring{Initialization of the \texttt{fun\_control}
Dictionary}{Initialization of the fun\_control Dictionary}}\label{initialization-of-the-fun_control-dictionary-1}}

\texttt{spotPython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotPython}.

First, we define an ``experiment name'' to identify the hyperparameter
tuning process. The experiment name is also used to create a directory
for the TensorBoard files.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
51-river_maans14_2023-11-08_11-12-23
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, colframe=quarto-callout-tip-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Since the \texttt{spot\_tensorboard\_path} argument is not
  \texttt{None}, which is the default, \texttt{spotPython} will log the
  optimization process in the TensorBoard folder.
\item
  Section~\ref{sec-tensorboard-10} describes how to start TensorBoard
  and access the TensorBoard dashboard.
\item
  The \texttt{TENSORBOARD\_CLEAN} argument is set to \texttt{True} to
  archive the TensorBoard folder if it already exists. This is useful if
  you want to start a hyperparameter tuning process from scratch. If you
  want to continue a hyperparameter tuning process, set
  \texttt{TENSORBOARD\_CLEAN} to \texttt{False}. Then the TensorBoard
  folder will not be archived and the old and new TensorBoard files will
  shown in the TensorBoard dashboard.
\end{itemize}

\end{tcolorbox}

\hypertarget{load-data-the-friedman-drift-data-1}{%
\section{Load Data: The Friedman Drift
Data}\label{load-data-the-friedman-drift-data-1}}

We will use the Friedman synthetic dataset with concept drifts
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
Each observation is composed of ten features. Each feature value is
sampled uniformly in {[}0, 1{]}. Only the first five features are
relevant. The target is defined by different functions depending on the
type of the drift. Global Recurring Abrupt drift will be used, i.e., the
concept drift appears over the whole instance space. There are two
points of concept drift. At the second point of drift the old concept
reoccurs.

The following parameters are used to generate and handle the data set:

\begin{itemize}
\tightlist
\item
  horizon: The prediction horizon in hours.
\item
  n\_samples: The number of samples in the data set.
\item
  p\_1: The position of the first concept drift.
\item
  p\_2: The position of the second concept drift.
\item
  position: The position of the concept drifts.
\item
  n\_train: The number of samples used for training.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{horizon }\OperatorTok{=} \DecValTok{7}\OperatorTok{*}\DecValTok{24}
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{100\_000}\NormalTok{)}
\NormalTok{p\_1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{25\_000}\NormalTok{)}
\NormalTok{p\_2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{50\_000}\NormalTok{)}
\NormalTok{position}\OperatorTok{=}\NormalTok{(p\_1, p\_2)}
\NormalTok{n\_train }\OperatorTok{=} \DecValTok{1\_000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.datasets }\ImportTok{import}\NormalTok{ synth}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{position,}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We will use \texttt{spotRiver}'s \texttt{convert\_to\_df} function
  \href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/utils/data_conversion.py}{{[}SOURCE{]}}
  to convert the \texttt{river} data set to a \texttt{pandas} data
  frame.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.utils.data\_conversion }\ImportTok{import}\NormalTok{ convert\_to\_df}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\NormalTok{df }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, target\_column}\OperatorTok{=}\NormalTok{target\_column, n\_total}\OperatorTok{=}\NormalTok{n\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Add column names x1 until x10 to the first 10 columns of the dataframe
  and the column name y to the last column of the dataframe.
\item
  Then split the data frame into a training and test data set. The train
  and test data sets are stored in the \texttt{fun\_control} dictionary.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\StringTok{"y"}\NormalTok{]}
\NormalTok{fun\_control.update(\{}\StringTok{"train"}\NormalTok{:  df[:n\_train],}
                    \StringTok{"test"}\NormalTok{:  df[n\_train:],}
                    \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
                    \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{specification-of-the-preprocessing-model-1}{%
\section{Specification of the Preprocessing
Model}\label{specification-of-the-preprocessing-model-1}}

\begin{itemize}
\tightlist
\item
  We use the \texttt{StandardScaler}
  \href{https://riverml.xyz/dev/api/preprocessing/StandardScaler/}{{[}SOURCE{]}}
  from \texttt{river} as the preprocessing model. The
  \texttt{StandardScaler} is used to standardize the data set, i.e., it
  has zero mean and unit variance.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river }\ImportTok{import}\NormalTok{ preprocessing}
\NormalTok{prep\_model }\OperatorTok{=}\NormalTok{ preprocessing.StandardScaler()}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{selectselect-model-algorithm-and-core_model_hyper_dict-1}{%
\section{\texorpdfstring{SelectSelect Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{SelectSelect Model (algorithm) and core\_model\_hyper\_dict}}\label{selectselect-model-algorithm-and-core_model_hyper_dict-1}}

\texttt{spotPython} hyperparameter tuning approach uses two components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a model (class) and
\item
  an associated hyperparameter dictionary.
\end{enumerate}

The corresponding hyperparameters are loaded from the associated
dictionary, which is stored as a JSON file
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}.
The JSON file contains hyperparameter type information, names, and
bounds.

The method \texttt{add\_core\_model\_to\_fun\_control} adds the model
and the hyperparameter dictionary to the \texttt{fun\_control}
dictionary.

Alternatively, you can load a local hyper\_dict. Simply set
\texttt{river\_hyper\_dict.json} as the filename. If \texttt{filename}is
set to \texttt{None}, which is the default, the hyper\_dict
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}
is loaded from the \texttt{spotRiver} package.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.forest }\ImportTok{import}\NormalTok{ AMFRegressor}
\ImportTok{from}\NormalTok{ spotRiver.data.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{AMFRegressor,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-1}{%
\section{\texorpdfstring{Modify \texttt{hyper\_dict} Hyperparameters for
the Selected Algorithm aka
\texttt{core\_model}}{Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-1}}

After the \texttt{core\_model} and the \texttt{core\_model\_hyper\_dict}
are added to the \texttt{fun\_control} dictionary, the hyperparameter
tuning can be started. However, in some settings, the user wants to
modify the hyperparameters of the \texttt{core\_model\_hyper\_dict}.
This can be done with the \texttt{modify\_hyper\_parameter\_bounds} and
\texttt{modify\_hyper\_parameter\_levels} functions
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}}.

The following code shows how hyperparameter of type numeric and integer
(boolean) can be modified. The \texttt{modify\_hyper\_parameter\_bounds}
function is used to modify the bounds of the hyperparameter
\texttt{delta} and \texttt{merit\_preprune}. Similar option exists for
the \texttt{modify\_hyper\_parameter\_levels} function to modify the
levels of categorical hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"n\_estimators"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{,}\DecValTok{100}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

::: \{.callout-note\} \#\#\#\# Note: Active and Inactive Hyperparameters
Hyperparameters can be excluded from the tuning procedure by selecting
identical values for the lower and upper bounds.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper | transform   |
|-----------------|--------|-----------|---------|---------|-------------|
| n_estimators    | int    |        10 |     2   |     100 | None        |
| step            | float  |         1 |     0.1 |      10 | None        |
| use_aggregation | factor |         1 |     0   |       1 | None        |
\end{verbatim}

\hypertarget{selection-of-the-objective-loss-function-1}{%
\section{Selection of the Objective (Loss)
Function}\label{selection-of-the-objective-loss-function-1}}

The \texttt{metric\_sklearn} is used for the sklearn based evaluation
via \texttt{eval\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_bml.py}{{[}SOURCE{]}}.
Here we use the \texttt{mean\_absolute\_error}
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html}{{[}SOURCE{]}}
as the objective function.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Additional metrics}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotRiver} also supports additional metrics. For example, the
\texttt{metric\_river} is used for the river based evaluation via
\texttt{eval\_oml\_iter\_progressive}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_oml.py}{{[}SOURCE{]}}.
The \texttt{metric\_river} is implemented to simulate the behaviour of
the ``original'' \texttt{river} metrics.

\end{tcolorbox}

\texttt{spotRiver} provides information about the model' s score
(metric), memory, and time. The hyperparamter tuner requires a single
objective. Therefore, a weighted sum of the metric, memory, and time is
computed. The weights are defined in the \texttt{weights} array.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Weights}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The \texttt{weights} provide a flexible way to define specific
requirements, e.g., if the memory is more important than the time, the
weight for the memory can be increased.

\end{tcolorbox}

The \texttt{oml\_grace\_period} defines the number of observations that
are used for the initial training of the model. The \texttt{step}
defines the iteration number at which to yield results. This only takes
into account the predictions, and not the training steps. The
\texttt{weight\_coeff} defines a multiplier for the results: results are
multiplied by (step/n\_steps)**weight\_coeff, where n\_steps is the
total number of iterations. Results from the beginning have a lower
weight than results from the end if weight\_coeff \textgreater{} 1. If
weight\_coeff == 0, all results have equal weight. Note, that the
\texttt{weight\_coeff} is only used internally for the tuner and does
not affect the results that are used for the evaluation or comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_absolute\_error}

\NormalTok{weights }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{])}\OperatorTok{*}\FloatTok{10\_000.0}
\NormalTok{oml\_grace\_period }\OperatorTok{=} \DecValTok{2}
\NormalTok{step }\OperatorTok{=} \DecValTok{100}
\NormalTok{weight\_coeff }\OperatorTok{=} \FloatTok{1.0}

\NormalTok{fun\_control.update(\{}
               \StringTok{"horizon"}\NormalTok{: horizon,}
               \StringTok{"oml\_grace\_period"}\NormalTok{: oml\_grace\_period,}
               \StringTok{"weights"}\NormalTok{: weights,}
               \StringTok{"step"}\NormalTok{: step,}
               \StringTok{"weight\_coeff"}\NormalTok{: weight\_coeff,}
               \StringTok{"metric\_sklearn"}\NormalTok{: mean\_absolute\_error}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{calling-the-spot-function-1}{%
\section{Calling the SPOT Function}\label{calling-the-spot-function-1}}

\hypertarget{prepare-the-spot-parameters-1}{%
\subsection{Prepare the SPOT
Parameters}\label{prepare-the-spot-parameters-1}}

The hyperparameter tuning configuration is stored in the
\texttt{fun\_control} dictionary. Since \texttt{Spot} can be used as an
optimization algorithm with a similar interface as optimization
algorithms from \texttt{scipy.optimize}
\href{https://docs.scipy.org/doc/scipy/reference/optimize.html\#module-scipy.optimize}{{[}LINK{]}},
the bounds and variable types have to be specified explicitely. The
\texttt{get\_var\_type}, \texttt{get\_var\_name}, and
\texttt{get\_bound\_values} functions
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}}
implement the required functionality.

\begin{itemize}
\tightlist
\item
  Get types and variable names as well as lower and upper bounds for the
  hyperparameters, so that they can be passed to the \texttt{Spot}
  function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (}
\NormalTok{    get\_var\_type,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_bound\_values}
\NormalTok{    )}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-the-objective-function-51}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-51}}

The objective function \texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py}{{[}SOURCE{]}}
is selected next.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.fun.hyperriver }\ImportTok{import}\NormalTok{ HyperRiver}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperRiver().fun\_oml\_horizon}
\end{Highlighting}
\end{Shaded}

The following code snippet shows how to get the default hyperparameters
as an array, so that they can be passed to the \texttt{Spot} function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer-6}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-6}}

The class \texttt{Spot}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}
is the hyperparameter tuning workhorse. It is initialized with the
following parameters:

\begin{itemize}
\tightlist
\item
  \texttt{fun}: the objective function
\item
  \texttt{lower}: lower bounds of the hyperparameters
\item
  \texttt{upper}: upper bounds of the hyperparameters
\item
  \texttt{fun\_evals}: number of function evaluations
\item
  \texttt{max\_time}: maximum time in seconds
\item
  \texttt{tolerance\_x}: tolerance for the hyperparameters
\item
  \texttt{var\_type}: variable types of the hyperparameters
\item
  \texttt{var\_name}: variable names of the hyperparameters
\item
  \texttt{show\_progress}: show progress bar
\item
  \texttt{fun\_control}: dictionary with control parameters for the
  objective function
\item
  \texttt{design\_control}: dictionary with control parameters for the
  initial design
\item
  \texttt{surrogate\_control}: dictionary with control parameters for
  the surrogate model
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Total run time}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The total run time may exceed the specified \texttt{max\_time}, because
the initial design (here: \texttt{init\_size} = INIT\_SIZE as specified
above) is always evaluated, even if this takes longer than
\texttt{max\_time}.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{\})}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.648253506073714 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x330c5ab90>
\end{verbatim}

\hypertarget{sec-tensorboard-10}{%
\subsection{TensorBoard}\label{sec-tensorboard-10}}

Now we can start TensorBoard in the background with the following
command, where \texttt{./runs} is the default directory for the
TensorBoard log files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TENSORBOARD\_PATH}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, colframe=quarto-callout-tip-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The TensorBoard path can be printed with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_tensorboard\_path}
\NormalTok{get\_tensorboard\_path(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'runs/'
\end{verbatim}

\end{tcolorbox}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/build/kriging.py}{{[}SOURCE{]}}
is plotted against the number of optimization steps.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/13_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}

\hypertarget{results-5}{%
\subsection{Results}\label{results-5}}

After the hyperparameter tuning run is finished, the results can be
saved and reloaded with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle}
\NormalTok{save\_pickle(spot\_tuner, experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_pickle}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ load\_pickle(experiment\_name)}
\end{Highlighting}
\end{Shaded}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-21-output-1.pdf}

}

\end{figure}

Results can also be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper |             tuned | transform   |   importance | stars   |
|-----------------|--------|-----------|---------|---------|-------------------|-------------|--------------|---------|
| n_estimators    | int    |      10.0 |     2.0 |     100 |              63.0 | None        |        17.46 | *       |
| step            | float  |       1.0 |     0.1 |      10 | 7.004318498645526 | None        |         0.00 |         |
| use_aggregation | factor |       1.0 |     0.0 |       1 |               0.0 | None        |       100.00 | ***     |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.0025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-23-output-1.pdf}

}

\end{figure}

\hypertarget{the-larger-data-set-1}{%
\section{The Larger Data Set}\label{the-larger-data-set-1}}

After the hyperparameter were tuned on a small data set, we can now
apply the hyperparameter configuration to a larger data set. The
following code snippet shows how to generate the larger data set.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Increased Friedman-Drift Data Set}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  The Friedman-Drift Data Set is increased by a factor of two to show
  the transferability of the hyperparameter tuning results.
\item
  Larger values of \texttt{K} lead to a longer run time.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{K }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{100\_000}\NormalTok{)}
\NormalTok{p\_1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{25\_000}\NormalTok{)}
\NormalTok{p\_2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{50\_000}\NormalTok{)}
\NormalTok{position}\OperatorTok{=}\NormalTok{(p\_1, p\_2)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{position,}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The larger data set is converted to a Pandas data frame and passed to
the \texttt{fun\_control} dictionary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, target\_column}\OperatorTok{=}\NormalTok{target\_column, n\_total}\OperatorTok{=}\NormalTok{n\_samples)}
\NormalTok{df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\StringTok{"y"}\NormalTok{]}
\NormalTok{fun\_control.update(\{}\StringTok{"train"}\NormalTok{: df[:n\_train],}
                    \StringTok{"test"}\NormalTok{: df[n\_train:],}
                    \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
                    \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-default-hyperparameters-6}{%
\section{Get Default
Hyperparameters}\label{get-default-hyperparameters-6}}

The default hyperparameters, whihc will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: \texttt{spotPython} tunes numpy arrays}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{spotPython} tunes numpy arrays, i.e., the hyperparameters are
  stored in a numpy array.
\end{itemize}

\end{tcolorbox}

The model with the default hyperparameters can be trained and evaluated
with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ eval\_oml\_horizon}

\NormalTok{df\_eval\_default, df\_true\_default }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_default,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

The three performance criteria, i.e., scaoe (metric), runtime, and
memory consumption, can be visualized with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ plot\_bml\_oml\_horizon\_metrics, plot\_bml\_oml\_horizon\_predictions}
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-29-output-1.pdf}

}

\end{figure}

\hypertarget{show-predictions-1}{%
\subsection{Show Predictions}\label{show-predictions-1}}

\begin{itemize}
\tightlist
\item
  Select a subset of the data set for the visualization of the
  predictions:

  \begin{itemize}
  \tightlist
  \item
    We use the mean, \(m\), of the data set as the center of the
    visualization.
  \item
    We use 100 data points, i.e., \(m \pm 50\) as the visualization
    window.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"test"}\NormalTok{].shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{a }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{{-}}\DecValTok{50}
\NormalTok{b }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-31-output-1.pdf}

}

\end{figure}

\hypertarget{get-spot-results-6}{%
\section{Get SPOT Results}\label{get-spot-results-6}}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval\_spot, df\_true\_spot }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_spot,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{, }\StringTok{"spot"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default, df\_eval\_spot], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{], filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_metrics.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-34-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b], df\_true\_spot[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_predictions.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-35-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_actual\_vs\_predicted}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_default[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_default[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"Default"}\NormalTok{)}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_spot[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_spot[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-36-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-36-output-2.pdf}

}

\end{figure}

\hypertarget{detailed-hyperparameter-plots-6}{%
\section{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-6}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_estimators:  17.463574463521653
use_aggregation:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-37-output-2.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plots-1}{%
\section{Parallel Coordinates
Plots}\label{parallel-coordinates-plots-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-6}{%
\section{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-6}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\hypertarget{river-hyperparameter-tuning-mondrian-tree-classifier-with-bananas-data}{%
\chapter{\texorpdfstring{\texttt{river} Hyperparameter Tuning: Mondrian
Tree Classifier with Bananas
Data}{river Hyperparameter Tuning: Mondrian Tree Classifier with Bananas Data}}\label{river-hyperparameter-tuning-mondrian-tree-classifier-with-bananas-data}}

This chapter demonstrates hyperparameter tuning for \texttt{river}'s
\texttt{Mondrian\ Tree\ Classifier} with the bananas data set
\href{https://riverml.xyz/0.19.0/api/datasets/Bananas/}{{[}SOURCE{]}}.

\hypertarget{sec-setup-13}{%
\section{Setup}\label{sec-setup-13}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time, initial design size, size of the data
set, and the experiment name.

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME}: The maximum run time in seconds for the
  hyperparameter tuning process.
\item
  \texttt{INIT\_SIZE}: The initial design size for the hyperparameter
  tuning process.
\item
  \texttt{PREFIX}: The prefix for the experiment name.
\item
  \texttt{K}: The factor that determines the number of samples in the
  data set.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{K} is the multiplier for the number of samples. If it is set
  to 1, then \texttt{100\_000}samples are taken. It is set to 0.1 for
  demonstration purposes. For real experiments, this should be increased
  to at least 1.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"52{-}river"}
\end{Highlighting}
\end{Shaded}

\hypertarget{initialization-of-the-fun_control-dictionary-2}{%
\section{\texorpdfstring{Initialization of the \texttt{fun\_control}
Dictionary}{Initialization of the fun\_control Dictionary}}\label{initialization-of-the-fun_control-dictionary-2}}

\texttt{spotPython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotPython}.

First, we define an ``experiment name'' to identify the hyperparameter
tuning process. The experiment name is also used to create a directory
for the TensorBoard files.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
52-river_maans14_2023-11-08_11-45-14
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TensorBoard}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, colframe=quarto-callout-tip-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Since the \texttt{spot\_tensorboard\_path} argument is not
  \texttt{None}, which is the default, \texttt{spotPython} will log the
  optimization process in the TensorBoard folder.
\item
  Section~\ref{sec-tensorboard-10} describes how to start TensorBoard
  and access the TensorBoard dashboard.
\item
  The \texttt{TENSORBOARD\_CLEAN} argument is set to \texttt{True} to
  archive the TensorBoard folder if it already exists. This is useful if
  you want to start a hyperparameter tuning process from scratch. If you
  want to continue a hyperparameter tuning process, set
  \texttt{TENSORBOARD\_CLEAN} to \texttt{False}. Then the TensorBoard
  folder will not be archived and the old and new TensorBoard files will
  shown in the TensorBoard dashboard.
\end{itemize}

\end{tcolorbox}

\hypertarget{load-data-the-bananas-dataset}{%
\section{Load Data: The Bananas
Dataset}\label{load-data-the-bananas-dataset}}

We will use the Bananas dataset
\href{https://riverml.xyz/0.19.0/api/datasets/Bananas/}{{[}SOURCE{]}}.

The following parameters are used to generate and handle the data set:

\begin{itemize}
\tightlist
\item
  horizon: The prediction horizon in hours.
\item
  n\_samples: The number of samples in the data set.
\item
  n\_train: The number of samples provided for the training.
\item
  oml\_grace\_period: The number of samples that are used for the
  initial training of the model.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{horizon }\OperatorTok{=} \DecValTok{53}
\NormalTok{n\_samples }\OperatorTok{=} \DecValTok{5300}
\NormalTok{n\_train }\OperatorTok{=} \DecValTok{100}
\NormalTok{oml\_grace\_period }\OperatorTok{=}\NormalTok{ n\_train}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.datasets }\ImportTok{import}\NormalTok{ Bananas}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Bananas(}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We will use \texttt{spotRiver}'s \texttt{convert\_to\_df} function
  \href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/utils/data_conversion.py}{{[}SOURCE{]}}
  to convert the \texttt{river} data set to a \texttt{pandas} data
  frame.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.utils.data\_conversion }\ImportTok{import}\NormalTok{ convert\_to\_df}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\NormalTok{df }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, target\_column}\OperatorTok{=}\NormalTok{target\_column, n\_total}\OperatorTok{=}\NormalTok{n\_samples)}
\NormalTok{df.describe(include}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& 1 & 2 & y \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
count & 5.300000e+03 & 5.300000e+03 & 5300 \\
unique & NaN & NaN & 2 \\
top & NaN & NaN & False \\
freq & NaN & NaN & 2924 \\
mean & -4.150943e-09 & -1.886792e-10 & NaN \\
std & 1.000000e+00 & 1.000000e+00 & NaN \\
min & -3.089839e+00 & -2.385937e+00 & NaN \\
25\% & -7.533490e-01 & -9.139027e-01 & NaN \\
50\% & -1.523150e-02 & -3.721500e-02 & NaN \\
75\% & 7.818312e-01 & 8.221040e-01 & NaN \\
max & 2.813360e+00 & 3.194302e+00 & NaN \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  Add column names x1 until x2 to the first 2 columns of the dataframe
  and the column name y to the last column of the dataframe.
\item
  Then split the data frame into a training and test data set. The train
  and test data sets are stored in the \texttt{fun\_control} dictionary.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, dataset.n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\StringTok{"y"}\NormalTok{]}
\CommentTok{\# map the target from False to integer 0 and from True to integer 1}
\NormalTok{df[}\StringTok{"y"}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[}\StringTok{"y"}\NormalTok{].astype(}\BuiltInTok{int}\NormalTok{)}

\NormalTok{fun\_control.update(\{}\StringTok{"train"}\NormalTok{:  df[:n\_train],}
                    \StringTok{"oml\_grace\_period"}\NormalTok{: oml\_grace\_period,}
                    \StringTok{"test"}\NormalTok{:  df[n\_train:],}
                    \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
                    \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{specification-of-the-preprocessing-model-2}{%
\section{Specification of the Preprocessing
Model}\label{specification-of-the-preprocessing-model-2}}

\begin{itemize}
\tightlist
\item
  We use the \texttt{StandardScaler}
  \href{https://riverml.xyz/dev/api/preprocessing/StandardScaler/}{{[}SOURCE{]}}
  from \texttt{river} as the preprocessing model. The
  \texttt{StandardScaler} is used to standardize the data set, i.e., it
  has zero mean and unit variance.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river }\ImportTok{import}\NormalTok{ preprocessing}
\NormalTok{prep\_model }\OperatorTok{=}\NormalTok{ preprocessing.StandardScaler()}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{selectselect-model-algorithm-and-core_model_hyper_dict-2}{%
\section{\texorpdfstring{SelectSelect Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{SelectSelect Model (algorithm) and core\_model\_hyper\_dict}}\label{selectselect-model-algorithm-and-core_model_hyper_dict-2}}

\texttt{spotPython} hyperparameter tuning approach uses two components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a model (class) and
\item
  an associated hyperparameter dictionary.
\end{enumerate}

The corresponding hyperparameters are loaded from the associated
dictionary, which is stored as a JSON file
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}.
The JSON file contains hyperparameter type information, names, and
bounds.

The method \texttt{add\_core\_model\_to\_fun\_control} adds the model
and the hyperparameter dictionary to the \texttt{fun\_control}
dictionary.

Alternatively, you can load a local hyper\_dict. Simply set
\texttt{river\_hyper\_dict.json} as the filename. If \texttt{filename}is
set to \texttt{None}, which is the default, the hyper\_dict
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}
is loaded from the \texttt{spotRiver} package.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.forest }\ImportTok{import}\NormalTok{ AMFClassifier}
\ImportTok{from}\NormalTok{ spotRiver.data.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{AMFClassifier,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-2}{%
\section{\texorpdfstring{Modify \texttt{hyper\_dict} Hyperparameters for
the Selected Algorithm aka
\texttt{core\_model}}{Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-2}}

After the \texttt{core\_model} and the \texttt{core\_model\_hyper\_dict}
are added to the \texttt{fun\_control} dictionary, the hyperparameter
tuning can be started. However, in some settings, the user wants to
modify the hyperparameters of the \texttt{core\_model\_hyper\_dict}.
This can be done with the \texttt{modify\_hyper\_parameter\_bounds} and
\texttt{modify\_hyper\_parameter\_levels} functions
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}}.

The following code shows how hyperparameter of type numeric and integer
(boolean) can be modified. The \texttt{modify\_hyper\_parameter\_bounds}
function is used to modify the bounds of the hyperparameter
\texttt{delta} and \texttt{merit\_preprune}. Similar option exists for
the \texttt{modify\_hyper\_parameter\_levels} function to modify the
levels of categorical hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"n\_estimators"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{,}\DecValTok{20}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"step"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{0.5}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

::: \{.callout-note\} \#\#\#\# Note: Active and Inactive Hyperparameters
Hyperparameters can be excluded from the tuning procedure by selecting
identical values for the lower and upper bounds.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper | transform   |
|-----------------|--------|-----------|---------|---------|-------------|
| n_estimators    | int    |      10   |    2    |      20 | None        |
| step            | float  |       1   |    0.5  |       2 | None        |
| use_aggregation | factor |       1   |    0    |       1 | None        |
| dirichlet       | float  |       0.5 |    0.01 |       1 | None        |
| split_pure      | factor |       0   |    0    |       1 | None        |
\end{verbatim}

\hypertarget{selection-of-the-objective-loss-function-2}{%
\section{Selection of the Objective (Loss)
Function}\label{selection-of-the-objective-loss-function-2}}

The \texttt{metric\_sklearn} is used for the sklearn based evaluation
via \texttt{eval\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_bml.py}{{[}SOURCE{]}}.
Here we use the \texttt{mean\_absolute\_error}
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html}{{[}SOURCE{]}}
as the objective function.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Additional metrics}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotRiver} also supports additional metrics. For example, the
\texttt{metric\_river} is used for the river based evaluation via
\texttt{eval\_oml\_iter\_progressive}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_oml.py}{{[}SOURCE{]}}.
The \texttt{metric\_river} is implemented to simulate the behaviour of
the ``original'' \texttt{river} metrics.

\end{tcolorbox}

\texttt{spotRiver} provides information about the model's score
(metric), memory, and time. The hyperparamter tuner requires a single
objective. Therefore, a weighted sum of the metric, memory, and time is
computed. The weights are defined in the \texttt{weights} array.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Weights}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The \texttt{weights} provide a flexible way to define specific
requirements, e.g., if the memory is more important than the time, the
weight for the memory can be increased.

\end{tcolorbox}

The \texttt{oml\_grace\_period} defines the number of observations that
are used for the initial training of the model. The
\texttt{weight\_coeff} defines a multiplier for the results: results are
multiplied by (step/n\_steps)**weight\_coeff, where n\_steps is the
total number of iterations. Results from the beginning have a lower
weight than results from the end if weight\_coeff \textgreater{} 1. If
weight\_coeff == 0, all results have equal weight. Note, that the
\texttt{weight\_coeff} is only used internally for the tuner and does
not affect the results that are used for the evaluation or comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ accuracy\_score}

\NormalTok{weights }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}} \DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{])}\OperatorTok{*}\FloatTok{10\_000.0}
\NormalTok{weight\_coeff }\OperatorTok{=} \FloatTok{1.0}

\NormalTok{fun\_control.update(\{}
               \StringTok{"horizon"}\NormalTok{: horizon,}
               \StringTok{"oml\_grace\_period"}\NormalTok{: oml\_grace\_period,}
               \StringTok{"weights"}\NormalTok{: weights,}
               \StringTok{"weight\_coeff"}\NormalTok{: weight\_coeff,}
               \StringTok{"metric\_sklearn"}\NormalTok{: accuracy\_score}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\hypertarget{calling-the-spot-function-2}{%
\section{Calling the SPOT Function}\label{calling-the-spot-function-2}}

\hypertarget{prepare-the-spot-parameters-2}{%
\subsection{Prepare the SPOT
Parameters}\label{prepare-the-spot-parameters-2}}

The hyperparameter tuning configuration is stored in the
\texttt{fun\_control} dictionary. Since \texttt{Spot} can be used as an
optimization algorithm with a similar interface as optimization
algorithms from \texttt{scipy.optimize}
\href{https://docs.scipy.org/doc/scipy/reference/optimize.html\#module-scipy.optimize}{{[}LINK{]}},
the bounds and variable types have to be specified explicitely. The
\texttt{get\_var\_type}, \texttt{get\_var\_name}, and
\texttt{get\_bound\_values} functions
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}}
implement the required functionality.

\begin{itemize}
\tightlist
\item
  Get types and variable names as well as lower and upper bounds for the
  hyperparameters, so that they can be passed to the \texttt{Spot}
  function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (}
\NormalTok{    get\_var\_type,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_bound\_values}
\NormalTok{    )}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-the-objective-function-13}{%
\subsection{The Objective
Function}\label{sec-the-objective-function-13}}

The objective function \texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py}{{[}SOURCE{]}}
is selected next.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.fun.hyperriver }\ImportTok{import}\NormalTok{ HyperRiver}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperRiver(log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{).fun\_oml\_horizon}
\end{Highlighting}
\end{Shaded}

The following code snippet shows how to get the default hyperparameters
as an array, so that they can be passed to the \texttt{Spot} function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\hypertarget{run-the-spot-optimizer-7}{%
\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-7}}

The class \texttt{Spot}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}
is the hyperparameter tuning workhorse. It is initialized with the
following parameters:

\begin{itemize}
\tightlist
\item
  \texttt{fun}: the objective function
\item
  \texttt{lower}: lower bounds of the hyperparameters
\item
  \texttt{upper}: upper bounds of the hyperparameters
\item
  \texttt{fun\_evals}: number of function evaluations
\item
  \texttt{max\_time}: maximum time in seconds
\item
  \texttt{tolerance\_x}: tolerance for the hyperparameters
\item
  \texttt{var\_type}: variable types of the hyperparameters
\item
  \texttt{var\_name}: variable names of the hyperparameters
\item
  \texttt{show\_progress}: show progress bar
\item
  \texttt{fun\_control}: dictionary with control parameters for the
  objective function
\item
  \texttt{design\_control}: dictionary with control parameters for the
  initial design
\item
  \texttt{surrogate\_control}: dictionary with control parameters for
  the surrogate model
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Total run time}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The total run time may exceed the specified \texttt{max\_time}, because
the initial design (here: \texttt{init\_size} = INIT\_SIZE as specified
above) is always evaluated, even if this takes longer than
\texttt{max\_time}.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{\},}
\NormalTok{                                      log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: -1.6882598276739205 [######----] 59.70% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: -1.6882598276739205 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x31c0afd50>
\end{verbatim}

\hypertarget{sec-tensorboard-10}{%
\subsection{TensorBoard}\label{sec-tensorboard-10}}

Now we can start TensorBoard in the background with the following
command, where \texttt{./runs} is the default directory for the
TensorBoard log files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TENSORBOARD\_PATH}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, colframe=quarto-callout-tip-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The TensorBoard path can be printed with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_tensorboard\_path}
\NormalTok{get\_tensorboard\_path(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'runs/'
\end{verbatim}

\end{tcolorbox}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/build/kriging.py}{{[}SOURCE{]}}
is plotted against the number of optimization steps.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/13_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}

\hypertarget{results-6}{%
\subsection{Results}\label{results-6}}

After the hyperparameter tuning run is finished, the results can be
saved and reloaded with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle}
\NormalTok{save\_pickle(spot\_tuner, experiment\_name)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_pickle}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ load\_pickle(experiment\_name)}
\end{Highlighting}
\end{Shaded}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-21-output-1.pdf}

}

\end{figure}

Results can also be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper |   tuned | transform   |   importance | stars   |
|-----------------|--------|-----------|---------|---------|---------|-------------|--------------|---------|
| n_estimators    | int    |      10.0 |     2.0 |    20.0 |    17.0 | None        |         0.00 |         |
| step            | float  |       1.0 |     0.5 |     2.0 |     0.5 | None        |         1.06 | *       |
| use_aggregation | factor |       1.0 |     0.0 |     1.0 |     1.0 | None        |       100.00 | ***     |
| dirichlet       | float  |       0.5 |    0.01 |     1.0 |     1.0 | None        |         0.22 | .       |
| split_pure      | factor |       0.0 |     0.0 |     1.0 |     0.0 | None        |        19.02 | *       |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.0025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-23-output-1.pdf}

}

\end{figure}

\hypertarget{get-default-hyperparameters-7}{%
\section{Get Default
Hyperparameters}\label{get-default-hyperparameters-7}}

The default hyperparameters, whihc will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: \texttt{spotPython} tunes numpy arrays}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{spotPython} tunes numpy arrays, i.e., the hyperparameters are
  stored in a numpy array.
\end{itemize}

\end{tcolorbox}

The model with the default hyperparameters can be trained and evaluated
with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ eval\_oml\_horizon}

\NormalTok{df\_eval\_default, df\_true\_default }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_default,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-spot-results-7}{%
\section{Get SPOT Results}\label{get-spot-results-7}}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval\_spot, df\_true\_spot }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_spot,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ plot\_bml\_oml\_horizon\_metrics}
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{, }\StringTok{"spot"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default, df\_eval\_spot], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{], filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_metrics.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-28-output-1.pdf}

}

\end{figure}

\hypertarget{compare-predictions}{%
\section{Compare Predictions}\label{compare-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_roc\_from\_dataframes}
\NormalTok{plot\_roc\_from\_dataframes([df\_true\_default, df\_true\_spot], model\_names}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{, }\StringTok{"spot"}\NormalTok{], target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{])  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-29-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_confusion\_matrix}
\NormalTok{plot\_confusion\_matrix(df}\OperatorTok{=}\NormalTok{df\_true\_default, title}\OperatorTok{=}\StringTok{"Default"}\NormalTok{, y\_true\_name}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{], y\_pred\_name}\OperatorTok{=}\StringTok{"Prediction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-30-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_confusion\_matrix(df}\OperatorTok{=}\NormalTok{df\_true\_spot, title}\OperatorTok{=}\StringTok{"Spot"}\NormalTok{, y\_true\_name}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{], y\_pred\_name}\OperatorTok{=}\StringTok{"Prediction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-31-output-1.pdf}

}

\end{figure}

\hypertarget{detailed-hyperparameter-plots-7}{%
\section{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-7}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
step:  1.0644851680637486
use_aggregation:  100.0
dirichlet:  0.21705028182135652
split_pure:  19.02272443784279
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-32-output-2.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-32-output-3.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-32-output-4.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-32-output-5.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-32-output-6.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{026_spot_hpt_river_bananas_amfc_files/figure-pdf/cell-32-output-7.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plots-2}{%
\section{Parallel Coordinates
Plots}\label{parallel-coordinates-plots-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-7}{%
\section{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-7}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\part{Hyperparameter Tuning with PyTorch}

\hypertarget{sec-hpt-pytorch}{%
\chapter{HPT: PyTorch}\label{sec-hpt-pytorch}}

\hypertarget{sec-hpt-pytorch-intro}{%
\section{Introduction to PyTorch}\label{sec-hpt-pytorch-intro}}

\hypertarget{sec-hyperparameter-tuning-for-pytorch-14}{%
\chapter{\texorpdfstring{HPT: PyTorch With \texttt{spotPython} and Ray
Tune on
CIFAR10}{HPT: PyTorch With spotPython and Ray Tune on CIFAR10}}\label{sec-hyperparameter-tuning-for-pytorch-14}}

In this tutorial, we will show how \texttt{spotPython} can be integrated
into the \texttt{PyTorch} training workflow. It is based on the tutorial
``Hyperparameter Tuning with Ray Tune'' from the \texttt{PyTorch}
documentation (PyTorch 2023a), which is an extension of the tutorial
``Training a Classifier'' (PyTorch 2023b) for training a CIFAR10 image
classifier.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: PyTorch and Lightning}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Instead of using the \texttt{PyTorch} interface directly as explained in
this chapter, we recommend using the \texttt{PyTorch\ Lightning}
interface. The \texttt{PyTorch\ Lightning} interface is explained in
Chapter~\ref{sec-hyperparameter-tuning-lightning-31}

\end{tcolorbox}

A typical hyperparameter tuning process with \texttt{spotPython}
consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Loading the data (training and test datasets), see
  Section~\ref{sec-data-loading-14}.
\item
  Specification of the preprocessing model, see
  Section~\ref{sec-specification-of-preprocessing-model-14}. This model
  is called \texttt{prep\_model} (``preparation'' or pre-processing).
  The information required for the hyperparameter tuning is stored in
  the dictionary \texttt{fun\_control}. Thus, the information needed for
  the execution of the hyperparameter tuning is available in a readable
  form.
\item
  Selection of the machine learning or deep learning model to be tuned,
  see Section~\ref{sec-selection-of-the-algorithm-14}. This is called
  the \texttt{core\_model}. Once the \texttt{core\_model} is defined,
  then the associated hyperparameters are stored in the
  \texttt{fun\_control} dictionary. First, the hyperparameters of the
  \texttt{core\_model} are initialized with the default values of the
  \texttt{core\_model}. As default values we use the default values
  contained in the \texttt{spotPython} package for the algorithms of the
  \texttt{torch} package.
\item
  Modification of the default values for the hyperparameters used in
  \texttt{core\_model}, see
  Section~\ref{sec-modification-of-default-values}. This step is
  optional.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    numeric parameters are modified by changing the bounds.
  \item
    categorical parameters are modified by changing the categories
    (``levels'').
  \end{enumerate}
\item
  Selection of target function (loss function) for the optimizer, see
  Section~\ref{sec-loss-functions-14}.
\item
  Calling SPOT with the corresponding parameters, see
  Section~\ref{sec-call-the-hyperparameter-tuner-14}. The results are
  stored in a dictionary and are available for further analysis.
\item
  Presentation, visualization and interpretation of the results, see
  Section~\ref{sec-results-14}.
\end{enumerate}

\texttt{spotPython} can be installed via pip\footnote{Alternatively, the
  source code can be downloaded from gitHub:
  \url{https://github.com/sequential-parameter-optimization/spotPython}.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{!pip install spotPython}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Uncomment the following lines if you want to for (re-)installation the
  latest version of \texttt{spotPython} from gitHub.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import sys}
\CommentTok{\# !\{sys.executable\} {-}m pip install {-}{-}upgrade build}
\CommentTok{\# !\{sys.executable\} {-}m pip install {-}{-}upgrade {-}{-}force{-}reinstall spotPython}
\end{Highlighting}
\end{Shaded}

Results that refer to the \texttt{Ray\ Tune} package are taken from
\url{https://PyTorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html}\footnote{We
  were not able to install \texttt{Ray\ Tune} on our system. Therefore,
  we used the results from the \texttt{PyTorch} tutorial.}.

\hypertarget{sec-setup-14}{%
\section{Step 1: Setup}\label{sec-setup-14}}

Before we consider the detailed experimental setup, we select the
parameters that affect run time, initial design size and the device that
is used.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  MAX\_TIME is set to one minute for demonstration purposes. For real
  experiments, this should be increased to at least 1 hour.
\item
  INIT\_SIZE is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Device selection}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  The device can be selected by setting the variable \texttt{DEVICE}.
\item
  Since we are using a simple neural net, the setting \texttt{"cpu"} is
  preferred (on Mac).
\item
  If you have a GPU, you can use \texttt{"cuda:0"} instead.
\item
  If DEVICE is set to \texttt{"auto"} or \texttt{None},
  \texttt{spotPython} will automatically select the device.

  \begin{itemize}
  \tightlist
  \item
    This might result in \texttt{"mps"} on Macs, which is not the best
    choice for simple neural nets.
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{DEVICE }\OperatorTok{=} \StringTok{"auto"} \CommentTok{\# "cpu"}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"14{-}torch"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\NormalTok{DEVICE }\OperatorTok{=}\NormalTok{ getDevice(DEVICE)}
\BuiltInTok{print}\NormalTok{(DEVICE)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mps
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-initialization-fun-control-14}{%
\section{\texorpdfstring{Step 2: Initialization of the
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the fun\_control Dictionary}}\label{sec-initialization-fun-control-14}}

\texttt{spotPython} uses a Python dictionary for storing the information
required for the hyperparameter tuning process. This dictionary is
called \texttt{fun\_control} and is initialized with the function
\texttt{fun\_control\_init}. The function \texttt{fun\_control\_init}
returns a skeleton dictionary. The dictionary is filled with the
required information for the hyperparameter tuning process. It stores
the hyperparameter tuning settings, e.g., the deep learning network
architecture that should be tuned, the classification (or regression)
problem, and the data that is used for the tuning. The dictionary is
used as an input for the SPOT function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}

\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    task}\OperatorTok{=}\StringTok{"classification"}\NormalTok{,}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    device}\OperatorTok{=}\NormalTok{DEVICE,)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-data-loading-14}{%
\section{Step 3: PyTorch Data Loading}\label{sec-data-loading-14}}

The data loading process is implemented in the same manner as described
in the Section ``Data loaders'' in PyTorch (2023a). The data loaders are
wrapped into the function \texttt{load\_data\_cifar10} which is
identical to the function \texttt{load\_data} in PyTorch (2023a). A
global data directory is used, which allows sharing the data directory
between different trials. The method \texttt{load\_data\_cifar10} is
part of the \texttt{spotPython} package and can be imported from
\texttt{spotPython.data.torchdata}.

In the following step, the test and train data are added to the
dictionary \texttt{fun\_control}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.data.torchdata }\ImportTok{import}\NormalTok{ load\_data\_cifar10}
\NormalTok{train, test }\OperatorTok{=}\NormalTok{ load\_data\_cifar10()}
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(train)}
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}
    \StringTok{"train"}\NormalTok{: train,}
    \StringTok{"test"}\NormalTok{: test,}
    \StringTok{"n\_samples"}\NormalTok{: n\_samples\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Files already downloaded and verified
\end{verbatim}

\begin{verbatim}
Files already downloaded and verified
\end{verbatim}

\hypertarget{sec-specification-of-preprocessing-model-14}{%
\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-14}}

After the training and test data are specified and added to the
\texttt{fun\_control} dictionary, \texttt{spotPython} allows the
specification of a data preprocessing pipeline, e.g., for the scaling of
the data or for the one-hot encoding of categorical variables. The
preprocessing model is called \texttt{prep\_model} (``preparation'' or
pre-processing) and includes steps that are not subject to the
hyperparameter tuning process. The preprocessing model is specified in
the \texttt{fun\_control} dictionary. The preprocessing model can be
implemented as a \texttt{sklearn} pipeline. The following code shows a
typical preprocessing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_columns = ["cities", "colors"]}
\NormalTok{one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore",}
\NormalTok{                                    sparse\_output=False)}
\NormalTok{prep\_model = ColumnTransformer(}
\NormalTok{        transformers=[}
\NormalTok{             ("categorical", one\_hot\_encoder, categorical\_columns),}
\NormalTok{         ],}
\NormalTok{         remainder=StandardScaler(),}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

Because the Ray Tune (\texttt{ray{[}tune{]}}) hyperparameter tuning as
described in PyTorch (2023a) does not use a preprocessing model, the
preprocessing model is set to \texttt{None} here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-selection-of-the-algorithm-14}{%
\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{sec-selection-of-the-algorithm-14}}

The same neural network model as implemented in the section
``Configurable neural network'' of the \texttt{PyTorch} tutorial
(PyTorch 2023a) is used here. We will show the implementation from
PyTorch (2023a) in Section~\ref{sec-implementation-with-raytune} first,
before the extended implementation with \texttt{spotPython} is shown in
Section~\ref{sec-implementation-with-spotpython-14}.

\hypertarget{sec-implementation-with-raytune}{%
\subsubsection{Implementing a Configurable Neural Network With Ray
Tune}\label{sec-implementation-with-raytune}}

We used the same hyperparameters that are implemented as configurable in
the \texttt{PyTorch} tutorial. We specify the layer sizes, namely
\texttt{l1} and \texttt{l2}, of the fully connected layers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{class Net(nn.Module):}
\NormalTok{    def \_\_init\_\_(self, l1=120, l2=84):}
\NormalTok{        super(Net, self).\_\_init\_\_()}
\NormalTok{        self.conv1 = nn.Conv2d(3, 6, 5)}
\NormalTok{        self.pool = nn.MaxPool2d(2, 2)}
\NormalTok{        self.conv2 = nn.Conv2d(6, 16, 5)}
\NormalTok{        self.fc1 = nn.Linear(16 * 5 * 5, l1)}
\NormalTok{        self.fc2 = nn.Linear(l1, l2)}
\NormalTok{        self.fc3 = nn.Linear(l2, 10)}

\NormalTok{    def forward(self, x):}
\NormalTok{        x = self.pool(F.relu(self.conv1(x)))}
\NormalTok{        x = self.pool(F.relu(self.conv2(x)))}
\NormalTok{        x = x.view({-}1, 16 * 5 * 5)}
\NormalTok{        x = F.relu(self.fc1(x))}
\NormalTok{        x = F.relu(self.fc2(x))}
\NormalTok{        x = self.fc3(x)}
\NormalTok{        return x}
\end{Highlighting}
\end{Shaded}

The learning rate, i.e., \texttt{lr}, of the optimizer is made
configurable, too:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-implementation-with-spotpython-14}{%
\subsubsection{Implementing a Configurable Neural Network With
spotPython}\label{sec-implementation-with-spotpython-14}}

\texttt{spotPython} implements a class which is similar to the class
described in the \texttt{PyTorch} tutorial. The class is called
\texttt{Net\_CIFAR10} and is implemented in the file
\texttt{netcifar10.py}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from torch import nn}
\NormalTok{import torch.nn.functional as F}
\NormalTok{import spotPython.torch.netcore as netcore}


\NormalTok{class Net\_CIFAR10(netcore.Net\_Core):}
\NormalTok{    def \_\_init\_\_(self, l1, l2, lr\_mult, batch\_size, epochs, k\_folds, patience,}
\NormalTok{    optimizer, sgd\_momentum):}
\NormalTok{        super(Net\_CIFAR10, self).\_\_init\_\_(}
\NormalTok{            lr\_mult=lr\_mult,}
\NormalTok{            batch\_size=batch\_size,}
\NormalTok{            epochs=epochs,}
\NormalTok{            k\_folds=k\_folds,}
\NormalTok{            patience=patience,}
\NormalTok{            optimizer=optimizer,}
\NormalTok{            sgd\_momentum=sgd\_momentum,}
\NormalTok{        )}
\NormalTok{        self.conv1 = nn.Conv2d(3, 6, 5)}
\NormalTok{        self.pool = nn.MaxPool2d(2, 2)}
\NormalTok{        self.conv2 = nn.Conv2d(6, 16, 5)}
\NormalTok{        self.fc1 = nn.Linear(16 * 5 * 5, l1)}
\NormalTok{        self.fc2 = nn.Linear(l1, l2)}
\NormalTok{        self.fc3 = nn.Linear(l2, 10)}

\NormalTok{    def forward(self, x):}
\NormalTok{        x = self.pool(F.relu(self.conv1(x)))}
\NormalTok{        x = self.pool(F.relu(self.conv2(x)))}
\NormalTok{        x = x.view({-}1, 16 * 5 * 5)}
\NormalTok{        x = F.relu(self.fc1(x))}
\NormalTok{        x = F.relu(self.fc2(x))}
\NormalTok{        x = self.fc3(x)}
\NormalTok{        return x}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-the-netcore-class-14}{%
\subsection{\texorpdfstring{The \texttt{Net\_Core}
class}{The Net\_Core class}}\label{sec-the-netcore-class-14}}

\texttt{Net\_CIFAR10} inherits from the class \texttt{Net\_Core} which
is implemented in the file \texttt{netcore.py}. It implements the
additional attributes that are common to all neural network models. The
\texttt{Net\_Core} class is implemented in the file \texttt{netcore.py}.
It implements hyperparameters as attributes, that are not used by the
\texttt{core\_model}, e.g.:

\begin{itemize}
\tightlist
\item
  optimizer (\texttt{optimizer}),
\item
  learning rate (\texttt{lr}),
\item
  batch size (\texttt{batch\_size}),
\item
  epochs (\texttt{epochs}),
\item
  k\_folds (\texttt{k\_folds}), and
\item
  early stopping criterion ``patience'' (\texttt{patience}).
\end{itemize}

Users can add further attributes to the class. The class
\texttt{Net\_Core} is shown below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from torch import nn}


\NormalTok{class Net\_Core(nn.Module):}
\NormalTok{    def \_\_init\_\_(self, lr\_mult, batch\_size, epochs, k\_folds, patience,}
\NormalTok{        optimizer, sgd\_momentum):}
\NormalTok{        super(Net\_Core, self).\_\_init\_\_()}
\NormalTok{        self.lr\_mult = lr\_mult}
\NormalTok{        self.batch\_size = batch\_size}
\NormalTok{        self.epochs = epochs}
\NormalTok{        self.k\_folds = k\_folds}
\NormalTok{        self.patience = patience}
\NormalTok{        self.optimizer = optimizer}
\NormalTok{        self.sgd\_momentum = sgd\_momentum}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-comparison}{%
\subsection{Comparison of the Approach Described in the PyTorch Tutorial
With spotPython}\label{sec-comparison}}

Comparing the class \texttt{Net} from the \texttt{PyTorch} tutorial and
the class \texttt{Net\_CIFAR10} from \texttt{spotPython}, we see that
the class \texttt{Net\_CIFAR10} has additional attributes and does not
inherit from \texttt{nn} directly. It adds an additional class,
\texttt{Net\_core}, that takes care of additional attributes that are
common to all neural network models, e.g., the learning rate multiplier
\texttt{lr\_mult} or the batch size \texttt{batch\_size}.

\texttt{spotPython}'s \texttt{core\_model} implements an instance of the
\texttt{Net\_CIFAR10} class. In addition to the basic neural network
model, the \texttt{core\_model} can use these additional attributes.
\texttt{spotPython} provides methods for handling these additional
attributes to guarantee 100\% compatibility with the \texttt{PyTorch}
classes. The method \texttt{add\_core\_model\_to\_fun\_control} adds the
hyperparameters and additional attributes to the \texttt{fun\_control}
dictionary. The method is shown below.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.torch.netcifar10 }\ImportTok{import}\NormalTok{ Net\_CIFAR10}
\ImportTok{from}\NormalTok{ spotPython.data.torch\_hyper\_dict }\ImportTok{import}\NormalTok{ TorchHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{core\_model }\OperatorTok{=}\NormalTok{ Net\_CIFAR10}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{core\_model,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{TorchHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-search-space-14}{%
\subsection{The Search Space:
Hyperparameters}\label{sec-search-space-14}}

In Section~\ref{sec-configuring-the-search-space-with-ray-tune}, we
first describe how to configure the search space with
\texttt{ray{[}tune{]}} (as shown in PyTorch (2023a)) and then how to
configure the search space with \texttt{spotPython} in -14.

\hypertarget{sec-configuring-the-search-space-with-ray-tune}{%
\subsection{Configuring the Search Space With Ray
Tune}\label{sec-configuring-the-search-space-with-ray-tune}}

Ray Tune's search space can be configured as follows (PyTorch 2023a):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{config = \{}
\NormalTok{    "l1": tune.sample\_from(lambda \_: 2**np.random.randint(2, 9)),}
\NormalTok{    "l2": tune.sample\_from(lambda \_: 2**np.random.randint(2, 9)),}
\NormalTok{    "lr": tune.loguniform(1e{-}4, 1e{-}1),}
\NormalTok{    "batch\_size": tune.choice([2, 4, 8, 16])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The \texttt{tune.sample\_from()} function enables the user to define
sample methods to obtain hyperparameters. In this example, the
\texttt{l1} and \texttt{l2} parameters should be powers of 2 between 4
and 256, so either 4, 8, 16, 32, 64, 128, or 256. The \texttt{lr}
(learning rate) should be uniformly sampled between 0.0001 and 0.1.
Lastly, the batch size is a choice between 2, 4, 8, and 16.

At each trial, \texttt{ray{[}tune{]}} will randomly sample a combination
of parameters from these search spaces. It will then train a number of
models in parallel and find the best performing one among these.
\texttt{ray{[}tune{]}} uses the \texttt{ASHAScheduler} which will
terminate bad performing trials early.

\hypertarget{sec-configuring-search-space-spotpython-14}{%
\subsection{Configuring the Search Space With
spotPython}\label{sec-configuring-search-space-spotpython-14}}

\hypertarget{the-hyper_dict-hyperparameters-for-the-selected-algorithm}{%
\subsubsection{\texorpdfstring{The \texttt{hyper\_dict} Hyperparameters
for the Selected
Algorithm}{The hyper\_dict Hyperparameters for the Selected Algorithm}}\label{the-hyper_dict-hyperparameters-for-the-selected-algorithm}}

\texttt{spotPython} uses \texttt{JSON} files for the specification of
the hyperparameters. Users can specify their individual \texttt{JSON}
files, or they can use the \texttt{JSON} files provided by
\texttt{spotPython}. The \texttt{JSON} file for the \texttt{core\_model}
is called \texttt{torch\_hyper\_dict.json}.

In contrast to \texttt{ray{[}tune{]}}, \texttt{spotPython} can handle
numerical, boolean, and categorical hyperparameters. They can be
specified in the \texttt{JSON} file in a similar way as the numerical
hyperparameters as shown below. Each entry in the \texttt{JSON} file
represents one hyperparameter with the following structure:
\texttt{type}, \texttt{default}, \texttt{transform}, \texttt{lower}, and
\texttt{upper}.

\begin{Shaded}
\begin{Highlighting}[]
\ErrorTok{"factor\_hyperparameter":} \FunctionTok{\{}
    \DataTypeTok{"levels"}\FunctionTok{:} \OtherTok{[}\StringTok{"A"}\OtherTok{,} \StringTok{"B"}\OtherTok{,} \StringTok{"C"}\OtherTok{]}\FunctionTok{,}
    \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"factor"}\FunctionTok{,}
    \DataTypeTok{"default"}\FunctionTok{:} \StringTok{"B"}\FunctionTok{,}
    \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"None"}\FunctionTok{,}
    \DataTypeTok{"core\_model\_parameter\_type"}\FunctionTok{:} \StringTok{"str"}\FunctionTok{,}
    \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{0}\FunctionTok{,}
    \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{2}\FunctionTok{\}}\ErrorTok{,}
\end{Highlighting}
\end{Shaded}

The corresponding entries for the core\_model` class are shown below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control[}\StringTok{\textquotesingle{}core\_model\_hyper\_dict\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'l1': {'type': 'int',
  'default': 5,
  'transform': 'transform_power_2_int',
  'lower': 2,
  'upper': 9},
 'l2': {'type': 'int',
  'default': 5,
  'transform': 'transform_power_2_int',
  'lower': 2,
  'upper': 9},
 'lr_mult': {'type': 'float',
  'default': 1.0,
  'transform': 'None',
  'lower': 0.1,
  'upper': 10.0},
 'batch_size': {'type': 'int',
  'default': 4,
  'transform': 'transform_power_2_int',
  'lower': 1,
  'upper': 4},
 'epochs': {'type': 'int',
  'default': 3,
  'transform': 'transform_power_2_int',
  'lower': 3,
  'upper': 4},
 'k_folds': {'type': 'int',
  'default': 1,
  'transform': 'None',
  'lower': 1,
  'upper': 1},
 'patience': {'type': 'int',
  'default': 5,
  'transform': 'None',
  'lower': 2,
  'upper': 10},
 'optimizer': {'levels': ['Adadelta',
   'Adagrad',
   'Adam',
   'AdamW',
   'SparseAdam',
   'Adamax',
   'ASGD',
   'NAdam',
   'RAdam',
   'RMSprop',
   'Rprop',
   'SGD'],
  'type': 'factor',
  'default': 'SGD',
  'transform': 'None',
  'class_name': 'torch.optim',
  'core_model_parameter_type': 'str',
  'lower': 0,
  'upper': 12},
 'sgd_momentum': {'type': 'float',
  'default': 0.0,
  'transform': 'None',
  'lower': 0.0,
  'upper': 1.0}}
\end{verbatim}

\hypertarget{sec-modification-of-hyperparameters-14}{%
\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{sec-modification-of-hyperparameters-14}}

Ray tune (PyTorch 2023a) does not provide a way to change the specified
hyperparameters without re-compilation. However, \texttt{spotPython}
provides functions for modifying the hyperparameters, their bounds and
factors as well as for activating and de-activating hyperparameters
without re-compilation of the Python source code. These functions are
described in the following.

\hypertarget{sec-modification-of-default-values}{%
\subsubsection{\texorpdfstring{Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{sec-modification-of-default-values}}

After specifying the model, the corresponding hyperparameters, their
types and bounds are loaded from the \texttt{JSON} file
\texttt{torch\_hyper\_dict.json}. After loading, the user can modify the
hyperparameters, e.g., the bounds. \texttt{spotPython} provides a simple
rule for de-activating hyperparameters: If the lower and the upper bound
are set to identical values, the hyperparameter is de-activated. This is
useful for the hyperparameter tuning, because it allows to specify a
hyperparameter in the \texttt{JSON} file, but to de-activate it in the
\texttt{fun\_control} dictionary. This is done in the next step.

\hypertarget{modify-hyperparameters-of-type-numeric-and-integer-boolean}{%
\subsubsection{Modify Hyperparameters of Type numeric and integer
(boolean)}\label{modify-hyperparameters-of-type-numeric-and-integer-boolean}}

Since the hyperparameter \texttt{k\_folds} is not used in the
\texttt{PyTorch} tutorial, it is de-activated here by setting the lower
and upper bound to the same value. Note, \texttt{k\_folds} is of type
``integer''.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }
    \StringTok{"batch\_size"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }
    \StringTok{"k\_folds"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }
    \StringTok{"patience"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{modify-hyperparameter-of-type-factor-5}{%
\subsubsection{Modify Hyperparameter of Type
factor}\label{modify-hyperparameter-of-type-factor-5}}

In a similar manner as for the numerical hyperparameters, the
categorical hyperparameters can be modified. New configurations can be
chosen by adding or deleting levels. For example, the hyperparameter
\texttt{optimizer} can be re-configured as follows:

In the following setting, two optimizers (\texttt{"SGD"} and
\texttt{"Adam"}) will be compared during the \texttt{spotPython}
hyperparameter tuning. The hyperparameter \texttt{optimizer} is active.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control,}
     \StringTok{"optimizer"}\NormalTok{, [}\StringTok{"SGD"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

The hyperparameter \texttt{optimizer} can be de-activated by choosing
only one value (level), here: \texttt{"SGD"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [}\StringTok{"SGD"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

As discussed in Section~\ref{sec-optimizers-14}, there are some issues
with the LBFGS optimizer. Therefore, the usage of the LBFGS optimizer is
not deactivated in \texttt{spotPython} by default. However, the LBFGS
optimizer can be activated by adding it to the list of optimizers.
\texttt{Rprop} was removed, because it does perform very poorly (as some
pre-tests have shown). However, it can also be activated by adding it to
the list of optimizers. Since \texttt{SparseAdam} does not support dense
gradients, \texttt{Adam} was used instead. Therefore, there are 10
default optimizers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"optimizer"}\NormalTok{,}
\NormalTok{    [}\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adagrad"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"AdamW"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{, }\StringTok{"ASGD"}\NormalTok{, }
    \StringTok{"NAdam"}\NormalTok{, }\StringTok{"RAdam"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-optimizers-14}{%
\subsection{Optimizers}\label{sec-optimizers-14}}

Table~\ref{tbl-optimizers} shows some of the optimizers available in
\texttt{PyTorch}:

\(a\) denotes (0.9,0.999), \(b\) (0.5,1.2), and \(c\) (1e-6, 50),
respectively. \(R\) denotes \texttt{required,\ but\ unspecified}. ``m''
denotes \texttt{momentum}, ``w\_d'' \texttt{weight\_decay}, ``d''
\texttt{dampening}, ``n'' \texttt{nesterov}, ``r'' \texttt{rho},
``l\_s'' \texttt{learning\ rate\ for\ scaling\ delta}, ``l\_d''
\texttt{lr\_decay}, ``b'' \texttt{betas}, ``l'' \texttt{lambd}, ``a''
\texttt{alpha}, ``m\_d'' for \texttt{momentum\_decay}, ``e''
\texttt{etas}, and ``s\_s'' for \texttt{step\_sizes}.

\hypertarget{tbl-optimizers}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.2051}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0769}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0769}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0769}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 28\tabcolsep) * \real{0.0513}}@{}}
\caption{\label{tbl-optimizers}Optimizers available in PyTorch
(selection). The default values are shown in the table.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Optimizer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
m
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
w\_d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
r
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
l\_s
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
l\_d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
b
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
l
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
a
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
m\_d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
e
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
s\_s
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Optimizer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
m
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
w\_d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
r
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
l\_s
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
l\_d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
b
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
l
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
a
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
m\_d
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
e
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
s\_s
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Adadelta & - & - & 0. & - & - & 0.9 & 1. & - & - & - & - & - & - & - \\
Adagrad & 1e-2 & - & 0. & - & - & - & - & 0. & - & - & - & - & - & - \\
Adam & 1e-3 & - & 0. & - & - & - & - & - & \(a\) & - & - & - & - & - \\
AdamW & 1e-3 & - & 1e-2 & - & - & - & - & - & \(a\) & - & - & - & - &
- \\
SparseAdam & 1e-3 & - & - & - & - & - & - & - & \(a\) & - & - & - & - &
- \\
Adamax & 2e-3 & - & 0. & - & - & - & - & - & \(a\) & - & - & - & - &
- \\
ASGD & 1e-2 & .9 & 0. & - & F & - & - & - & - & 1e-4 & .75 & - & - &
- \\
LBFGS & 1. & - & - & - & - & - & - & - & - & - & - & - & - & - \\
NAdam & 2e-3 & - & 0. & - & - & - & - & - & \(a\) & - & - & 0 & - & - \\
RAdam & 1e-3 & - & 0. & - & - & - & - & - & \(a\) & - & - & - & - & - \\
RMSprop & 1e-2 & 0. & 0. & - & - & - & - & - & \(a\) & - & - & - & - &
- \\
Rprop & 1e-2 & - & - & - & - & - & - & - & - & - & \(b\) & \(c\) & - &
- \\
SGD & \(R\) & 0. & 0. & 0. & F & - & - & - & - & - & - & - & - & - \\
\end{longtable}

\texttt{spotPython} implements an \texttt{optimization} handler that
maps the optimizer names to the corresponding \texttt{PyTorch}
optimizers.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{A note on LBFGS}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

We recommend deactivating \texttt{PyTorch}'s LBFGS optimizer, because it
does not perform very well. The \texttt{PyTorch} documentation, see
\url{https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html\#torch.optim.LBFGS},
states:

\begin{quote}
This is a very memory intensive optimizer (it requires additional
\texttt{param\_bytes\ *\ (history\_size\ +\ 1)} bytes). If it doesn't
fit in memory try reducing the history size, or use a different
algorithm.
\end{quote}

Furthermore, the LBFGS optimizer is not compatible with the
\texttt{PyTorch} tutorial. The reason is that the LBFGS optimizer
requires the \texttt{closure} function, which is not implemented in the
\texttt{PyTorch} tutorial. Therefore, the \texttt{LBFGS} optimizer is
recommended here. Since there are ten optimizers in the portfolio, it is
not recommended tuning the hyperparameters that effect one single
optimizer only.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{A note on the learning rate}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotPython} provides a multiplier for the default learning
rates, \texttt{lr\_mult}, because optimizers use different learning
rates. Using a multiplier for the learning rates might enable a
simultaneous tuning of the learning rates for all optimizers. However,
this is not recommended, because the learning rates are not comparable
across optimizers. Therefore, we recommend fixing the learning rate for
all optimizers if multiple optimizers are used. This can be done by
setting the lower and upper bounds of the learning rate multiplier to
the same value as shown below.

Thus, the learning rate, which affects the \texttt{SGD} optimizer, will
be set to a fixed value. We choose the default value of \texttt{1e-3}
for the learning rate, because it is used in other \texttt{PyTorch}
examples (it is also the default value used by \texttt{spotPython} as
defined in the \texttt{optimizer\_handler()} method). We recommend
tuning the learning rate later, when a reduced set of optimizers is
fixed. Here, we will demonstrate how to select in a screening phase the
optimizers that should be used for the hyperparameter tuning.

\end{tcolorbox}

For the same reason, we will fix the \texttt{sgd\_momentum} to
\texttt{0.9}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control,}
    \StringTok{"lr\_mult"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control,}
    \StringTok{"sgd\_momentum"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.9}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-7-selection-of-the-objective-loss-function-5}{%
\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function-5}}

\hypertarget{sec-data-splitting-14}{%
\subsection{Evaluation: Data Splitting}\label{sec-data-splitting-14}}

The evaluation procedure requires the specification of the way how the
data is split into a train and a test set and the loss function (and a
metric). As a default, \texttt{spotPython} provides a standard hold-out
data split and cross validation.

\hypertarget{hold-out-data-split}{%
\subsection{Hold-out Data Split}\label{hold-out-data-split}}

If a hold-out data split is used, the data will be partitioned into a
training, a validation, and a test data set. The split depends on the
setting of the \texttt{eval} parameter. If \texttt{eval} is set to
\texttt{train\_hold\_out}, one data set, usually the original training
data set, is split into a new training and a validation data set. The
training data set is used for training the model. The validation data
set is used for the evaluation of the hyperparameter configuration and
early stopping to prevent overfitting. In this case, the original test
data set is not used.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{spotPython} returns the hyperparameters of the machine learning
and deep learning models, e.g., number of layers, learning rate, or
optimizer, but not the model weights. Therefore, after the SPOT run is
finished, the corresponding model with the optimized architecture has to
be trained again with the best hyperparameter configuration. The
training is performed on the training data set. The test data set is
used for the final evaluation of the model.

Summarizing, the following splits are performed in the hold-out setting:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run \texttt{spotPython} with \texttt{eval} set to
  \texttt{train\_hold\_out} to determine the best hyperparameter
  configuration.
\item
  Train the model with the best hyperparameter configuration
  (``architecture'') on the training data set:
  \texttt{train\_tuned(model\_spot,\ train,\ "model\_spot.pt")}.
\item
  Test the model on the test data:
  \texttt{test\_tuned(model\_spot,\ test,\ "model\_spot.pt")}
\end{enumerate}

These steps will be exemplified in the following sections.

\end{tcolorbox}

In addition to this \texttt{hold-out} setting, \texttt{spotPython}
provides another hold-out setting, where an explicit test data is
specified by the user that will be used as the validation set. To choose
this option, the \texttt{eval} parameter is set to
\texttt{test\_hold\_out}. In this case, the training data set is used
for the model training. Then, the explicitly defined test data set is
used for the evaluation of the hyperparameter configuration (the
validation).

\hypertarget{cross-validation-4}{%
\subsection{Cross-Validation}\label{cross-validation-4}}

The cross validation setting is used by setting the \texttt{eval}
parameter to \texttt{train\_cv} or \texttt{test\_cv}. In both cases, the
data set is split into \(k\) folds. The model is trained on \(k-1\)
folds and evaluated on the remaining fold. This is repeated \(k\) times,
so that each fold is used exactly once for evaluation. The final
evaluation is performed on the test data set. The cross validation
setting is useful for small data sets, because it allows to use all data
for training and evaluation. However, it is computationally expensive,
because the model has to be trained \(k\) times.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Combinations of the above settings are possible, e.g., cross validation
can be used for training and hold-out for evaluation or \emph{vice
versa}. Also, cross validation can be used for training and testing.
Because cross validation is not used in the \texttt{PyTorch} tutorial
(PyTorch 2023a), it is not considered further here.

\end{tcolorbox}

\hypertarget{overview-of-the-evaluation-settings}{%
\subsection{Overview of the Evaluation
Settings}\label{overview-of-the-evaluation-settings}}

\hypertarget{settings-for-the-hyperparameter-tuning}{%
\subsubsection{Settings for the Hyperparameter
Tuning}\label{settings-for-the-hyperparameter-tuning}}

An overview of the training evaluations is shown in
Table~\ref{tbl-eval-settings}. \texttt{"train\_cv"} and
\texttt{"test\_cv"} use \texttt{sklearn.model\_selection.KFold()}
internally. More details on the data splitting are provided in
Section~\ref{sec-detailed-data-splitting} (in the Appendix).

\hypertarget{tbl-eval-settings}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2857}}@{}}
\caption{\label{tbl-eval-settings}Overview of the evaluation
settings.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\texttt{eval}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\texttt{train}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\texttt{test}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
comment
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\texttt{eval}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\texttt{train}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\texttt{test}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
comment
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{"train\_hold\_out"} & \(\checkmark\) & &
\texttt{train\_one\_epoch()}, \texttt{validate\_one\_epoch()} for early
stopping & splits the \texttt{train} data set internally \\
\texttt{"test\_hold\_out"} & \(\checkmark\) & \(\checkmark\) &
\texttt{train\_one\_epoch()}, \texttt{validate\_one\_epoch()} for early
stopping & use the \texttt{test\ data\ set} for
\texttt{validate\_one\_epoch()} \\
\texttt{"train\_cv"} & \(\checkmark\) & &
\texttt{evaluate\_cv(net,\ train)} & CV using the \texttt{train} data
set \\
\texttt{"test\_cv"} & & \(\checkmark\) &
\texttt{evaluate\_cv(net,\ test)} & CV using the \texttt{test} data set
. Identical to \texttt{"train\_cv"}, uses only test data. \\
\end{longtable}

\hypertarget{settings-for-the-final-evaluation-of-the-tuned-architecture}{%
\subsubsection{Settings for the Final Evaluation of the Tuned
Architecture}\label{settings-for-the-final-evaluation-of-the-tuned-architecture}}

\hypertarget{training-of-the-tuned-architecture}{%
\paragraph{Training of the Tuned
Architecture}\label{training-of-the-tuned-architecture}}

\texttt{train\_tuned(model,\ train)}: train the model with the best
hyperparameter configuration (or simply the default) on the training
data set. It splits the \texttt{train}data into new \texttt{train} and
\texttt{validation} sets using
\texttt{create\_train\_val\_data\_loaders()}, which calls
\texttt{torch.utils.data.random\_split()} internally. Currently, 60\% of
the data is used for training and 40\% for validation. The
\texttt{train} data is used for training the model with
\texttt{train\_hold\_out()}. The \texttt{validation} data is used for
early stopping using \texttt{validate\_fold\_or\_hold\_out()} on the
\texttt{validation} data set.

\hypertarget{testing-of-the-tuned-architecture}{%
\paragraph{Testing of the Tuned
Architecture}\label{testing-of-the-tuned-architecture}}

\texttt{test\_tuned(model,\ test)}: test the model on the test data set.
No data splitting is performed. The (trained) model is evaluated using
the \texttt{validate\_fold\_or\_hold\_out()} function. Note: During
training, \texttt{"shuffle"} is set to \texttt{True}, whereas during
testing, \texttt{"shuffle"} is set to \texttt{False}.

Section~\ref{sec-final-model-evaluation} describes the final evaluation
of the tuned architecture.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
    \StringTok{"eval"}\NormalTok{: }\StringTok{"train\_hold\_out"}\NormalTok{,}
    \StringTok{"path"}\NormalTok{: }\StringTok{"torch\_model.pt"}\NormalTok{,}
    \StringTok{"shuffle"}\NormalTok{: }\VariableTok{True}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-loss-functions-14}{%
\subsection{Evaluation: Loss Functions and
Metrics}\label{sec-loss-functions-14}}

The key \texttt{"loss\_function"} specifies the loss function which is
used during the optimization. There are several different loss functions
under \texttt{PyTorch}'s \texttt{nn} package. For example, a simple loss
is \texttt{MSELoss}, which computes the mean-squared error between the
output and the target. In this tutorial we will use
\texttt{CrossEntropyLoss}, because it is also used in the
\texttt{PyTorch} tutorial.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torch.nn }\ImportTok{import}\NormalTok{ CrossEntropyLoss}
\NormalTok{loss\_function }\OperatorTok{=}\NormalTok{ CrossEntropyLoss()}
\NormalTok{fun\_control.update(\{}\StringTok{"loss\_function"}\NormalTok{: loss\_function\})}
\end{Highlighting}
\end{Shaded}

In addition to the loss functions, \texttt{spotPython} provides access
to a large number of metrics.

\begin{itemize}
\tightlist
\item
  The key \texttt{"metric\_sklearn"} is used for metrics that follow the
  \texttt{scikit-learn} conventions.
\item
  The key \texttt{"river\_metric"} is used for the river based
  evaluation (Montiel et al. 2021) via
  \texttt{eval\_oml\_iter\_progressive}, and
\item
  the key \texttt{"metric\_torch"} is used for the metrics from
  \texttt{TorchMetrics}.
\end{itemize}

\texttt{TorchMetrics} is a collection of more than 90 PyTorch metrics,
see \url{https://torchmetrics.readthedocs.io/en/latest/}. Because the
\texttt{PyTorch} tutorial uses the accuracy as metric, we use the same
metric here. Currently, accuracy is computed in the tutorial's example
code. We will use \texttt{TorchMetrics} instead, because it offers more
flexibilty, e.g., it can be used for regression and classification.
Furthermore, \texttt{TorchMetrics} offers the following advantages:

\begin{verbatim}
* A standardized interface to increase reproducibility
* Reduces Boilerplate
* Distributed-training compatible
* Rigorously tested
* Automatic accumulation over batches
* Automatic synchronization between multiple devices
\end{verbatim}

Therefore, we set

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torchmetrics}
\NormalTok{metric\_torch }\OperatorTok{=}\NormalTok{ torchmetrics.Accuracy(task}\OperatorTok{=}\StringTok{"multiclass"}\NormalTok{, num\_classes}\OperatorTok{=}\DecValTok{10}\NormalTok{).to(fun\_control[}\StringTok{"device"}\NormalTok{])}
\NormalTok{fun\_control.update(\{}\StringTok{"metric\_torch"}\NormalTok{: metric\_torch\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{step-8-calling-the-spot-function-5}{%
\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-5}}

\hypertarget{sec-prepare-spot-call-14}{%
\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-14}}

The following code passes the information about the parameter ranges and
bounds to \texttt{spot}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (}
\NormalTok{    get\_var\_type,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_bound\_values}
\NormalTok{    )}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}

\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, the dictionary \texttt{fun\_control} contains all information
needed for the hyperparameter tuning. Before the hyperparameter tuning
is started, it is recommended to take a look at the experimental design.
The method \texttt{gen\_design\_table} generates a design table as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name         | type   | default   |   lower |   upper | transform             |
|--------------|--------|-----------|---------|---------|-----------------------|
| l1           | int    | 5         |     2   |     9   | transform_power_2_int |
| l2           | int    | 5         |     2   |     9   | transform_power_2_int |
| lr_mult      | float  | 1.0       |     1   |     1   | None                  |
| batch_size   | int    | 4         |     1   |     5   | transform_power_2_int |
| epochs       | int    | 3         |     3   |     4   | transform_power_2_int |
| k_folds      | int    | 1         |     0   |     0   | None                  |
| patience     | int    | 5         |     3   |     3   | None                  |
| optimizer    | factor | SGD       |     0   |     9   | None                  |
| sgd_momentum | float  | 0.0       |     0.9 |     0.9 | None                  |
\end{verbatim}

This allows to check if all information is available and if the
information is correct. \textbf{?@tbl-design} shows the experimental
design for the hyperparameter tuning. The table shows the
hyperparameters, their types, default values, lower and upper bounds,
and the transformation function. The transformation function is used to
transform the hyperparameter values from the unit hypercube to the
original domain. The transformation function is applied to the
hyperparameter values before the evaluation of the objective function.
Hyperparameter transformations are shown in the column ``transform'',
e.g., the \texttt{l1} default is \texttt{5}, which results in the value
\(2^5 = 32\) for the network, because the transformation
\texttt{transform\_power\_2\_int} was selected in the \texttt{JSON}
file. The default value of the \texttt{batch\_size} is set to
\texttt{4}, which results in a batch size of \(2^4 = 16\).

\hypertarget{sec-the-objective-function-14}{%
\subsection{\texorpdfstring{The Objective Function
\texttt{fun\_torch}}{The Objective Function fun\_torch}}\label{sec-the-objective-function-14}}

The objective function \texttt{fun\_torch} is selected next. It
implements an interface from \texttt{PyTorch}'s training, validation,
and testing methods to \texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hypertorch }\ImportTok{import}\NormalTok{ HyperTorch}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperTorch().fun\_torch}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-default-hyperparameters}{%
\subsection{Using Default Hyperparameters or Results from Previous
Runs}\label{sec-default-hyperparameters}}

We add the default setting to the initial design:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-call-the-hyperparameter-tuner-14}{%
\subsection{Starting the Hyperparameter
Tuning}\label{sec-call-the-hyperparameter-tuner-14}}

The \texttt{spotPython} hyperparameter tuning is started by calling the
\texttt{Spot} function. Here, we will run the tuner for approximately 30
minutes (\texttt{max\_time}). Note: the initial design is always
evaluated in the \texttt{spotPython} run. As a consequence, the run may
take longer than specified by \texttt{max\_time}, because the evaluation
time of initial design (here: \texttt{init\_size}, 10 points) is
performed independently of \texttt{max\_time}. During the run, results
from the training is shown. These results can be visualized with
Tensorboard as will be shown in Section~\ref{sec-tensorboard-14}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}
\NormalTok{                                      \})}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

config: {'l1': 128, 'l2': 8, 'lr_mult': 1.0, 'batch_size': 32, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.3838500082492828 | Loss: 1.6562047554016113 | Acc: 0.3838500000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4587000012397766 | Loss: 1.4801345227241516 | Acc: 0.4587000000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5033000111579895 | Loss: 1.3707357211112976 | Acc: 0.5033000000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5238500237464905 | Loss: 1.3132523689270019 | Acc: 0.5238500000000000.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5549499988555908 | Loss: 1.2375486762046815 | Acc: 0.5549500000000001.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5734999775886536 | Loss: 1.2021079837799071 | Acc: 0.5735000000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5772500038146973 | Loss: 1.1980135177612306 | Acc: 0.5772500000000000.
Epoch: 8 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5938000082969666 | Loss: 1.1599345854759215 | Acc: 0.5938000000000000.
Epoch: 9 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6009500026702881 | Loss: 1.1328843992233277 | Acc: 0.6009500000000000.
Epoch: 10 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6024000048637390 | Loss: 1.1520775830268859 | Acc: 0.6024000000000000.
Epoch: 11 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6004999876022339 | Loss: 1.1797576458930970 | Acc: 0.6005000000000000.
Epoch: 12 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6086500287055969 | Loss: 1.1477203011512755 | Acc: 0.6086500000000000.
Early stopping at epoch 11
Returned to Spot: Validation loss: 1.1477203011512755

config: {'l1': 16, 'l2': 16, 'lr_mult': 1.0, 'batch_size': 8, 'epochs': 8, 'k_folds': 0, 'patience': 3, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4344500005245209 | Loss: 1.5370126734256744 | Acc: 0.4344500000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4992499947547913 | Loss: 1.3821711903572083 | Acc: 0.4992500000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5210499763488770 | Loss: 1.3544268280982972 | Acc: 0.5210500000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5225499868392944 | Loss: 1.3431447335362434 | Acc: 0.5225500000000000.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5267999768257141 | Loss: 1.3126116187691688 | Acc: 0.5268000000000000.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5393000245094299 | Loss: 1.3071551247596740 | Acc: 0.5393000000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5383499860763550 | Loss: 1.3307678449213505 | Acc: 0.5383500000000000.
Epoch: 8 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5449500083923340 | Loss: 1.2984912836074829 | Acc: 0.5449500000000000.
Returned to Spot: Validation loss: 1.298491283607483

config: {'l1': 256, 'l2': 128, 'lr_mult': 1.0, 'batch_size': 2, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'RMSprop', 'sgd_momentum': 0.9}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.1054999977350235 | Loss: 2.3146803474247455 | Acc: 0.1055000000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.1067499965429306 | Loss: 2.3664452521443367 | Acc: 0.1067500000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.0958999991416931 | Loss: 2.3053415326595306 | Acc: 0.0959000000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.1058500036597252 | Loss: 2.3030879536867142 | Acc: 0.1058500000000000.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.1409499943256378 | Loss: 2.5993600514886870 | Acc: 0.1409500000000000.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.0986500009894371 | Loss: 2.3089555664300918 | Acc: 0.0986500000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.0958499982953072 | Loss: 2.3065662771463393 | Acc: 0.0958500000000000.
Early stopping at epoch 6
Returned to Spot: Validation loss: 2.3065662771463393

config: {'l1': 8, 'l2': 32, 'lr_mult': 1.0, 'batch_size': 4, 'epochs': 8, 'k_folds': 0, 'patience': 3, 'optimizer': 'Adamax', 'sgd_momentum': 0.9}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.3878000080585480 | Loss: 1.6677710754990578 | Acc: 0.3878000000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4491499960422516 | Loss: 1.5083286433458327 | Acc: 0.4491500000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4769999980926514 | Loss: 1.4328012544035911 | Acc: 0.4770000000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5037000179290771 | Loss: 1.3618832346677781 | Acc: 0.5037000000000000.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5237500071525574 | Loss: 1.3237895696029067 | Acc: 0.5237500000000000.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5248000025749207 | Loss: 1.3229136003062130 | Acc: 0.5248000000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5267000198364258 | Loss: 1.3368793054573238 | Acc: 0.5266999999999999.
Epoch: 8 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5468500256538391 | Loss: 1.3025472436670213 | Acc: 0.5468499999999999.
Returned to Spot: Validation loss: 1.3025472436670213

config: {'l1': 64, 'l2': 512, 'lr_mult': 1.0, 'batch_size': 16, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'Adagrad', 'sgd_momentum': 0.9}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4503999948501587 | Loss: 1.5053875568389892 | Acc: 0.4504000000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4817500114440918 | Loss: 1.4276890246868132 | Acc: 0.4817500000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4971500039100647 | Loss: 1.3758006053924561 | Acc: 0.4971500000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5052999854087830 | Loss: 1.3617287966728211 | Acc: 0.5053000000000000.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5246499776840210 | Loss: 1.3144499421119691 | Acc: 0.5246499999999999.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5333999991416931 | Loss: 1.2918155289649964 | Acc: 0.5334000000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5371000170707703 | Loss: 1.2804483198165895 | Acc: 0.5371000000000000.
Epoch: 8 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5475000143051147 | Loss: 1.2682600169181824 | Acc: 0.5475000000000000.
Epoch: 9 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5515000224113464 | Loss: 1.2482085966110230 | Acc: 0.5515000000000000.
Epoch: 10 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5543000102043152 | Loss: 1.2493398176908492 | Acc: 0.5543000000000000.
Epoch: 11 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5570499897003174 | Loss: 1.2347032400369644 | Acc: 0.5570500000000000.
Epoch: 12 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5618000030517578 | Loss: 1.2204034348726274 | Acc: 0.5618000000000000.
Epoch: 13 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5636000037193298 | Loss: 1.2188638792991637 | Acc: 0.5636000000000000.
Epoch: 14 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5701000094413757 | Loss: 1.2036190267801286 | Acc: 0.5701000000000001.
Epoch: 15 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5735499858856201 | Loss: 1.2017679643630981 | Acc: 0.5735500000000000.
Epoch: 16 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5740000009536743 | Loss: 1.1927833241224288 | Acc: 0.5740000000000000.
Returned to Spot: Validation loss: 1.1927833241224288

config: {'l1': 128, 'l2': 16, 'lr_mult': 1.0, 'batch_size': 32, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4125500023365021 | Loss: 1.5910222095489501 | Acc: 0.4125500000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4715499877929688 | Loss: 1.4488346719741820 | Acc: 0.4715500000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5084999799728394 | Loss: 1.3695043066978454 | Acc: 0.5085000000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5306500196456909 | Loss: 1.3158891970634461 | Acc: 0.5306500000000000.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5375000238418579 | Loss: 1.3249270143508911 | Acc: 0.5375000000000000.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5588499903678894 | Loss: 1.2504237797737121 | Acc: 0.5588500000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5788999795913696 | Loss: 1.2066600365638733 | Acc: 0.5789000000000000.
Epoch: 8 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5720999836921692 | Loss: 1.2174073032379151 | Acc: 0.5721000000000001.
Epoch: 9 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5911499857902527 | Loss: 1.1804030963897705 | Acc: 0.5911500000000000.
Epoch: 10 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5900999903678894 | Loss: 1.1776824831962585 | Acc: 0.5901000000000000.
Epoch: 11 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5999000072479248 | Loss: 1.1676074934005738 | Acc: 0.5999000000000000.
Epoch: 12 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5935000181198120 | Loss: 1.1752061429977416 | Acc: 0.5935000000000000.
Epoch: 13 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6051499843597412 | Loss: 1.1578538383483887 | Acc: 0.6051500000000000.
Epoch: 14 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6034500002861023 | Loss: 1.1724235271453858 | Acc: 0.6034500000000000.
Epoch: 15 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6014999747276306 | Loss: 1.1903479053497314 | Acc: 0.6015000000000000.
Epoch: 16 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.6006500124931335 | Loss: 1.1868701674461364 | Acc: 0.6006500000000000.
Early stopping at epoch 15
Returned to Spot: Validation loss: 1.1868701674461364
spotPython tuning: 1.1477203011512755 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x308657150>
\end{verbatim}

\hypertarget{sec-tensorboard-14}{%
\section{Step 9: Tensorboard}\label{sec-tensorboard-14}}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard.

\hypertarget{tensorboard-start-tensorboard}{%
\subsection{Tensorboard: Start
Tensorboard}\label{tensorboard-start-tensorboard}}

Start TensorBoard through the command line to visualize data you logged.
Specify the root log directory as used in
\texttt{fun\_control\ =\ fun\_control\_init(task="regression",\ tensorboard\_path="runs/24\_spot\_torch\_regression")}
as the \texttt{tensorboard\_path}. The argument logdir points to
directory where TensorBoard will look to find event files that it can
display. TensorBoard will recursively walk the directory structure
rooted at logdir, looking for .\emph{tfevents.} files.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir=runs}
\end{Highlighting}
\end{Shaded}

Go to the URL it provides or to \url{http://localhost:6006/}. The
following figures show some screenshots of Tensorboard.

\begin{figure}

{\centering \includegraphics{./figures_static/14-torch_p040025_10min_5init_2023-06-07_12-41-06_tensorboard_01.png}

}

\caption{\label{fig-tensorboard_0}Tensorboard}

\end{figure}

\begin{figure}

{\centering \includegraphics{./figures_static/14-torch_p040025_10min_5init_2023-06-07_12-41-06_tensorboard_02.png}

}

\caption{\label{fig-tensorboard_hdparams}Tensorboard}

\end{figure}

\hypertarget{sec-saving-the-state-of-the-notebook}{%
\subsection{Saving the State of the
Notebook}\label{sec-saving-the-state-of-the-notebook}}

The state of the notebook can be saved and reloaded as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pickle}
\NormalTok{SAVE }\OperatorTok{=} \VariableTok{False}
\NormalTok{LOAD }\OperatorTok{=} \VariableTok{False}

\ControlFlowTok{if}\NormalTok{ SAVE:}
\NormalTok{    result\_file\_name }\OperatorTok{=} \StringTok{"res\_"} \OperatorTok{+}\NormalTok{ experiment\_name }\OperatorTok{+} \StringTok{".pkl"}
    \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(result\_file\_name, }\StringTok{\textquotesingle{}wb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        pickle.dump(spot\_tuner, f)}

\ControlFlowTok{if}\NormalTok{ LOAD:}
\NormalTok{    result\_file\_name }\OperatorTok{=} \StringTok{"add\_the\_name\_of\_the\_result\_file\_here.pkl"}
    \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(result\_file\_name, }\StringTok{\textquotesingle{}rb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        spot\_tuner }\OperatorTok{=}\NormalTok{  pickle.load(f)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-results-14}{%
\section{Step 10: Results}\label{sec-results-14}}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The following code generates
the progress plot from \textbf{?@fig-progress}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, }
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{028_spot_ray_hpt_torch_cifar10_files/figure-pdf/cell-25-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\textbf{?@fig-progress} shows a typical behaviour that can be observed
in many hyperparameter studies (Bartz et al. 2022): the largest
improvement is obtained during the evaluation of the initial design. The
surrogate model based optimization-optimization with the surrogate
refines the results. \textbf{?@fig-progress} also illustrates one major
difference between \texttt{ray{[}tune{]}} as used in PyTorch (2023a) and
\texttt{spotPython}: the \texttt{ray{[}tune{]}} uses a random search and
will generate results similar to the \emph{black} dots, whereas
\texttt{spotPython} uses a surrogate model based optimization and
presents results represented by \emph{red} dots in
\textbf{?@fig-progress}. The surrogate model based optimization is
considered to be more efficient than a random search, because the
surrogate model guides the search towards promising regions in the
hyperparameter space.

In addition to the improved (``optimized'') hyperparameter values,
\texttt{spotPython} allows a statistical analysis, e.g., a sensitivity
analysis, of the results. We can print the results of the hyperparameter
tuning, see \textbf{?@tbl-results}. The table shows the hyperparameters,
their types, default values, lower and upper bounds, and the
transformation function. The column ``tuned'' shows the tuned values.
The column ``importance'' shows the importance of the hyperparameters.
The column ``stars'' shows the importance of the hyperparameters in
stars. The importance is computed by the SPOT software.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name         | type   | default   |   lower |   upper |   tuned | transform             |   importance | stars   |
|--------------|--------|-----------|---------|---------|---------|-----------------------|--------------|---------|
| l1           | int    | 5         |     2.0 |     9.0 |     7.0 | transform_power_2_int |         0.00 |         |
| l2           | int    | 5         |     2.0 |     9.0 |     3.0 | transform_power_2_int |         0.00 |         |
| lr_mult      | float  | 1.0       |     1.0 |     1.0 |     1.0 | None                  |         0.00 |         |
| batch_size   | int    | 4         |     1.0 |     5.0 |     5.0 | transform_power_2_int |         0.81 | .       |
| epochs       | int    | 3         |     3.0 |     4.0 |     4.0 | transform_power_2_int |         6.06 | *       |
| k_folds      | int    | 1         |     0.0 |     0.0 |     0.0 | None                  |         0.00 |         |
| patience     | int    | 5         |     3.0 |     3.0 |     3.0 | None                  |         0.00 |         |
| optimizer    | factor | SGD       |     0.0 |     9.0 |     3.0 | None                  |       100.00 | ***     |
| sgd_momentum | float  | 0.0       |     0.9 |     0.9 |     0.9 | None                  |         0.00 |         |
\end{verbatim}

To visualize the most important hyperparameters, \texttt{spotPython}
provides the function \texttt{plot\_importance}. The following code
generates the importance plot from \textbf{?@fig-importance}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{028_spot_ray_hpt_torch_cifar10_files/figure-pdf/cell-27-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}

\hypertarget{sec-get-spot-results-14}{%
\subsection{Get the Tuned Architecture (SPOT
Results)}\label{sec-get-spot-results-14}}

The architecture of the \texttt{spotPython} model can be obtained as
follows. First, the numerical representation of the hyperparameters are
obtained, i.e., the numpy array \texttt{X} is generated. This array is
then used to generate the model \texttt{model\_spot} by the function
\texttt{get\_one\_core\_model\_from\_X}. The model \texttt{model\_spot}
has the following architecture:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\NormalTok{model\_spot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Net_CIFAR10(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=8, bias=True)
  (fc3): Linear(in_features=8, out_features=10, bias=True)
)
\end{verbatim}

\hypertarget{get-default-hyperparameters-8}{%
\subsection{Get Default
Hyperparameters}\label{get-default-hyperparameters-8}}

In a similar manner as in Section~\ref{sec-get-spot-results-14}, the
default hyperparameters can be obtained.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fun\_control was modified, we generate a new one with the original }
\CommentTok{\# default hyperparameters}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control)}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Net_CIFAR10(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=32, bias=True)
  (fc3): Linear(in_features=32, out_features=10, bias=True)
)
\end{verbatim}

\hypertarget{evaluation-of-the-default-architecture}{%
\subsection{Evaluation of the Default
Architecture}\label{evaluation-of-the-default-architecture}}

The method \texttt{train\_tuned} takes a model architecture without
trained weights and trains this model with the train data. The train
data is split into train and validation data. The validation data is
used for early stopping. The trained model weights are saved as a
dictionary.

This evaluation is similar to the final evaluation in PyTorch (2023a).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.torch.traintest }\ImportTok{import}\NormalTok{ (}
\NormalTok{    train\_tuned,}
\NormalTok{    test\_tuned,}
\NormalTok{    )}
\NormalTok{train\_tuned(net}\OperatorTok{=}\NormalTok{model\_default, train\_dataset}\OperatorTok{=}\NormalTok{train, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        loss\_function}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"loss\_function"}\NormalTok{],}
\NormalTok{        metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_torch"}\NormalTok{],}
\NormalTok{        device }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"device"}\NormalTok{], show\_batch\_interval}\OperatorTok{=}\DecValTok{1\_000\_000}\NormalTok{,}
\NormalTok{        path}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{        task}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"task"}\NormalTok{],)}

\NormalTok{test\_tuned(net}\OperatorTok{=}\NormalTok{model\_default, test\_dataset}\OperatorTok{=}\NormalTok{test, }
\NormalTok{        loss\_function}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"loss\_function"}\NormalTok{],}
\NormalTok{        metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_torch"}\NormalTok{],}
\NormalTok{        shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{, }
\NormalTok{        device }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"device"}\NormalTok{],}
\NormalTok{        task}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"task"}\NormalTok{],)        }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.0995000004768372 | Loss: 2.2999522211074828 | Acc: 0.0995000000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.1305000036954880 | Loss: 2.2869610927581787 | Acc: 0.1305000000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.1363500058650970 | Loss: 2.2477050289154055 | Acc: 0.1363500000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.2006500065326691 | Loss: 2.1781324967384337 | Acc: 0.2006500000000000.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.2182500064373016 | Loss: 2.1153550539016726 | Acc: 0.2182500000000000.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.2264499962329865 | Loss: 2.0708753735542298 | Acc: 0.2264500000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.2435999959707260 | Loss: 2.0414400042533876 | Acc: 0.2436000000000000.
Epoch: 8 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.2531499862670898 | Loss: 2.0214681243896484 | Acc: 0.2531500000000000.
Returned to Spot: Validation loss: 2.0214681243896484
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.2574999928474426 | Loss: 2.0162517908096311 | Acc: 0.2575000000000000.
Final evaluation: Validation loss: 2.016251790809631
Final evaluation: Validation metric: 0.2574999928474426
----------------------------------------------
\end{verbatim}

\begin{verbatim}
(2.016251790809631, nan, tensor(0.2575, device='mps:0'))
\end{verbatim}

\hypertarget{evaluation-of-the-tuned-architecture}{%
\subsection{Evaluation of the Tuned
Architecture}\label{evaluation-of-the-tuned-architecture}}

The following code trains the model \texttt{model\_spot}.

If \texttt{path} is set to a filename, e.g.,
\texttt{path\ =\ "model\_spot\_trained.pt"}, the weights of the trained
model will be saved to this file.

If \texttt{path} is set to a filename, e.g.,
\texttt{path\ =\ "model\_spot\_trained.pt"}, the weights of the trained
model will be loaded from this file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train\_tuned(net}\OperatorTok{=}\NormalTok{model\_spot, train\_dataset}\OperatorTok{=}\NormalTok{train,}
\NormalTok{        loss\_function}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"loss\_function"}\NormalTok{],}
\NormalTok{        metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_torch"}\NormalTok{],}
\NormalTok{        shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        device }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"device"}\NormalTok{],}
\NormalTok{        path}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{        task}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"task"}\NormalTok{],)}
\NormalTok{test\_tuned(net}\OperatorTok{=}\NormalTok{model\_spot, test\_dataset}\OperatorTok{=}\NormalTok{test,}
\NormalTok{            shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{            loss\_function}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"loss\_function"}\NormalTok{],}
\NormalTok{            metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_torch"}\NormalTok{],}
\NormalTok{            device }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"device"}\NormalTok{],}
\NormalTok{            task}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"task"}\NormalTok{],)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Epoch: 1 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4187499880790710 | Loss: 1.5769843736648559 | Acc: 0.4187500000000000.
Epoch: 2 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.4675000011920929 | Loss: 1.4508240690231324 | Acc: 0.4675000000000000.
Epoch: 3 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5137000083923340 | Loss: 1.3569832351684570 | Acc: 0.5137000000000000.
Epoch: 4 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5307499766349792 | Loss: 1.3070075393676759 | Acc: 0.5307500000000001.
Epoch: 5 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5393499732017517 | Loss: 1.2841068645477296 | Acc: 0.5393500000000000.
Epoch: 6 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5460500121116638 | Loss: 1.2693293420791627 | Acc: 0.5460500000000000.
Epoch: 7 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5558999776840210 | Loss: 1.2405844126701355 | Acc: 0.5558999999999999.
Epoch: 8 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5686500072479248 | Loss: 1.2219975400924683 | Acc: 0.5686500000000000.
Epoch: 9 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5616499781608582 | Loss: 1.2282655703544616 | Acc: 0.5616500000000000.
Epoch: 10 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5766999721527100 | Loss: 1.2177041532516479 | Acc: 0.5767000000000000.
Epoch: 11 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5799999833106995 | Loss: 1.2102932430267335 | Acc: 0.5800000000000000.
Epoch: 12 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5819000005722046 | Loss: 1.2291380684852600 | Acc: 0.5819000000000000.
Epoch: 13 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5869500041007996 | Loss: 1.2013124509811401 | Acc: 0.5869500000000000.
Epoch: 14 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5739499926567078 | Loss: 1.2463197503089904 | Acc: 0.5739500000000000.
Epoch: 15 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5835999846458435 | Loss: 1.2343276533126830 | Acc: 0.5836000000000000.
Epoch: 16 | 
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5806499719619751 | Loss: 1.2479942989349366 | Acc: 0.5806500000000000.
Early stopping at epoch 15
Returned to Spot: Validation loss: 1.2479942989349366
\end{verbatim}

\begin{verbatim}
MulticlassAccuracy: 0.5892999768257141 | Loss: 1.2312500318780113 | Acc: 0.5893000000000000.
Final evaluation: Validation loss: 1.2312500318780113
Final evaluation: Validation metric: 0.5892999768257141
----------------------------------------------
\end{verbatim}

\begin{verbatim}
(1.2312500318780113, nan, tensor(0.5893, device='mps:0'))
\end{verbatim}

\hypertarget{detailed-hyperparameter-plots-8}{%
\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-8}}

The contour plots in this section visualize the interactions of the
three most important hyperparameters. Since some of these
hyperparameters take fatorial or integer values, sometimes step-like
fitness landcapes (or response surfaces) are generated. SPOT draws the
interactions of the main hyperparameters by default. It is also possible
to visualize all interactions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
batch_size:  0.808580970629426
epochs:  6.061007441106652
optimizer:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{028_spot_ray_hpt_torch_cifar10_files/figure-pdf/cell-32-output-2.pdf}

}

\caption{Contour plots.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{028_spot_ray_hpt_torch_cifar10_files/figure-pdf/cell-32-output-3.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{028_spot_ray_hpt_torch_cifar10_files/figure-pdf/cell-32-output-4.pdf}

}

\end{figure}

The figures (\textbf{?@fig-contour}) show the contour plots of the loss
as a function of the hyperparameters. These plots are very helpful for
benchmark studies and for understanding neural networks.
\texttt{spotPython} provides additional tools for a visual inspection of
the results and give valuable insights into the hyperparameter tuning
process. This is especially useful for model explainability,
transparency, and trustworthiness. In addition to the contour plots,
\textbf{?@fig-parallel} shows the parallel plot of the hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

Parallel coordinates plots

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{sec-summary}{%
\section{Summary and Outlook}\label{sec-summary}}

This tutorial presents the hyperparameter tuning open source software
\texttt{spotPython} for \texttt{PyTorch}. To show its basic features, a
comparison with the ``official'' \texttt{PyTorch} hyperparameter tuning
tutorial (PyTorch 2023a) is presented. Some of the advantages of
\texttt{spotPython} are:

\begin{itemize}
\tightlist
\item
  Numerical and categorical hyperparameters.
\item
  Powerful surrogate models.
\item
  Flexible approach and easy to use.
\item
  Simple JSON files for the specification of the hyperparameters.
\item
  Extension of default and user specified network classes.
\item
  Noise handling techniques.
\item
  Interaction with \texttt{tensorboard}.
\end{itemize}

Currently, only rudimentary parallel and distributed neural network
training is possible, but these capabilities will be extended in the
future. The next version of \texttt{spotPython} will also include a more
detailed documentation and more examples.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Important: This tutorial does not present a complete benchmarking study
(Bartz-Beielstein et al. 2020). The results are only preliminary and
highly dependent on the local configuration (hard- and software). Our
goal is to provide a first impression of the performance of the
hyperparameter tuning package \texttt{spotPython}. To demonstrate its
capabilities, a quick comparison with \texttt{ray{[}tune{]}} was
performed. \texttt{ray{[}tune{]}} was chosen, because it is presented as
``an industry standard tool for distributed hyperparameter tuning.'' The
results should be interpreted with care.

\end{tcolorbox}

\hypertarget{sec-appendix}{%
\section{Appendix}\label{sec-appendix}}

\hypertarget{sample-output-from-ray-tunes-run}{%
\subsection{Sample Output From Ray Tune's
Run}\label{sample-output-from-ray-tunes-run}}

The output from \texttt{ray{[}tune{]}} could look like this (PyTorch
2023b):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Number of trials: 10 (10 TERMINATED)}
\NormalTok{{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\NormalTok{|   l1 |   l2 |          lr |   batch\_size |    loss |   accuracy | training\_iteration |}
\NormalTok{+{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\NormalTok{|   64 |    4 | 0.00011629  |            2 | 1.87273 |     0.244  |                  2 |}
\NormalTok{|   32 |   64 | 0.000339763 |            8 | 1.23603 |     0.567  |                  8 |}
\NormalTok{|    8 |   16 | 0.00276249  |           16 | 1.1815  |     0.5836 |                 10 |}
\NormalTok{|    4 |   64 | 0.000648721 |            4 | 1.31131 |     0.5224 |                  8 |}
\NormalTok{|   32 |   16 | 0.000340753 |            8 | 1.26454 |     0.5444 |                  8 |}
\NormalTok{|    8 |    4 | 0.000699775 |            8 | 1.99594 |     0.1983 |                  2 |}
\NormalTok{|  256 |    8 | 0.0839654   |           16 | 2.3119  |     0.0993 |                  1 |}
\NormalTok{|   16 |  128 | 0.0758154   |           16 | 2.33575 |     0.1327 |                  1 |}
\NormalTok{|   16 |    8 | 0.0763312   |           16 | 2.31129 |     0.1042 |                  4 |}
\NormalTok{|  128 |   16 | 0.000124903 |            4 | 2.26917 |     0.1945 |                  1 |}
\NormalTok{+{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\NormalTok{Best trial config: \{\textquotesingle{}l1\textquotesingle{}: 8, \textquotesingle{}l2\textquotesingle{}: 16, \textquotesingle{}lr\textquotesingle{}: 0.00276249, \textquotesingle{}batch\_size\textquotesingle{}: 16, \textquotesingle{}data\_dir\textquotesingle{}: \textquotesingle{}...\textquotesingle{}\}}
\NormalTok{Best trial final validation loss: 1.181501}
\NormalTok{Best trial final validation accuracy: 0.5836}
\NormalTok{Best trial test set accuracy: 0.5806}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-hyperparameter-tuning-lightning-31}{%
\chapter{HPT PyTorch Lightning:
VBDP}\label{sec-hyperparameter-tuning-lightning-31}}

In this tutorial, we will show how \texttt{spotPython} can be integrated
into the \texttt{PyTorch} Lightning training workflow for a
classification task.

This chapter describes the hyperparameter tuning of a
\texttt{PyTorch\ Lightning} network on the Vector Borne Disease
Prediction (VBDP) data set.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Vector Borne Disease Prediction Data Set}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, colframe=quarto-callout-important-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

This chapter uses the Vector Borne Disease Prediction data set from
Kaggle. It is a categorical dataset for eleven Vector Borne Diseases
with associated symptoms.

\begin{quote}
The person who associated a work with this deed has dedicated the work
to the public domain by waiving all of his or her rights to the work
worldwide under copyright law, including all related and neighboring
rights, to the extent allowed by law.You can copy, modify, distribute
and perform the work, even for commercial purposes, all without asking
permission. See Other Information below, see
\url{https://creativecommons.org/publicdomain/zero/1.0/}.
\end{quote}

The data set is available at:
\url{https://www.kaggle.com/datasets/richardbernat/vector-borne-disease-prediction},

The data should be downloaded and stored in the \texttt{data/VBDP}
subfolder. The data set is not available as a part of the
\texttt{spotPython} package.

\end{tcolorbox}

This document refers to the latest \texttt{spotPython} version, which
can be installed via pip. Alternatively, the source code can be
downloaded from gitHub:
\url{https://github.com/sequential-parameter-optimization/spotPython}.

\begin{itemize}
\tightlist
\item
  Uncomment the following lines if you want to for (re-)installation the
  latest version of \texttt{spotPython} from GitHub.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import sys}
\CommentTok{\# !\{sys.executable\} {-}m pip install {-}{-}upgrade build}
\CommentTok{\# !\{sys.executable\} {-}m pip install {-}{-}upgrade {-}{-}force{-}reinstall spotPython}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-setup-31}{%
\section{Step 1: Setup}\label{sec-setup-31}}

\begin{itemize}
\tightlist
\item
  Before we consider the detailed experimental setup, we select the
  parameters that affect run time, initial design size, etc.
\item
  The parameter \texttt{MAX\_TIME} specifies the maximum run time in
  seconds.
\item
  The parameter \texttt{INIT\_SIZE} specifies the initial design size.
\item
  The parameter \texttt{WORKERS} specifies the number of workers.
\item
  The prefix \texttt{PREFIX} is used for the experiment name and the
  name of the log file.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{WORKERS }\OperatorTok{=} \DecValTok{0}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"31"}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{WORKERS} is set to 0 for demonstration purposes. For real
  experiments, this should be increased. See the warnings that are
  printed when the number of workers is set to 0.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Device selection}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Although there are no .cuda() or .to(device) calls required, because
  Lightning does these for you, see
  \href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{LIGHTNINGMODULE},
  we would like to know which device is used. Threrefore, we imitate the
  LightningModule behaviour which selects the highest device.
\item
  The method \texttt{spotPython.utils.device.getDevice()} returns the
  device that is used by Lightning.
\end{itemize}

\end{tcolorbox}

\hypertarget{step-2-initialization-of-the-fun_control-dictionary}{%
\section{\texorpdfstring{Step 2: Initialization of the
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the fun\_control Dictionary}}\label{step-2-initialization-of-the-fun_control-dictionary}}

\texttt{spotPython} uses a Python dictionary for storing the information
required for the hyperparameter tuning process, which was described in
Section~\ref{sec-initialization-fun-control-14}, see
\href{https://sequential-parameter-optimization.github.io/spotPython/14_spot_ray_hpt_torch_cifar10.html\#sec-initialization-fun-control-14}{Initialization
of the fun\_control Dictionary} in the documentation.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_name, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}

\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(prefix}\OperatorTok{=}\NormalTok{PREFIX)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    spot\_tensorboard\_path}\OperatorTok{=}\NormalTok{get\_spot\_tensorboard\_path(experiment\_name),}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{WORKERS,}
\NormalTok{    device}\OperatorTok{=}\NormalTok{getDevice(),}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{64}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{11}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control[}\StringTok{"device"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'mps'
\end{verbatim}

\hypertarget{sec-data-loading-31}{%
\section{Step 3: PyTorch Data Loading}\label{sec-data-loading-31}}

\hypertarget{lightning-dataset-and-datamodule}{%
\subsection{Lightning Dataset and
DataModule}\label{lightning-dataset-and-datamodule}}

The data loading and preprocessing is handled by \texttt{Lightning} and
\texttt{PyTorch}. It comprehends the following classes:

\begin{itemize}
\tightlist
\item
  \texttt{CSVDataset}: A class that loads the data from a CSV file.
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/csvdataset.py}{{[}SOURCE{]}}
\item
  \texttt{CSVDataModule}: A class that prepares the data for training
  and testing.
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/csvdatamodule.py}{{[}SOURCE{]}}
\end{itemize}

Section Section~\ref{sec-taking-a-look-at-the-data-31} illustrates how
to access the data.

\hypertarget{sec-preprocessing-31}{%
\section{Step 4: Preprocessing}\label{sec-preprocessing-31}}

Preprocessing is handled by \texttt{Lightning} and \texttt{PyTorch}. It
can be implemented in the \texttt{CSVDataModule} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/csvdatamodule.py}{{[}SOURCE{]}}
and is described in the
\href{https://lightning.ai/docs/pytorch/stable/data/datamodule.html}{LIGHTNINGDATAMODULE}
documentation. Here you can find information about the
\texttt{transforms} methods.

\hypertarget{sec-selection-of-the-algorithm-31}{%
\section{\texorpdfstring{Step 5: Select the NN Model
(\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select the NN Model (algorithm) and core\_model\_hyper\_dict}}\label{sec-selection-of-the-algorithm-31}}

\texttt{spotPython} includes the \texttt{NetLightBase} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/netlightbase.py}{{[}SOURCE{]}}
for configurable neural networks. The class is imported here. It
inherits from the class \texttt{Lightning.LightningModule}, which is the
base class for all models in \texttt{Lightning}.
\texttt{Lightning.LightningModule} is a subclass of
\texttt{torch.nn.Module} and provides additional functionality for the
training and testing of neural networks. The class
\texttt{Lightning.LightningModule} is described in the
\href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{Lightning
documentation}.

\begin{itemize}
\tightlist
\item
  Here we simply add the NN Model to the fun\_control dictionary by
  calling the function \texttt{add\_core\_model\_to\_fun\_control}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.netlightbase }\ImportTok{import}\NormalTok{ NetLightBase }
\ImportTok{from}\NormalTok{ spotPython.data.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{NetLightBase,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{ LightHyperDict)}
\end{Highlighting}
\end{Shaded}

The \texttt{NetLightBase} is a configurable neural network. The
hyperparameters of the model are specified in the
\texttt{core\_model\_hyper\_dict} dictionary
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/data/light_hyper_dict.json}{{[}SOURCE{]}}.

\hypertarget{sec-modification-of-hyperparameters-31}{%
\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{sec-modification-of-hyperparameters-31}}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modification-of-hyperparameters-14}.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Small number of epochs for demonstration purposes}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  \texttt{epochs} and \texttt{patience} are set to small values for
  demonstration purposes. These values are too small for a real
  application.
\item
  More resonable values are, e.g.:

  \begin{itemize}
  \tightlist
  \item
    \texttt{modify\_hyper\_parameter\_bounds(fun\_control,\ "epochs",\ bounds={[}7,\ 9{]})}
    and
  \item
    \texttt{modify\_hyper\_parameter\_bounds(fun\_control,\ "patience",\ bounds={[}2,\ 7{]})}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}

\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"l1"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{5}\NormalTok{,}\DecValTok{8}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"epochs"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{6}\NormalTok{,}\DecValTok{13}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"optimizer"}\NormalTok{,[}\StringTok{"Adam"}\NormalTok{, }\StringTok{"AdamW"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{, }\StringTok{"NAdam"}\NormalTok{])}
\CommentTok{\# modify\_hyper\_parameter\_levels(fun\_control, "optimizer", ["Adam"])}
\end{Highlighting}
\end{Shaded}

Now, the dictionary \texttt{fun\_control} contains all information
needed for the hyperparameter tuning. Before the hyperparameter tuning
is started, it is recommended to take a look at the experimental design.
The method \texttt{gen\_design\_table}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/utils/eda.py}{{[}SOURCE{]}}
generates a design table as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |     5   |    8    | transform_power_2_int |
| epochs         | int    | 4         |     6   |   13    | transform_power_2_int |
| batch_size     | int    | 4         |     2   |    8    | transform_power_2_int |
| act_fn         | factor | ReLU      |     0   |    5    | None                  |
| optimizer      | factor | SGD       |     0   |    3    | None                  |
| dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |
| lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |
| patience       | int    | 2         |     2   |    6    | transform_power_2_int |
| initialization | factor | Default   |     0   |    2    | None                  |
\end{verbatim}

This allows to check if all information is available and if the
information is correct.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Hyperparameters of the Tuned Model and the \texttt{fun\_control}
Dictionary}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The updated \texttt{fun\_control} dictionary can be shown with the
command \texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}}.

\end{tcolorbox}

\hypertarget{step-7-data-splitting-the-objective-loss-function-and-the-metric}{%
\section{Step 7: Data Splitting, the Objective (Loss) Function and the
Metric}\label{step-7-data-splitting-the-objective-loss-function-and-the-metric}}

\hypertarget{sec-selection-of-target-function-31}{%
\subsection{Evaluation}\label{sec-selection-of-target-function-31}}

The evaluation procedure requires the specification of two elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the way how the data is split into a train and a test set (see
  Section~\ref{sec-data-splitting-14})
\item
  the loss function (and a metric).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Data Splitting in Lightning}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  The data splitting is handled by \texttt{Lightning}.
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-loss-functions-and-metrics-31}{%
\subsection{Loss Functions and
Metrics}\label{sec-loss-functions-and-metrics-31}}

The loss function is specified in the configurable network class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/netlightbase.py}{{[}SOURCE{]}}
We will use CrossEntropy loss for the multiclass-classification task.

\hypertarget{sec-metric-31}{%
\subsection{Metric}\label{sec-metric-31}}

\begin{itemize}
\tightlist
\item
  We will use the MAP@k metric
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/torch/mapk.py}{{[}SOURCE{]}}
  for the evaluation of the model.
\item
  An example, how this metric works, is shown in the Appendix, see
  Section \{Section~\ref{sec-the-mapk-metric-31}\}.
\end{itemize}

Similar to the loss function, the metric is specified in the
configurable network class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/netlightbase.py}{{[}SOURCE{]}}.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Loss Function and Metric in Lightning}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  The loss function and the metric are not hyperparameters that can be
  tuned with \texttt{spotPython}.
\item
  They are handled by \texttt{Lightning}.
\end{itemize}

\end{tcolorbox}

\hypertarget{step-8-calling-the-spot-function-6}{%
\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-6}}

\hypertarget{sec-prepare-spot-call-31}{%
\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-31}}

The following code passes the information about the parameter ranges and
bounds to \texttt{spot}. It extracts the variable types, names, and
bounds

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ (get\_bound\_values,}
\NormalTok{    get\_var\_name,}
\NormalTok{    get\_var\_type,)}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ get\_var\_type(fun\_control)}
\NormalTok{var\_name }\OperatorTok{=}\NormalTok{ get\_var\_name(fun\_control)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"lower"}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ get\_bound\_values(fun\_control, }\StringTok{"upper"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-the-objective-function-31}{%
\subsection{\texorpdfstring{The Objective Function
\texttt{fun}}{The Objective Function fun}}\label{sec-the-objective-function-31}}

The objective function \texttt{fun} from the class \texttt{HyperLight}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/hyperlight.py}{{[}SOURCE{]}}
is selected next. It implements an interface from \texttt{PyTorch}'s
training, validation, and testing methods to \texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-call-the-hyperparameter-tuner-31}{%
\subsection{Starting the Hyperparameter
Tuning}\label{sec-call-the-hyperparameter-tuner-31}}

The \texttt{spotPython} hyperparameter tuning is started by calling the
\texttt{Spot} function
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}
as described in Section~\ref{sec-call-the-hyperparameter-tuner-14}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                   fun\_evals }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ MAX\_TIME,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type }\OperatorTok{=}\NormalTok{ var\_type,}
\NormalTok{                   var\_name }\OperatorTok{=}\NormalTok{ var\_name,}
\NormalTok{                   show\_progress}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: INIT\_SIZE\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{True}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\BuiltInTok{len}\NormalTok{(var\_name),}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{10\_000}\NormalTok{,}
\NormalTok{                                      \})}
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.2744216918945312
         val_acc            0.24381625652313232
        val_loss            2.2744216918945312
       valid_mapk           0.3699631094932556

fun: train_model returned
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.2821764945983887
         val_acc             0.268551230430603
        val_loss            2.2821764945983887
       valid_mapk           0.3694058656692505

fun: train_model returned
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.40523099899292
         val_acc            0.13780918717384338
        val_loss             2.40523099899292
       valid_mapk           0.1892361044883728

fun: train_model returned
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.301100015640259
         val_acc            0.23674911260604858
        val_loss             2.301100015640259
       valid_mapk           0.3090277910232544

fun: train_model returned
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.2979023456573486
         val_acc            0.22968198359012604
        val_loss            2.2979023456573486
       valid_mapk           0.3073495328426361

fun: train_model returned
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.285515546798706
         val_acc            0.24028268456459045
        val_loss             2.285515546798706
       valid_mapk           0.3344908058643341

fun: train_model returned
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.2744216918945312 [######----] 60.46% 
\end{verbatim}

\begin{verbatim}
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.285055637359619
         val_acc            0.2473498284816742
        val_loss             2.285055637359619
       valid_mapk           0.3562082052230835

fun: train_model returned
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.2744216918945312 [######----] 64.89% 
\end{verbatim}

\begin{verbatim}
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.276766061782837
         val_acc            0.2473498284816742
        val_loss             2.276766061782837
       valid_mapk           0.3588348925113678

fun: train_model returned
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.2744216918945312 [#########-] 85.93% 
\end{verbatim}

\begin{verbatim}
fun: Calling train_model
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.272672414779663
         val_acc            0.2473498284816742
        val_loss             2.272672414779663
       valid_mapk           0.3421362042427063

fun: train_model returned
\end{verbatim}

\begin{verbatim}
spotPython tuning: 2.272672414779663 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x308702110>
\end{verbatim}

\hypertarget{sec-tensorboard-31}{%
\section{Step 9: Tensorboard}\label{sec-tensorboard-31}}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="runs/"}
\end{Highlighting}
\end{Shaded}

Further information can be found in the
\href{https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html}{PyTorch
Lightning documentation} for Tensorboard.

\hypertarget{sec-results-31}{%
\section{Step 10: Results}\label{sec-results-31}}

After the hyperparameter tuning run is finished, the results can be
analyzed as described in Section~\ref{sec-results-14}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{029_spot_lightning_csv_files/figure-pdf/cell-13-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper |              tuned | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     5.0 |     8.0 |                5.0 | transform_power_2_int |         0.12 | .       |
| epochs         | int    | 4         |     6.0 |    13.0 |                7.0 | transform_power_2_int |       100.00 | ***     |
| batch_size     | int    | 4         |     2.0 |     8.0 |                2.0 | transform_power_2_int |         0.00 |         |
| act_fn         | factor | ReLU      |     0.0 |     5.0 |                3.0 | None                  |         4.78 | *       |
| optimizer      | factor | SGD       |     0.0 |     3.0 |                2.0 | None                  |         0.01 |         |
| dropout_prob   | float  | 0.01      |     0.0 |    0.25 |               0.25 | None                  |         0.02 |         |
| lr_mult        | float  | 1.0       |     0.1 |    10.0 | 2.1136416734217627 | None                  |         0.00 |         |
| patience       | int    | 2         |     2.0 |     6.0 |                5.0 | transform_power_2_int |         0.00 |         |
| initialization | factor | Default   |     0.0 |     2.0 |                0.0 | None                  |         0.00 |         |
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{029_spot_lightning_csv_files/figure-pdf/cell-15-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}

\hypertarget{sec-get-spot-results-31}{%
\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-31}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.utils }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Test on the full data set
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.traintest }\ImportTok{import}\NormalTok{ test\_model}
\NormalTok{test\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

       Test metric             DataLoader 0

        hp_metric            2.19404935836792
     test_mapk_epoch        0.42514118552207947
         val_acc            0.3465346395969391
        val_loss             2.19404935836792

\end{verbatim}

\begin{verbatim}
(2.19404935836792, 0.3465346395969391)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.traintest }\ImportTok{import}\NormalTok{ load\_light\_from\_checkpoint}

\NormalTok{model\_loaded }\OperatorTok{=}\NormalTok{ load\_light\_from\_checkpoint(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading model from runs/saved_models/6235191941824894532_TEST/last.ckpt
\end{verbatim}

\hypertarget{cross-validation-with-lightning}{%
\subsection{Cross Validation With
Lightning}\label{cross-validation-with-lightning}}

\begin{itemize}
\tightlist
\item
  The \texttt{KFold} class from \texttt{sklearn.model\_selection} is
  used to generate the folds for cross-validation.
\item
  These mechanism is used to generate the folds for the final evaluation
  of the model.
\item
  The \texttt{CrossValidationDataModule} class
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/crossvalidationdatamodule.py}{{[}SOURCE{]}}
  is used to generate the folds for the hyperparameter tuning process.
\item
  It is called from the \texttt{cv\_model} function
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/traintest.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.traintest }\ImportTok{import}\NormalTok{ cv\_model}
\CommentTok{\# set the number of folds to 10}
\NormalTok{fun\_control[}\StringTok{"k\_folds"}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\NormalTok{cv\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
k: 0
Train Dataset Size: 636
Val Dataset Size: 71
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.1368539333343506
         val_acc            0.4084506928920746
        val_loss            2.1368539333343506
       valid_mapk           0.4606481194496155

train_model result: {'valid_mapk': 0.4606481194496155, 'val_loss': 2.1368539333343506, 'val_acc': 0.4084506928920746, 'hp_metric': 2.1368539333343506}
k: 1
Train Dataset Size: 636
Val Dataset Size: 71
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.2264761924743652
         val_acc            0.30985915660858154
        val_loss            2.2264761924743652
       valid_mapk           0.4004629850387573

train_model result: {'valid_mapk': 0.4004629850387573, 'val_loss': 2.2264761924743652, 'val_acc': 0.30985915660858154, 'hp_metric': 2.2264761924743652}
k: 2
Train Dataset Size: 636
Val Dataset Size: 71
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.3198959827423096
         val_acc            0.23943662643432617
        val_loss            2.3198959827423096
       valid_mapk           0.3009259104728699

train_model result: {'valid_mapk': 0.3009259104728699, 'val_loss': 2.3198959827423096, 'val_acc': 0.23943662643432617, 'hp_metric': 2.3198959827423096}
k: 3
Train Dataset Size: 636
Val Dataset Size: 71
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.2523036003112793
         val_acc            0.2535211145877838
        val_loss            2.2523036003112793
       valid_mapk           0.3634259104728699

train_model result: {'valid_mapk': 0.3634259104728699, 'val_loss': 2.2523036003112793, 'val_acc': 0.2535211145877838, 'hp_metric': 2.2523036003112793}
k: 4
Train Dataset Size: 636
Val Dataset Size: 71
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.302013635635376
         val_acc            0.22535210847854614
        val_loss             2.302013635635376
       valid_mapk           0.2847222089767456

train_model result: {'valid_mapk': 0.2847222089767456, 'val_loss': 2.302013635635376, 'val_acc': 0.22535210847854614, 'hp_metric': 2.302013635635376}
k: 5
Train Dataset Size: 636
Val Dataset Size: 71
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.2929389476776123
         val_acc            0.19718310236930847
        val_loss            2.2929389476776123
       valid_mapk           0.3379629850387573

train_model result: {'valid_mapk': 0.3379629850387573, 'val_loss': 2.2929389476776123, 'val_acc': 0.19718310236930847, 'hp_metric': 2.2929389476776123}
k: 6
Train Dataset Size: 636
Val Dataset Size: 71
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.294412851333618
         val_acc            0.22535210847854614
        val_loss             2.294412851333618
       valid_mapk           0.3356481194496155

train_model result: {'valid_mapk': 0.3356481194496155, 'val_loss': 2.294412851333618, 'val_acc': 0.22535210847854614, 'hp_metric': 2.294412851333618}
k: 7
Train Dataset Size: 637
Val Dataset Size: 70
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.260765790939331
         val_acc            0.2571428716182709
        val_loss             2.260765790939331
       valid_mapk           0.37731483578681946

train_model result: {'valid_mapk': 0.37731483578681946, 'val_loss': 2.260765790939331, 'val_acc': 0.2571428716182709, 'hp_metric': 2.260765790939331}
k: 8
Train Dataset Size: 637
Val Dataset Size: 70
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric           2.3352291584014893
         val_acc            0.17142857611179352
        val_loss            2.3352291584014893
       valid_mapk           0.3194444477558136

train_model result: {'valid_mapk': 0.3194444477558136, 'val_loss': 2.3352291584014893, 'val_acc': 0.17142857611179352, 'hp_metric': 2.3352291584014893}
k: 9
Train Dataset Size: 637
Val Dataset Size: 70
\end{verbatim}

\begin{verbatim}

     Validate metric           DataLoader 0

        hp_metric            2.22326922416687
         val_acc            0.3142857253551483
        val_loss             2.22326922416687
       valid_mapk           0.36574074625968933

train_model result: {'valid_mapk': 0.36574074625968933, 'val_loss': 2.22326922416687, 'val_acc': 0.3142857253551483, 'hp_metric': 2.22326922416687}
\end{verbatim}

\begin{verbatim}
0.3546296268701553
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Evaluation for the Final Comaprison}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  This is the evaluation that will be used in the comparison.
\end{itemize}

\end{tcolorbox}

\hypertarget{detailed-hyperparameter-plots-9}{%
\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-9}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
l1:  0.11651205242017379
epochs:  100.0
act_fn:  4.7762099479218705
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{029_spot_lightning_csv_files/figure-pdf/cell-20-output-2.pdf}

}

\caption{Contour plots.}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{029_spot_lightning_csv_files/figure-pdf/cell-20-output-3.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{029_spot_lightning_csv_files/figure-pdf/cell-20-output-4.pdf}

}

\end{figure}

\hypertarget{parallel-coordinates-plot-5}{%
\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-5}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

Parallel coordinates plots

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{plot-all-combinations-of-hyperparameters-8}{%
\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-8}}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizing-the-activation-distribution}{%
\subsection{Visualizing the Activation
Distribution}\label{visualizing-the-activation-distribution}}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Reference:}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  The following code is based on
  \href{https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/02-activation-functions.html}{{[}PyTorch
  Lightning TUTORIAL 2: ACTIVATION FUNCTIONS{]}}, Author: Phillip Lippe,
  License: \href{https://creativecommons.org/licenses/by-sa/3.0/}{{[}CC
  BY-SA{]}}, Generated: 2023-03-15T09:52:39.179933.
\end{itemize}

\end{tcolorbox}

After we have trained the models, we can look at the actual activation
values that find inside the model. For instance, how many neurons are
set to zero in ReLU? Where do we find most values in Tanh? To answer
these questions, we can write a simple function which takes a trained
model, applies it to a batch of images, and plots the histogram of the
activations inside the network:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.torch.activation }\ImportTok{import}\NormalTok{ Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish}
\NormalTok{act\_fn\_by\_name }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigmoid"}\NormalTok{: Sigmoid, }\StringTok{"tanh"}\NormalTok{: Tanh, }\StringTok{"relu"}\NormalTok{: ReLU, }\StringTok{"leakyrelu"}\NormalTok{: LeakyReLU, }\StringTok{"elu"}\NormalTok{: ELU, }\StringTok{"swish"}\NormalTok{: Swish\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_config\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_one\_config\_from\_X(X, fun\_control)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\DecValTok{64}\NormalTok{, \_L\_out}\OperatorTok{=}\DecValTok{11}\NormalTok{)}
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NetLightBase(
  (train_mapk): MAPK()
  (valid_mapk): MAPK()
  (test_mapk): MAPK()
  (layers): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Linear(in_features=32, out_features=16, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.25, inplace=False)
    (6): Linear(in_features=16, out_features=16, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.25, inplace=False)
    (9): Linear(in_features=16, out_features=8, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.25, inplace=False)
    (12): Linear(in_features=8, out_features=11, bias=True)
  )
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ visualize\_activations}
\NormalTok{visualize\_activations(model, color}\OperatorTok{=}\SpecialStringTok{f"C}\SpecialCharTok{\{}\DecValTok{0}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{029_spot_lightning_csv_files/figure-pdf/cell-25-output-1.pdf}

}

\end{figure}

\hypertarget{submission}{%
\section{Submission}\label{submission}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OrdinalEncoder}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OrdinalEncoder}
\NormalTok{train\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/train.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\CommentTok{\# remove the id column}
\CommentTok{\# train\_df = train\_df.drop(columns=[\textquotesingle{}id\textquotesingle{}])}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{n\_features }\OperatorTok{=}\NormalTok{ train\_df.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"prognosis"}
\CommentTok{\# Encode our prognosis labels as integers for easier decoding later}
\NormalTok{enc }\OperatorTok{=}\NormalTok{ OrdinalEncoder()}
\NormalTok{y }\OperatorTok{=}\NormalTok{ enc.fit\_transform(train\_df[[target\_column]])}
\NormalTok{test\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/VBDP/test.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{test\_df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& sudden\_fever & headache & mouth\_bleed & nose\_bleed & muscle\_pain &
joint\_pain & vomiting & rash & diarrhea & hypotension & ... &
lymph\_swells & breathing\_restriction & toe\_inflammation &
finger\_inflammation & lips\_irritation & itchiness & ulcers &
toenail\_loss & speech\_problem & bullseye\_rash \\
id & & & & & & & & & & & & & & & & & & & & & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
707 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
708 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
709 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
710 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
711 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... &
... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
1005 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
1006 & 1.0 & 0.0 & 1.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
1007 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & ... &
1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
1008 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
1009 & 1.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 0.0 & 1.0 & 0.0 & ... &
0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\NormalTok{X\_tensor }\OperatorTok{=}\NormalTok{ torch.Tensor(test\_df.values)}
\NormalTok{X\_tensor }\OperatorTok{=}\NormalTok{ X\_tensor.to(fun\_control[}\StringTok{"device"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ model\_loaded(X\_tensor)}
\NormalTok{y.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
torch.Size([303, 11])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# convert the predictions to a numpy array}
\NormalTok{y }\OperatorTok{=}\NormalTok{ y.cpu().detach().numpy()}
\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[9.34182387e-03, 5.37784013e-04, 2.97997333e-03, ...,
        1.40091812e-03, 2.82173569e-04, 6.32511335e-04],
       [9.99848008e-01, 7.51649758e-18, 1.86774578e-25, ...,
        4.66555216e-25, 7.63578790e-28, 1.09261675e-27],
       [0.00000000e+00, 7.19368104e-34, 4.33420110e-03, ...,
        3.09993569e-02, 8.19055815e-13, 9.93454279e-08],
       ...,
       [0.00000000e+00, 0.00000000e+00, 8.14403385e-16, ...,
        2.04248618e-09, 1.57991721e-34, 7.37077867e-24],
       [0.00000000e+00, 2.22367631e-28, 2.23893952e-02, ...,
        8.89114141e-02, 1.02690148e-10, 2.38016310e-06],
       [0.00000000e+00, 4.63367610e-30, 1.01549635e-02, ...,
        5.43198213e-02, 2.20471991e-11, 7.28340353e-07]], dtype=float32)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_sorted\_prediction\_ids }\OperatorTok{=}\NormalTok{ np.argsort(}\OperatorTok{{-}}\NormalTok{y, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{test\_top\_3\_prediction\_ids }\OperatorTok{=}\NormalTok{ test\_sorted\_prediction\_ids[:,:}\DecValTok{3}\NormalTok{]}
\NormalTok{original\_shape }\OperatorTok{=}\NormalTok{ test\_top\_3\_prediction\_ids.shape}
\NormalTok{test\_top\_3\_prediction }\OperatorTok{=}\NormalTok{ enc.inverse\_transform(test\_top\_3\_prediction\_ids.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{test\_top\_3\_prediction }\OperatorTok{=}\NormalTok{ test\_top\_3\_prediction.reshape(original\_shape)}
\NormalTok{test\_df[}\StringTok{\textquotesingle{}prognosis\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.apply\_along\_axis(}\KeywordTok{lambda}\NormalTok{ x: np.array(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{.join(x), dtype}\OperatorTok{=}\StringTok{"object"}\NormalTok{), }\DecValTok{1}\NormalTok{, test\_top\_3\_prediction)}
\NormalTok{test\_df[}\StringTok{\textquotesingle{}prognosis\textquotesingle{}}\NormalTok{].reset\_index().to\_csv(}\StringTok{\textquotesingle{}./data/VBDP/submission.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{differences-to-the-spotpython-approaches-for-torch-sklearn-and-river}{%
\subsection{\texorpdfstring{Differences to the spotPython Approaches for
\texttt{torch}, \texttt{sklearn} and
\texttt{river}}{Differences to the spotPython Approaches for torch, sklearn and river}}\label{differences-to-the-spotpython-approaches-for-torch-sklearn-and-river}}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Data Loading in Lightning}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{itemize}
\tightlist
\item
  Data loading is handled independently from the \texttt{fun\_control}
  dictionary by \texttt{Lightning} and \texttt{PyTorch}.
\item
  In contrast to \texttt{spotPython} with \texttt{torch}, \texttt{river}
  and \texttt{sklearn}, the data sets are not added to the
  \texttt{fun\_control} dictionary.
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-specification-of-preprocessing-model-31}{%
\subsubsection{Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-31}}

The \texttt{fun\_control} dictionary, the \texttt{torch},
\texttt{sklearn}and \texttt{river} versions of \texttt{spotPython} allow
the specification of a data preprocessing pipeline, e.g., for the
scaling of the data or for the one-hot encoding of categorical
variables, see
Section~\ref{sec-specification-of-preprocessing-model-14}. This feature
is not used in the \texttt{Lightning} version.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Data preprocessing in Lightning}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-caution-color!10!white, toptitle=1mm, colframe=quarto-callout-caution-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

Lightning allows the data preprocessing to be specified in the
\texttt{LightningDataModule} class. It is not considered here, because
it should be computed at one location only.

\end{tcolorbox}

\hypertarget{sec-taking-a-look-at-the-data-31}{%
\subsection{Taking a Look at the
Data}\label{sec-taking-a-look-at-the-data-31}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ spotPython.light.vbdpdataset }\ImportTok{import}\NormalTok{ CSVDataset}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\ImportTok{from}\NormalTok{ torchvision.transforms }\ImportTok{import}\NormalTok{ ToTensor}

\CommentTok{\# Create an instance of CSVDataset}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ CSVDataset(csv\_file}\OperatorTok{=}\StringTok{"./data/VBDP/train.csv"}\NormalTok{, train}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\CommentTok{\# show the dimensions of the input data}
\BuiltInTok{print}\NormalTok{(dataset[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{].shape)}
\CommentTok{\# show the first element of the input data}
\BuiltInTok{print}\NormalTok{(dataset[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{])}
\CommentTok{\# show the size of the dataset}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dataset Size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
torch.Size([64])
tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
Dataset Size: 707
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{3}
\CommentTok{\# Create DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Batch Size: 3
---------------
Inputs: tensor([[1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,
         0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,
         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,
         1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
Targets: tensor([4, 5, 8])
\end{verbatim}

\hypertarget{sec-the-mapk-metric-31}{%
\subsection{The MAPK Metric}\label{sec-the-mapk-metric-31}}

Here is an example how the MAPK metric is calculated.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.torch.mapk }\ImportTok{import}\NormalTok{ MAPK}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{mapk }\OperatorTok{=}\NormalTok{ MAPK(k}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{target }\OperatorTok{=}\NormalTok{ torch.tensor([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{])}
\NormalTok{preds }\OperatorTok{=}\NormalTok{ torch.tensor(}
\NormalTok{    [}
\NormalTok{        [}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{],  }\CommentTok{\# 0 is in top 2}
\NormalTok{        [}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.2}\NormalTok{],  }\CommentTok{\# 1 is in top 2}
\NormalTok{        [}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.3}\NormalTok{],  }\CommentTok{\# 2 is in top 2}
\NormalTok{        [}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.1}\NormalTok{],  }\CommentTok{\# 2 isn\textquotesingle{}t in top 2}
\NormalTok{    ]}
\NormalTok{)}
\NormalTok{mapk.update(preds, target)}
\BuiltInTok{print}\NormalTok{(mapk.compute()) }\CommentTok{\# tensor(0.6250)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tensor(0.6250)
\end{verbatim}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\hypertarget{introduction-to-jupyter-notebook}{%
\chapter{Introduction to Jupyter
Notebook}\label{introduction-to-jupyter-notebook}}

Jupyter Notebook is a widely used tool in the Data Science community. It
is easy to use and the produced code can be run per cell. This has a
huge advantage, because with other tools e.g.~(pycharm, vscode, etc.)
the whole script is executed. This can be a time consuming process,
especially when working with huge data sets.

\hypertarget{different-notebook-cells}{%
\section{Different Notebook cells}\label{different-notebook-cells}}

There are different cells that the notebook is currently supporting:

\begin{itemize}
\item
  code cells
\item
  markdown cells
\item
  raw cells
\end{itemize}

As a default, every cells in jupyter is set to ``code''

\hypertarget{code-cells}{%
\subsection{Code cells}\label{code-cells}}

The code cells are used to execute the code. They are following the
logic of the choosen kernel. Therefore, it is important to keep in mind
which programming language is currently used. Otherwise one might yield
an error because of the wrong syntax.

The code cells are executed my be \textbf{ Run} button (can be found in
the header of the notebook).

\hypertarget{markdown-cells}{%
\subsection{Markdown cells}\label{markdown-cells}}

The markdown cells are a usefull tool to comment the written code.
Especially with the help of headers can the code be brought in a more
readable format. If you are not familiar with the markdown syntax, you
can find a usefull cheat sheet here:
\href{https://www.ibm.com/docs/en/db2-event-store/2.0.0?topic=notebooks-markdown-jupyter-cheatsheet}{Markdown
Cheat Sheeet}

\hypertarget{raw-cells}{%
\subsection{Raw cells}\label{raw-cells}}

The ``Raw NBConvert'' cell type can be used to render different code
formats into HTML or LaTeX by Sphinx. This information is stored in the
notebook metadata and converted appropriately.

\hypertarget{usage}{%
\subsubsection{Usage}\label{usage}}

To select a desired format from within Jupyter, select the cell
containing your special code and choose options from the following
dropdown menus:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select ``Raw NBConvert''
\item
  Switch the Cell Toolbar to ``Raw Cell Format'' (The cell toolbar can
  be found under View)
\item
  Chose the appropriate ``Raw NBConvert Format'' within the cell
\end{enumerate}

Data Science is fun

\hypertarget{install-packages}{%
\section{Install Packages}\label{install-packages}}

Because python is a heavily used programming language, there are many
different packags that can make your life easier. Sadly, there are only
a few standard packages that are already included in your python
enviroment. If you have the need to install a new package in your
enviroment, you can simply do that by exectuing the following code
snippet in a \textbf{code cell}

\texttt{!pip\ install\ numpy}

\begin{itemize}
\item
  The \emph{!} is used to run the cell as a shell command
\item
  \emph{pip} is package manager for python packages.
\item
  \emph{numpy} is the the package you want to install
\end{itemize}

\textbf{Hint:} It is often usefull to restart the kernel after
installing a package, otherwise loading the package could lead to an
error.

\hypertarget{load-packages}{%
\section{Load Packages}\label{load-packages}}

After successfully installing the package it is necessary to import them
before you can work with them. The import of the packages is done in the
following way:

\texttt{import\ numpy\ as\ np}

The imported packages are often abbreviated. This is because you need to
specify where the function is coming from.

The most common abbreviations for data science packages are:

\begin{longtable}[]{@{}lll@{}}
\caption{Abbreviations for data science packages}\tabularnewline
\toprule\noalign{}
Abbreviation & Package & Import \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Abbreviation & Package & Import \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
np & numpy & import numpy as np \\
pd & pandas & import pandas as pd \\
plt & matplotlib & import matplotlib.pyplot as plt \\
px & plotly & import plotly.exprss as px \\
tf & tensorflow & import tensorflow as tf \\
sns & seaborn & import seaborn as sns \\
dt & datetime & import datetime as dt \\
pkl & pickle & import pickle as pkl \\
\end{longtable}

\hypertarget{functions-in-python}{%
\section{Functions in Python}\label{functions-in-python}}

Because python is not using Semicolon's it is import to keep track of
indentation in your code. The indentation works as a placeholder for the
semicolons. This is especially important if your are defining loops,
functions, etc. \ldots{}

\textbf{Example:} We are defining a function that calculates the squared
sum of its input parameters

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ squared\_sum(x,y): }
\NormalTok{    z }\OperatorTok{=}\NormalTok{ x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ y}\OperatorTok{**}\DecValTok{2}
    \ControlFlowTok{return}\NormalTok{ z}
\end{Highlighting}
\end{Shaded}

If you are working with something that needs indentation, it will be
already done by the notebook.

\textbf{Hint:} Keep in mind that is good practice to use the
\emph{return} parameter. If you are not using \emph{return} and a
function has multiple paramaters that you would like to return, it will
only return the last one defined.

\hypertarget{list-of-useful-jupyter-notebook-shortcuts}{%
\section{List of Useful Jupyter Notebook
Shortcuts}\label{list-of-useful-jupyter-notebook-shortcuts}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{List of useful Jupyter Notebook Shortcuts}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Keyboard Shortcut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Menu Tools
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Keyboard Shortcut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Menu Tools
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Save notebook & Esc + s & File  Save and Checkpoint \\
Create new Cell & Esc + a (above), Esc + b (below) & Insert  Cell
above; Insert  Cell below \\
Run Cell & Ctrl + enter & Cell  Run Cell \\
Copy Cell & c & Copy Key \\
Paste Cell & v & Paste Key \\
Interrupt Kernel & Esc + i i & Kernel  Interrupt \\
Restart Kernel & Esc + 0 0 & Kernel  Restart \\
\end{longtable}

If you combine everything you can create beautiful graphics

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Generate 100 random data points along 3 dimensions}
\NormalTok{x, y, scale }\OperatorTok{=}\NormalTok{ np.random.randn(}\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\CommentTok{\# Map each onto a scatterplot we\textquotesingle{}ll create with Matplotlib}
\NormalTok{ax.scatter(x}\OperatorTok{=}\NormalTok{x, y}\OperatorTok{=}\NormalTok{y, c}\OperatorTok{=}\NormalTok{scale, s}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{abs}\NormalTok{(scale)}\OperatorTok{*}\DecValTok{500}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{"Some random data, created with the Jupyter Notebook!"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{a_01_intro_to_notebooks_files/figure-pdf/cell-3-output-1.pdf}

}

\end{figure}

\hypertarget{git-introduction}{%
\chapter{Git Introduction}\label{git-introduction}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

In this learning unit, you will learn how to set up Git as a version
control system for a project. The most important Git commands will be
explained. You will learn how to track and manage changes to your
projects with Git. Specifically:

\begin{itemize}
\tightlist
\item
  Initializing a repository: \texttt{git\ init}
\item
  Ignoring files: \texttt{.gitignore}
\item
  Adding files to the staging area: \texttt{git\ add}
\item
  Checking status changes: \texttt{git\ status}
\item
  Reviewing history: \texttt{git\ log}
\item
  Creating a new branch: \texttt{git\ branch}
\item
  Switching to the current branch: \texttt{git\ switch} and
  \texttt{git\ checkout}
\item
  Merging two branches: \texttt{git\ merge}
\item
  Resolving conflicts
\item
  Reverting changes: \texttt{git\ revert}
\item
  Uploading changes to GitLab: \texttt{git\ push}
\item
  Downloading changes from GitLab: \texttt{git\ pull}
\item
  Advanced: \texttt{git\ rebase}
\end{itemize}

\hypertarget{basics-of-git}{%
\section{Basics of Git}\label{basics-of-git}}

\hypertarget{initializing-a-repository-git-init}{%
\subsection{\texorpdfstring{Initializing a Repository:
\texttt{git\ init}}{Initializing a Repository: git init}}\label{initializing-a-repository-git-init}}

To set up Git as a version control system for your project, you need to
initialize a new Git repository at the top-level folder, which is the
working directory of your project. This is done using the
\texttt{git\ init} command.

All files in this folder and its subfolders will automatically become
part of the repository. Creating a Git repository is similar to adding
an all-powerful passive observer of all things to your project. Git sits
there, observes, and takes note of even the smallest changes, such as a
single character in a file within a repository with hundreds of files.
And it will tell you where these changes occurred if you forget. Once
Git is initialized, it monitors all changes made within the working
directory, and it tracks the history of events from that point forward.
For this purpose, a historical timeline is created for your project,
referred to as a ``branch,'' and the initial branch is named
\texttt{main}. So, when someone says they are on the
\texttt{main\ branch} or working on the \texttt{main\ branch}, it means
they are in the historical main timeline of the project. The Git
repository, often abbreviated as \texttt{repo}, is a virtual
representation of your project, including its history and branches, a
book, if you will, where you can look up and retrieve the entire history
of the project: you work in your working directory, and the Git
repository tracks and stores your work.

\hypertarget{ignoring-files-.gitignore}{%
\subsection{\texorpdfstring{Ignoring Files:
\texttt{.gitignore}}{Ignoring Files: .gitignore}}\label{ignoring-files-.gitignore}}

It's useful that Git watches and keeps an eye on everything in your
project. However, in most projects, there are files and folders that you
don't need or want to keep an eye on. These may include system files,
local project settings, libraries with dependencies, and so on.

You can exclude any file or folder from your Git repository by including
them in the \texttt{.gitignore} file. In the \texttt{.gitignore} file,
you create a list of file names, folder names, and other items that Git
should not track, and Git will ignore these items. Hence the name
``gitignore.'' Do you want to track a file that you previously ignored?
Simply remove the mention of the file in the gitignore file, and Git
will start tracking it again.

\hypertarget{adding-changes-to-the-staging-area-git-add}{%
\subsection{\texorpdfstring{Adding Changes to the Staging Area:
\texttt{git\ add}}{Adding Changes to the Staging Area: git add}}\label{adding-changes-to-the-staging-area-git-add}}

The interesting thing about Git as an all-powerful, passive observer of
all things is that it's very passive. As long as you don't tell Git what
to remember, it will passively observe the changes in the project folder
but do nothing.

When you make a change to your project that you want Git to include in
the project's history to take a snapshot of so you can refer back to it
later, your personal checkpoint, if you will, you need to first stage
the changes in the staging area. What is the staging area? The staging
area is where you collect changes to files that you want to include in
the project's history.

This is done using the \texttt{git\ add} command. You can specify which
files you want to add by naming them, or you can add all of them using
\texttt{-A}. By doing this, you're telling Git that you've made changes
and want it to remember these particular changes so you can recall them
later if needed. This is important because you can choose which changes
you want to stage, and those are the changes that will eventually be
transferred to the history.

\textbf{Note:} When you run \texttt{git\ add}, the changes are not
transferred to the project's history. They are only transferred to the
staging area.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example of git add from the beginning}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new directory for your}
\CommentTok{\# repository and navigate to that directory:}

\FunctionTok{mkdir}\NormalTok{ my{-}repo}
\BuiltInTok{cd}\NormalTok{ my{-}repo}

\CommentTok{\# Initialize the repository with git init:}

\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a .gitignore file for Python code.}
\CommentTok{\# You can use a template from GitHub:}

\ExtensionTok{curl}\NormalTok{ https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore }\AttributeTok{{-}o}\NormalTok{ .gitignore}

\CommentTok{\# Add your files to the repository using git add:}

\FunctionTok{git}\NormalTok{ add .}
\end{Highlighting}
\end{Shaded}

This adds all files in the current directory to the repository, except
for the files listed in the .gitignore file.

\end{tcolorbox}

\hypertarget{transferring-changes-to-memory-git-commit}{%
\subsection{\texorpdfstring{Transferring Changes to Memory:
\texttt{git\ commit}}{Transferring Changes to Memory: git commit}}\label{transferring-changes-to-memory-git-commit}}

The power of Git becomes evident when you start transferring changes to
the project history. This is done using the \texttt{git\ commit}
command. When you run \texttt{git\ commit}, you inform Git that the
changes in the staging area should be added to the history of the
project so that they can be referenced or retrieved later.

Additionally, you can add a commit message with the \texttt{-m} option
to explain what changes were made. So when you look back at the project
history, you can see that you added a new feature.

\texttt{git\ commit} creates a snapshot, an image of the current state
of your project at that specific time, and adds it to the branch you are
currently working on.

As you work on your project and transfer more snapshots, the branch
grows and forms a timeline of events. This means you can now look back
at every transfer in the branch and see what your code looked like at
that time.

You can compare any phase of your code with any other phase of your code
to find errors, restore deleted code, or do things that would otherwise
not be possible, such as resetting the project to a previous state or
creating a new timeline from any point.

So how often should you add these commits? My rule of thumb is not to
commit too often. It's better to have a Git repository with too many
commits than one with too few commits.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

After adding your files with \texttt{git\ add}, you can create a commit
to save your changes. Use the \texttt{git\ commit} command with the
\texttt{-m} option to specify your commit message:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"My first commit message"}
\end{Highlighting}
\end{Shaded}

This creates a new commit with the added files and the specified commit
message.

\end{tcolorbox}

\hypertarget{check-the-status-of-your-repository-git-status}{%
\subsection{\texorpdfstring{Check the Status of Your Repository:
\texttt{git\ status}}{Check the Status of Your Repository: git status}}\label{check-the-status-of-your-repository-git-status}}

If you're wondering what you've changed in your project since the last
commit snapshot, you can always check the Git status. Git will list
every modified file and the current status of each file.

This status can be either:

\begin{itemize}
\tightlist
\item
  Unchanged (\texttt{unmodified}), meaning nothing has changed since you
  last transferred it, or
\item
  It's been changed (\texttt{changed}) but not staged (\texttt{staged})
  to be transferred into the history, or
\item
  Something has been added to staging (\texttt{staged}) and is ready to
  be transferred into the history.
\end{itemize}

When you run \texttt{git\ status}, you get an overview of the current
state of your project.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

The \texttt{git\ status} command displays the status of your working
directory and the staging area. It shows you which files have been
modified, which files are staged for commit, and which files are not yet
being tracked:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ status}
\end{Highlighting}
\end{Shaded}

\texttt{git\ status} is a useful tool to keep track of your changes and
ensure that you have added all the desired files for commit.

\end{tcolorbox}

\hypertarget{review-your-repositorys-history-git-log}{%
\subsection{\texorpdfstring{Review Your Repository's History:
\texttt{git\ log}}{Review Your Repository's History: git log}}\label{review-your-repositorys-history-git-log}}

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

You can view the history of your commits with the \texttt{git\ log}
command. This command displays a list of all the commits in the current
branch, along with information such as the author, date, and commit
message:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ log}
\end{Highlighting}
\end{Shaded}

There are many options to customize the output of \texttt{git\ log}. For
example, you can use the \texttt{-\/-pretty} option to change the format
of the output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ log }\AttributeTok{{-}{-}pretty}\OperatorTok{=}\NormalTok{oneline}
\end{Highlighting}
\end{Shaded}

This displays each commit in a single line.

\end{tcolorbox}

\hypertarget{branches-timelines}{%
\section{Branches (Timelines)}\label{branches-timelines}}

\hypertarget{creating-an-alternative-timeline-git-branch}{%
\subsection{\texorpdfstring{Creating an Alternative Timeline:
\texttt{git\ branch}}{Creating an Alternative Timeline: git branch}}\label{creating-an-alternative-timeline-git-branch}}

In the course of developing a project, you often reach a point where you
want to add a new feature, but doing so might require changing the
existing code in a way that could be challenging to undo later.

Or maybe you just want to experiment and be able to discard your work if
the experiment fails. In such cases, Git allows you to create an
alternative timeline called a \texttt{branch} to work in.

This new \texttt{branch} has its own name and exists in parallel with
the \texttt{main\ branch} and all other branches in your project.

During development, you can switch between branches and work on
different versions of your code concurrently. This way, you can have a
stable codebase in the \texttt{main\ branch} while developing an
experimental feature in a separate \texttt{branch}. When you switch from
one \texttt{branch} to another, the code you're working on is
automatically reset to the latest commit of the branch you're currently
in.

If you're working in a team, different team members can work on their
own branches, creating an entire universe of alternative timelines for
your project. When features are completed, they can be seamlessly merged
back into the \texttt{main\ branch}.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

To create a new \texttt{branch}, you can use the \texttt{git\ branch}
command with the name of the new \texttt{branch} as an argument:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ branch my{-}tests}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{the-pointer-to-the-current-branch-head}{%
\subsection{\texorpdfstring{The Pointer to the Current Branch:
\texttt{HEAD}}{The Pointer to the Current Branch: HEAD}}\label{the-pointer-to-the-current-branch-head}}

How does Git know where you are on the timeline, and how can you keep
track of your position?

You're always working at the tip (\texttt{HEAD}) of the currently active
branch. The \texttt{HEAD} pointer points there quite literally. In a new
project archive with just a single \texttt{main\ branch} and only new
commits being added, \texttt{HEAD} always points to the latest commit in
the \texttt{main\ branch}. That's where you are.

However, if you're in a repository with multiple branches, meaning
multiple alternative timelines, \texttt{HEAD} will point to the latest
commit in the branch you're currently working on.

\hypertarget{switching-to-an-alternative-timeline-git-switch}{%
\subsection{\texorpdfstring{Switching to an Alternative Timeline:
\texttt{git\ switch}}{Switching to an Alternative Timeline: git switch}}\label{switching-to-an-alternative-timeline-git-switch}}

As your project grows, and you have multiple branches, you need to be
able to switch between these branches. This is where the \texttt{switch}
command comes into play.

At any time, you can use the \texttt{git\ switch} command with the name
of the branch you want to switch to, and \texttt{HEAD} moves from your
current branch to the one you specified.

If you've made changes to your code before switching, Git will attempt
to carry those changes over to the branch you're switching to. However,
if these changes conflict with the target branch, the switch will be
canceled.

To resolve this issue without losing your changes, return to the
original branch, add and commit your recent changes, and then perform
the \texttt{switch}.

\hypertarget{switching-to-an-alternative-timeline-and-making-changes-git-checkout}{%
\subsection{\texorpdfstring{Switching to an Alternative Timeline and
Making Changes:
\texttt{git\ checkout}}{Switching to an Alternative Timeline and Making Changes: git checkout}}\label{switching-to-an-alternative-timeline-and-making-changes-git-checkout}}

To switch between branches, you can also use the \texttt{git\ checkout}
command. It works similarly to \texttt{git\ switch} for this purpose:
you pass the name of the branch you want to switch to, and \texttt{HEAD}
moves to the beginning of that branch.

But \texttt{checkout} can do more than just switch to another timeline.
With \texttt{git\ checkout}, you can also move to any commit point in
any timeline. In other words, you can travel back in time and work on
code from the past.

To do this, use \texttt{git\ checkout} and provide the commit ID. This
is an automatically generated, random combination of letters and numbers
that identifies each commit. You can retrieve the commit ID using
\texttt{git\ log}. When you run \texttt{git\ log}, you get a list of all
the commits in your repository, starting with the most recent ones.

When you use \texttt{git\ checkout} with an older commit ID, you check
out a commit in the middle of a branch. This disrupts the timeline, as
you're actively attempting to change history. Git doesn't want you to do
that because, much like in a science fiction movie, altering the past
might also alter the future. In our case, it would break the version
control branch's coherence.

To prevent you from accidentally disrupting time and altering history,
checking out an earlier commit in any branch results in the warning
``Detached Head,'' which sounds rather ominous. The ``Detached Head''
warning is appropriate because it accurately describes what's happening.
Git literally detaches the head from the branch and sets it aside.

Now, you're working outside of time in a space unbound to any timeline,
which again sounds rather threatening but is perfectly fine in reality.

To continue working on this past code, all you need to do is reattach it
to the timeline. You can use \texttt{git\ branch} to create a new
branch, and the detached head will automatically attach to this new
branch.

Instead of breaking the history, you've now created a new alternative
timeline that starts in the past, allowing you to work safely. You can
continue working on the branch as usual.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

To switch to a new branch, you can use the \texttt{git\ checkout}
command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ checkout meine{-}tests}
\end{Highlighting}
\end{Shaded}

Now you're using the new branch and can make changes independently from
the original branch.

\end{tcolorbox}

\hypertarget{the-difference-between-checkout-and-switch}{%
\subsection{\texorpdfstring{The Difference Between \texttt{checkout} and
\texttt{switch}}{The Difference Between checkout and switch}}\label{the-difference-between-checkout-and-switch}}

What is the difference between \texttt{git\ switch} and
\texttt{git\ checkout}? \texttt{git\ switch} and \texttt{git\ checkout}
are two different commands that both serve the purpose of switching
between branches. You can use both to switch between branches, but they
have an important distinction. \texttt{git\ switch} is a new command
introduced with Git 2.23. \texttt{git\ checkout} is an older command
that has existed since Git 1.6.0. So, \texttt{git\ switch} and
\texttt{git\ checkout} have different origins. \texttt{git\ switch} was
introduced to separate the purposes of \texttt{git\ checkout}.
\texttt{git\ checkout} has two different purposes: 1. It can be used to
switch between branches, and 2. It can be used to reset files to the
state of the last commit.

Here's an example: In my project, I made a change since the last commit,
but I haven't staged it yet. Then, I realized that I actually don't want
this change. I want to reset the file to the state before the last
commit. As long as I haven't committed my changes, I can do this with
\texttt{git\ checkout} by targeting the specific file. So, if that file
is named \texttt{main.js}, I can say: \texttt{git\ checkout\ main.js}.
And the file will be reset to the state of the last commit, which makes
sense. I'm checking out the file from the last commit.

But that's quite different from switching between the beginning of one
branch to another. \texttt{git\ switch} and \texttt{git\ restore} were
introduced to separate these two operations. \texttt{git\ switch} is for
switching between branches, and \texttt{git\ restore} is for resetting
the specified file to the state of the last commit. If you try to
restore a file with \texttt{git\ switch}, it simply won't work. It's not
intended for that. As I mentioned earlier, it's about separating
concerns.

:::\{.callout-note\} \#\#\#\# Difference Between \texttt{checkout} and
\texttt{switch} \texttt{git\ checkout} and \texttt{git\ switch} are both
commands for switching between branches in a Git repository. The main
difference between the two commands is that \texttt{git\ switch} is a
newer command specifically designed for branch switching, while
\texttt{git\ checkout} is an older command that can be used for various
tasks, including branch switching.

Here's an example demonstrating how to initialize a repository and
switch between branches:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new directory for your repository}
\CommentTok{\# and navigate to that directory:}
\FunctionTok{mkdir}\NormalTok{ my{-}repo}
\BuiltInTok{cd}\NormalTok{ my{-}repo}

\CommentTok{\# Initialize the repository with git init:}
\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a new branch with git branch:}
\FunctionTok{git}\NormalTok{ branch my{-}new{-}branch}

\CommentTok{\# Switch to the new branch using git switch:}
\FunctionTok{git}\NormalTok{ switch my{-}new{-}branch}

\CommentTok{\# Alternatively, you can also use git checkout}
\CommentTok{\# to switch to the new branch:}

\FunctionTok{git}\NormalTok{ checkout my{-}new{-}branch}
\end{Highlighting}
\end{Shaded}

Both commands lead to the same result: You are now on the new branch.

\hypertarget{merging-branches-and-resolving-conflicts}{%
\section{Merging Branches and Resolving
Conflicts}\label{merging-branches-and-resolving-conflicts}}

\hypertarget{git-merge-merging-two-timelines}{%
\subsection{\texorpdfstring{git \texttt{merge}: Merging Two
Timelines}{git merge: Merging Two Timelines}}\label{git-merge-merging-two-timelines}}

Git allows you to split your development work into as many branches or
alternative timelines as you like, enabling you to work on many
different versions of your code simultaneously without losing or
overwriting any of your work.

This is all well and good, but at some point, you need to bring those
various versions of your code back together into one branch. That's
where \texttt{git\ merge} comes in.

Consider an example where you have two branches, a \texttt{main\ branch}
and an experimental branch called \texttt{experimental-branch}. In the
experimental branch, there is a new feature. To merge these two
branches, you set \texttt{HEAD} to the branch where you want to
incorporate the code and execute \texttt{git\ merge} followed by the
name of the branch you want to merge. \texttt{HEAD} is a special pointer
that points to the current branch. When you run \texttt{git\ merge}, it
combines the code from the branch associated with \texttt{HEAD} with the
code from the branch specified by the branch name you provide.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize the repository}
\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a new branch called "experimental{-}branch"}
\FunctionTok{git}\NormalTok{ branch experimental{-}branch}

\CommentTok{\# Switch to the "experimental{-}branch"}
\FunctionTok{git}\NormalTok{ checkout experimental{-}branch}

\CommentTok{\# Add the new feature here and}
\CommentTok{\# make a commit}
\CommentTok{\# ...}

\CommentTok{\# Switch back to the "main" branch}
\FunctionTok{git}\NormalTok{ checkout main}

\CommentTok{\# Perform the merge}
\FunctionTok{git}\NormalTok{ merge experimental{-}branch}
\end{Highlighting}
\end{Shaded}

During the merge, matching pieces of code in the branches overlap, and
any new code from the branch being merged is added to the project. So
now, the main branch also contains the code from the experimental
branch, and the events of the two separate timelines have been merged
into a single one. What's interesting is that even though the
experimental branch was merged with the main branch, the last commit of
the experimental branch remains intact, allowing you to continue working
on the experimental branch separately if you wish.

\hypertarget{resolving-conflicts-when-merging}{%
\subsection{Resolving Conflicts When
Merging}\label{resolving-conflicts-when-merging}}

Merging branches where there are no code changes at the same place in
both branches is a straightforward process. It's also a rare process. In
most cases, there will be some form of conflict between the branches --
the same code or the same code area has been modified differently in the
different branches. Merging two branches with such conflicts will not
work, at least not automatically.

In this case, Git doesn't know how to merge this code. So, when such a
situation occurs, it's marked as a conflict, and the merging process is
halted. This might sound more dramatic than it is. When you get a
conflict warning, Git is saying there are two different versions here,
and Git needs to know which one you want to keep. To help you figure out
the conflict, Git combines all the code into a single file and
automatically marks the conflicting code as the current change, which is
the original code from the branch you're working on, or as the incoming
change, which is the code from the file you're trying to merge.

To resolve this conflict, you'll edit the file to literally resolve the
code conflict. This might mean accepting either the current or incoming
change and discarding the other. It could mean combining both changes or
something else entirely. It's up to you. So, you edit the code to
resolve the conflict. Once you've resolved the conflict by editing the
code, you add the new conflict-free version to the staging area with
\texttt{git\ add} and then commit the merged code with
\texttt{git\ commit}. That's how the conflict is resolved.

A merge conflict occurs when Git struggles to automatically merge
changes from two different branches. This usually happens when changes
were made to the same line in the same file in both branches. To resolve
a merge conflict, you must manually edit the affected files and choose
the desired changes. Git marks the conflict areas in the file with
special markings like
\texttt{\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}},
\texttt{=======}, and
\texttt{\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}}.
You can search for these markings and manually select the desired
changes. After resolving the conflicts, you can add the changes with
\texttt{git\ add} and create a new commit with \texttt{git\ commit} to
complete the merge.

Here's an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform the merge (this will cause a conflict)}
\FunctionTok{git}\NormalTok{ merge experimenteller{-}branch}

\CommentTok{\# Open the affected file in an editor and manually resolve the conflicts}
\CommentTok{\# ...}

\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add }\OperatorTok{\textless{}}\NormalTok{filename}\OperatorTok{\textgreater{}}

\CommentTok{\# Create a new commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Resolved conflicts"}
\end{Highlighting}
\end{Shaded}

\hypertarget{git-revert-undoing-something}{%
\subsection{\texorpdfstring{git \texttt{revert}: Undoing
Something}{git revert: Undoing Something}}\label{git-revert-undoing-something}}

One of the most powerful features of any software tool is the ``Undo''
button. Make a mistake, press ``Undo,'' and it's as if it never
happened. However, that's not quite as simple when an all-powerful,
passive observer is watching and recording your project's history. How
do you undo something that you've added to the history without rewriting
the history?

The answer is that you can overwrite the history with the
\texttt{git\ reset} command, but that's quite risky and not a good
practice.

A better solution is to work with the historical timeline and simply
place an older version of your code at the top of the branch. This is
done with \texttt{git\ revert}. To make this work, you need to know the
commit ID of the commit you want to go back to.

The commit ID is a machine-generated set of random numbers and letters,
also known as a hash. To get a list of all the commits in the
repository, including the commit ID and commit message, you can run
\texttt{git\ log}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Show the list of all operations in the repository}
\NormalTok{git log}
\end{Highlighting}
\end{Shaded}

By the way, it's a good idea to leave clear and informative commit
messages for this reason. This way, you know what happened in your
previous commits. Once you've found the commit you want to revert to,
call that commit ID with \texttt{git\ revert}, and then the ID. This
will create a new commit at the top of the branch with the code from the
reference commit. To transfer the code to the branch, add a commit
message and save it. Now, the last commit in your branch matches the
commit you're reverting to, and your project's history remains intact.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ revert}}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Initialize a new repository}
\NormalTok{git init}

\NormalTok{\# Create a new file}
\NormalTok{echo "Hello, World" \textgreater{} file.txt}

\NormalTok{\# Add the file to the repository}
\NormalTok{git add file.txt}

\NormalTok{\# Create a new commit}
\NormalTok{git commit {-}m "First commit"}

\NormalTok{\# Modify the file}
\NormalTok{echo "Goodbye, World" \textgreater{} file.txt}

\NormalTok{\# Add the modified file}
\NormalTok{git add file.txt}

\NormalTok{\# Create a new commit}
\NormalTok{git commit {-}m "Second commit"}

\NormalTok{\# Use git log to find the commit ID of the second commit}
\NormalTok{git log}

\NormalTok{\# Use git revert to undo the changes from the second commit}
\NormalTok{git revert \textless{}commit{-}id\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

To download the \texttt{students} branch from the repository
\texttt{git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git}
to your local machine, add a file, and upload the changes, you can
follow these steps:

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ clone}, \texttt{git\ checkout},
\texttt{git\ add}, \texttt{git\ commit}, \texttt{git\ push}}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clone the repository to your local machine:}
\FunctionTok{git}\NormalTok{ clone git@git{-}ce.rwth{-}aachen.de:spotseven{-}lab/numerische{-}mathematik{-}sommersemester2023.git}

\CommentTok{\# Change to the cloned repository:}
\BuiltInTok{cd}\NormalTok{ numerische{-}mathematik{-}sommersemester2023}

\CommentTok{\# Switch to the students branch:}
\FunctionTok{git}\NormalTok{ checkout students}

\CommentTok{\# Create the Test folder if it doesn\textquotesingle{}t exist:}
\FunctionTok{mkdir}\NormalTok{ Test}

\CommentTok{\# Create the Testdatei.txt file in the Test folder:}
\FunctionTok{touch}\NormalTok{ Test/Testdatei.txt}

\CommentTok{\# Add the file with git add:}
\FunctionTok{git}\NormalTok{ add Test/Testdatei.txt}

\CommentTok{\# Commit the changes with git commit:}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Added Testdatei.txt"}

\CommentTok{\# Push the changes with git push:}
\FunctionTok{git}\NormalTok{ push origin students}
\end{Highlighting}
\end{Shaded}

This will upload the changes to the server and update the students
branch in the repository.

\end{tcolorbox}

\hypertarget{downloading-from-gitlab}{%
\section{Downloading from GitLab}\label{downloading-from-gitlab}}

To download changes from a GitLab repository to your local machine, you
can use the \texttt{git\ pull} command. This command downloads the
latest changes from the specified remote repository and merges them with
your local repository.

Here is an example:

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ pull}}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Navigate to the local repository}
\CommentTok{\# linked to the GitHub repository:}
\BuiltInTok{cd}\NormalTok{ my{-}local{-}repository}

\CommentTok{\# Make sure you are in the correct branch:}
\FunctionTok{git}\NormalTok{ checkout main}

\CommentTok{\# Download the latest changes from GitHub:}
\FunctionTok{git}\NormalTok{ pull origin main}
\end{Highlighting}
\end{Shaded}

This downloads the latest changes from the main branch of the remote
repository named ``origin'' and merges them with your local repository.

\end{tcolorbox}

If there are conflicts between the downloaded changes and your local
changes, you will need to resolve them manually before proceeding.

\hypertarget{advanced}{%
\section{Advanced}\label{advanced}}

\hypertarget{git-rebase-moving-the-base-of-a-branch}{%
\subsection{\texorpdfstring{git \texttt{rebase}: Moving the Base of a
Branch}{git rebase: Moving the Base of a Branch}}\label{git-rebase-moving-the-base-of-a-branch}}

In some cases, you may need to ``rewrite history.'' A common scenario is
that you've been working on a new feature in a feature branch, and you
realize that the work should have actually happened in the
\texttt{main\ branch}.

To resolve this issue and make it appear as if the work occurred in the
\texttt{main\ branch}, you can reset the experimental branch. ``Rebase''
literally means detaching the base of the experimental branch and moving
it to the beginning of another branch, giving the branch a new base,
thus ``rebasing.''

This operation is performed from the branch you want to ``rebase.'' You
use \texttt{git\ rebase} and specify the branch you want to use as the
new base. If there are no conflicts between the experimental branch and
the branch you want to rebase onto, this process happens automatically.

If there are conflicts, Git will guide you through the conflict
resolution process for each commit from the rebase branch.

This may sound like a lot, but there's a good reason for it. You are
literally rewriting history by transferring commits from one branch to
another. To maintain the coherence of the new version history, there
should be no conflicts within the commits. So, you need to resolve them
one by one until the history is clean. It goes without saying that this
can be a fairly labor-intensive process. Therefore, you should not use
\texttt{git\ rebase} frequently.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, bottomrule=.15mm, colback=white, coltitle=black, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ rebase}}, arc=.35mm, toprule=.15mm, titlerule=0mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, colframe=quarto-callout-note-color-frame, left=2mm, bottomtitle=1mm, opacityback=0, rightrule=.15mm]

\texttt{git\ rebase} is a command used to change the base of a branch.
This means that commits from the branch are applied to a new base, which
is usually another branch. It can be used to clean up the repository
history and avoid merge conflicts.

Here is an example showing how to use \texttt{git\ rebase}:

\begin{itemize}
\item
  In this example, we initialize a new Git repository and create a new
  file. We add the file to the repository and make an initial commit.
  Then, we create a new branch called ``feature'' and switch to that
  branch. We make changes to the file in the feature branch and create a
  new commit.
\item
  Then, we switch back to the main branch and make changes to the file
  again. We add the modified file and make another commit.
\item
  To rebase the feature branch onto the main branch, we first switch to
  the feature branch and then use the \texttt{git\ rebase} command with
  the name of the main branch as an argument. This applies the commits
  from the feature branch to the main branch and changes the base of the
  feature branch.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize a new repository}
\FunctionTok{git}\NormalTok{ init}
\CommentTok{\# Create a new file}
\BuiltInTok{echo} \StringTok{"Hello World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the file to the repository}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create an initial commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Initial commit"}
\CommentTok{\# Create a new branch called "feature"}
\FunctionTok{git}\NormalTok{ branch feature}
\CommentTok{\# Switch to the "feature" branch}
\FunctionTok{git}\NormalTok{ checkout feature}
\CommentTok{\# Make changes to the file in the "feature" branch}
\BuiltInTok{echo} \StringTok{"Hello Feature World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create a new commit in the "feature" branch}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Feature commit"}
\CommentTok{\# Switch back to the "main" branch}
\FunctionTok{git}\NormalTok{ checkout main}
\CommentTok{\# Make changes to the file in the "main" branch}
\BuiltInTok{echo} \StringTok{"Hello Main World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create a new commit in the "main" branch}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Main commit"}
\CommentTok{\# Use git rebase to rebase the "feature" branch}
\CommentTok{\# onto the "main" branch}
\FunctionTok{git}\NormalTok{ checkout feature}
\FunctionTok{git}\NormalTok{ rebase main}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{exercises-10}{%
\section{Exercises}\label{exercises-10}}

In order to be able to carry out this exercise, we provide you with a
functional working environment. This can be accessed
\href{https://hub.0x3e8.de/}{here}. You can log in using your GMID. If
you do not have one, you can generate one
\href{https://id.gm.fh-koeln.de/registrierung.php}{here}. Once you have
successfully logged in to the server, you must open a terminal instance.
You are now in a position to carry out the exercise.

Alternatively, you can also carry out the exercise locally on your
computer, but then you will need to install git.

\hypertarget{create-project-folder}{%
\subsection{Create project folder}\label{create-project-folder}}

First create the \texttt{test-repo} folder via the command line and then
navigate to this folder using the corresponding command.

\hypertarget{initialize-repo}{%
\section{Initialize repo}\label{initialize-repo}}

Now initialize the repository so that the future project, which will be
saved in the \texttt{test-repo} folder, and all associated files are
versioned.

\hypertarget{do-not-upload-ignore-certain-file-types}{%
\subsection{Do not upload / ignore certain file
types}\label{do-not-upload-ignore-certain-file-types}}

In order to carry out this exercise, you must first download a file
which you then have git ignore. To do this, download the current
examination regulations for the Bachelor's degree program in Electrical
Engineering using the following command
\texttt{curl\ -o\ pruefungsordnung.pdf\ https://www.th-koeln.de/mam/downloads/deutsch/studium/studiengaenge/f07/ordnungen\_plaene/f07\_bpo\_ba\_ekb\_2021\_01\_04.pdf}.

The PDF file has been stored in the root directory of your repo and you
must now exclude it from being uploaded so that no changes to this file
are tracked. Please note that not only this one PDF file should be
ignored, but all PDF files in the repo.

\hypertarget{create-file-and-stage-it}{%
\subsection{Create file and stage it}\label{create-file-and-stage-it}}

In order to be able to commit a change later and thus make it traceable,
it must first be staged. However, as we only have a PDF file so far,
which is to be ignored by git, we cannot stage anything. Therefore, in
this task, a file \texttt{test.txt} with some string as content is to be
created and then staged.

\hypertarget{create-another-file-and-check-status}{%
\subsection{Create another file and check
status}\label{create-another-file-and-check-status}}

To understand the status function, you should create the file
\texttt{test2.txt} and then call the status function of git.

\hypertarget{commit-changes}{%
\subsection{Commit changes}\label{commit-changes}}

After the changes to the \texttt{test.txt} file have been staged and
these are now to be transferred to the project process, they must be
committed. Therefore, in this step you should perform a corresponding
commit in the current branch with the message \texttt{test-commit}.
Finally, you should also display the history of the commits.

\hypertarget{create-a-new-branch-and-switch-to-it}{%
\subsection{Create a new branch and switch to
it}\label{create-a-new-branch-and-switch-to-it}}

In this task, you are to create a new branch with the name
\texttt{change-text} in which you will later make changes. You should
then switch to this branch.

\hypertarget{commit-changes-in-the-new-branch}{%
\subsection{Commit changes in the new
branch}\label{commit-changes-in-the-new-branch}}

To be able to merge the new branch into the main branch later, you must
first make changes to the \texttt{test.txt} file. To do this, open the
file and simply change the character string in this file before saving
the changes and closing the file. Before you now commit the file, you
should reset the file to the status of the last commit for practice
purposes and thus undo the change. After you have done this, open the
file \texttt{test.txt} again and change the character string again
before saving and closing the file. This time you should commit the file
\texttt{test.txt} and then commit it with the message
\texttt{test-commit2}.

\hypertarget{merge-branch-into-main}{%
\subsection{Merge branch into main}\label{merge-branch-into-main}}

After you have committed the change to the \texttt{test.txt} file, you
should merge the \texttt{change-text} branch including the change into
the main branch so that it is also available there.

\hypertarget{resolve-merge-conflict}{%
\subsection{Resolve merge conflict}\label{resolve-merge-conflict}}

To simulate a merge conflict, you must first change the content of the
\texttt{test.txt} file before you commit the change. Then switch to the
branch \texttt{change-text} and change the file \texttt{test.txt} there
as well before you commit the change. Now you should try to merge the
branch \texttt{change-text} into the main branch and solve the problems
that occur in order to be able to perform the merge successfully.

\hypertarget{python-introduction}{%
\chapter{Python Introduction}\label{python-introduction}}

\hypertarget{recommendations}{%
\section{Recommendations}\label{recommendations}}

\href{https://wiki.python.org/moin/BeginnersGuide}{Beginner's Guide to
Python}

\hypertarget{documentation-of-the-sequential-parameter-optimization}{%
\chapter{Documentation of the Sequential Parameter
Optimization}\label{documentation-of-the-sequential-parameter-optimization}}

This document describes the \texttt{Spot} features. The official
\texttt{spotPython} documentation can be found here:
\url{https://sequential-parameter-optimization.github.io/spotPython/}.

\hypertarget{example-spot}{%
\section{Example: spot}\label{example-spot}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-objective-function-1}{%
\subsection{The Objective Function}\label{the-objective-function-1}}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[f(x) = x^2\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{a_04_spot_doc_files/figure-pdf/cell-4-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{100}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{7}\NormalTok{,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{],}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                   n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ \{\},}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{5}\NormalTok{,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{1}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\DecValTok{1}\NormalTok{,}
                                      \StringTok{"model\_optimizer"}\NormalTok{: differential\_evolution,}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{1000}\NormalTok{,}
\NormalTok{                                      \})}
\end{Highlighting}
\end{Shaded}

\texttt{spot}'s \texttt{\_\_init\_\_} method sets the control
parameters. There are two parameter groups:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  external parameters can be specified by the user
\item
  internal parameters, which are handled by \texttt{spot}.
\end{enumerate}

\hypertarget{external-parameters}{%
\subsection{External Parameters}\label{external-parameters}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
external parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
mandatory
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{fun} & object & objective function & & yes \\
\texttt{lower} & array & lower bound & & yes \\
\texttt{upper} & array & upper bound & & yes \\
\texttt{fun\_evals} & int & number of function evaluations & 15 & no \\
\texttt{fun\_evals} & int & number of function evaluations & 15 & no \\
\texttt{fun\_control} & dict & noise etc. & \{\} & n \\
\texttt{max\_time} & int & max run time budget & \texttt{inf} & no \\
\texttt{noise} & bool & if repeated evaluations of \texttt{fun} results
in different values, then \texttt{noise} should be set to \texttt{True}.
& \texttt{False} & no \\
\texttt{tolerance\_x} & float & tolerance for new x solutions. Minimum
distance of new solutions, generated by \texttt{suggest\_new\_X}, to
already existing solutions. If zero (which is the default), every new
solution is accepted. & \texttt{0} & no \\
\texttt{var\_type} & list & list of type information, can be either
\texttt{"num"} or \texttt{"factor"} & \texttt{{[}"num"{]}} & no \\
\texttt{infill\_criterion} & string & Can be \texttt{"y"}, \texttt{"s"},
\texttt{"ei"} (negative expected improvement), or \texttt{"all"} &
\texttt{"y"} & no \\
\texttt{n\_points} & int & number of infill points & 1 & no \\
\texttt{seed} & int & initial seed. If \texttt{Spot.run()} is called
twice, different results will be generated. To reproduce results, the
\texttt{seed} can be used. & \texttt{123} & no \\
\texttt{log\_level} & int & log level with the following settings:
\texttt{NOTSET} (\texttt{0}), \texttt{DEBUG} (\texttt{10}: Detailed
information, typically of interest only when diagnosing problems.),
\texttt{INFO} (\texttt{20}: Confirmation that things are working as
expected.), \texttt{WARNING} (\texttt{30}: An indication that something
unexpected happened, or indicative of some problem in the near future
(e.g.~`disk space low'). The software is still working as expected.),
\texttt{ERROR} (\texttt{40}: Due to a more serious problem, the software
has not been able to perform some function.), and \texttt{CRITICAL}
(\texttt{50}: A serious error, indicating that the program itself may be
unable to continue running.) & \texttt{50} & no \\
\texttt{show\_models} & bool & Plot model. Currently only 1-dim
functions are supported & \texttt{False} & no \\
\texttt{design} & object & experimental design & \texttt{None} & no \\
\texttt{design\_control} & dict & control parameters & see below & no \\
\texttt{surrogate} & & surrogate model & \texttt{kriging} & no \\
\texttt{surrogate\_control} & dict & control parameters & see below &
no \\
\texttt{optimizer} & object & optimizer & see below & no \\
\texttt{optimizer\_control} & dict & control parameters & see below &
no \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  Besides these single parameters, the following parameter dictionaries
  can be specified by the user:

  \begin{itemize}
  \tightlist
  \item
    \texttt{fun\_control}
  \item
    \texttt{design\_control}
  \item
    \texttt{surrogate\_control}
  \item
    \texttt{optimizer\_control}
  \end{itemize}
\end{itemize}

\hypertarget{the-fun_control-dictionary}{%
\section{\texorpdfstring{The \texttt{fun\_control}
Dictionary}{The fun\_control Dictionary}}\label{the-fun_control-dictionary}}

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
external parameter & type & description & default & mandatory \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{sigma} & float & noise: standard deviation & \texttt{0} & yes \\
\texttt{seed} & int & seed for rng & \texttt{124} & yes \\
\end{longtable}

\hypertarget{the-design_control-dictionary}{%
\section{\texorpdfstring{The \texttt{design\_control}
Dictionary}{The design\_control Dictionary}}\label{the-design_control-dictionary}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
external parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
mandatory
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{init\_size} & int & initial sample size & \texttt{10} & yes \\
\texttt{repeats} & int & number of repeats of the initial sammples &
\texttt{1} & yes \\
\end{longtable}

\hypertarget{the-surrogate_control-dictionary}{%
\section{\texorpdfstring{The \texttt{surrogate\_control}
Dictionary}{The surrogate\_control Dictionary}}\label{the-surrogate_control-dictionary}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
external parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
mandatory
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{noise} & & & & \\
\texttt{model\_optimizer} & object & optimizer &
\texttt{differential\_evolution} & no \\
\texttt{model\_fun\_evals} & & & & \\
\texttt{min\_theta} & & & \texttt{-3.} & \\
\texttt{max\_theta} & & & \texttt{3.} & \\
\texttt{n\_theta} & & & \texttt{1} & \\
\texttt{n\_p} & & & \texttt{1} & \\
\texttt{optim\_p} & & & \texttt{False} & \\
\texttt{cod\_type} & & & \texttt{"norm"} & \\
\texttt{var\_type} & & & & \\
\texttt{use\_cod\_y} & bool & & \texttt{False} & \\
\end{longtable}

\hypertarget{the-optimizer_control-dictionary}{%
\section{\texorpdfstring{The \texttt{optimizer\_control}
Dictionary}{The optimizer\_control Dictionary}}\label{the-optimizer_control-dictionary}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
external parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
mandatory
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{max\_iter} & int & max number of iterations. Note: these are the
cheap evaluations on the surrogate. & \texttt{1000} & no \\
\end{longtable}

\hypertarget{run}{%
\section{Run}\label{run}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{a_04_spot_doc_files/figure-pdf/cell-6-output-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{a_04_spot_doc_files/figure-pdf/cell-6-output-2.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 1.6142446477388548 [#########-] 85.71% 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{a_04_spot_doc_files/figure-pdf/cell-6-output-4.pdf}

}

\end{figure}

\begin{verbatim}
spotPython tuning: 0.29984480579304645 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2ed7b8c10>
\end{verbatim}

\hypertarget{print-the-results-5}{%
\section{Print the Results}\label{print-the-results-5}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.29984480579304645
x0: -0.5475808668982568
\end{verbatim}

\begin{verbatim}
[['x0', -0.5475808668982568]]
\end{verbatim}

\hypertarget{show-the-progress-2}{%
\section{Show the Progress}\label{show-the-progress-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{a_04_spot_doc_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\hypertarget{visualize-the-surrogate}{%
\section{Visualize the Surrogate}\label{visualize-the-surrogate}}

\begin{itemize}
\tightlist
\item
  The plot method of the \texttt{kriging} surrogate is used.
\item
  Note: the plot uses the interval defined by the ranges of the natural
  variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Figure size 2700x1800 with 0 Axes>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{a_04_spot_doc_files/figure-pdf/cell-9-output-2.pdf}

}

\end{figure}

\hypertarget{run-with-a-specific-start-design}{%
\section{Run With a Specific Start
Design}\label{run-with-a-specific-start-design}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_x0 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{7}\NormalTok{,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                   tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                   var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{],}
\NormalTok{                   infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                   n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                   log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{,}
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                   fun\_control }\OperatorTok{=}\NormalTok{ \{\},}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: }\DecValTok{5}\NormalTok{,}
                                   \StringTok{"repeats"}\NormalTok{: }\DecValTok{1}\NormalTok{\},}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"noise"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
                                      \StringTok{"cod\_type"}\NormalTok{: }\StringTok{"norm"}\NormalTok{,}
                                      \StringTok{"min\_theta"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{4}\NormalTok{,}
                                      \StringTok{"max\_theta"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
                                      \StringTok{"n\_theta"}\NormalTok{: }\DecValTok{2}\NormalTok{,}
                                      \StringTok{"model\_optimizer"}\NormalTok{: differential\_evolution,}
                                      \StringTok{"model\_fun\_evals"}\NormalTok{: }\DecValTok{1000}\NormalTok{,}
\NormalTok{                                      \})}
\NormalTok{spot\_x0.run(X\_start}\OperatorTok{=}\NormalTok{np.array([}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{]))}
\NormalTok{spot\_x0.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 9.869670875300805 [#########-] 85.71% 
\end{verbatim}

\begin{verbatim}
spotPython tuning: 1.5261843012598162 [##########] 100.00% Done...
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{a_04_spot_doc_files/figure-pdf/cell-10-output-3.pdf}

}

\end{figure}

\hypertarget{init-build-initial-design-1}{%
\section{Init: Build Initial Design}\label{init-build-initial-design-1}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\DecValTok{0}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 8.97647221 13.41926847]
 [ 0.66946019  1.22344228]
 [ 5.23614115 13.78185824]
 [ 5.6149825  11.5851384 ]
 [-1.72963184  1.66516096]
 [-4.26945568  7.1325531 ]
 [ 1.26363761 10.17935555]
 [ 2.88779942  8.05508969]
 [-3.39111089  4.15213772]
 [ 7.30131231  5.22275244]]
[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975
  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]
\end{verbatim}

\hypertarget{replicability}{%
\section{Replicability}\label{replicability}}

Seed

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{345}\NormalTok{)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X0, X1, X2, X3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]),
 array([[0.78373509, 0.86811887],
        [0.06692621, 0.6058029 ],
        [0.41374778, 0.00525456]]),
 array([[0.121357  , 0.69043832],
        [0.41906219, 0.32838498],
        [0.86742658, 0.52910374]]),
 array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]))
\end{verbatim}

\hypertarget{surrogates-1}{%
\section{Surrogates}\label{surrogates-1}}

\hypertarget{a-simple-predictor-1}{%
\subsection{A Simple Predictor}\label{a-simple-predictor-1}}

The code below shows how to use a simple model for prediction. Assume
that only two (very costly) measurements are available:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  f(0) = 0.5
\item
  f(2) = 2.5
\end{enumerate}

We are interested in the value at \(x_0 = 1\), i.e., \(f(x_0 = 1)\), but
cannot run an additional, third experiment.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{], [}\DecValTok{2}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{])}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ S\_lm.fit(X, y)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{]])}
\NormalTok{y0 }\OperatorTok{=}\NormalTok{ S\_lm.predict(X0)}
\BuiltInTok{print}\NormalTok{(y0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.5]
\end{verbatim}

Central Idea: Evaluation of the surrogate model \texttt{S\_lm} is much
cheaper (or / and much faster) than running the real-world experiment
\(f\).

\hypertarget{demotest-objective-function-fails}{%
\section{Demo/Test: Objective Function
Fails}\label{demotest-objective-function-fails}}

SPOT expects \texttt{np.nan} values from failed objective function
values. These are handled. Note: SPOT's counter considers only
successful executions of the objective function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\CommentTok{\# number of initial points:}
\NormalTok{ni }\OperatorTok{=} \DecValTok{20}
\CommentTok{\# number of points}
\NormalTok{n }\OperatorTok{=} \DecValTok{30}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_random\_error}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{design\_control}\OperatorTok{=}\NormalTok{\{}\StringTok{"init\_size"}\NormalTok{: ni\}}

\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{            lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{            upper}\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{            fun\_evals }\OperatorTok{=}\NormalTok{ n,}
\NormalTok{            show\_progress}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{            design\_control}\OperatorTok{=}\NormalTok{design\_control,)}
\NormalTok{spot\_1.run()}
\CommentTok{\# To check whether the run was successfully completed,}
\CommentTok{\# we compare the number of evaluated points to the specified}
\CommentTok{\# number of points.}
\ControlFlowTok{assert}\NormalTok{ spot\_1.y.shape[}\DecValTok{0}\NormalTok{] }\OperatorTok{==}\NormalTok{ n}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[ 0.53176481 -0.9053821  -0.02203599         nan  0.78240941 -0.58120945
 -0.3923345   0.67234256  0.31802454 -0.68898927 -0.75129705  0.97550354
  0.41757584  0.0786237   0.82585329  0.23700598 -0.49274073 -0.82319082
         nan  0.1481835 ]
[-1.]
\end{verbatim}

\begin{verbatim}
[0.95541987]
[0.17335968]
[-0.58552368]
\end{verbatim}

\begin{verbatim}
[-0.20126111]
[-0.60100809]
[-0.97897336]
[-0.2748985]
\end{verbatim}

\begin{verbatim}
[0.8359486]
\end{verbatim}

\begin{verbatim}
[0.99035591]
\end{verbatim}

\begin{verbatim}
[0.01641232]
\end{verbatim}

\begin{verbatim}
[0.5629346]
\end{verbatim}

\newpage{}

\hypertarget{sec-detailed-data-splitting}{%
\section{PyTorch: Detailed Description of the Data
Splitting}\label{sec-detailed-data-splitting}}

\hypertarget{description-of-the-train_hold_out-setting}{%
\subsection{\texorpdfstring{Description of the
\texttt{"train\_hold\_out"}
Setting}{Description of the "train\_hold\_out" Setting}}\label{description-of-the-train_hold_out-setting}}

The \texttt{"train\_hold\_out"} setting is used by default. It uses the
loss function specfied in \texttt{fun\_control} and the metric specified
in \texttt{fun\_control}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, the method \texttt{HyperTorch().fun\_torch} is called.
\item
  \texttt{fun\_torc()}, which is implemented in the file
  \texttt{hypertorch.py}, calls \texttt{evaluate\_hold\_out()} as
  follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval, \_ = evaluate\_hold\_out(}
\NormalTok{    model,}
\NormalTok{    train\_dataset=fun\_control["train"],}
\NormalTok{    shuffle=self.fun\_control["shuffle"],}
\NormalTok{    loss\_function=self.fun\_control["loss\_function"],}
\NormalTok{    metric=self.fun\_control["metric\_torch"],}
\NormalTok{    device=self.fun\_control["device"],}
\NormalTok{    show\_batch\_interval=self.fun\_control["show\_batch\_interval"],}
\NormalTok{    path=self.fun\_control["path"],}
\NormalTok{    task=self.fun\_control["task"],}
\NormalTok{    writer=self.fun\_control["writer"],}
\NormalTok{    writerId=config\_id,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note: Only the data set \texttt{fun\_control{[}"train"{]}} is used for
training and validation. It is used in \texttt{evaluate\_hold\_out} as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainloader, valloader = create\_train\_val\_data\_loaders(}
\NormalTok{                dataset=train\_dataset, batch\_size=batch\_size\_instance, shuffle=shuffle}
\NormalTok{            )}
\end{Highlighting}
\end{Shaded}

\texttt{create\_train\_val\_data\_loaders()} splits the
\texttt{train\_dataset} into \texttt{trainloader} and \texttt{valloader}
using \texttt{torch.utils.data.random\_split()} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{def create\_train\_val\_data\_loaders(dataset, batch\_size, shuffle, num\_workers=0):}
\NormalTok{    test\_abs = int(len(dataset) * 0.6)}
\NormalTok{    train\_subset, val\_subset = random\_split(dataset, [test\_abs, len(dataset) {-} test\_abs])}
\NormalTok{    trainloader = torch.utils.data.DataLoader(}
\NormalTok{        train\_subset, batch\_size=int(batch\_size), shuffle=shuffle, num\_workers=num\_workers}
\NormalTok{    )}
\NormalTok{    valloader = torch.utils.data.DataLoader(}
\NormalTok{        val\_subset, batch\_size=int(batch\_size), shuffle=shuffle, num\_workers=num\_workers}
\NormalTok{    )}
\NormalTok{    return trainloader, valloader}
\end{Highlighting}
\end{Shaded}

The optimizer is set up as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer\_instance = net.optimizer}
\NormalTok{lr\_mult\_instance = net.lr\_mult}
\NormalTok{sgd\_momentum\_instance = net.sgd\_momentum}
\NormalTok{optimizer = optimizer\_handler(}
\NormalTok{    optimizer\_name=optimizer\_instance,}
\NormalTok{    params=net.parameters(),}
\NormalTok{    lr\_mult=lr\_mult\_instance,}
\NormalTok{    sgd\_momentum=sgd\_momentum\_instance,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \texttt{evaluate\_hold\_out()} sets the \texttt{net} attributes such
  as \texttt{epochs}, \texttt{batch\_size}, \texttt{optimizer}, and
  \texttt{patience}. For each epoch, the methods
  \texttt{train\_one\_epoch()} and \texttt{validate\_one\_epoch()} are
  called, the former for training and the latter for validation and
  early stopping. The validation loss from the last epoch (not the best
  validation loss) is returned from \texttt{evaluate\_hold\_out}.
\item
  The method \texttt{train\_one\_epoch()} is implemented as follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{def train\_one\_epoch(}
\NormalTok{    net,}
\NormalTok{    trainloader,}
\NormalTok{    batch\_size,}
\NormalTok{    loss\_function,}
\NormalTok{    optimizer,}
\NormalTok{    device,}
\NormalTok{    show\_batch\_interval=10\_000,}
\NormalTok{    task=None,}
\NormalTok{):}
\NormalTok{    running\_loss = 0.0}
\NormalTok{    epoch\_steps = 0}
\NormalTok{    for batch\_nr, data in enumerate(trainloader, 0):}
\NormalTok{        input, target = data}
\NormalTok{        input, target = input.to(device), target.to(device)}
\NormalTok{        optimizer.zero\_grad()}
\NormalTok{        output = net(input)}
\NormalTok{        if task == "regression":}
\NormalTok{            target = target.unsqueeze(1)}
\NormalTok{            if target.shape == output.shape:}
\NormalTok{                loss = loss\_function(output, target)}
\NormalTok{            else:}
\NormalTok{                raise ValueError(f"Shapes of target and output do not match:}
\NormalTok{                 \{target.shape\} vs \{output.shape\}")}
\NormalTok{        elif task == "classification":}
\NormalTok{            loss = loss\_function(output, target)}
\NormalTok{        else:}
\NormalTok{            raise ValueError(f"Unknown task: \{task\}")}
\NormalTok{        loss.backward()}
\NormalTok{        torch.nn.utils.clip\_grad\_norm\_(net.parameters(), max\_norm=1.0)}
\NormalTok{        optimizer.step()}
\NormalTok{        running\_loss += loss.item()}
\NormalTok{        epoch\_steps += 1}
\NormalTok{        if batch\_nr \% show\_batch\_interval == (show\_batch\_interval {-} 1):  }
\NormalTok{            print(}
\NormalTok{                "Batch: \%5d. Batch Size: \%d. Training Loss (running): \%.3f"}
\NormalTok{                \% (batch\_nr + 1, int(batch\_size), running\_loss / epoch\_steps)}
\NormalTok{            )}
\NormalTok{            running\_loss = 0.0}
\NormalTok{    return loss.item()}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The method \texttt{validate\_one\_epoch()} is implemented as follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{def validate\_one\_epoch(net, valloader, loss\_function, metric, device, task):}
\NormalTok{    val\_loss = 0.0}
\NormalTok{    val\_steps = 0}
\NormalTok{    total = 0}
\NormalTok{    correct = 0}
\NormalTok{    metric.reset()}
\NormalTok{    for i, data in enumerate(valloader, 0):}
\NormalTok{        \# get batches}
\NormalTok{        with torch.no\_grad():}
\NormalTok{            input, target = data}
\NormalTok{            input, target = input.to(device), target.to(device)}
\NormalTok{            output = net(input)}
\NormalTok{            \# print(f"target: \{target\}")}
\NormalTok{            \# print(f"output: \{output\}")}
\NormalTok{            if task == "regression":}
\NormalTok{                target = target.unsqueeze(1)}
\NormalTok{                if target.shape == output.shape:}
\NormalTok{                    loss = loss\_function(output, target)}
\NormalTok{                else:}
\NormalTok{                    raise ValueError(f"Shapes of target and output }
\NormalTok{                        do not match: \{target.shape\} vs \{output.shape\}")}
\NormalTok{                metric\_value = metric.update(output, target)}
\NormalTok{            elif task == "classification":}
\NormalTok{                loss = loss\_function(output, target)}
\NormalTok{                metric\_value = metric.update(output, target)}
\NormalTok{                \_, predicted = torch.max(output.data, 1)}
\NormalTok{                total += target.size(0)}
\NormalTok{                correct += (predicted == target).sum().item()}
\NormalTok{            else:}
\NormalTok{                raise ValueError(f"Unknown task: \{task\}")}
\NormalTok{            val\_loss += loss.cpu().numpy()}
\NormalTok{            val\_steps += 1}
\NormalTok{    loss = val\_loss / val\_steps}
\NormalTok{    print(f"Loss on hold{-}out set: \{loss\}")}
\NormalTok{    if task == "classification":}
\NormalTok{        accuracy = correct / total}
\NormalTok{        print(f"Accuracy on hold{-}out set: \{accuracy\}")}
\NormalTok{    \# metric on all batches using custom accumulation}
\NormalTok{    metric\_value = metric.compute()}
\NormalTok{    metric\_name = type(metric).\_\_name\_\_}
\NormalTok{    print(f"\{metric\_name\} value on hold{-}out data: \{metric\_value\}")}
\NormalTok{    return metric\_value, loss}
\end{Highlighting}
\end{Shaded}

\hypertarget{description-of-the-test_hold_out-setting}{%
\subsubsection{\texorpdfstring{Description of the
\texttt{"test\_hold\_out"}
Setting}{Description of the "test\_hold\_out" Setting}}\label{description-of-the-test_hold_out-setting}}

It uses the loss function specfied in \texttt{fun\_control} and the
metric specified in \texttt{fun\_control}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, the method \texttt{HyperTorch().fun\_torch} is called.
\item
  \texttt{fun\_torc()} calls
  \texttt{spotPython.torch.traintest.evaluate\_hold\_out()} similar to
  the \texttt{"train\_hold\_out"} setting with one exception: It passes
  an additional \texttt{test} data set to \texttt{evaluate\_hold\_out()}
  as follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_dataset=fun\_control["test"]}
\end{Highlighting}
\end{Shaded}

\texttt{evaluate\_hold\_out()} calls
\texttt{create\_train\_test\_data\_loaders} instead of
\texttt{create\_train\_val\_data\_loaders}: The two data sets are used
in \texttt{create\_train\_test\_data\_loaders} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{def create\_train\_test\_data\_loaders(dataset, batch\_size, shuffle, test\_dataset, }
\NormalTok{        num\_workers=0):}
\NormalTok{    trainloader = torch.utils.data.DataLoader(}
\NormalTok{        dataset, batch\_size=int(batch\_size), shuffle=shuffle, }
\NormalTok{        num\_workers=num\_workers}
\NormalTok{    )}
\NormalTok{    testloader = torch.utils.data.DataLoader(}
\NormalTok{        test\_dataset, batch\_size=int(batch\_size), shuffle=shuffle, }
\NormalTok{        num\_workers=num\_workers}
\NormalTok{    )}
\NormalTok{    return trainloader, testloader}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The following steps are identical to the \texttt{"train\_hold\_out"}
  setting. Only a different data loader is used for testing.
\end{enumerate}

\hypertarget{detailed-description-of-the-train_cv-setting}{%
\subsubsection{\texorpdfstring{Detailed Description of the
\texttt{"train\_cv"}
Setting}{Detailed Description of the "train\_cv" Setting}}\label{detailed-description-of-the-train_cv-setting}}

It uses the loss function specfied in \texttt{fun\_control} and the
metric specified in \texttt{fun\_control}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, the method \texttt{HyperTorch().fun\_torch} is called.
\item
  \texttt{fun\_torc()} calls
  \texttt{spotPython.torch.traintest.evaluate\_cv()} as follows (Note:
  Only the data set \texttt{fun\_control{[}"train"{]}} is used for CV.):
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval, \_ = evaluate\_cv(}
\NormalTok{    model,}
\NormalTok{    dataset=fun\_control["train"],}
\NormalTok{    shuffle=self.fun\_control["shuffle"],}
\NormalTok{    device=self.fun\_control["device"],}
\NormalTok{    show\_batch\_interval=self.fun\_control["show\_batch\_interval"],}
\NormalTok{    task=self.fun\_control["task"],}
\NormalTok{    writer=self.fun\_control["writer"],}
\NormalTok{    writerId=config\_id,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  In `evaluate\_cv(), the following steps are performed: The optimizer
  is set up as follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer\_instance = net.optimizer}
\NormalTok{lr\_instance = net.lr}
\NormalTok{sgd\_momentum\_instance = net.sgd\_momentum}
\NormalTok{optimizer = optimizer\_handler(optimizer\_name=optimizer\_instance,}
\NormalTok{     params=net.parameters(), lr\_mult=lr\_mult\_instance)}
\end{Highlighting}
\end{Shaded}

\texttt{evaluate\_cv()} sets the \texttt{net} attributes such as
\texttt{epochs}, \texttt{batch\_size}, \texttt{optimizer}, and
\texttt{patience}. CV is implemented as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{def evaluate\_cv(}
\NormalTok{    net,}
\NormalTok{    dataset,}
\NormalTok{    shuffle=False,}
\NormalTok{    loss\_function=None,}
\NormalTok{    num\_workers=0,}
\NormalTok{    device=None,}
\NormalTok{    show\_batch\_interval=10\_000,}
\NormalTok{    metric=None,}
\NormalTok{    path=None,}
\NormalTok{    task=None,}
\NormalTok{    writer=None,}
\NormalTok{    writerId=None,}
\NormalTok{):}
\NormalTok{    lr\_mult\_instance = net.lr\_mult}
\NormalTok{    epochs\_instance = net.epochs}
\NormalTok{    batch\_size\_instance = net.batch\_size}
\NormalTok{    k\_folds\_instance = net.k\_folds}
\NormalTok{    optimizer\_instance = net.optimizer}
\NormalTok{    patience\_instance = net.patience}
\NormalTok{    sgd\_momentum\_instance = net.sgd\_momentum}
\NormalTok{    removed\_attributes, net = get\_removed\_attributes\_and\_base\_net(net)}
\NormalTok{    metric\_values = \{\}}
\NormalTok{    loss\_values = \{\}}
\NormalTok{    try:}
\NormalTok{        device = getDevice(device=device)}
\NormalTok{        if torch.cuda.is\_available():}
\NormalTok{            device = "cuda:0"}
\NormalTok{            if torch.cuda.device\_count() \textgreater{} 1:}
\NormalTok{                print("We will use", torch.cuda.device\_count(), "GPUs!")}
\NormalTok{                net = nn.DataParallel(net)}
\NormalTok{        net.to(device)}
\NormalTok{        optimizer = optimizer\_handler(}
\NormalTok{            optimizer\_name=optimizer\_instance,}
\NormalTok{            params=net.parameters(),}
\NormalTok{            lr\_mult=lr\_mult\_instance,}
\NormalTok{            sgd\_momentum=sgd\_momentum\_instance,}
\NormalTok{        )}
\NormalTok{        kfold = KFold(n\_splits=k\_folds\_instance, shuffle=shuffle)}
\NormalTok{        for fold, (train\_ids, val\_ids) in enumerate(kfold.split(dataset)):}
\NormalTok{            print(f"Fold: \{fold + 1\}")}
\NormalTok{            train\_subsampler = torch.utils.data.SubsetRandomSampler(train\_ids)}
\NormalTok{            val\_subsampler = torch.utils.data.SubsetRandomSampler(val\_ids)}
\NormalTok{            trainloader = torch.utils.data.DataLoader(}
\NormalTok{                dataset, batch\_size=batch\_size\_instance, }
\NormalTok{                sampler=train\_subsampler, num\_workers=num\_workers}
\NormalTok{            )}
\NormalTok{            valloader = torch.utils.data.DataLoader(}
\NormalTok{                dataset, batch\_size=batch\_size\_instance, }
\NormalTok{                sampler=val\_subsampler, num\_workers=num\_workers}
\NormalTok{            )}
\NormalTok{            \# each fold starts with new weights:}
\NormalTok{            reset\_weights(net)}
\NormalTok{            \# Early stopping parameters}
\NormalTok{            best\_val\_loss = float("inf")}
\NormalTok{            counter = 0}
\NormalTok{            for epoch in range(epochs\_instance):}
\NormalTok{                print(f"Epoch: \{epoch + 1\}")}
\NormalTok{                \# training loss from one epoch:}
\NormalTok{                training\_loss = train\_one\_epoch(}
\NormalTok{                    net=net,}
\NormalTok{                    trainloader=trainloader,}
\NormalTok{                    batch\_size=batch\_size\_instance,}
\NormalTok{                    loss\_function=loss\_function,}
\NormalTok{                    optimizer=optimizer,}
\NormalTok{                    device=device,}
\NormalTok{                    show\_batch\_interval=show\_batch\_interval,}
\NormalTok{                    task=task,}
\NormalTok{                )}
\NormalTok{                \# Early stopping check. Calculate validation loss from one epoch:}
\NormalTok{                metric\_values[fold], loss\_values[fold] = validate\_one\_epoch(}
\NormalTok{                    net, valloader=valloader, loss\_function=loss\_function, }
\NormalTok{                    metric=metric, device=device, task=task}
\NormalTok{                )}
\NormalTok{                \# Log the running loss averaged per batch}
\NormalTok{                metric\_name = "Metric"}
\NormalTok{                if metric is None:}
\NormalTok{                    metric\_name = type(metric).\_\_name\_\_}
\NormalTok{                    print(f"\{metric\_name\} value on hold{-}out data: }
\NormalTok{                        \{metric\_values[fold]\}")}
\NormalTok{                if writer is not None:}
\NormalTok{                    writer.add\_scalars(}
\NormalTok{                        "evaluate\_cv fold:" + str(fold + 1) + }
\NormalTok{                        ". Train \& Val Loss and Val Metric" + writerId,}
\NormalTok{                        \{"Train loss": training\_loss, "Val loss": }
\NormalTok{                        loss\_values[fold], metric\_name: metric\_values[fold]\},}
\NormalTok{                        epoch + 1,}
\NormalTok{                    )}
\NormalTok{                    writer.flush()}
\NormalTok{                if loss\_values[fold] \textless{} best\_val\_loss:}
\NormalTok{                    best\_val\_loss = loss\_values[fold]}
\NormalTok{                    counter = 0}
\NormalTok{                    \# save model:}
\NormalTok{                    if path is not None:}
\NormalTok{                        torch.save(net.state\_dict(), path)}
\NormalTok{                else:}
\NormalTok{                    counter += 1}
\NormalTok{                    if counter \textgreater{}= patience\_instance:}
\NormalTok{                        print(f"Early stopping at epoch \{epoch\}")}
\NormalTok{                        break}
\NormalTok{        df\_eval = sum(loss\_values.values()) / len(loss\_values.values())}
\NormalTok{        df\_metrics = sum(metric\_values.values()) / len(metric\_values.values())}
\NormalTok{        df\_preds = np.nan}
\NormalTok{    except Exception as err:}
\NormalTok{        print(f"Error in Net\_Core. Call to evaluate\_cv() failed. \{err=\}, }
\NormalTok{            \{type(err)=\}")}
\NormalTok{        df\_eval = np.nan}
\NormalTok{        df\_preds = np.nan}
\NormalTok{    add\_attributes(net, removed\_attributes)}
\NormalTok{    if writer is not None:}
\NormalTok{        metric\_name = "Metric"}
\NormalTok{        if metric is None:}
\NormalTok{            metric\_name = type(metric).\_\_name\_\_}
\NormalTok{        writer.add\_scalars(}
\NormalTok{            "CV: Val Loss and Val Metric" + writerId,}
\NormalTok{            \{"CV{-}loss": df\_eval, metric\_name: df\_metrics\},}
\NormalTok{            epoch + 1,}
\NormalTok{        )}
\NormalTok{        writer.flush()}
\NormalTok{    return df\_eval, df\_preds, df\_metrics}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  The method \texttt{train\_fold()} is implemented as shown above.
\item
  The method \texttt{validate\_one\_epoch()} is implemented as shown
  above. In contrast to the hold-out setting, it is called for each of
  the \(k\) folds. The results are stored in a dictionaries
  \texttt{metric\_values} and \texttt{loss\_values}. The results are
  averaged over the \(k\) folds and returned as \texttt{df\_eval}.
\end{enumerate}

\hypertarget{detailed-description-of-the-test_cv-setting}{%
\subsubsection{\texorpdfstring{Detailed Description of the
\texttt{"test\_cv"}
Setting}{Detailed Description of the "test\_cv" Setting}}\label{detailed-description-of-the-test_cv-setting}}

It uses the loss function specfied in \texttt{fun\_control} and the
metric specified in \texttt{fun\_control}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, the method \texttt{HyperTorch().fun\_torch} is called.
\item
  \texttt{fun\_torc()} calls
  \texttt{spotPython.torch.traintest.evaluate\_cv()} as follows:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval, \_ = evaluate\_cv(}
\NormalTok{    model,}
\NormalTok{    dataset=fun\_control["test"],}
\NormalTok{    shuffle=self.fun\_control["shuffle"],}
\NormalTok{    device=self.fun\_control["device"],}
\NormalTok{    show\_batch\_interval=self.fun\_control["show\_batch\_interval"],}
\NormalTok{    task=self.fun\_control["task"],}
\NormalTok{    writer=self.fun\_control["writer"],}
\NormalTok{    writerId=config\_id,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note: The data set \texttt{fun\_control{[}"test"{]}} is used for CV. The
rest is the same as for the \texttt{"train\_cv"} setting.

\hypertarget{sec-final-model-evaluation}{%
\subsubsection{Detailed Description of the Final Model Training and
Evaluation}\label{sec-final-model-evaluation}}

There are two methods that can be used for the final evaluation of a
Pytorch model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{"train\_tuned} and
\item
  \texttt{"test\_tuned"}.
\end{enumerate}

\texttt{train\_tuned()} is just a wrapper to
\texttt{evaluate\_hold\_out} using the \texttt{train} data set. It is
implemented as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{def train\_tuned(}
\NormalTok{    net,}
\NormalTok{    train\_dataset,}
\NormalTok{    shuffle,}
\NormalTok{    loss\_function,}
\NormalTok{    metric,}
\NormalTok{    device=None,}
\NormalTok{    show\_batch\_interval=10\_000,}
\NormalTok{    path=None,}
\NormalTok{    task=None,}
\NormalTok{    writer=None,}
\NormalTok{):}
\NormalTok{    evaluate\_hold\_out(}
\NormalTok{        net=net,}
\NormalTok{        train\_dataset=train\_dataset,}
\NormalTok{        shuffle=shuffle,}
\NormalTok{        test\_dataset=None,}
\NormalTok{        loss\_function=loss\_function,}
\NormalTok{        metric=metric,}
\NormalTok{        device=device,}
\NormalTok{        show\_batch\_interval=show\_batch\_interval,}
\NormalTok{        path=path,}
\NormalTok{        task=task,}
\NormalTok{        writer=writer,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

The \texttt{test\_tuned()} procedure is implemented as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{def test\_tuned(net, shuffle, test\_dataset=None, loss\_function=None,}
\NormalTok{    metric=None, device=None, path=None, task=None):}
\NormalTok{    batch\_size\_instance = net.batch\_size}
\NormalTok{    removed\_attributes, net = get\_removed\_attributes\_and\_base\_net(net)}
\NormalTok{    if path is not None:}
\NormalTok{        net.load\_state\_dict(torch.load(path))}
\NormalTok{        net.eval()}
\NormalTok{    try:}
\NormalTok{        device = getDevice(device=device)}
\NormalTok{        if torch.cuda.is\_available():}
\NormalTok{            device = "cuda:0"}
\NormalTok{            if torch.cuda.device\_count() \textgreater{} 1:}
\NormalTok{                print("We will use", torch.cuda.device\_count(), "GPUs!")}
\NormalTok{                net = nn.DataParallel(net)}
\NormalTok{        net.to(device)}
\NormalTok{        valloader = torch.utils.data.DataLoader(}
\NormalTok{            test\_dataset, batch\_size=int(batch\_size\_instance),}
\NormalTok{            shuffle=shuffle, }
\NormalTok{            num\_workers=0}
\NormalTok{        )}
\NormalTok{        metric\_value, loss = validate\_one\_epoch(}
\NormalTok{            net, valloader=valloader, loss\_function=loss\_function,}
\NormalTok{            metric=metric, device=device, task=task}
\NormalTok{        )}
\NormalTok{        df\_eval = loss}
\NormalTok{        df\_metric = metric\_value}
\NormalTok{        df\_preds = np.nan}
\NormalTok{    except Exception as err:}
\NormalTok{        print(f"Error in Net\_Core. Call to test\_tuned() failed. \{err=\}, }
\NormalTok{            \{type(err)=\}")}
\NormalTok{        df\_eval = np.nan}
\NormalTok{        df\_metric = np.nan}
\NormalTok{        df\_preds = np.nan}
\NormalTok{    add\_attributes(net, removed\_attributes)}
\NormalTok{    print(f"Final evaluation: Validation loss: \{df\_eval\}")}
\NormalTok{    print(f"Final evaluation: Validation metric: \{df\_metric\}")}
\NormalTok{    print("{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}")}
\NormalTok{    return df\_eval, df\_preds, df\_metric}
\end{Highlighting}
\end{Shaded}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bart21i}{}}%
Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf
Mersmann, eds. 2022. \emph{{Hyperparameter Tuning for Machine and Deep
Learning with R - A Practical Guide}}. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-bart23e}{}}%
Bartz-Beielstein, Thomas. 2023. {``{PyTorch} Hyperparameter Tuning with
{SPOT}: Comparison with {Ray Tuner} and Default Hyperparameters on
{CIFAR10}.''}
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb}.

\leavevmode\vadjust pre{\hypertarget{ref-Bart13j}{}}%
Bartz-Beielstein, Thomas, Jrgen Branke, Jrn Mehnen, and Olaf Mersmann.
2014. {``Evolutionary Algorithms.''} \emph{Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery} 4 (3): 178--95.

\leavevmode\vadjust pre{\hypertarget{ref-bart20gArxiv}{}}%
Bartz-Beielstein, Thomas, Carola Doerr, Jakob Bossek, Sowmya
Chandrasekaran, Tome Eftimov, Andreas Fischbach, Pascal Kerschke, et al.
2020. {``Benchmarking in Optimization: Best Practice and Open Issues.''}
arXiv. \url{https://arxiv.org/abs/2007.03488}.

\leavevmode\vadjust pre{\hypertarget{ref-BLP05}{}}%
Bartz-Beielstein, Thomas, Christian Lasarczyk, and Mike Preuss. 2005.
{``{Sequential Parameter Optimization}.''} In \emph{{Proceedings 2005
Congress on Evolutionary Computation (CEC'05), Edinburgh, Scotland}},
edited by B McKay et al., 773--80. Piscataway NJ: {IEEE Press}.

\leavevmode\vadjust pre{\hypertarget{ref-Torczon00}{}}%
Lewis, R M, V Torczon, and M W Trosset. 2000. {``{Direct search methods:
Then and now}.''} \emph{Journal of Computational and Applied
Mathematics} 124 (1--2): 191--207.

\leavevmode\vadjust pre{\hypertarget{ref-Li16a}{}}%
Li, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and
Ameet Talwalkar. 2016. {``{Hyperband: A Novel Bandit-Based Approach to
Hyperparameter Optimization}.''} \emph{arXiv e-Prints}, March,
arXiv:1603.06560.

\leavevmode\vadjust pre{\hypertarget{ref-Meignan:2015vp}{}}%
Meignan, David, Sigrid Knust, Jean-Marc Frayet, Gilles Pesant, and
Nicolas Gaud. 2015. {``{A Review and Taxonomy of Interactive
Optimization Methods in Operations Research}.''} \emph{ACM Transactions
on Interactive Intelligent Systems}, September.

\leavevmode\vadjust pre{\hypertarget{ref-mont20a}{}}%
Montiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey
Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021.
{``River: Machine Learning for Streaming Data in Python.''}

\leavevmode\vadjust pre{\hypertarget{ref-pyto23a}{}}%
PyTorch. 2023a. {``Hyperparameter Tuning with Ray Tune.''}
\url{https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html}.

\leavevmode\vadjust pre{\hypertarget{ref-pyto23b}{}}%
---------. 2023b. {``Training a Classifier.''}
\url{https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html}.

\end{CSLReferences}



\end{document}
