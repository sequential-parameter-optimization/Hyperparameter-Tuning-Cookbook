% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Hyperparameter Tuning Cookbook},
  pdfauthor={Thomas Bartz-Beielstein},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Hyperparameter Tuning Cookbook}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A guide for scikit-learn, PyTorch, river, and spotPython}
\author{Thomas Bartz-Beielstein}
\date{Jan 15, 2024}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\begin{quote}
This document provides a comprehensive guide to hyperparameter tuning
using spotPython for scikit-learn, scipy-optimize, River, and PyTorch.
The first part introduces fundamental ideas from optimization. The
second part discusses numerical issues and introduces spotPython's
surrogate model-based optimization process. The thirs part focuses on
hyperparameter tuning. Several case studies are presented, including
hyperparameter tuning for sklearn models such as Support Vector
Classification, Random Forests, Gradient Boosting (XGB), and K-nearest
neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from
river. The integration of spotPython into the PyTorch and PyTorch
Lightning training workflow is also discussed. With a hands-on approach
and step-by-step explanations, this cookbook serves as a practical
starting point for anyone interested in hyperparameter tuning with
Python. Highlights include the interplay between Tensorboard, PyTorch
Lightning, spotPython, spotRiver, and River. This publication is under
development, with updates available on the corresponding webpage.
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important: This book is still under development.}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-important-color-frame, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The most recent version of this book is available at
\url{https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/}

\end{tcolorbox}

\section*{Book Structure}\label{book-structure}
\addcontentsline{toc}{section}{Book Structure}

\markright{Book Structure}

This document is structured in three parts. The first part presents an
introduction to optimization. The second part describes numerical
methods, and the third part presents hyperparameter tuning.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Hyperparameter Tuning Reference}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The open access book Bartz et al. (2022) provides a comprehensive
  overview of hyperparameter tuning. It can be downloaded from
  \url{https://link.springer.com/book/10.1007/978-981-19-5170-1}.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The \texttt{.ipynb} notebook (Bartz-Beielstein 2023) is updated
regularly and reflects updates and changes in the \texttt{spotPython}
package. It can be downloaded from
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb}.

\end{tcolorbox}

\section*{Software Used in this Book}\label{software-used-in-this-book}
\addcontentsline{toc}{section}{Software Used in this Book}

\markright{Software Used in this Book}

\href{https://scikit-learn.org}{scikit-learn} is a Python module for
machine learning built on top of SciPy and is distributed under the
3-Clause BSD license. The project was started in 2007 by David
Cournapeau as a Google Summer of Code project, and since then many
volunteers have contributed.

\href{https://pytorch.org}{PyTorch} is an optimized tensor library for
deep learning using GPUs and CPUs.
\href{https://lightning.ai/docs/pytorch/latest/}{Lightning} is a
lightweight PyTorch wrapper for high-performance AI research. It allows
you to decouple the research from the engineering.

\href{https://riverml.xyz}{River} is a Python library for online machine
learning. It is designed to be used in real-world environments, where
not all data is available at once, but streaming in.

\href{https://github.com/sequential-parameter-optimization/spotPython}{spotPython}
(``Sequential Parameter Optimization Toolbox in Python'') is the Python
version of the well-known hyperparameter tuner SPOT, which has been
developed in the R programming environment for statistical analysis for
over a decade. The related open-access book is available here:
\href{https://link.springer.com/book/10.1007/978-981-19-5170-1}{Hyperparameter
Tuning for Machine and Deep Learning with R---A Practical Guide}.

\href{https://github.com/sequential-parameter-optimization/spotRiver}{spotRiver}
provides an interface between
\href{https://github.com/sequential-parameter-optimization/spotPython}{spotPython}
and \href{https://riverml.xyz}{River}.

\section*{Citation}\label{citation}
\addcontentsline{toc}{section}{Citation}

\markright{Citation}

If this document has been useful to you and you wish to cite it in a
scientific publication, please refer to the following paper, which can
be found on arXiv: \url{https://arxiv.org/abs/2307.10262}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{@ARTICLE\{bart23iArXiv,}
\NormalTok{      author = \{\{Bartz{-}Beielstein\}, Thomas\},}
\NormalTok{      title = "\{Hyperparameter Tuning Cookbook:}
\NormalTok{          A guide for scikit{-}learn, PyTorch, river, and spotPython\}",}
\NormalTok{     journal = \{arXiv e{-}prints\},}
\NormalTok{    keywords = \{Computer Science {-} Machine Learning,}
\NormalTok{      Computer Science {-} Artificial Intelligence, 90C26, I.2.6, G.1.6\},}
\NormalTok{         year = 2023,}
\NormalTok{        month = jul,}
\NormalTok{          eid = \{arXiv:2307.10262\},}
\NormalTok{        pages = \{arXiv:2307.10262\},}
\NormalTok{          doi = \{10.48550/arXiv.2307.10262\},}
\NormalTok{archivePrefix = \{arXiv\},}
\NormalTok{       eprint = \{2307.10262\},}
\NormalTok{ primaryClass = \{cs.LG\},}
\NormalTok{       adsurl = \{https://ui.adsabs.harvard.edu/abs/2023arXiv230710262B\},}
\NormalTok{      adsnote = \{Provided by the SAO/NASA Astrophysics Data System\}}
\NormalTok{\}}

\end{Highlighting}
\end{Shaded}

\part{Optimization}

\chapter{Introduction: Optimization}\label{introduction-optimization}

\section{Optimization, Simulation, and Surrogate
Modeling}\label{optimization-simulation-and-surrogate-modeling}

\begin{itemize}
\tightlist
\item
  We will consider the interplay between

  \begin{itemize}
  \tightlist
  \item
    mathematical models,
  \item
    numerical approximation,
  \item
    simulation,
  \item
    computer experiments, and
  \item
    field data
  \end{itemize}
\item
  Experimental design will play a key role in our developments, but not
  in the classical regression and response surface methodology sense
\item
  Challenging real-data/real-simulation examples benefiting from modern
  surrogate modeling methodology
\item
  We will consider the classical, response surface methodology (RSM)
  approach, and then move on to more modern approaches
\item
  All approaches are based on surrogates
\end{itemize}

\section{Surrogates}\label{surrogates}

\begin{itemize}
\tightlist
\item
  Gathering data is \textbf{expensive}, and sometimes getting exactly
  the data you want is impossible or unethical
\item
  \textbf{Surrogate}: substitute for the real thing
\item
  In statistics, draws from predictive equations derived from a fitted
  model can act as a surrogate for the data-generating mechanism
\item
  Benefits of the surrogate approach:

  \begin{itemize}
  \tightlist
  \item
    Surrogate could represent a cheaper way to explore relationships,
    and entertain ``what ifs?''
  \item
    Surrogates favor faithful yet pragmatic reproduction of dynamics:

    \begin{itemize}
    \tightlist
    \item
      interpretation,
    \item
      establishing causality, or
    \item
      identification
    \end{itemize}
  \item
    Many numerical simulators are \textbf{deterministic}, whereas field
    observations are noisy or have measurement error
  \end{itemize}
\end{itemize}

\subsection{Costs of Simulation}\label{costs-of-simulation}

\begin{itemize}
\tightlist
\item
  Computer simulations are generally cheaper (but not always!) than
  physical observation
\item
  Some computer simulations can be just as expensive as field
  experimentation, but computer modeling is regarded as easier because:

  \begin{itemize}
  \tightlist
  \item
    the experimental apparatus is better understood
  \item
    more aspects may be controlled.
  \end{itemize}
\end{itemize}

\subsection{Mathematical Models and
Meta-Models}\label{mathematical-models-and-meta-models}

\begin{itemize}
\tightlist
\item
  Use of mathematical models leveraging numerical solvers has been
  commonplace for some time
\item
  Mathematical models became more complex, requiring more resources to
  simulate/solve numerically
\item
  Practitioners increasingly relied on \textbf{meta-models} built off of
  limited simulation campaigns
\end{itemize}

\subsection{Surrogates = Trained
Meta-models}\label{surrogates-trained-meta-models}

\begin{itemize}
\tightlist
\item
  Data collected via expensive computer evaluations tuned flexible
  functional forms that could be used in lieu of further simulation to

  \begin{itemize}
  \tightlist
  \item
    save money or computational resources;
  \item
    cope with an inability to perform future runs (expired licenses,
    off-line or over-impacted supercomputers)
  \end{itemize}
\item
  Trained meta-models became known as \textbf{surrogates}
\end{itemize}

\subsection{Computer Experiments}\label{computer-experiments}

\begin{itemize}
\tightlist
\item
  \textbf{Computer experiment}: design, running, and fitting
  meta-models.

  \begin{itemize}
  \tightlist
  \item
    Like an ordinary statistical experiment, except the data are
    generated by computer codes rather than physical or field
    observations, or surveys
  \end{itemize}
\item
  \textbf{Surrogate modeling} is statistical modeling of computer
  experiments
\end{itemize}

\subsection{Limits of Mathematical
Modeling}\label{limits-of-mathematical-modeling}

\begin{itemize}
\tightlist
\item
  Mathematical biologists, economists and others had reached the limit
  of equilibrium-based mathematical modeling with cute closed-form
  solutions
\item
  \textbf{Stochastic simulations replace deterministic solvers} based on
  FEM, Navier--Stokes or Euler methods
\item
  Agent-based simulation models are used to explore predator-prey
  (Lotka--Voltera) dynamics, spread of disease, management of inventory
  or patients in health insurance markets
\item
  Consequence: the distinction between surrogate and statistical model
  is all but gone
\end{itemize}

\subsection{Example: Why Computer Simulations are
Necessary}\label{example-why-computer-simulations-are-necessary}

\begin{itemize}
\tightlist
\item
  You can't seed a real community with Ebola and watch what happens
\item
  If there's (real) field data, say on a historical epidemic, further
  experimentation may be almost entirely limited to the mathematical and
  computer modeling side
\item
  Classical statistical methods offer little guidance
\end{itemize}

\subsection{Simulation Requirements}\label{simulation-requirements}

\begin{itemize}
\tightlist
\item
  Simulation should

  \begin{itemize}
  \tightlist
  \item
    enable rich \textbf{diagnostics} to help criticize that models
  \item
    \textbf{understanding} its sensitivity to inputs and other
    configurations
  \item
    providing the ability to \textbf{optimize} and
  \item
    refine both \textbf{automatically} and with expert intervention
  \end{itemize}
\item
  And it has to do all that while remaining \textbf{computationally
  tractable}
\item
  One perspective is so-called \textbf{response surface methods} (RSMs):
\item
  a poster child from industrial statistics' heyday, well before
  information technology became a dominant industry
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Goals}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-important-color-frame, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  How to choose models and optimizers for solving real-world problems
\item
  How to use simulation to understand and improve processes
\end{itemize}

\end{tcolorbox}

\section{Jupyter Notebook}\label{jupyter-notebook}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/001_optimization_surrogate.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Aircraft Wing Weight
Example}\label{aircraft-wing-weight-example}

\section{AWWE Equation}\label{awwe-equation}

\begin{itemize}
\tightlist
\item
  Example from Forrester et al.~
\item
  Understand the \textbf{weight} of an unpainted light aircraft wing as
  a function of nine design and operational parameters:
\end{itemize}

\[ W = 0.036 S_W^{0.758} \times W_{fw}^{0.0035} \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times  q^{0.006}  \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49}\]

\section{AWWE Parameters and Equations (Part
1)}\label{awwe-parameters-and-equations-part-1}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1392}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5063}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1266}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1139}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1139}}@{}}
\caption{Aircraft Wing Weight Parameters}\label{tbl-awwe}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(S_W\) & Wing area (\(ft^2\)) & 174 & 150 & 200 \\
\(W_{fw}\) & Weight of fuel in wing (lb) & 252 & 220 & 300 \\
\(A\) & Aspect ratio & 7.52 & 6 & 10 \\
\(\Lambda\) & Quarter-chord sweep (deg) & 0 & -10 & 10 \\
\(q\) & Dynamic pressure at cruise (\(lb/ft^2\)) & 34 & 16 & 45 \\
\(\lambda\) & Taper ratio & 0.672 & 0.5 & 1 \\
\(R_{tc}\) & Aerofoil thickness to chord ratio & 0.12 & 0.08 & 0.18 \\
\(N_z\) & Ultimate load factor & 3.8 & 2.5 & 6 \\
\(W_{dg}\) & Flight design gross weight (lb) & 2000 & 1700 & 2500 \\
\(W_p\) & paint weight (lb/ft\^{}2) & 0.064 & 0.025 & 0.08 \\
\end{longtable}

The study begins with a baseline Cessna C172 Skyhawk Aircraft as its
reference point. It aims to investigate the impact of wing area and fuel
weight on the overall weight of the aircraft. Two crucial parameters in
this analysis are the aspect ratio (\(A\)), defined as the ratio of the
wing's length to the average chord (thickness of the airfoil), and the
taper ratio (\(\lambda\)), which represents the ratio of the maximum to
the minimum thickness of the airfoil or the maximum to minimum chord.

It's important to note that the equation used in this context is not a
computer simulation but will be treated as one for the purpose of
illustration. This approach involves employing a true mathematical
equation, even if it's considered unknown, as a useful tool for
generating realistic settings to test the methodology. The functional
form of this equation was derived by ``calibrating'' known physical
relationships to curves obtained from existing aircraft data, as
referenced in Raymer 2012. Essentially, it acts as a surrogate for
actual measurements of aircraft weight.

Examining the mathematical properties of the AWWE (Aircraft Weight With
Wing Area and Fuel Weight Equation), it is evident that the response is
highly nonlinear concerning its inputs. While it's common to apply the
logarithm to simplify equations with complex exponents, even when
modeling the logarithm, which transforms powers into slope coefficients
and products into sums, the response remains nonlinear due to the
presence of trigonometric terms. Given the combination of nonlinearity
and high input dimension, simple linear and quadratic response surface
approximations are likely to be inadequate for this analysis.

\section{Goals: Understanding and
Optimization}\label{goals-understanding-and-optimization}

The primary goals of this study revolve around understanding and
optimization:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Understanding}: One of the straightforward objectives is to
  gain a deep understanding of the input-output relationships in this
  context. Given the global perspective implied by this setting, it
  becomes evident that a more sophisticated model is almost necessary.
  At this stage, let's focus on this specific scenario to establish a
  clear understanding.
\item
  \textbf{Optimization}: Another application of this analysis could be
  optimization. There may be an interest in minimizing the weight of the
  aircraft, but it's likely that there will be constraints in place. For
  example, the presence of wings with a nonzero area is essential for
  the aircraft to be capable of flying. In situations involving
  (constrained) optimization, a global perspective and, consequently,
  the use of flexible modeling are vital.
\end{enumerate}

The provided Python code serves as a genuine computer implementation
that ``solves'' a mathematical model. It accepts arguments encoded in
the unit cube, with defaults used to represent baseline settings, as
detailed in the table labeled as Table~\ref{tbl-awwe}. To map values
from the interval \([a, b]\) to the interval \([0, 1]\), the following
formula can be employed:

\[y = f(x) = \frac{x - a}{b - a}.\]

To reverse this mapping and obtain the original values, the formula
\[g(y) = a + (b - a) y\] can be used.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ wingwt(Sw}\OperatorTok{=}\FloatTok{0.48}\NormalTok{, Wfw}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, A}\OperatorTok{=}\FloatTok{0.38}\NormalTok{, L}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, q}\OperatorTok{=}\FloatTok{0.62}\NormalTok{, l}\OperatorTok{=}\FloatTok{0.344}\NormalTok{,  Rtc}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, Nz}\OperatorTok{=}\FloatTok{0.37}\NormalTok{, Wdg}\OperatorTok{=}\FloatTok{0.38}\NormalTok{):}
    \CommentTok{\# put coded inputs back on natural scale}
\NormalTok{    Sw }\OperatorTok{=}\NormalTok{ Sw }\OperatorTok{*}\NormalTok{ (}\DecValTok{200} \OperatorTok{{-}} \DecValTok{150}\NormalTok{) }\OperatorTok{+} \DecValTok{150} 
\NormalTok{    Wfw }\OperatorTok{=}\NormalTok{ Wfw }\OperatorTok{*}\NormalTok{ (}\DecValTok{300} \OperatorTok{{-}} \DecValTok{220}\NormalTok{) }\OperatorTok{+} \DecValTok{220} 
\NormalTok{    A }\OperatorTok{=}\NormalTok{ A }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}} \DecValTok{6}\NormalTok{) }\OperatorTok{+} \DecValTok{6} 
\NormalTok{    L }\OperatorTok{=}\NormalTok{ (L }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}}\NormalTok{ (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{)) }\OperatorTok{{-}} \DecValTok{10}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.pi}\OperatorTok{/}\DecValTok{180}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ q }\OperatorTok{*}\NormalTok{ (}\DecValTok{45} \OperatorTok{{-}} \DecValTok{16}\NormalTok{) }\OperatorTok{+} \DecValTok{16} 
\NormalTok{    l }\OperatorTok{=}\NormalTok{ l }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+} \FloatTok{0.5}  
\NormalTok{    Rtc }\OperatorTok{=}\NormalTok{ Rtc }\OperatorTok{*}\NormalTok{ (}\FloatTok{0.18} \OperatorTok{{-}} \FloatTok{0.08}\NormalTok{) }\OperatorTok{+} \FloatTok{0.08}
\NormalTok{    Nz }\OperatorTok{=}\NormalTok{ Nz }\OperatorTok{*}\NormalTok{ (}\DecValTok{6} \OperatorTok{{-}} \FloatTok{2.5}\NormalTok{) }\OperatorTok{+} \FloatTok{2.5}
\NormalTok{    Wdg }\OperatorTok{=}\NormalTok{ Wdg}\OperatorTok{*}\NormalTok{(}\DecValTok{2500} \OperatorTok{{-}} \DecValTok{1700}\NormalTok{) }\OperatorTok{+} \DecValTok{1700}
    \CommentTok{\# calculation on natural scale}
\NormalTok{    W }\OperatorTok{=} \FloatTok{0.036} \OperatorTok{*}\NormalTok{ Sw}\OperatorTok{**}\FloatTok{0.758} \OperatorTok{*}\NormalTok{ Wfw}\OperatorTok{**}\FloatTok{0.0035} \OperatorTok{*}\NormalTok{ (A}\OperatorTok{/}\NormalTok{np.cos(L)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\FloatTok{0.6} \OperatorTok{*}\NormalTok{ q}\OperatorTok{**}\FloatTok{0.006} 
\NormalTok{    W }\OperatorTok{=}\NormalTok{ W }\OperatorTok{*}\NormalTok{ l}\OperatorTok{**}\FloatTok{0.04} \OperatorTok{*}\NormalTok{ (}\DecValTok{100}\OperatorTok{*}\NormalTok{Rtc}\OperatorTok{/}\NormalTok{np.cos(L))}\OperatorTok{**}\NormalTok{(}\OperatorTok{{-}}\FloatTok{0.3}\NormalTok{) }\OperatorTok{*}\NormalTok{ (Nz}\OperatorTok{*}\NormalTok{Wdg)}\OperatorTok{**}\NormalTok{(}\FloatTok{0.49}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{(W)}
\end{Highlighting}
\end{Shaded}

\section{Properties of the Python
``Solver''}\label{properties-of-the-python-solver}

The compute time required by the ``wingwt'' solver is extremely short
and can be considered trivial in terms of computational resources. The
approximation error is exceptionally small, effectively approaching
machine precision, which indicates the high accuracy of the solver's
results.

To simulate time-consuming evaluations, a deliberate delay is introduced
by incorporating a \texttt{sleep(3600)} command, which effectively
synthesizes a one-hour execution time for a particular evaluation.

Moving on to the AWWE visualization, plotting in two dimensions is
considerably simpler than dealing with nine dimensions. To aid in
creating visual representations, the code provided below establishes a
grid within the unit square to facilitate the generation of sliced
visuals. This involves generating a ``meshgrid'' as outlined in the
code.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zp }\OperatorTok{=} \BuiltInTok{zip}\NormalTok{(np.ravel(X), np.ravel(Y))}
\BuiltInTok{list}\NormalTok{(zp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[(0.0, 0.0),
 (0.5, 0.0),
 (1.0, 0.0),
 (0.0, 0.5),
 (0.5, 0.5),
 (1.0, 0.5),
 (0.0, 1.0),
 (0.5, 1.0),
 (1.0, 1.0)]
\end{verbatim}

The coding used to transform inputs from natural units is largely a
matter of taste, so long as it's easy to undo for reporting back on
original scales

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{matplotlib inline}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\CommentTok{\# plt.style.use(\textquotesingle{}seaborn{-}white\textquotesingle{})}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Plot 1: Load Factor (\(N_z\)) and Aspect Ratio
(\(A\))}{Plot 1: Load Factor (N\_z) and Aspect Ratio (A)}}\label{plot-1-load-factor-n_z-and-aspect-ratio-a}

We will vary \(N_z\) and \(A\), with other inputs fixed at their
baseline values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(A }\OperatorTok{=}\NormalTok{ X, Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{7.}\NormalTok{, }\FloatTok{5.}\NormalTok{))}
\NormalTok{plt.contourf(X, Y, z, }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Load factor (Nz) vs. Aspect Ratio (A)"}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{002_awwe_files/figure-pdf/cell-5-output-1.pdf}

Contour plots can be refined, e.g., by adding explicit contour lines as
shown in the following figure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\includegraphics{002_awwe_files/figure-pdf/cell-6-output-1.pdf}

The interpretation of the AWWE plot can be summarized as follows:

\begin{itemize}
\tightlist
\item
  The figure displays the weight response as a function of two
  variables, \(N_z\) and \(A\), using an image-contour plot.
\item
  The slight curvature observed in the contours suggests an interaction
  between these two variables.
\item
  Notably, the range of outputs depicted in the figure, spanning from
  approximately 160 to 320, nearly encompasses the entire range of
  outputs observed from various input settings within the full
  9-dimensional input space.
\item
  The plot indicates that aircraft wings tend to be heavier when the
  aspect ratios (\(A\)) are high.
\item
  This observation aligns with the idea that wings are designed to
  withstand and accommodate high gravitational forces (\(g\)-forces,
  large \(N_z\)), and there may be a compounding effect where larger
  values of \(N_z\) contribute to increased wing weight.
\item
  It's plausible that this phenomenon is related to the design
  considerations of fighter jets, which cannot have the efficient and
  lightweight glider-like wings typically found in other types of
  aircraft.
\end{itemize}

\section{Plot 2: Taper Ratio and Fuel
Weight}\label{plot-2-taper-ratio-and-fuel-weight}

\begin{itemize}
\tightlist
\item
  The same experiment for two other inputs, e.g., taper ratio
  \(\lambda\) and fuel weight \(W_{fw}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(Wfw }\OperatorTok{=}\NormalTok{ X,  Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"WfW"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"l"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\includegraphics{002_awwe_files/figure-pdf/cell-7-output-1.pdf}

\begin{itemize}
\tightlist
\item
  Interpretation of Taper Ratio (\(l\)) and Fuel Weight (\(W_{fw}\))

  \begin{itemize}
  \tightlist
  \item
    Apparently, neither input has much effect on wing weight:

    \begin{itemize}
    \tightlist
    \item
      with \(\lambda\) having a marginally greater effect, covering less
      than 4 percent of the span of weights observed in the
      \(A \times N_z\) plane
    \end{itemize}
  \item
    There's no interaction evident in \(\lambda \times W_{fw}\)
  \end{itemize}
\end{itemize}

\section{The Big Picture: Combining all
Variables}\label{the-big-picture-combining-all-variables}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pl }\OperatorTok{=}\NormalTok{ [}\StringTok{"Sw"}\NormalTok{, }\StringTok{"Wfw"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"q"}\NormalTok{, }\StringTok{"l"}\NormalTok{,  }\StringTok{"Rtc"}\NormalTok{, }\StringTok{"Nz"}\NormalTok{, }\StringTok{"Wdg"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\NormalTok{Z }\OperatorTok{=}\NormalTok{ []}
\NormalTok{Zlab }\OperatorTok{=}\NormalTok{ []}
\NormalTok{l }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(pl)}
\CommentTok{\# lc = math.comb(l,2)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(l):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, l):}
    \CommentTok{\# for j in range(l):}
        \CommentTok{\# print(pl[i], pl[j])}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ \{pl[i]: X, pl[j]: Y\}}
\NormalTok{        Z.append(wingwt(}\OperatorTok{**}\NormalTok{d))}
\NormalTok{        Zlab.append([pl[i],pl[j]])}
\end{Highlighting}
\end{Shaded}

Now we can generate all 36 combinations, e.g., our first example is
combination \texttt{p\ =\ 19}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{19}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['A', 'Nz']
\end{verbatim}

To help interpret outputs from experiments such as this one---to level
the playing field when comparing outputs from other pairs of
inputs---code below sets up a color palette that can be re-used from one
experiment to the next. We use the arguments \texttt{vmin=180} and
\texttt{vmax\ =360} to implement comparibility

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\includegraphics{002_awwe_files/figure-pdf/cell-11-output-1.pdf}

\begin{itemize}
\tightlist
\item
  Let's plot the second example, taper ratio \(\lambda\) and fuel weight
  \(W_{fw}\)
\item
  This is combination \texttt{11}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{11}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['Wfw', 'l']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\includegraphics{002_awwe_files/figure-pdf/cell-13-output-1.pdf}

\begin{itemize}
\tightlist
\item
  Using a global colormap indicates that these variables have minor
  effects on the wing weight.
\item
  Important factors can be detected by visual inspection
\item
  Plotting the Big Picture: we can plot all 36 combinations in one
  figure.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.axes\_grid1 }\ImportTok{import}\NormalTok{ ImageGrid}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{20.}\NormalTok{, }\FloatTok{20.}\NormalTok{))}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ ImageGrid(fig, }\DecValTok{111}\NormalTok{,  }\CommentTok{\# similar to subplot(111)}
\NormalTok{                 nrows\_ncols}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{),  }\CommentTok{\# creates 2x2 grid of axes}
\NormalTok{                 axes\_pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,  }\CommentTok{\# pad between axes in inch.}
\NormalTok{                 share\_all}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                 label\_mode}\OperatorTok{=}\StringTok{"0"}\NormalTok{,}
\NormalTok{                 ) }
\NormalTok{i }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ ax, im }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(grid, Z):}
    \CommentTok{\# Iterating over the grid returns the Axes.}
\NormalTok{    ax.set\_xlabel(Zlab[i][}\DecValTok{0}\NormalTok{])}
\NormalTok{    ax.set\_ylabel(Zlab[i][}\DecValTok{1}\NormalTok{])}
    \CommentTok{\# ax.set\_title(Zlab[i][1] + " vs. " + Zlab[i][0])}
\NormalTok{    ax.contourf(X, Y, im, }\DecValTok{30}\NormalTok{, cmap }\OperatorTok{=} \StringTok{"jet"}\NormalTok{,  vmin }\OperatorTok{=} \DecValTok{180}\NormalTok{, vmax }\OperatorTok{=} \DecValTok{360}\NormalTok{)}
\NormalTok{    i }\OperatorTok{=}\NormalTok{ i }\OperatorTok{+} \DecValTok{1}
       
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{002_awwe_files/figure-pdf/cell-14-output-1.pdf}

\section{AWWE Landscape}\label{awwe-landscape}

\begin{itemize}
\tightlist
\item
  Our Observations

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The load factor \(N_z\), which determines the magnitude of the
    maximum aerodynamic load on the wing, is very active and involved in
    interactions with other variables.
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Classic example: the interaction of \(N_z\) with the aspect ratio
    \(A\) indicates a heavy wing for high aspect ratios and large
    \(g\)-forces
  \item
    This is the reaon why highly manoeuvrable fighter jets cannot have
    very efficient, glider wings)
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Aspect ratio \(A\) and airfoil thickness to chord ratio \(R_{tc}\)
    have nonlinear interactions.
  \item
    Most important variables:
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Ultimate load factor \(N_z\), wing area \(S_w\), and flight design
    gross weight\(W_{dg}\).
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Little impact: dynamic pressure \(q\), taper ratio \(l\), and
    quarter-chord sweep \(L\).
  \end{enumerate}
\item
  Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Aircraft designers know that the overall weight of the aircraft and
    the wing area must be kept to a minimum
  \item
    the latter usually dictated by constraints such as required stall
    speed, landing distance, turn rate, etc.
  \end{itemize}
\end{itemize}

\section{Summary of the First
Experiments}\label{summary-of-the-first-experiments}

\begin{itemize}
\tightlist
\item
  First, we considered two pairs of inputs, out of 36 total pairs
\item
  Then, the ``Big Picture'':

  \begin{itemize}
  \tightlist
  \item
    For each pair we evaluated \texttt{wingwt} 10,000 times
  \end{itemize}
\item
  Doing the same for all pairs would require 360K evaluations:

  \begin{itemize}
  \tightlist
  \item
    not a reasonable number with a real computer simulation that takes
    any non-trivial amount of time to evaluate
  \item
    Only 1s per evaluation: \(>100\) hours
  \end{itemize}
\item
  Many solvers take minutes/hours/days to execute a single run
\item
  And: three-way interactions?
\item
  Consequence: a different strategy is needed
\end{itemize}

\section{Exercise}\label{exercise}

\subsection{Adding Paint Weight}\label{adding-paint-weight}

\begin{itemize}
\tightlist
\item
  Paint weight is not considered.
\item
  Add Paint Weight \(W_p\) to formula (the updated formula is shown
  below) and update the functions and plots in the notebook.
\end{itemize}

\[ W = 0.036S_W^{0.758} \times W_{fw}^{0.0035} \times \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times q^{0.006} \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49} + S_w W_p\]

\section{Jupyter Notebook}\label{jupyter-notebook-1}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/002_awwe.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{\texorpdfstring{Introduction to
\texttt{scipy.optimize}}{Introduction to scipy.optimize}}\label{introduction-to-scipy.optimize}

\href{https://scipy.org}{SciPy} provides algorithms for optimization,
integration, interpolation, eigenvalue problems, algebraic equations,
differential equations, statistics and many other classes of problems.
SciPy is a collection of mathematical algorithms and convenience
functions built on NumPy. It adds significant power to Python by
providing the user with high-level commands and classes for manipulating
and visualizing data.

\href{https://docs.scipy.org/doc/scipy/reference/optimize.html\#module-scipy.optimize}{SciPy
optimize} provides functions for minimizing (or maximizing) objective
functions, possibly subject to constraints. It includes solvers for
nonlinear problems (with support for both local and global optimization
algorithms), linear programing, constrained and nonlinear least-squares,
root finding, and curve fitting.

In this notebook, we will learn how to use the \texttt{scipy.optimize}
module to solve optimization problems. See:
\url{https://docs.scipy.org/doc/scipy/tutorial/optimize.html}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  This content is based on information from the scipy.optimize package.
\item
  The \texttt{scipy.optimize} package provides several commonly used
  optimization algorithms. A detailed listing is available in
  \texttt{scipy.optimize} (can also be found by
  \texttt{help(scipy.optimize)}).
\end{itemize}

\end{tcolorbox}

Common functions and objects, shared across different SciPy optimize
solvers, are shown in Table~\ref{tbl-shared-functions}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\caption{Common functions and objects, shared across different SciPy
optimize solvers}\label{tbl-shared-functions}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function or Object
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function or Object
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html\#scipy.optimize.show_options}{show\_options({[}solver,
method, disp{]})} & Show documentation for additional options of
optimization solvers. \\
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html\#scipy.optimize.OptimizeResult}{OptimizeResult}
& Represents the optimization result. \\
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeWarning.html\#scipy.optimize.OptimizeWarning}{OptimizeWarning}
& Warning issued by solvers. \\
\end{longtable}

We will introduce unconstrained minimization of multivariate scalar
functions in this chapter. The
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\#scipy.optimize.minimize}{minimize}
function provides a common interface to unconstrained and constrained
minimization algorithms for multivariate scalar functions in
\texttt{scipy.optimize}. To demonstrate the minimization function,
consider the problem of minimizing the Rosenbrock function of \emph{N}
variables:

\[
f(J) = \sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2)^2 + (1 - x_i)^2
\]

The minimum value of this function is 0, which is achieved when (x\_i =
1).

Note that the Rosenbrock function and its derivatives are included in
\texttt{scipy.optimize}. The implementations shown in the following
sections provide examples of how to define an objective function as well
as its Jacobian and Hessian functions. Objective functions in
\texttt{scipy.optimize} expect a numpy array as their first parameter,
which is to be optimized and must return a float value. The exact
calling signature must be \texttt{f(x,\ *args)}, where \texttt{x}
represents a numpy array, and \texttt{args} is a tuple of additional
arguments supplied to the objective function.

\section{Derivative-free Optimization
Algorithms}\label{derivative-free-optimization-algorithms}

Section~\ref{sec-nelder-mead-simplex-algorithm} and
Section~\ref{sec-powells-method} present two approaches that do not need
gradient information to find the minimum. They use function evaluations
to find the minimum.

\subsection{Nelder-Mead Simplex
Algorithm}\label{sec-nelder-mead-simplex-algorithm}

\texttt{method=\textquotesingle{}Nelder-Mead\textquotesingle{}}: In the
example below, the \texttt{minimize} routine is used with the
\emph{Nelder-Mead} simplex algorithm (selected through the
\texttt{method} parameter):

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\KeywordTok{def}\NormalTok{ rosen(x):}
    \CommentTok{"""The Rosenbrock function"""}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(}\FloatTok{100.0} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{, }\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 339
         Function evaluations: 571
[1. 1. 1. 1. 1.]
\end{verbatim}

The simplex algorithm is probably the simplest way to minimize a
well-behaved function. It requires only function evaluations and is a
good choice for simple minimization problems. However, because it does
not use any gradient evaluations, it may take longer to find the
minimum.

\subsection{Powell's Method}\label{sec-powells-method}

Another optimization algorithm that needs only function calls to find
the minimum is \emph{Powell}'s method, which can be selected by setting
the \texttt{method} parameter to
\texttt{\textquotesingle{}powell\textquotesingle{}} in the
\texttt{minimize} function.

To demonstrate how to supply additional arguments to an objective
function, let's consider minimizing the Rosenbrock function with an
additional scaling factor \(a\) and an offset \(b\):

\[
f(J, a, b) = \sum_{i=1}^{N-1} a (x_{i+1} - x_i^2)^2 + (1 - x_i)^2 + b
\]

You can achieve this using the \texttt{minimize} routine with the
example parameters \(a=0.5\) and \(b=1\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_with\_args(x, a, b):}
    \CommentTok{"""The Rosenbrock function with additional arguments"""}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(a }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ b}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen\_with\_args, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               args}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.}\NormalTok{), options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{, }\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 1.000000
         Iterations: 319
         Function evaluations: 525
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

As an alternative to using the \texttt{args} parameter of
\texttt{minimize}, you can wrap the objective function in a new function
that accepts only \texttt{x}. This approach is also useful when it is
necessary to pass additional parameters to the objective function as
keyword arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_with\_args(x, a, }\OperatorTok{*}\NormalTok{, b):  }\CommentTok{\# b is a keyword{-}only argument}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(a }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ b}

\KeywordTok{def}\NormalTok{ wrapped\_rosen\_without\_args(x):}
    \ControlFlowTok{return}\NormalTok{ rosen\_with\_args(x, }\FloatTok{0.5}\NormalTok{, b}\OperatorTok{=}\FloatTok{1.}\NormalTok{)  }\CommentTok{\# pass in \textasciigrave{}a\textasciigrave{} and \textasciigrave{}b\textasciigrave{}}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(wrapped\_rosen\_without\_args, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{,\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

Another alternative is to use \texttt{functools.partial}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ functools }\ImportTok{import}\NormalTok{ partial}

\NormalTok{partial\_rosen }\OperatorTok{=}\NormalTok{ partial(rosen\_with\_args, a}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, b}\OperatorTok{=}\FloatTok{1.}\NormalTok{)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(partial\_rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{,\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

\section{Gradient-based optimization
algorithms}\label{gradient-based-optimization-algorithms}

\subsection{An Introductory Example: Broyden-Fletcher-Goldfarb-Shanno
Algorithm (BFGS)}\label{sec-bfgs-intro}

This section introduces an optimization algorithm that uses gradient
information to find the minimum. The Broyden-Fletcher-Goldfarb-Shanno
(BFGS) algorithm (selected by setting
\texttt{method=\textquotesingle{}BFGS\textquotesingle{}}) is an
optimization algorithm that aims to converge quickly to the solution.
This algorithm uses the gradient of the objective function. If the
gradient is not provided by the user, it is estimated using
first-differences. The BFGS method typically requires fewer function
calls compared to the simplex algorithm, even when the gradient needs to
be estimated.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: BFGS}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

To demonstrate the BFGS algorithm, let's use the Rosenbrock function
again. The gradient of the Rosenbrock function is a vector described by
the following mathematical expression:

\begin{align}
\frac{\partial f}{\partial x_j} = \sum_{i=1}^{N} 200(x_i - x_{i-1}^2)(\delta_{i,j} - 2x_{i-1}\delta_{i-1,j}) - 2(1 - x_{i-1})\delta_{i-1,j} \\
= 200(x_j - x_{j-1}^2) - 400x_j(x_{j+1} - x_j^2) - 2(1 - x_j)
\end{align}

This expression is valid for interior derivatives, but special cases
are:

\[
\frac{\partial f}{\partial x_0} = -400x_0(x_1 - x_0^2) - 2(1 - x_0)
\]

\[
\frac{\partial f}{\partial x_{N-1}} = 200(x_{N-1} - x_{N-2}^2)
\]

Here's a Python function that computes this gradient:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_der(x):}
\NormalTok{    xm }\OperatorTok{=}\NormalTok{ x[}\DecValTok{1}\NormalTok{:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    xm\_m1 }\OperatorTok{=}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}
\NormalTok{    xm\_p1 }\OperatorTok{=}\NormalTok{ x[}\DecValTok{2}\NormalTok{:]}
\NormalTok{    der }\OperatorTok{=}\NormalTok{ np.zeros\_like(x)}
\NormalTok{    der[}\DecValTok{1}\NormalTok{:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{200}\OperatorTok{*}\NormalTok{(xm}\OperatorTok{{-}}\NormalTok{xm\_m1}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{400}\OperatorTok{*}\NormalTok{(xm\_p1 }\OperatorTok{{-}}\NormalTok{ xm}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{xm }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{xm)}
\NormalTok{    der[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{400}\OperatorTok{*}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\OperatorTok{*}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{])}
\NormalTok{    der[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{200}\OperatorTok{*}\NormalTok{(x[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{x[}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ der}
\end{Highlighting}
\end{Shaded}

You can specify this gradient information in the minimize function using
the jac parameter as illustrated below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}BFGS\textquotesingle{}}\NormalTok{, jac}\OperatorTok{=}\NormalTok{rosen\_der,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 25
         Function evaluations: 30
         Gradient evaluations: 30
[1.00000004 1.0000001  1.00000021 1.00000044 1.00000092]
\end{verbatim}

\end{tcolorbox}

\subsection{Background and Basics for Gradient-based
Optimization}\label{background-and-basics-for-gradient-based-optimization}

\subsection{Gradient}\label{gradient}

The gradient \(\nabla f(J)\) for a scalar function \(f(J)\) with \(n\)
different variables is defined by its partial derivatives:

\[\nabla f(J) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right]\]

\subsection{Jacobian Matrix}\label{jacobian-matrix}

The Jacobian matrix \(J(J)\) for a vector-valued function
\(F(J) = [f_1(J), f_2(J), \ldots, f_m(J)]\) is defined as:

\(J(J) = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \ldots & \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \ldots & \frac{\partial f_2}{\partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \ldots & \frac{\partial f_m}{\partial x_n} \end{bmatrix}\)

It consists of the first order partial derivatives and gives therefore
an overview about the gradients of a vector valued function.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Jacobian matrix}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Consider a vector-valued function
\(f : \mathbb{R}^2 \rightarrow \mathbb{R}^3\) defined as follows:
\[f(J) = \begin{bmatrix} x_1^2 + 2x_2 \\ 3x_1 - \sin(x_2) \\ e^{x_1 + x_2} \end{bmatrix}\]

Let's compute the partial derivatives and construct the Jacobian matrix:

\(\frac{\partial f_1}{\partial x_1} = 2x_1, \quad \frac{\partial f_1}{\partial x_2} = 2\)

\(\frac{\partial f_2}{\partial x_1} = 3, \quad \frac{\partial f_2}{\partial x_2} = -\cos(x_2)\)

\(\frac{\partial f_3}{\partial x_1} = e^{x_1 + x_2}, \quad \frac{\partial f_3}{\partial x_2} = e^{x_1 + x_2}\)

So, the Jacobian matrix is:

\[J(J) = \begin{bmatrix} 2x_1 & 2 \\ 3 & -\cos(x_2) \\ e^{x_1 + x_2} & e^{x_1 + x_2} \end{bmatrix}\]

This Jacobian matrix provides information about how small changes in the
input variables \(x_1\) and \(x_2\) affect the corresponding changes in
each component of the output vector.

\end{tcolorbox}

\subsection{Hessian Matrix}\label{hessian-matrix}

The Hessian matrix \(H(J)\) for a scalar function \(f(J)\) is defined
as:

\(H(J) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \ldots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}\)

So, the Hessian matrix consists of the second order dervatives of the
function. It provides information about the local curvature of the
function with respect to changes in the input variables.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Hessian matrix}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Consider a scalar-valued function:
\[f(J) = x_1^2 + 2x_2^2 + \sin(x_1   x_2)\]

The Hessian matrix of this scalar-valued function is the matrix of its
second-order partial derivatives with respect to the input variables:
\[H(J) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} \end{bmatrix}\]

Let's compute the second-order partial derivatives and construct the
Hessian matrix:

\begin{align}
\frac{\partial^2 f}{\partial x_1^2} &= 2 + \cos(x_1 x_2) x_2^2\\
\frac{\partial^2 f}{\partial x_1 \partial x_2} &= 2x_1  x_2 \cos(x_1 x_2) - \sin(x_1  x_2)\\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &= 2x_1  x_2  \cos(x_1  x_2) - \sin(x_1  x_2)\\
\frac{\partial^2 f}{\partial x_2^2} &= 4x_2^2 + \cos(x_1  x_2) x_1^2
\end{align}

So, the Hessian matrix is:

\[H(J) = \begin{bmatrix} 2 + \cos(x_1   x_2)   x_2^2 & 2x_1   x_2   \cos(x_1   x_2) - \sin(x_1   x_2) \\ 2x_1   x_2   \cos(x_1   x_2) - \sin(x_1   x_2) & 4x_2^2 + \cos(x_1   x_2)   x_1^2 \end{bmatrix}\]

\end{tcolorbox}

\subsection{Gradient for Optimization}\label{gradient-for-optimization}

In optimization, the goal is to find the minimum or maximum of a
function. Gradient-based optimization methods utilize information about
the gradient (or derivative) of the function to guide the search for the
optimal solution. This is particularly useful when dealing with complex,
high-dimensional functions where an exhaustive search is impractical.

The gradient descent method can be divided in the following steps:

\begin{itemize}
\tightlist
\item
  \textbf{Initialize:} start with an initial guess for the parameters of
  the function to be optimized.
\item
  \textbf{Compute Gradient:} Calculate the gradient (partial
  derivatives) of the function with respect to each parameter at the
  current point. The gradient indicates the direction of the steepest
  increase in the function.
\item
  \textbf{Update Parameters:} Adjust the parameters in the opposite
  direction of the gradient, scaled by a learning rate. This step aims
  to move towards the minimum of the function:

  \begin{itemize}
  \tightlist
  \item
    \(x_{k+1} = x_k - \alpha \times \nabla f(x_{k})\)
  \item
    \(x_{x}\) is current parameter vector or point in the parameter
    space.
  \item
    \(\alpha\) is the learning rate, a positive scalar that determines
    the step size in each iteration.
  \item
    \(\nabla f(x)\) is the gradient of the objective function.
  \end{itemize}
\item
  \textbf{Iterate:} Repeat the above steps until convergence or a
  predefined number of iterations. Convergence is typically determined
  when the change in the function value or parameters becomes
  negligible.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Gradient Descent}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Let's consider a simple quadratic function as an example:
\[f(x) = x^2 + 4x + y^2 + 2y + 4.\]

We'll use gradient descent to find the minimum of this function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}

\CommentTok{\# Define the quadratic function}
\KeywordTok{def}\NormalTok{ quadratic\_function(x, y):}
    \ControlFlowTok{return}\NormalTok{ x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\NormalTok{ y}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{2}\OperatorTok{*}\NormalTok{y }\OperatorTok{+} \DecValTok{4}

\CommentTok{\# Define the gradient of the quadratic function}
\KeywordTok{def}\NormalTok{ gradient\_quadratic\_function(x, y):}
\NormalTok{    grad\_x }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{x }\OperatorTok{+} \DecValTok{4}
\NormalTok{    grad\_y }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{y }\OperatorTok{+} \DecValTok{2}
    \ControlFlowTok{return}\NormalTok{ np.array([grad\_x, grad\_y])}

\CommentTok{\# Gradient Descent for optimization in 2D}
\KeywordTok{def}\NormalTok{ gradient\_descent(initial\_point, learning\_rate, num\_iterations):}
\NormalTok{    points }\OperatorTok{=}\NormalTok{ [np.array(initial\_point)]}
    
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_iterations):}
\NormalTok{        current\_point }\OperatorTok{=}\NormalTok{ points[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{        gradient }\OperatorTok{=}\NormalTok{ gradient\_quadratic\_function(}\OperatorTok{*}\NormalTok{current\_point)}
\NormalTok{        new\_point }\OperatorTok{=}\NormalTok{ current\_point }\OperatorTok{{-}}\NormalTok{ learning\_rate }\OperatorTok{*}\NormalTok{ gradient}
        
\NormalTok{        points.append(new\_point)}
        
    \ControlFlowTok{return}\NormalTok{ points}

\CommentTok{\# Visualization of optimization process with 3D surface and consistent arrow sizes}
\KeywordTok{def}\NormalTok{ plot\_optimization\_process\_3d\_consistent\_arrows(points):}
\NormalTok{    fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\NormalTok{    ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}

\NormalTok{    x\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{    y\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{    X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x\_vals, y\_vals)}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ quadratic\_function(X, Y)}

\NormalTok{    ax.plot\_surface(X, Y, Z, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}viridis\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{)}
\NormalTok{    ax.scatter(}\OperatorTok{*}\BuiltInTok{zip}\NormalTok{(}\OperatorTok{*}\NormalTok{points), [quadratic\_function(}\OperatorTok{*}\NormalTok{p) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ points], c}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Optimization Trajectory\textquotesingle{}}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(points) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):  }
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ points[i]}
\NormalTok{        dx, dy }\OperatorTok{=}\NormalTok{ points[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ points[i]}
\NormalTok{        dz }\OperatorTok{=}\NormalTok{ quadratic\_function(}\OperatorTok{*}\NormalTok{(points[i }\OperatorTok{+} \DecValTok{1}\NormalTok{])) }\OperatorTok{{-}}\NormalTok{ quadratic\_function(}\OperatorTok{*}\NormalTok{points[i])}
\NormalTok{        gradient\_length }\OperatorTok{=} \FloatTok{0.5}

\NormalTok{        ax.quiver(x, y, quadratic\_function(}\OperatorTok{*}\NormalTok{points[i]), dx, dy, dz, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, length}\OperatorTok{=}\NormalTok{gradient\_length, normalize}\OperatorTok{=}\VariableTok{False}\NormalTok{, arrow\_length\_ratio}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}

\NormalTok{    ax.set\_title(}\StringTok{\textquotesingle{}Gradient{-}Based Optimization with 2D Quadratic Function\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_zlabel(}\StringTok{\textquotesingle{}f(x, y)\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.legend()}
\NormalTok{    plt.show()}

\CommentTok{\# Initial guess and parameters}
\NormalTok{initial\_guess }\OperatorTok{=}\NormalTok{ [}\OperatorTok{{-}}\FloatTok{9.0}\NormalTok{, }\OperatorTok{{-}}\FloatTok{9.0}\NormalTok{]}
\NormalTok{learning\_rate }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{num\_iterations }\OperatorTok{=} \DecValTok{10}

\CommentTok{\# Run gradient descent in 2D and visualize the optimization process with 3D surface and consistent arrow sizes}
\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ gradient\_descent(initial\_guess, learning\_rate, num\_iterations)}
\NormalTok{plot\_optimization\_process\_3d\_consistent\_arrows(trajectory)}
\end{Highlighting}
\end{Shaded}

\includegraphics{003_scipy_optimize_intro_files/figure-pdf/cell-8-output-1.pdf}

\end{tcolorbox}

\subsection{Newton Method}\label{newton-method}

\textbf{Initialization:} Start with an initial guess for the optimal
solution: \(x_0\).

\textbf{Iteration:} Repeat the following three steps until convergence
or a predefined stopping criterion is met:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Calculate the gradient (\(\nabla\)) and the Hessian matrix
  (\(\nabla^2\)) of the objective function at the current point:
  \[\nabla f(x_k) \quad \text{and} \quad \nabla^2 f(x_k)\]
\item
  Update the current solution using the Newton-Raphson update formula
  \[x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k),\] where
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(\nabla f(x_k)\) is the gradient (first derivative) of the objective
  function with respect to the variable \(x\), evaluated at the current
  solution \(x_k\).
\item
  \(\nabla^2 f(x_k)\): The Hessian matrix (second derivative) of the
  objective function with respect to \(x\), evaluated at the current
  solution \(x_k\).
\item
  \(x_k\): The current solution or point in the optimization process.
\item
  \(\nabla^2 f(x_k)]^{-1}\): The inverse of the Hessian matrix at the
  current point, representing the approximation of the curvature of the
  objective function.
\item
  \(x_{k+1}\): The updated solution or point after applying the
  Newton-Raphson update.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Check for convergence.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Newton Method}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

We want to optimize the Rosenbrock function and use the Hessian and the
Jacobian (which is equal to the gradient vector for scalar objective
function) to the \texttt{minimize} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosenbrock(x):}
    \ControlFlowTok{return} \DecValTok{100} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{])}\OperatorTok{**}\DecValTok{2}

\KeywordTok{def}\NormalTok{ rosenbrock\_gradient(x):}
\NormalTok{    dfdx0 }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{400} \OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{])}
\NormalTok{    dfdx1 }\OperatorTok{=} \DecValTok{200} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ np.array([dfdx0, dfdx1])}

\KeywordTok{def}\NormalTok{ rosenbrock\_hessian(x):}
\NormalTok{    d2fdx0 }\OperatorTok{=} \DecValTok{1200} \OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{400} \OperatorTok{*}\NormalTok{ x[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{2}
\NormalTok{    d2fdx1 }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{400} \OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ np.array([[d2fdx0, d2fdx1], [d2fdx1, }\DecValTok{200}\NormalTok{]])}

\KeywordTok{def}\NormalTok{ classical\_newton\_optimization\_2d(initial\_guess, tol}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{, max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ initial\_guess.copy()}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(max\_iter):}
\NormalTok{        gradient }\OperatorTok{=}\NormalTok{ rosenbrock\_gradient(x)}
\NormalTok{        hessian }\OperatorTok{=}\NormalTok{ rosenbrock\_hessian(x)}

        \CommentTok{\# Solve the linear system H * d = {-}g for d}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ np.linalg.solve(hessian, }\OperatorTok{{-}}\NormalTok{gradient)}

        \CommentTok{\# Update x}
\NormalTok{        x }\OperatorTok{+=}\NormalTok{ d}

        \CommentTok{\# Check for convergence}
        \ControlFlowTok{if}\NormalTok{ np.linalg.norm(gradient, }\BuiltInTok{ord}\OperatorTok{=}\NormalTok{np.inf) }\OperatorTok{\textless{}}\NormalTok{ tol:}
            \ControlFlowTok{break}

    \ControlFlowTok{return}\NormalTok{ x}

\CommentTok{\# Initial guess}
\NormalTok{initial\_guess\_2d }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{])}

\CommentTok{\# Run classical Newton optimization for the 2D Rosenbrock function}
\NormalTok{result\_2d }\OperatorTok{=}\NormalTok{ classical\_newton\_optimization\_2d(initial\_guess\_2d)}

\CommentTok{\# Print the result}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal solution:"}\NormalTok{, result\_2d)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Objective value:"}\NormalTok{, rosenbrock(result\_2d))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimal solution: [1. 1.]
Objective value: 0.0
\end{verbatim}

\end{tcolorbox}

\subsection{BFGS-Algorithm}\label{bfgs-algorithm}

BFGS is an optimization algorithm designed for unconstrained
optimization problems. It belongs to the class of quasi-Newton methods
and is known for its efficiency in finding the minimum of a smooth,
unconstrained objective function.

\subsection{Procedure:}\label{procedure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initialization:}

  \begin{itemize}
  \tightlist
  \item
    Start with an initial guess for the parameters of the objective
    function.
  \item
    Initialize an approximation of the Hessian matrix (inverse) denoted
    by \(H\).\\
  \end{itemize}
\item
  \textbf{Iterative Update:}

  \begin{itemize}
  \tightlist
  \item
    At each iteration, compute the gradient vector at the current point.
  \item
    Update the parameters using the BFGS update formula, which involves
    the inverse Hessian matrix approximation, the gradient, and the
    difference in parameter vectors between successive iterations:
    \[x_{k+1} = x_k - H_k^{-1} \nabla f(x_k).\]
  \item
    Update the inverse Hessian approximation using the BFGS update
    formula for the inverse Hessian.
    \[H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^T}{\Delta x_k^T \Delta g_k} - \frac{H_k g_k g_k^T H_k}{g_k^T H_k g_k},\]
    where:
  \item
    \(x_k\) and \(x_{k+1}\) are the parameter vectors at the current and
    updated iterations, respectively.
  \item
    \(\nabla f(x_k)\) is the gradient vector at the current iteration.
  \item
    \(\Delta x_k = x_{k+1} - x_k\) is the change in parameter vectors.
  \item
    \(\Delta g_k = \nabla f(x_{k+1}) - \nabla f(x_k)\) is the change in
    gradient vectors.
  \end{itemize}
\item
  \textbf{Convergence:}

  \begin{itemize}
  \tightlist
  \item
    Repeat the iterative update until the optimization converges.
    Convergence is typically determined by reaching a sufficiently low
    gradient or parameter change.
  \end{itemize}
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: BFGS for Rosenbrock}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\CommentTok{\# Define the 2D Rosenbrock function}
\KeywordTok{def}\NormalTok{ rosenbrock(x):}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{])}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{100} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\CommentTok{\# Initial guess}
\NormalTok{initial\_guess }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{])}

\CommentTok{\# Minimize the Rosenbrock function using BFGS}
\NormalTok{minimize(rosenbrock, initial\_guess, method}\OperatorTok{=}\StringTok{\textquotesingle{}BFGS\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: 2.843987518235081e-11
        x: [ 1.000e+00  1.000e+00]
      nit: 19
      jac: [ 3.987e-06 -2.844e-06]
 hess_inv: [[ 4.948e-01  9.896e-01]
            [ 9.896e-01  1.984e+00]]
     nfev: 72
     njev: 24
\end{verbatim}

\end{tcolorbox}

\subsection{Visualization BFGS for
Rosenbrock}\label{visualization-bfgs-for-rosenbrock}

A visualization of the BFGS search process on Rosenbrock's function can
be found here:
\url{https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Tasks}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  In which situations is it possible to use algorithms like BFGS, but
  not the classical Newton method?
\item
  Investigate the Newton-CG method
\item
  Use an objective function of your choice and apply Newton-CG
\item
  Compare the Newton-CG method with the BFGS. What are the similarities
  and differences between the two algorithms?
\end{itemize}

\end{tcolorbox}

\section{Gradient- and Hessian-based optimization
algorithms}\label{gradient--and-hessian-based-optimization-algorithms}

Section~\ref{sec-newton-cg} presents an optimization algorithm that uses
gradient and Hessian information to find the minimum.
Section~\ref{sec-trust-region-newton} presents an optimization algorithm
that uses gradient and Hessian information to find the minimum.
Section~\ref{sec-trust-region-truncated} presents an optimization
algorithm that uses gradient and Hessian information to find the
minimum.

The methods Newton-CG, trust-ncg and trust-krylov are suitable for
dealing with large-scale problems (problems with thousands of
variables). That is because the conjugate gradient algorithm
approximately solve the trust-region subproblem (or invert the Hessian)
by iterations without the explicit Hessian factorization. Since only the
product of the Hessian with an arbitrary vector is needed, the algorithm
is specially suited for dealing with sparse Hessians, allowing low
storage requirements and significant time savings for those sparse
problems.

\subsection{Newton-Conjugate-Gradient Algorithm}\label{sec-newton-cg}

Newton-Conjugate Gradient algorithm is a modified Newton's method and
uses a conjugate gradient algorithm to (approximately) invert the local
Hessian.

\subsection{Trust-Region Newton-Conjugate-Gradient
Algorithm}\label{sec-trust-region-newton}

\subsection{Trust-Region Truncated Generalized Lanczos / Conjugate
Gradient Algorithm}\label{sec-trust-region-truncated}

\section{Global Optimization}\label{global-optimization}

Global optimization aims to find the global minimum of a function within
given bounds, in the presence of potentially many local minima.
Typically, global minimizers efficiently search the parameter space,
while using a local minimizer (e.g., minimize) under the hood. SciPy
contains a number of good global optimizers. Here, we'll use those on
the same objective function, namely the (aptly named) eggholder
function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ eggholder(x):}
    \ControlFlowTok{return}\NormalTok{ (}\OperatorTok{{-}}\NormalTok{(x[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{47}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.sin(np.sqrt(}\BuiltInTok{abs}\NormalTok{(x[}\DecValTok{0}\NormalTok{]}\OperatorTok{/}\DecValTok{2} \OperatorTok{+}\NormalTok{ (x[}\DecValTok{1}\NormalTok{]  }\OperatorTok{+} \DecValTok{47}\NormalTok{))))}
            \OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ np.sin(np.sqrt(}\BuiltInTok{abs}\NormalTok{(x[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ (x[}\DecValTok{1}\NormalTok{]  }\OperatorTok{+} \DecValTok{47}\NormalTok{)))))}

\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{512}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{512}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{513}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{513}\NormalTok{)}
\NormalTok{xgrid, ygrid }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{xy }\OperatorTok{=}\NormalTok{ np.stack([xgrid, ygrid])}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.view\_init(}\DecValTok{45}\NormalTok{, }\OperatorTok{{-}}\DecValTok{45}\NormalTok{)}
\NormalTok{ax.plot\_surface(xgrid, ygrid, eggholder(xy), cmap}\OperatorTok{=}\StringTok{\textquotesingle{}terrain\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}eggholder(x, y)\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{003_scipy_optimize_intro_files/figure-pdf/cell-12-output-1.pdf}

We now use the global optimizers to obtain the minimum and the function
value at the minimum. We'll store the results in a dictionary so we can
compare different optimization results later.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ optimize}
\NormalTok{results }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{()}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.shgo(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -935.3379515605789
    funl: [-9.353e+02]
       x: [ 4.395e+02  4.540e+02]
      xl: [[ 4.395e+02  4.540e+02]]
     nit: 1
    nfev: 45
   nlfev: 40
   nljev: 10
   nlhev: 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.dual\_annealing(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: ['Maximum number of iteration reached']
 success: True
  status: 0
     fun: -956.918231623826
       x: [ 4.824e+02  4.329e+02]
     nit: 1000
    nfev: 4157
    njev: 52
    nhev: 0
\end{verbatim}

All optimizers return an \texttt{OptimizeResult}, which in addition to
the solution contains information on the number of function evaluations,
whether the optimization was successful, and more. For brevity, we won't
show the full output of the other optimizers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.differential\_evolution(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -959.6406627208507
       x: [ 5.120e+02  4.042e+02]
     nit: 35
    nfev: 1104
     jac: [-3.386e+00  0.000e+00]
\end{verbatim}

\texttt{shgo} has a second method, which returns all local minima rather
than only what it thinks is the global minimum:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.shgo(eggholder, bounds, n}\OperatorTok{=}\DecValTok{200}\NormalTok{, iters}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                                      sampling\_method}\OperatorTok{=}\StringTok{\textquotesingle{}sobol\textquotesingle{}}\NormalTok{)}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -959.640662720831
    funl: [-9.596e+02 -9.353e+02 ... -6.591e+01 -6.387e+01]
       x: [ 5.120e+02  4.042e+02]
      xl: [[ 5.120e+02  4.042e+02]
           [ 4.395e+02  4.540e+02]
           ...
           [ 3.165e+01 -8.523e+01]
           [ 5.865e+01 -5.441e+01]]
     nit: 5
    nfev: 3529
   nlfev: 2327
   nljev: 634
   nlhev: 0
\end{verbatim}

We'll now plot all found minima on a heatmap of the function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(eggholder(xy), interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}bilinear\textquotesingle{}}\NormalTok{, origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{               cmap}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}

\KeywordTok{def}\NormalTok{ plot\_point(res, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
\NormalTok{    ax.plot(}\DecValTok{512}\OperatorTok{+}\NormalTok{res.x[}\DecValTok{0}\NormalTok{], }\DecValTok{512}\OperatorTok{+}\NormalTok{res.x[}\DecValTok{1}\NormalTok{], marker}\OperatorTok{=}\NormalTok{marker, color}\OperatorTok{=}\NormalTok{color, ms}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{)  }\CommentTok{\# differential\_evolution {-} cyan}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{)  }\CommentTok{\# dual\_annealing.        {-} white}

\CommentTok{\# SHGO produces multiple minima, plot them all (with a smaller marker size)}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{)}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl.shape[}\DecValTok{0}\NormalTok{]):}
\NormalTok{    ax.plot(}\DecValTok{512} \OperatorTok{+}\NormalTok{ results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl[i, }\DecValTok{0}\NormalTok{],}
            \DecValTok{512} \OperatorTok{+}\NormalTok{ results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl[i, }\DecValTok{1}\NormalTok{],}
            \StringTok{\textquotesingle{}ro\textquotesingle{}}\NormalTok{, ms}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax.set\_xlim([}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{514}\OperatorTok{*}\DecValTok{2}\NormalTok{])}
\NormalTok{ax.set\_ylim([}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{514}\OperatorTok{*}\DecValTok{2}\NormalTok{])}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{003_scipy_optimize_intro_files/figure-pdf/cell-17-output-1.pdf}

\subsection{Dual Annealing
Optimization}\label{dual-annealing-optimization}

This function implements the Dual Annealing optimization.

\subsection{Differential Evolution}\label{differential-evolution}

Differential Evolution is an algorithm used for finding the global
minimum of multivariate functions. It is stochastic in nature (does not
use gradient methods), and can search large areas of candidate space,
but often requires larger numbers of function evaluations than
conventional gradient based techniques.

\subsection{DIRECT}\label{direct}

DIviding RECTangles (DIRECT) is a deterministic global optimization
algorithm capable of minimizing a black box function with its variables
subject to lower and upper bound constraints by sampling potential
solutions in the search space

\subsection{SHGO}\label{shgo}

SHGO stands for ``simplicial homology global optimization''. It is
considered appropriate for solving general purpose NLP and blackbox
optimization problems to global optimality (low-dimensional problems).

\subsection{Basin-hopping}\label{basin-hopping}

Basin-hopping is a two-phase method that combines a global stepping
algorithm with local minimization at each step. Designed to mimic the
natural process of energy minimization of clusters of atoms, it works
well for similar problems with ``funnel-like, but rugged'' energy
landscapes

\section{Jupyter Notebook}\label{jupyter-notebook-2}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/003_scipy_optimize_intro.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{\texorpdfstring{Sequential Parameter Optimization: Using
\texttt{scipy}
Optimizers}{Sequential Parameter Optimization: Using scipy Optimizers}}\label{sec-scipy-optimizers}

As a default optimizer, \texttt{spotPython} uses
\texttt{differential\_evolution} from the \texttt{scipy.optimize}
package. Alternatively, any other optimizer from the
\texttt{scipy.optimize} package can be used. This chapter describes how
different optimizers from the \texttt{scipy\ optimize} package can be
used on the surrogate. The optimization algorithms are available from
\url{https://docs.scipy.org/doc/scipy/reference/optimize.html}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ dual\_annealing}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ basinhopping}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, optimizer\_control\_init, surrogate\_control\_init}
\end{Highlighting}
\end{Shaded}

\section{The Objective Function
Branin}\label{the-objective-function-branin}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula. Here we will use
the Branin function. The 2-dim Branin function is

\begin{verbatim}
$$y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * cos(x1) + s,$$ 
where values of a, b, c, r, s and t are: 
$a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10$ and $t = 1 / (8*pi)$.
\end{verbatim}

\begin{itemize}
\item
  It has three global minima:

  \(f(x) = 0.397887\) at \((-\pi, 12.275)\), \((\pi, 2.275)\), and
  \((9.42478, 2.475)\).
\item
  Input Domain: This function is usually evaluated on the square x1 in
  {[}-5, 10{]} x x2 in {[}0, 15{]}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\end{Highlighting}
\end{Shaded}

\section{The Optimizer}\label{the-optimizer}

Differential Evolution (DE) from the \texttt{scikit.optimize} package,
see
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html\#scipy.optimize.differential_evolution}
is the default optimizer for the search on the surrogate. Other
optimiers that are available in \texttt{spotPython}, see
\url{https://docs.scipy.org/doc/scipy/reference/optimize.html\#global-optimization}.

\begin{itemize}
\tightlist
\item
  \texttt{dual\_annealing}
\item
  \texttt{direct}
\item
  \texttt{shgo}
\item
  \texttt{basinhopping}
\end{itemize}

These optimizers can be selected as follows:

\texttt{surrogate\_control\ =\ "model\_optimizer":\ differential\_evolution}

As noted above, we will use \texttt{differential\_evolution}. The
optimizer can use \texttt{1000} evaluations. This value will be passed
to the \texttt{differential\_evolution} method, which has the argument
\texttt{maxiter} (int). It defines the maximum number of generations
over which the entire differential evolution population is evolved, see
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html\#scipy.optimize.differential_evolution}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Similar to the one-dimensional case, which is discussed in
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use a similar code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                    lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                    upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                    fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                    PREFIX }\OperatorTok{=} \StringTok{"04\_DE\_"}
\NormalTok{                    )}
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(}
\NormalTok{                    n\_theta}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(lower))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/04_DE__p040025_2024-01-15_00-08-02 for SummaryWriter()
\end{verbatim}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_de.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.8004644561334935 [######----] 55.00% 
spotPython tuning: 3.8004644561334935 [######----] 60.00% 
spotPython tuning: 3.1590379739505225 [######----] 65.00% 
spotPython tuning: 3.1345599589760926 [#######---] 70.00% 
spotPython tuning: 2.8987595919440583 [########--] 75.00% 
spotPython tuning: 0.4124604824941809 [########--] 80.00% 
spotPython tuning: 0.40391426855740775 [########--] 85.00% 
spotPython tuning: 0.3990718447916741 [#########-] 90.00% 
spotPython tuning: 0.3990718447916741 [##########] 95.00% 
spotPython tuning: 0.3990718447916741 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2a81951d0>
\end{verbatim}

\subsection{TensorBoard}\label{tensorboard-1}

If the \texttt{prefix} argument in \texttt{fun\_control\_init()}is not
\texttt{None} (as above, where the \texttt{prefix} was set to
\texttt{04\_DE\_}) , we can start TensorBoard in the background with the
following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/05_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}%

\section{Print the Results}\label{print-the-results}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.3990718447916741
x0: 3.149600915888656
x1: 2.2983701643039107
\end{verbatim}

\begin{verbatim}
[['x0', 3.149600915888656], ['x1', 2.2983701643039107]]
\end{verbatim}

\section{Show the Progress}\label{show-the-progress}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-7-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-8-output-1.pdf}

\section{Exercises}\label{exercises}

\subsection{\texorpdfstring{\texttt{dual\_annealing}}{dual\_annealing}}\label{dual_annealing}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm, see
  \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html}{scipy.optimize.dual\_annealing}.
\item
  Use the algorithm as an optimizer on the surrogate.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

We can run spotPython with the \texttt{dual\_annealing} optimizer as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_da }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{dual\_annealing,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_da.run()}
\NormalTok{spot\_da.print\_results()}
\NormalTok{spot\_da.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_da.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.800452934057194 [######----] 55.00% 
spotPython tuning: 3.800452934057194 [######----] 60.00% 
spotPython tuning: 3.1590242778566413 [######----] 65.00% 
spotPython tuning: 3.1341475332648105 [#######---] 70.00% 
spotPython tuning: 2.8915909597236436 [########--] 75.00% 
spotPython tuning: 0.4195069442130439 [########--] 80.00% 
spotPython tuning: 0.401848680281649 [########--] 85.00% 
spotPython tuning: 0.3992571870039132 [#########-] 90.00% 
spotPython tuning: 0.3992571870039132 [##########] 95.00% 
spotPython tuning: 0.3992571870039132 [##########] 100.00% Done...

min y: 0.3992571870039132
x0: 3.150936988317143
x1: 2.2985561477641263
\end{verbatim}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-9-output-2.pdf}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-9-output-3.pdf}

\end{tcolorbox}

\subsection{\texorpdfstring{\texttt{direct}}{direct}}\label{direct-1}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

We can run spotPython with the \texttt{direct} optimizer as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_di }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{direct,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_di.run()}
\NormalTok{spot\_di.print\_results()}
\NormalTok{spot\_di.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_di.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.812970247994418 [######----] 55.00% 
spotPython tuning: 3.812970247994418 [######----] 60.00% 
spotPython tuning: 3.162514679816068 [######----] 65.00% 
spotPython tuning: 3.1189615135325983 [#######---] 70.00% 
spotPython tuning: 2.6597698275013 [########--] 75.00% 
spotPython tuning: 0.3984917773445744 [########--] 80.00% 
spotPython tuning: 0.3984917773445744 [########--] 85.00% 
spotPython tuning: 0.3984917773445744 [#########-] 90.00% 
spotPython tuning: 0.3984917773445744 [##########] 95.00% 
spotPython tuning: 0.3984917773445744 [##########] 100.00% Done...

min y: 0.3984917773445744
x0: 3.1378600823045257
x1: 2.3010973936899863
\end{verbatim}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-10-output-2.pdf}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-10-output-3.pdf}

\end{tcolorbox}

\subsection{\texorpdfstring{\texttt{shgo}}{shgo}}\label{shgo-1}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

We can run spotPython with the \texttt{direct} optimizer as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_sh }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{shgo,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_sh.run()}
\NormalTok{spot\_sh.print\_results()}
\NormalTok{spot\_sh.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_sh.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.8004552384813834 [######----] 55.00% 
spotPython tuning: 3.8004552384813834 [######----] 60.00% 
spotPython tuning: 3.1590504084857294 [######----] 65.00% 
spotPython tuning: 3.1341080537914 [#######---] 70.00% 
spotPython tuning: 2.8853849830561646 [########--] 75.00% 
spotPython tuning: 0.4239413355798014 [########--] 80.00% 
spotPython tuning: 0.4016765366794104 [########--] 85.00% 
spotPython tuning: 0.3993233052368623 [#########-] 90.00% 
spotPython tuning: 0.3993233052368623 [##########] 95.00% 
spotPython tuning: 0.3993233052368623 [##########] 100.00% Done...

min y: 0.3993233052368623
x0: 3.1514683455618786
x1: 2.2984189502295536
\end{verbatim}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-11-output-2.pdf}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-11-output-3.pdf}

\end{tcolorbox}

\subsection{\texorpdfstring{\texttt{basinhopping}}{basinhopping}}\label{basinhopping}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

We can run spotPython with the \texttt{direct} optimizer as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_bh }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{basinhopping,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_bh.run()}
\NormalTok{spot\_bh.print\_results()}
\NormalTok{spot\_bh.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_bh.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.80045375093536 [######----] 55.00% 
spotPython tuning: 3.80045375093536 [######----] 60.00% 
spotPython tuning: 3.159009257538889 [######----] 65.00% 
spotPython tuning: 3.1341512916720102 [#######---] 70.00% 
spotPython tuning: 2.8796407604155867 [########--] 75.00% 
spotPython tuning: 0.414633458827506 [########--] 80.00% 
spotPython tuning: 0.40117926479755717 [########--] 85.00% 
spotPython tuning: 0.3993792812618935 [#########-] 90.00% 
spotPython tuning: 0.3993792812618935 [##########] 95.00% 
min y: 0.3993792812618935
x0: 3.150848989866496
x1: 2.3006645011798197
\end{verbatim}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-12-output-2.pdf}

\includegraphics{004_spot_sklearn_optimization_files/figure-pdf/cell-12-output-3.pdf}

\end{tcolorbox}

\subsection{Performance Comparison}\label{performance-comparison}

Compare the performance and run time of the 5 different optimizers:

\begin{itemize}
\tightlist
\item
  \texttt{differential\_evolution}
\item
  \texttt{dual\_annealing}
\item
  \texttt{direct}
\item
  \texttt{shgo}
\item
  \texttt{basinhopping}.
\end{itemize}

The Branin function has three global minima:

\begin{itemize}
\tightlist
\item
  \(f(x) = 0.397887\) at

  \begin{itemize}
  \tightlist
  \item
    \((-\pi, 12.275)\),
  \item
    \((\pi, 2.275)\), and
  \item
    \((9.42478, 2.475)\).\\
  \end{itemize}
\item
  Which optima are found by the optimizers?
\item
  Does the \texttt{seed} argument in
  \texttt{fun\ =\ analytical(seed=123).fun\_branin} change this
  behavior?
\end{itemize}

\section{Jupyter Notebook}\label{jupyter-notebook-3}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/004_spot_sklearn_optimization.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\part{Numerical Methods}

\chapter{Introduction: Numerical
Methods}\label{introduction-numerical-methods}

This part deals with numerical implementations of optimization methods.
The goal is to understand the implementation of optimization methods and
to solve real-world problems numerically and efficiently. We will focus
on the implementation of surrogate models, because they are the most
efficient way to solve real-world problems.

Starting point is the well-established response surface methodology. It
will be extended to the design and analysis of computer experiments
(DACE). The DACE methodology is a modern extension of the response
surface methodology. It is based on the use of surrogate models, which
are used to replace the real-world problem with a simpler problem. The
simpler problem is then solved numerically. The solution of the simpler
problem is then used to solve the real-world problem.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Numerical methods: Goals}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-important-color-frame, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  Understand implementation of optimization methods
\item
  Solve real-world problems numerically and efficiently
\end{itemize}

\end{tcolorbox}

\section{Response Surface Methods: What is
RSM?}\label{response-surface-methods-what-is-rsm}

Response Surface Methods (RSM) refer to a collection of statistical and
mathematical tools that are valuable for developing, improving, and
optimizing processes. The overarching theme of RSM involves studying how
input variables that control a product or process can potentially
influence a response that measures performance or quality
characteristics.

The advantages of RSM include a rich literature, well-established
methods often used in manufacturing, the importance of careful
experimental design combined with a well-understood model, and the
potential to add significant value to scientific inquiry, process
refinement, optimization, and more. However, there are also drawbacks to
RSM, such as the use of simple and crude surrogates, the hands-on nature
of the methods, and the limitation of local methods.

RSM is related to various fields, including Design of Experiments (DoE),
quality management, reliability, and productivity. Its applications are
widespread in industry and manufacturing, focusing on designing,
developing, and formulating new products and improving existing ones, as
well as from laboratory research. RSM is commonly applied in domains
such as materials science, manufacturing, applied chemistry, climate
science, and many others.

An example of RSM involves studying the relationship between a response
variable, such as yield (\(y\)) in a chemical process, and two process
variables: reaction time (\(\xi_1\)) and reaction temperature
(\(\xi_2\)). The provided code illustrates this scenario, following a
variation of the so-called ``banana function.''

In the context of visualization, RSM offers the choice between 3D plots
and contour plots. In a 3D plot, the independent variables \(\xi_1\) and
\(\xi_2\) are represented, with \(y\) as the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\KeywordTok{def}\NormalTok{ fun\_rosen(x1, x2):}
\NormalTok{    b }\OperatorTok{=} \DecValTok{10}
    \ControlFlowTok{return}\NormalTok{ (x1}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ b}\OperatorTok{*}\NormalTok{(x2}\OperatorTok{{-}}\NormalTok{x1}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zs }\OperatorTok{=}\NormalTok{ np.array(fun\_rosen(np.ravel(X), np.ravel(Y)))}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ zs.reshape(X.shape)}

\NormalTok{ax.plot\_surface(X, Y, Z)}

\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}X1\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}X2\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{005_num_rsm_files/figure-pdf/cell-2-output-1.pdf}

\begin{itemize}
\tightlist
\item
  contour plot example:

  \begin{itemize}
  \tightlist
  \item
    \(x_1\) and \(x_2\) are the independent variables
  \item
    \(y\) is the dependent variable
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_rosen(X1, X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y , }\DecValTok{50}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Rosenbrock\textquotesingle{}s Banana Function"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, "Rosenbrock's Banana Function")
\end{verbatim}

\includegraphics{005_num_rsm_files/figure-pdf/cell-3-output-2.pdf}

\begin{itemize}
\tightlist
\item
  Visual inspection: yield is optimized near \((\xi_1. \xi_2)\)
\end{itemize}

\subsection{Visualization: Problems in
Practice}\label{visualization-problems-in-practice}

\begin{itemize}
\tightlist
\item
  True response surface is unknown in practice
\item
  When yield evaluation is not as simple as a toy banana function, but a
  process requiring care to monitor, reconfigure and run, it's far too
  expensive to observe over a dense grid
\item
  And, measuring yield may be a noisy/inexact process
\item
  That's where stats (RSM) comes in
\end{itemize}

\subsection{RSM: Strategies}\label{rsm-strategies}

\begin{itemize}
\item
  RSMs consist of experimental strategies for
\item
  \textbf{exploring} the space of the process (i.e., independent/input)
  variables (above \(\xi_1\) and \(\xi2)\)
\item
  empirical statistical \textbf{modeling} targeted toward development of
  an appropriate approximating relationship between the response (yield)
  and process variables local to a study region of interest
\item
  \textbf{optimization} methods for sequential refinement in search of
  the levels or values of process variables that produce desirable
  responses (e.g., that maximize yield or explain variation)
\item
  RSM used for fitting an Empirical Model
\item
  True response surface driven by an unknown physical mechanism
\item
  Observations corrupted by noise
\item
  Helpful: fit an empirical model to output collected under different
  process configurations
\item
  Consider response \(Y\) that depends on controllable input variables
  \(\xi_1, \xi_2, \ldots, \xi_m\)
\item
  RSM: Equations of the Empirical Model

  \begin{itemize}
  \tightlist
  \item
    \(Y=f(\xi_1, \xi_2, \ldots, \xi_m) + \epsilon\)
  \item
    \(\mathbb{E}\{Y\} = \eta = f(\xi1_1, \xi_2, \ldots, \xi_m)\)
  \item
    \(\epsilon\) is treated as zero mean idiosyncratic noise possibly
    representing

    \begin{itemize}
    \tightlist
    \item
      inherent variation, or
    \item
      the effect of other systems or
    \item
      variables not under our purview at this time
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{RSM: Noise in the Empirical
Model}\label{rsm-noise-in-the-empirical-model}

\begin{itemize}
\tightlist
\item
  Typical simplifying assumption: \(\epsilon \sim N(0,\sigma^2)\)
\item
  We seek estimates for \(f\) and \(\sigma^2\) from noisy observations
  \(Y\) at inputs \(\xi\)
\end{itemize}

\subsection{RSM: Natural and Coded
Variables}\label{rsm-natural-and-coded-variables}

\begin{itemize}
\tightlist
\item
  Inputs \(\xi_1, \xi_2, \ldots, \xi_m\) called \textbf{natural
  variables}:

  \begin{itemize}
  \tightlist
  \item
    expressed in natural units of measurement, e.g., degrees Celsius,
    pounds per square inch (psi), etc.
  \end{itemize}
\item
  Transformed to \textbf{coded variables} \(x_1, x_2, \ldots, x_m\):

  \begin{itemize}
  \tightlist
  \item
    to mitigate hassles and confusion that can arise when working with a
    multitude of scales of measurement
  \end{itemize}
\item
  Typical \textbf{Transformations} offering dimensionless inputs
  \(x_1, x_2, \ldots, x_m\)

  \begin{itemize}
  \tightlist
  \item
    in the unit cube, or
  \item
    scaled to have a mean of zero and standard deviation of one, are
    common choices.
  \end{itemize}
\item
  Empirical model becomes \(\eta = f(x_1, x_2, \ldots, x_m)\)
\end{itemize}

\subsection{RSM Low-order Polynomials}\label{rsm-low-order-polynomials}

\begin{itemize}
\tightlist
\item
  Low-order polynomial make the following simplifying Assumptions

  \begin{itemize}
  \tightlist
  \item
    Learning about \(f\) is lots easier if we make some simplifying
    approximations
  \item
    Appealing to \textbf{Taylor's theorem}, a low-order polynomial in a
    small, localized region of the input (\(x\)) space is one way
    forward
  \item
    Classical RSM:

    \begin{itemize}
    \tightlist
    \item
      disciplined application of \textbf{local analysis} and
    \item
      \textbf{sequential refinement} of locality through conservative
      extrapolation
    \end{itemize}
  \item
    Inherently a \textbf{hands-on process}
  \end{itemize}
\end{itemize}

\section{First-Order Models (Main Effects
Model)}\label{first-order-models-main-effects-model}

\begin{itemize}
\tightlist
\item
  \textbf{First-order model} (sometimes called main effects model)
  useful in parts of the input space where it's believed that there's
  little curvature in \(f\):
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \]
\item
  For example: \[\eta = 50 + 8 x_1 + 3x_2\]
\item
  In practice, such a surface would be obtained by fitting a model to
  the outcome of a designed experiment
\item
  First-Order Model in python Evaluated on a Grid
\item
  Evaluate model on a grid in a double-unit square centered at the
  origin
\item
  Coded units are chosen arbitrarily, although one can imagine deploying
  this approximating function nearby \(x^{(0)} = (0,0)\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_1(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_1(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model: $50 + 8x\_1 + 3x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model: $50 + 8x_1 + 3x_2$')
\end{verbatim}

\includegraphics{005_num_rsm_files/figure-pdf/cell-5-output-2.pdf}

\subsection{First-Order Model
Properties}\label{first-order-model-properties}

\begin{itemize}
\tightlist
\item
  First-order model in 2d traces out a \textbf{plane} in
  \(y \times (x_1, x_2)\) space
\item
  Only be appropriate for the most trivial of response surfaces, even
  when applied in a highly localized part of the input space
\item
  Adding \textbf{curvature} is key to most applications:

  \begin{itemize}
  \tightlist
  \item
    First-order model with \textbf{interactions} induces limited degree
    of curvature via different rates of change of \(y\) as \(x_1\) is
    varied for fixed \(x_2\), and vice versa:
    \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_{12} \]
  \end{itemize}
\item
  For example \(\eta = 50+8x_1+3x_2-4x_1x_2\)
\end{itemize}

\subsection{First-order Model with Interactions in
python}\label{first-order-model-with-interactions-in-python}

\begin{itemize}
\tightlist
\item
  Code below facilitates evaluations for pairs \((x_1, x_2)\)
\item
  Responses may be observed over a mesh in the same double-unit square
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_11(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_11(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model with Interactions\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model with Interactions')
\end{verbatim}

\includegraphics{005_num_rsm_files/figure-pdf/cell-7-output-2.pdf}

\subsection{Observations: First-Order Model with
Interactions}\label{observations-first-order-model-with-interactions}

\begin{itemize}
\tightlist
\item
  Mean response \(\eta\) is increasing marginally in both \(x_1\) and
  \(x_2\), or conditional on a fixed value of the other until \(x_1\) is
  0.75
\item
  Rate of increase slows as both coordinates grow simultaneously since
  the coefficient in front of the interaction term \(x_1 x_2\) is
  negative
\item
  Compared to the first-order model (without interactions): surface is
  far more useful locally
\item
  Least squares regressions often flag up significant interactions when
  fit to data collected on a design far from local optima
\end{itemize}

\section{Second-Order Models}\label{second-order-models}

\begin{itemize}
\item
  Second-order model may be appropriate near local optima where \(f\)
  would have substantial curvature:
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \beta_{11}x_1^2 + \beta_{22}x^2 + \beta_{12} x_1 x_2\]
\item
  For example \[\eta = 50 + 8 x_1 + 3x_2 - 7x_1^2 - 3 x_2^2 - 4x_1x_2\]
\item
  Implementation of the Second-Order Model as \texttt{fun\_2()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_2(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{7} \OperatorTok{*}\NormalTok{ x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_2(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Second Order Model with Interactions. Maximum near about $(0.6,0.2)$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')
\end{verbatim}

\includegraphics{005_num_rsm_files/figure-pdf/cell-9-output-2.pdf}

\subsection{Second-Order Models:
Properties}\label{second-order-models-properties}

\begin{itemize}
\tightlist
\item
  Not all second-order models would have a single stationary point (in
  RSM jargon called ``a simple maximum'')
\item
  In ``yield maximizing'' setting we're presuming response surface is
  \textbf{concave} down from a global viewpoint

  \begin{itemize}
  \tightlist
  \item
    even though local dynamics may be more nuanced
  \end{itemize}
\item
  Exact criteria depend upon the eigenvalues of a certain matrix built
  from those coefficients
\item
  Box and Draper (2007) provide a diagram categorizing all of the kinds
  of second-order surfaces in RSM analysis, where finding local maxima
  is the goal
\end{itemize}

\subsection{Example: Stationary Ridge}\label{example-stationary-ridge}

\begin{itemize}
\tightlist
\item
  Example set of coefficients describing what's called a
  \textbf{stationary ridge} is provided by the code below
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Example of a stationary ridge\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Example of a stationary ridge')
\end{verbatim}

\includegraphics{005_num_rsm_files/figure-pdf/cell-11-output-2.pdf}

\subsection{Observations: Second-Order Model
(Ridge)}\label{observations-second-order-model-ridge}

\begin{itemize}
\tightlist
\item
  \textbf{Ridge}: a whole line of stationary points corresponding to
  maxima
\item
  Situation means that the practitioner has some flexibility when it
  comes to optimizing:

  \begin{itemize}
  \tightlist
  \item
    can choose the precise setting of \((x_1, x_2)\) either arbitrarily
    or (more commonly) by consulting some tertiary criteria
  \end{itemize}
\end{itemize}

\subsection{Example: Rising Ridge}\label{example-rising-ridge}

\begin{itemize}
\tightlist
\item
  An example of a rising ridge is implemented by the code below.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge\_rise(x1, x2):}
     \ControlFlowTok{return} \DecValTok{80} \OperatorTok{{-}} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{12}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge\_rise(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Rising ridge: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 3x\_1\^{}2 {-} 12x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Rising ridge: $\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')
\end{verbatim}

\includegraphics{005_num_rsm_files/figure-pdf/cell-13-output-2.pdf}

\subsection{Summary: Rising Ridge}\label{summary-rising-ridge}

\begin{itemize}
\tightlist
\item
  The stationary point is remote to the study region
\item
  Ccontinuum of (local) stationary points along any line going through
  the 2d space, excepting one that lies directly on the ridge
\item
  Although estimated response will increase while moving along the axis
  of symmetry toward its stationary point, this situation indicates

  \begin{itemize}
  \tightlist
  \item
    either a poor fit by the approximating second-order function, or
  \item
    that the study region is not yet precisely in the vicinity of a
    local optima---often both.
  \end{itemize}
\end{itemize}

\subsection{Falling Ridge}\label{falling-ridge}

\begin{itemize}
\tightlist
\item
  Inversion of a rising ridge is a falling ridge
\item
  Similarly indicating one is far from local optima, except that the
  response decreases as you move toward the stationary point
\item
  Finding a falling ridge system can be a back-to-the-drawing-board
  affair.
\end{itemize}

\subsection{Saddle Point}\label{saddle-point}

\begin{itemize}
\tightlist
\item
  Finally, we can get what's called a saddle or minimax system.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_saddle(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_saddle(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Saddle Point: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 2x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Saddle Point: $\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')
\end{verbatim}

\includegraphics{005_num_rsm_files/figure-pdf/cell-15-output-2.pdf}

\subsection{Interpretation: Saddle
Points}\label{interpretation-saddle-points}

\begin{itemize}
\tightlist
\item
  Likely further data collection, and/or outside expertise, is needed
  before determining a course of action in this situation
\end{itemize}

\subsection{Summary: Ridge Analysis}\label{summary-ridge-analysis}

\begin{itemize}
\tightlist
\item
  Finding a simple maximum, or stationary ridge, represents ideals in
  the spectrum of second-order approximating functions
\item
  But getting there can be a bit of a slog
\item
  Using models fitted from data means uncertainty due to noise, and
  therefore uncertainty in the type of fitted second-order model
\item
  A ridge analysis attempts to offer a principled approach to navigating
  uncertainties when one is seeking local maxima
\item
  The two-dimensional setting exemplified above is convenient for
  visualization, but rare in practice
\item
  Complications compound when studying the effect of more than two
  process variables
\end{itemize}

\section{General RSM Models}\label{general-rsm-models}

\begin{itemize}
\tightlist
\item
  General \textbf{first-order model} on \(m\) process variables
  \(x_1, x_2, \cdots, x_m\) is
  \[\eta = \beta_0 + \beta_1x_1 + \cdots + \beta_m x_m\]
\item
  General \textbf{second-order model} on \(m\) process variables \[
  \eta= \beta_0 + \sum_{j=1}^m + \sum_{j=1}^m x_j^2 + \sum_{j=2}^m \sum_{k=1}^j \beta_{kj}x_k x_j.
  \]
\end{itemize}

\subsection{Ordinary Least Squares}\label{ordinary-least-squares}

\begin{itemize}
\tightlist
\item
  Inference from data is carried out by \textbf{ordinary least squares}
  (OLS)
\item
  For an excellent review including R examples, see Sheather (2009)
\item
  OLS and maximum likelihood estimators (MLEs) are in the typical
  Gaussian linear modeling setup basically equivalent
\end{itemize}

\section{Designs}\label{designs}

\begin{itemize}
\tightlist
\item
  Important: Organize the data collection phase of a response surface
  study carefully
\item
  \textbf{Design}: choice of \(x\)'s where we plan to observe \(y\)'s,
  for the purpose of approximating \(f\)
\item
  Analyses and designs need to be carefully matched
\item
  When using a first-order model, some designs are preferred over others
\item
  When using a second-order model to capture curvature, a different sort
  of design is appropriate
\item
  Design choices often contain features enabling modeling assumptions to
  be challenged

  \begin{itemize}
  \tightlist
  \item
    e.g., to check if initial impressions are supported by the data
    ultimately collected
  \end{itemize}
\end{itemize}

\subsection{Different Designs}\label{different-designs}

\begin{itemize}
\tightlist
\item
  \textbf{Screening desings}: determine which variables matter so that
  subsequent experiments may be smaller and/or more focused
\item
  Then there are designs tailored to the form of model (first- or
  second-order, say) in the screened variables
\item
  And then there are more designs still
\end{itemize}

\section{RSM Experimentation}\label{rsm-experimentation}

\subsection{First Step}\label{first-step}

\begin{itemize}
\tightlist
\item
  RSM-based experimentation begins with a \textbf{first-order model},
  possibly with interactions
\item
  Presumption: current process operating \textbf{far from optimal}
  conditions
\item
  Collect data and apply \textbf{method of steepest ascent} (gradient)
  on fitted surfaces to move to the optimum
\end{itemize}

\subsection{Second Step}\label{second-step}

\begin{itemize}
\tightlist
\item
  Eventually, if all goes well after several such carefully iterated
  refinements, \textbf{second-order models} are used on appropriate
  designs in order to zero-in on ideal operating conditions
\item
  Careful analysis of the fitted surface:

  \begin{itemize}
  \tightlist
  \item
    Ridge analysis with further refinement using gradients of, and
  \item
    standard errors associated with, the fitted surfaces, and so on
  \end{itemize}
\end{itemize}

\subsection{Third Step}\label{third-step}

\begin{itemize}
\tightlist
\item
  Once the practitioner is satisfied with the full arc of

  \begin{itemize}
  \tightlist
  \item
    design(s),
  \item
    fit(s), and
  \item
    decision(s):
  \end{itemize}
\item
  A small experiment called \textbf{confirmation test} may be performed
  to check if the predicted optimal settings are realizable in practice
\end{itemize}

\section{RSM: Review and General
Considerations}\label{rsm-review-and-general-considerations}

\begin{itemize}
\item
  First Glimpse, RSM seems sensible, and pretty straightforward as
  quantitative statistics-based analysis goes
\item
  But: RSM can get complicated, especially when input dimensions are not
  very low
\item
  Design considerations are particularly nuanced, since the goal is to
  obtain reliable estimates of main effects, interaction, and curvature
  while minimizing sampling effort/expense
\item
  RSM Downside: Inefficiency

  \begin{itemize}
  \tightlist
  \item
    Despite intuitive appeal, several RSM downsides become apparent upon
    reflection
  \item
    Problems in practice
  \item
    Stepwise nature of sequential decision making is inefficient:

    \begin{itemize}
    \tightlist
    \item
      Not obvious how to re-use or update analysis from earlier phases,
      or couple with data from other sources/related experiments
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Locality

  \begin{itemize}
  \tightlist
  \item
    In addition to being local in experiment-time (stepwise approach),
    it's local in experiment-space
  \item
    Balance between

    \begin{itemize}
    \tightlist
    \item
      exploration (maybe we're barking up the wrong tree) and
    \item
      exploitation (let's make things a little better) is modest at best
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Interjection of expert knowledge is limited to hunches about
    relevant variables (i.e., the screening phase), where to initialize
    search, how to design the experiments
  \item
    Yet at the same time classical RSMs rely heavily on constant
    examination throughout stages of modeling and design and on the
    instincts of seasoned practitioners
  \end{itemize}
\item
  RSM Downside: Replicability

  \begin{itemize}
  \tightlist
  \item
    Parallel analyses, conducted according to the same best intentions,
    rarely lead to the same designs, model fits and so on
  \item
    Sometimes that means they lead to different conclusions, which can
    be cause for concern
  \end{itemize}
\end{itemize}

\subsection{Historical Considerations about
RSM}\label{historical-considerations-about-rsm}

\begin{itemize}
\tightlist
\item
  In spite of those criticisms, however, there was historically little
  impetus to revise the status quo
\item
  Classical RSM was comfortable in its skin, consistently led to
  improvements or compelling evidence that none can reasonably be
  expected
\item
  But then in the late 20th century came an explosive expansion in
  computational capability, and with it a means of addressing many of
  those downsides
\end{itemize}

\subsection{Status Quo}\label{status-quo}

\begin{itemize}
\tightlist
\item
  Nowadays, field experiments and statistical models, designs and
  optimizations are coupled with with mathematical models
\item
  Simple equations are not regarded as sufficient to describe real-world
  systems anymore
\item
  Physicists figured that out fifty years ago; industrial engineers
  followed, biologists, social scientists, climate scientists and
  weather forecasters, etc.
\item
  Systems of equations are required, solved over meshes (e.g., finite
  elements), or stochastically interacting agents
\item
  Goals for those simulation experiments are as diverse as their
  underlying dynamics
\item
  Optimization of systems is common, e.g., to identify worst-case
  scenarios
\end{itemize}

\subsection{The Role of Statistics}\label{the-role-of-statistics}

\begin{itemize}
\tightlist
\item
  Solving systems of equations, or interacting agents, requires
  computing
\item
  Statistics involved at various stages:

  \begin{itemize}
  \tightlist
  \item
    choosing the mathematical model
  \item
    solving by stochastic simulation (Monte Carlo)
  \item
    designing the computer experiment
  \item
    smoothing over idiosyncrasies or noise
  \item
    finding optimal conditions, or
  \item
    calibrating mathematical/computer models to data from field
    experiments
  \end{itemize}
\end{itemize}

\subsection{New RSM is needed: DACE}\label{new-rsm-is-needed-dace}

\begin{itemize}
\tightlist
\item
  Classical RSMs are not well-suited to any of those tasks, because

  \begin{itemize}
  \tightlist
  \item
    they lack the fidelity required to model these data
  \item
    their intended application is too local
  \item
    they're also too hands-on.
  \end{itemize}
\item
  Once computers are involved, a natural inclination is to automate---to
  remove humans from the loop and set the computer running on the
  analysis in order to maximize computing throughput, or minimize idle
  time
\item
  \textbf{Design and Analysis of Computer Experiments} as a modern
  extension of RSM
\item
  Experimentation is changing due to advances in machine learning
\item
  \textbf{Gaussian process} (GP) regression is the canonical surrogate
  model
\item
  Origins in geostatistics (gold mining)
\item
  Wide applicability in contexts where prediction is king
\item
  Machine learners exposed GPs as powerful predictors for all sorts of
  tasks:
\item
  from regression to classification,
\item
  active learning/sequential design,
\item
  reinforcement learning and optimization,
\item
  latent variable modeling, and so on
\end{itemize}

\section{Exercises}\label{exercises-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate 3d Plots for the Contour Plots in this notebook.
\item
  Write a \texttt{plot\_3d} function, that takes the objective function
  \texttt{fun} as an argument.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface: \texttt{plot\_3d(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Write a \texttt{plot\_contour} function, that takes the objective
  function \texttt{fun} as an argument:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface:
  \texttt{plot\_contour(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Consider further arguments that might be useful for both function,
  e.g., ranges, size, etc.
\end{enumerate}

\section{Jupyter Notebook}\label{jupyter-notebook-4}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/005_num_rsm.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Kriging (Gaussian Process
Regression)}\label{kriging-gaussian-process-regression}

\section{DACE and RSM}\label{dace-and-rsm}

Mathematical models implemented in computer codes are used to circumvent
the need for expensive field data collection. These models are
particularly useful when dealing with highly nonlinear response
surfaces, high signal-to-noise ratios (which often involve deterministic
evaluations), and a global scope. As a result, a new approach is
required in comparison to Response Surface Methodology (RSM).

With the improvement in computing power and simulation fidelity,
researchers gain higher confidence and a better understanding of the
dynamics in physical, biological, and social systems. However, the
expansion of configuration spaces and increasing input dimensions
necessitates more extensive designs. High-performance computing (HPC)
allows for thousands of runs, whereas previously only tens were
possible. This shift towards larger models and training data presents
new computational challenges.

Research questions for DACE (Design and Analysis of Computer
Experiments) include how to design computer experiments that make
efficient use of computation and how to meta-model computer codes to
save on simulation effort. The choice of surrogate model for computer
codes significantly impacts the optimal experiment design, and the
preferred model-design pairs can vary depending on the specific goal.

The combination of computer simulation, design, and modeling with field
data from similar real-world experiments introduces a new category of
computer model tuning problems. The ultimate goal is to automate these
processes to the greatest extent possible, allowing for the deployment
of HPC with minimal human intervention.

One of the remaining differences between RSM and DACE lies in how they
handle noise. DACE employs replication, a technique that would not be
used in a deterministic setting, to separate signal from noise.
Traditional RSM is best suited for situations where a substantial
proportion of the variability in the data is due to noise, and where the
acquisition of data values can be severely limited. Consequently, RSM is
better suited for a different class of problems, aligning with its
intended purposes.

Two very good texts on computer experiments and surrogate modeling are
Santner, Williams, and Notz (2003) and Forrester, Sóbester, and Keane
(2008). The former is the canonical reference in the statistics
literature and the latter is perhaps more popular in engineering.

\section{Background: Expectation, Mean, Standard
Deviation}\label{background-expectation-mean-standard-deviation}

The distribution of a random vector is characterized by some indexes.
One of them is the expected value, which is defined as \[
E[X] = \sum_{x \in D_X} xp_X(x)  \qquad \text{if $X$ is discrete}
\] \[
E[X] = \int\limits_{x \in D_X} xf_X(x)\mathrm{d}x  \quad  \text{if $X$ is continuous.}
\]

The mean, \(\mu\), of a probability distribution is a measure of its
central tendency or location. That is, \(E(X)\) is defined as the
average of all possible values of \(X\), weighted by their
probabilities.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Expectation}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Let \(X\) denote the number produced by rolling a fair die. Then \[
E(X) = 1 \times 1/6 + 2 \times 1/6 + 3 \times 1/6 + 4 \times 1/6 + 5 \times 1/6 + 6\times 1/6 = 3.5.
\]

\end{tcolorbox}

\subsection{Sample Mean}\label{sample-mean}

The sample mean is an important estimate of the population mean. The
sample mean of a sample \(\{x_i\}\) (\(i=1,2,\ldots,n\)) is defined as
\[\overline{x}  = \frac{1}{n} \sum_i x_i.\]

\subsection{Variance and Standard
Deviation}\label{variance-and-standard-deviation}

If we are trying to predict the value of a random variable \(X\) by its
mean \(\mu = E(X)\), the error will be \(X-\mu\). In many situations it
is useful to have an idea how large this deviation or error is. Since
\(E(X-\mu) = E(X) -\mu = 0\), it is necessary to use the absolute value
or the square of (\(X-\mu\)). The squared error is the first choice,
because the derivatives are easier to calculate. These considerations
motivate the definition of the variance:

The variance of a random variable \(X\) is the mean squared deviation of
\(X\) from its expected value \(\mu = E(X)\). \begin{equation}
Var(X) = E[ (X-\mu)^2].
\end{equation}

\subsection{Standard Deviation}\label{standard-deviation}

Taking the square root of the variance to get back to the same scale of
units as \(X\) gives the standard deviation. The standard deviation of
\(X\) is the square root of the variance of \(X\). \begin{equation}
sd(X) = \sqrt{Var(X)}.
\end{equation}

\subsection{Calculation of the Standard Deviation with
Python}\label{calculation-of-the-standard-deviation-with-python}

The function \texttt{numpy.std} returns the standard deviation, a
measure of the spread of a distribution, of the array elements. The
argument \texttt{ddof} specifies the Delta Degrees of Freedom. The
divisor used in calculations is \texttt{N\ -\ ddof}, where \texttt{N}
represents the number of elements. By default \texttt{ddof} is zero,
i.e., \texttt{std} uses the formula
\begin{equation}  \sqrt{  \frac{1}{N} \sum_i \left( x_i - \bar{x} \right)^2  } \qquad \text{with } \quad \bar{x} = \sum_{i=1}^N x_i /N. \end{equation}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Standard Deviation with Python}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Consider the array \([1,2,3]\): Since \(\bar{x} = 2\), the following
value is computed:
\[ \sqrt{1/3 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/3}.\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]])}
\NormalTok{np.std(a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.816496580927726
\end{verbatim}

\end{tcolorbox}

\subsection{The Empirical Standard
Deviation}\label{the-empirical-standard-deviation}

The empirical standard deviation (which uses \(N-1\)),
\(\sqrt{1/2 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/2}\),
can be calculated as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.0
\end{verbatim}

\subsection{The Argument ``axis''}\label{the-argument-axis}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Axes along which the standard deviation is computed}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  When you compute np.std with axis=0, it calculates the standard
  deviation along the vertical axis, meaning it computes the standard
  deviation for each column of the array.
\item
  On the other hand, when you compute np.std with axis=1, it calculates
  the standard deviation along the horizontal axis, meaning it computes
  the standard deviation for each row of the array.
\item
  If the axis parameter is not specified, np.std computes the standard
  deviation of the flattened array.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]])}
\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1, 2],
       [3, 4]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.118033988749895
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1., 1.])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.5, 0.5])
\end{verbatim}

\section{Data Types and Precision in
Python}\label{data-types-and-precision-in-python}

We consider single versus double precision in Python. In single
precision, \texttt{std()} can be inaccurate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.zeros((}\DecValTok{2}\NormalTok{, }\DecValTok{4}\OperatorTok{*}\DecValTok{4}\NormalTok{), dtype}\OperatorTok{=}\NormalTok{np.float32)}
\NormalTok{a[}\DecValTok{0}\NormalTok{, :] }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{a[}\DecValTok{1}\NormalTok{, :] }\OperatorTok{=} \FloatTok{0.1}
\NormalTok{a }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,
        1. , 1. , 1. ],
       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
        0.1, 0.1, 0.1]], dtype=float32)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
       0.45, 0.45, 0.45, 0.45, 0.45], dtype=float32)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0., 0.], dtype=float32)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(}\FloatTok{0.45} \OperatorTok{{-}}\NormalTok{ np.std(a))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.7881393421514957e-08
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Float data types}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  float32 and float64 are data types in numpy that specify the precision
  of floating point numbers.
\item
  float32 is a single-precision floating point number that occupies 32
  bits of memory. It has a precision of about 7 decimal digits.
\item
  float64 is a double-precision floating point number that occupies 64
  bits of memory. It has a precision of about 15 decimal digits.
\item
  The main difference between float32 and float64 is the precision and
  memory usage. float64 provides a higher precision but uses more
  memory, while float32 uses less memory but has a lower precision.
\end{itemize}

\end{tcolorbox}

Computing the standard deviation in float64 is more accurate (result may
vary), see
\url{https://numpy.org/devdocs/reference/generated/numpy.std.html}.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(}\FloatTok{0.45} \OperatorTok{{-}}\NormalTok{ np.std(a, dtype}\OperatorTok{=}\NormalTok{np.float64))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
7.450580707946131e-10
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: 32 versus 64 bit}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a number}
\NormalTok{num }\OperatorTok{=} \FloatTok{0.123456789123456789}

\CommentTok{\# Convert to float32 and float64}
\NormalTok{num\_float32 }\OperatorTok{=}\NormalTok{ np.float32(num)}
\NormalTok{num\_float64 }\OperatorTok{=}\NormalTok{ np.float64(num)}

\CommentTok{\# Print the number in both formats}
\BuiltInTok{print}\NormalTok{(}\StringTok{"float32: "}\NormalTok{, num\_float32)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"float64: "}\NormalTok{, num\_float64)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
float32:  0.12345679
float64:  0.12345678912345678
\end{verbatim}

\end{tcolorbox}

The float32 data type in numpy represents a single-precision floating
point number. It uses 32 bits of memory, which gives it a precision of
about 7 decimal digits. On the other hand, float64 represents a
double-precision floating point number. It uses 64 bits of memory, which
gives it a precision of about 15 decimal digits.

The reason float32 shows fewer digits is because it has less precision
due to using less memory. The bits of memory are used to store the sign,
exponent, and fraction parts of the floating point number, and with
fewer bits, you can represent fewer digits accurately.

\section{Distributions and Random Numbers in
Python}\label{distributions-and-random-numbers-in-python}

Results from computers are deterministic, so it sounds like a
contradiction in terms to generate random numbers on a computer.
Standard computers generate pseudo-randomnumbers, i.e., numbers that
behave as if they were drawn randomly.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Deterministic Random Numbers}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  Idea: Generate deterministically numbers that \textbf{look} (behave)
  as if they were drawn randomly.
\end{itemize}

\end{tcolorbox}

\subsection{The Uniform Distribution}\label{the-uniform-distribution}

The probability density function of the uniform distribution is defined
as: \[
f_X(x) = \frac{1}{b-a} \qquad \text{for $x \in [a,b]$}.
\]

Generate 10 random numbers from a uniform distribution between \(a=0\)
and \(b=1\):

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\CommentTok{\# Initialize the random number generator}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{123456789}\NormalTok{)}
\NormalTok{n }\OperatorTok{=} \DecValTok{10}
\NormalTok{x }\OperatorTok{=}\NormalTok{ rng.uniform(low}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, high}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, size}\OperatorTok{=}\NormalTok{n)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.02771274, 0.90670006, 0.88139355, 0.62489728, 0.79071481,
       0.82590801, 0.84170584, 0.47172795, 0.95722878, 0.94659153])
\end{verbatim}

Generate 10,000 random numbers from a uniform distribution between 0 and
10 and plot a histogram of the numbers:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Initialize the random number generator}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{123456789}\NormalTok{)}

\CommentTok{\# Generate random numbers from a uniform distribution}
\NormalTok{x }\OperatorTok{=}\NormalTok{ rng.uniform(low}\OperatorTok{=}\DecValTok{0}\NormalTok{, high}\OperatorTok{=}\DecValTok{10}\NormalTok{, size}\OperatorTok{=}\DecValTok{10000}\NormalTok{)}

\CommentTok{\# Plot a histogram of the numbers}
\NormalTok{plt.hist(x, bins}\OperatorTok{=}\DecValTok{50}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Uniform Distribution [0,10]\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Value\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{006_num_gp_files/figure-pdf/cell-15-output-1.pdf}

\subsection{The Normal Distribution}\label{the-normal-distribution}

The probability density function of the normal distribution is defined
as: \begin{equation}\phantomsection\label{eq-normal-one}{
f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2\right),
}\end{equation} where: \(\mu\) is the mean; \(\sigma\) is the standard
deviation.

To generate ten random numbers from a normal distribution, the following
command can be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# generate 10 random numbers between from a normal distribution}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}
\NormalTok{n }\OperatorTok{=} \DecValTok{10}
\NormalTok{mu, sigma }\OperatorTok{=} \DecValTok{2}\NormalTok{, }\FloatTok{0.1}
\NormalTok{x }\OperatorTok{=}\NormalTok{ rng.normal(mu, sigma, n)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1.92246028, 2.13064739, 1.87916577, 2.13222155, 2.07729291,
       2.04886785, 2.16793645, 1.95492483, 1.87879115, 1.9502618 ])
\end{verbatim}

Verify the mean:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(mu }\OperatorTok{{-}}\NormalTok{ np.mean(x))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.014256998009770427
\end{verbatim}

Note: To verify the standard deviation, we use \texttt{ddof\ =\ 1}
(empirical standard deviation):

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(sigma }\OperatorTok{{-}}\NormalTok{ np.std(x, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.009997036325877895
\end{verbatim}

A normally distributed random variable is a random variable whose
associated probability distribution is the normal (or Gaussian)
distribution. The normal distribution is a continuous probability
distribution characterized by a symmetric bell-shaped curve.

The distribution is defined by two parameters: the mean \(\mu\) and the
standard deviation \(\sigma\). The mean indicates the center of the
distribution, while the standard deviation measures the spread or
dispersion of the distribution.

This distribution is widely used in statistics and the natural and
social sciences as a simple model for random variables with unknown
distributions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_normal\_distribution(mu}\OperatorTok{=}\DecValTok{0}\NormalTok{, sigma}\OperatorTok{=}\DecValTok{1}\NormalTok{, num\_samples}\OperatorTok{=}\DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{006_num_gp_files/figure-pdf/cell-20-output-1.pdf}

\subsection{Visualization of the Standard
Deviation}\label{visualization-of-the-standard-deviation}

The standard deviation of normal distributed can be visualized in terms
of the histogram of \(X\):

\begin{itemize}
\tightlist
\item
  about 68\% of the values will lie in the interval within one standard
  deviation of the mean
\item
  95\% lie within two standard deviation of the mean
\item
  and 99.9\% lie within 3 standard deviations of the mean.
\end{itemize}

\includegraphics{006_num_gp_files/figure-pdf/cell-21-output-1.pdf}

\subsection{Standardization of Random
Variables}\label{standardization-of-random-variables}

To compare statistical properties of random variables which use
different units, it is a common practice to transform these random
variables into standardized variables. If a random variable \(X\) has
expectation \(E(X) = \mu\) and standard deviation \(sd(X) = \sigma >0\),
the random variable \[
X^{\ast} = (X-\mu)/\sigma
\] is called \(X\) in standard units. It has \(E(X^{\ast}) = 0\) and
\(sd(X^{\ast}) =1\).

\subsection{Realizations of a Normal
Distribution}\label{realizations-of-a-normal-distribution}

Realizations of a normal distribution refers to the actual values that
you get when you draw samples from a normal distribution. Each sample
drawn from the distribution is a realization of that distribution.

For example, if you have a normal distribution with a mean of 0 and a
standard deviation of 1, each number you draw from that distribution is
a realization.

Here's a Python example:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define the parameters of the normal distribution}
\NormalTok{mu }\OperatorTok{=} \DecValTok{0}
\NormalTok{sigma }\OperatorTok{=} \DecValTok{1}

\CommentTok{\# Draw 10 samples (realizations) from the normal distribution}
\NormalTok{realizations }\OperatorTok{=}\NormalTok{ np.random.normal(mu, sigma, }\DecValTok{10}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(realizations)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[ 0.48951662  0.23879586 -0.44811181 -0.610795   -2.02994507  0.60794659
 -0.35410888  0.15258149  0.50127485 -0.78640277]
\end{verbatim}

In this code, np.random.normal generates 10 realizations of a normal
distribution with a mean of 0 and a standard deviation of 1. The
realizations array contains the actual values drawn from the
distribution.

\subsection{The Multivariate Normal
Distribution}\label{the-multivariate-normal-distribution}

The multivariate normal, multinormal, or Gaussian distribution serves as
a generalization of the one-dimensional normal distribution to higher
dimensions. We will consider \(k\)-dimensional random vectors
\(X = (X_1, X_2, \ldots, X_k)\). When drawing samples from this
distribution, it results in a set of values represented as
\(\{x_1, x_2, \ldots, x_k\}\). To fully define this distribution, it is
necessary to specify its mean \(\mu\) and covariance matrix \(\Sigma\).
These parameters are analogous to the mean, which represents the central
location, and the variance (squared standard deviation) of the
one-dimensional normal distribution introduced in
Equation~\ref{eq-normal-one}.

In the context of the multivariate normal distribution, the mean takes
the form of a coordinate within an \(k\)-dimensional space. This
coordinate represents the location where samples are most likely to be
generated, akin to the peak of the bell curve in a one-dimensional or
univariate normal distribution.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Covariance of two random variables}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

For two random variables \(X\) and \(Y\), the covariance is defined as
the expected value (or mean) of the product of their deviations from
their individual expected values: \[
\operatorname{cov}(X, Y) = \operatorname{E}{\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]}
\]

The covariance within the multivariate normal distribution denotes the
extent to which two variables vary together. The elements of the
covariance matrix, such as \(\Sigma_{ij}\), represent the covariances
between the variables \(x_i\) and \(x_j\). These covariances describe
how the different variables in the distribution are related to each
other in terms of their variability.

The probability density function (PDF) of the multivariate normal
distribution is defined as: \[
f_X(x) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp\left(-\frac{1}{2} (x-\mu)^T\Sigma^{-1} (x-\mu)\right),
\] where: \(\mu\) is the \(k \times 1\) mean vector; \(\Sigma\) is the
\(k \times k\) covariance matrix. The covariance matrix \(\Sigma\) is
assumed to be positive definite, so that its determinant is strictly
positive.

For discrete random variables, covariance can be written as: \[
\operatorname{cov} (X,Y) = \frac{1}{n}\sum_{i=1}^n (x_i-E(X)) (y_i-E(Y)).
\]

\end{tcolorbox}

Figure~\ref{fig-bi9040} shows draws from a bivariate normal distribution
with \(\mu = \begin{pmatrix}0 \\ 0\end{pmatrix}\) and
\(\Sigma=\begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix}\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{mean }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]}
\NormalTok{cov }\OperatorTok{=}\NormalTok{ [[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]]  }\CommentTok{\# diagonal covariance}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(mean, cov, }\DecValTok{1000}\NormalTok{).T}
\CommentTok{\# Create a scatter plot of the numbers}
\NormalTok{plt.scatter(x, y, s}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{plt.axis(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\NormalTok{plt.title(}\SpecialStringTok{f"Bivariate Normal. Mean zero and positive covariance: }\SpecialCharTok{\{}\NormalTok{cov}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-bi9040-output-1.pdf}

}

\caption{\label{fig-bi9040}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & 4 \\ 4 & 9\end{pmatrix}\)}

\end{figure}%

The covariance matrix of a bivariate normal distribution determines the
shape, orientation, and spread of the distribution in the
two-dimensional space.

The diagonal elements of the covariance matrix (\(\sigma_1^2\),
\(\sigma_2^2\)) are the variances of the individual variables. They
determine the spread of the distribution along each axis. A larger
variance corresponds to a greater spread along that axis.

The off-diagonal elements of the covariance matrix
(\(\sigma_{12}, \sigma_{21}\)) are the covariances between the
variables. They determine the orientation and shape of the distribution.
If the covariance is positive, the distribution is stretched along the
line \(y=x\), indicating that the variables tend to increase together.
If the covariance is negative, the distribution is stretched along the
line \(y=-x\), indicating that one variable tends to decrease as the
other increases. If the covariance is zero, the variables are
uncorrelated and the distribution is axis-aligned.

In Figure~\ref{fig-bi9040}, the variances are identical and the
variables are correlated (covariance is 4), so the distribution is
stretched along the line \(y=x\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ multivariate\_normal}

\CommentTok{\# Parameters}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{cov }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}

\CommentTok{\# Create grid and multivariate normal}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x,y)}
\NormalTok{pos }\OperatorTok{=}\NormalTok{ np.empty(X.shape }\OperatorTok{+}\NormalTok{ (}\DecValTok{2}\NormalTok{,))}
\NormalTok{pos[:, :, }\DecValTok{0}\NormalTok{] }\OperatorTok{=}\NormalTok{ X}\OperatorTok{;}\NormalTok{ pos[:, :, }\DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ Y}
\NormalTok{rv }\OperatorTok{=}\NormalTok{ multivariate\_normal(mu, cov)}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ plt.axes(projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)  }
\NormalTok{surf}\OperatorTok{=}\NormalTok{ax.plot\_surface(X, Y, rv.pdf(pos),cmap}\OperatorTok{=}\StringTok{\textquotesingle{}viridis\textquotesingle{}}\NormalTok{,linewidth}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}X axis\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}Y axis\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}Z axis\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Bivariate Normal Distribution\textquotesingle{}}\NormalTok{)}
\NormalTok{fig.colorbar(surf, shrink}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, aspect}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-bi90403d-output-1.pdf}

}

\caption{\label{fig-bi90403d}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & 4 \\ 4 & 9\end{pmatrix}\)}

\end{figure}%

\subsection{\texorpdfstring{The Bivariate Normal Distribution with Mean
Zero and Zero Covariances
\(\sigma_{12} = \sigma_{21} = 0\)}{The Bivariate Normal Distribution with Mean Zero and Zero Covariances \textbackslash sigma\_\{12\} = \textbackslash sigma\_\{21\} = 0}}\label{the-bivariate-normal-distribution-with-mean-zero-and-zero-covariances-sigma_12-sigma_21-0}

\(\Sigma=\begin{pmatrix} 9 & 0 \\ 0 & 9\end{pmatrix}\)

\begin{figure}

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-bi9000-output-1.pdf}

}

\caption{\label{fig-bi9000}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & 0 \\ 0 & 9\end{pmatrix}\)}

\end{figure}%

\subsection{\texorpdfstring{The Bivariate Normal Distribution with Mean
Zero and Negative Covariances
\(\sigma_{12} = \sigma_{21} = -4\)}{The Bivariate Normal Distribution with Mean Zero and Negative Covariances \textbackslash sigma\_\{12\} = \textbackslash sigma\_\{21\} = -4}}\label{the-bivariate-normal-distribution-with-mean-zero-and-negative-covariances-sigma_12-sigma_21--4}

\(\Sigma=\begin{pmatrix} 9 & -4 \\ -4 & 9\end{pmatrix}\)

\begin{figure}

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-bi9449-output-1.pdf}

}

\caption{\label{fig-bi9449}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & -4 \\ -4 & 9\end{pmatrix}\)}

\end{figure}%

\section{Cholesky Decomposition and Positive Definite
Matrices}\label{cholesky-decomposition-and-positive-definite-matrices}

The covariance matrix must be positive definite for a multivariate
normal distribution for a couple of reasons:

\begin{itemize}
\tightlist
\item
  Semidefinite vs Definite: A covariance matrix is always symmetric and
  positive semidefinite. However, for a multivariate normal
  distribution, it must be positive definite, not just semidefinite.
  This is because a positive semidefinite matrix can have zero
  eigenvalues, which would imply that some dimensions in the
  distribution have zero variance, collapsing the distribution in those
  dimensions. A positive definite matrix has all positive eigenvalues,
  ensuring that the distribution has positive variance in all
  dimensions.
\item
  Invertibility: The multivariate normal distribution's probability
  density function involves the inverse of the covariance matrix. If the
  covariance matrix is not positive definite, it may not be invertible,
  and the density function would be undefined.
\end{itemize}

In summary, the covariance matrix being positive definite ensures that
the multivariate normal distribution is well-defined and has positive
variance in all dimensions.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ is\_positive\_definite(matrix):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{all}\NormalTok{(np.linalg.eigvals(matrix) }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(is\_positive\_definite(matrix))  }\CommentTok{\# Outputs: True}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

More effficent (and check if symmetric) is based on Cholesky
decomposition.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ is\_pd(K):}
    \ControlFlowTok{try}\NormalTok{:}
\NormalTok{        np.linalg.cholesky(K)}
        \ControlFlowTok{return} \VariableTok{True}
    \ControlFlowTok{except}\NormalTok{ np.linalg.linalg.LinAlgError }\ImportTok{as}\NormalTok{ err:}
        \ControlFlowTok{if} \StringTok{\textquotesingle{}Matrix is not positive definite\textquotesingle{}} \KeywordTok{in}\NormalTok{ err.message:}
            \ControlFlowTok{return} \VariableTok{False}
        \ControlFlowTok{else}\NormalTok{:}
            \ControlFlowTok{raise}
\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(is\_pd(matrix))  }\CommentTok{\# Outputs: True}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Cholesky decomposition.}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\texttt{linalg.cholesky} computes the Cholesky decomposition of a
matrix, i.e., it computes a lower triangular matrix \(L\) such that
\(LL^T = A\). If the matrix is not positive definite, an error
(\texttt{LinAlgError}) is raised.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a Hermitian, positive{-}definite matrix}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]]) }

\CommentTok{\# Compute the Cholesky decomposition}
\NormalTok{L }\OperatorTok{=}\NormalTok{ np.linalg.cholesky(A)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"L = }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, L)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L*LT = }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.dot(L, L.T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
L = 
 [[3.         0.        ]
 [1.33333333 2.68741925]]
L*LT = 
 [[9. 4.]
 [4. 9.]]
\end{verbatim}

\end{tcolorbox}

\section{Maximum Likelihood Estimation: Multivariate Normal
Distribution}\label{maximum-likelihood-estimation-multivariate-normal-distribution}

Consider the first \(n\) terms of an identically and independently
distributed (i.i..d.) sequence \({X^{(j)}}\) of \(k\)-dimensional
multivariate normal random vectors, i.e.,
\(X^{(j)} \sim N(\mu, \Sigma)\), \(j=1,2,\ldots\). The joint probability
density function of the \(j\)-th term of the sequence is \[
f_X(x_j) = \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right),
\]

where: \(\mu\) is the \(k \times 1\) mean vector; \(\Sigma\) is the
\(k \times k\) covariance matrix. The covariance matrix \(\Sigma\) is
assumed to be positive definite, so that its determinant is strictly
positive. We use \(x_1, \ldots x_n\), i.e., the realizations of the
first \(n\) random vectors in the sequence, to estimate the two unknown
parameters \(\mu\) and \(\Sigma\).

The likelihood function is defined as the joint probability density
function of the observed data, viewed as a function of the unknown
parameters. Since the terms in the sequence are independent, their joint
density is equal to the product of their marginal densities. As a
consequence, the likelihood function can be written as the product of
the individual densities:

\[
L(\mu, \Sigma) = \prod_{j=1}^n f_X(x_j) = \prod_{j=1}^n \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right)
\] \[
= \frac{1}{(2\pi)^{nk/2} \det(\Sigma)^{n/2}} \exp\left(-\frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right).
\] The log-likelihood function is \[
\ell(\mu, \Sigma) = -\frac{nk}{2} \ln(2\pi) - \frac{n}{2} \ln(\det(\Sigma)) - \frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu).
\] The likelihood function is well-defined only if \(\det(\Sigma)>0\).

\section{Introduction to Gaussian
Processes}\label{introduction-to-gaussian-processes}

The concept of GP (Gaussian Process) regression can be understood as a
simple extension of linear modeling. It is worth noting that this
approach goes by various names and acronyms, including ``kriging,'' a
term derived from geostatistics, as introduced by Matheron in 1963.
Additionally, it is referred to as Gaussian spatial modeling or a
Gaussian stochastic process, and machine learning (ML) researchers often
use the term Gaussian process regression (GPR). In all of these
instances, the central focus is on regression. This involves training on
both inputs and outputs, with the ultimate objective of making
predictions and quantifying uncertainty (referred to as uncertainty
quantification or UQ).

However, it's important to emphasize that GPs are not a universal
solution for every problem. Specialized tools may outperform GPs in
specific, non-generic contexts, and GPs have their own set of
limitations that need to be considered.

\subsection{Gaussian Process Prior}\label{gaussian-process-prior}

In the context of GP, any finite collection of realizations, which is
represented by \(n\) observations, is modeled as having a multivariate
normal (MVN) distribution. The characteristics of these realizations can
be fully described by two key parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Their mean, denoted as an \(n\)-vector \(\mu\).
\item
  The covariance matrix, denoted as an \(n \times n\) matrix \(\Sigma\).
  This covariance matrix encapsulates the relationships and variability
  between the individual realizations within the collection.
\end{enumerate}

\subsection{Covariance Function}\label{covariance-function}

The covariance function is defined by inverse exponentiated squared
Euclidean distance: \[
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 \},
\] where \(\vec{x}\) and \(\vec{x}'\) are two points in the
\(k\)-dimensional input space and \(\| \cdot \|\) denotes the Euclidean
distance, i.e., \[
|| \vec{x} - \vec{x}'||^2 = \sum_{i=1}^k (x_i - x_i')^2.
\]

An 1-d example is shown in Figure~\ref{fig-exp2euclid}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_inverse\_exp\_squared\_distance(}\DecValTok{5}\NormalTok{, }\FloatTok{0.0}\NormalTok{, [}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{2.0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-exp2euclid-output-1.pdf}

}

\caption{\label{fig-exp2euclid}One-dim inverse exponentiated squared
Euclidean distance}

\end{figure}%

The covariance function is also referred to as the kernel function. The
\emph{Gaussian} kernel uses an additional parameter, \(\sigma^2\), to
control the rate of decay. This parameter is referred to as the length
scale or the characteristic length scale. The covariance function is
then defined as

\begin{equation}\phantomsection\label{eq-Sigma}{
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 / (2 \sigma^2) \}.
}\end{equation}

The covariance decays exponentially fast as \(\vec{x}\) and \(\vec{x}'\)
become farther apart. Observe that

\[
\Sigma(\vec{x},\vec{x}) = 1
\] and

\[
\Sigma(\vec{x}, \vec{x}') < 1
\] for \(\vec{x} \neq \vec{x}'\). The function
\(\Sigma(\vec{x},\vec{x}')\) must be positive definite.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Positive Definiteness}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Positive definiteness in the context of the covariance matrix
\(\Sigma_n\) is a fundamental requirement. It is determined by
evaluating \(\Sigma(x_i, x_j)\) at pairs of \(n\) \(\vec{x}\)-values,
denoted as \(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\). The condition
for positive definiteness is that for all \(\vec{x}\) vectors that are
not equal to zero, the expression \(\vec{x}^\top \Sigma_n \vec{x}\) must
be greater than zero. This property is essential when intending to use
\(\Sigma_n\) as a covariance matrix in multivariate normal (MVN)
analysis. It is analogous to the requirement in univariate Gaussian
distributions where the variance parameter, \(\sigma^2\), must be
positive.

\end{tcolorbox}

Gaussian Processes (GPs) can be effectively utilized to generate random
data that follows a smooth functional relationship. The process involves
the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select a set of \(\vec{x}\)-values, denoted as
  \(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\).
\item
  Define the covariance matrix \(\Sigma_n\) by evaluating
  \(\Sigma_n^{ij} = \Sigma(\vec{x}_i, \vec{x}_j)\) for
  \(i, j = 1, 2, \ldots, n\).
\item
  Generate an \(n\)-variate realization \(Y\) that follows a
  multivariate normal distribution with a mean of zero and a covariance
  matrix \(\Sigma_n\), expressed as
  \(Y \sim \mathcal{N}_n(0, \Sigma_n)\).
\item
  Visualize the result by plotting it in the \(x\)-\(y\) plane.
\end{enumerate}

\subsection{Construction of the Covariance
Matrix}\label{construction-of-the-covariance-matrix}

Here is an one-dimensional example. The process begins by creating an
input grid using \(\vec{x}\)-values. This grid consists of 100 elements,
providing the basis for further analysis and visualization.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{n }\OperatorTok{=} \DecValTok{100}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the context of this discussion, the construction of the covariance
matrix, denoted as \(\Sigma_n\), relies on the concept of inverse
exponentiated squared Euclidean distances. However, it's important to
note that a modification is introduced later in the process.
Specifically, the diagonal of the covariance matrix is augmented with a
small value, represented as ``eps'' or \(\epsilon\).

The reason for this augmentation is that while inverse exponentiated
distances theoretically ensure the covariance matrix's positive
definiteness, in practical applications, the matrix can sometimes become
numerically ill-conditioned. By adding a small value to the diagonal,
such as \(\epsilon\), this ill-conditioning issue is mitigated. In this
context, \(\epsilon\) is often referred to as ``jitter.''

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ multivariate\_normal}
\KeywordTok{def}\NormalTok{ build\_Sigma(X, sigma2):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=} \DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{sigma2[l])}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigma2 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Sigma }\OperatorTok{=}\NormalTok{ build\_Sigma(X, sigma2)}
\NormalTok{np.}\BuiltInTok{round}\NormalTok{(Sigma[:}\DecValTok{3}\NormalTok{,:], }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783, 0.726,
        0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278, 0.236,
        0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044, 0.034,
        0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003, 0.002,
        0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783,
        0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278,
        0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044,
        0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003,
        0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.98 , 0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835,
        0.783, 0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325,
        0.278, 0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056,
        0.044, 0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004,
        0.003, 0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.imshow(Sigma, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}hot\textquotesingle{}}\NormalTok{, interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}nearest\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{006_num_gp_files/figure-pdf/cell-35-output-1.pdf}

\subsection{Generation of Random Samples and Plotting the Realizations
of the Random
Function}\label{generation-of-random-samples-and-plotting-the-realizations-of-the-random-function}

In the context of the multivariate normal distribution, the next step is
to utilize the previously constructed covariance matrix denoted as
\texttt{Sigma}. It is used as an essential component in generating
random samples from the multivariate normal distribution.

The function \texttt{multivariate\_normal} is employed for this purpose.
It serves as a random number generator specifically designed for the
multivariate normal distribution. In this case, the mean of the
distribution is set equal to \texttt{mean}, and the covariance matrix is
provided as \texttt{Psi}. The argument \texttt{size} specifies the
number of realizations, which, in this specific scenario, is set to one.

By default, the mean vector is initialized to zero. To match the number
of samples, which is equivalent to the number of rows in the \texttt{X}
and \texttt{Sigma} matrices, the argument \texttt{zeros(n)} is used,
where \texttt{n} represents the number of samples (here taken from the
size of the matrix, e.g.,: \texttt{Sigma.shape{[}0{]}}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{12345}\NormalTok{)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(zeros(Sigma.shape[}\DecValTok{0}\NormalTok{]), Sigma, size }\OperatorTok{=} \DecValTok{1}\NormalTok{, check\_valid}\OperatorTok{=}\StringTok{"raise"}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Y.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(100, 1)
\end{verbatim}

Now we can plot the results, i.e., a finite realization of the random
function \(Y()\) under a GP prior with a particular covariance
structure. We will plot those \texttt{X} and \texttt{Y} pairs as
connected points on an \(x\)-\(y\) plane.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(X, Y)}
\NormalTok{plt.title(}\StringTok{"Realization of Random Functions under a GP prior.}\CharTok{\textbackslash{}n}\StringTok{ sigma2: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sigma2[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-mvn1-1-output-1.pdf}

}

\caption{\label{fig-mvn1-1}Realization of one random function under a GP
prior. sigma2: 1.0}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{12345}\NormalTok{)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(zeros(Sigma.shape[}\DecValTok{0}\NormalTok{]), Sigma, size }\OperatorTok{=} \DecValTok{3}\NormalTok{, check\_valid}\OperatorTok{=}\StringTok{"raise"}\NormalTok{)}
\NormalTok{plt.plot(X, Y.T)}
\NormalTok{plt.title(}\StringTok{"Realization of Three Random Functions under a GP prior.}\CharTok{\textbackslash{}n}\StringTok{ sigma2: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sigma2[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-mvn1-3-output-1.pdf}

}

\caption{\label{fig-mvn1-3}Realization of three random functions under a
GP prior. sigma2: 1.0}

\end{figure}%

\subsection{Properties of the 1d
Example}\label{properties-of-the-1d-example}

\subsubsection{Several Bumps:}\label{several-bumps}

In this analysis, we observe several bumps in the \(x\)-range of
\([0,10]\). These bumps in the function occur because shorter distances
exhibit high correlation, while longer distances tend to be essentially
uncorrelated. This leads to variations in the function's behavior:

\begin{itemize}
\tightlist
\item
  When \(x\) and \(x'\) are one \(\sigma\) unit apart, the correlation
  is
  \(\exp\left(-\sigma^2 / (2\sigma^2)\right) = \exp(-1/2) \approx 0.61\),
  i.e., a relative high correlation.
\item
  \(2\sigma\) apart means correlation \(\exp(− 2^2 /2) \approx 0.14\),
  i.e., only small correlation.
\item
  \(4\sigma\) apart means correlation \(\exp(− 4^2 /2) \approx 0.0003\),
  i.e., nearly no correlation---variables are considered independent for
  almost all practical application.
\end{itemize}

\subsubsection{Smoothness:}\label{smoothness}

The function plotted in Figure~\ref{fig-mvn1-1} represents only a finite
realization, which means that we have data for a limited number of
pairs, specifically 100 points. These points appear smooth in a tactile
sense because they are closely spaced, and the plot function connects
the dots with lines to create the appearance of smoothness. The complete
surface, which can be conceptually extended to an infinite realization
over a compact domain, is exceptionally smooth in a calculus sense due
to the covariance function's property of being infinitely
differentiable.

\subsubsection{Scale of Two:}\label{scale-of-two}

Regarding the scale of the \(Y\) values, they have a range of
approximately \([-2,2]\), with a 95\% probability of falling within this
range. In standard statistical terms, 95\% of the data points typically
fall within two standard deviations of the mean, which is a common
measure of the spread or range of data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ multivariate\_normal}

\KeywordTok{def}\NormalTok{ build\_Sigma(X, sigma2):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=} \DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{sigma2[l])}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}

\KeywordTok{def}\NormalTok{ plot\_mvn( a}\OperatorTok{=}\DecValTok{0}\NormalTok{, b}\OperatorTok{=}\DecValTok{10}\NormalTok{, sigma2}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, size}\OperatorTok{=}\DecValTok{1}\NormalTok{, n}\OperatorTok{=}\DecValTok{100}\NormalTok{, show}\OperatorTok{=}\VariableTok{True}\NormalTok{):    }
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.linspace(a, b, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{    sigma2 }\OperatorTok{=}\NormalTok{ np.array([sigma2])}
\NormalTok{    Sigma }\OperatorTok{=}\NormalTok{ build\_Sigma(X, sigma2)}
\NormalTok{    rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{12345}\NormalTok{)}
\NormalTok{    Y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(zeros(Sigma.shape[}\DecValTok{0}\NormalTok{]), Sigma, size }\OperatorTok{=}\NormalTok{ size, check\_valid}\OperatorTok{=}\StringTok{"raise"}\NormalTok{)}
\NormalTok{    plt.plot(X, Y.T)}
\NormalTok{    plt.title(}\StringTok{"Realization of Random Functions under a GP prior.}\CharTok{\textbackslash{}n}\StringTok{ sigma2: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sigma2[}\DecValTok{0}\NormalTok{]))}
    \ControlFlowTok{if}\NormalTok{ show:}
\NormalTok{        plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_mvn(a}\OperatorTok{=}\DecValTok{0}\NormalTok{, b}\OperatorTok{=}\DecValTok{10}\NormalTok{, sigma2}\OperatorTok{=}\FloatTok{10.0}\NormalTok{, size}\OperatorTok{=}\DecValTok{3}\NormalTok{, n}\OperatorTok{=}\DecValTok{250}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-mvn2-output-1.pdf}

}

\caption{\label{fig-mvn2}Realization of Random Functions under a GP
prior. sigma2: 10}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_mvn(a}\OperatorTok{=}\DecValTok{0}\NormalTok{, b}\OperatorTok{=}\DecValTok{10}\NormalTok{, sigma2}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, size}\OperatorTok{=}\DecValTok{3}\NormalTok{, n}\OperatorTok{=}\DecValTok{250}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-mvn5-output-1.pdf}

}

\caption{\label{fig-mvn5}Realization of Random Functions under a GP
prior. sigma2: 0.1}

\end{figure}%

\section{Kriging: Modeling Basics}\label{kriging-modeling-basics}

\subsection{The Kriging Idea in a
Nutshell}\label{the-kriging-idea-in-a-nutshell}

We consider observed data of an unknown function \(f\) at \(n\) points
\(x_1, \ldots, x_n\), see Figure~\ref{fig-unknownf}. These measurements
a considered as realizations of MVN random variables
\(Y_1, \ldots, Y_n\) with mean \(\mu\) and covariance matrix
\(\Sigma_n\) as shown in Figure~\ref{fig-mvn1-3}, Figure~\ref{fig-mvn2}
or Figure~\ref{fig-mvn5}. In Kriging, a more general covariance matrix
(or equivalently, a correlation matrix \(\Psi\)) is used, see
Equation~\ref{eq-krigingbase}. Using a maximum likelihood approach, we
can estimate the unknown parameters \(\mu\) and \(\Sigma_n\) from the
data so that the likelihood function is maximized.

\begin{figure}

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-unknownf-output-1.pdf}

}

\caption{\label{fig-unknownf}Eight measurements of an unknown function}

\end{figure}%

\subsection{The Kriging Basis
Function}\label{the-kriging-basis-function}

\(k\)-dimensional basis functions of the form
\begin{equation}\phantomsection\label{eq-krigingbase}{
\psi(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp \left( - \sum_{l=1}^k \theta_l | x_{l}^{(i)} - x_{l}^{(j)} | ^{p_l} \right)
}\end{equation} are used in a method known as Kriging. Note,
\(\vec{x}^{(i)}\) denotes the \(k\)-dim vector
\(\vec{x}^{(i)}= (x_1^{(i)}, \ldots, x_k^{(i)})^T\).

The Kriging basis function is related to the 1-dim Gaussian basis
function (Equation~\ref{eq-Sigma}), which is defined as
\begin{equation}\phantomsection\label{eq-Sigma2}{
\Sigma(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp\{ - || \vec{x}^{(i)} - \vec{x}^{(j)}||^2 / (2\sigma^2) \}.
}\end{equation}

There are some differences between Gaussian basis functions and Kriging
basis functions:

\begin{itemize}
\tightlist
\item
  Where the Gaussian basis function has \(1/(2\sigma^2)\), the Kriging
  basis has a vector
  \(\theta = [\theta_1, \theta_2, \ldots, \theta_k]^T\).
\item
  The \(\theta\) vector allows the width of the basis function to vary
  from dimension to dimension.
\item
  In the Gaussian basis function, the exponent is fixed at 2, Kriging
  allows this exponent \(p_l\) to vary (typically from 1 to 2).
\end{itemize}

\subsection{The Correlation
Coefficient}\label{the-correlation-coefficient}

In a bivariate normal distribution, the covariance matrix and the
correlation coefficient are closely related. The covariance matrix
\(\Sigma\) for a bivariate normal distribution is a \(2\times 2\) matrix
that looks like this:

\[
\Sigma =
\begin{pmatrix}
\sigma_1^2 & \sigma_{12}\\
\sigma_{21} & \sigma_2^2
\end{pmatrix},
\] where \(\sigma_1^2\) and \(\sigma_2^2\) are the variances of \(X_1\)
and \(X_2\), and \(\sigma_{12} = \sigma_{21}\) is the covariance between
\(X_1\) and \(X_2\).

The correlation coefficient, often denoted as \(\rho\), is a normalized
measure of the linear relationship between two variables. It is
calculated from the covariance and the standard deviations \(\sigma_1\)
and \(\sigma_2\) (or the square roots of the variances) of \(X_1\) and
\(X_2\) as follows: \[
\rho = \sigma_{12} / (\sqrt{\sigma_1^2} \times \sqrt{\sigma_2^2}) = \sigma_{12} / (\sigma_1 \times \sigma_2).
\]

So we can express the correlation coefficient \(\rho\) in terms of the
elements of the covariance matrix \(\Sigma\). It can be interpreted as
follows: The correlation coefficient ranges from -1 to 1. A value of 1
means that \(X_1\) and \(X_2\) are perfectly positively correlated, a
value of -1 means they are perfectly negatively correlated, and a value
of 0 means they are uncorrelated. This gives the same information as the
covariance, but on a standardized scale that does not depend on the
units of \(X_1\) and \(X_2\).

\subsection{Covariance Matrix and Correlation
Matrix}\label{covariance-matrix-and-correlation-matrix}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Covariance and Correlation (taken from @Forr08a)}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Covariance is a measure of the correlation between two or more sets of
random variables.

\[
\text{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
\]

From the covariance, we can derive the correlation

\begin{equation}\phantomsection\label{eq-corrxy}{
\text{Corr}(X,Y) = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}} = \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y}.
}\end{equation}

For a vector of random variables

\[
Y = 
\begin{pmatrix}
(Y^{(l)}, \ldots, Y^{(n)})
\end{pmatrix}^T
\]

the covariance matrix is a matrix of covariances between the random
variables

\[
\Sigma =
\text{Cov}(Y, Y) =
\begin{pmatrix}
\text{Cov}(Y^{(1)}, Y^{(1)}) & \ldots & \text{Cov}(Y^{(1)}, Y^{(n)}) \\
\vdots & \ddots & \vdots \\
\text{Cov}(Y^{(n)}, Y^{(1)}) & \ldots & \text{Cov}(Y^{(n)}, Y^{(n)})
\end{pmatrix},
\]

and from Equation~\ref{eq-corrxy}

\[
\text{Cov}(Y, Y) = \sigma_Y^2 \text{Cor}(Y, Y).
\]

\end{tcolorbox}

You can compute the correlation matrix \(\Psi\) from a covariance matrix
\(\Sigma\) in Python using the numpy library. The correlation matrix is
computed by dividing each element of the covariance matrix by the
product of the standard deviations of the corresponding variables.

The function \texttt{covariance\_to\_correlation} first computes the
standard deviations of the variables with
\texttt{np.sqrt(np.diag(cov))}. It then computes the correlation matrix
by dividing each element of the covariance matrix by the product of the
standard deviations of the corresponding variables with
\texttt{cov\ /\ np.outer(std\_devs,\ std\_devs)}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ covariance\_to\_correlation(cov):}
    \CommentTok{\# Compute standard deviations}
\NormalTok{    std\_devs }\OperatorTok{=}\NormalTok{ np.sqrt(np.diag(cov))}
    
    \CommentTok{\# Compute correlation matrix}
\NormalTok{    corr }\OperatorTok{=}\NormalTok{ cov }\OperatorTok{/}\NormalTok{ np.outer(std\_devs, std\_devs)}
    
    \ControlFlowTok{return}\NormalTok{ corr}

\NormalTok{cov }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\OperatorTok{{-}}\DecValTok{4}\NormalTok{], [}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(covariance\_to\_correlation(cov))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 1.         -0.44444444]
 [-0.44444444  1.        ]]
\end{verbatim}

\subsection{The Kriging Model}\label{the-kriging-model}

Consider sample data \(\vec{X}\) and \(\vec{y}\) from \(n\) locations
that are available in matrix form: \(\vec{X}\) is a \((n \times k)\)
matrix, where \(k\) denotes the problem dimension and \(\vec{y}\) is a
\((n\times 1)\) vector.

The observed responses \(\vec{y}\) are considered as if they are from a
stochastic process, which will be denoted as \[
\begin{pmatrix}
\vec{Y}(\vec{x}^{(1)})\\
\vdots\\
\vec{Y}(\vec{x}^{(n)})\\
\end{pmatrix}.
\]

The set of random vectors (also referred to as a \emph{random field})
has a mean of \(\vec{1} \mu\), which is a \((n\times 1)\) vector.

\subsection{Correlations}\label{correlations}

The random vectors are correlated with each other using the basis
function expression from Equation~\ref{eq-krigingbase}: \[
\text{cor} \left(\vec{Y}(\vec{x}^{(i)}),\vec{Y}(\vec{x}^{(l)}) \right) = \exp\left\{ - \sum_{j=1}^k \theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\right\}.
\]

The \((n \times n)\) correlation matrix of the observed sample data is

\[
\vec{\Psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)}) 
\right) & \ldots &
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)}) 
\right)\\
\vdots  & \vdots &  \vdots\\
 \text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)}) 
\right)&
\ldots &
\text{cor}\left(
\vec{Y}(\vec{x}^{(i)}),
\vec{Y}(\vec{x}^{(l)}) 
\right)
\end{pmatrix}.
\]

Note: correlations depend on the absolute distances between sample
points \(|x_j^{(n)} - x_j^{(n)}|\) and the parameters \(p_j\) and
\(\theta_j\).

Correlation is intuitive, because when two points move close together,
then \(|x_l^{(i)} - x_l| \to 0\) and \(\exp(-|x_l^{(i)} - x_l| \to 1\),
points show very close correlation and \(Y(x_l^{(i)}) = Y(x_l)\).

\(\theta\) can be seen as a width parameter:

\begin{itemize}
\tightlist
\item
  low \(\theta_j\) means that all points will have a high correlation,
  with \(Y(x_j)\) being similar across the sample.
\item
  high \(\theta_j\) means that there is a significant difference between
  the \(Y(x_j)\)'s.
\item
  \(\theta_j\) is a measure of how active the function we are
  approximating is.
\item
  High \(\theta_j\) indicate important parameters, see
  Figure~\ref{fig-theta12}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_inverse\_exp\_squared\_distance(}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, theta\_values}\OperatorTok{=}\NormalTok{[}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{2.0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{006_num_gp_files/figure-pdf/fig-theta12-output-1.pdf}

}

\caption{\label{fig-theta12}Theta set to 1/2, 1, and 2}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The Correlation Matrix (Detailed Computation)}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Let \(n=4\) and \(k=3\). The sample plan is represented by the following
matrix \(X\): \[
X = \begin{pmatrix} x_{11} & x_{12} & x_{13}\\
x_{21} & x_{22} & x_{23}\\
x_{31} & x_{32} & x_{33}\\
x_{41} & x_{42} & x_{43}\\ 
\end{pmatrix}
\]

To compute the elements of the matrix \(\Psi\), the following \(k\) (one
for each of the \(k\) dimensions) \((n,n)\)-matrices have to be
computed: \[
D_1 = \begin{pmatrix} x_{11} - x_{11} & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  x_{21} - x_{11} & x_{21} - x_{21} & x_{21} -x_{31} & x_{21} - x_{41} \\ x_{31} - x_{11} & x_{31} - x_{21} & x_{31} -x_{31} & x_{31} - x_{41} \\ x_{41} - x_{11} & x_{41} - x_{21} & x_{41} -x_{31} & x_{41} - x_{41} \\
\end{pmatrix}
\]

\[
D_2 = \begin{pmatrix} x_{12} - x_{12} & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  x_{22} - x_{12} & x_{22} - x_{22} & x_{22} -x_{32} & x_{22} - x_{42} \\ x_{32} - x_{12} & x_{32} - x_{22} & x_{32} -x_{32} & x_{32} - x_{42} \\ x_{42} - x_{12} & x_{42} - x_{22} & x_{42} -x_{32} & x_{42} - x_{42} \\
\end{pmatrix}
\]

\[
D_3 = \begin{pmatrix} x_{13} - x_{13} & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  x_{23} - x_{13} & x_{23} - x_{23} & x_{23} -x_{33} & x_{23} - x_{43} \\ x_{33} - x_{13} & x_{33} - x_{23} & x_{33} -x_{33} & x_{33} - x_{43} \\ x_{43} - x_{13} & x_{43} - x_{23} & x_{43} -x_{33} & x_{43} - x_{43} \\\end{pmatrix}
\]

Since the matrices are symmetric and the main diagonals are zero, it is
sufficient to compute the following matrices: \[
D_1 = \begin{pmatrix} 0 & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  0 &  0 & x_{21} -x_{31} & x_{21} - x_{41} \\ 0 & 0 & 0 & x_{31} - x_{41} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\] \[
D_2 = \begin{pmatrix} 0 & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  0 & 0 & x_{22} -x_{32} & x_{22} - x_{42} \\ 0 & 0 & 0 & x_{32} - x_{42} \\ 0 & 0 & 0 & 0 \\
\end{pmatrix}
\]

\[
D_3 = \begin{pmatrix} 0 & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  0 & 0 & x_{23} -x_{33} & x_{23} - x_{43} \\ 0 & 0 & 0 & x_{33} - x_{43} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

We will consider \(p_l=2\). The differences will be squared and
multiplied by \(\theta_i\), i.e.:

\[
D_1 = \theta_1 \begin{pmatrix} 0 & (x_{11} - x_{21})^2 & (x_{11} -x_{31})^2 & (x_{11} - x_{41})^2 \\  0 &  0 & (x_{21} -x_{31})^2 & (x_{21} - x_{41})^2 \\ 0 & 0 & 0 & (x_{31} - x_{41})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

\[
D_2 = \theta_2 \begin{pmatrix} 0 & (x_{12} - x_{22})^2 & (x_{12} -x_{32})^2 & (x_{12} - x_{42})^2 \\  0 & 0 & (x_{22} -x_{32})^2 & (x_{22} - x_{42})^2 \\ 0 & 0 & 0 & (x_{32} - x_{42})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

\[
D_3 = \theta_3 \begin{pmatrix} 0 & (x_{13} - x_{23})^2 & (x_{13} -x_{33})^2 & (x_{13} - x_{43})^2 \\  0 & 0 & (x_{23} -x_{33})^2 & (x_{23} - x_{43})^2 \\ 0 & 0 & 0 & (x_{33} - x_{43})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

The sum of the three matrices \(D=D_1+ D_2 + D_3\) will be calculated
next:

\[
\begin{pmatrix} 0 & 
\theta_1  (x_{11} - x_{21})^2 + \theta_2 (x_{12} - x_{22})^2 + \theta_3  (x_{13} - x_{23})^2  &
\theta_1 (x_{11} -x_{31})^2 + \theta_2  (x_{12} -x_{32})^2 + \theta_3  (x_{13} -x_{33})^2 &
\theta_1  (x_{11} - x_{41})^2 + \theta_2  (x_{12} - x_{42})^2 + \theta_3 (x_{13} - x_{43})^2
\\  0 &  0 & 
\theta_1  (x_{21} -x_{31})^2 + \theta_2 (x_{22} -x_{32})^2 + \theta_3  (x_{23} -x_{33})^2 &
\theta_1  x_{21} - x_{41})^2 + \theta_2  (x_{22} - x_{42})^2 + \theta_3 (x_{23} - x_{43})^2
\\ 0 & 0 & 0 & 
\theta_1 (x_{31} - x_{41})^2 + \theta_2 (x_{32} - x_{42})^2 + \theta_3 (x_{33} - x_{43})^2
\\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

Finally, \[ \Psi = \exp(-D)\] is computed.

Next, we will demonstrate how this computation can be implemented in
Python.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ (array, zeros, power, ones, exp, multiply,}
\NormalTok{                    eye, linspace, mat, spacing, sqrt, arange,}
\NormalTok{                    append, ravel)}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([ [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{], [}\DecValTok{101}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{]])}
\NormalTok{X}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[  1,   0,   0],
       [  0,   1,   0],
       [100, 100, 100],
       [101, 100, 100]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_Psi(X, theta):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=}\NormalTok{ theta[l]}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])
\end{verbatim}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The Correlation Matrix (Using Existing Functions)}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The same result as computed in the previous example can be obtained with
existing python functions, e.g., from the package \texttt{scipy}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ squareform}
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ pdist}

\KeywordTok{def}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{ squareform(pdist(X,}
\NormalTok{                            metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{,}
\NormalTok{                            out}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{                            w}\OperatorTok{=}\NormalTok{theta))) }\OperatorTok{+}\NormalTok{  multiply(eye(X.shape[}\DecValTok{0}\NormalTok{]),}
\NormalTok{                                                   eps)}

\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\FloatTok{.0}\NormalTok{)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])
\end{verbatim}

\end{tcolorbox}

\subsection{The Condition Number}\label{the-condition-number}

A small value, \texttt{eps}, can be passed to the function
\texttt{build\_Psi} to improve the condition number. For example,
\texttt{eps=sqrt(spacing(1))} can be used. The numpy function
\texttt{spacing()} returns the distance between a number and its nearest
adjacent number.

The condition number of a matrix is a measure of its sensitivity to
small changes in its elements. It is used to estimate how much the
output of a function will change if the input is slightly altered.

A matrix with a low condition number is well-conditioned, which means
its behavior is relatively stable, while a matrix with a high condition
number is ill-conditioned, meaning its behavior is unstable with respect
to numerical precision.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a well{-}conditioned matrix (low condition number)}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{], [}\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Condition number of A: "}\NormalTok{, np.linalg.cond(A))}

\CommentTok{\# Define an ill{-}conditioned matrix (high condition number)}
\NormalTok{B }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\FloatTok{0.99999999}\NormalTok{], [}\FloatTok{0.99999999}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Condition number of B: "}\NormalTok{, np.linalg.cond(B))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Condition number of A:  1.2222222222222225
Condition number of B:  200000000.53159264
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.linalg.cond(Psi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2.163953413738652
\end{verbatim}

\subsection{\texorpdfstring{MLE to estimate \(\theta\) and
\(p\)}{MLE to estimate \textbackslash theta and p}}\label{mle-to-estimate-theta-and-p}

We know what the correlations mean, but how do we estimate the values of
\(\theta_j\) and where does our observed data \(y\) come in? To estimate
the values of \(\vec{\theta}\) and \(\vec{p}\), they are chosen to
maximize the likelihood of \(\vec{y}\), which can be expressed in terms
of the sample data
\[L\left(\vec{Y}(\vec{x}^{(1)}), \ldots, \vec{Y}(\vec{x}^{(n)}) | \mu, \sigma \right) = \frac{1}{(2\pi \sigma)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left\{ \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}\right\},\]
and formulated as the log-likelihood:
\[\ln(L) = - \frac{n}{2} \ln(2\pi \sigma) - \frac{1}{2} \ln |\vec{\Psi}| \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}.\]

Optimization of the log-likelihood by taking derivatives with respect to
\(\mu\) and \(\sigma\) results in
\[\hat{\mu} = \frac{\vec{1}^T \vec{\Psi}^{-1} \vec{y}^T}{\vec{1}^T \vec{\Psi}^{-1} \vec{1}^T}\]
and
\[\hat{\sigma} = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{n}.\]

Combining the equations leads to the concentrated log-likelihood:
\begin{equation}\phantomsection\label{eq-concentrated-loglikelihood}{\ln(L) = - \frac{n}{2} \ln(\hat{\sigma}) - \frac{1}{2} \ln |\vec{\Psi}|.}\end{equation}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: The Concentrated Log-Likelihood}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The first term in Equation~\ref{eq-concentrated-loglikelihood}
  requires information about the measured point (observations) \(y_i\).
\item
  To maximize \(\ln(L)\), optimal values of \(\vec{\theta}\) and
  \(\vec{p}\) are determined numerically, because the equation is not
  differentiable.
\end{itemize}

\end{tcolorbox}

\subsection{\texorpdfstring{Tuning \(\theta\) and
\(p\)}{Tuning \textbackslash theta and p}}\label{tuning-theta-and-p}

Optimizers such as Nelder-Mead, Conjugate Gradient, or Simulated
Annealing can be used to determine optimal values for \(\theta\) and
\(p\). After the optimization, the correlation matrix \(\Psi\) is build
with the optimized \(\theta\) and \(p\) values. This is best (most
likely) Kriging model for the given data \(y\).

\section{Kriging Prediction}\label{kriging-prediction}

\subsection{The Augmented Correlation
Matrix}\label{the-augmented-correlation-matrix}

We will use the Kriging correlation \(\Psi\) to predict new values based
on the observed data. The matrix algebra involved for calculating the
likelihood is the most computationally intensive part of the Kriging
process. Care must be taken that the computer code is as efficient as
possible.

Basic elements of the Kriging based surrogate optimization such as
interpolation, expected improvement, and regression are presented. The
presentation follows the approach described in Forrester, Sóbester, and
Keane (2008) and Bartz et al. (2022).

Main idea for prediction is that the new \(Y(\vec{x})\) should be
consistent with the old sample data \(X\). For a new prediction
\(\hat{y}\) at \(\vec{x}\), the value of \(\hat{y}\) is chosen so that
it maximizes the likelihood of the sample data \(\vec{X}\) and the
prediction, given the (optimized) correlation parameter \(\vec{\theta}\)
and \(\vec{p}\) from above. The observed data \(\vec{y}\) is augmented
with the new prediction \(\hat{y}\) which results in the augmented
vector \(\vec{\tilde{y}} = ( \vec{y}^T, \hat{y})^T\). A vector of
correlations between the observed data and the new prediction is defined
as

\[ \vec{\psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(1)}),
\vec{Y}(\vec{x}) 
\right) \\
\vdots  \\
\text{cor}\left(
\vec{Y}(\vec{x}^{(n)}),
\vec{Y}(\vec{x}) 
\right)
\end{pmatrix}
=
\begin{pmatrix}
\vec{\psi}^{(1)}\\
\vdots\\
\vec{\psi}^{(n)}
\end{pmatrix}.
\] The augmented correlation matrix is constructed as
\[ \tilde{\vec{\Psi}} =
\begin{pmatrix}
\vec{\Psi} & \vec{\psi} \\
\vec{\psi}^T & 1
\end{pmatrix}.
\]

The log-likelihood of the augmented data is \[
\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\hat{\sigma}^2) - \frac{1}{2} \ln |\vec{\hat{\Psi}}| -  \frac{(\vec{\tilde{y}} - \vec{1}\hat{\mu})^T \vec{\tilde{\Psi}}^{-1}(\vec{\tilde{y}} - \vec{1}\hat{\mu})}{2 \hat{\sigma}^2}.
\]

The MLE for \(\hat{y}\) can be calculated as
\begin{equation}\phantomsection\label{eq-mle-yhat}{
\hat{y}(\vec{x}) = \hat{\mu} + \vec{\psi}^T \vec{\tilde{\Psi}}^{-1} (\vec{y} - \vec{1}\hat{\mu}).
}\end{equation}

\subsection{Properties of the
Predictor}\label{properties-of-the-predictor}

Equation~\ref{eq-mle-yhat} reveals two important properties of the
Kriging predictor:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Basis functions: The basis function impacts the vector \(\vec{\psi}\),
  which contains the \(n\) correlations between the new point
  \(\vec{x}\) and the observed locations. Values from the \(n\) basis
  functions are added to a mean base term \(\mu\) with weightings
  \(\vec{w} = \vec{\tilde{\Psi}}^{(-1)} (\vec{y} - \vec{1}\hat{\mu})\).
\item
  Interpolation: The predictions interpolate the sample data. When
  calculating the prediction at the \(i\)th sample point,
  \(\vec{x}^{(i)}\), the \(i\)th column of \(\vec{\Psi}^{-1}\) is
  \(\vec{\psi}\), and \(\vec{\psi}  \vec{\Psi}^{-1}\) is the \(i\)th
  unit vector. Hence, \(\hat{y}(\vec{x}^{(i)}) = y^{(i)}\).
\end{enumerate}

\section{Kriging Example: Sinusoid
Function}\label{kriging-example-sinusoid-function}

Toy example in 1d where the response is a simple sinusoid measured at
eight equally spaced \(x\)-locations in the span of a single period of
oscillation.

\subsection{\texorpdfstring{Calculating the Correlation Matrix
\(\Psi\)}{Calculating the Correlation Matrix \textbackslash Psi}}\label{calculating-the-correlation-matrix-psi}

The correlation matrix \(\Psi\) is based on the pairwise squared
distances between the input locations. Here we will use \(n=8\) sample
locations and \(\theta\) is set to 1.0.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{8}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\CommentTok{\# theta should be an array (of one value, for the moment, will be changed later)}
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\end{Highlighting}
\end{Shaded}

Evaluate at sample points

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(X)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{)}
\NormalTok{plt.title(}\SpecialStringTok{f"Sin(x) evaluated at }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ points"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{006_num_gp_files/figure-pdf/cell-54-output-1.pdf}

\subsection{\texorpdfstring{Computing the \(\psi\)
Vector}{Computing the \textbackslash psi Vector}}\label{computing-the-psi-vector}

Distances between testing locations \(x\) and training data locations
\(X\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ cdist}

\KeywordTok{def}\NormalTok{ build\_psi(X, x, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ x.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    theta }\OperatorTok{=}\NormalTok{ theta }\OperatorTok{*}\NormalTok{ ones(k)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ cdist(x.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{,}
\NormalTok{              out}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{              w}\OperatorTok{=}\NormalTok{theta)}
    \BuiltInTok{print}\NormalTok{(D.shape)}
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}
    \CommentTok{\# return psi transpose to be consistent with the literature}
    \ControlFlowTok{return}\NormalTok{(psi.T)}
\end{Highlighting}
\end{Shaded}

\subsection{Predicting at New
Locations}\label{predicting-at-new-locations}

We would like to predict at \(m = 100\) new locations in the interval
\([0, 2\pi]\). The new locations are stored in the variable \texttt{x}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=} \DecValTok{100}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, m, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{psi }\OperatorTok{=}\NormalTok{ build\_psi(X, x, theta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(100, 8)
\end{verbatim}

Computation of the predictive equations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U }\OperatorTok{=}\NormalTok{ cholesky(Psi).T}
\NormalTok{one }\OperatorTok{=}\NormalTok{ np.ones(n).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ (one.T.dot(solve(U, solve(U.T, y)))) }\OperatorTok{/}\NormalTok{ one.T.dot(solve(U, solve(U.T, one)))}
\NormalTok{f }\OperatorTok{=}\NormalTok{ mu }\OperatorTok{*}\NormalTok{ ones(m).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ psi.T.dot(solve(U, solve(U.T, y }\OperatorTok{{-}}\NormalTok{ one }\OperatorTok{*}\NormalTok{ mu)))}
\end{Highlighting}
\end{Shaded}

To compute \(f\), Equation~\ref{eq-mle-yhat} is used.

\subsection{Visualization}\label{visualization}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(x, f, color }\OperatorTok{=} \StringTok{"orange"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Fitted"}\NormalTok{)}
\NormalTok{plt.plot(x, np.sin(x), color }\OperatorTok{=} \StringTok{"grey"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Original"}\NormalTok{)}
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Measurements"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Kriging prediction of sin(x) with }\SpecialCharTok{\{\}}\StringTok{ points.}\CharTok{\textbackslash{}n}\StringTok{ theta: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(n, theta[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper right\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{006_num_gp_files/figure-pdf/cell-58-output-1.pdf}

\section{Cholesky Example With Two
Points}\label{cholesky-example-with-two-points}

\subsection{Cholesky Decomposition}\label{cholesky-decomposition}

We consider \(k=1\) and \(n=2\) sample points. The sample points are
located at \(x_1=1\) and \(x_2=5\). The response values are \(y_1=2\)
and \(y_2=10\). The correlation parameter is \(\theta=1\) and \(p\) is
set to \(1\). Using Equation~\ref{eq-krigingbase}, we can compute the
correlation matrix \(\Psi\):

\[
\Psi = \begin{pmatrix}
1 & e^{-1}\\
e^{-1} & 1
\end{pmatrix}.
\]

To determine MLE as in Equation~\ref{eq-mle-yhat}, we need to compute
\(\Psi^{-1}\):

\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e & -1\\
-1 & e
\end{pmatrix}.
\]

Cholesky-decomposition of \(\Psi\) is recommended to compute
\(\Psi^{-1}\). Cholesky decomposition is a decomposition of a positive
definite symmetric matrix into the product of a lower triangular matrix
\(L\), a diagonal matrix \(D\) and the transpose of \(L\), which is
denoted as \(L^T\). Consider the following example:

\[
LDL^T=
\begin{pmatrix}
1 & 0 \\
l_{21} & 1
\end{pmatrix}
\begin{pmatrix}
d_{11} & 0 \\
0 & d_{22}
\end{pmatrix}
\begin{pmatrix}
1 & l_{21} \\
0 & 1
\end{pmatrix}=
\]

\begin{equation}\phantomsection\label{eq-cholex}{
\begin{pmatrix}
d_{11} & 0 \\
d_{11} l_{21} & d_{22}
\end{pmatrix}
\begin{pmatrix}
1 & l_{21} \\
0 & 1
\end{pmatrix}
=
\begin{pmatrix}
d_{11} & d_{11} l_{21} \\
d_{11} l_{21} & d_{11} l_{21}^2 + d_{22}
\end{pmatrix}.
}\end{equation}

Using Equation~\ref{eq-cholex}, we can compute the Cholesky
decomposition of \(\Psi\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(d_{11} = 1\),
\item
  \(l_{21}d_{11} = e^{-1} \Rightarrow l_{21} = e^{-1}\), and
\item
  \(d_{11} l_{21}^2 + d_{22} = 1 \Rightarrow d_{22} = 1 - e^{-2}\).
\end{enumerate}

The Cholesky decomposition of \(\Psi\) is \[
\Psi = \begin{pmatrix}
1 & 0\\
e^{-1} & 1\\
\end{pmatrix}
\begin{pmatrix}
1 & 0\\
0 & 1 - e^{-2}\\
\end{pmatrix}
\begin{pmatrix}
1 & e^{-1}\\
0 & 1\\
\end{pmatrix}
= LDL^T\]

Some programs use \(U\) instead of \(L\). The Cholesky decomposition of
\(\Psi\) is \[
\Psi = LDL^T = U^TDU.
\]

Using \[
\sqrt{D} =\begin{pmatrix}
1 & 0\\
0 & \sqrt{1 - e^{-2}}\\
\end{pmatrix},
\] we can write the Cholesky decomposition of \(\Psi\) without a
diagonal matrix \(D\) as \[
\Psi = \begin{pmatrix}
1 & 0\\
e^{-1} & \sqrt{1 - e^{-2}}\\
\end{pmatrix}
\begin{pmatrix}
1 & e^{-1}\\
0 & \sqrt{1 - e^{-2}}\\
\end{pmatrix}
= U^TU.
\]

\subsection{Computation of the Inverse
Matrix}\label{computation-of-the-inverse-matrix}

To compute the inverse of a matrix using the Cholesky decomposition, you
can follow these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decompose the matrix \(A\) into \(L\) and \(L^T\), where \(L\) is a
  lower triangular matrix and \(L^T\) is the transpose of \(L\).
\item
  Compute \(L^{-1}\), the inverse of \(L\).
\item
  The inverse of \(A\) is then \((L^{-1})^T  L^-1\).
\end{enumerate}

Please note that this method only applies to symmetric,
positive-definite matrices.

The inverse of the matrix \(\Psi\) from above is:

\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e & -1\\
-1 & e
\end{pmatrix}.
\]

Here's an example of how to compute the inverse of a matrix using
Cholesky decomposition in Python:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ cholesky, inv}
\NormalTok{E }\OperatorTok{=}\NormalTok{ np.exp(}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Psi is a symmetric, positive{-}definite matrix }
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\NormalTok{E], [}\DecValTok{1}\OperatorTok{/}\NormalTok{E, }\DecValTok{1}\NormalTok{]])}
\NormalTok{L }\OperatorTok{=}\NormalTok{ cholesky(Psi, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{L\_inv }\OperatorTok{=}\NormalTok{ inv(L)}
\CommentTok{\# The inverse of A is (L\^{}{-}1)\^{}T * L\^{}{-}1}
\NormalTok{Psi\_inv }\OperatorTok{=}\NormalTok{ np.dot(L\_inv.T, L\_inv)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Psi:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Psi)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Psi Inverse:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Psi\_inv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Psi:
 [[1.         0.36787944]
 [0.36787944 1.        ]]
Psi Inverse:
 [[ 1.15651764 -0.42545906]
 [-0.42545906  1.15651764]]
\end{verbatim}

\section{Jupyter Notebook}\label{jupyter-notebook-5}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_num_gp.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Introduction to spotPython}\label{sec-spot}

Surrogate model based optimization methods are common approaches in
simulation and optimization. SPOT was developed because there is a great
need for sound statistical analysis of simulation and optimization
algorithms. SPOT includes methods for tuning based on classical
regression and analysis of variance techniques. It presents tree-based
models such as classification and regression trees and random forests as
well as Bayesian optimization (Gaussian process models, also known as
Kriging). Combinations of different meta-modeling approaches are
possible. SPOT comes with a sophisticated surrogate model based
optimization method, that can handle discrete and continuous inputs.
Furthermore, any model implemented in \texttt{scikit-learn} can be used
out-of-the-box as a surrogate in \texttt{spotPython}.

SPOT implements key techniques such as exploratory fitness landscape
analysis and sensitivity analysis. It can be used to understand the
performance of various algorithms, while simultaneously giving insights
into their algorithmic behavior.

The \texttt{spot} loop consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Init: Build initial design \(X\)
\item
  Evaluate initial design on real objective \(f\): \(y = f(X)\)
\item
  Build surrogate: \(S = S(X,y)\)
\item
  Optimize on surrogate: \(X_0 =  \text{optimize}(S)\)
\item
  Evaluate on real objective: \(y_0 = f(X_0)\)
\item
  Impute (Infill) new points: \(X = X \cup X_0\), \(y = y \cup y_0\).
\item
  Goto 3.
\end{enumerate}

Central Idea: Evaluation of the surrogate model \texttt{S} is much
cheaper (or / and much faster) than running the real-world experiment
\(f\). We start with a small example.

\section{\texorpdfstring{Example: \texttt{Spot} and the Sphere
Function}{Example: Spot and the Sphere Function}}\label{example-spot-and-the-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function:
Sphere}\label{the-objective-function-sphere}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[
f(x) = x^2
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

We can apply the function \texttt{fun} to input values and plot the
result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x, y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{007_num_spot_intro_files/figure-pdf/cell-4-output-1.pdf}

\subsection{\texorpdfstring{The \texttt{Spot} Method as an Optimization
Algorithm Using a Surrogate
Model}{The Spot Method as an Optimization Algorithm Using a Surrogate Model}}\label{the-spot-method-as-an-optimization-algorithm-using-a-surrogate-model}

We initialize the \texttt{fun\_control} dictionary. The
\texttt{fun\_control} dictionary contains the parameters for the
objective function. The \texttt{fun\_control} dictionary is passed to
the \texttt{Spot} method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                     upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]))}
\NormalTok{spot\_0 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{spot\_0.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.2026789271012512e-09 [#######---] 73.33% 
spotPython tuning: 1.2026789271012512e-09 [########--] 80.00% 
spotPython tuning: 1.2026789271012512e-09 [#########-] 86.67% 
spotPython tuning: 1.2026789271012512e-09 [#########-] 93.33% 
spotPython tuning: 3.7010904275056666e-10 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2ad8380d0>
\end{verbatim}

The method \texttt{print\_results()} prints the results, i.e., the best
objective function value (``min y'') and the corresponding input value
(``x0'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 3.7010904275056666e-10
x0: 1.9238218284201025e-05
\end{verbatim}

\begin{verbatim}
[['x0', 1.9238218284201025e-05]]
\end{verbatim}

To plot the search progress, the method \texttt{plot\_progress()} can be
used. The parameter \texttt{log\_y} is used to plot the objective
function values on a logarithmic scale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{007_num_spot_intro_files/figure-pdf/fig-spot-progress-output-1.pdf}

}

\caption{\label{fig-spot-progress}Visualization of the search progress
of the \texttt{Spot} method. The black elements (points and line)
represent the initial design, before the surrogate is build. The red
elements represent the search on the surrogate.}

\end{figure}%

If the dimension of the input space is one, the method
\texttt{plot\_model()} can be used to visualize the model and the
underlying objective function values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.plot\_model()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{007_num_spot_intro_files/figure-pdf/fig-spot-model-1d-output-1.pdf}

}

\caption{\label{fig-spot-model-1d}Visualization of the model and the
underlying objective function values.}

\end{figure}%

\section{\texorpdfstring{\texttt{Spot} Parameters: \texttt{fun\_evals},
\texttt{init\_size} and
\texttt{show\_models}}{Spot Parameters: fun\_evals, init\_size and show\_models}}\label{spot-parameters-fun_evals-init_size-and-show_models}

We will modify three parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The number of function evaluations (\texttt{fun\_evals}) will be set
  to \texttt{10} (instead of 15, which is the default value) in the
  \texttt{fun\_control} dictionary.
\item
  The parameter \texttt{show\_models}, which visualizes the search
  process for each single iteration for 1-dim functions, in the
  \texttt{fun\_control} dictionary.
\item
  The size of the initial design (\texttt{init\_size}) in the
  \texttt{design\_control} dictionary.
\end{enumerate}

The full list of the \texttt{Spot} parameters is shown in code reference
on GitHub, see
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/spot/spot/\#spotPython.spot.spot}{Spot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                     upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                     fun\_evals }\OperatorTok{=} \DecValTok{10}\NormalTok{,}
\NormalTok{                     show\_models }\OperatorTok{=} \VariableTok{True}\NormalTok{)               }
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\includegraphics{007_num_spot_intro_files/figure-pdf/cell-9-output-1.pdf}

\includegraphics{007_num_spot_intro_files/figure-pdf/cell-9-output-2.pdf}

\begin{verbatim}
spotPython tuning: 1.2031167009156832e-09 [##########] 100.00% Done...
\end{verbatim}

\section{Print the Results}\label{print-the-results-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.2031167009156832e-09
x0: -3.468597268227724e-05
\end{verbatim}

\begin{verbatim}
[['x0', -3.468597268227724e-05]]
\end{verbatim}

\section{Show the Progress}\label{show-the-progress-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{007_num_spot_intro_files/figure-pdf/cell-11-output-1.pdf}

\section{Visualizing the Optimization and Hyperparameter Tuning Process
with TensorBoard}\label{sec-visualizing-tensorboard-01}

\texttt{spotPython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotPython}.

First, we define an ``PREFIX'' to identify the hyperparameter tuning
process. The PREFIX is used to create a directory for the TensorBoard
files.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX }\OperatorTok{=} \StringTok{"01"}\NormalTok{,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{]))}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/01_p040025_2024-01-15_00-11-11 for SummaryWriter()
\end{verbatim}

Since the \texttt{PREFIX} is not None, \texttt{spotPython} will log the
optimization process in the TensorBoard files.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_tuner.run()}
\NormalTok{spot\_tuner.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.7705278094872058e-05 [####------] 40.00% 
spotPython tuning: 8.061545220547415e-07 [#####-----] 46.67% 
spotPython tuning: 7.385022589686283e-07 [#####-----] 53.33% 
spotPython tuning: 3.677917685242894e-07 [######----] 60.00% 
spotPython tuning: 4.911502304103013e-09 [#######---] 66.67% 
spotPython tuning: 4.911502304103013e-09 [#######---] 73.33% 
spotPython tuning: 4.911502304103013e-09 [########--] 80.00% 
spotPython tuning: 4.911502304103013e-09 [#########-] 86.67% 
spotPython tuning: 4.911502304103013e-09 [#########-] 93.33% 
spotPython tuning: 4.911502304103013e-09 [##########] 100.00% Done...

min y: 4.911502304103013e-09
x0: -7.008211115615035e-05
\end{verbatim}

\begin{verbatim}
[['x0', -7.008211115615035e-05]]
\end{verbatim}

Now we can start TensorBoard in the background. The TensorBoard process
will read the TensorBoard files and visualize the hyperparameter tuning
process. From the terminal, we can start TensorBoard with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\texttt{logdir} is the directory where the TensorBoard files are stored.
In our case, the TensorBoard files are stored in the directory
\texttt{./runs}.

TensorBoard will start a web server on port 6006. We can access the
TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The first TensorBoard visualization shows the objective function values
plotted against the wall time. The wall time is the time that has passed
since the start of the hyperparameter tuning process. The five initial
design points are shown in the upper left region of the plot. The line
visualizes the optimization process.
\includegraphics{figures_static/01_tensorboard_01.png}

The second TensorBoard visualization shows the input values, i.e.,
\(x_0\), plotted against the wall time.
\includegraphics{figures_static/01_tensorboard_02.png}

The third TensorBoard plot illustrates how \texttt{spotPython} can be
used as a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures_static/01_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotPython process.}

\end{figure}%

\section{Jupyter Notebook}\label{jupyter-notebook-6}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/007_num_spot_intro.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Multi-dimensional Functions}\label{sec-multi-dim}

This chapter illustrates how high-dimensional functions can be optimized
and analyzed.

\section{\texorpdfstring{Example: \texttt{Spot} and the 3-dim Sphere
Function}{Example: Spot and the 3-dim Sphere Function}}\label{example-spot-and-the-3-dim-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: 3-dim
Sphere}\label{the-objective-function-3-dim-sphere}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[
f(x) = \sum_i^k x_i^2.
\]

It is avaliable as \texttt{fun\_sphere} in the \texttt{analytical} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/objectivefunctions.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

Here we will use problem dimension \(k=3\), which can be specified by
the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
array determines the problem dimension. If we select
\texttt{-1.0\ *\ np.ones(3)}, a three-dimensional function is created.
In contrast to the one-dimensional case
(Section~\ref{sec-visualizing-tensorboard-01}), where only one
\texttt{theta} value was used, we will use three different
\texttt{theta} values (one for each dimension), i.e., we set
\texttt{n\_theta=3} in the \texttt{surrogate\_control}. The prefix is
set to \texttt{"03"} to distinguish the results from the one-dimensional
case. Again, TensorBoard can be used to monitor the progress of the
optimization.

We can also add interpreable labels to the dimensions, which will be
used in the plots. Therefore, we set
\texttt{var\_name={[}"Pressure",\ "Temp",\ "Lambda"{]}} instead of the
default \texttt{var\_name=None}, which would result in the labels
\texttt{x\_0}, \texttt{x\_1}, and \texttt{x\_2}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"03"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{1.0}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{              var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{"Pressure"}\NormalTok{, }\StringTok{"Temp"}\NormalTok{, }\StringTok{"Lambda"}\NormalTok{],}
\NormalTok{              show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{spot\_3 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_3.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/03_p040025_2024-01-15_00-11-36 for SummaryWriter()
spotPython tuning: 0.03443324167631616 [#######---] 73.33% 
spotPython tuning: 0.03134655155643102 [########--] 80.00% 
spotPython tuning: 0.0009630181526749273 [#########-] 86.67% 
spotPython tuning: 8.570154459856623e-05 [#########-] 93.33% 
spotPython tuning: 6.496172516667557e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2d415e910>
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{Results}\label{results}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_3.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 6.496172516667557e-05
Pressure: 0.005280070995399376
Temp: 0.0019490323308060742
Lambda: 0.005769215581315232
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-6-output-1.pdf}

\subsection{A Contour Plot}\label{a-contour-plot}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

We have specified identical \texttt{min\_z} and \texttt{max\_z} values
to generate comparable plots.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-7-output-1.pdf}

\begin{itemize}
\tightlist
\item
  In a similar manner, we can plot dimension \(i=0\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-8-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The final combination is \(i=1\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{1}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-9-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The three plots look very similar, because the \texttt{fun\_sphere} is
  symmetric.
\item
  This can also be seen from the variable importance:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_3.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pressure:  95.79368533570627
Temp:  99.99999999999999
Lambda:  87.19542775477797
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-11-output-1.pdf}

\subsection{TensorBoard}\label{tensorboard-2}

\begin{figure}[H]

{\centering \includegraphics{figures_static/02_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython process. Objective
function values plotted against wall time.}

\end{figure}%

The second TensorBoard visualization shows the input values, i.e.,
\(x_0, \ldots, x_2\), plotted against the wall time.
\includegraphics{figures_static/02_tensorboard_02.png}

The third TensorBoard plot illustrates how \texttt{spotPython} can be
used as a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/02_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotPython surrogate model.}

\end{figure}%

\section{Conclusion}\label{conclusion}

Based on this quick analysis, we can conclude that all three dimensions
are equally important (as expected, because the analytical function is
known).

\section{Exercises}\label{exercises-2}

\subsection{\texorpdfstring{1. The Three Dimensional
\texttt{fun\_cubed}}{1. The Three Dimensional fun\_cubed}}\label{the-three-dimensional-fun_cubed}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-1 \leq x \leq 1\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{\texorpdfstring{2. The Ten Dimensional
\texttt{fun\_wing\_wt}}{2. The Ten Dimensional fun\_wing\_wt}}\label{the-ten-dimensional-fun_wing_wt}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{10}. The search range is
  \(0 \leq x \leq 1\) for all dimensions.
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \item
    Generate contour plots for the three most important variables. Do
    they confirm your selection?
  \end{itemize}
\end{itemize}

\subsection{\texorpdfstring{3. The Three Dimensional
\texttt{fun\_runge}}{3. The Three Dimensional fun\_runge}}\label{the-three-dimensional-fun_runge}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-5 \leq x \leq 5\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{\texorpdfstring{4. The Three Dimensional
\texttt{fun\_linear}}{4. The Three Dimensional fun\_linear}}\label{the-three-dimensional-fun_linear}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-5 \leq x \leq 5\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{\texorpdfstring{5. The Two Dimensional Rosenbrock Function
\texttt{fun\_rosen}}{5. The Two Dimensional Rosenbrock Function fun\_rosen}}\label{sec-08-exer-rosenbrock}

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{2}. The search range is
  \(-5 \leq x \leq 10\) for all dimensions.
\item
  See
  \href{https://en.wikipedia.org/wiki/Rosenbrock_function}{Rosenbrock
  function} and
  \href{https://www.sfu.ca/~ssurjano/rosen.html}{Rosenbrock Function}
  for details.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\section{Selected Solutions}\label{selected-solutions}

\subsection{\texorpdfstring{Solution to Exercise
Section~\ref{sec-08-exer-rosenbrock}: The Two-dimensional Rosenbrock
Function
\texttt{fun\_rosen}}{Solution to Exercise Section~: The Two-dimensional Rosenbrock Function fun\_rosen}}\label{solution-to-exercise-sec-08-exer-rosenbrock-the-two-dimensional-rosenbrock-function-fun_rosen}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

\subsubsection{The Objective Function: 2-dim
fun\_rosen}\label{the-objective-function-2-dim-fun_rosen}

The \texttt{spotPython} package provides several classes of objective
functions. We will use the \texttt{fun\_rosen} in the
\texttt{analytical} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/objectivefunctions.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_rosen }\OperatorTok{=}\NormalTok{ analytical().fun\_rosen}
\end{Highlighting}
\end{Shaded}

Here we will use problem dimension \(k=2\), which can be specified by
the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
array determines the problem dimension. If we select
\texttt{-5.0\ *\ np.ones(2)}, a two-dimensional function is created. In
contrast to the one-dimensional case, where only one \texttt{theta}
value is used, we will use \(k\) different \texttt{theta} values (one
for each dimension), i.e., we set \texttt{n\_theta=3} in the
\texttt{surrogate\_control}. The prefix is set to \texttt{"ROSEN"}.
Again, TensorBoard can be used to monitor the progress of the
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"ROSEN"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{5.0}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{2}\NormalTok{),}
\NormalTok{              upper }\OperatorTok{=} \DecValTok{10}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{2}\NormalTok{),}
\NormalTok{              show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{              fun\_evals}\OperatorTok{=}\DecValTok{25}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{spot\_rosen }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun\_rosen,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_rosen.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/ROSEN_p040025_2024-01-15_00-11-38 for SummaryWriter()
spotPython tuning: 90.7801015955818 [####------] 44.00% 
spotPython tuning: 1.0172832635943474 [#####-----] 48.00% 
spotPython tuning: 1.0172832635943474 [#####-----] 52.00% 
spotPython tuning: 1.0172832635943474 [######----] 56.00% 
spotPython tuning: 1.0172832635943474 [######----] 60.00% 
spotPython tuning: 1.0172832635943474 [######----] 64.00% 
spotPython tuning: 1.0172832635943474 [#######---] 68.00% 
spotPython tuning: 1.0172832635943474 [#######---] 72.00% 
spotPython tuning: 1.0172832635943474 [########--] 76.00% 
spotPython tuning: 1.0172832635943474 [########--] 80.00% 
spotPython tuning: 0.9921822630967522 [########--] 84.00% 
spotPython tuning: 0.7147779101762312 [#########-] 88.00% 
spotPython tuning: 0.7147779101762312 [#########-] 92.00% 
spotPython tuning: 0.7147779101762312 [##########] 96.00% 
spotPython tuning: 0.7147779101762312 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2d4bd3e10>
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsubsection{Results}\label{results-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.7147779101762312
x0: 0.19951670458655138
x1: 0.1258327277797004
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-16-output-1.pdf}

\subsubsection{A Contour Plot}\label{a-contour-plot-1}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

For higher dimensions, it might be useful to have identical
\texttt{min\_z} and \texttt{max\_z} values to generate comparable plots.
The default values are \texttt{min\_z=None} and \texttt{max\_z=None},
which will be replaced by the minimum and maximum values of the
objective function.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{max\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{spot\_rosen.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-17-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The variable importance can be calculated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0:  100.0
x1:  1.2641431841859785
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\includegraphics{008_num_spot_multidim_files/figure-pdf/cell-19-output-1.pdf}

\subsubsection{TensorBoard}\label{tensorboard-3}

TBD

\section{Jupyter Notebook}\label{jupyter-notebook-7}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/008_num_spot_multidim.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Isotropic and Anisotropic Kriging}\label{sec-iso-aniso-kriging}

This chapter illustrates the difference between isotropic and
anisotropic Kriging models. The difference is illustrated with the help
of the \texttt{spotPython} package. Isotropic Kriging models use the
same \texttt{theta} value for every dimension. Anisotropic Kriging
models use different \texttt{theta} values for each dimension.

\section{\texorpdfstring{Example: Isotropic \texttt{Spot} Surrogate and
the 2-dim Sphere
Function}{Example: Isotropic Spot Surrogate and the 2-dim Sphere Function}}\label{sec-spot-2d-sphere-iso}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"003"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: 2-dim
Sphere}\label{the-objective-function-2-dim-sphere}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula:

\[
f(x, y) = x^2 + y^2
\] The size of the \texttt{lower} bound vector determines the problem
dimension. Here we will use \texttt{np.array({[}-1,\ -1{]})}, i.e., a
two-dimensional function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{                               lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                               upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/003_p040025_2024-01-15_00-12-02 for SummaryWriter()
\end{verbatim}

Although the default \texttt{spot} surrogate model is an isotropic
Kriging model, we will explicitly set the \texttt{n\_theta} parameter to
a value of \texttt{1}, so that the same theta value is used for both
dimensions. This is done to illustrate the difference between isotropic
and anisotropic Kriging models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}

\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.801603872454505e-05 [#######---] 73.33% 
spotPython tuning: 1.801603872454505e-05 [########--] 80.00% 
spotPython tuning: 1.801603872454505e-05 [#########-] 86.67% 
spotPython tuning: 1.801603872454505e-05 [#########-] 93.33% 
spotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2cc395690>
\end{verbatim}

\subsection{Results}\label{results-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.801603872454505e-05
x0: 0.0019077911677074135
x1: 0.003791618596979743
\end{verbatim}

\begin{verbatim}
[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-7-output-1.pdf}

\section{Example With Anisotropic
Kriging}\label{example-with-anisotropic-kriging}

As described in Section~\ref{sec-spot-2d-sphere-iso}, the default
parameter setting of \texttt{spotPython}'s Kriging surrogate uses the
same \texttt{theta} value for every dimension. This is referred to as
``using an isotropic kernel''. If different \texttt{theta} values are
used for each dimension, then an anisotropic kernel is used. To enable
anisotropic models in \texttt{spotPython}, the number of \texttt{theta}
values should be larger than one. We can use
\texttt{surrogate\_control=surrogate\_control\_init(n\_theta=2)} to
enable this behavior (\texttt{2} is the problem dimension).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{spot\_2\_anisotropic }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_anisotropic.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.783225688095949e-05 [#######---] 73.33% 
spotPython tuning: 1.783225688095949e-05 [########--] 80.00% 
spotPython tuning: 1.783225688095949e-05 [#########-] 86.67% 
spotPython tuning: 3.0185289245739795e-06 [#########-] 93.33% 
spotPython tuning: 3.0185289245739795e-06 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2cd091750>
\end{verbatim}

The search progress of the optimization with the anisotropic model can
be visualized:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-9-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 3.0185289245739795e-06
x0: -0.0001531610695200253
x1: -0.0017306272306182697
\end{verbatim}

\begin{verbatim}
[['x0', -0.0001531610695200253], ['x1', -0.0017306272306182697]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-11-output-1.pdf}

\subsection{\texorpdfstring{Taking a Look at the \texttt{theta}
Values}{Taking a Look at the theta Values}}\label{taking-a-look-at-the-theta-values}

\subsubsection{\texorpdfstring{\texttt{theta} Values from the
\texttt{spot}
Model}{theta Values from the spot Model}}\label{theta-values-from-the-spot-model}

We can check, whether one or several \texttt{theta} values were used.
The \texttt{theta} values from the surrogate can be printed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.surrogate.theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-0.29237522, -0.13253124])
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the surrogate from the isotropic setting was stored as
  \texttt{spot\_2}, we can also take a look at the \texttt{theta} value
  from this model:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-0.04189656])
\end{verbatim}

\subsubsection{TensorBoard}\label{tensorboard-4}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/03_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotPython surrogate model.}

\end{figure}%

\section{Exercises}\label{exercises-3}

\subsection{\texorpdfstring{1. The Branin Function
\texttt{fun\_branin}}{1. The Branin Function fun\_branin}}\label{the-branin-function-fun_branin}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 10\) and \(0 \leq x_2 \leq 15\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion: instead of the number of evaluations
  (which is specified via \texttt{fun\_evals}), the time should be used
  as the termination criterion. This can be done as follows
  (\texttt{max\_time=1} specifies a run time of one minute):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{              max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{2. The Two-dimensional Sin-Cos Function
\texttt{fun\_sin\_cos}}{2. The Two-dimensional Sin-Cos Function fun\_sin\_cos}}\label{the-two-dimensional-sin-cos-function-fun_sin_cos}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-2\pi \leq x_1 \leq 2\pi\) and \(-2\pi \leq x_2 \leq 2\pi\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{3. The Two-dimensional Runge Function
\texttt{fun\_runge}}{3. The Two-dimensional Runge Function fun\_runge}}\label{the-two-dimensional-runge-function-fun_runge}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 5\) and \(-5 \leq x_2 \leq 5\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{4. The Ten-dimensional Wing-Weight Function
\texttt{fun\_wingwt}}{4. The Ten-dimensional Wing-Weight Function fun\_wingwt}}\label{the-ten-dimensional-wing-weight-function-fun_wingwt}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{10}. The search ranges are between 0
    and 1 (values are mapped internally to their natural bounds).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{5. The Two-dimensional Rosenbrock Function
\texttt{fun\_rosen}}{5. The Two-dimensional Rosenbrock Function fun\_rosen}}\label{sec-09-exercise-rosen}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search ranges are between -5
    and 10.
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\section{Selected Solutions}\label{selected-solutions-1}

\subsection{\texorpdfstring{Solution to Exercise
Section~\ref{sec-09-exercise-rosen}: The Two-dimensional Rosenbrock
Function
\texttt{fun\_rosen}}{Solution to Exercise Section~: The Two-dimensional Rosenbrock Function fun\_rosen}}\label{solution-to-exercise-sec-09-exercise-rosen-the-two-dimensional-rosenbrock-function-fun_rosen}

\subsubsection{\texorpdfstring{The Two Dimensional \texttt{fun\_rosen}:
The Isotropic
Case}{The Two Dimensional fun\_rosen: The Isotropic Case}}\label{the-two-dimensional-fun_rosen-the-isotropic-case}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

The \texttt{spotPython} package provides several classes of objective
functions. We will use the \texttt{fun\_rosen} in the
\texttt{analytical} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/objectivefunctions.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_rosen }\OperatorTok{=}\NormalTok{ analytical().fun\_rosen}
\end{Highlighting}
\end{Shaded}

Here we will use problem dimension \(k=2\), which can be specified by
the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
array determines the problem dimension.

The prefix is set to \texttt{"ROSEN"} to distinguish the results from
the one-dimensional case. Again, TensorBoard can be used to monitor the
progress of the optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"ROSEN"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\OperatorTok{{-}}\DecValTok{5}\NormalTok{]),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{]),}
\NormalTok{              show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{spot\_rosen }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun\_rosen,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_rosen.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/ROSEN_p040025_2024-01-15_00-12-05 for SummaryWriter()
spotPython tuning: 52.87631878551649 [#######---] 73.33% 
spotPython tuning: 52.361867783356715 [########--] 80.00% 
spotPython tuning: 52.361867783356715 [#########-] 86.67% 
spotPython tuning: 43.44273941029301 [#########-] 93.33% 
spotPython tuning: 12.275684138292505 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2cc669690>
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\paragraph{Results}\label{results-3}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 12.275684138292505
x0: -2.3708459318321333
x1: 5.923082873674319
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-19-output-1.pdf}

\paragraph{A Contour Plot}\label{a-contour-plot-2}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{max\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{spot\_rosen.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-20-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The variable importance cannot be calculated, because only one
  \texttt{theta} value was used.
\end{itemize}

\paragraph{TensorBoard}\label{tensorboard-5}

TBD

\subsubsection{\texorpdfstring{The Two Dimensional \texttt{fun\_rosen}:
The Anisotropic
Case}{The Two Dimensional fun\_rosen: The Anisotropic Case}}\label{the-two-dimensional-fun_rosen-the-anisotropic-case}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

The \texttt{spotPython} package provides several classes of objective
functions. We will use the \texttt{fun\_rosen} in the
\texttt{analytical} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/objectivefunctions.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_rosen }\OperatorTok{=}\NormalTok{ analytical().fun\_rosen}
\end{Highlighting}
\end{Shaded}

Here we will use problem dimension \(k=2\), which can be specified by
the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
array determines the problem dimension.

We can also add interpreable labels to the dimensions, which will be
used in the plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"ROSEN"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\OperatorTok{{-}}\DecValTok{5}\NormalTok{]),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{]),}
\NormalTok{              show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{spot\_rosen }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun\_rosen,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_rosen.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/ROSEN_p040025_2024-01-15_00-12-06 for SummaryWriter()
spotPython tuning: 90.7801015955818 [#######---] 73.33% 
spotPython tuning: 1.0172832635943474 [########--] 80.00% 
spotPython tuning: 1.0172832635943474 [#########-] 86.67% 
spotPython tuning: 1.0172832635943474 [#########-] 93.33% 
spotPython tuning: 1.0172832635943474 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2d407f190>
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\paragraph{Results}\label{results-4}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.0172832635943474
x0: 0.0028122200003174065
x1: -0.04784582169505708
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-25-output-1.pdf}

\paragraph{A Contour Plot}\label{a-contour-plot-3}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{max\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{spot\_rosen.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-26-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The variable importance can be calculated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0:  100.0
x1:  2.2276773313197755
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\includegraphics{009_num_spot_anisotropic_files/figure-pdf/cell-28-output-1.pdf}

\paragraph{TensorBoard}\label{tensorboard-6}

TBD

\section{Jupyter Notebook}\label{jupyter-notebook-8}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/009_num_spot_anisotropic.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{\texorpdfstring{Using \texttt{sklearn} Surrogates in
\texttt{spotPython}}{Using sklearn Surrogates in spotPython}}\label{sec-sklearn-surrogates}

Besides the internal kriging surrogate, which is used as a default by
\texttt{spotPython}, any surrogate model from \texttt{scikit-learn} can
be used as a surrogate in \texttt{spotPython}. This chapter explains how
to use \texttt{scikit-learn} surrogates in \texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Example: Branin Function with
\texttt{spotPython}'s Internal Kriging
Surrogate}{Example: Branin Function with spotPython's Internal Kriging Surrogate}}\label{example-branin-function-with-spotpythons-internal-kriging-surrogate}

\subsection{The Objective Function
Branin}\label{the-objective-function-branin-1}

\begin{itemize}
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula.
\item
  Here we will use the Branin function:

\begin{verbatim}
  y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,
  where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2),
  c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).
\end{verbatim}
\item
  It has three global minima:

\begin{verbatim}
  f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).
\end{verbatim}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"04"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf)}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/04_p040025_2024-01-15_00-12-47 for SummaryWriter()
\end{verbatim}

\end{tcolorbox}

\subsection{\texorpdfstring{Running the surrogate model based optimizer
\texttt{Spot}:}{Running the surrogate model based optimizer Spot:}}\label{running-the-surrogate-model-based-optimizer-spot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.146824136952164 [######----] 55.00% 
spotPython tuning: 3.146824136952164 [######----] 60.00% 
spotPython tuning: 3.146824136952164 [######----] 65.00% 
spotPython tuning: 3.146824136952164 [#######---] 70.00% 
spotPython tuning: 1.1487233101571483 [########--] 75.00% 
spotPython tuning: 1.0236891516766402 [########--] 80.00% 
spotPython tuning: 0.41994270072214057 [########--] 85.00% 
spotPython tuning: 0.40193544341108023 [#########-] 90.00% 
spotPython tuning: 0.3991519598268951 [##########] 95.00% 
spotPython tuning: 0.3991519598268951 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2d2596590>
\end{verbatim}

\subsection{TensorBoard}\label{tensorboard-8}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/04_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}%

\subsection{Print the Results}\label{print-the-results-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.3991519598268951
x0: 3.1546575195040987
x1: 2.285931113926263
\end{verbatim}

\begin{verbatim}
[['x0', 3.1546575195040987], ['x1', 2.285931113926263]]
\end{verbatim}

\subsection{Show the Progress and the
Surrogate}\label{show-the-progress-and-the-surrogate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-8-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-9-output-1.pdf}

\section{Example: Using Surrogates From
scikit-learn}\label{example-using-surrogates-from-scikit-learn}

\begin{itemize}
\tightlist
\item
  Default is the \texttt{spotPython} (i.e., the internal)
  \texttt{kriging} surrogate.
\item
  It can be called explicitely and passed to \texttt{Spot}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\NormalTok{S\_0 }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Alternatively, models from \texttt{scikit-learn} can be selected,
  e.g., Gaussian Process, RBFs, Regression Trees, etc.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Here are some additional models that might be useful later:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_Tree }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{S\_LM }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_Ridge }\OperatorTok{=}\NormalTok{ linear\_model.Ridge()}
\NormalTok{S\_RF }\OperatorTok{=}\NormalTok{ RandomForestRegressor(max\_depth}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{GaussianProcessRegressor as a
Surrogate}\label{gaussianprocessregressor-as-a-surrogate}

\begin{itemize}
\tightlist
\item
  To use a Gaussian Process model from \texttt{sklearn}, that is similar
  to \texttt{spotPython}'s \texttt{Kriging}, we can proceed as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  The scikit-learn GP model \texttt{S\_GP} is selected for \texttt{Spot}
  as follows:

  \texttt{surrogate\ =\ S\_GP}
\item
  We can check the kind of surogate model with the command
  \texttt{isinstance}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S\_GP, GaussianProcessRegressor) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S\_0, Kriging)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Similar to the \texttt{Spot} run with the internal \texttt{Kriging}
  model, we can call the run with the \texttt{scikit-learn} surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\NormalTok{spot\_2\_GP }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                     surrogate }\OperatorTok{=}\NormalTok{ S\_GP)}
\NormalTok{spot\_2\_GP.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 18.865121449825782 [######----] 55.00% 
spotPython tuning: 4.06700305855078 [######----] 60.00% 
spotPython tuning: 3.461906927549384 [######----] 65.00% 
spotPython tuning: 3.461906927549384 [#######---] 70.00% 
spotPython tuning: 1.3280944252046556 [########--] 75.00% 
spotPython tuning: 0.9548334920645392 [########--] 80.00% 
spotPython tuning: 0.9344485781421579 [########--] 85.00% 
spotPython tuning: 0.39916716809341857 [#########-] 90.00% 
spotPython tuning: 0.3982254000779708 [##########] 95.00% 
spotPython tuning: 0.3982254000779708 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2d903dc10>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_GP.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-17-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.3982254000779708
x0: 3.1499822680266343
x1: 2.268811272474469
\end{verbatim}

\begin{verbatim}
[['x0', 3.1499822680266343], ['x1', 2.268811272474469]]
\end{verbatim}

\section{\texorpdfstring{Example: One-dimensional Sphere Function With
\texttt{spotPython}'s
Kriging}{Example: One-dimensional Sphere Function With spotPython's Kriging}}\label{example-one-dimensional-sphere-function-with-spotpythons-kriging}

\begin{itemize}
\tightlist
\item
  In this example, we will use an one-dimensional function, which allows
  us to visualize the optimization process.

  \begin{itemize}
  \tightlist
  \item
    \texttt{show\_models=\ True} is added to the argument list.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-1.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-2.pdf}

\begin{verbatim}
spotPython tuning: 0.03475493366922229 [####------] 40.00% 
spotPython tuning: 0.03475483461229862 [#####-----] 50.00% 
spotPython tuning: 0.03475338954992179 [######----] 60.00% 
spotPython tuning: 0.03437475313644103 [#######---] 70.00% 
spotPython tuning: 0.015290217643803946 [########--] 80.00% 
spotPython tuning: 0.0017932523576966073 [#########-] 90.00% 
spotPython tuning: 8.771851669068651e-06 [##########] 100.00% Done...
\end{verbatim}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-4.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-5.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-6.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-7.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-8.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-9.pdf}

\subsection{Results}\label{results-5}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 8.771851669068651e-06
x0: 0.002961731194600322
\end{verbatim}

\begin{verbatim}
[['x0', 0.002961731194600322]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-22-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The method \texttt{plot\_model} plots the final surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_model()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-23-output-1.pdf}

\section{\texorpdfstring{Example: \texttt{Sklearn} Model
GaussianProcess}{Example: Sklearn Model GaussianProcess}}\label{example-sklearn-model-gaussianprocess}

\begin{itemize}
\tightlist
\item
  This example visualizes the search process on the
  \texttt{GaussianProcessRegression} surrogate from \texttt{sklearn}.
\item
  Therefore \texttt{surrogate\ =\ S\_GP} is added to the argument list.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{spot\_1\_GP }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                      fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                      design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                      surrogate }\OperatorTok{=}\NormalTok{ S\_GP)}
\NormalTok{spot\_1\_GP.run()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-1.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-2.pdf}

\begin{verbatim}
spotPython tuning: 0.004925671374769521 [####------] 40.00% 
spotPython tuning: 0.002612062924748803 [#####-----] 50.00% 
spotPython tuning: 3.6666409852957783e-07 [######----] 60.00% 
spotPython tuning: 4.638244203084832e-08 [#######---] 70.00% 
spotPython tuning: 3.2711094860544125e-09 [########--] 80.00% 
spotPython tuning: 2.2493573831304313e-10 [#########-] 90.00% 
spotPython tuning: 2.2493573831304313e-10 [##########] 100.00% Done...
\end{verbatim}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-4.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-5.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-6.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-7.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-8.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-9.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.2493573831304313e-10
x0: 1.499785779079943e-05
\end{verbatim}

\begin{verbatim}
[['x0', 1.499785779079943e-05]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-26-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.plot\_model()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-27-output-1.pdf}

\section{Exercises}\label{exercises-4}

\subsection{\texorpdfstring{1. A decision tree regressor:
\texttt{DecisionTreeRegressor}}{1. A decision tree regressor: DecisionTreeRegressor}}\label{sec-10-exercise-01}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{2. A random forest regressor:
\texttt{RandomForestRegressor}}{2. A random forest regressor: RandomForestRegressor}}\label{sec-10-exercise-02}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{3. Ordinary least squares Linear Regression:
\texttt{LinearRegression}}{3. Ordinary least squares Linear Regression: LinearRegression}}\label{sec-10-exercise-03}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{4. Linear least squares with l2
regularization:
\texttt{Ridge}}{4. Linear least squares with l2 regularization: Ridge}}\label{sec-10-exercise-04}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{5. Gradient Boosting:
\texttt{HistGradientBoostingRegressor}}{5. Gradient Boosting: HistGradientBoostingRegressor}}\label{sec-10-exercise-05}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html\#sklearn.ensemble.HistGradientBoostingRegressor}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{6. Comparison of Surrogates}\label{sec-10-exercise-06}

\begin{itemize}
\item
  Use the following two objective functions

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    the 1-dim sphere function
    \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/objectivefunctions.py}{\texttt{fun\_sphere}}
    and
  \item
    the two-dim Branin function
    \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/objectivefunctions.py}{\texttt{fun\_branin}}:
  \end{enumerate}

  for a comparison of the performance of the five different surrogates:

  \begin{itemize}
  \tightlist
  \item
    \texttt{spotPython}'s internal Kriging
  \item
    \texttt{DecisionTreeRegressor}
  \item
    \texttt{RandomForestRegressor}
  \item
    \texttt{linear\_model.LinearRegression}
  \item
    \texttt{linear\_model.Ridge}.
  \end{itemize}
\item
  Generate a table with the results (number of function evaluations,
  best function value, and best parameter vector) for each surrogate and
  each function as shown in Table~\ref{tbl-results}.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1529}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0824}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1647}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1412}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2235}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1176}}@{}}
\caption{Result table}\label{tbl-results}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\texttt{surrogate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{fun}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{fun\_evals}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{max\_time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{x\_0}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{min\_y}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\texttt{surrogate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{fun}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{fun\_evals}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{max\_time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{x\_0}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{min\_y}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{Kriging} & \texttt{fun\_sphere} & 10 & \texttt{inf} & & & \\
\texttt{Kriging} & \texttt{fun\_branin} & 10 & \texttt{inf} & & & \\
\texttt{DecisionTreeRegressor} & \texttt{fun\_sphere} & 10 &
\texttt{inf} & & & \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & & & \\
\texttt{Ridge} & \texttt{fun\_branin} & 10 & \texttt{inf} & & & \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  Discuss the results. Which surrogate is the best for which function?
  Why?
\end{itemize}

\section{Selected Solutions}\label{selected-solutions-2}

\subsection{\texorpdfstring{Solution to Exercise
Section~\ref{sec-10-exercise-05}: Gradient
Boosting}{Solution to Exercise Section~: Gradient Boosting}}\label{solution-to-exercise-sec-10-exercise-05-gradient-boosting}

\subsubsection{Branin: Using SPOT}\label{branin-using-spot}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The Objective Function Branin
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"BRANIN"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf)}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/BRANIN_p040025_2024-01-15_00-13-01 for SummaryWriter()
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Running the surrogate model based optimizer \texttt{Spot}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 3.146824136952164 [######----] 55.00% 
spotPython tuning: 3.146824136952164 [######----] 60.00% 
spotPython tuning: 3.146824136952164 [######----] 65.00% 
spotPython tuning: 3.146824136952164 [#######---] 70.00% 
spotPython tuning: 1.1487233101571483 [########--] 75.00% 
spotPython tuning: 1.0236891516766402 [########--] 80.00% 
spotPython tuning: 0.41994270072214057 [########--] 85.00% 
spotPython tuning: 0.40193544341108023 [#########-] 90.00% 
spotPython tuning: 0.3991519598268951 [##########] 95.00% 
spotPython tuning: 0.3991519598268951 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2da31e490>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Print the results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.3991519598268951
x0: 3.1546575195040987
x1: 2.285931113926263
\end{verbatim}

\begin{verbatim}
[['x0', 3.1546575195040987], ['x1', 2.285931113926263]]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Show the optimization progress:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-32-output-1.pdf}

\begin{itemize}
\tightlist
\item
  Generate a surrogate model plot:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-33-output-1.pdf}

\subsubsection{Branin: Using Surrogates From
scikit-learn}\label{branin-using-surrogates-from-scikit-learn}

\begin{itemize}
\tightlist
\item
  The \texttt{HistGradientBoostingRegressor} model from
  \texttt{scikit-learn} is selected:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ HistGradientBoostingRegressor}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{S\_XGB }\OperatorTok{=}\NormalTok{ HistGradientBoostingRegressor()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The scikit-learn XGB model \texttt{S\_XGB} is selected for
  \texttt{Spot} as follows: \texttt{surrogate\ =\ S\_XGB}.
\item
  Similar to the \texttt{Spot} run with the internal \texttt{Kriging}
  model, we can call the run with the \texttt{scikit-learn} surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\NormalTok{spot\_2\_XGB }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                     surrogate }\OperatorTok{=}\NormalTok{ S\_XGB)}
\NormalTok{spot\_2\_XGB.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 30.69410528614059 [######----] 55.00% 
spotPython tuning: 30.69410528614059 [######----] 60.00% 
spotPython tuning: 30.69410528614059 [######----] 65.00% 
spotPython tuning: 30.69410528614059 [#######---] 70.00% 
spotPython tuning: 1.3263745845108854 [########--] 75.00% 
spotPython tuning: 1.3263745845108854 [########--] 80.00% 
spotPython tuning: 1.3263745845108854 [########--] 85.00% 
spotPython tuning: 1.3263745845108854 [#########-] 90.00% 
spotPython tuning: 1.3263745845108854 [##########] 95.00% 
spotPython tuning: 1.3263745845108854 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2df388610>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Print the Results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_XGB.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.3263745845108854
x0: -2.872730773493426
x1: 10.874313833535739
\end{verbatim}

\begin{verbatim}
[['x0', -2.872730773493426], ['x1', 10.874313833535739]]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Show the Progress
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_XGB.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-37-output-1.pdf}

\begin{itemize}
\tightlist
\item
  Since the \texttt{sklearn} model does not provide a \texttt{plot}
  method, we cannot generate a surrogate model plot.
\end{itemize}

\subsubsection{\texorpdfstring{One-dimensional Sphere Function With
\texttt{spotPython}'s
Kriging}{One-dimensional Sphere Function With spotPython's Kriging}}\label{one-dimensional-sphere-function-with-spotpythons-kriging}

\begin{itemize}
\tightlist
\item
  In this example, we will use an one-dimensional function, which allows
  us to visualize the optimization process.

  \begin{itemize}
  \tightlist
  \item
    \texttt{show\_models=\ True} is added to the argument list.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-1.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-2.pdf}

\begin{verbatim}
spotPython tuning: 0.03475493366922229 [####------] 40.00% 
spotPython tuning: 0.03475483461229862 [#####-----] 50.00% 
spotPython tuning: 0.03475338954992179 [######----] 60.00% 
spotPython tuning: 0.03437475313644103 [#######---] 70.00% 
spotPython tuning: 0.015290217643803946 [########--] 80.00% 
spotPython tuning: 0.0017932523576966073 [#########-] 90.00% 
spotPython tuning: 8.771851669068651e-06 [##########] 100.00% Done...
\end{verbatim}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-4.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-5.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-6.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-7.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-8.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-9.pdf}

\begin{itemize}
\tightlist
\item
  Print the Results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 8.771851669068651e-06
x0: 0.002961731194600322
\end{verbatim}

\begin{verbatim}
[['x0', 0.002961731194600322]]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Show the Progress
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-41-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The method \texttt{plot\_model} plots the final surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_model()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-42-output-1.pdf}

\subsubsection{\texorpdfstring{One-dimensional Sphere Function With
\texttt{Sklearn} Model
HistGradientBoostingRegressor}{One-dimensional Sphere Function With Sklearn Model HistGradientBoostingRegressor}}\label{one-dimensional-sphere-function-with-sklearn-model-histgradientboostingregressor}

\begin{itemize}
\tightlist
\item
  This example visualizes the search process on the
  \texttt{HistGradientBoostingRegressor} surrogate from
  \texttt{sklearn}.
\item
  Therefore \texttt{surrogate\ =\ S\_XGB} is added to the argument list.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{spot\_1\_XGB }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                      fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                      design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                      surrogate }\OperatorTok{=}\NormalTok{ S\_XGB)}
\NormalTok{spot\_1\_XGB.run()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-1.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-2.pdf}

\begin{verbatim}
spotPython tuning: 0.0018828816523185745 [####------] 40.00% 
spotPython tuning: 0.0018828816523185745 [#####-----] 50.00% 
spotPython tuning: 0.0018828816523185745 [######----] 60.00% 
spotPython tuning: 0.0018828816523185745 [#######---] 70.00% 
spotPython tuning: 0.0018828816523185745 [########--] 80.00% 
spotPython tuning: 0.0018828816523185745 [#########-] 90.00% 
spotPython tuning: 0.0018828816523185745 [##########] 100.00% Done...
\end{verbatim}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-4.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-5.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-6.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-7.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-8.pdf}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-9.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_XGB.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.0018828816523185745
x0: 0.04339218423078717
\end{verbatim}

\begin{verbatim}
[['x0', 0.04339218423078717]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_XGB.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-45-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_XGB.plot\_model()}
\end{Highlighting}
\end{Shaded}

\includegraphics{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-46-output-1.pdf}

\section{Jupyter Notebook}\label{jupyter-notebook-9}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/010_num_spot_sklearn_surrogate.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Sequential Parameter Optimization: Gaussian Process
Models}\label{sec-gaussian-process-models}

This chapter analyzes differences between the \texttt{Kriging}
implementation in \texttt{spotPython} and the
\texttt{GaussianProcessRegressor} in \texttt{scikit-learn}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ m}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Gaussian Processes Regression: Basic
Introductory \texttt{scikit-learn}
Example}{Gaussian Processes Regression: Basic Introductory scikit-learn Example}}\label{gaussian-processes-regression-basic-introductory-scikit-learn-example}

\begin{itemize}
\item
  This is the example from
  \href{https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html}{scikit-learn:
  https://scikit-learn.org/stable/auto\_examples/gaussian\_process/plot\_gpr\_noisy\_targets.html}
\item
  After fitting our model, we see that the hyperparameters of the kernel
  have been optimized.
\item
  Now, we will use our kernel to compute the mean prediction of the full
  dataset and plot the 95\% confidence interval.
\end{itemize}

\subsection{Train and Test Data}\label{train-and-test-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Building the Surrogate With
\texttt{Sklearn}}{Building the Surrogate With Sklearn}}\label{building-the-surrogate-with-sklearn}

\begin{itemize}
\tightlist
\item
  The model building with \texttt{sklearn} consisits of three steps:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Instantiating the model, then
  \item
    fitting the model (using \texttt{fit}), and
  \item
    making predictions (using \texttt{predict})
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{gaussian\_process }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{gaussian\_process.fit(X\_train, y\_train)}
\NormalTok{mean\_prediction, std\_prediction }\OperatorTok{=}\NormalTok{ gaussian\_process.predict(X, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Plotting the
\texttt{Sklearn}Model}{Plotting the SklearnModel}}\label{plotting-the-sklearnmodel}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"sk{-}learn Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-5-output-1.pdf}

\subsection{\texorpdfstring{The \texttt{spotPython}
Version}{The spotPython Version}}\label{the-spotpython-version}

\begin{itemize}
\tightlist
\item
  The \texttt{spotPython} version is very similar:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Instantiating the model, then
  \item
    fitting the model and
  \item
    making predictions (using \texttt{predict}).
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}
\NormalTok{S\_mean\_prediction, S\_std\_prediction, S\_ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, S\_mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    S\_mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ S\_std\_prediction,}
\NormalTok{    S\_mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ S\_std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"spotPython Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-7-output-1.pdf}

\subsection{\texorpdfstring{Visualizing the Differences Between the
\texttt{spotPython} and the \texttt{sklearn} Model
Fits}{Visualizing the Differences Between the spotPython and the sklearn Model Fits}}\label{visualizing-the-differences-between-the-spotpython-and-the-sklearn-model-fits}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, S\_mean\_prediction, label}\OperatorTok{=}\StringTok{"spotPython Mean prediction"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Sklearn Mean Prediction"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Comparing Mean Predictions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-8-output-1.pdf}

\section{Exercises}\label{exercises-5}

\subsection{\texorpdfstring{\texttt{Schonlau\ Example\ Function}}{Schonlau Example Function}}\label{schonlau-example-function}

\begin{itemize}
\tightlist
\item
  The Schonlau Example Function is based on sample points only (there is
  no analytical function description available):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{, }\FloatTok{2.}\NormalTok{, }\FloatTok{3.}\NormalTok{, }\FloatTok{4.}\NormalTok{, }\FloatTok{12.}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.75}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Since there is no analytical function available, you might be
  interested in adding some points and describe the effects.
\end{itemize}

\subsection{\texorpdfstring{\texttt{Forrester\ Example\ Function}}{Forrester Example Function}}\label{forrester-example-function}

\begin{itemize}
\item
  The Forrester Example Function is defined as follows:

  \texttt{f(x)\ =\ (6x-\ 2)\^{}2\ sin(12x-4)\ for\ x\ in\ {[}0,1{]}.}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\FloatTok{0.5}\NormalTok{, stop}\OperatorTok{=}\FloatTok{1.5}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.175}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.375}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X\_train, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.2}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_runge\ Function\ (1-dim)}}{fun\_runge Function (1-dim)}}\label{fun_runge-function-1-dim}

\begin{itemize}
\item
  The Runge function is defined as follows:

  \texttt{f(x)\ =\ 1/\ (1\ +\ sum(x\_i))\^{}2}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.025}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.05}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_cubed\ (1-dim)}}{fun\_cubed (1-dim)}}\label{fun_cubed-1-dim}

\begin{itemize}
\item
  The Cubed function is defined as follows:

  \texttt{np.sum(X{[}i{]}**\ 3)}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.025}\NormalTok{,}
\NormalTok{                lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{]))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotPython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.05}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.025}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{The Effect of Noise}\label{the-effect-of-noise}

How does the behavior of the \texttt{spotPython} fit changes when the
argument \texttt{noise} is set to \texttt{True}, i.e.,

\texttt{S\ =\ Kriging(name=\textquotesingle{}kriging\textquotesingle{},\ \ seed=123,\ n\_theta=1,\ noise=True)}

is used?

\chapter{Expected Improvement}\label{sec-expected-improvement}

This chapter describes, analyzes, and compares different infill
criterion. An infill criterion defines how the next point \(x_{n+1}\) is
selected from the surrogate model \(S\). Expected improvement is a
popular infill criterion in Bayesian optimization.

\section{\texorpdfstring{Example: \texttt{Spot} and the 1-dim Sphere
Function}{Example: Spot and the 1-dim Sphere Function}}\label{example-spot-and-the-1-dim-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init, design\_control\_init}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: 1-dim
Sphere}\label{the-objective-function-1-dim-sphere}

\begin{itemize}
\tightlist
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula: \[f(x) = x^2 \]
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The size of the \texttt{lower} bound vector determines the problem
  dimension.
\item
  Here we will use \texttt{np.array({[}-1{]})}, i.e., a one-dim
  function.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_Y"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),)}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-14-12 for SummaryWriter()
\end{verbatim}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(}
\NormalTok{            fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{            fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{            design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.2026789271012512e-09 [####------] 44.00% 
spotPython tuning: 1.2026789271012512e-09 [#####-----] 48.00% 
spotPython tuning: 1.2026789271012512e-09 [#####-----] 52.00% 
spotPython tuning: 1.2026789271012512e-09 [######----] 56.00% 
spotPython tuning: 3.7010904275056666e-10 [######----] 60.00% 
spotPython tuning: 3.7010904275056666e-10 [######----] 64.00% 
spotPython tuning: 3.7010904275056666e-10 [#######---] 68.00% 
spotPython tuning: 3.7010904275056666e-10 [#######---] 72.00% 
spotPython tuning: 3.7010904275056666e-10 [########--] 76.00% 
spotPython tuning: 3.7010904275056666e-10 [########--] 80.00% 
spotPython tuning: 3.7010904275056666e-10 [########--] 84.00% 
spotPython tuning: 3.7010904275056666e-10 [#########-] 88.00% 
spotPython tuning: 2.802111689321758e-11 [#########-] 92.00% 
spotPython tuning: 2.802111689321758e-11 [##########] 96.00% 
spotPython tuning: 2.802111689321758e-11 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2b44e3b50>
\end{verbatim}

\subsection{Results}\label{results-6}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.802111689321758e-11
x0: -5.293497604912803e-06
\end{verbatim}

\begin{verbatim}
[['x0', -5.293497604912803e-06]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-7-output-1.pdf}

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/07_tensorboard_Y.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}%

\section{Same, but with EI as
infill\_criterion}\label{same-but-with-ei-as-infill_criterion}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_EI\_ISO"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{    infill\_criterion }\OperatorTok{=} \StringTok{"ei"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/07_EI_ISO_p040025_2024-01-15_00-14-15 for SummaryWriter()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{spot\_1\_ei.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 9.993558891826623e-09 [####------] 44.00% 
spotPython tuning: 9.993558891826623e-09 [#####-----] 48.00% 
spotPython tuning: 9.993558891826623e-09 [#####-----] 52.00% 
spotPython tuning: 9.993558891826623e-09 [######----] 56.00% 
spotPython tuning: 3.016921825539976e-12 [######----] 60.00% 
spotPython tuning: 3.016921825539976e-12 [######----] 64.00% 
spotPython tuning: 3.016921825539976e-12 [#######---] 68.00% 
spotPython tuning: 3.016921825539976e-12 [#######---] 72.00% 
spotPython tuning: 3.016921825539976e-12 [########--] 76.00% 
spotPython tuning: 3.016921825539976e-12 [########--] 80.00% 
spotPython tuning: 3.016921825539976e-12 [########--] 84.00% 
spotPython tuning: 3.016921825539976e-12 [#########-] 88.00% 
spotPython tuning: 3.016921825539976e-12 [#########-] 92.00% 
spotPython tuning: 3.016921825539976e-12 [##########] 96.00% 
spotPython tuning: 3.016921825539976e-12 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2b46d8850>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-10-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 3.016921825539976e-12
x0: 1.7369288487269638e-06
\end{verbatim}

\begin{verbatim}
[['x0', 1.7369288487269638e-06]]
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/07_tensorboard_EI_ISO.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model. Expected improvement, isotropic
Kriging.}

\end{figure}%

\section{Non-isotropic Kriging}\label{non-isotropic-kriging}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_EI\_NONISO"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{    infill\_criterion }\OperatorTok{=} \StringTok{"ei"}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(}
\NormalTok{    n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/07_EI_NONISO_p040025_2024-01-15_00-14-17 for SummaryWriter()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_ei\_noniso.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.035369116580917e-05 [####------] 44.00% 
spotPython tuning: 2.035369116580917e-05 [#####-----] 48.00% 
spotPython tuning: 2.035369116580917e-05 [#####-----] 52.00% 
spotPython tuning: 1.0764759208059285e-05 [######----] 56.00% 
spotPython tuning: 1.0764759208059285e-05 [######----] 60.00% 
spotPython tuning: 1.2512039520452527e-07 [######----] 64.00% 
spotPython tuning: 1.2512039520452527e-07 [#######---] 68.00% 
spotPython tuning: 1.2512039520452527e-07 [#######---] 72.00% 
spotPython tuning: 1.2512039520452527e-07 [########--] 76.00% 
spotPython tuning: 1.2512039520452527e-07 [########--] 80.00% 
spotPython tuning: 1.2512039520452527e-07 [########--] 84.00% 
spotPython tuning: 1.2512039520452527e-07 [#########-] 88.00% 
spotPython tuning: 1.2512039520452527e-07 [#########-] 92.00% 
spotPython tuning: 1.2512039520452527e-07 [##########] 96.00% 
spotPython tuning: 1.2512039520452527e-07 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2b5052710>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-14-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.2512039520452527e-07
x0: -0.00023903776922459534
x1: 0.0002607323150065108
\end{verbatim}

\begin{verbatim}
[['x0', -0.00023903776922459534], ['x1', 0.0002607323150065108]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-16-output-1.pdf}

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/07_tensorboard_EI_NONISO.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model. Expected improvement, isotropic
Kriging.}

\end{figure}%

\section{\texorpdfstring{Using \texttt{sklearn}
Surrogates}{Using sklearn Surrogates}}\label{using-sklearn-surrogates}

\subsection{The spot Loop}\label{the-spot-loop}

The \texttt{spot} loop consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Init: Build initial design \(X\)
\item
  Evaluate initial design on real objective \(f\): \(y = f(X)\)
\item
  Build surrogate: \(S = S(X,y)\)
\item
  Optimize on surrogate: \(X_0 =  \text{optimize}(S)\)
\item
  Evaluate on real objective: \(y_0 = f(X_0)\)
\item
  Impute (Infill) new points: \(X = X \cup X_0\), \(y = y \cup y_0\).
\item
  Got 3.
\end{enumerate}

The \texttt{spot} loop is implemented in \texttt{R} as follows:

\begin{figure}[H]

{\centering \includegraphics{figures_static/spotModel.png}

}

\caption{Visual representation of the model based search with SPOT.
Taken from: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter
tuning approaches. In Hyperparameter Tuning for Machine and Deep
Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M.
Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch.~4, pp.~67--114.}

\end{figure}%

\subsection{spot: The Initial Model}\label{spot-the-initial-model}

\subsubsection{Example: Modifying the initial design
size}\label{example-modifying-the-initial-design-size}

This is the ``Example: Modifying the initial design size'' from Chapter
4.5.1 in {[}bart21i{]}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_ei }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                upper}\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])), }
\NormalTok{                design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{))}
\NormalTok{spot\_ei.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 0.1377171852680486 [####------] 40.00% 
spotPython tuning: 0.008763557388693657 [#####-----] 46.67% 
spotPython tuning: 0.002832279071142736 [#####-----] 53.33% 
spotPython tuning: 0.0008138662965600185 [######----] 60.00% 
spotPython tuning: 0.00036637583790222027 [#######---] 66.67% 
spotPython tuning: 0.00036006945938022686 [#######---] 73.33% 
spotPython tuning: 0.0003591078890308837 [########--] 80.00% 
spotPython tuning: 0.00032713515580249373 [#########-] 86.67% 
spotPython tuning: 0.0002785854368057176 [#########-] 93.33% 
spotPython tuning: 0.0001638494253170647 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2b4fe2890>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_ei.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-18-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.}\BuiltInTok{min}\NormalTok{(spot\_1.y), np.}\BuiltInTok{min}\NormalTok{(spot\_ei.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(2.802111689321758e-11, 0.0001638494253170647)
\end{verbatim}

\subsection{Init: Build Initial Design}\label{init-build-initial-design}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 8.97647221 13.41926847]
 [ 0.66946019  1.22344228]
 [ 5.23614115 13.78185824]
 [ 5.6149825  11.5851384 ]
 [-1.72963184  1.66516096]
 [-4.26945568  7.1325531 ]
 [ 1.26363761 10.17935555]
 [ 2.88779942  8.05508969]
 [-3.39111089  4.15213772]
 [ 7.30131231  5.22275244]]
[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975
  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{S.fit(X, y)}
\NormalTok{S.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-21-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{345}\NormalTok{)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X0, X1, X2, X3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]),
 array([[0.78373509, 0.86811887],
        [0.06692621, 0.6058029 ],
        [0.41374778, 0.00525456]]),
 array([[0.121357  , 0.69043832],
        [0.41906219, 0.32838498],
        [0.86742658, 0.52910374]]),
 array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]))
\end{verbatim}

\subsection{Evaluate}\label{evaluate}

\subsection{Build Surrogate}\label{build-surrogate}

\subsection{A Simple Predictor}\label{a-simple-predictor}

The code below shows how to use a simple model for prediction.

\begin{itemize}
\item
  Assume that only two (very costly) measurements are available:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    f(0) = 0.5
  \item
    f(2) = 2.5
  \end{enumerate}
\item
  We are interested in the value at \(x_0 = 1\), i.e., \(f(x_0 = 1)\),
  but cannot run an additional, third experiment.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{], [}\DecValTok{2}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{])}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ S\_lm.fit(X, y)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{]])}
\NormalTok{y0 }\OperatorTok{=}\NormalTok{ S\_lm.predict(X0)}
\BuiltInTok{print}\NormalTok{(y0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.5]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Central Idea:

  \begin{itemize}
  \tightlist
  \item
    Evaluation of the surrogate model \texttt{S\_lm} is much cheaper (or
    / and much faster) than running the real-world experiment \(f\).
  \end{itemize}
\end{itemize}

\section{Gaussian Processes regression: basic introductory
example}\label{gaussian-processes-regression-basic-introductory-example}

This example was taken from
\href{https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html}{scikit-learn}.
After fitting our model, we see that the hyperparameters of the kernel
have been optimized. Now, we will use our kernel to compute the mean
prediction of the full dataset and plot the 95\% confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ m}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}

\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{gaussian\_process }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{gaussian\_process.fit(X\_train, y\_train)}
\NormalTok{gaussian\_process.kernel\_}

\NormalTok{mean\_prediction, std\_prediction }\OperatorTok{=}\NormalTok{ gaussian\_process.predict(X, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"sk{-}learn Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-24-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}


\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{std\_prediction}

\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"spotPython Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-25-output-1.pdf}

\section{The Surrogate: Using scikit-learn
models}\label{the-surrogate-using-scikit-learn-models}

Default is the internal \texttt{kriging} surrogate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_0 }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Models from \texttt{scikit-learn} can be selected, e.g., Gaussian
Process:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  and many more:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_Tree }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{S\_LM }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_Ridge }\OperatorTok{=}\NormalTok{ linear\_model.Ridge()}
\NormalTok{S\_RF }\OperatorTok{=}\NormalTok{ RandomForestRegressor(max\_depth}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The scikit-learn GP model \texttt{S\_GP} is selected.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ S\_GP}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S, GaussianProcessRegressor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{15}\NormalTok{)    }
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{spot\_GP }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun, }
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate}\OperatorTok{=}\NormalTok{S, }
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_GP.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 24.51465459019188 [####------] 40.00% 
spotPython tuning: 11.003077541587748 [#####-----] 46.67% 
spotPython tuning: 11.003077541587748 [#####-----] 53.33% 
spotPython tuning: 7.281227279299504 [######----] 60.00% 
spotPython tuning: 7.281227279299504 [#######---] 66.67% 
spotPython tuning: 7.281227279299504 [#######---] 73.33% 
spotPython tuning: 2.9519489314482 [########--] 80.00% 
spotPython tuning: 2.9519489314482 [#########-] 86.67% 
spotPython tuning: 2.104972804244822 [#########-] 93.33% 
spotPython tuning: 1.9431600962086772 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2b5303410>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([ 69.32459936, 152.38491454, 107.92560483,  24.51465459,
        76.73500031,  86.30425303,  11.00307754,  16.11742138,
         7.28122728,  21.82317903,  10.96088904,   2.95194893,
         3.02910742,   2.1049728 ,   1.9431601 ])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-34-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.9431600962086772
x0: 10.0
x1: 2.9985482809555464
\end{verbatim}

\begin{verbatim}
[['x0', 10.0], ['x1', 2.9985482809555464]]
\end{verbatim}

\section{Additional Examples}\label{additional-examples}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}

\NormalTok{S\_K }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{              seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{              log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{              infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{              n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{              noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{              cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{)}

\NormalTok{spot\_S\_K }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     surrogate}\OperatorTok{=}\NormalTok{S\_K,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                     surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_S\_K.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 0.13771718778810743 [##--------] 24.00% 
spotPython tuning: 0.008768000187888899 [###-------] 28.00% 
spotPython tuning: 0.0028300907437246053 [###-------] 32.00% 
spotPython tuning: 0.0008148020998531609 [####------] 36.00% 
spotPython tuning: 0.00036681248440550095 [####------] 40.00% 
spotPython tuning: 0.00035607605553701025 [####------] 44.00% 
spotPython tuning: 0.00035607605553701025 [#####-----] 48.00% 
spotPython tuning: 0.00033033596693814263 [#####-----] 52.00% 
spotPython tuning: 0.0002774179969789593 [######----] 56.00% 
spotPython tuning: 0.00016886412273302311 [######----] 60.00% 
spotPython tuning: 2.0349536932144563e-05 [######----] 64.00% 
spotPython tuning: 1.6621220007683266e-06 [#######---] 68.00% 
spotPython tuning: 4.905822935561126e-07 [#######---] 72.00% 
spotPython tuning: 4.7634545282279014e-07 [########--] 76.00% 
spotPython tuning: 3.966290585455581e-07 [########--] 80.00% 
spotPython tuning: 1.9602185212475464e-07 [########--] 84.00% 
spotPython tuning: 1.7115221726800905e-07 [#########-] 88.00% 
spotPython tuning: 1.7115221726800905e-07 [#########-] 92.00% 
spotPython tuning: 1.7115221726800905e-07 [##########] 96.00% 
spotPython tuning: 1.7115221726800905e-07 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2bfb55f50>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-39-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-40-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.7115221726800905e-07
x0: 0.0003105897139994429
x1: 0.0002732878460995902
\end{verbatim}

\begin{verbatim}
[['x0', 0.0003105897139994429], ['x1', 0.0002732878460995902]]
\end{verbatim}

\subsection{Optimize on Surrogate}\label{optimize-on-surrogate}

\subsection{Evaluate on Real
Objective}\label{evaluate-on-real-objective}

\subsection{Impute / Infill new Points}\label{impute-infill-new-points}

\section{Tests}\label{tests}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}

\NormalTok{fun\_sphere }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{                    lower}\OperatorTok{=}\NormalTok{np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                    upper}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{                    n\_points }\OperatorTok{=} \DecValTok{2}\NormalTok{)}
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{fun\_sphere,}
\NormalTok{    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{)}

\CommentTok{\# (S{-}2) Initial Design:}
\NormalTok{spot\_1.X }\OperatorTok{=}\NormalTok{ spot\_1.design.scipy\_lhd(}
\NormalTok{    spot\_1.design\_control[}\StringTok{"init\_size"}\NormalTok{], lower}\OperatorTok{=}\NormalTok{spot\_1.lower, upper}\OperatorTok{=}\NormalTok{spot\_1.upper}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(spot\_1.X)}

\CommentTok{\# (S{-}3): Eval initial design:}
\NormalTok{spot\_1.y }\OperatorTok{=}\NormalTok{ spot\_1.fun(spot\_1.X)}
\BuiltInTok{print}\NormalTok{(spot\_1.y)}

\NormalTok{spot\_1.fit\_surrogate()}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ spot\_1.suggest\_new\_X()}
\BuiltInTok{print}\NormalTok{(X0)}
\ControlFlowTok{assert}\NormalTok{ X0.size }\OperatorTok{==}\NormalTok{ spot\_1.n\_points }\OperatorTok{*}\NormalTok{ spot\_1.k}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.86352963  0.7892358 ]
 [-0.24407197 -0.83687436]
 [ 0.36481882  0.8375811 ]
 [ 0.415331    0.54468512]
 [-0.56395091 -0.77797854]
 [-0.90259409 -0.04899292]
 [-0.16484832  0.35724741]
 [ 0.05170659  0.07401196]
 [-0.78548145 -0.44638164]
 [ 0.64017497 -0.30363301]]
[1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764
 0.15480068 0.00815134 0.81623768 0.502017  ]
[[0.00159092 0.00410652]
 [0.00190779 0.00379162]]
\end{verbatim}

\section{EI: The Famous Schonlau
Example}\label{ei-the-famous-schonlau-example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_train0 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{12}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{5}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{, }\FloatTok{2.}\NormalTok{, }\FloatTok{3.}\NormalTok{, }\FloatTok{4.}\NormalTok{, }\FloatTok{12.}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.75}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.}\NormalTok{])}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\ControlFlowTok{if} \VariableTok{True}\NormalTok{:}
\NormalTok{    plt.fill\_between(}
\NormalTok{        X.ravel(),}
\NormalTok{        mean\_prediction }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        mean\_prediction }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-44-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\CommentTok{\# plt.scatter(X\_train, y\_train, label="Observations")}
\NormalTok{plt.plot(X, }\OperatorTok{{-}}\NormalTok{ei, label}\OperatorTok{=}\StringTok{"Expected Improvement"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-45-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'negLnLike': array([1.20788205]),
 'theta': array([-0.9900252]),
 'p': [],
 'Lambda': []}
\end{verbatim}

\section{EI: The Forrester Example}\label{ei-the-forrester-example}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}

\CommentTok{\# exact x locations are unknown:}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.175}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.375}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_EI\_FORRESTER"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{1.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X\_train, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{1}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\ControlFlowTok{if} \VariableTok{True}\NormalTok{:}
\NormalTok{    plt.fill\_between(}
\NormalTok{        X.ravel(),}
\NormalTok{        mean\_prediction }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        mean\_prediction }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/07_EI_FORRESTER_p040025_2024-01-15_00-14-38 for SummaryWriter()
\end{verbatim}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-47-output-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\CommentTok{\# plt.scatter(X\_train, y\_train, label="Observations")}
\NormalTok{plt.plot(X, }\OperatorTok{{-}}\NormalTok{ei, label}\OperatorTok{=}\StringTok{"Expected Improvement"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-48-output-1.pdf}

\section{Noise}\label{noise}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_Y"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-14-39 for SummaryWriter()
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[-1.57464135 16.13714981  2.77008442 93.14904827 71.59322218 14.28895359
 15.9770567  12.96468767 39.82265329 59.88028242]
\end{verbatim}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-49-output-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'negLnLike': array([26.18505386]),
 'theta': array([-1.10547474]),
 'p': [],
 'Lambda': []}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-51-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.log}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'negLnLike': array([21.82059174]),
 'theta': array([-2.96946062]),
 'p': [],
 'Lambda': array([4.28985898e-05])}
\end{verbatim}

\section{Cubic Function}\label{cubic-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_Y"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{10.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Cubed: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-14-39 for SummaryWriter()
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[ 2.56406437e-01 -6.93071067e+01 -8.56027124e-02  8.93405931e+02
 -5.82561927e+02 -4.76028022e+01  7.16445311e+01  4.09512920e+01
  2.57319028e+02 -4.70871982e+02]
\end{verbatim}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-53-output-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{0}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Cubed: Gaussian process with nugget regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-54-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_Y"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/07_Y_p040025_2024-01-15_00-14-40 for SummaryWriter()
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[0.712453   0.05595118 0.83735691 0.0106654  0.01413372 0.07074765
 0.05479457 0.07763503 0.02412205 0.01625354]
\end{verbatim}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-55-output-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-56-output-1.pdf}

\section{Modifying Lambda Search
Space}\label{modifying-lambda-search-space}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{            min\_Lambda}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{            max\_Lambda}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Lambda: }\SpecialCharTok{\{}\NormalTok{S}\SpecialCharTok{.}\NormalTok{Lambda}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Lambda: 0.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression with nugget on noisy dataset. Modified Lambda search space."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{012_num_spot_ei_files/figure-pdf/cell-58-output-1.pdf}

\section{Factors}\label{factors}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[}\StringTok{"num"}\NormalTok{] }\OperatorTok{*} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['num', 'num', 'num']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{n }\OperatorTok{=} \DecValTok{30}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin\_factor}
\CommentTok{\#fun = analytical(sigma=0).fun\_sphere}

\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(n, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ np.random.randint(low}\OperatorTok{=}\DecValTok{1}\NormalTok{, high}\OperatorTok{=}\DecValTok{3}\NormalTok{, size}\OperatorTok{=}\NormalTok{(n,))}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.c\_[X0, X1]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X)}
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{])}
\NormalTok{S.fit(X, y)}
\NormalTok{Sf }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{, noise}\OperatorTok{=}\VariableTok{False}\NormalTok{, var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{, }\StringTok{"factor"}\NormalTok{])}
\NormalTok{Sf.fit(X, y)}
\NormalTok{n }\OperatorTok{=} \DecValTok{50}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(n, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ np.random.randint(low}\OperatorTok{=}\DecValTok{1}\NormalTok{, high}\OperatorTok{=}\DecValTok{3}\NormalTok{, size}\OperatorTok{=}\NormalTok{(n,))}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.c\_[X0, X1]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X)}
\NormalTok{s}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(S.predict(X)[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ y))}
\NormalTok{sf}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(Sf.predict(X)[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ y))}
\NormalTok{sf }\OperatorTok{{-}}\NormalTok{ s}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
-55.49075685088792
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# vars(S)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# vars(Sf)}
\end{Highlighting}
\end{Shaded}

\chapter{Handling Noise}\label{sec-noise}

This chapter demonstrates how noisy functions can be handled by
\texttt{Spot} and how noise can be simulated, i.e., added to the
objective function.

\section{\texorpdfstring{Example: \texttt{Spot} and the Noisy Sphere
Function}{Example: Spot and the Noisy Sphere Function}}\label{example-spot-and-the-noisy-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"08"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: Noisy
Sphere}\label{the-objective-function-noisy-sphere}

The \texttt{spotPython} package provides several classes of objective
functions, which return a one-dimensional output \(y=f(x)\) for a given
input \(x\) (independent variable). Several objective functions allow
one- or multidimensional input, some also combinations of real-valued
and categorial input values.

An objective function is considered as ``analytical'' if it can be
described by a closed mathematical formula, e.g., \[
f(x, y) = x^2 + y^2.
\]

To simulate measurement errors, adding artificial noise to the function
value \(y\) is a common practice, e.g.,:

\[
f(x, y) = x^2 + y^2 + \epsilon.
\]

Usually, noise is assumed to be normally distributed with mean \(\mu=0\)
and standard deviation \(\sigma\). spotPython uses numpy's
\texttt{scale} parameter, which specifies the standard deviation (spread
or ``width'') of the distribution is used. This must be a non-negative
value, see
\url{https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html}.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The sphere function without noise}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The default setting does not use any noise.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-3-output-1.pdf}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The sphere function with noise}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Noise can be added to the sphere function as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{).fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-4-output-1.pdf}

\end{tcolorbox}

\subsection{Reproducibility: Noise Generation and Seed
Handling}\label{reproducibility-noise-generation-and-seed-handling}

spotPython provides two mechanisms for generating random noise:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The seed is initialized once, i.e., when the objective function is
  instantiated. This can be done using the following call:
  \texttt{fun\ =\ analytical(sigma=0.02,\ seed=123).fun\_sphere}.
\item
  The seed is set every time the objective function is called. This can
  be done using the following call:
  \texttt{y\ =\ fun(x,\ sigma=0.02,\ seed=123)}.
\end{enumerate}

These two different ways lead to different results as explained in the
following tables:

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Noise added to the sphere function}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Since \texttt{sigma} is set to \texttt{0.02}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{fun(x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0: [0.98021757]
1: [0.99264427]
2: [1.02575851]
\end{verbatim}

The seed is set once. Every call to \texttt{fun()} results in a
different value. The whole experiment can be repeated, the initial seed
is used to generate the same sequence as shown below:

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Noise added to the sphere function}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Since \texttt{sigma} is set to \texttt{0.02}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical(sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{fun(x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0: [0.98021757]
1: [0.99264427]
2: [1.02575851]
\end{verbatim}

\end{tcolorbox}

If \texttt{spotPython} is used as a hyperparameter tuner, it is
important that only one realization of the noise function is optimized.
This behaviour can be accomplished by passing the same seed via the
dictionary \texttt{fun\_control} to every call of the objective function
\texttt{fun} as shown below:

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The same noise added to the sphere function}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

Since \texttt{sigma} is set to \texttt{0.02}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{fun(x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/08_p040025_2024-01-15_00-15-01 for SummaryWriter()
0: [0.98021757]
1: [0.98021757]
2: [0.98021757]
\end{verbatim}

\end{tcolorbox}

\section{spotPython's Noise Handling
Approaches}\label{spotpythons-noise-handling-approaches}

The following setting will be used for the next steps:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/08_p040025_2024-01-15_00-15-01 for SummaryWriter()
\end{verbatim}

\texttt{spotPython} is adopted as follows to cope with noisy functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_repeats} is set to a value larger than 1 (here: 2)
\item
  \texttt{noise} is set to \texttt{true}. Therefore, a nugget
  (\texttt{Lambda}) term is added to the correlation matrix
\item
  \texttt{init\ size} (of the \texttt{design\_control} dictionary) is
  set to a value larger than 1 (here: 3)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                                    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                                    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                                    fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                                    fun\_repeats }\OperatorTok{=} \DecValTok{2}\NormalTok{,}
\NormalTok{                                    noise }\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                                    show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, repeats}\OperatorTok{=}\DecValTok{2}\NormalTok{),}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(noise}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.run()}
\end{Highlighting}
\end{Shaded}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-1.pdf}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-2.pdf}

\begin{verbatim}
spotPython tuning: 0.034754930918721325 [####------] 40.00% 
spotPython tuning: 0.03339769765455772 [#####-----] 50.00% 
spotPython tuning: 0.015786142156557437 [######----] 60.00% 
spotPython tuning: 0.0005791214064992311 [#######---] 70.00% 
spotPython tuning: 3.5506618676925576e-05 [########--] 80.00% 
spotPython tuning: 5.325186406243954e-07 [#########-] 90.00% 
spotPython tuning: 4.335518265610199e-07 [##########] 100.00% Done...
\end{verbatim}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-4.pdf}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-5.pdf}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-6.pdf}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-7.pdf}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-8.pdf}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-10-output-9.pdf}

\section{Print the Results}\label{print-the-results-3}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 4.335518265610199e-07
x0: 0.0006584465252099216
min mean y: 4.335518265610199e-07
x0: 0.0006584465252099216
\end{verbatim}

\begin{verbatim}
[['x0', 0.0006584465252099216], ['x0', 0.0006584465252099216]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+} \StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{013_num_spot_noisy_files/figure-pdf/cell-12-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}%

\section{Noise and Surrogates: The Nugget
Effect}\label{noise-and-surrogates-the-nugget-effect}

\subsection{The Noisy Sphere}\label{the-noisy-sphere}

\subsubsection{The Data}\label{the-data}

\begin{itemize}
\tightlist
\item
  We prepare some data first:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/08_p040025_2024-01-15_00-15-07 for SummaryWriter()
\end{verbatim}

\begin{itemize}
\tightlist
\item
  A surrogate without nugget is fitted to these data:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-14-output-1.pdf}

\begin{itemize}
\tightlist
\item
  In comparison to the surrogate without nugget, we fit a surrogate with
  nugget to the data:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S\_nug.fit(X\_train, y\_train)}
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S\_nug.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{013_num_spot_noisy_files/figure-pdf/cell-15-output-1.pdf}

\begin{itemize}
\tightlist
\item
  The value of the nugget term can be extracted from the model as
  follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.00055921881757264
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We see:

  \begin{itemize}
  \tightlist
  \item
    the first model \texttt{S} has no nugget,
  \item
    whereas the second model has a nugget value (\texttt{Lambda}) larger
    than zero.
  \end{itemize}
\end{itemize}

\section{Exercises}\label{exercises-6}

\subsection{\texorpdfstring{Noisy
\texttt{fun\_cubed}}{Noisy fun\_cubed}}\label{noisy-fun_cubed}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_cubed} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_runge} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_forrester}}{fun\_forrester}}\label{fun_forrester}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_forrester} function
  with the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_xsin}}{fun\_xsin}}\label{fun_xsin}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_xsin} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\FloatTok{1.}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_xsin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\chapter{\texorpdfstring{Optimal Computational Budget Allocation in
\texttt{Spot}}{Optimal Computational Budget Allocation in Spot}}\label{sec-ocba}

This chapter demonstrates how noisy functions can be handled with
Optimal Computational Budget Allocation (OCBA) by \texttt{Spot}.

\section{\texorpdfstring{Example: \texttt{Spot}, OCBA, and the Noisy
Sphere
Function}{Example: Spot, OCBA, and the Noisy Sphere Function}}\label{example-spot-ocba-and-the-noisy-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"09"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: Noisy
Sphere}\label{the-objective-function-noisy-sphere-1}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function with noise,
i.e., a function that can be described by a (closed) formula:
\[f(x) = x^2 + \epsilon\]

Since \texttt{sigma} is set to \texttt{0.1}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/09_p040025_2024-01-15_00-15-26 for SummaryWriter()
\end{verbatim}

A plot illustrates the noise:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-4-output-1.pdf}

\texttt{Spot} is adopted as follows to cope with noisy functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_repeats} is set to a value larger than 1 (here: 2)
\item
  \texttt{noise} is set to \texttt{true}. Therefore, a nugget
  (\texttt{Lambda}) term is added to the correlation matrix
\item
  \texttt{init\ size} (of the \texttt{design\_control} dictionary) is
  set to a value larger than 1 (here: 2)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init( }
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{2}\NormalTok{,}
\NormalTok{                   infill\_criterion}\OperatorTok{=}\StringTok{"ei"}\NormalTok{,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   tolerance\_x}\OperatorTok{=}\FloatTok{0.0}\NormalTok{,}
\NormalTok{                   ocba\_delta }\OperatorTok{=} \DecValTok{1}\NormalTok{,                   }
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, repeats}\OperatorTok{=}\DecValTok{2}\NormalTok{),}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(noise}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.run()}
\end{Highlighting}
\end{Shaded}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-1.pdf}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-2.pdf}

\begin{verbatim}
spotPython tuning: 0.03475493366922229 [####------] 40.00% 
spotPython tuning: 0.0004463018568303854 [#####-----] 50.00% 
spotPython tuning: 0.0004463018568303854 [######----] 60.00% 
spotPython tuning: 0.0001590474610240226 [#######---] 70.00% 
spotPython tuning: 4.2454542934289965e-09 [########--] 80.00% 
spotPython tuning: 2.2370853591440457e-10 [#########-] 90.00% 
spotPython tuning: 2.2370853591440457e-10 [##########] 100.00% Done...
\end{verbatim}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-4.pdf}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-5.pdf}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-6.pdf}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-7.pdf}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-8.pdf}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-6-output-9.pdf}

\section{Print the Results}\label{print-the-results-4}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.2370853591440457e-10
x0: -1.4956889245909544e-05
min mean y: 2.2370853591440457e-10
x0: -1.4956889245909544e-05
\end{verbatim}

\begin{verbatim}
[['x0', -1.4956889245909544e-05], ['x0', -1.4956889245909544e-05]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-8-output-1.pdf}

\section{Noise and Surrogates: The Nugget
Effect}\label{noise-and-surrogates-the-nugget-effect-1}

\subsection{The Noisy Sphere}\label{the-noisy-sphere-1}

\subsubsection{The Data}\label{the-data-1}

We prepare some data first:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotPython}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{125}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

A surrogate without nugget is fitted to these data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-10-output-1.pdf}

In comparison to the surrogate without nugget, we fit a surrogate with
nugget to the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{S\_nug.fit(X\_train, y\_train)}
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S\_nug.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{014_num_spot_ocba_files/figure-pdf/cell-11-output-1.pdf}

The value of the nugget term can be extracted from the model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
8.374496269458742e-05
\end{verbatim}

We see:

\begin{itemize}
\tightlist
\item
  the first model \texttt{S} has no nugget,
\item
  whereas the second model has a nugget value (\texttt{Lambda}) larger
  than zero.
\end{itemize}

\section{Exercises}\label{exercises-7}

\subsection{\texorpdfstring{Noisy
\texttt{fun\_cubed}}{Noisy fun\_cubed}}\label{noisy-fun_cubed-1}

Analyse the effect of noise on the \texttt{fun\_cubed} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge-1}

Analyse the effect of noise on the \texttt{fun\_runge} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_forrester}}{fun\_forrester}}\label{fun_forrester-1}

Analyse the effect of noise on the \texttt{fun\_forrester} function with
the following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\DecValTok{5}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_xsin}}{fun\_xsin}}\label{fun_xsin-1}

Analyse the effect of noise on the \texttt{fun\_xsin} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\FloatTok{1.}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_xsin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\chapter{Kriging with Varying
Correlation-p}\label{sec-num-spot-correlation-p}

This chapter illustrates the difference between Kriging models with
varying p.~The difference is illustrated with the help of the
\texttt{spotPython} package.

\section{\texorpdfstring{Example: \texttt{Spot} Surrogate and the 2-dim
Sphere
Function}{Example: Spot Surrogate and the 2-dim Sphere Function}}\label{example-spot-surrogate-and-the-2-dim-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"015"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: 2-dim
Sphere}\label{the-objective-function-2-dim-sphere-1}

\begin{itemize}
\tightlist
\item
  The \texttt{spotPython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula: \[f(x, y) = x^2 + y^2\]
\item
  The size of the \texttt{lower} bound vector determines the problem
  dimension.
\item
  Here we will use \texttt{np.array({[}-1,\ -1{]})}, i.e., a two-dim
  function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{                               lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                               upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Created spot_tensorboard_path: runs/spot_logs/015_p040025_2024-01-15_00-15-53 for SummaryWriter()
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Although the default \texttt{spot} surrogate model is an isotropic
  Kriging model, we will explicitly set the \texttt{theta} parameter to
  a value of \texttt{1} for both dimensions. This is done to illustrate
  the difference between isotropic and anisotropic Kriging models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                         p\_val}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}

\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.801603872454505e-05 [#######---] 73.33% 
spotPython tuning: 1.801603872454505e-05 [########--] 80.00% 
spotPython tuning: 1.801603872454505e-05 [#########-] 86.67% 
spotPython tuning: 1.801603872454505e-05 [#########-] 93.33% 
spotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2c9c7fb50>
\end{verbatim}

\subsection{Results}\label{results-7}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.801603872454505e-05
x0: 0.0019077911677074135
x1: 0.003791618596979743
\end{verbatim}

\begin{verbatim}
[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-7-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-8-output-1.pdf}

\section{Example With Modified p}\label{example-with-modified-p}

\begin{itemize}
\tightlist
\item
  We can use set \texttt{p} to a value other than \texttt{2} to obtain a
  different Kriging model.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                           p\_val}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\NormalTok{spot\_2\_p1}\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_p1.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.801603872454505e-05 [#######---] 73.33% 
spotPython tuning: 1.801603872454505e-05 [########--] 80.00% 
spotPython tuning: 1.801603872454505e-05 [#########-] 86.67% 
spotPython tuning: 1.801603872454505e-05 [#########-] 93.33% 
spotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2c9e15bd0>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The search progress of the optimization with the anisotropic model can
  be visualized:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-10-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.801603872454505e-05
x0: 0.0019077911677074135
x1: 0.003791618596979743
\end{verbatim}

\begin{verbatim}
[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-12-output-1.pdf}

\subsection{\texorpdfstring{Taking a Look at the \texttt{p}
Values}{Taking a Look at the p Values}}\label{taking-a-look-at-the-p-values}

\subsubsection{\texorpdfstring{\texttt{p} Values from the \texttt{spot}
Model}{p Values from the spot Model}}\label{p-values-from-the-spot-model}

\begin{itemize}
\tightlist
\item
  We can check, which \texttt{p} values the \texttt{spot} model has
  used:
\item
  The \texttt{p} values from the surrogate can be printed as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1.])
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the surrogate from the isotropic setting was stored as
  \texttt{spot\_2}, we can also take a look at the \texttt{theta} value
  from this model:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([2.])
\end{verbatim}

\section{\texorpdfstring{Optimization of the \texttt{p}
Values}{Optimization of the p Values}}\label{optimization-of-the-p-values}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                           optim\_p}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_2\_pm}\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_pm.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.893023485380876e-05 [#######---] 73.33% 
spotPython tuning: 1.893023485380876e-05 [########--] 80.00% 
spotPython tuning: 1.893023485380876e-05 [#########-] 86.67% 
spotPython tuning: 1.893023485380876e-05 [#########-] 93.33% 
spotPython tuning: 1.893023485380876e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2cf86e610>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-16-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.893023485380876e-05
x0: 0.0017549984724977892
x1: 0.003981232876300906
\end{verbatim}

\begin{verbatim}
[['x0', 0.0017549984724977892], ['x1', 0.003981232876300906]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-18-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1.77398298])
\end{verbatim}

\section{\texorpdfstring{Optimization of Multiple \texttt{p}
Values}{Optimization of Multiple p Values}}\label{optimization-of-multiple-p-values}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{                                           optim\_p}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_2\_pmo}\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_pmo.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.162397189403005e-05 [#######---] 73.33% 
spotPython tuning: 2.162397189403005e-05 [########--] 80.00% 
spotPython tuning: 2.162397189403005e-05 [#########-] 86.67% 
spotPython tuning: 2.162397189403005e-05 [#########-] 93.33% 
spotPython tuning: 2.162397189403005e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2d0ca5510>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-21-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.162397189403005e-05
x0: 0.0018245082309241386
x1: 0.00427728203527896
\end{verbatim}

\begin{verbatim}
[['x0', 0.0018245082309241386], ['x1', 0.00427728203527896]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{015_num_spot_correlation_p_files/figure-pdf/cell-23-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1.09037777, 1.76346322])
\end{verbatim}

\section{Exercises}\label{exercises-8}

\subsection{\texorpdfstring{\texttt{fun\_branin}}{fun\_branin}}\label{fun_branin}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 10\) and \(0 \leq x_2 \leq 15\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} runs with different
  options for \texttt{p}.
\item
  Modify the termination criterion: instead of the number of evaluations
  (which is specified via \texttt{fun\_evals}), the time should be used
  as the termination criterion. This can be done as follows
  (\texttt{max\_time=1} specifies a run time of one minute):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_sin\_cos}}{fun\_sin\_cos}}\label{fun_sin_cos}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-2\pi \leq x_1 \leq 2\pi\) and \(-2\pi \leq x_2 \leq 2\pi\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge-2}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 5\) and \(-5 \leq x_2 \leq 5\).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} runs with different
  options for \texttt{p}.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{\texttt{fun\_wingwt}}{fun\_wingwt}}\label{fun_wingwt}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{10}. The search ranges are between 0
    and 1 (values are mapped internally to their natural bounds).
  \end{itemize}
\item
  Compare the results from \texttt{spotPython} runs with different
  options for \texttt{p}.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\section{Jupyter Notebook}\label{jupyter-notebook-10}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/015_num_spot_correlation_p.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\part{Hyperparameter Tuning with River}

\chapter{\texorpdfstring{\texttt{river} Hyperparameter Tuning: Mondrian
Tree Regressor with Friedman Drift
Data}{river Hyperparameter Tuning: Mondrian Tree Regressor with Friedman Drift Data}}\label{sec-river-hpt-51}

This chapter demonstrates hyperparameter tuning for \texttt{river}'s
\texttt{Mondrian\ Tree\ Regressor} with the Friedman drift data set
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
The \texttt{Mondrian\ Tree\ Regressor} is a regression tree, i.e., it
predicts a real value for each sample.

\section{Setup}\label{sec-setup-51}

Before we consider the detailed experimental setup, we select the
parameters that affect run time, initial design size, size of the data
set, and the experiment name.

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME}: The maximum run time in seconds for the
  hyperparameter tuning process.
\item
  \texttt{INIT\_SIZE}: The initial design size for the hyperparameter
  tuning process.
\item
  \texttt{PREFIX}: The prefix for the experiment name.
\item
  \texttt{K}: The factor that determines the number of samples in the
  data set.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{K} is the multiplier for the number of samples. If it is set
  to 1, then \texttt{100\_000}samples are taken. It is set to 0.1 for
  demonstration purposes. For real experiments, this should be increased
  to at least 1.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{30}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{10}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"025RIVER"}
\NormalTok{K }\OperatorTok{=} \FloatTok{0.1}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  This notebook exemplifies hyperparameter tuning with SPOT (spotPython
  and spotRiver).
\item
  The hyperparameter software SPOT is available in Python. It was
  developed in R (statistical programming language), see Open Access
  book ``Hyperparameter Tuning for Machine and Deep Learning with R - A
  Practical Guide'', available here:
  \url{https://link.springer.com/book/10.1007/978-981-19-5170-1}.
\item
  This notebook demonstrates hyperparameter tuning for \texttt{river}.
  It is based on the notebook ``Incremental decision trees in river: the
  Hoeffding Tree case'', see:
  \url{https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/\#42-regression-tree-splitters}.
\item
  Here we will use the river \texttt{AMFRegressor} functions, see:
  \url{https://riverml.xyz/0.19.0/api/forest/AMFRegressor/}.
\end{itemize}

\section{\texorpdfstring{Initialization of the \texttt{fun\_control}
Dictionary}{Initialization of the fun\_control Dictionary}}\label{initialization-of-the-fun_control-dictionary}

\texttt{spotPython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotPython}.

First, we define an ``experiment name'' to identify the hyperparameter
tuning process. The experiment name is also used to create a directory
for the TensorBoard files.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{MAX\_TIME,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_15_01_34_57
Created spot_tensorboard_path: runs/spot_logs/025RIVER_p040025_2024-01-15_01-34-57 for SummaryWriter()
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TensorBoard}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  Since the \texttt{spot\_tensorboard\_path} argument is not
  \texttt{None}, which is the default, \texttt{spotPython} will log the
  optimization process in the TensorBoard folder.
\item
  Section~\ref{sec-tensorboard-10} describes how to start TensorBoard
  and access the TensorBoard dashboard.
\item
  The \texttt{TENSORBOARD\_CLEAN} argument is set to \texttt{True} to
  archive the TensorBoard folder if it already exists. This is useful if
  you want to start a hyperparameter tuning process from scratch. If you
  want to continue a hyperparameter tuning process, set
  \texttt{TENSORBOARD\_CLEAN} to \texttt{False}. Then the TensorBoard
  folder will not be archived and the old and new TensorBoard files will
  shown in the TensorBoard dashboard.
\end{itemize}

\end{tcolorbox}

\section{Load Data: The Friedman Drift
Data}\label{load-data-the-friedman-drift-data}

We will use the Friedman synthetic dataset with concept drifts
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
Each observation is composed of ten features. Each feature value is
sampled uniformly in {[}0, 1{]}. Only the first five features are
relevant. The target is defined by different functions depending on the
type of the drift. Global Recurring Abrupt drift will be used, i.e., the
concept drift appears over the whole instance space. There are two
points of concept drift. At the second point of drift the old concept
reoccurs.

The following parameters are used to generate and handle the data set:

\begin{itemize}
\tightlist
\item
  horizon: The prediction horizon in hours.
\item
  n\_samples: The number of samples in the data set.
\item
  p\_1: The position of the first concept drift.
\item
  p\_2: The position of the second concept drift.
\item
  position: The position of the concept drifts.
\item
  n\_train: The number of samples used for training.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{horizon }\OperatorTok{=} \DecValTok{7}\OperatorTok{*}\DecValTok{24}
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{100\_000}\NormalTok{)}
\NormalTok{p\_1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{25\_000}\NormalTok{)}
\NormalTok{p\_2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{50\_000}\NormalTok{)}
\NormalTok{position}\OperatorTok{=}\NormalTok{(p\_1, p\_2)}
\NormalTok{n\_train }\OperatorTok{=} \DecValTok{1\_000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.datasets }\ImportTok{import}\NormalTok{ synth}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{position,}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  We will use \texttt{spotRiver}'s \texttt{convert\_to\_df} function
  \href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/utils/data_conversion.py}{{[}SOURCE{]}}
  to convert the \texttt{river} data set to a \texttt{pandas} data
  frame.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.utils.data\_conversion }\ImportTok{import}\NormalTok{ convert\_to\_df}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\NormalTok{df }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, target\_column}\OperatorTok{=}\NormalTok{target\_column, n\_total}\OperatorTok{=}\NormalTok{n\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Add column names x1 until x10 to the first 10 columns of the dataframe
  and the column name y to the last column of the dataframe.
\item
  Then split the data frame into a training and test data set. The train
  and test data sets are stored in the \texttt{fun\_control} dictionary.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\NormalTok{df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\StringTok{"y"}\NormalTok{]}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"train"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{df[:n\_train],}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"test"}\NormalTok{, df[n\_train:], }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"n\_samples"}\NormalTok{, n\_samples, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"target\_column"}\NormalTok{, target\_column, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Specification of the Preprocessing
Model}\label{specification-of-the-preprocessing-model}

\begin{itemize}
\tightlist
\item
  We use the \texttt{StandardScaler}
  \href{https://riverml.xyz/dev/api/preprocessing/StandardScaler/}{{[}SOURCE{]}}
  from \texttt{river} as the preprocessing model. The
  \texttt{StandardScaler} is used to standardize the data set, i.e., it
  has zero mean and unit variance.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river }\ImportTok{import}\NormalTok{ preprocessing}
\NormalTok{prep\_model }\OperatorTok{=}\NormalTok{ preprocessing.StandardScaler()}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"prep\_model"}\NormalTok{, prep\_model, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{SelectSelect Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{SelectSelect Model (algorithm) and core\_model\_hyper\_dict}}\label{selectselect-model-algorithm-and-core_model_hyper_dict}

\texttt{spotPython} hyperparameter tuning approach uses two components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a model (class) and
\item
  an associated hyperparameter dictionary.
\end{enumerate}

The corresponding hyperparameters are loaded from the associated
dictionary, which is stored as a JSON file
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}.
The JSON file contains hyperparameter type information, names, and
bounds.

The method \texttt{add\_core\_model\_to\_fun\_control} adds the model
and the hyperparameter dictionary to the \texttt{fun\_control}
dictionary.

Alternatively, you can load a local hyper\_dict. Simply set
\texttt{river\_hyper\_dict.json} as the filename. If \texttt{filename}is
set to \texttt{None}, which is the default, the hyper\_dict
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json}{{[}SOURCE{]}}
is loaded from the \texttt{spotRiver} package.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.forest }\ImportTok{import}\NormalTok{ AMFRegressor}
\ImportTok{from}\NormalTok{ spotRiver.data.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{AMFRegressor,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Modify \texttt{hyper\_dict} Hyperparameters for
the Selected Algorithm aka
\texttt{core\_model}}{Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model}

After the \texttt{core\_model} and the \texttt{core\_model\_hyper\_dict}
are added to the \texttt{fun\_control} dictionary, the hyperparameter
tuning can be started. However, in some settings, the user wants to
modify the hyperparameters of the \texttt{core\_model\_hyper\_dict}.
This can be done with the \texttt{modify\_hyper\_parameter\_bounds} and
\texttt{modify\_hyper\_parameter\_levels} functions
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}}.

The following code shows how hyperparameter of type numeric and integer
(boolean) can be modified. The \texttt{modify\_hyper\_parameter\_bounds}
function is used to modify the bounds of the hyperparameter
\texttt{delta} and \texttt{merit\_preprune}. Similar option exists for
the \texttt{modify\_hyper\_parameter\_levels} function to modify the
levels of categorical hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from spotPython.hyperparameters.values import modify\_hyper\_parameter\_bounds}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "n\_estimators", bounds=[2,100])}

\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_hyperparameter\_value}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"n\_estimators"}\NormalTok{, [}\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

::: \{.callout-note\} \#\#\#\# Note: Active and Inactive Hyperparameters
Hyperparameters can be excluded from the tuning procedure by selecting
identical values for the lower and upper bounds.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper | transform   |
|-----------------|--------|-----------|---------|---------|-------------|
| n_estimators    | int    |        10 |     2   |     100 | None        |
| step            | float  |         1 |     0.1 |      10 | None        |
| use_aggregation | factor |         1 |     0   |       1 | None        |
\end{verbatim}

\section{Selection of the Objective (Loss)
Function}\label{selection-of-the-objective-loss-function}

The \texttt{metric\_sklearn} is used for the sklearn based evaluation
via \texttt{eval\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_bml.py}{{[}SOURCE{]}}.
Here we use the \texttt{mean\_absolute\_error}
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html}{{[}SOURCE{]}}
as the objective function.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Additional metrics}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\texttt{spotRiver} also supports additional metrics. For example, the
\texttt{metric\_river} is used for the river based evaluation via
\texttt{eval\_oml\_iter\_progressive}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_oml.py}{{[}SOURCE{]}}.
The \texttt{metric\_river} is implemented to simulate the behaviour of
the ``original'' \texttt{river} metrics.

\end{tcolorbox}

\texttt{spotRiver} provides information about the model' s score
(metric), memory, and time. The hyperparamter tuner requires a single
objective. Therefore, a weighted sum of the metric, memory, and time is
computed. The weights are defined in the \texttt{weights} array.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Weights}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The \texttt{weights} provide a flexible way to define specific
requirements, e.g., if the memory is more important than the time, the
weight for the memory can be increased.

\end{tcolorbox}

The \texttt{oml\_grace\_period} defines the number of observations that
are used for the initial training of the model. The \texttt{step}
defines the iteration number at which to yield results. This only takes
into account the predictions, and not the training steps. The
\texttt{weight\_coeff} defines a multiplier for the results: results are
multiplied by (step/n\_steps)**weight\_coeff, where n\_steps is the
total number of iterations. Results from the beginning have a lower
weight than results from the end if weight\_coeff \textgreater{} 1. If
weight\_coeff == 0, all results have equal weight. Note, that the
\texttt{weight\_coeff} is only used internally for the tuner and does
not affect the results that are used for the evaluation or comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_absolute\_error}

\NormalTok{weights }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{1000}\NormalTok{])}\OperatorTok{*}\FloatTok{10\_000.0}
\NormalTok{oml\_grace\_period }\OperatorTok{=} \DecValTok{2}
\NormalTok{step }\OperatorTok{=} \DecValTok{100}
\NormalTok{weight\_coeff }\OperatorTok{=} \FloatTok{1.0}

\CommentTok{\# fun\_control.update(\{}
\CommentTok{\#                "horizon": horizon,}
\CommentTok{\#                "oml\_grace\_period": oml\_grace\_period,}
\CommentTok{\#                "weights": weights,}
\CommentTok{\#                "step": step,}
\CommentTok{\#                "weight\_coeff": weight\_coeff,}
\CommentTok{\#                "metric\_sklearn": mean\_absolute\_error}
\CommentTok{\#                \})}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"horizon"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{horizon,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"oml\_grace\_period"}\NormalTok{, oml\_grace\_period, }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"weights"}\NormalTok{, weights, }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"step"}\NormalTok{, step, }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"weight\_coeff"}\NormalTok{, weight\_coeff, }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"metric\_sklearn"}\NormalTok{, mean\_absolute\_error, }\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Calling the SPOT Function}\label{calling-the-spot-function}

\subsection{The Objective Function}\label{sec-the-objective-function-51}

The objective function \texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py}{{[}SOURCE{]}}
is selected next.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.fun.hyperriver }\ImportTok{import}\NormalTok{ HyperRiver}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperRiver().fun\_oml\_horizon}
\end{Highlighting}
\end{Shaded}

The following code snippet shows how to get the default hyperparameters
as an array, so that they can be passed to the \texttt{Spot} function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer}

The class \texttt{Spot}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}
is the hyperparameter tuning workhorse. It is initialized with the
following parameters:

\begin{itemize}
\tightlist
\item
  \texttt{fun}: the objective function
\item
  \texttt{fun\_control}: the dictionary with the control parameters for
  the objective function
\item
  \texttt{design}: the experimental design
\item
  \texttt{design\_control}: the dictionary with the control parameters
  for the experimental design
\item
  \texttt{surrogate}: the surrogate model
\item
  \texttt{surrogate\_control}: the dictionary with the control
  parameters for the surrogate model
\item
  \texttt{optimizer}: the optimizer
\item
  \texttt{optimizer\_control}: the dictionary with the control
  parameters for the optimizer
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Total run time}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The total run time may exceed the specified \texttt{max\_time}, because
the initial design (here: \texttt{init\_size} = INIT\_SIZE as specified
above) is always evaluated, even if this takes longer than
\texttt{max\_time}.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ design\_control\_init, surrogate\_control\_init}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init()}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"init\_size"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{INIT\_SIZE,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(noise}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                                           n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.6518492573623873 [#---------] 12.68% 
spotPython tuning: 2.6451199275131057 [###-------] 25.24% 
spotPython tuning: 2.6451199275131057 [####------] 37.71% 
spotPython tuning: 2.6451199275131057 [#####-----] 50.40% 
spotPython tuning: 2.6451199275131057 [######----] 62.36% 
spotPython tuning: 2.6451199275131057 [#######---] 74.28% 
spotPython tuning: 2.6451199275131057 [########--] 80.74% 
spotPython tuning: 2.6451199275131057 [#########-] 87.25% 
spotPython tuning: 2.6451199275131057 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2b7898fd0>
\end{verbatim}

\subsection{TensorBoard}\label{sec-tensorboard-10}

Now we can start TensorBoard in the background with the following
command, where \texttt{./runs} is the default directory for the
TensorBoard log files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TENSORBOARD\_PATH}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The TensorBoard path can be printed with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_tensorboard\_path}
\NormalTok{get\_tensorboard\_path(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'runs/'
\end{verbatim}

\end{tcolorbox}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotPython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/build/kriging.py}{{[}SOURCE{]}}
is plotted against the number of optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures_static/13_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotPython optimization
process and the surrogate model.}

\end{figure}%

\subsection{Results}\label{results-8}

After the hyperparameter tuning run is finished, the results can be
saved and reloaded with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle, get\_experiment\_name,  load\_pickle}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(PREFIX)}
\NormalTok{SAVE\_AND\_LOAD }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ SAVE\_AND\_LOAD }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
    \ImportTok{from}\NormalTok{ spotPython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle, get\_experiment\_name,  load\_pickle}
\NormalTok{    save\_pickle(spot\_tuner, experiment\_name)}
\NormalTok{    spot\_tuner }\OperatorTok{=}\NormalTok{ load\_pickle(experiment\_name)}
\end{Highlighting}
\end{Shaded}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-20-output-1.pdf}

Results can also be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper |             tuned | transform   |   importance | stars   |
|-----------------|--------|-----------|---------|---------|-------------------|-------------|--------------|---------|
| n_estimators    | int    |      10.0 |     2.0 |     100 |              68.0 | None        |       100.00 | ***     |
| step            | float  |       1.0 |     0.1 |      10 | 5.681843800398889 | None        |        32.18 | *       |
| use_aggregation | factor |       1.0 |     0.0 |       1 |               0.0 | None        |        17.60 | *       |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.0025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-22-output-1.pdf}

\section{The Larger Data Set}\label{the-larger-data-set}

After the hyperparameter were tuned on a small data set, we can now
apply the hyperparameter configuration to a larger data set. The
following code snippet shows how to generate the larger data set.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Increased Friedman-Drift Data Set}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The Friedman-Drift Data Set is increased by a factor of two to show
  the transferability of the hyperparameter tuning results.
\item
  Larger values of \texttt{K} lead to a longer run time.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{K }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{100\_000}\NormalTok{)}
\NormalTok{p\_1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{25\_000}\NormalTok{)}
\NormalTok{p\_2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(K}\OperatorTok{*}\DecValTok{50\_000}\NormalTok{)}
\NormalTok{position}\OperatorTok{=}\NormalTok{(p\_1, p\_2)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{position,}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The larger data set is converted to a Pandas data frame and passed to
the \texttt{fun\_control} dictionary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, target\_column}\OperatorTok{=}\NormalTok{target\_column, n\_total}\OperatorTok{=}\NormalTok{n\_samples)}
\NormalTok{df.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\StringTok{"y"}\NormalTok{]}
\CommentTok{\# fun\_control.update(\{"train": df[:n\_train],}
\CommentTok{\#                     "test": df[n\_train:],}
\CommentTok{\#                     "n\_samples": n\_samples,}
\CommentTok{\#                     "target\_column": target\_column\})}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"train"}\NormalTok{, df[:n\_train], }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"test"}\NormalTok{, df[n\_train:], }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"n\_samples"}\NormalTok{, n\_samples, }\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(fun\_control, }\StringTok{"target\_column"}\NormalTok{, target\_column, }\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Get Default Hyperparameters}\label{get-default-hyperparameters}

The default hyperparameters, whihc will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: \texttt{spotPython} tunes numpy arrays}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{spotPython} tunes numpy arrays, i.e., the hyperparameters are
  stored in a numpy array.
\end{itemize}

\end{tcolorbox}

The model with the default hyperparameters can be trained and evaluated
with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ eval\_oml\_horizon}

\NormalTok{df\_eval\_default, df\_true\_default }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_default,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

The three performance criteria, i.e., scaoe (metric), runtime, and
memory consumption, can be visualized with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotRiver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ plot\_bml\_oml\_horizon\_metrics, plot\_bml\_oml\_horizon\_predictions}
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-28-output-1.pdf}

\subsection{Show Predictions}\label{show-predictions}

\begin{itemize}
\tightlist
\item
  Select a subset of the data set for the visualization of the
  predictions:

  \begin{itemize}
  \tightlist
  \item
    We use the mean, \(m\), of the data set as the center of the
    visualization.
  \item
    We use 100 data points, i.e., \(m \pm 50\) as the visualization
    window.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"test"}\NormalTok{].shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{a }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{{-}}\DecValTok{50}
\NormalTok{b }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels)}
\end{Highlighting}
\end{Shaded}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-30-output-1.pdf}

\section{Get SPOT Results}\label{get-spot-results}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval\_spot, df\_true\_spot }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_spot,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{, }\StringTok{"spot"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default, df\_eval\_spot], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{], filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_metrics.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-33-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b], df\_true\_spot[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_predictions.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-34-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.plot.validation }\ImportTok{import}\NormalTok{ plot\_actual\_vs\_predicted}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_default[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_default[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"Default"}\NormalTok{)}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_spot[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_spot[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-35-output-1.pdf}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-35-output-2.pdf}

\section{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_estimators:  100.0
step:  32.17679643870097
use_aggregation:  17.601797153236607
\end{verbatim}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-36-output-2.pdf}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-36-output-3.pdf}

\includegraphics{025_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-36-output-4.pdf}

\section{Parallel Coordinates Plots}\label{parallel-coordinates-plots}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\section{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\part{Hyperparameter Tuning with PyTorch Lightning}

\chapter{HPT PyTorch Lightning:
Diabetes}\label{hpt-pytorch-lightning-diabetes}

In this tutorial, we will show how \texttt{spotPython} can be integrated
into the \texttt{PyTorch} Lightning training workflow for a regression
task.

This chapter describes the hyperparameter tuning of a
\texttt{PyTorch\ Lightning} network on the Diabetes data set. This is a
PyTorch Dataset for regression. A toy data set from scikit-learn. Ten
baseline variables, age, sex, body mass index, average blood pressure,
and six blood serum measurements were obtained for each of n = 442
diabetes patients, as well as the response of interest, a quantitative
measure of disease progression one year after baseline.

\section{Step 1: Setup}\label{sec-setup-31}

\begin{itemize}
\tightlist
\item
  Before we consider the detailed experimental setup, we select the
  parameters that affect run time, initial design size, etc.
\item
  The parameter \texttt{MAX\_TIME} specifies the maximum run time in
  seconds.
\item
  The parameter \texttt{INIT\_SIZE} specifies the initial design size.
\item
  The parameter \texttt{WORKERS} specifies the number of workers.
\item
  The prefix \texttt{PREFIX} is used for the experiment name and the
  name of the log file.
\item
  The parameter \texttt{DEVICE} specifies the device to use for
  training.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}

\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{FUN\_EVALS }\OperatorTok{=}\NormalTok{ inf}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{WORKERS }\OperatorTok{=} \DecValTok{0}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"031"}
\NormalTok{DEVICE }\OperatorTok{=}\NormalTok{ getDevice()}
\NormalTok{DEVICES }\OperatorTok{=} \DecValTok{1}
\NormalTok{TEST\_SIZE }\OperatorTok{=} \FloatTok{0.1}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{WORKERS} is set to 0 for demonstration purposes. For real
  experiments, this should be increased. See the warnings that are
  printed when the number of workers is set to 0.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Device selection}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  Although there are no .cuda() or .to(device) calls required, because
  Lightning does these for you, see
  \href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{LIGHTNINGMODULE},
  we would like to know which device is used. Threrefore, we imitate the
  LightningModule behaviour which selects the highest device.
\item
  The method \texttt{spotPython.utils.device.getDevice()} returns the
  device that is used by Lightning.
\end{itemize}

\end{tcolorbox}

\section{\texorpdfstring{Step 2: Initialization of the
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the fun\_control Dictionary}}\label{step-2-initialization-of-the-fun_control-dictionary}

\texttt{spotPython} uses a Python dictionary for storing the information
required for the hyperparameter tuning process.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    device}\OperatorTok{=}\NormalTok{DEVICE,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{FUN\_EVALS,}
\NormalTok{    log\_level}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{MAX\_TIME,}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{WORKERS,}
\NormalTok{    show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{    tolerance\_x}\OperatorTok{=}\NormalTok{np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_15_02_52_16
Created spot_tensorboard_path: runs/spot_logs/031_p040025_2024-01-15_02-52-16 for SummaryWriter()
\end{verbatim}

\section{Step 3: Loading the Diabetes Data
Set}\label{step-3-loading-the-diabetes-data-set}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotPython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"data\_set"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{dataset,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(dataset))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
442
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Data Set and Data Loader}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  As shown below, a DataLoader from \texttt{torch.utils.data} can be
  used to check the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,
          0.0199, -0.0176],
        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,
         -0.0683, -0.0922],
        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,
          0.0029, -0.0259],
        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,
          0.0227, -0.0094],
        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,
         -0.0320, -0.0466]])
Targets: tensor([151.,  75., 141., 206., 135.])
\end{verbatim}

\end{tcolorbox}

\section{Step 4: Preprocessing}\label{sec-preprocessing-31}

Preprocessing is handled by \texttt{Lightning} and \texttt{PyTorch}. It
is described in the
\href{https://lightning.ai/docs/pytorch/stable/data/datamodule.html}{LIGHTNINGDATAMODULE}
documentation. Here you can find information about the
\texttt{transforms} methods.

\section{\texorpdfstring{Step 5: Select the Core Model
(\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select the Core Model (algorithm) and core\_model\_hyper\_dict}}\label{sec-selection-of-the-algorithm-31}

\texttt{spotPython} includes the \texttt{NetLightRegression} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/netlightregression.py}{{[}SOURCE{]}}
for configurable neural networks. The class is imported here. It
inherits from the class \texttt{Lightning.LightningModule}, which is the
base class for all models in \texttt{Lightning}.
\texttt{Lightning.LightningModule} is a subclass of
\texttt{torch.nn.Module} and provides additional functionality for the
training and testing of neural networks. The class
\texttt{Lightning.LightningModule} is described in the
\href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{Lightning
documentation}.

\begin{itemize}
\tightlist
\item
  Here we simply add the NN Model to the fun\_control dictionary by
  calling the function \texttt{add\_core\_model\_to\_fun\_control}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.regression.netlightregression }\ImportTok{import}\NormalTok{ NetLightRegression}
\ImportTok{from}\NormalTok{ spotPython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              core\_model}\OperatorTok{=}\NormalTok{NetLightRegression,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{LightHyperDict)}
\end{Highlighting}
\end{Shaded}

The hyperparameters of the model are specified in the
\texttt{core\_model\_hyper\_dict} dictionary
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperdict/light_hyper_dict.json}{{[}SOURCE{]}}.

\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{sec-modification-of-hyperparameters-31}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Small number of epochs for demonstration purposes}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{epochs} and \texttt{patience} are set to small values for
  demonstration purposes. These values are too small for a real
  application.
\item
  More resonable values are, e.g.:

  \begin{itemize}
  \tightlist
  \item
    \texttt{set\_control\_hyperparameter\_value(fun\_control,\ "epochs",\ {[}7,\ 9{]})}
    and
  \item
    \texttt{set\_control\_hyperparameter\_value(fun\_control,\ "patience",\ {[}2,\ 7{]})}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_hyperparameter\_value}

\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [}
                \StringTok{"Adadelta"}\NormalTok{,}
                \StringTok{"Adagrad"}\NormalTok{,}
                \StringTok{"Adam"}\NormalTok{,}
                \StringTok{"AdamW"}\NormalTok{,}
                \StringTok{"Adamax"}\NormalTok{,                }
                \StringTok{"NAdam"}\NormalTok{,}
                \StringTok{"RAdam"}\NormalTok{,}
                \StringTok{"RMSprop"}\NormalTok{,}
                \StringTok{"Rprop"}
\NormalTok{            ])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"lr\_mult"}\NormalTok{, [}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.0}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"act\_fn"}\NormalTok{,[}
                \StringTok{"Sigmoid"}\NormalTok{,}
                \StringTok{"ReLU"}\NormalTok{,}
                \StringTok{"LeakyReLU"}\NormalTok{,}
                \StringTok{"Swish"}
\NormalTok{            ] )}
\end{Highlighting}
\end{Shaded}

Now, the dictionary \texttt{fun\_control} contains all information
needed for the hyperparameter tuning. Before the hyperparameter tuning
is started, it is recommended to take a look at the experimental design.
The method \texttt{gen\_design\_table}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/utils/eda.py}{{[}SOURCE{]}}
generates a design table as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |    4    |     6   | transform_power_2_int |
| epochs         | int    | 4         |    9    |    10   | transform_power_2_int |
| batch_size     | int    | 4         |    4    |     5   | transform_power_2_int |
| act_fn         | factor | ReLU      |    0    |     3   | None                  |
| optimizer      | factor | SGD       |    0    |     8   | None                  |
| dropout_prob   | float  | 0.01      |    0.01 |     0.1 | None                  |
| lr_mult        | float  | 1.0       |    0.5  |     5   | None                  |
| patience       | int    | 2         |    5    |     7   | transform_power_2_int |
| initialization | factor | Default   |    0    |     2   | None                  |
\end{verbatim}

This allows to check if all information is available and if the
information is correct.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Hyperparameters of the Tuned Model and the \texttt{fun\_control}
Dictionary}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The updated \texttt{fun\_control} dictionary can be shown with the
command \texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}}.

\end{tcolorbox}

\section{Step 7: Data Splitting, the Objective (Loss) Function and the
Metric}\label{step-7-data-splitting-the-objective-loss-function-and-the-metric}

\subsection{Evaluation}\label{sec-selection-of-target-function-31}

The evaluation procedure requires the specification of two elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the way how the data is split into a train and a test set
\item
  the loss function (and a metric).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Data Splitting in Lightning}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The data splitting is handled by \texttt{Lightning}.

\end{tcolorbox}

\subsection{Loss Function}\label{sec-loss-function-31}

The loss function is specified in the configurable network class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/regression/netlightregression.py}{{[}SOURCE{]}}
We will use MSE.

\subsection{Metric}\label{sec-metric-31}

\begin{itemize}
\tightlist
\item
  Similar to the loss function, the metric is specified in the
  configurable network class
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/regression/netlightregression.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Loss Function and Metric in Lightning}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The loss function and the metric are not hyperparameters that can be
  tuned with \texttt{spotPython}.
\item
  They are handled by \texttt{Lightning}.
\end{itemize}

\end{tcolorbox}

\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function}

\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-31}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ design\_control\_init, surrogate\_control\_init}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\NormalTok{INIT\_SIZE)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(noise}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                                            n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Modifying Values in the Control Dictionaries}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The values in the control dictionaries can be modified with the
  function \texttt{set\_control\_key\_value}
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}},
  for example:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{set\_control\_key\_value(control\_dict=surrogate\_control,}
\NormalTok{                        key="noise",}
\NormalTok{                        value=True,}
\NormalTok{                        replace=True)                       }
\NormalTok{set\_control\_key\_value(control\_dict=surrogate\_control,}
\NormalTok{                        key="n\_theta",}
\NormalTok{                        value=2,}
\NormalTok{                        replace=True)      }
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{\texorpdfstring{The Objective Function
\texttt{fun}}{The Objective Function fun}}\label{sec-the-objective-function-31}

The objective function \texttt{fun} from the class \texttt{HyperLight}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/hyperlight.py}{{[}SOURCE{]}}
is selected next. It implements an interface from \texttt{PyTorch}'s
training, validation, and testing methods to \texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight(log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{).fun}
\end{Highlighting}
\end{Shaded}

\subsection{Showing the fun\_control
Dictionary}\label{sec-show-fun-control-31}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\NormalTok{pprint.pprint(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'CHECKPOINT_PATH': 'runs/saved_models/',
 'DATASET_PATH': 'data/',
 'RESULTS_PATH': 'results/',
 'TENSORBOARD_PATH': 'runs/',
 '_L_in': 10,
 '_L_out': 1,
 'accelerator': 'auto',
 'core_model': <class 'spotPython.light.regression.netlightregression.NetLightRegression'>,
 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',
                                      'core_model_parameter_type': 'instance()',
                                      'default': 'ReLU',
                                      'levels': ['Sigmoid',
                                                 'ReLU',
                                                 'LeakyReLU',
                                                 'Swish'],
                                      'lower': 0,
                                      'transform': 'None',
                                      'type': 'factor',
                                      'upper': 3},
                           'batch_size': {'default': 4,
                                          'lower': 4,
                                          'transform': 'transform_power_2_int',
                                          'type': 'int',
                                          'upper': 5},
                           'dropout_prob': {'default': 0.01,
                                            'lower': 0.01,
                                            'transform': 'None',
                                            'type': 'float',
                                            'upper': 0.1},
                           'epochs': {'default': 4,
                                      'lower': 9,
                                      'transform': 'transform_power_2_int',
                                      'type': 'int',
                                      'upper': 10},
                           'initialization': {'core_model_parameter_type': 'str',
                                              'default': 'Default',
                                              'levels': ['Default',
                                                         'Kaiming',
                                                         'Xavier'],
                                              'lower': 0,
                                              'transform': 'None',
                                              'type': 'factor',
                                              'upper': 2},
                           'l1': {'default': 3,
                                  'lower': 4,
                                  'transform': 'transform_power_2_int',
                                  'type': 'int',
                                  'upper': 6},
                           'lr_mult': {'default': 1.0,
                                       'lower': 0.5,
                                       'transform': 'None',
                                       'type': 'float',
                                       'upper': 5.0},
                           'optimizer': {'class_name': 'torch.optim',
                                         'core_model_parameter_type': 'str',
                                         'default': 'SGD',
                                         'levels': ['Adadelta',
                                                    'Adagrad',
                                                    'Adam',
                                                    'AdamW',
                                                    'Adamax',
                                                    'NAdam',
                                                    'RAdam',
                                                    'RMSprop',
                                                    'Rprop'],
                                         'lower': 0,
                                         'transform': 'None',
                                         'type': 'factor',
                                         'upper': 8},
                           'patience': {'default': 2,
                                        'lower': 5,
                                        'transform': 'transform_power_2_int',
                                        'type': 'int',
                                        'upper': 7}},
 'counter': 0,
 'data': None,
 'data_dir': './data',
 'data_module': None,
 'data_set': <spotPython.data.diabetes.Diabetes object at 0x2c289aed0>,
 'design': None,
 'device': 'mps',
 'devices': 1,
 'enable_progress_bar': False,
 'eval': None,
 'fun_evals': inf,
 'fun_repeats': 1,
 'infill_criterion': 'y',
 'k_folds': 3,
 'log_level': 10,
 'loss_function': None,
 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),
 'max_time': 1,
 'metric_params': {},
 'metric_river': None,
 'metric_sklearn': None,
 'metric_torch': None,
 'model_dict': {},
 'n_points': 1,
 'n_samples': None,
 'noise': False,
 'num_workers': 0,
 'ocba_delta': 0,
 'optimizer': None,
 'path': None,
 'prep_model': None,
 'save_model': False,
 'seed': 123,
 'show_batch_interval': 1000000,
 'show_models': False,
 'show_progress': True,
 'shuffle': None,
 'sigma': 0.0,
 'spot_tensorboard_path': 'runs/spot_logs/031_p040025_2024-01-15_02-52-16',
 'spot_writer': <torch.utils.tensorboard.writer.SummaryWriter object at 0x2be8314d0>,
 'target_column': None,
 'task': None,
 'test': None,
 'test_seed': 1234,
 'test_size': 0.1,
 'tolerance_x': 1.4901161193847656e-08,
 'train': None,
 'upper': array([ 8.  ,  9.  ,  4.  ,  5.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),
 'var_name': ['l1',
              'epochs',
              'batch_size',
              'act_fn',
              'optimizer',
              'dropout_prob',
              'lr_mult',
              'patience',
              'initialization'],
 'var_type': ['int',
              'int',
              'int',
              'factor',
              'factor',
              'float',
              'float',
              'int',
              'factor'],
 'verbosity': 0,
 'weights': 1.0}
\end{verbatim}

\subsection{Starting the Hyperparameter
Tuning}\label{sec-call-the-hyperparameter-tuner-31}

The \texttt{spotPython} hyperparameter tuning is started by calling the
\texttt{Spot} function
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                       fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                       design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                       surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3704.84814453125, 'hp_metric': 3704.84814453125}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3881.30419921875, 'hp_metric': 3881.30419921875}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2666.55078125, 'hp_metric': 2666.55078125}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5838.70849609375, 'hp_metric': 5838.70849609375}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3217.487060546875, 'hp_metric': 3217.487060546875}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3745.490478515625, 'hp_metric': 3745.490478515625}
spotPython tuning: 2666.55078125 [###-------] 33.95% 
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3020.19091796875, 'hp_metric': 3020.19091796875}
spotPython tuning: 2666.55078125 [#########-] 85.16% 
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 4797.9697265625, 'hp_metric': 4797.9697265625}
spotPython tuning: 2666.55078125 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3704.84814453125      │
│         val_loss          │     3704.84814453125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3881.30419921875      │
│         val_loss          │     3881.30419921875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       2666.55078125       │
│         val_loss          │       2666.55078125       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     5838.70849609375      │
│         val_loss          │     5838.70849609375      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3217.487060546875     │
│         val_loss          │     3217.487060546875     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3745.490478515625     │
│         val_loss          │     3745.490478515625     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3020.19091796875      │
│         val_loss          │     3020.19091796875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      4797.9697265625      │
│         val_loss          │      4797.9697265625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2c5abd990>
\end{verbatim}

\section{Step 9: Tensorboard}\label{sec-tensorboard-31}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="runs/"}
\end{Highlighting}
\end{Shaded}

Further information can be found in the
\href{https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html}{PyTorch
Lightning documentation} for Tensorboard.

\section{Step 10: Results}\label{sec-results-31}

After the hyperparameter tuning run is finished, the results can be
analyzed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-13-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     4.0 |     6.0 | 5.0                 | transform_power_2_int |         0.02 |         |
| epochs         | int    | 4         |     9.0 |    10.0 | 9.0                 | transform_power_2_int |         1.88 | *       |
| batch_size     | int    | 4         |     4.0 |     5.0 | 4.0                 | transform_power_2_int |        57.05 | **      |
| act_fn         | factor | ReLU      |     0.0 |     3.0 | Swish               | None                  |         0.11 | .       |
| optimizer      | factor | SGD       |     0.0 |     8.0 | RMSprop             | None                  |        27.01 | *       |
| dropout_prob   | float  | 0.01      |    0.01 |     0.1 | 0.08951095646892425 | None                  |         0.46 | .       |
| lr_mult        | float  | 1.0       |     0.5 |     5.0 | 3.9340958113143683  | None                  |        29.97 | *       |
| patience       | int    | 2         |     5.0 |     7.0 | 7.0                 | transform_power_2_int |         1.20 | *       |
| initialization | factor | Default   |     0.0 |     2.0 | Xavier              | None                  |       100.00 | ***     |
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+} \StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-15-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}%

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-31}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner, fun\_control)}
\BuiltInTok{print}\NormalTok{(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'l1': 32, 'epochs': 512, 'batch_size': 16, 'act_fn': Swish(), 'optimizer': 'RMSprop', 'dropout_prob': 0.08951095646892425, 'lr_mult': 3.9340958113143683, 'patience': 128, 'initialization': 'Xavier'}
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Test on the full data set
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.testmodel }\ImportTok{import}\NormalTok{ test\_model}
\NormalTok{test\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule: train_dataloader(). Training set size: 71
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
LightDataModule: test_dataloader(). Training set size: 266
LightDataModule: test_dataloader(). batch_size: 16
LightDataModule: test_dataloader(). num_workers: 0
test_model result: {'val_loss': 5430.9619140625, 'hp_metric': 5430.9619140625}
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      5430.9619140625      │
│         val_loss          │      5430.9619140625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
(5430.9619140625, 5430.9619140625)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.loadmodel }\ImportTok{import}\NormalTok{ load\_light\_from\_checkpoint}

\NormalTok{model\_loaded }\OperatorTok{=}\NormalTok{ load\_light\_from\_checkpoint(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
config: {'l1': 32, 'epochs': 512, 'batch_size': 16, 'act_fn': Swish(), 'optimizer': 'RMSprop', 'dropout_prob': 0.08951095646892425, 'lr_mult': 3.9340958113143683, 'patience': 128, 'initialization': 'Xavier'}
Loading model with 32_512_16_Swish_RMSprop_0.0895_3.9341_128_Xavier_TEST from runs/saved_models/32_512_16_Swish_RMSprop_0.0895_3.9341_128_Xavier_TEST/last.ckpt
Model: NetLightRegression(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=32, bias=True)
    (1): Swish()
    (2): Dropout(p=0.08951095646892425, inplace=False)
    (3): Linear(in_features=32, out_features=16, bias=True)
    (4): Swish()
    (5): Dropout(p=0.08951095646892425, inplace=False)
    (6): Linear(in_features=16, out_features=16, bias=True)
    (7): Swish()
    (8): Dropout(p=0.08951095646892425, inplace=False)
    (9): Linear(in_features=16, out_features=8, bias=True)
    (10): Swish()
    (11): Dropout(p=0.08951095646892425, inplace=False)
    (12): Linear(in_features=8, out_features=1, bias=True)
  )
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
epochs:  1.8807805068719348
batch_size:  57.05142175062917
act_fn:  0.1107426719028105
optimizer:  27.012920682264415
dropout_prob:  0.45724124818032186
lr_mult:  29.965265752518814
patience:  1.201168496292324
initialization:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-2.pdf}

}

\caption{Contour plots.}

\end{figure}%

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-3.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-4.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-5.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-6.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-7.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-8.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-9.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-10.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-11.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-12.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-13.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-14.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-15.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-16.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-17.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-18.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-19.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-20.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-21.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-22.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-23.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-24.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-25.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-26.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-27.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-28.pdf}

\includegraphics{031_spot_lightning_linear_diabetes_files/figure-pdf/cell-19-output-29.pdf}

\subsection{Parallel Coordinates Plot}\label{parallel-coordinates-plot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

Parallel coordinates plots

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\subsection{Cross Validation With
Lightning}\label{cross-validation-with-lightning}

\begin{itemize}
\tightlist
\item
  The \texttt{KFold} class from \texttt{sklearn.model\_selection} is
  used to generate the folds for cross-validation.
\item
  These mechanism is used to generate the folds for the final evaluation
  of the model.
\item
  The \texttt{CrossValidationDataModule} class
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/data/lightcrossvalidationdatamodule.py}{{[}SOURCE{]}}
  is used to generate the folds for the hyperparameter tuning process.
\item
  It is called from the \texttt{cv\_model} function
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/cvmodel.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.cvmodel }\ImportTok{import}\NormalTok{ cv\_model}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"k\_folds"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"test\_size"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\FloatTok{0.6}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{cv\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
k: 0
Train Dataset Size: 221
Val Dataset Size: 221
train_model result: {'val_loss': 3624.390869140625, 'hp_metric': 3624.390869140625}
k: 1
Train Dataset Size: 221
Val Dataset Size: 221
train_model result: {'val_loss': 3789.607177734375, 'hp_metric': 3789.607177734375}
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3624.390869140625     │
│         val_loss          │     3624.390869140625     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3789.607177734375     │
│         val_loss          │     3789.607177734375     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
3706.9990234375
\end{verbatim}

\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-1}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\subsection{Visualizing the Activation Distribution (Under
Development)}\label{visualizing-the-activation-distribution-under-development}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Reference:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The following code is based on
  \href{https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/02-activation-functions.html}{{[}PyTorch
  Lightning TUTORIAL 2: ACTIVATION FUNCTIONS{]}}, Author: Phillip Lippe,
  License: \href{https://creativecommons.org/licenses/by-sa/3.0/}{{[}CC
  BY-SA{]}}, Generated: 2023-03-15T09:52:39.179933.
\end{itemize}

\end{tcolorbox}

After we have trained the models, we can look at the actual activation
values that find inside the model. For instance, how many neurons are
set to zero in ReLU? Where do we find most values in Tanh? To answer
these questions, we can write a simple function which takes a trained
model, applies it to a batch of images, and plots the histogram of the
activations inside the network:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.torch.activation }\ImportTok{import}\NormalTok{ Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish}
\NormalTok{act\_fn\_by\_name }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigmoid"}\NormalTok{: Sigmoid, }\StringTok{"tanh"}\NormalTok{: Tanh, }\StringTok{"relu"}\NormalTok{: ReLU, }\StringTok{"leakyrelu"}\NormalTok{: LeakyReLU, }\StringTok{"elu"}\NormalTok{: ELU, }\StringTok{"swish"}\NormalTok{: Swish\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_config\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_one\_config\_from\_X(X, fun\_control)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\DecValTok{64}\NormalTok{, \_L\_out}\OperatorTok{=}\DecValTok{11}\NormalTok{)}
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NetLightRegression(
  (layers): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): Swish()
    (2): Dropout(p=0.08951095646892425, inplace=False)
    (3): Linear(in_features=32, out_features=16, bias=True)
    (4): Swish()
    (5): Dropout(p=0.08951095646892425, inplace=False)
    (6): Linear(in_features=16, out_features=16, bias=True)
    (7): Swish()
    (8): Dropout(p=0.08951095646892425, inplace=False)
    (9): Linear(in_features=16, out_features=8, bias=True)
    (10): Swish()
    (11): Dropout(p=0.08951095646892425, inplace=False)
    (12): Linear(in_features=8, out_features=11, bias=True)
  )
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from spotPython.utils.eda import visualize\_activations}
\CommentTok{\# visualize\_activations(model, color=f"C\{0\}")}
\end{Highlighting}
\end{Shaded}

\chapter{HPT PyTorch Lightning: Diabetes Using a Recurrent Neural
Network}\label{hpt-pytorch-lightning-diabetes-using-a-recurrent-neural-network}

In this tutorial, we will show how \texttt{spotPython} can be integrated
into the \texttt{PyTorch} Lightning training workflow for a regression
task.

This chapter describes the hyperparameter tuning of a
\texttt{PyTorch\ Lightning} network on the Diabetes data set. This is a
PyTorch Dataset for regression. A toy data set from scikit-learn. Ten
baseline variables, age, sex, body mass index, average blood pressure,
and six blood serum measurements were obtained for each of n = 442
diabetes patients, as well as the response of interest, a quantitative
measure of disease progression one year after baseline.

\section{Step 1: Setup}\label{sec-setup-32}

\begin{itemize}
\tightlist
\item
  Before we consider the detailed experimental setup, we select the
  parameters that affect run time, initial design size, etc.
\item
  The parameter \texttt{MAX\_TIME} specifies the maximum run time in
  seconds.
\item
  The parameter \texttt{INIT\_SIZE} specifies the initial design size.
\item
  The parameter \texttt{WORKERS} specifies the number of workers.
\item
  The prefix \texttt{PREFIX} is used for the experiment name and the
  name of the log file.
\item
  The parameter \texttt{DEVICE} specifies the device to use for
  training.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{FUN\_EVALS }\OperatorTok{=}\NormalTok{ inf}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{5}
\NormalTok{WORKERS }\OperatorTok{=} \DecValTok{0}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"032"}
\NormalTok{DEVICE }\OperatorTok{=}\NormalTok{ getDevice()}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{FUN\_EVALS} is set to infinity.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{WORKERS} is set to 0 for demonstration purposes. For real
  experiments, this should be increased. See the warnings that are
  printed when the number of workers is set to 0.
\item
  \texttt{PREFIX} is set to ``032''. This is used for the experiment
  name and the name of the log file.
\item
  \texttt{DEVICE} is set to the device that is returned by
  \texttt{getDevice()}, e.g., \texttt{gpu}.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Device selection}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  Although there are no .cuda() or .to(device) calls required, because
  Lightning does these for you, see
  \href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{LIGHTNINGMODULE},
  we would like to know which device is used. Threrefore, we imitate the
  LightningModule behaviour which selects the highest device.
\item
  The method \texttt{spotPython.utils.device.getDevice()} returns the
  device that is used by Lightning.
\end{itemize}

\end{tcolorbox}

\section{\texorpdfstring{Step 2: Initialization of the
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the fun\_control Dictionary}}\label{step-2-initialization-of-the-fun_control-dictionary-1}

\texttt{spotPython} uses a Python dictionary for storing the information
required for the hyperparameter tuning process.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    device}\OperatorTok{=}\NormalTok{DEVICE,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{FUN\_EVALS,}
\NormalTok{    log\_level}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{MAX\_TIME,}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{WORKERS,}
\NormalTok{    show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{    tolerance\_x}\OperatorTok{=}\NormalTok{np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{    verbosity}\OperatorTok{=}\DecValTok{1}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_15_03_08_31
Created spot_tensorboard_path: runs/spot_logs/032_p040025_2024-01-15_03-08-31 for SummaryWriter()
\end{verbatim}

\section{Step 3: Loading the Diabetes Data
Set}\label{step-3-loading-the-diabetes-data-set-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotPython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"data\_set"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{dataset,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(dataset))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
442
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Data Set and Data Loader}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  As shown below, a DataLoader from \texttt{torch.utils.data} can be
  used to check the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,
          0.0199, -0.0176],
        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,
         -0.0683, -0.0922],
        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,
          0.0029, -0.0259],
        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,
          0.0227, -0.0094],
        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,
         -0.0320, -0.0466]])
Targets: tensor([151.,  75., 141., 206., 135.])
\end{verbatim}

\end{tcolorbox}

\section{Step 4: Preprocessing}\label{sec-preprocessing-32}

Preprocessing is handled by \texttt{Lightning} and \texttt{PyTorch}. It
is described in the
\href{https://lightning.ai/docs/pytorch/stable/data/datamodule.html}{LIGHTNINGDATAMODULE}
documentation. Here you can find information about the
\texttt{transforms} methods.

\section{\texorpdfstring{Step 5: Select the Core Model
(\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select the Core Model (algorithm) and core\_model\_hyper\_dict}}\label{sec-selection-of-the-algorithm-32}

\texttt{spotPython} includes the \texttt{NetLightRegression} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/netlightregression.py}{{[}SOURCE{]}}
for configurable neural networks. The class is imported here. It
inherits from the class \texttt{Lightning.LightningModule}, which is the
base class for all models in \texttt{Lightning}.
\texttt{Lightning.LightningModule} is a subclass of
\texttt{torch.nn.Module} and provides additional functionality for the
training and testing of neural networks. The class
\texttt{Lightning.LightningModule} is described in the
\href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{Lightning
documentation}.

\begin{itemize}
\tightlist
\item
  Here we simply add the NN Model to the fun\_control dictionary by
  calling the function \texttt{add\_core\_model\_to\_fun\_control}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.regression.rnnlightregression }\ImportTok{import}\NormalTok{ RNNLightRegression}
\ImportTok{from}\NormalTok{ spotPython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              core\_model}\OperatorTok{=}\NormalTok{RNNLightRegression,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{LightHyperDict)}
\end{Highlighting}
\end{Shaded}

The hyperparameters of the model are specified in the
\texttt{core\_model\_hyper\_dict} dictionary
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperdict/light_hyper_dict.json}{{[}SOURCE{]}}.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: User specified models and hyperparameter dictionaries}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The user can specify a model and a hyperparameter dictionary in a
  subfolder, e.g., \texttt{userRNN} in the current working directory.
\item
  The model and the hyperparameter dictionary are imported with the
  following code:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from spotPython.hyperparameters.values import add\_core\_model\_to\_fun\_control}
\NormalTok{import sys}
\NormalTok{sys.path.insert(0, \textquotesingle{}./userRNN\textquotesingle{})}
\NormalTok{import userrnn}
\NormalTok{import user\_hyper\_dict}
\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control=fun\_control,}
\NormalTok{                              core\_model=userrnn.RNNLightRegression,}
\NormalTok{                              hyper\_dict=user\_hyper\_dict.UserHyperDict)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Example files can be found in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/tree/main/userRNN}{userRNN}
  folder.
\item
  These files can be modified by the user.
\item
  They can be used without re-compilation of the \texttt{spotPython}
  source code, if they are located in a subfolder of the current working
  directory.
\end{itemize}

\end{tcolorbox}

\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{sec-modification-of-hyperparameters-32}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Small number of epochs for demonstration purposes}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{epochs} and \texttt{patience} are set to small values for
  demonstration purposes. These values are too small for a real
  application.
\item
  More resonable values are, e.g.:

  \begin{itemize}
  \tightlist
  \item
    \texttt{set\_control\_hyperparameter\_value(fun\_control,\ "epochs",\ {[}7,\ 9{]})}
    and
  \item
    \texttt{set\_control\_hyperparameter\_value(fun\_control,\ "patience",\ {[}2,\ 7{]})}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_hyperparameter\_value}

\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{, }\DecValTok{8}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [}
                \StringTok{"Adadelta"}\NormalTok{,}
                \StringTok{"Adagrad"}\NormalTok{,}
                \StringTok{"Adam"}\NormalTok{,}
                \StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.25}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"lr\_mult"}\NormalTok{, [}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.0}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{3}\NormalTok{, }\DecValTok{9}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"act\_fn"}\NormalTok{,[}\StringTok{"ReLU"}\NormalTok{] )}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"initialization"}\NormalTok{,[}\StringTok{"Default"}\NormalTok{] )}
\end{Highlighting}
\end{Shaded}

Now, the dictionary \texttt{fun\_control} contains all information
needed for the hyperparameter tuning. Before the hyperparameter tuning
is started, it is recommended to take a look at the experimental design.
The method \texttt{gen\_design\_table}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/utils/eda.py}{{[}SOURCE{]}}
generates a design table as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |    3    |    8    | transform_power_2_int |
| epochs         | int    | 4         |    7    |    9    | transform_power_2_int |
| batch_size     | int    | 4         |    2    |    6    | transform_power_2_int |
| act_fn         | factor | ReLU      |    0    |    0    | None                  |
| optimizer      | factor | SGD       |    0    |    3    | None                  |
| dropout_prob   | float  | 0.01      |    0.01 |    0.25 | None                  |
| lr_mult        | float  | 1.0       |    0.5  |    5    | None                  |
| patience       | int    | 2         |    3    |    9    | transform_power_2_int |
| initialization | factor | Default   |    0    |    0    | None                  |
\end{verbatim}

This allows to check if all information is available and if the
information is correct.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Hyperparameters of the Tuned Model and the \texttt{fun\_control}
Dictionary}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The updated \texttt{fun\_control} dictionary can be shown with the
command \texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}}.

\end{tcolorbox}

\section{Step 7: Data Splitting, the Objective (Loss) Function and the
Metric}\label{step-7-data-splitting-the-objective-loss-function-and-the-metric-1}

\subsection{Evaluation}\label{sec-selection-of-target-function-32}

The evaluation procedure requires the specification of two elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the way how the data is split into a train and a test set
\item
  the loss function (and a metric).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Data Splitting in Lightning}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The data splitting is handled by \texttt{Lightning}.

\end{tcolorbox}

\subsection{Loss Function}\label{sec-loss-function-32}

The loss function is specified in the configurable network class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/regression/netlightregression.py}{{[}SOURCE{]}}
We will use MSE.

\subsection{Metric}\label{sec-metric-32}

\begin{itemize}
\tightlist
\item
  Similar to the loss function, the metric is specified in the
  configurable network class
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/regression/netlightregression.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Loss Function and Metric in Lightning}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The loss function and the metric are not hyperparameters that can be
  tuned with \texttt{spotPython}.
\item
  They are handled by \texttt{Lightning}.
\end{itemize}

\end{tcolorbox}

\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-1}

\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-32}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ design\_control\_init, surrogate\_control\_init}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init()}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"init\_size"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{INIT\_SIZE,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init()}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{surrogate\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"noise"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)                       }
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{surrogate\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"n\_theta"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)      }
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{The Objective Function
\texttt{fun}}{The Objective Function fun}}\label{sec-the-objective-function-32}

The objective function \texttt{fun} from the class \texttt{HyperLight}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/hyperlight.py}{{[}SOURCE{]}}
is selected next. It implements an interface from \texttt{PyTorch}'s
training, validation, and testing methods to \texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight(log\_level}\OperatorTok{=}\DecValTok{10}\NormalTok{).fun}
\end{Highlighting}
\end{Shaded}

\subsection{Showing the fun\_control
Dictionary}\label{sec-show-fun-control-32}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\NormalTok{pprint.pprint(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'CHECKPOINT_PATH': 'runs/saved_models/',
 'DATASET_PATH': 'data/',
 'RESULTS_PATH': 'results/',
 'TENSORBOARD_PATH': 'runs/',
 '_L_in': 10,
 '_L_out': 1,
 'accelerator': 'auto',
 'core_model': <class 'spotPython.light.regression.rnnlightregression.RNNLightRegression'>,
 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',
                                      'core_model_parameter_type': 'instance()',
                                      'default': 'ReLU',
                                      'levels': ['ReLU'],
                                      'lower': 0,
                                      'transform': 'None',
                                      'type': 'factor',
                                      'upper': 0},
                           'batch_size': {'default': 4,
                                          'lower': 2,
                                          'transform': 'transform_power_2_int',
                                          'type': 'int',
                                          'upper': 6},
                           'dropout_prob': {'default': 0.01,
                                            'lower': 0.01,
                                            'transform': 'None',
                                            'type': 'float',
                                            'upper': 0.25},
                           'epochs': {'default': 4,
                                      'lower': 7,
                                      'transform': 'transform_power_2_int',
                                      'type': 'int',
                                      'upper': 9},
                           'initialization': {'core_model_parameter_type': 'str',
                                              'default': 'Default',
                                              'levels': ['Default'],
                                              'lower': 0,
                                              'transform': 'None',
                                              'type': 'factor',
                                              'upper': 0},
                           'l1': {'default': 3,
                                  'lower': 3,
                                  'transform': 'transform_power_2_int',
                                  'type': 'int',
                                  'upper': 8},
                           'lr_mult': {'default': 1.0,
                                       'lower': 0.5,
                                       'transform': 'None',
                                       'type': 'float',
                                       'upper': 5.0},
                           'optimizer': {'class_name': 'torch.optim',
                                         'core_model_parameter_type': 'str',
                                         'default': 'SGD',
                                         'levels': ['Adadelta',
                                                    'Adagrad',
                                                    'Adam',
                                                    'Adamax'],
                                         'lower': 0,
                                         'transform': 'None',
                                         'type': 'factor',
                                         'upper': 3},
                           'patience': {'default': 2,
                                        'lower': 3,
                                        'transform': 'transform_power_2_int',
                                        'type': 'int',
                                        'upper': 9}},
 'counter': 0,
 'data': None,
 'data_dir': './data',
 'data_module': None,
 'data_set': <spotPython.data.diabetes.Diabetes object at 0x2b469a810>,
 'design': None,
 'device': 'mps',
 'devices': 1,
 'enable_progress_bar': False,
 'eval': None,
 'fun_evals': inf,
 'fun_repeats': 1,
 'infill_criterion': 'y',
 'k_folds': 3,
 'log_level': 10,
 'loss_function': None,
 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),
 'max_time': 1,
 'metric_params': {},
 'metric_river': None,
 'metric_sklearn': None,
 'metric_torch': None,
 'model_dict': {},
 'n_points': 1,
 'n_samples': None,
 'noise': False,
 'num_workers': 0,
 'ocba_delta': 0,
 'optimizer': None,
 'path': None,
 'prep_model': None,
 'save_model': False,
 'seed': 123,
 'show_batch_interval': 1000000,
 'show_models': False,
 'show_progress': True,
 'shuffle': None,
 'sigma': 0.0,
 'spot_tensorboard_path': 'runs/spot_logs/032_p040025_2024-01-15_03-08-31',
 'spot_writer': <torch.utils.tensorboard.writer.SummaryWriter object at 0x14344e290>,
 'target_column': None,
 'task': None,
 'test': None,
 'test_seed': 1234,
 'test_size': 0.1,
 'tolerance_x': 1.4901161193847656e-08,
 'train': None,
 'upper': array([ 8.  ,  9.  ,  4.  ,  1.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),
 'var_name': ['l1',
              'epochs',
              'batch_size',
              'act_fn',
              'optimizer',
              'dropout_prob',
              'lr_mult',
              'patience',
              'initialization'],
 'var_type': ['int',
              'int',
              'int',
              'factor',
              'factor',
              'float',
              'float',
              'int',
              'factor'],
 'verbosity': 1,
 'weights': 1.0}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pprint.pprint(design\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'init_size': 5, 'repeats': 1}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pprint.pprint(surrogate\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'log_level': 50,
 'max_Lambda': 1,
 'max_theta': 2.0,
 'min_Lambda': 1e-09,
 'min_theta': -3.0,
 'model_fun_evals': 10000,
 'model_optimizer': <function differential_evolution at 0x1511ba5c0>,
 'n_p': 1,
 'n_theta': 2,
 'noise': True,
 'optim_p': False,
 'p_val': 2.0,
 'seed': 124,
 'theta_init_zero': True,
 'var_type': None}
\end{verbatim}

\subsection{Starting the Hyperparameter
Tuning}\label{sec-call-the-hyperparameter-tuner-32}

The \texttt{spotPython} hyperparameter tuning is started by calling the
\texttt{Spot} function
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                       fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                       design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                       surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 64,
 'dropout_prob': 0.19355651674791854,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 16,
 'lr_mult': 1.5691149440098038,
 'optimizer': 'Adam',
 'patience': 32}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5832.97998046875, 'hp_metric': 5832.97998046875}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 16,
 'dropout_prob': 0.09424169914869776,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 3.35818256351233,
 'optimizer': 'Adadelta',
 'patience': 512}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3652.889404296875, 'hp_metric': 3652.889404296875}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 4,
 'dropout_prob': 0.21164199382623602,
 'epochs': 512,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 0.9336514668325573,
 'optimizer': 'Adamax',
 'patience': 16}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 4
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 4
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2414.205078125, 'hp_metric': 2414.205078125}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 8,
 'dropout_prob': 0.05728504399550885,
 'epochs': 128,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': 4.575980093998586,
 'optimizer': 'Adam',
 'patience': 32}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 8
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 8
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5862.02099609375, 'hp_metric': 5862.02099609375}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 16,
 'dropout_prob': 0.14352914208400058,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 8,
 'lr_mult': 2.4204853123355816,
 'optimizer': 'Adagrad',
 'patience': 128}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3681.71923828125, 'hp_metric': 3681.71923828125}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 4,
 'dropout_prob': 0.2116436615948213,
 'epochs': 512,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 0.9336534446907111,
 'optimizer': 'Adamax',
 'patience': 16}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 4
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 4
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3089.423828125, 'hp_metric': 3089.423828125}
spotPython tuning: 2414.205078125 [########--] 76.27% 

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 4,
 'dropout_prob': 0.21163960937948934,
 'epochs': 512,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 0.9336299317038221,
 'optimizer': 'Adamax',
 'patience': 16}
train_model(): Test set size: 45
train_model(): Train set size: 359
train_model(): Batch size: 4
LightDataModule: train_dataloader(). Training set size: 359
LightDataModule: train_dataloader(). batch_size: 4
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3275.68994140625, 'hp_metric': 3275.68994140625}
spotPython tuning: 2414.205078125 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     5832.97998046875      │
│         val_loss          │     5832.97998046875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3652.889404296875     │
│         val_loss          │     3652.889404296875     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      2414.205078125       │
│         val_loss          │      2414.205078125       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     5862.02099609375      │
│         val_loss          │     5862.02099609375      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3681.71923828125      │
│         val_loss          │     3681.71923828125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      3089.423828125       │
│         val_loss          │      3089.423828125       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3275.68994140625      │
│         val_loss          │     3275.68994140625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2b78bea90>
\end{verbatim}

\section{Step 9: Tensorboard}\label{sec-tensorboard-32}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="runs/"}
\end{Highlighting}
\end{Shaded}

Further information can be found in the
\href{https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html}{PyTorch
Lightning documentation} for Tensorboard.

\section{Step 10: Results}\label{sec-results-32}

After the hyperparameter tuning run is finished, the results can be
analyzed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+} \StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{032_spot_lightning_rnn_diabetes_files/figure-pdf/cell-15-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     8.0 | 7.0                 | transform_power_2_int |         0.00 |         |
| epochs         | int    | 4         |     7.0 |     9.0 | 9.0                 | transform_power_2_int |         0.00 |         |
| batch_size     | int    | 4         |     2.0 |     6.0 | 2.0                 | transform_power_2_int |         0.00 |         |
| act_fn         | factor | ReLU      |     0.0 |     0.0 | ReLU                | None                  |         0.00 |         |
| optimizer      | factor | SGD       |     0.0 |     3.0 | Adamax              | None                  |         0.37 | .       |
| dropout_prob   | float  | 0.01      |    0.01 |    0.25 | 0.21164199382623602 | None                  |         0.00 |         |
| lr_mult        | float  | 1.0       |     0.5 |     5.0 | 0.9336514668325573  | None                  |         0.00 |         |
| patience       | int    | 2         |     3.0 |     9.0 | 4.0                 | transform_power_2_int |       100.00 | ***     |
| initialization | factor | Default   |     0.0 |     0.0 | Default             | None                  |         0.00 |         |
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+} \StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{032_spot_lightning_rnn_diabetes_files/figure-pdf/cell-17-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}%

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-32}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner, fun\_control)}
\BuiltInTok{print}\NormalTok{(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'l1': 128, 'epochs': 512, 'batch_size': 4, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.21164199382623602, 'lr_mult': 0.9336514668325573, 'patience': 16, 'initialization': 'Default'}
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Test on the full data set
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.testmodel }\ImportTok{import}\NormalTok{ test\_model}
\NormalTok{test\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule: train_dataloader(). Training set size: 71
LightDataModule: train_dataloader(). batch_size: 4
LightDataModule: train_dataloader(). num_workers: 0
LightDataModule: test_dataloader(). Training set size: 266
LightDataModule: test_dataloader(). batch_size: 4
LightDataModule: test_dataloader(). num_workers: 0
test_model result: {'val_loss': 3773.025634765625, 'hp_metric': 3773.025634765625}
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3773.025634765625     │
│         val_loss          │     3773.025634765625     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
(3773.025634765625, 3773.025634765625)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.loadmodel }\ImportTok{import}\NormalTok{ load\_light\_from\_checkpoint}

\NormalTok{model\_loaded }\OperatorTok{=}\NormalTok{ load\_light\_from\_checkpoint(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
config: {'l1': 128, 'epochs': 512, 'batch_size': 4, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.21164199382623602, 'lr_mult': 0.9336514668325573, 'patience': 16, 'initialization': 'Default'}
Loading model with 128_512_4_ReLU_Adamax_0.2116_0.9337_16_Default_TEST from runs/saved_models/128_512_4_ReLU_Adamax_0.2116_0.9337_16_Default_TEST/last.ckpt
Model: RNNLightRegression(
  (rnn_layer): RNN(10, 128, batch_first=True)
  (fc): Linear(in_features=128, out_features=128, bias=True)
  (output_layer): Linear(in_features=128, out_features=1, bias=True)
  (dropout1): Dropout(p=0.21164199382623602, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (activation_fct): ReLU()
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
optimizer:  0.3650734171034668
patience:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{032_spot_lightning_rnn_diabetes_files/figure-pdf/cell-21-output-2.pdf}

}

\caption{Contour plots.}

\end{figure}%

\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

Parallel coordinates plots

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\subsection{Cross Validation With
Lightning}\label{cross-validation-with-lightning-1}

\begin{itemize}
\tightlist
\item
  The \texttt{KFold} class from \texttt{sklearn.model\_selection} is
  used to generate the folds for cross-validation.
\item
  These mechanism is used to generate the folds for the final evaluation
  of the model.
\item
  The \texttt{CrossValidationDataModule} class
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/data/lightcrossvalidationdatamodule.py}{{[}SOURCE{]}}
  is used to generate the folds for the hyperparameter tuning process.
\item
  It is called from the \texttt{cv\_model} function
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/cvmodel.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.cvmodel }\ImportTok{import}\NormalTok{ cv\_model}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"k\_folds"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"test\_size"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{cv\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
k: 0
Train Dataset Size: 221
Val Dataset Size: 221
train_model result: {'val_loss': 3405.75830078125, 'hp_metric': 3405.75830078125}
k: 1
Train Dataset Size: 221
Val Dataset Size: 221
train_model result: {'val_loss': 3102.15576171875, 'hp_metric': 3102.15576171875}
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3405.75830078125      │
│         val_loss          │     3405.75830078125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3102.15576171875      │
│         val_loss          │     3102.15576171875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
3253.95703125
\end{verbatim}

\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-2}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\subsection{Visualizing the Activation Distribution (Under
Development)}\label{visualizing-the-activation-distribution-under-development-1}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Reference:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The following code is based on
  \href{https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/02-activation-functions.html}{{[}PyTorch
  Lightning TUTORIAL 2: ACTIVATION FUNCTIONS{]}}, Author: Phillip Lippe,
  License: \href{https://creativecommons.org/licenses/by-sa/3.0/}{{[}CC
  BY-SA{]}}, Generated: 2023-03-15T09:52:39.179933.
\end{itemize}

\end{tcolorbox}

After we have trained the models, we can look at the actual activation
values that find inside the model. For instance, how many neurons are
set to zero in ReLU? Where do we find most values in Tanh? To answer
these questions, we can write a simple function which takes a trained
model, applies it to a batch of images, and plots the histogram of the
activations inside the network:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.torch.activation }\ImportTok{import}\NormalTok{ Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish}
\NormalTok{act\_fn\_by\_name }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigmoid"}\NormalTok{: Sigmoid, }\StringTok{"tanh"}\NormalTok{: Tanh, }\StringTok{"relu"}\NormalTok{: ReLU, }\StringTok{"leakyrelu"}\NormalTok{: LeakyReLU, }\StringTok{"elu"}\NormalTok{: ELU, }\StringTok{"swish"}\NormalTok{: Swish\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_config\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_one\_config\_from\_X(X, fun\_control)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\DecValTok{64}\NormalTok{, \_L\_out}\OperatorTok{=}\DecValTok{11}\NormalTok{)}
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RNNLightRegression(
  (rnn_layer): RNN(64, 128, batch_first=True)
  (fc): Linear(in_features=128, out_features=128, bias=True)
  (output_layer): Linear(in_features=128, out_features=11, bias=True)
  (dropout1): Dropout(p=0.21164199382623602, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (activation_fct): ReLU()
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from spotPython.utils.eda import visualize\_activations}
\CommentTok{\# visualize\_activations(model, color=f"C\{0\}")}
\end{Highlighting}
\end{Shaded}

\chapter{HPT PyTorch Lightning: User Specified Data Set and Regression
Model}\label{hpt-pytorch-lightning-user-specified-data-set-and-regression-model}

In this tutorial, we will show how \texttt{spotPython} can be integrated
into the \texttt{PyTorch} Lightning training workflow for a regression
task with a user specified data set and a user specified regression
model.

This chapter describes the hyperparameter tuning of a
\texttt{PyTorch\ Lightning} network on a user data set, which can be
found in the subfolder of this notebook, \texttt{userData}. The network
can be found in the subfolder \texttt{userModel}.

\section{Step 1: Setup}\label{sec-setup-33}

\begin{itemize}
\tightlist
\item
  Before we consider the detailed experimental setup, we select the
  parameters that affect run time, initial design size, etc.
\item
  The parameter \texttt{MAX\_TIME} specifies the maximum run time in
  seconds.
\item
  The parameter \texttt{INIT\_SIZE} specifies the initial design size.
\item
  The parameter \texttt{WORKERS} specifies the number of workers.
\item
  The prefix \texttt{PREFIX} is used for the experiment name and the
  name of the log file.
\item
  The parameter \texttt{DEVICE} specifies the device to use for
  training.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}

\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{10}
\NormalTok{FUN\_EVALS }\OperatorTok{=}\NormalTok{ inf}
\NormalTok{FUN\_REPEATS }\OperatorTok{=} \DecValTok{2}
\NormalTok{OCBA\_DELTA }\OperatorTok{=} \DecValTok{1}
\NormalTok{REPEATS }\OperatorTok{=} \DecValTok{2}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{10}
\NormalTok{WORKERS }\OperatorTok{=} \DecValTok{0}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"032"}
\NormalTok{DEVICE }\OperatorTok{=}\NormalTok{ getDevice()}
\NormalTok{DEVICES }\OperatorTok{=} \DecValTok{1}
\NormalTok{TEST\_SIZE }\OperatorTok{=} \FloatTok{0.3}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME} is set to one minute for demonstration purposes.
  For real experiments, this should be increased to at least 1 hour.
\item
  \texttt{INIT\_SIZE} is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\item
  \texttt{WORKERS} is set to 0 for demonstration purposes. For real
  experiments, this should be increased. See the warnings that are
  printed when the number of workers is set to 0.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Device selection}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  Although there are no .cuda() or .to(device) calls required, because
  Lightning does these for you, see
  \href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{LIGHTNINGMODULE},
  we would like to know which device is used. Threrefore, we imitate the
  LightningModule behaviour which selects the highest device.
\item
  The method \texttt{spotPython.utils.device.getDevice()} returns the
  device that is used by Lightning.
\end{itemize}

\end{tcolorbox}

\section{\texorpdfstring{Step 2: Initialization of the
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the fun\_control Dictionary}}\label{step-2-initialization-of-the-fun_control-dictionary-2}

\texttt{spotPython} uses a Python dictionary for storing the information
required for the hyperparameter tuning process.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{133}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    device}\OperatorTok{=}\NormalTok{DEVICE,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{FUN\_EVALS,}
\NormalTok{    fun\_repeats}\OperatorTok{=}\NormalTok{FUN\_REPEATS,}
\NormalTok{    log\_level}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{MAX\_TIME,}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{WORKERS,}
\NormalTok{    ocba\_delta }\OperatorTok{=}\NormalTok{ OCBA\_DELTA,}
\NormalTok{    show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    test\_size}\OperatorTok{=}\NormalTok{TEST\_SIZE,}
\NormalTok{    tolerance\_x}\OperatorTok{=}\NormalTok{np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{    verbosity}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_15_03_37_22
Created spot_tensorboard_path: runs/spot_logs/032_p040025_2024-01-15_03-37-22 for SummaryWriter()
\end{verbatim}

\section{Step 3: Loading the User Specified Data
Set}\label{step-3-loading-the-user-specified-data-set}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotPython.data.pkldataset }\ImportTok{import}\NormalTok{ PKLDataset}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ PKLDataset(directory}\OperatorTok{=}\StringTok{"./userData/"}\NormalTok{,}
\NormalTok{                     filename}\OperatorTok{=}\StringTok{"data\_sensitive.pkl"}\NormalTok{,}
\NormalTok{                     target\_column}\OperatorTok{=}\StringTok{\textquotesingle{}N\textquotesingle{}}\NormalTok{,}
\NormalTok{                     feature\_type}\OperatorTok{=}\NormalTok{torch.float32,}
\NormalTok{                     target\_type}\OperatorTok{=}\NormalTok{torch.float32,}
\NormalTok{                     rmNA}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"data\_set"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{dataset,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(dataset))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2381
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Data Set and Data Loader}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  As shown below, a DataLoader from \texttt{torch.utils.data} can be
  used to check the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Batch Size: 5
Inputs Shape: torch.Size([5, 133])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[6.2000e+01, 5.0100e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 2.0000e+00,
         1.8000e+01, 1.2310e+03, 1.6000e+01, 1.8950e+03, 4.0000e+00, 1.3000e+01,
         1.6070e+03, 6.3700e+02, 5.1600e+02, 5.1000e+01, 1.0000e+00, 1.0000e+00,
         1.0000e+00, 6.0000e+00, 0.0000e+00, 4.5000e+01, 4.6000e+01, 2.0000e+00,
         1.4210e+03, 1.2490e+03, 1.8890e+03, 1.8880e+03, 1.8670e+03, 5.5500e+02,
         1.8040e+03, 1.0000e+00, 1.8000e+01, 1.1880e+03, 7.7300e+02, 7.7400e+02,
         7.8200e+02, 7.8000e+02, 6.2600e+02, 7.5700e+02, 7.5100e+02, 3.4700e+02,
         7.5000e+02, 7.7900e+02, 6.2900e+02, 5.9000e+01, 7.1500e+02, 7.1700e+02,
         7.3000e+02, 6.5600e+02, 2.1000e+01, 5.4200e+02, 1.7300e+02, 4.6500e+02,
         1.1000e+02, 3.9900e+02, 7.7800e+02, 7.7700e+02, 6.2900e+02, 7.8200e+02,
         7.8200e+02, 6.7500e+02, 6.7500e+02, 7.7100e+02, 7.7000e+01, 7.9000e+01,
         7.7900e+02, 7.7900e+02, 7.0500e+02, 6.6100e+02, 7.2900e+02, 7.3400e+02,
         4.7800e+02, 7.7300e+02, 7.7400e+02, 7.7300e+02, 7.7400e+02, 7.7100e+02,
         7.7200e+02, 7.7200e+02, 5.9000e+01, 7.1500e+02, 0.0000e+00, 7.2400e+02,
         7.7100e+02, 1.3000e+02, 4.9700e+02, 0.0000e+00, 7.0100e+02, 1.9800e+02,
         7.8200e+02, 1.4900e+02, 7.6300e+02, 7.0900e+02, 7.1700e+02, 7.3200e+02,
         6.2000e+01, 7.2000e+02, 7.7400e+02, 6.6000e+01, 7.8300e+02, 7.8300e+02,
         4.0100e+02, 7.2500e+02, 1.2600e+02, 5.0000e+00, 6.4300e+02, 3.2000e+01,
         3.2000e+01, 3.0000e+01, 7.4400e+02, 6.1100e+02, 7.3100e+02, 7.8300e+02,
         7.7600e+02, 7.8200e+02, 7.5700e+02, 7.8200e+02, 7.8300e+02, 7.8200e+02,
         2.9600e+02, 2.9600e+02, 6.9500e+02, 5.6900e+02, 7.4800e+02, 7.8200e+02,
         7.8300e+02, 4.0500e+02, 1.3400e+02, 7.1700e+02, 6.6100e+02, 3.6000e+01,
         3.6000e+01],
        [5.6000e+01, 4.9800e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 2.0000e+00,
         2.1000e+01, 1.2040e+03, 1.6000e+01, 1.8680e+03, 4.0000e+00, 1.2000e+01,
         5.1400e+02, 7.1500e+02, 6.5100e+02, 4.8000e+01, 4.7000e+01, 5.5000e+01,
         4.8000e+01, 2.9000e+01, 0.0000e+00, 6.7000e+01, 6.3000e+01, 2.0000e+00,
         2.3620e+03, 2.3560e+03, 1.8030e+03, 1.8950e+03, 1.9590e+03, 7.0400e+02,
         1.4150e+03, 4.0000e+00, 2.1000e+01, 1.1690e+03, 7.7300e+02, 7.7400e+02,
         7.8200e+02, 7.8000e+02, 6.2600e+02, 7.5700e+02, 7.5100e+02, 3.4700e+02,
         7.5000e+02, 7.7900e+02, 6.2900e+02, 5.9000e+01, 7.1500e+02, 7.1700e+02,
         7.3000e+02, 6.5600e+02, 2.1000e+01, 5.4200e+02, 1.7300e+02, 4.6500e+02,
         1.1000e+02, 3.9900e+02, 7.7800e+02, 7.7700e+02, 6.2900e+02, 7.8200e+02,
         7.8200e+02, 6.7500e+02, 6.7500e+02, 7.7100e+02, 7.7000e+01, 7.9000e+01,
         7.7900e+02, 7.7900e+02, 7.0500e+02, 6.6100e+02, 7.2900e+02, 7.3400e+02,
         4.7800e+02, 7.7300e+02, 7.7400e+02, 7.7300e+02, 7.7400e+02, 7.7100e+02,
         7.7200e+02, 7.7200e+02, 5.9000e+01, 7.1500e+02, 0.0000e+00, 7.2400e+02,
         7.7100e+02, 1.3000e+02, 4.9700e+02, 0.0000e+00, 7.0100e+02, 1.9800e+02,
         7.8200e+02, 1.4900e+02, 7.6300e+02, 7.0900e+02, 7.1700e+02, 7.3200e+02,
         6.2000e+01, 7.2000e+02, 7.7400e+02, 6.6000e+01, 7.8300e+02, 7.8300e+02,
         4.0100e+02, 7.2500e+02, 1.2600e+02, 5.0000e+00, 6.4300e+02, 3.2000e+01,
         3.2000e+01, 3.0000e+01, 7.4400e+02, 6.1100e+02, 7.3100e+02, 7.8300e+02,
         7.7600e+02, 7.8200e+02, 7.5700e+02, 7.8200e+02, 7.8300e+02, 7.8200e+02,
         2.9600e+02, 2.9600e+02, 6.9500e+02, 5.6900e+02, 7.4800e+02, 7.8200e+02,
         7.8300e+02, 4.0500e+02, 1.3400e+02, 7.1700e+02, 6.6100e+02, 3.4000e+01,
         3.4000e+01],
        [1.0000e+01, 4.6800e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 1.0000e+00,
         2.4000e+01, 1.1970e+03, 1.8000e+01, 2.2500e+03, 3.0000e+00, 1.6000e+01,
         5.9900e+02, 6.9200e+02, 6.3200e+02, 5.6000e+01, 4.5000e+01, 5.1000e+01,
         4.2000e+01, 2.7000e+01, 0.0000e+00, 2.8000e+01, 3.0000e+01, 1.0000e+00,
         2.2200e+02, 2.9600e+02, 2.2540e+03, 2.2530e+03, 2.2450e+03, 7.3800e+02,
         2.1920e+03, 2.0000e+00, 2.4000e+01, 1.1600e+03, 7.7300e+02, 7.7400e+02,
         7.8200e+02, 7.8000e+02, 6.2600e+02, 7.5700e+02, 7.5100e+02, 3.4700e+02,
         7.5000e+02, 7.7900e+02, 6.2900e+02, 5.9000e+01, 7.1500e+02, 7.1700e+02,
         7.3000e+02, 6.5600e+02, 2.1000e+01, 5.4200e+02, 1.7300e+02, 4.6500e+02,
         1.1000e+02, 3.9900e+02, 7.7800e+02, 7.7700e+02, 6.2900e+02, 7.8200e+02,
         7.8200e+02, 6.7500e+02, 6.7500e+02, 7.7100e+02, 7.7000e+01, 7.9000e+01,
         7.7900e+02, 7.7900e+02, 7.0500e+02, 6.6100e+02, 7.2900e+02, 7.3400e+02,
         4.7800e+02, 7.7300e+02, 7.7400e+02, 7.7300e+02, 7.7400e+02, 7.7100e+02,
         7.7200e+02, 7.7200e+02, 5.9000e+01, 7.1500e+02, 0.0000e+00, 7.2400e+02,
         7.7100e+02, 1.3000e+02, 4.9700e+02, 0.0000e+00, 7.0100e+02, 1.9800e+02,
         7.8200e+02, 1.4900e+02, 7.6300e+02, 7.0900e+02, 7.1700e+02, 7.3200e+02,
         6.2000e+01, 7.2000e+02, 7.7400e+02, 6.6000e+01, 7.8300e+02, 7.8300e+02,
         4.0100e+02, 7.2500e+02, 1.2600e+02, 5.0000e+00, 6.4300e+02, 3.2000e+01,
         3.2000e+01, 3.0000e+01, 7.4400e+02, 6.1100e+02, 7.3100e+02, 7.8300e+02,
         7.7600e+02, 7.8200e+02, 7.5700e+02, 7.8200e+02, 7.8300e+02, 7.8200e+02,
         2.9600e+02, 2.9600e+02, 6.9500e+02, 5.6900e+02, 7.4800e+02, 7.8200e+02,
         7.8300e+02, 4.0500e+02, 1.3400e+02, 7.1700e+02, 6.6100e+02, 3.9000e+01,
         3.9000e+01],
        [3.7000e+01, 3.6600e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 0.0000e+00,
         3.5000e+01, 1.1860e+03, 1.6000e+01, 1.8620e+03, 4.0000e+00, 1.2000e+01,
         1.2020e+03, 1.3080e+03, 1.1710e+03, 4.8000e+01, 4.7000e+01, 5.5000e+01,
         4.8000e+01, 2.9000e+01, 0.0000e+00, 6.7000e+01, 6.3000e+01, 2.0000e+00,
         1.3200e+02, 1.1200e+02, 1.8770e+03, 1.8690e+03, 1.8450e+03, 1.0800e+03,
         4.7700e+02, 3.0000e+00, 3.5000e+01, 1.1290e+03, 7.6300e+02, 7.7700e+02,
         7.7700e+02, 7.7400e+02, 1.5100e+02, 7.4700e+02, 7.1700e+02, 3.5200e+02,
         7.2800e+02, 7.5200e+02, 6.2800e+02, 5.1000e+01, 7.0400e+02, 7.0700e+02,
         7.1500e+02, 6.4200e+02, 2.0000e+01, 5.4000e+02, 1.7500e+02, 4.7000e+02,
         1.0900e+02, 4.0000e+02, 7.7300e+02, 7.7000e+02, 6.2800e+02, 7.7500e+02,
         7.7500e+02, 2.4200e+02, 2.4200e+02, 7.6500e+02, 8.0000e+01, 1.1000e+02,
         7.5200e+02, 7.5200e+02, 2.6800e+02, 2.3800e+02, 7.1700e+02, 7.2900e+02,
         8.0000e+00, 7.6300e+02, 7.7100e+02, 7.6300e+02, 7.7700e+02, 7.6500e+02,
         7.7400e+02, 7.7400e+02, 5.1000e+01, 7.0400e+02, 0.0000e+00, 7.3300e+02,
         7.7600e+02, 1.3200e+02, 4.5000e+01, 3.0000e+00, 1.4100e+02, 7.0100e+02,
         7.7400e+02, 2.5500e+02, 7.3400e+02, 2.7500e+02, 7.0700e+02, 7.1500e+02,
         7.3000e+01, 7.1300e+02, 7.7000e+02, 8.1000e+01, 7.6700e+02, 7.8200e+02,
         2.0000e+00, 7.8100e+02, 1.5400e+02, 1.1000e+01, 6.1700e+02, 4.3000e+01,
         6.4000e+01, 5.2000e+01, 7.2000e+02, 1.1000e+01, 3.7300e+02, 7.7400e+02,
         6.8500e+02, 7.7700e+02, 7.4600e+02, 7.7800e+02, 7.6900e+02, 7.7400e+02,
         2.5100e+02, 2.5100e+02, 6.8500e+02, 5.7100e+02, 4.7300e+02, 7.6700e+02,
         7.8200e+02, 2.0000e+00, 1.0000e+00, 7.0700e+02, 1.4000e+02, 3.4000e+01,
         3.4000e+01],
        [7.0000e+01, 7.2700e+02, 1.0000e+00, 2.0000e+00, 2.0000e+00, 7.0000e+00,
         5.7000e+01, 1.1300e+03, 1.9000e+01, 2.2730e+03, 2.0000e+00, 1.8000e+01,
         8.7000e+01, 3.8500e+02, 2.9300e+02, 5.9000e+01, 4.9000e+01, 5.6000e+01,
         4.7000e+01, 4.0000e+00, 0.0000e+00, 7.6000e+01, 6.8000e+01, 3.0000e+00,
         1.8500e+02, 1.9800e+02, 2.2710e+03, 2.2740e+03, 2.2690e+03, 4.2200e+02,
         1.4580e+03, 2.0000e+00, 5.7000e+01, 1.0940e+03, 7.6700e+02, 7.6500e+02,
         7.5900e+02, 7.5000e+02, 3.0100e+02, 7.1100e+02, 6.8500e+02, 5.6200e+02,
         7.1000e+02, 7.0400e+02, 6.2600e+02, 5.8000e+01, 6.8000e+02, 6.9300e+02,
         6.9200e+02, 6.3200e+02, 2.2000e+01, 5.4100e+02, 2.1200e+02, 4.7300e+02,
         1.1100e+02, 4.0300e+02, 7.5900e+02, 7.6000e+02, 6.2600e+02, 7.4200e+02,
         7.4200e+02, 4.0000e+02, 4.0000e+02, 7.5400e+02, 1.6000e+02, 1.8900e+02,
         7.0400e+02, 7.0400e+02, 4.2800e+02, 3.7700e+02, 7.0100e+02, 7.0900e+02,
         8.8000e+01, 7.6700e+02, 7.6000e+02, 7.6700e+02, 7.6600e+02, 7.5400e+02,
         7.6100e+02, 7.6100e+02, 5.8000e+01, 6.8000e+02, 0.0000e+00, 7.2600e+02,
         7.7200e+02, 2.1600e+02, 1.8700e+02, 6.9000e+01, 3.1200e+02, 5.6600e+02,
         7.4000e+02, 3.0600e+02, 7.0600e+02, 4.3200e+02, 6.9300e+02, 6.9100e+02,
         9.1000e+01, 6.9300e+02, 7.5900e+02, 1.1700e+02, 7.2400e+02, 7.5600e+02,
         3.1000e+01, 7.5200e+02, 1.7900e+02, 4.3000e+01, 5.7300e+02, 9.2000e+01,
         1.0900e+02, 9.7000e+01, 6.9700e+02, 1.6400e+02, 5.1600e+02, 7.5500e+02,
         7.1800e+02, 7.6000e+02, 7.5200e+02, 7.4600e+02, 7.3000e+02, 7.3800e+02,
         2.1200e+02, 2.1200e+02, 6.4900e+02, 5.7200e+02, 5.8000e+02, 7.2200e+02,
         7.5600e+02, 3.2000e+01, 2.5000e+01, 6.9300e+02, 3.2600e+02, 4.1000e+01,
         4.1000e+01]])
Targets: tensor([ 558.,  660.,  636., 1220.,  308.])
\end{verbatim}

\end{tcolorbox}

\section{Step 4: Preprocessing}\label{sec-preprocessing-33}

Preprocessing is handled by \texttt{Lightning} and \texttt{PyTorch}. It
is described in the
\href{https://lightning.ai/docs/pytorch/stable/data/datamodule.html}{LIGHTNINGDATAMODULE}
documentation. Here you can find information about the
\texttt{transforms} methods.

\section{\texorpdfstring{Step 5: Select the Core Model
(\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select the Core Model (algorithm) and core\_model\_hyper\_dict}}\label{sec-selection-of-the-algorithm-33}

\texttt{spotPython} includes the \texttt{NetLightRegression} class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/netlightregression.py}{{[}SOURCE{]}}
for configurable neural networks. The class is imported here. It
inherits from the class \texttt{Lightning.LightningModule}, which is the
base class for all models in \texttt{Lightning}.
\texttt{Lightning.LightningModule} is a subclass of
\texttt{torch.nn.Module} and provides additional functionality for the
training and testing of neural networks. The class
\texttt{Lightning.LightningModule} is described in the
\href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{Lightning
documentation}.

\begin{itemize}
\tightlist
\item
  Here we simply add the NN Model to the fun\_control dictionary by
  calling the function \texttt{add\_core\_model\_to\_fun\_control}:
\end{itemize}

We can use aconfiguration from the \texttt{spotPython} package:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.regression.netlightregression }\ImportTok{import}\NormalTok{ NetLightRegression}
\ImportTok{from}\NormalTok{ spotPython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              core\_model}\OperatorTok{=}\NormalTok{NetLightRegression,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{LightHyperDict)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Alternatively, we can use a userr configuration from the subdirectory
  \texttt{userModel}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{import}\NormalTok{ sys}
\NormalTok{sys.path.insert(}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}./userModel\textquotesingle{}}\NormalTok{)}
\ImportTok{import}\NormalTok{ netlightregression}
\ImportTok{import}\NormalTok{ light\_hyper\_dict}
\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              core\_model}\OperatorTok{=}\NormalTok{netlightregression.NetLightRegression,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{light\_hyper\_dict.LightHyperDict)}
\end{Highlighting}
\end{Shaded}

The hyperparameters of the model are specified in the
\texttt{core\_model\_hyper\_dict} dictionary
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperdict/light_hyper_dict.json}{{[}SOURCE{]}}.

\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{sec-modification-of-hyperparameters-33}

\texttt{spotPython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Small number of epochs for demonstration purposes}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  \texttt{epochs} and \texttt{patience} are set to small values for
  demonstration purposes. These values are too small for a real
  application.
\item
  More resonable values are, e.g.:

  \begin{itemize}
  \tightlist
  \item
    \texttt{set\_control\_hyperparameter\_value(fun\_control,\ "epochs",\ {[}7,\ 9{]})}
    and
  \item
    \texttt{set\_control\_hyperparameter\_value(fun\_control,\ "patience",\ {[}2,\ 7{]})}
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Pre-experimental Results}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The following hyperparameters
  \{Table~\ref{tbl-pre-experimental-results-33}\} have generated
  acceptable results (obtained in in pre-experimental runs):
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\caption{Table 1: Pre-experimental results for the user specified data
set. The test set size is 715, the train set size is 1167, and the batch
size is 16.}\label{tbl-pre-experimental-results-33}\tabularnewline
\toprule\noalign{}
Hyperparameter & Value \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Hyperparameter & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{act\_fn} & \texttt{LeakyReLU} \\
\texttt{batch\_size} & \texttt{16} \\
\texttt{dropout\_prob} & \texttt{0.01} \\
\texttt{epochs} & \texttt{512} \\
\texttt{initialization} & \texttt{Default} \\
\texttt{l1} & \texttt{128} \\
\texttt{lr\_mult} & \texttt{0.5} \\
\texttt{optimizer} & \texttt{Adagrad} \\
\texttt{patience} & \texttt{16} \\
\end{longtable}

Therefore, we will use these values as the starting poing for the
hyperparameter tuning.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_hyperparameter\_value}

\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{5}\NormalTok{, }\DecValTok{9}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [}
                \StringTok{"Adadelta"}\NormalTok{,}
                \StringTok{"Adamax"}\NormalTok{,}
                \StringTok{"Adagrad"}
\NormalTok{            ])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.005}\NormalTok{, }\FloatTok{0.25}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"lr\_mult"}\NormalTok{, [}\FloatTok{0.25}\NormalTok{, }\FloatTok{5.0}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{])}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"act\_fn"}\NormalTok{,[}
                \StringTok{"ReLU"}\NormalTok{,}
                \StringTok{"LeakyReLU"}\NormalTok{,}
\NormalTok{            ] )}
\NormalTok{set\_control\_hyperparameter\_value(fun\_control, }\StringTok{"initialization"}\NormalTok{,[}\StringTok{"Default"}\NormalTok{] )}
\end{Highlighting}
\end{Shaded}

Now, the dictionary \texttt{fun\_control} contains all information
needed for the hyperparameter tuning. Before the hyperparameter tuning
is started, it is recommended to take a look at the experimental design.
The method \texttt{gen\_design\_table}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/utils/eda.py}{{[}SOURCE{]}}
generates a design table as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |   5     |    9    | transform_power_2_int |
| epochs         | int    | 4         |   5     |   10    | transform_power_2_int |
| batch_size     | int    | 4         |   3     |    6    | transform_power_2_int |
| act_fn         | factor | ReLU      |   0     |    1    | None                  |
| optimizer      | factor | SGD       |   0     |    2    | None                  |
| dropout_prob   | float  | 0.01      |   0.005 |    0.25 | None                  |
| lr_mult        | float  | 1.0       |   0.25  |    5    | None                  |
| patience       | int    | 2         |   3     |    5    | transform_power_2_int |
| initialization | factor | Default   |   0     |    0    | None                  |
\end{verbatim}

This allows to check if all information is available and if the
information is correct.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Hyperparameters of the Tuned Model and the \texttt{fun\_control}
Dictionary}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The updated \texttt{fun\_control} dictionary can be shown with the
command \texttt{fun\_control{[}"core\_model\_hyper\_dict"{]}}.

\end{tcolorbox}

\section{Step 7: Data Splitting, the Objective (Loss) Function and the
Metric}\label{step-7-data-splitting-the-objective-loss-function-and-the-metric-2}

\subsection{Evaluation}\label{sec-selection-of-target-function-33}

The evaluation procedure requires the specification of two elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the way how the data is split into a train and a test set
\item
  the loss function (and a metric).
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Data Splitting in Lightning}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The data splitting is handled by \texttt{Lightning}.

\end{tcolorbox}

\subsection{Loss Function}\label{sec-loss-function-33}

The loss function is specified in the configurable network class
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/regression/netlightregression.py}{{[}SOURCE{]}}
We will use MSE.

\subsection{Metric}\label{sec-metric-33}

\begin{itemize}
\tightlist
\item
  Similar to the loss function, the metric is specified in the
  configurable network class
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/regression/netlightregression.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Loss Function and Metric in Lightning}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The loss function and the metric are not hyperparameters that can be
  tuned with \texttt{spotPython}.
\item
  They are handled by \texttt{Lightning}.
\end{itemize}

\end{tcolorbox}

\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-2}

\subsection{Preparing the SPOT Call}\label{sec-prepare-spot-call-33}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ design\_control\_init, surrogate\_control\_init}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\NormalTok{INIT\_SIZE,}
\NormalTok{                                     repeats}\OperatorTok{=}\NormalTok{REPEATS,)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(noise}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                                            n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{                                            min\_Lambda}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{,}
\NormalTok{                                            max\_Lambda}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{                                            log\_level}\OperatorTok{=}\DecValTok{10}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Modifying Values in the Control Dictionaries}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The values in the control dictionaries can be modified with the
  function \texttt{set\_control\_key\_value}
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/hyperparameters/values.py}{{[}SOURCE{]}},
  for example:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{set\_control\_key\_value(control\_dict=surrogate\_control,}
\NormalTok{                        key="noise",}
\NormalTok{                        value=True,}
\NormalTok{                        replace=True)                       }
\NormalTok{set\_control\_key\_value(control\_dict=surrogate\_control,}
\NormalTok{                        key="n\_theta",}
\NormalTok{                        value=2,}
\NormalTok{                        replace=True)      }
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{\texorpdfstring{The Objective Function
\texttt{fun}}{The Objective Function fun}}\label{sec-the-objective-function-33}

The objective function \texttt{fun} from the class \texttt{HyperLight}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/fun/hyperlight.py}{{[}SOURCE{]}}
is selected next. It implements an interface from \texttt{PyTorch}'s
training, validation, and testing methods to \texttt{spotPython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight(log\_level}\OperatorTok{=}\DecValTok{10}\NormalTok{).fun}
\end{Highlighting}
\end{Shaded}

\subsection{Showing the fun\_control
Dictionary}\label{sec-show-fun-control-33}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\NormalTok{pprint.pprint(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'CHECKPOINT_PATH': 'runs/saved_models/',
 'DATASET_PATH': 'data/',
 'RESULTS_PATH': 'results/',
 'TENSORBOARD_PATH': 'runs/',
 '_L_in': 133,
 '_L_out': 1,
 'accelerator': 'auto',
 'core_model': <class 'netlightregression.NetLightRegression'>,
 'core_model_hyper_dict': {'act_fn': {'class_name': 'spotPython.torch.activation',
                                      'core_model_parameter_type': 'instance()',
                                      'default': 'ReLU',
                                      'levels': ['ReLU', 'LeakyReLU'],
                                      'lower': 0,
                                      'transform': 'None',
                                      'type': 'factor',
                                      'upper': 1},
                           'batch_size': {'default': 4,
                                          'lower': 3,
                                          'transform': 'transform_power_2_int',
                                          'type': 'int',
                                          'upper': 6},
                           'dropout_prob': {'default': 0.01,
                                            'lower': 0.005,
                                            'transform': 'None',
                                            'type': 'float',
                                            'upper': 0.25},
                           'epochs': {'default': 4,
                                      'lower': 5,
                                      'transform': 'transform_power_2_int',
                                      'type': 'int',
                                      'upper': 10},
                           'initialization': {'core_model_parameter_type': 'str',
                                              'default': 'Default',
                                              'levels': ['Default'],
                                              'lower': 0,
                                              'transform': 'None',
                                              'type': 'factor',
                                              'upper': 0},
                           'l1': {'default': 3,
                                  'lower': 5,
                                  'transform': 'transform_power_2_int',
                                  'type': 'int',
                                  'upper': 9},
                           'lr_mult': {'default': 1.0,
                                       'lower': 0.25,
                                       'transform': 'None',
                                       'type': 'float',
                                       'upper': 5.0},
                           'optimizer': {'class_name': 'torch.optim',
                                         'core_model_parameter_type': 'str',
                                         'default': 'SGD',
                                         'levels': ['Adadelta',
                                                    'Adamax',
                                                    'Adagrad'],
                                         'lower': 0,
                                         'transform': 'None',
                                         'type': 'factor',
                                         'upper': 2},
                           'patience': {'default': 2,
                                        'lower': 3,
                                        'transform': 'transform_power_2_int',
                                        'type': 'int',
                                        'upper': 5}},
 'counter': 0,
 'data': None,
 'data_dir': './data',
 'data_module': None,
 'data_set': <spotPython.data.pkldataset.PKLDataset object at 0x2cea1f090>,
 'design': None,
 'device': 'mps',
 'devices': 1,
 'enable_progress_bar': False,
 'eval': None,
 'fun_evals': inf,
 'fun_repeats': 2,
 'infill_criterion': 'y',
 'k_folds': 3,
 'log_level': 10,
 'loss_function': None,
 'lower': array([3. , 4. , 1. , 0. , 0. , 0. , 0.1, 2. , 0. ]),
 'max_time': 10,
 'metric_params': {},
 'metric_river': None,
 'metric_sklearn': None,
 'metric_torch': None,
 'model_dict': {},
 'n_points': 1,
 'n_samples': None,
 'noise': False,
 'num_workers': 0,
 'ocba_delta': 1,
 'optimizer': None,
 'path': None,
 'prep_model': None,
 'save_model': False,
 'seed': 123,
 'show_batch_interval': 1000000,
 'show_models': False,
 'show_progress': True,
 'shuffle': None,
 'sigma': 0.0,
 'spot_tensorboard_path': 'runs/spot_logs/032_p040025_2024-01-15_03-37-22',
 'spot_writer': <torch.utils.tensorboard.writer.SummaryWriter object at 0x2c8a313d0>,
 'target_column': None,
 'task': None,
 'test': None,
 'test_seed': 1234,
 'test_size': 0.3,
 'tolerance_x': 1.4901161193847656e-08,
 'train': None,
 'upper': array([ 8.  ,  9.  ,  4.  ,  5.  , 11.  ,  0.25, 10.  ,  6.  ,  2.  ]),
 'var_name': ['l1',
              'epochs',
              'batch_size',
              'act_fn',
              'optimizer',
              'dropout_prob',
              'lr_mult',
              'patience',
              'initialization'],
 'var_type': ['int',
              'int',
              'int',
              'factor',
              'factor',
              'float',
              'float',
              'int',
              'factor'],
 'verbosity': 1,
 'weights': 1.0}
\end{verbatim}

\subsection{Starting the Hyperparameter
Tuning}\label{sec-call-the-hyperparameter-tuner-33}

The \texttt{spotPython} hyperparameter tuning is started by calling the
\texttt{Spot} function
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                       fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                       design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                       surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.20560368458693354,
 'epochs': 128,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': 4.561411125937148,
 'optimizer': 'Adamax',
 'patience': 8}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 9691.404296875, 'hp_metric': 9691.404296875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.20560368458693354,
 'epochs': 128,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': 4.561411125937148,
 'optimizer': 'Adamax',
 'patience': 8}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 82330.359375, 'hp_metric': 82330.359375}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.02426280739731016,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.322803150188513,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5680.0966796875, 'hp_metric': 5680.0966796875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.02426280739731016,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.322803150188513,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2287.1826171875, 'hp_metric': 2287.1826171875}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 8,
 'dropout_prob': 0.07443535216560176,
 'epochs': 512,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': 2.863550239963548,
 'optimizer': 'Adamax',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 8
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 8
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 45320.10546875, 'hp_metric': 45320.10546875}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 8,
 'dropout_prob': 0.07443535216560176,
 'epochs': 512,
 'initialization': 'Default',
 'l1': 32,
 'lr_mult': 2.863550239963548,
 'optimizer': 'Adamax',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 8
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 8
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 44621.0390625, 'hp_metric': 44621.0390625}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 32,
 'dropout_prob': 0.18567441685948113,
 'epochs': 64,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': 4.089366778039889,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 109158.8515625, 'hp_metric': 109158.8515625}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 32,
 'dropout_prob': 0.18567441685948113,
 'epochs': 64,
 'initialization': 'Default',
 'l1': 64,
 'lr_mult': 4.089366778039889,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 151683.59375, 'hp_metric': 151683.59375}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 32,
 'dropout_prob': 0.13080206803532848,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 2.0077669995722074,
 'optimizer': 'Adamax',
 'patience': 8}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 8908.8837890625, 'hp_metric': 8908.8837890625}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 32,
 'dropout_prob': 0.13080206803532848,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 2.0077669995722074,
 'optimizer': 'Adamax',
 'patience': 8}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 16831.779296875, 'hp_metric': 16831.779296875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 32,
 'dropout_prob': 0.11983781472185623,
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 0.7181246235247182,
 'optimizer': 'Adadelta',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 170177.1875, 'hp_metric': 170177.1875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 32,
 'dropout_prob': 0.11983781472185623,
 'epochs': 1024,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 0.7181246235247182,
 'optimizer': 'Adadelta',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 32
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 32
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 4781.1748046875, 'hp_metric': 4781.1748046875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 16,
 'dropout_prob': 0.09366384017851244,
 'epochs': 64,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 3.301460285082441,
 'optimizer': 'Adadelta',
 'patience': 32}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 363511.125, 'hp_metric': 363511.125}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 16,
 'dropout_prob': 0.09366384017851244,
 'epochs': 64,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 3.301460285082441,
 'optimizer': 'Adadelta',
 'patience': 32}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 18567.58203125, 'hp_metric': 18567.58203125}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 16,
 'dropout_prob': 0.03540914713540524,
 'epochs': 32,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 3.7811076839948083,
 'optimizer': 'Adamax',
 'patience': 32}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2381.155517578125, 'hp_metric': 2381.155517578125}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 16,
 'dropout_prob': 0.03540914713540524,
 'epochs': 32,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 3.7811076839948083,
 'optimizer': 'Adamax',
 'patience': 32}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 1633.8154296875, 'hp_metric': 1633.8154296875}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 16,
 'dropout_prob': 0.23933451103594558,
 'epochs': 128,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 0.9693203335370014,
 'optimizer': 'Adamax',
 'patience': 8}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 19519.423828125, 'hp_metric': 19519.423828125}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 16,
 'dropout_prob': 0.23933451103594558,
 'epochs': 128,
 'initialization': 'Default',
 'l1': 128,
 'lr_mult': 0.9693203335370014,
 'optimizer': 'Adamax',
 'patience': 8}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 6648.47900390625, 'hp_metric': 6648.47900390625}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 16,
 'dropout_prob': 0.15827496187066842,
 'epochs': 512,
 'initialization': 'Default',
 'l1': 512,
 'lr_mult': 2.4208305977937328,
 'optimizer': 'Adamax',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 6104.76220703125, 'hp_metric': 6104.76220703125}

In fun(): config:
{'act_fn': ReLU(),
 'batch_size': 16,
 'dropout_prob': 0.15827496187066842,
 'epochs': 512,
 'initialization': 'Default',
 'l1': 512,
 'lr_mult': 2.4208305977937328,
 'optimizer': 'Adamax',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 16
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 16
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2992.532958984375, 'hp_metric': 2992.532958984375}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.04825016876158548,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4769991072624946,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2337.91748046875, 'hp_metric': 2337.91748046875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.04825016876158548,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4769991072624946,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3264.527099609375, 'hp_metric': 3264.527099609375}
spotPython tuning: 1633.8154296875 [#---------] 7.76% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05037009219038317,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.490628647450869,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 1807.1495361328125, 'hp_metric': 1807.1495361328125}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05037009219038317,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.490628647450869,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3191.741943359375, 'hp_metric': 3191.741943359375}
spotPython tuning: 1633.8154296875 [#---------] 14.46% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05260472442202331,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.505002772751222,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 8475.75, 'hp_metric': 8475.75}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05260472442202331,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.505002772751222,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5522.3935546875, 'hp_metric': 5522.3935546875}
spotPython tuning: 1633.8154296875 [##--------] 18.76% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05413475936399615,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5148817736749298,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3902.89501953125, 'hp_metric': 3902.89501953125}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05413475936399615,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5148817736749298,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2615.920654296875, 'hp_metric': 2615.920654296875}
spotPython tuning: 1633.8154296875 [##--------] 22.57% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.055933023612466014,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5264365695558901,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3183.96435546875, 'hp_metric': 3183.96435546875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.055933023612466014,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5264365695558901,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 56039.2890625, 'hp_metric': 56039.2890625}
spotPython tuning: 1633.8154296875 [###-------] 27.11% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05263943958400707,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5052098265780478,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2725.66357421875, 'hp_metric': 2725.66357421875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05263943958400707,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5052098265780478,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 6538.43310546875, 'hp_metric': 6538.43310546875}
spotPython tuning: 1633.8154296875 [###-------] 30.52% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05316632156290128,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5086702767555806,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5130.96630859375, 'hp_metric': 5130.96630859375}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05316632156290128,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5086702767555806,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2350.8046875, 'hp_metric': 2350.8046875}
spotPython tuning: 1633.8154296875 [###-------] 34.35% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.053771679137023694,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5124802181853274,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 10390.2138671875, 'hp_metric': 10390.2138671875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.053771679137023694,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5124802181853274,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2203.74755859375, 'hp_metric': 2203.74755859375}
spotPython tuning: 1633.8154296875 [####------] 40.11% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05402750909623282,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5141703050361923,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3288.8427734375, 'hp_metric': 3288.8427734375}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05402750909623282,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5141703050361923,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5342.2548828125, 'hp_metric': 5342.2548828125}
spotPython tuning: 1633.8154296875 [####------] 44.19% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05448797256538013,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5170610907476367,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 1620.1785888671875, 'hp_metric': 1620.1785888671875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05448797256538013,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5170610907476367,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3572.1640625, 'hp_metric': 3572.1640625}
spotPython tuning: 1620.1785888671875 [#####-----] 48.67% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.055174351358467405,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5215060127606514,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2750.79541015625, 'hp_metric': 2750.79541015625}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.055174351358467405,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5215060127606514,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5175.3466796875, 'hp_metric': 5175.3466796875}
spotPython tuning: 1620.1785888671875 [#####-----] 53.28% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.055696958537016866,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.524867959583039,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5328.39404296875, 'hp_metric': 5328.39404296875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.055696958537016866,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.524867959583039,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5937.96533203125, 'hp_metric': 5937.96533203125}
spotPython tuning: 1620.1785888671875 [######----] 58.84% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05589887172269804,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.526150803701278,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2388.19189453125, 'hp_metric': 2388.19189453125}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05589887172269804,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.526150803701278,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 7502.37548828125, 'hp_metric': 7502.37548828125}
spotPython tuning: 1620.1785888671875 [#######---] 65.69% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.0562113034404144,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5280954720377455,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 36894.0625, 'hp_metric': 36894.0625}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.0562113034404144,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5280954720377455,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 5805.9248046875, 'hp_metric': 5805.9248046875}
spotPython tuning: 1620.1785888671875 [#######---] 68.86% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.052806491168032314,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5067848529726446,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 9569.8994140625, 'hp_metric': 9569.8994140625}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.052806491168032314,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5067848529726446,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3403.76953125, 'hp_metric': 3403.76953125}
spotPython tuning: 1620.1785888671875 [########--] 75.34% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05245253149077113,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5040814155528057,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 4728.78515625, 'hp_metric': 4728.78515625}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05245253149077113,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.5040814155528057,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 12009.7314453125, 'hp_metric': 12009.7314453125}
spotPython tuning: 1620.1785888671875 [########--] 79.78% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05196894703679841,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.501019508422615,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3601.85400390625, 'hp_metric': 3601.85400390625}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05196894703679841,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.501019508422615,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 12909.5517578125, 'hp_metric': 12909.5517578125}
spotPython tuning: 1620.1785888671875 [########--] 84.37% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.051474986340905554,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4979255024445266,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2890.673583984375, 'hp_metric': 2890.673583984375}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.051474986340905554,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4979255024445266,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 4002.3369140625, 'hp_metric': 4002.3369140625}
spotPython tuning: 1620.1785888671875 [#########-] 88.33% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05080741682078949,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4935919826787676,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 3963.92041015625, 'hp_metric': 3963.92041015625}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.05080741682078949,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4935919826787676,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 6113.154296875, 'hp_metric': 6113.154296875}
spotPython tuning: 1620.1785888671875 [#########-] 92.56% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.050055475077940365,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4887675150407989,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 6603.4912109375, 'hp_metric': 6603.4912109375}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.050055475077940365,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4887675150407989,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 2470.803955078125, 'hp_metric': 2470.803955078125}
spotPython tuning: 1620.1785888671875 [##########] 96.64% 

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.04912791894409341,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4828174957876712,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 9845.26171875, 'hp_metric': 9845.26171875}

In fun(): config:
{'act_fn': LeakyReLU(),
 'batch_size': 64,
 'dropout_prob': 0.04912791894409341,
 'epochs': 256,
 'initialization': 'Default',
 'l1': 256,
 'lr_mult': 1.4828174957876712,
 'optimizer': 'Adagrad',
 'patience': 16}
train_model(): Test set size: 715
train_model(): Train set size: 1167
train_model(): Batch size: 64
LightDataModule: train_dataloader(). Training set size: 1167
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
train_model result: {'val_loss': 8919.93359375, 'hp_metric': 8919.93359375}
spotPython tuning: 1620.1785888671875 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      9691.404296875       │
│         val_loss          │      9691.404296875       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       82330.359375        │
│         val_loss          │       82330.359375        │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      5680.0966796875      │
│         val_loss          │      5680.0966796875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      2287.1826171875      │
│         val_loss          │      2287.1826171875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      45320.10546875       │
│         val_loss          │      45320.10546875       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       44621.0390625       │
│         val_loss          │       44621.0390625       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      109158.8515625       │
│         val_loss          │      109158.8515625       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       151683.59375        │
│         val_loss          │       151683.59375        │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      8908.8837890625      │
│         val_loss          │      8908.8837890625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      16831.779296875      │
│         val_loss          │      16831.779296875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │        170177.1875        │
│         val_loss          │        170177.1875        │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      4781.1748046875      │
│         val_loss          │      4781.1748046875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │        363511.125         │
│         val_loss          │        363511.125         │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      18567.58203125       │
│         val_loss          │      18567.58203125       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2381.155517578125     │
│         val_loss          │     2381.155517578125     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      1633.8154296875      │
│         val_loss          │      1633.8154296875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      19519.423828125      │
│         val_loss          │      19519.423828125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     6648.47900390625      │
│         val_loss          │     6648.47900390625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     6104.76220703125      │
│         val_loss          │     6104.76220703125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2992.532958984375     │
│         val_loss          │     2992.532958984375     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2337.91748046875      │
│         val_loss          │     2337.91748046875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3264.527099609375     │
│         val_loss          │     3264.527099609375     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │    1807.1495361328125     │
│         val_loss          │    1807.1495361328125     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3191.741943359375     │
│         val_loss          │     3191.741943359375     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │          8475.75          │
│         val_loss          │          8475.75          │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      5522.3935546875      │
│         val_loss          │      5522.3935546875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3902.89501953125      │
│         val_loss          │     3902.89501953125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2615.920654296875     │
│         val_loss          │     2615.920654296875     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3183.96435546875      │
│         val_loss          │     3183.96435546875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       56039.2890625       │
│         val_loss          │       56039.2890625       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2725.66357421875      │
│         val_loss          │     2725.66357421875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     6538.43310546875      │
│         val_loss          │     6538.43310546875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     5130.96630859375      │
│         val_loss          │     5130.96630859375      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       2350.8046875        │
│         val_loss          │       2350.8046875        │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     10390.2138671875      │
│         val_loss          │     10390.2138671875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2203.74755859375      │
│         val_loss          │     2203.74755859375      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      3288.8427734375      │
│         val_loss          │      3288.8427734375      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      5342.2548828125      │
│         val_loss          │      5342.2548828125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │    1620.1785888671875     │
│         val_loss          │    1620.1785888671875     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       3572.1640625        │
│         val_loss          │       3572.1640625        │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2750.79541015625      │
│         val_loss          │     2750.79541015625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      5175.3466796875      │
│         val_loss          │      5175.3466796875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     5328.39404296875      │
│         val_loss          │     5328.39404296875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     5937.96533203125      │
│         val_loss          │     5937.96533203125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2388.19189453125      │
│         val_loss          │     2388.19189453125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     7502.37548828125      │
│         val_loss          │     7502.37548828125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │        36894.0625         │
│         val_loss          │        36894.0625         │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      5805.9248046875      │
│         val_loss          │      5805.9248046875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      9569.8994140625      │
│         val_loss          │      9569.8994140625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       3403.76953125       │
│         val_loss          │       3403.76953125       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       4728.78515625       │
│         val_loss          │       4728.78515625       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     12009.7314453125      │
│         val_loss          │     12009.7314453125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3601.85400390625      │
│         val_loss          │     3601.85400390625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     12909.5517578125      │
│         val_loss          │     12909.5517578125      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2890.673583984375     │
│         val_loss          │     2890.673583984375     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      4002.3369140625      │
│         val_loss          │      4002.3369140625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     3963.92041015625      │
│         val_loss          │     3963.92041015625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      6113.154296875       │
│         val_loss          │      6113.154296875       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      6603.4912109375      │
│         val_loss          │      6603.4912109375      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     2470.803955078125     │
│         val_loss          │     2470.803955078125     │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       9845.26171875       │
│         val_loss          │       9845.26171875       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │       8919.93359375       │
│         val_loss          │       8919.93359375       │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2d337c750>
\end{verbatim}

\section{Step 9: Tensorboard}\label{sec-tensorboard-33}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="runs/"}
\end{Highlighting}
\end{Shaded}

Further information can be found in the
\href{https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html}{PyTorch
Lightning documentation} for Tensorboard.

\section{Step 10: Results}\label{sec-results-33}

After the hyperparameter tuning run is finished, the results can be
analyzed.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ spot\_tuner.noise:}
    \BuiltInTok{print}\NormalTok{(spot\_tuner.min\_mean\_X)}
    \BuiltInTok{print}\NormalTok{(spot\_tuner.min\_mean\_y)}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(spot\_tuner.min\_X)}
    \BuiltInTok{print}\NormalTok{(spot\_tuner.min\_y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[8.         8.         6.         1.         2.         0.05448797
 1.51706109 4.        ]
1620.1785888671875
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+}\StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{033_spot_lightning_linear_sensitive_files/figure-pdf/cell-15-output-1.pdf}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.eda }\ImportTok{import}\NormalTok{ gen\_design\_table}
\BuiltInTok{print}\NormalTok{(gen\_design\_table(fun\_control}\OperatorTok{=}\NormalTok{fun\_control, spot}\OperatorTok{=}\NormalTok{spot\_tuner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     5.0 |     9.0 | 8.0                 | transform_power_2_int |         0.00 |         |
| epochs         | int    | 4         |     5.0 |    10.0 | 8.0                 | transform_power_2_int |        79.92 | **      |
| batch_size     | int    | 4         |     3.0 |     6.0 | 6.0                 | transform_power_2_int |         0.00 |         |
| act_fn         | factor | ReLU      |     0.0 |     1.0 | LeakyReLU           | None                  |         0.01 |         |
| optimizer      | factor | SGD       |     0.0 |     2.0 | Adagrad             | None                  |       100.00 | ***     |
| dropout_prob   | float  | 0.01      |   0.005 |    0.25 | 0.05448797256538013 | None                  |         0.01 |         |
| lr_mult        | float  | 1.0       |    0.25 |     5.0 | 1.5170610907476367  | None                  |         0.00 |         |
| patience       | int    | 2         |     3.0 |     5.0 | 4.0                 | transform_power_2_int |         0.00 |         |
| initialization | factor | Default   |     0.0 |     0.0 | Default             | None                  |         0.00 |         |
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.025}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+} \StringTok{"\_importance.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{033_spot_lightning_linear_sensitive_files/figure-pdf/cell-17-output-1.pdf}

}

\caption{Variable importance plot, threshold 0.025.}

\end{figure}%

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-33}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner, fun\_control)}
\BuiltInTok{print}\NormalTok{(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'l1': 256, 'epochs': 256, 'batch_size': 64, 'act_fn': LeakyReLU(), 'optimizer': 'Adagrad', 'dropout_prob': 0.05448797256538013, 'lr_mult': 1.5170610907476367, 'patience': 16, 'initialization': 'Default'}
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Test on the full data set
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.testmodel }\ImportTok{import}\NormalTok{ test\_model}
\NormalTok{test\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule: train_dataloader(). Training set size: 381
LightDataModule: train_dataloader(). batch_size: 64
LightDataModule: train_dataloader(). num_workers: 0
LightDataModule: test_dataloader(). Training set size: 1429
LightDataModule: test_dataloader(). batch_size: 64
LightDataModule: test_dataloader(). num_workers: 0
test_model result: {'val_loss': 6563.67822265625, 'hp_metric': 6563.67822265625}
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     6563.67822265625      │
│         val_loss          │     6563.67822265625      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
(6563.67822265625, 6563.67822265625)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.loadmodel }\ImportTok{import}\NormalTok{ load\_light\_from\_checkpoint}

\NormalTok{model\_loaded }\OperatorTok{=}\NormalTok{ load\_light\_from\_checkpoint(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
config: {'l1': 256, 'epochs': 256, 'batch_size': 64, 'act_fn': LeakyReLU(), 'optimizer': 'Adagrad', 'dropout_prob': 0.05448797256538013, 'lr_mult': 1.5170610907476367, 'patience': 16, 'initialization': 'Default'}
Loading model with 256_256_64_LeakyReLU_Adagrad_0.0545_1.5171_16_Default_TEST from runs/saved_models/256_256_64_LeakyReLU_Adagrad_0.0545_1.5171_16_Default_TEST/last.ckpt
Model: NetLightRegression(
  (layers): Sequential(
    (0): Linear(in_features=133, out_features=256, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.05448797256538013, inplace=False)
    (3): Linear(in_features=256, out_features=128, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.05448797256538013, inplace=False)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.05448797256538013, inplace=False)
    (9): Linear(in_features=128, out_features=64, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.05448797256538013, inplace=False)
    (12): Linear(in_features=64, out_features=1, bias=True)
  )
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OperatorTok{=} \StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\NormalTok{filename)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
epochs:  79.92198502293363
optimizer:  100.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{033_spot_lightning_linear_sensitive_files/figure-pdf/cell-21-output-2.pdf}

}

\caption{Contour plots.}

\end{figure}%

\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

Parallel coordinates plots

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\subsection{Cross Validation With
Lightning}\label{cross-validation-with-lightning-2}

\begin{itemize}
\tightlist
\item
  The \texttt{KFold} class from \texttt{sklearn.model\_selection} is
  used to generate the folds for cross-validation.
\item
  These mechanism is used to generate the folds for the final evaluation
  of the model.
\item
  The \texttt{CrossValidationDataModule} class
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/data/lightcrossvalidationdatamodule.py}{{[}SOURCE{]}}
  is used to generate the folds for the hyperparameter tuning process.
\item
  It is called from the \texttt{cv\_model} function
  \href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/light/cvmodel.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.light.cvmodel }\ImportTok{import}\NormalTok{ cv\_model}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"k\_folds"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"test\_size"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\FloatTok{0.6}\NormalTok{,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{cv\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
k: 0
Train Dataset Size: 1190
Val Dataset Size: 1191
train_model result: {'val_loss': 4961.3193359375, 'hp_metric': 4961.3193359375}
k: 1
Train Dataset Size: 1191
Val Dataset Size: 1190
train_model result: {'val_loss': 11453.3232421875, 'hp_metric': 11453.3232421875}
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │      4961.3193359375      │
│         val_loss          │      4961.3193359375      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         hp_metric         │     11453.3232421875      │
│         val_loss          │     11453.3232421875      │
└───────────────────────────┴───────────────────────────┘
\end{verbatim}

\begin{verbatim}
8207.3212890625
\end{verbatim}

\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters-3}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\subsection{Visualizing the Activation Distribution (Under
Development)}\label{visualizing-the-activation-distribution-under-development-2}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Reference:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The following code is based on
  \href{https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/02-activation-functions.html}{{[}PyTorch
  Lightning TUTORIAL 2: ACTIVATION FUNCTIONS{]}}, Author: Phillip Lippe,
  License: \href{https://creativecommons.org/licenses/by-sa/3.0/}{{[}CC
  BY-SA{]}}, Generated: 2023-03-15T09:52:39.179933.
\end{itemize}

\end{tcolorbox}

After we have trained the models, we can look at the actual activation
values that find inside the model. For instance, how many neurons are
set to zero in ReLU? Where do we find most values in Tanh? To answer
these questions, we can write a simple function which takes a trained
model, applies it to a batch of images, and plots the histogram of the
activations inside the network:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.torch.activation }\ImportTok{import}\NormalTok{ Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish}
\NormalTok{act\_fn\_by\_name }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigmoid"}\NormalTok{: Sigmoid, }\StringTok{"tanh"}\NormalTok{: Tanh, }\StringTok{"relu"}\NormalTok{: ReLU, }\StringTok{"leakyrelu"}\NormalTok{: LeakyReLU, }\StringTok{"elu"}\NormalTok{: ELU, }\StringTok{"swish"}\NormalTok{: Swish\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_config\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_one\_config\_from\_X(X, fun\_control)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\DecValTok{64}\NormalTok{, \_L\_out}\OperatorTok{=}\DecValTok{11}\NormalTok{)}
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NetLightRegression(
  (layers): Sequential(
    (0): Linear(in_features=64, out_features=256, bias=True)
    (1): LeakyReLU()
    (2): Dropout(p=0.05448797256538013, inplace=False)
    (3): Linear(in_features=256, out_features=128, bias=True)
    (4): LeakyReLU()
    (5): Dropout(p=0.05448797256538013, inplace=False)
    (6): Linear(in_features=128, out_features=128, bias=True)
    (7): LeakyReLU()
    (8): Dropout(p=0.05448797256538013, inplace=False)
    (9): Linear(in_features=128, out_features=64, bias=True)
    (10): LeakyReLU()
    (11): Dropout(p=0.05448797256538013, inplace=False)
    (12): Linear(in_features=64, out_features=11, bias=True)
  )
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from spotPython.utils.eda import visualize\_activations}
\CommentTok{\# visualize\_activations(model, color=f"C\{0\}")}
\end{Highlighting}
\end{Shaded}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Introduction to Jupyter
Notebook}\label{introduction-to-jupyter-notebook}

Jupyter Notebook is a widely used tool in the Data Science community. It
is easy to use and the produced code can be run per cell. This has a
huge advantage, because with other tools e.g.~(pycharm, vscode, etc.)
the whole script is executed. This can be a time consuming process,
especially when working with huge data sets.

\section{Different Notebook cells}\label{different-notebook-cells}

There are different cells that the notebook is currently supporting:

\begin{itemize}
\item
  code cells
\item
  markdown cells
\item
  raw cells
\end{itemize}

As a default, every cells in jupyter is set to ``code''

\subsection{Code cells}\label{code-cells}

The code cells are used to execute the code. They are following the
logic of the choosen kernel. Therefore, it is important to keep in mind
which programming language is currently used. Otherwise one might yield
an error because of the wrong syntax.

The code cells are executed my be \textbf{▶ Run} button (can be found in
the header of the notebook).

\subsection{Markdown cells}\label{markdown-cells}

The markdown cells are a usefull tool to comment the written code.
Especially with the help of headers can the code be brought in a more
readable format. If you are not familiar with the markdown syntax, you
can find a usefull cheat sheet here:
\href{https://www.ibm.com/docs/en/db2-event-store/2.0.0?topic=notebooks-markdown-jupyter-cheatsheet}{Markdown
Cheat Sheeet}

\subsection{Raw cells}\label{raw-cells}

The ``Raw NBConvert'' cell type can be used to render different code
formats into HTML or LaTeX by Sphinx. This information is stored in the
notebook metadata and converted appropriately.

\subsubsection{Usage}\label{usage}

To select a desired format from within Jupyter, select the cell
containing your special code and choose options from the following
dropdown menus:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select ``Raw NBConvert''
\item
  Switch the Cell Toolbar to ``Raw Cell Format'' (The cell toolbar can
  be found under View)
\item
  Chose the appropriate ``Raw NBConvert Format'' within the cell
\end{enumerate}

Data Science is fun

\section{Install Packages}\label{install-packages}

Because python is a heavily used programming language, there are many
different packags that can make your life easier. Sadly, there are only
a few standard packages that are already included in your python
enviroment. If you have the need to install a new package in your
enviroment, you can simply do that by exectuing the following code
snippet in a \textbf{code cell}

\texttt{!pip\ install\ numpy}

\begin{itemize}
\item
  The \emph{!} is used to run the cell as a shell command
\item
  \emph{pip} is package manager for python packages.
\item
  \emph{numpy} is the the package you want to install
\end{itemize}

\textbf{Hint:} It is often usefull to restart the kernel after
installing a package, otherwise loading the package could lead to an
error.

\section{Load Packages}\label{load-packages}

After successfully installing the package it is necessary to import them
before you can work with them. The import of the packages is done in the
following way:

\texttt{import\ numpy\ as\ np}

The imported packages are often abbreviated. This is because you need to
specify where the function is coming from.

The most common abbreviations for data science packages are:

\begin{longtable}[]{@{}lll@{}}
\caption{Abbreviations for data science packages}\tabularnewline
\toprule\noalign{}
Abbreviation & Package & Import \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Abbreviation & Package & Import \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
np & numpy & import numpy as np \\
pd & pandas & import pandas as pd \\
plt & matplotlib & import matplotlib.pyplot as plt \\
px & plotly & import plotly.exprss as px \\
tf & tensorflow & import tensorflow as tf \\
sns & seaborn & import seaborn as sns \\
dt & datetime & import datetime as dt \\
pkl & pickle & import pickle as pkl \\
\end{longtable}

\section{Functions in Python}\label{functions-in-python}

Because python is not using Semicolon's it is import to keep track of
indentation in your code. The indentation works as a placeholder for the
semicolons. This is especially important if your are defining loops,
functions, etc. \ldots{}

\textbf{Example:} We are defining a function that calculates the squared
sum of its input parameters

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ squared\_sum(x,y): }
\NormalTok{    z }\OperatorTok{=}\NormalTok{ x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ y}\OperatorTok{**}\DecValTok{2}
    \ControlFlowTok{return}\NormalTok{ z}
\end{Highlighting}
\end{Shaded}

If you are working with something that needs indentation, it will be
already done by the notebook.

\textbf{Hint:} Keep in mind that is good practice to use the
\emph{return} parameter. If you are not using \emph{return} and a
function has multiple paramaters that you would like to return, it will
only return the last one defined.

\section{List of Useful Jupyter Notebook
Shortcuts}\label{list-of-useful-jupyter-notebook-shortcuts}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{List of useful Jupyter Notebook Shortcuts}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Keyboard Shortcut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Menu Tools
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Keyboard Shortcut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Menu Tools
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Save notebook & Esc + s & File → Save and Checkpoint \\
Create new Cell & Esc + a (above), Esc + b (below) & Insert → Cell
above; Insert → Cell below \\
Run Cell & Ctrl + enter & Cell → Run Cell \\
Copy Cell & c & Copy Key \\
Paste Cell & v & Paste Key \\
Interrupt Kernel & Esc + i i & Kernel → Interrupt \\
Restart Kernel & Esc + 0 0 & Kernel → Restart \\
\end{longtable}

If you combine everything you can create beautiful graphics

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Generate 100 random data points along 3 dimensions}
\NormalTok{x, y, scale }\OperatorTok{=}\NormalTok{ np.random.randn(}\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\CommentTok{\# Map each onto a scatterplot we\textquotesingle{}ll create with Matplotlib}
\NormalTok{ax.scatter(x}\OperatorTok{=}\NormalTok{x, y}\OperatorTok{=}\NormalTok{y, c}\OperatorTok{=}\NormalTok{scale, s}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{abs}\NormalTok{(scale)}\OperatorTok{*}\DecValTok{500}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{"Some random data, created with the Jupyter Notebook!"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{a_01_intro_to_notebooks_files/figure-pdf/cell-3-output-1.pdf}

\chapter{Git Introduction}\label{git-introduction}

\section{Learning Objectives}\label{learning-objectives}

In this learning unit, you will learn how to set up Git as a version
control system for a project. The most important Git commands will be
explained. You will learn how to track and manage changes to your
projects with Git. Specifically:

\begin{itemize}
\tightlist
\item
  Initializing a repository: \texttt{git\ init}
\item
  Ignoring files: \texttt{.gitignore}
\item
  Adding files to the staging area: \texttt{git\ add}
\item
  Checking status changes: \texttt{git\ status}
\item
  Reviewing history: \texttt{git\ log}
\item
  Creating a new branch: \texttt{git\ branch}
\item
  Switching to the current branch: \texttt{git\ switch} and
  \texttt{git\ checkout}
\item
  Merging two branches: \texttt{git\ merge}
\item
  Resolving conflicts
\item
  Reverting changes: \texttt{git\ revert}
\item
  Uploading changes to GitLab: \texttt{git\ push}
\item
  Downloading changes from GitLab: \texttt{git\ pull}
\item
  Advanced: \texttt{git\ rebase}
\end{itemize}

\section{Basics of Git}\label{basics-of-git}

\subsection{\texorpdfstring{Initializing a Repository:
\texttt{git\ init}}{Initializing a Repository: git init}}\label{initializing-a-repository-git-init}

To set up Git as a version control system for your project, you need to
initialize a new Git repository at the top-level folder, which is the
working directory of your project. This is done using the
\texttt{git\ init} command.

All files in this folder and its subfolders will automatically become
part of the repository. Creating a Git repository is similar to adding
an all-powerful passive observer of all things to your project. Git sits
there, observes, and takes note of even the smallest changes, such as a
single character in a file within a repository with hundreds of files.
And it will tell you where these changes occurred if you forget. Once
Git is initialized, it monitors all changes made within the working
directory, and it tracks the history of events from that point forward.
For this purpose, a historical timeline is created for your project,
referred to as a ``branch,'' and the initial branch is named
\texttt{main}. So, when someone says they are on the
\texttt{main\ branch} or working on the \texttt{main\ branch}, it means
they are in the historical main timeline of the project. The Git
repository, often abbreviated as \texttt{repo}, is a virtual
representation of your project, including its history and branches, a
book, if you will, where you can look up and retrieve the entire history
of the project: you work in your working directory, and the Git
repository tracks and stores your work.

\subsection{\texorpdfstring{Ignoring Files:
\texttt{.gitignore}}{Ignoring Files: .gitignore}}\label{ignoring-files-.gitignore}

It's useful that Git watches and keeps an eye on everything in your
project. However, in most projects, there are files and folders that you
don't need or want to keep an eye on. These may include system files,
local project settings, libraries with dependencies, and so on.

You can exclude any file or folder from your Git repository by including
them in the \texttt{.gitignore} file. In the \texttt{.gitignore} file,
you create a list of file names, folder names, and other items that Git
should not track, and Git will ignore these items. Hence the name
``gitignore.'' Do you want to track a file that you previously ignored?
Simply remove the mention of the file in the gitignore file, and Git
will start tracking it again.

\subsection{\texorpdfstring{Adding Changes to the Staging Area:
\texttt{git\ add}}{Adding Changes to the Staging Area: git add}}\label{adding-changes-to-the-staging-area-git-add}

The interesting thing about Git as an all-powerful, passive observer of
all things is that it's very passive. As long as you don't tell Git what
to remember, it will passively observe the changes in the project folder
but do nothing.

When you make a change to your project that you want Git to include in
the project's history to take a snapshot of so you can refer back to it
later, your personal checkpoint, if you will, you need to first stage
the changes in the staging area. What is the staging area? The staging
area is where you collect changes to files that you want to include in
the project's history.

This is done using the \texttt{git\ add} command. You can specify which
files you want to add by naming them, or you can add all of them using
\texttt{-A}. By doing this, you're telling Git that you've made changes
and want it to remember these particular changes so you can recall them
later if needed. This is important because you can choose which changes
you want to stage, and those are the changes that will eventually be
transferred to the history.

\textbf{Note:} When you run \texttt{git\ add}, the changes are not
transferred to the project's history. They are only transferred to the
staging area.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example of git add from the beginning}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new directory for your}
\CommentTok{\# repository and navigate to that directory:}

\FunctionTok{mkdir}\NormalTok{ my{-}repo}
\BuiltInTok{cd}\NormalTok{ my{-}repo}

\CommentTok{\# Initialize the repository with git init:}

\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a .gitignore file for Python code.}
\CommentTok{\# You can use a template from GitHub:}

\ExtensionTok{curl}\NormalTok{ https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore }\AttributeTok{{-}o}\NormalTok{ .gitignore}

\CommentTok{\# Add your files to the repository using git add:}

\FunctionTok{git}\NormalTok{ add .}
\end{Highlighting}
\end{Shaded}

This adds all files in the current directory to the repository, except
for the files listed in the .gitignore file.

\end{tcolorbox}

\subsection{\texorpdfstring{Transferring Changes to Memory:
\texttt{git\ commit}}{Transferring Changes to Memory: git commit}}\label{transferring-changes-to-memory-git-commit}

The power of Git becomes evident when you start transferring changes to
the project history. This is done using the \texttt{git\ commit}
command. When you run \texttt{git\ commit}, you inform Git that the
changes in the staging area should be added to the history of the
project so that they can be referenced or retrieved later.

Additionally, you can add a commit message with the \texttt{-m} option
to explain what changes were made. So when you look back at the project
history, you can see that you added a new feature.

\texttt{git\ commit} creates a snapshot, an image of the current state
of your project at that specific time, and adds it to the branch you are
currently working on.

As you work on your project and transfer more snapshots, the branch
grows and forms a timeline of events. This means you can now look back
at every transfer in the branch and see what your code looked like at
that time.

You can compare any phase of your code with any other phase of your code
to find errors, restore deleted code, or do things that would otherwise
not be possible, such as resetting the project to a previous state or
creating a new timeline from any point.

So how often should you add these commits? My rule of thumb is not to
commit too often. It's better to have a Git repository with too many
commits than one with too few commits.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

After adding your files with \texttt{git\ add}, you can create a commit
to save your changes. Use the \texttt{git\ commit} command with the
\texttt{-m} option to specify your commit message:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"My first commit message"}
\end{Highlighting}
\end{Shaded}

This creates a new commit with the added files and the specified commit
message.

\end{tcolorbox}

\subsection{\texorpdfstring{Check the Status of Your Repository:
\texttt{git\ status}}{Check the Status of Your Repository: git status}}\label{check-the-status-of-your-repository-git-status}

If you're wondering what you've changed in your project since the last
commit snapshot, you can always check the Git status. Git will list
every modified file and the current status of each file.

This status can be either:

\begin{itemize}
\tightlist
\item
  Unchanged (\texttt{unmodified}), meaning nothing has changed since you
  last transferred it, or
\item
  It's been changed (\texttt{changed}) but not staged (\texttt{staged})
  to be transferred into the history, or
\item
  Something has been added to staging (\texttt{staged}) and is ready to
  be transferred into the history.
\end{itemize}

When you run \texttt{git\ status}, you get an overview of the current
state of your project.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

The \texttt{git\ status} command displays the status of your working
directory and the staging area. It shows you which files have been
modified, which files are staged for commit, and which files are not yet
being tracked:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ status}
\end{Highlighting}
\end{Shaded}

\texttt{git\ status} is a useful tool to keep track of your changes and
ensure that you have added all the desired files for commit.

\end{tcolorbox}

\subsection{\texorpdfstring{Review Your Repository's History:
\texttt{git\ log}}{Review Your Repository's History: git log}}\label{review-your-repositorys-history-git-log}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

You can view the history of your commits with the \texttt{git\ log}
command. This command displays a list of all the commits in the current
branch, along with information such as the author, date, and commit
message:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ log}
\end{Highlighting}
\end{Shaded}

There are many options to customize the output of \texttt{git\ log}. For
example, you can use the \texttt{-\/-pretty} option to change the format
of the output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ log }\AttributeTok{{-}{-}pretty}\OperatorTok{=}\NormalTok{oneline}
\end{Highlighting}
\end{Shaded}

This displays each commit in a single line.

\end{tcolorbox}

\section{Branches (Timelines)}\label{branches-timelines}

\subsection{\texorpdfstring{Creating an Alternative Timeline:
\texttt{git\ branch}}{Creating an Alternative Timeline: git branch}}\label{creating-an-alternative-timeline-git-branch}

In the course of developing a project, you often reach a point where you
want to add a new feature, but doing so might require changing the
existing code in a way that could be challenging to undo later.

Or maybe you just want to experiment and be able to discard your work if
the experiment fails. In such cases, Git allows you to create an
alternative timeline called a \texttt{branch} to work in.

This new \texttt{branch} has its own name and exists in parallel with
the \texttt{main\ branch} and all other branches in your project.

During development, you can switch between branches and work on
different versions of your code concurrently. This way, you can have a
stable codebase in the \texttt{main\ branch} while developing an
experimental feature in a separate \texttt{branch}. When you switch from
one \texttt{branch} to another, the code you're working on is
automatically reset to the latest commit of the branch you're currently
in.

If you're working in a team, different team members can work on their
own branches, creating an entire universe of alternative timelines for
your project. When features are completed, they can be seamlessly merged
back into the \texttt{main\ branch}.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

To create a new \texttt{branch}, you can use the \texttt{git\ branch}
command with the name of the new \texttt{branch} as an argument:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ branch my{-}tests}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{\texorpdfstring{The Pointer to the Current Branch:
\texttt{HEAD}}{The Pointer to the Current Branch: HEAD}}\label{the-pointer-to-the-current-branch-head}

How does Git know where you are on the timeline, and how can you keep
track of your position?

You're always working at the tip (\texttt{HEAD}) of the currently active
branch. The \texttt{HEAD} pointer points there quite literally. In a new
project archive with just a single \texttt{main\ branch} and only new
commits being added, \texttt{HEAD} always points to the latest commit in
the \texttt{main\ branch}. That's where you are.

However, if you're in a repository with multiple branches, meaning
multiple alternative timelines, \texttt{HEAD} will point to the latest
commit in the branch you're currently working on.

\subsection{\texorpdfstring{Switching to an Alternative Timeline:
\texttt{git\ switch}}{Switching to an Alternative Timeline: git switch}}\label{switching-to-an-alternative-timeline-git-switch}

As your project grows, and you have multiple branches, you need to be
able to switch between these branches. This is where the \texttt{switch}
command comes into play.

At any time, you can use the \texttt{git\ switch} command with the name
of the branch you want to switch to, and \texttt{HEAD} moves from your
current branch to the one you specified.

If you've made changes to your code before switching, Git will attempt
to carry those changes over to the branch you're switching to. However,
if these changes conflict with the target branch, the switch will be
canceled.

To resolve this issue without losing your changes, return to the
original branch, add and commit your recent changes, and then perform
the \texttt{switch}.

\subsection{\texorpdfstring{Switching to an Alternative Timeline and
Making Changes:
\texttt{git\ checkout}}{Switching to an Alternative Timeline and Making Changes: git checkout}}\label{switching-to-an-alternative-timeline-and-making-changes-git-checkout}

To switch between branches, you can also use the \texttt{git\ checkout}
command. It works similarly to \texttt{git\ switch} for this purpose:
you pass the name of the branch you want to switch to, and \texttt{HEAD}
moves to the beginning of that branch.

But \texttt{checkout} can do more than just switch to another timeline.
With \texttt{git\ checkout}, you can also move to any commit point in
any timeline. In other words, you can travel back in time and work on
code from the past.

To do this, use \texttt{git\ checkout} and provide the commit ID. This
is an automatically generated, random combination of letters and numbers
that identifies each commit. You can retrieve the commit ID using
\texttt{git\ log}. When you run \texttt{git\ log}, you get a list of all
the commits in your repository, starting with the most recent ones.

When you use \texttt{git\ checkout} with an older commit ID, you check
out a commit in the middle of a branch. This disrupts the timeline, as
you're actively attempting to change history. Git doesn't want you to do
that because, much like in a science fiction movie, altering the past
might also alter the future. In our case, it would break the version
control branch's coherence.

To prevent you from accidentally disrupting time and altering history,
checking out an earlier commit in any branch results in the warning
``Detached Head,'' which sounds rather ominous. The ``Detached Head''
warning is appropriate because it accurately describes what's happening.
Git literally detaches the head from the branch and sets it aside.

Now, you're working outside of time in a space unbound to any timeline,
which again sounds rather threatening but is perfectly fine in reality.

To continue working on this past code, all you need to do is reattach it
to the timeline. You can use \texttt{git\ branch} to create a new
branch, and the detached head will automatically attach to this new
branch.

Instead of breaking the history, you've now created a new alternative
timeline that starts in the past, allowing you to work safely. You can
continue working on the branch as usual.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Continuing the example from above:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

To switch to a new branch, you can use the \texttt{git\ checkout}
command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ checkout meine{-}tests}
\end{Highlighting}
\end{Shaded}

Now you're using the new branch and can make changes independently from
the original branch.

\end{tcolorbox}

\subsection{\texorpdfstring{The Difference Between \texttt{checkout} and
\texttt{switch}}{The Difference Between checkout and switch}}\label{the-difference-between-checkout-and-switch}

What is the difference between \texttt{git\ switch} and
\texttt{git\ checkout}? \texttt{git\ switch} and \texttt{git\ checkout}
are two different commands that both serve the purpose of switching
between branches. You can use both to switch between branches, but they
have an important distinction. \texttt{git\ switch} is a new command
introduced with Git 2.23. \texttt{git\ checkout} is an older command
that has existed since Git 1.6.0. So, \texttt{git\ switch} and
\texttt{git\ checkout} have different origins. \texttt{git\ switch} was
introduced to separate the purposes of \texttt{git\ checkout}.
\texttt{git\ checkout} has two different purposes: 1. It can be used to
switch between branches, and 2. It can be used to reset files to the
state of the last commit.

Here's an example: In my project, I made a change since the last commit,
but I haven't staged it yet. Then, I realized that I actually don't want
this change. I want to reset the file to the state before the last
commit. As long as I haven't committed my changes, I can do this with
\texttt{git\ checkout} by targeting the specific file. So, if that file
is named \texttt{main.js}, I can say: \texttt{git\ checkout\ main.js}.
And the file will be reset to the state of the last commit, which makes
sense. I'm checking out the file from the last commit.

But that's quite different from switching between the beginning of one
branch to another. \texttt{git\ switch} and \texttt{git\ restore} were
introduced to separate these two operations. \texttt{git\ switch} is for
switching between branches, and \texttt{git\ restore} is for resetting
the specified file to the state of the last commit. If you try to
restore a file with \texttt{git\ switch}, it simply won't work. It's not
intended for that. As I mentioned earlier, it's about separating
concerns.

:::\{.callout-note\} \#\#\#\# Difference Between \texttt{checkout} and
\texttt{switch} \texttt{git\ checkout} and \texttt{git\ switch} are both
commands for switching between branches in a Git repository. The main
difference between the two commands is that \texttt{git\ switch} is a
newer command specifically designed for branch switching, while
\texttt{git\ checkout} is an older command that can be used for various
tasks, including branch switching.

Here's an example demonstrating how to initialize a repository and
switch between branches:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new directory for your repository}
\CommentTok{\# and navigate to that directory:}
\FunctionTok{mkdir}\NormalTok{ my{-}repo}
\BuiltInTok{cd}\NormalTok{ my{-}repo}

\CommentTok{\# Initialize the repository with git init:}
\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a new branch with git branch:}
\FunctionTok{git}\NormalTok{ branch my{-}new{-}branch}

\CommentTok{\# Switch to the new branch using git switch:}
\FunctionTok{git}\NormalTok{ switch my{-}new{-}branch}

\CommentTok{\# Alternatively, you can also use git checkout}
\CommentTok{\# to switch to the new branch:}

\FunctionTok{git}\NormalTok{ checkout my{-}new{-}branch}
\end{Highlighting}
\end{Shaded}

Both commands lead to the same result: You are now on the new branch.

\section{Merging Branches and Resolving
Conflicts}\label{merging-branches-and-resolving-conflicts}

\subsection{\texorpdfstring{git \texttt{merge}: Merging Two
Timelines}{git merge: Merging Two Timelines}}\label{git-merge-merging-two-timelines}

Git allows you to split your development work into as many branches or
alternative timelines as you like, enabling you to work on many
different versions of your code simultaneously without losing or
overwriting any of your work.

This is all well and good, but at some point, you need to bring those
various versions of your code back together into one branch. That's
where \texttt{git\ merge} comes in.

Consider an example where you have two branches, a \texttt{main\ branch}
and an experimental branch called \texttt{experimental-branch}. In the
experimental branch, there is a new feature. To merge these two
branches, you set \texttt{HEAD} to the branch where you want to
incorporate the code and execute \texttt{git\ merge} followed by the
name of the branch you want to merge. \texttt{HEAD} is a special pointer
that points to the current branch. When you run \texttt{git\ merge}, it
combines the code from the branch associated with \texttt{HEAD} with the
code from the branch specified by the branch name you provide.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize the repository}
\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a new branch called "experimental{-}branch"}
\FunctionTok{git}\NormalTok{ branch experimental{-}branch}

\CommentTok{\# Switch to the "experimental{-}branch"}
\FunctionTok{git}\NormalTok{ checkout experimental{-}branch}

\CommentTok{\# Add the new feature here and}
\CommentTok{\# make a commit}
\CommentTok{\# ...}

\CommentTok{\# Switch back to the "main" branch}
\FunctionTok{git}\NormalTok{ checkout main}

\CommentTok{\# Perform the merge}
\FunctionTok{git}\NormalTok{ merge experimental{-}branch}
\end{Highlighting}
\end{Shaded}

During the merge, matching pieces of code in the branches overlap, and
any new code from the branch being merged is added to the project. So
now, the main branch also contains the code from the experimental
branch, and the events of the two separate timelines have been merged
into a single one. What's interesting is that even though the
experimental branch was merged with the main branch, the last commit of
the experimental branch remains intact, allowing you to continue working
on the experimental branch separately if you wish.

\subsection{Resolving Conflicts When
Merging}\label{resolving-conflicts-when-merging}

Merging branches where there are no code changes at the same place in
both branches is a straightforward process. It's also a rare process. In
most cases, there will be some form of conflict between the branches --
the same code or the same code area has been modified differently in the
different branches. Merging two branches with such conflicts will not
work, at least not automatically.

In this case, Git doesn't know how to merge this code. So, when such a
situation occurs, it's marked as a conflict, and the merging process is
halted. This might sound more dramatic than it is. When you get a
conflict warning, Git is saying there are two different versions here,
and Git needs to know which one you want to keep. To help you figure out
the conflict, Git combines all the code into a single file and
automatically marks the conflicting code as the current change, which is
the original code from the branch you're working on, or as the incoming
change, which is the code from the file you're trying to merge.

To resolve this conflict, you'll edit the file to literally resolve the
code conflict. This might mean accepting either the current or incoming
change and discarding the other. It could mean combining both changes or
something else entirely. It's up to you. So, you edit the code to
resolve the conflict. Once you've resolved the conflict by editing the
code, you add the new conflict-free version to the staging area with
\texttt{git\ add} and then commit the merged code with
\texttt{git\ commit}. That's how the conflict is resolved.

A merge conflict occurs when Git struggles to automatically merge
changes from two different branches. This usually happens when changes
were made to the same line in the same file in both branches. To resolve
a merge conflict, you must manually edit the affected files and choose
the desired changes. Git marks the conflict areas in the file with
special markings like
\texttt{\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}},
\texttt{=======}, and
\texttt{\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}}.
You can search for these markings and manually select the desired
changes. After resolving the conflicts, you can add the changes with
\texttt{git\ add} and create a new commit with \texttt{git\ commit} to
complete the merge.

Here's an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform the merge (this will cause a conflict)}
\FunctionTok{git}\NormalTok{ merge experimenteller{-}branch}

\CommentTok{\# Open the affected file in an editor and manually resolve the conflicts}
\CommentTok{\# ...}

\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add }\OperatorTok{\textless{}}\NormalTok{filename}\OperatorTok{\textgreater{}}

\CommentTok{\# Create a new commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Resolved conflicts"}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{git \texttt{revert}: Undoing
Something}{git revert: Undoing Something}}\label{git-revert-undoing-something}

One of the most powerful features of any software tool is the ``Undo''
button. Make a mistake, press ``Undo,'' and it's as if it never
happened. However, that's not quite as simple when an all-powerful,
passive observer is watching and recording your project's history. How
do you undo something that you've added to the history without rewriting
the history?

The answer is that you can overwrite the history with the
\texttt{git\ reset} command, but that's quite risky and not a good
practice.

A better solution is to work with the historical timeline and simply
place an older version of your code at the top of the branch. This is
done with \texttt{git\ revert}. To make this work, you need to know the
commit ID of the commit you want to go back to.

The commit ID is a machine-generated set of random numbers and letters,
also known as a hash. To get a list of all the commits in the
repository, including the commit ID and commit message, you can run
\texttt{git\ log}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Show the list of all operations in the repository}
\NormalTok{git log}
\end{Highlighting}
\end{Shaded}

By the way, it's a good idea to leave clear and informative commit
messages for this reason. This way, you know what happened in your
previous commits. Once you've found the commit you want to revert to,
call that commit ID with \texttt{git\ revert}, and then the ID. This
will create a new commit at the top of the branch with the code from the
reference commit. To transfer the code to the branch, add a commit
message and save it. Now, the last commit in your branch matches the
commit you're reverting to, and your project's history remains intact.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ revert}}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\# Initialize a new repository}
\NormalTok{git init}

\NormalTok{\# Create a new file}
\NormalTok{echo "Hello, World" \textgreater{} file.txt}

\NormalTok{\# Add the file to the repository}
\NormalTok{git add file.txt}

\NormalTok{\# Create a new commit}
\NormalTok{git commit {-}m "First commit"}

\NormalTok{\# Modify the file}
\NormalTok{echo "Goodbye, World" \textgreater{} file.txt}

\NormalTok{\# Add the modified file}
\NormalTok{git add file.txt}

\NormalTok{\# Create a new commit}
\NormalTok{git commit {-}m "Second commit"}

\NormalTok{\# Use git log to find the commit ID of the second commit}
\NormalTok{git log}

\NormalTok{\# Use git revert to undo the changes from the second commit}
\NormalTok{git revert \textless{}commit{-}id\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

To download the \texttt{students} branch from the repository
\texttt{git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git}
to your local machine, add a file, and upload the changes, you can
follow these steps:

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ clone}, \texttt{git\ checkout},
\texttt{git\ add}, \texttt{git\ commit}, \texttt{git\ push}}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clone the repository to your local machine:}
\FunctionTok{git}\NormalTok{ clone git@git{-}ce.rwth{-}aachen.de:spotseven{-}lab/numerische{-}mathematik{-}sommersemester2023.git}

\CommentTok{\# Change to the cloned repository:}
\BuiltInTok{cd}\NormalTok{ numerische{-}mathematik{-}sommersemester2023}

\CommentTok{\# Switch to the students branch:}
\FunctionTok{git}\NormalTok{ checkout students}

\CommentTok{\# Create the Test folder if it doesn\textquotesingle{}t exist:}
\FunctionTok{mkdir}\NormalTok{ Test}

\CommentTok{\# Create the Testdatei.txt file in the Test folder:}
\FunctionTok{touch}\NormalTok{ Test/Testdatei.txt}

\CommentTok{\# Add the file with git add:}
\FunctionTok{git}\NormalTok{ add Test/Testdatei.txt}

\CommentTok{\# Commit the changes with git commit:}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Added Testdatei.txt"}

\CommentTok{\# Push the changes with git push:}
\FunctionTok{git}\NormalTok{ push origin students}
\end{Highlighting}
\end{Shaded}

This will upload the changes to the server and update the students
branch in the repository.

\end{tcolorbox}

\section{Downloading from GitLab}\label{downloading-from-gitlab}

To download changes from a GitLab repository to your local machine, you
can use the \texttt{git\ pull} command. This command downloads the
latest changes from the specified remote repository and merges them with
your local repository.

Here is an example:

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ pull}}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Navigate to the local repository}
\CommentTok{\# linked to the GitHub repository:}
\BuiltInTok{cd}\NormalTok{ my{-}local{-}repository}

\CommentTok{\# Make sure you are in the correct branch:}
\FunctionTok{git}\NormalTok{ checkout main}

\CommentTok{\# Download the latest changes from GitHub:}
\FunctionTok{git}\NormalTok{ pull origin main}
\end{Highlighting}
\end{Shaded}

This downloads the latest changes from the main branch of the remote
repository named ``origin'' and merges them with your local repository.

\end{tcolorbox}

If there are conflicts between the downloaded changes and your local
changes, you will need to resolve them manually before proceeding.

\section{Advanced}\label{advanced}

\subsection{\texorpdfstring{git \texttt{rebase}: Moving the Base of a
Branch}{git rebase: Moving the Base of a Branch}}\label{git-rebase-moving-the-base-of-a-branch}

In some cases, you may need to ``rewrite history.'' A common scenario is
that you've been working on a new feature in a feature branch, and you
realize that the work should have actually happened in the
\texttt{main\ branch}.

To resolve this issue and make it appear as if the work occurred in the
\texttt{main\ branch}, you can reset the experimental branch. ``Rebase''
literally means detaching the base of the experimental branch and moving
it to the beginning of another branch, giving the branch a new base,
thus ``rebasing.''

This operation is performed from the branch you want to ``rebase.'' You
use \texttt{git\ rebase} and specify the branch you want to use as the
new base. If there are no conflicts between the experimental branch and
the branch you want to rebase onto, this process happens automatically.

If there are conflicts, Git will guide you through the conflict
resolution process for each commit from the rebase branch.

This may sound like a lot, but there's a good reason for it. You are
literally rewriting history by transferring commits from one branch to
another. To maintain the coherence of the new version history, there
should be no conflicts within the commits. So, you need to resolve them
one by one until the history is clean. It goes without saying that this
can be a fairly labor-intensive process. Therefore, you should not use
\texttt{git\ rebase} frequently.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{An example with \texttt{git\ rebase}}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-note-color-frame, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\texttt{git\ rebase} is a command used to change the base of a branch.
This means that commits from the branch are applied to a new base, which
is usually another branch. It can be used to clean up the repository
history and avoid merge conflicts.

Here is an example showing how to use \texttt{git\ rebase}:

\begin{itemize}
\item
  In this example, we initialize a new Git repository and create a new
  file. We add the file to the repository and make an initial commit.
  Then, we create a new branch called ``feature'' and switch to that
  branch. We make changes to the file in the feature branch and create a
  new commit.
\item
  Then, we switch back to the main branch and make changes to the file
  again. We add the modified file and make another commit.
\item
  To rebase the feature branch onto the main branch, we first switch to
  the feature branch and then use the \texttt{git\ rebase} command with
  the name of the main branch as an argument. This applies the commits
  from the feature branch to the main branch and changes the base of the
  feature branch.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize a new repository}
\FunctionTok{git}\NormalTok{ init}
\CommentTok{\# Create a new file}
\BuiltInTok{echo} \StringTok{"Hello World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the file to the repository}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create an initial commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Initial commit"}
\CommentTok{\# Create a new branch called "feature"}
\FunctionTok{git}\NormalTok{ branch feature}
\CommentTok{\# Switch to the "feature" branch}
\FunctionTok{git}\NormalTok{ checkout feature}
\CommentTok{\# Make changes to the file in the "feature" branch}
\BuiltInTok{echo} \StringTok{"Hello Feature World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create a new commit in the "feature" branch}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Feature commit"}
\CommentTok{\# Switch back to the "main" branch}
\FunctionTok{git}\NormalTok{ checkout main}
\CommentTok{\# Make changes to the file in the "main" branch}
\BuiltInTok{echo} \StringTok{"Hello Main World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create a new commit in the "main" branch}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Main commit"}
\CommentTok{\# Use git rebase to rebase the "feature" branch}
\CommentTok{\# onto the "main" branch}
\FunctionTok{git}\NormalTok{ checkout feature}
\FunctionTok{git}\NormalTok{ rebase main}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Exercises}\label{exercises-9}

In order to be able to carry out this exercise, we provide you with a
functional working environment. This can be accessed
\href{https://hub.0x3e8.de/}{here}. You can log in using your GMID. If
you do not have one, you can generate one
\href{https://id.gm.fh-koeln.de/registrierung.php}{here}. Once you have
successfully logged in to the server, you must open a terminal instance.
You are now in a position to carry out the exercise.

Alternatively, you can also carry out the exercise locally on your
computer, but then you will need to install git.

\subsection{Create project folder}\label{create-project-folder}

First create the \texttt{test-repo} folder via the command line and then
navigate to this folder using the corresponding command.

\section{Initialize repo}\label{initialize-repo}

Now initialize the repository so that the future project, which will be
saved in the \texttt{test-repo} folder, and all associated files are
versioned.

\subsection{Do not upload / ignore certain file
types}\label{do-not-upload-ignore-certain-file-types}

In order to carry out this exercise, you must first download a file
which you then have git ignore. To do this, download the current
examination regulations for the Bachelor's degree program in Electrical
Engineering using the following command
\texttt{curl\ -o\ pruefungsordnung.pdf\ https://www.th-koeln.de/mam/downloads/deutsch/studium/studiengaenge/f07/ordnungen\_plaene/f07\_bpo\_ba\_ekb\_2021\_01\_04.pdf}.

The PDF file has been stored in the root directory of your repo and you
must now exclude it from being uploaded so that no changes to this file
are tracked. Please note that not only this one PDF file should be
ignored, but all PDF files in the repo.

\subsection{Create file and stage it}\label{create-file-and-stage-it}

In order to be able to commit a change later and thus make it traceable,
it must first be staged. However, as we only have a PDF file so far,
which is to be ignored by git, we cannot stage anything. Therefore, in
this task, a file \texttt{test.txt} with some string as content is to be
created and then staged.

\subsection{Create another file and check
status}\label{create-another-file-and-check-status}

To understand the status function, you should create the file
\texttt{test2.txt} and then call the status function of git.

\subsection{Commit changes}\label{commit-changes}

After the changes to the \texttt{test.txt} file have been staged and
these are now to be transferred to the project process, they must be
committed. Therefore, in this step you should perform a corresponding
commit in the current branch with the message \texttt{test-commit}.
Finally, you should also display the history of the commits.

\subsection{Create a new branch and switch to
it}\label{create-a-new-branch-and-switch-to-it}

In this task, you are to create a new branch with the name
\texttt{change-text} in which you will later make changes. You should
then switch to this branch.

\subsection{Commit changes in the new
branch}\label{commit-changes-in-the-new-branch}

To be able to merge the new branch into the main branch later, you must
first make changes to the \texttt{test.txt} file. To do this, open the
file and simply change the character string in this file before saving
the changes and closing the file. Before you now commit the file, you
should reset the file to the status of the last commit for practice
purposes and thus undo the change. After you have done this, open the
file \texttt{test.txt} again and change the character string again
before saving and closing the file. This time you should commit the file
\texttt{test.txt} and then commit it with the message
\texttt{test-commit2}.

\subsection{Merge branch into main}\label{merge-branch-into-main}

After you have committed the change to the \texttt{test.txt} file, you
should merge the \texttt{change-text} branch including the change into
the main branch so that it is also available there.

\subsection{Resolve merge conflict}\label{resolve-merge-conflict}

To simulate a merge conflict, you must first change the content of the
\texttt{test.txt} file before you commit the change. Then switch to the
branch \texttt{change-text} and change the file \texttt{test.txt} there
as well before you commit the change. Now you should try to merge the
branch \texttt{change-text} into the main branch and solve the problems
that occur in order to be able to perform the merge successfully.

\chapter{Python Introduction}\label{python-introduction}

\section{Recommendations}\label{recommendations}

\href{https://wiki.python.org/moin/BeginnersGuide}{Beginner's Guide to
Python}

\chapter{Documentation of the Sequential Parameter
Optimization}\label{documentation-of-the-sequential-parameter-optimization}

This document describes the \texttt{Spot} features. The official
\texttt{spotPython} documentation can be found here:
\url{https://sequential-parameter-optimization.github.io/spotPython/}.

\section{An Initial Example}\label{an-initial-example}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

The \texttt{spotPython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[
f(x) = x^2.
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics{a_04_spot_doc_files/figure-pdf/cell-4-output-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init, optimizer\_control\_init}
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                        lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                        upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{100}\NormalTok{]),}
\NormalTok{                        fun\_evals }\OperatorTok{=} \DecValTok{7}\NormalTok{,}
\NormalTok{                        fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                        noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                        tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                        var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{],}
\NormalTok{                        infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                        n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                        log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(}
\NormalTok{                        init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                        repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{),}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(}
\NormalTok{                        noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                        min\_theta}\OperatorTok{={-}}\DecValTok{4}\NormalTok{,}
\NormalTok{                        max\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{                        n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                        model\_optimizer}\OperatorTok{=}\NormalTok{differential\_evolution,}
\NormalTok{                        model\_fun\_evals}\OperatorTok{=}\DecValTok{10000}\NormalTok{))}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.0106521524877827 [#########-] 85.71% 
spotPython tuning: 0.01033163973935242 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2c35e4950>
\end{verbatim}

\section{Organization}\label{organization}

\texttt{Spot} organizes the surrogate based optimization process in four
steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Selection of the objective function: \texttt{fun}.
\item
  Selection of the initial design: \texttt{design}.
\item
  Selection of the optimization algorithm: \texttt{optimizer}.
\item
  Selection of the surrogate model: \texttt{surrogate}.
\end{enumerate}

For each of these steps, the user can specify an object:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_sphere}
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\NormalTok{design }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ differential\_evolution}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\NormalTok{surrogate }\OperatorTok{=}\NormalTok{ Kriging()}
\end{Highlighting}
\end{Shaded}

For each of these steps, the user can specify a dictionary of control
parameters.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_control}
\item
  \texttt{design\_control}
\item
  \texttt{optimizer\_control}
\item
  \texttt{surrogate\_control}
\end{enumerate}

Each of these dictionaries has an initialzaion method, e.g.,
\texttt{fun\_control\_init()}. The initialization methods set the
default values for the control parameters.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important:}, opacitybacktitle=0.6, bottomrule=.15mm, opacityback=0, left=2mm, colback=white, leftrule=.75mm, colframe=quarto-callout-important-color-frame, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, breakable, arc=.35mm]

\begin{itemize}
\tightlist
\item
  The specification of an lower bound in \texttt{fun\_control} is
  mandatory.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, optimizer\_control\_init, surrogate\_control\_init}
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(lower}\OperatorTok{=}\NormalTok{np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                            upper}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]))}
\NormalTok{design\_control}\OperatorTok{=}\NormalTok{design\_control\_init()}
\NormalTok{optimizer\_control}\OperatorTok{=}\NormalTok{optimizer\_control\_init()}
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init()}
\end{Highlighting}
\end{Shaded}

\section{The Spot Object}\label{the-spot-object}

Based on the definition of the \texttt{fun}, \texttt{design},
\texttt{optimizer}, and \texttt{surrogate} objects, and their
corresponding control parameter dictionaries, \texttt{fun\_control},
\texttt{design\_control}, \texttt{optimizer\_control}, and
\texttt{surrogate\_control}, the \texttt{spot} object can be build as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                       fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                       design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                       optimizer\_control}\OperatorTok{=}\NormalTok{optimizer\_control,}
\NormalTok{                       surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\end{Highlighting}
\end{Shaded}

\section{Run}\label{run}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 1.801603872454505e-05 [#######---] 73.33% 
spotPython tuning: 1.801603872454505e-05 [########--] 80.00% 
spotPython tuning: 1.801603872454505e-05 [#########-] 86.67% 
spotPython tuning: 1.801603872454505e-05 [#########-] 93.33% 
spotPython tuning: 1.801603872454505e-05 [##########] 100.00% Done...
\end{verbatim}

\begin{verbatim}
<spotPython.spot.spot.Spot at 0x2c6c8d350>
\end{verbatim}

\section{Print the Results}\label{print-the-results-5}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.801603872454505e-05
x0: 0.0019077911677074135
x1: 0.003791618596979743
\end{verbatim}

\begin{verbatim}
[['x0', 0.0019077911677074135], ['x1', 0.003791618596979743]]
\end{verbatim}

\section{Show the Progress}\label{show-the-progress-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\includegraphics{a_04_spot_doc_files/figure-pdf/cell-11-output-1.pdf}

\section{Visualize the Surrogate}\label{visualize-the-surrogate}

\begin{itemize}
\tightlist
\item
  The plot method of the \texttt{kriging} surrogate is used.
\item
  Note: the plot uses the interval defined by the ranges of the natural
  variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\includegraphics{a_04_spot_doc_files/figure-pdf/cell-12-output-1.pdf}

\section{Run With a Specific Start
Design}\label{run-with-a-specific-start-design}

To pass a specific start design, use the \texttt{X\_start} argument of
the \texttt{run} method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_x0 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                        lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                        upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{100}\NormalTok{]),}
\NormalTok{                        fun\_evals }\OperatorTok{=} \DecValTok{7}\NormalTok{,}
\NormalTok{                        fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                        noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                        tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                        var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{],}
\NormalTok{                        infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                        n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                        log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{),}
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(}
\NormalTok{                        init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                        repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{),}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(}
\NormalTok{                        noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                        min\_theta}\OperatorTok{={-}}\DecValTok{4}\NormalTok{,}
\NormalTok{                        max\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{                        n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                        model\_optimizer}\OperatorTok{=}\NormalTok{differential\_evolution,}
\NormalTok{                        model\_fun\_evals}\OperatorTok{=}\DecValTok{10000}\NormalTok{))}
\NormalTok{spot\_x0.run(X\_start}\OperatorTok{=}\NormalTok{np.array([}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{]))}
\NormalTok{spot\_x0.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotPython tuning: 2.0106521524877827 [#########-] 85.71% 
spotPython tuning: 0.01033163973935242 [##########] 100.00% Done...
\end{verbatim}

\includegraphics{a_04_spot_doc_files/figure-pdf/cell-13-output-2.pdf}

\section{Init: Build Initial Design}\label{init-build-initial-design-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotPython.design.spacefilling }\ImportTok{import}\NormalTok{ spacefilling}
\ImportTok{from}\NormalTok{ spotPython.build.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_branin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\DecValTok{0}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 8.97647221 13.41926847]
 [ 0.66946019  1.22344228]
 [ 5.23614115 13.78185824]
 [ 5.6149825  11.5851384 ]
 [-1.72963184  1.66516096]
 [-4.26945568  7.1325531 ]
 [ 1.26363761 10.17935555]
 [ 2.88779942  8.05508969]
 [-3.39111089  4.15213772]
 [ 7.30131231  5.22275244]]
[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975
  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]
\end{verbatim}

\section{Replicability}\label{replicability}

Seed

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{345}\NormalTok{)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ spacefilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X0, X1, X2, X3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]),
 array([[0.78373509, 0.86811887],
        [0.06692621, 0.6058029 ],
        [0.41374778, 0.00525456]]),
 array([[0.121357  , 0.69043832],
        [0.41906219, 0.32838498],
        [0.86742658, 0.52910374]]),
 array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]))
\end{verbatim}

\section{Surrogates}\label{surrogates-1}

\subsection{A Simple Predictor}\label{a-simple-predictor-1}

The code below shows how to use a simple model for prediction. Assume
that only two (very costly) measurements are available:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  f(0) = 0.5
\item
  f(2) = 2.5
\end{enumerate}

We are interested in the value at \(x_0 = 1\), i.e., \(f(x_0 = 1)\), but
cannot run an additional, third experiment.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{], [}\DecValTok{2}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{])}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ S\_lm.fit(X, y)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{]])}
\NormalTok{y0 }\OperatorTok{=}\NormalTok{ S\_lm.predict(X0)}
\BuiltInTok{print}\NormalTok{(y0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.5]
\end{verbatim}

Central Idea: Evaluation of the surrogate model \texttt{S\_lm} is much
cheaper (or / and much faster) than running the real-world experiment
\(f\).

\section{Demo/Test: Objective Function
Fails}\label{demotest-objective-function-fails}

SPOT expects \texttt{np.nan} values from failed objective function
values. These are handled. Note: SPOT's counter considers only
successful executions of the objective function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotPython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ analytical}
\ImportTok{from}\NormalTok{ spotPython.spot }\ImportTok{import}\NormalTok{ spot}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\CommentTok{\# number of initial points:}
\NormalTok{ni }\OperatorTok{=} \DecValTok{20}
\CommentTok{\# number of points}
\NormalTok{n }\OperatorTok{=} \DecValTok{30}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ analytical().fun\_random\_error}
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper}\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=}\NormalTok{ n,}
\NormalTok{    show\_progress}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(init\_size}\OperatorTok{=}\NormalTok{ni)}

\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ spot.Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\CommentTok{\# To check whether the run was successfully completed,}
\CommentTok{\# we compare the number of evaluated points to the specified}
\CommentTok{\# number of points.}
\ControlFlowTok{assert}\NormalTok{ spot\_1.y.shape[}\DecValTok{0}\NormalTok{] }\OperatorTok{==}\NormalTok{ n}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[        nan         nan -0.02203599 -0.21843718  0.78240941         nan
 -0.3923345   0.67234256  0.31802454 -0.68898927 -0.75129705  0.97550354
  0.41757584         nan  0.82585329         nan -0.49274073         nan
 -0.17991251  0.1481835 ]
[-1.]
[nan]
[-0.14624037]
[0.166475]
[nan]
[-0.3352401]
[-0.47259301]
[0.95541987]
[0.17335968]
[-0.58552368]
[-0.20126111]
[-0.60100809]
[-0.97897336]
[-0.2748985]
[0.8359486]
[0.99035591]
[0.01641232]
[0.5629346]
\end{verbatim}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bart21i}
Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf
Mersmann, eds. 2022. \emph{{Hyperparameter Tuning for Machine and Deep
Learning with R - A Practical Guide}}. Springer.

\bibitem[\citeproctext]{ref-bart23e}
Bartz-Beielstein, Thomas. 2023. {``{PyTorch} Hyperparameter Tuning with
{SPOT}: Comparison with {Ray Tuner} and Default Hyperparameters on
{CIFAR10}.''}
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb}.

\bibitem[\citeproctext]{ref-Forr08a}
Forrester, Alexander, András Sóbester, and Andy Keane. 2008.
\emph{{Engineering Design via Surrogate Modelling}}. Wiley.

\bibitem[\citeproctext]{ref-Sant03a}
Santner, T J, B J Williams, and W I Notz. 2003. \emph{{The Design and
Analysis of Computer Experiments}}. Berlin, Heidelberg, New York:
Springer.

\end{CSLReferences}



\end{document}
