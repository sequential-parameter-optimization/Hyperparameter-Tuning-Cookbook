% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  10pt,
  a4paperpaper,
]{scrbook}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{mdframed}
\usepackage{pseudo}
\usepackage{makeidx}
\usepackage[acronym,toc]{glossaries}
\newglossary[slg]{symbolslist}{syi}{syg}{Symbolslist}
\makeglossaries
\newglossaryentry{s}{name=\ensuremath{s}, description={sample standard deviation}, type=symbolslist}
\newacronym{adwin}{ADWIN}{Adaptive Windowing}
\newacronym{alma}{ALMA}{Approximative Large-Margin-Algorithmus}
\newacronym{APCS}{APCS}{Approximate Probability of Correct Selection}
\newacronym{bip}{BIP}{Bruttoinlandsprodukt}
\newacronym{bml}{BML}{Batch Machine Learning}
\newacronym{bo}{BO}{Bayesian Optimization}
\newacronym{cart}{CART}{Classification And Regression Tree}
\newacronym{cv}{CV}{Cross Validation}
\newacronym{cvfdt}{CVFDT}{Concept-adapting Very Fast Decision Tree}
\newacronym{dace}{DACE}{Design and Analysis of Computer Experiments}
\newacronym{ddm}{DDM}{Drift Detection Method}
\newacronym{dl}{DL}{Deep Learning}
\newacronym{doe}{DOE}{Design of Experiments}
\newacronym{efdt}{EFDT}{Extremely Fast Decision Tree}
\newacronym{gbrt}{gbrt}{Gradient Boosting Regression Tree}
\newacronym{gcd}{GCD}{Greatest Common Divisor}
\newacronym{gra}{GRA}{Global Recurring Abrupt}
\newacronym{hat}{HAT}{Hoeffding Adaptive Tree}
\newacronym{hatc}{HATC}{Hoeffding Adaptive Tree Classifier}
\newacronym{hatr}{HATR}{Hoeffding Adaptive Tree Regressor}
\newacronym{hpt}{HPT}{Hyperparameter Tuning}
\newacronym{ht}{HT}{Hoeffding Tree}
\newacronym{htc}{HTC}{Hoeffding Tree Classifier}
\newacronym{htr}{HTR}{Hoeffding Tree Regressor}
\newacronym{ki}{KI}{KÃ¼nstliche Intelligenz}
\newacronym{kpi}{KPI}{Key Performance Indicator}
\newacronym{mae}{MAE}{Mean Absolute Error}
\newacronym{ml}{ML}{Machine Learning}
\newacronym{moa}{MOA}{Massive Online Analysis}
\newacronym{mse}{MSE}{Mean Squared Error}
\newacronym{oml}{OML}{Online Machine Learning}
\newacronym{pa}{PA}{Passive-Aggressive}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{rf}{RF}{Random Forest}
\newacronym{river}{river}{River: Online machine learning in Python}
\newacronym{rmoa}{RMOA}{Massive Online Analysis in R}
\newacronym{rocauc}{ROC AUC}{AUC (Area Under The Curve) ROC (Receiver Operating Characteristics)}
\newacronym{sea}{SEA}{SEA synthetic dataset}
\newacronym{sklearn}{sklearn}{scikit-learn: Machine Learning in Python}
\newacronym{smbo}{SMBO}{Surrogate Model Based Optimization}
\newacronym{smote}{SMOTE}{Synthetic Minority Oversampling Technique}
\newacronym{spo}{SPO}{Sequential Parameter Optimization}
\newacronym{spot}{SPOT}{Sequential Parameter Optimization Toolbox}
\newacronym{spotpython}{spotPython}{Sequential Parameter Optimization Toolbox for Python}
\newacronym{spotriver}{spotRiver}{Sequential Parameter Optimization Toolbox for River}
\newacronym{sgd}{SGD}{Stochastic Gradient Descent}
\newacronym{svm}{SVM}{Support Vector Machine}
\newacronym{vfdt}{VFDT}{Very Fast Decision Tree}
\makeindex
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Hyperparameter Tuning Cookbook},
  pdfauthor={Thomas Bartz-Beielstein},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Hyperparameter Tuning Cookbook}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A guide for scikit-learn, PyTorch, river, and spotpython}
\author{Thomas Bartz-Beielstein}
\date{May 6, 2025}

\begin{document}
\frontmatter
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\begin{quote}
This document provides a comprehensive guide to hyperparameter tuning
using spotpython for scikit-learn, scipy-optimize, River, and PyTorch.
The first part introduces fundamental ideas from optimization. The
second part discusses numerical issues and introduces spotpython's
surrogate model-based optimization process. The thirs part focuses on
hyperparameter tuning. Several case studies are presented, including
hyperparameter tuning for sklearn models such as Support Vector
Classification, Random Forests, Gradient Boosting (XGB), and K-nearest
neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from
river. The integration of spotpython into the PyTorch and PyTorch
Lightning training workflow is also discussed. With a hands-on approach
and step-by-step explanations, this cookbook serves as a practical
starting point for anyone interested in hyperparameter tuning with
Python. Highlights include the interplay between Tensorboard, PyTorch
Lightning, spotpython, spotriver, and River. This publication is under
development, with updates available on the corresponding webpage.
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important: This book is still under development.}]

The most recent version of this book is available at
\url{https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/}

\end{tcolorbox}

\section*{Book Structure}\label{book-structure}
\addcontentsline{toc}{section}{Book Structure}

\markright{Book Structure}

This document is structured in three parts. The first part presents an
introduction to optimization. The second part describes numerical
methods, and the third part presents hyperparameter tuning.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Hyperparameter Tuning Reference}]

\begin{itemize}
\tightlist
\item
  The open access book Bartz et al. (2022) provides a comprehensive
  overview of hyperparameter tuning. It can be downloaded from
  \url{https://link.springer.com/book/10.1007/978-981-19-5170-1}.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

The \texttt{.ipynb} notebook (Bartz-Beielstein 2023a) is updated
regularly and reflects updates and changes in the \texttt{spotpython}
package. It can be downloaded from
\url{https://github.com/sequential-parameter-optimization/spotpython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb}.

\end{tcolorbox}

\section*{Software Used in this Book}\label{software-used-in-this-book}
\addcontentsline{toc}{section}{Software Used in this Book}

\markright{Software Used in this Book}

\href{https://scikit-learn.org}{scikit-learn} is a Python module for
machine learning built on top of SciPy and is distributed under the
3-Clause BSD license. The project was started in 2007 by David
Cournapeau as a Google Summer of Code project, and since then many
volunteers have contributed.

\href{https://pytorch.org}{PyTorch} is an optimized tensor library for
deep learning using GPUs and CPUs.
\href{https://lightning.ai/docs/pytorch/latest/}{Lightning} is a
lightweight PyTorch wrapper for high-performance AI research. It allows
you to decouple the research from the engineering.

\href{https://riverml.xyz}{River} is a Python library for online machine
learning. It is designed to be used in real-world environments, where
not all data is available at once, but streaming in.

\href{https://github.com/sequential-parameter-optimization/spotpython}{spotpython}
(``Sequential Parameter Optimization Toolbox in Python'') is the Python
version of the well-known hyperparameter tuner SPOT, which has been
developed in the R programming environment for statistical analysis for
over a decade. The related open-access book is available here:
\href{https://link.springer.com/book/10.1007/978-981-19-5170-1}{Hyperparameter
Tuning for Machine and Deep Learning with R---A Practical Guide}.

\href{https://github.com/sequential-parameter-optimization/spotriver}{spotriver}
provides an interface between
\href{https://github.com/sequential-parameter-optimization/spotpython}{spotpython}
and \href{https://riverml.xyz}{River}.

\section*{Citation}\label{citation}
\addcontentsline{toc}{section}{Citation}

\markright{Citation}

If this document has been useful to you and you wish to cite it in a
scientific publication, please refer to the following paper, which can
be found on arXiv: \url{https://arxiv.org/abs/2307.10262}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{@ARTICLE\{bart23iArXiv,}
\NormalTok{      author = \{\{Bartz{-}Beielstein\}, Thomas\},}
\NormalTok{      title = "\{Hyperparameter Tuning Cookbook:}
\NormalTok{          A guide for scikit{-}learn, PyTorch, river, and spotpython\}",}
\NormalTok{     journal = \{arXiv e{-}prints\},}
\NormalTok{    keywords = \{Computer Science {-} Machine Learning,}
\NormalTok{      Computer Science {-} Artificial Intelligence, 90C26, I.2.6, G.1.6\},}
\NormalTok{         year = 2023,}
\NormalTok{        month = jul,}
\NormalTok{          eid = \{arXiv:2307.10262\},}
\NormalTok{        pages = \{arXiv:2307.10262\},}
\NormalTok{          doi = \{10.48550/arXiv.2307.10262\},}
\NormalTok{archivePrefix = \{arXiv\},}
\NormalTok{       eprint = \{2307.10262\},}
\NormalTok{ primaryClass = \{cs.LG\},}
\NormalTok{       adsurl = \{https://ui.adsabs.harvard.edu/abs/2023arXiv230710262B\},}
\NormalTok{      adsnote = \{Provided by the SAO/NASA Astrophysics Data System\}}
\NormalTok{\}}

\end{Highlighting}
\end{Shaded}

\part{Optimization}

\chapter{Introduction: Optimization}\label{introduction-optimization}

\section{Optimization, Simulation, and Surrogate
Modeling}\label{optimization-simulation-and-surrogate-modeling}

\begin{itemize}
\tightlist
\item
  We will consider the interplay between

  \begin{itemize}
  \tightlist
  \item
    mathematical models,
  \item
    numerical approximation,
  \item
    simulation,
  \item
    computer experiments, and
  \item
    field data
  \end{itemize}
\item
  Experimental design will play a key role in our developments, but not
  in the classical regression and response surface methodology sense
\item
  Challenging real-data/real-simulation examples benefiting from modern
  surrogate modeling methodology
\item
  We will consider the classical, response surface methodology (RSM)
  approach, and then move on to more modern approaches
\item
  All approaches are based on surrogates
\end{itemize}

\section{Surrogates}\label{surrogates}

\begin{itemize}
\tightlist
\item
  Gathering data is \textbf{expensive}, and sometimes getting exactly
  the data you want is impossible or unethical
\item
  \textbf{Surrogate}: substitute for the real thing
\item
  In statistics, draws from predictive equations derived from a fitted
  model can act as a surrogate for the data-generating mechanism
\item
  Benefits of the surrogate approach:

  \begin{itemize}
  \tightlist
  \item
    Surrogate could represent a cheaper way to explore relationships,
    and entertain ``what ifs?''
  \item
    Surrogates favor faithful yet pragmatic reproduction of dynamics:

    \begin{itemize}
    \tightlist
    \item
      interpretation,
    \item
      establishing causality, or
    \item
      identification
    \end{itemize}
  \item
    Many numerical simulators are \textbf{deterministic}, whereas field
    observations are noisy or have measurement error
  \end{itemize}
\end{itemize}

\subsection{Costs of Simulation}\label{costs-of-simulation}

\begin{itemize}
\tightlist
\item
  Computer simulations are generally cheaper (but not always!) than
  physical observation
\item
  Some computer simulations can be just as expensive as field
  experimentation, but computer modeling is regarded as easier because:

  \begin{itemize}
  \tightlist
  \item
    the experimental apparatus is better understood
  \item
    more aspects may be controlled.
  \end{itemize}
\end{itemize}

\subsection{Mathematical Models and
Meta-Models}\label{mathematical-models-and-meta-models}

\begin{itemize}
\tightlist
\item
  Use of mathematical models leveraging numerical solvers has been
  commonplace for some time
\item
  Mathematical models became more complex, requiring more resources to
  simulate/solve numerically
\item
  Practitioners increasingly relied on \textbf{meta-models} built off of
  limited simulation campaigns
\end{itemize}

\subsection{Surrogates = Trained
Meta-models}\label{surrogates-trained-meta-models}

\begin{itemize}
\tightlist
\item
  Data collected via expensive computer evaluations tuned flexible
  functional forms that could be used in lieu of further simulation to

  \begin{itemize}
  \tightlist
  \item
    save money or computational resources;
  \item
    cope with an inability to perform future runs (expired licenses,
    off-line or over-impacted supercomputers)
  \end{itemize}
\item
  Trained meta-models became known as \textbf{surrogates}
\end{itemize}

\subsection{Computer Experiments}\label{computer-experiments}

\begin{itemize}
\tightlist
\item
  \textbf{Computer experiment}: design, running, and fitting
  meta-models.

  \begin{itemize}
  \tightlist
  \item
    Like an ordinary statistical experiment, except the data are
    generated by computer codes rather than physical or field
    observations, or surveys
  \end{itemize}
\item
  \textbf{Surrogate modeling} is statistical modeling of computer
  experiments
\end{itemize}

\subsection{Limits of Mathematical
Modeling}\label{limits-of-mathematical-modeling}

\begin{itemize}
\tightlist
\item
  Mathematical biologists, economists and others had reached the limit
  of equilibrium-based mathematical modeling with cute closed-form
  solutions
\item
  \textbf{Stochastic simulations replace deterministic solvers} based on
  FEM, Navier--Stokes or Euler methods
\item
  Agent-based simulation models are used to explore predator-prey
  (Lotka--Voltera) dynamics, spread of disease, management of inventory
  or patients in health insurance markets
\item
  Consequence: the distinction between surrogate and statistical model
  is all but gone
\end{itemize}

\subsection{Example: Why Computer Simulations are
Necessary}\label{example-why-computer-simulations-are-necessary}

\begin{itemize}
\tightlist
\item
  You can't seed a real community with Ebola and watch what happens
\item
  If there's (real) field data, say on a historical epidemic, further
  experimentation may be almost entirely limited to the mathematical and
  computer modeling side
\item
  Classical statistical methods offer little guidance
\end{itemize}

\subsection{Simulation Requirements}\label{simulation-requirements}

\begin{itemize}
\tightlist
\item
  Simulation should

  \begin{itemize}
  \tightlist
  \item
    enable rich \textbf{diagnostics} to help criticize that models
  \item
    \textbf{understanding} its sensitivity to inputs and other
    configurations
  \item
    providing the ability to \textbf{optimize} and
  \item
    refine both \textbf{automatically} and with expert intervention
  \end{itemize}
\item
  And it has to do all that while remaining \textbf{computationally
  tractable}
\item
  One perspective is so-called \textbf{response surface methods} (RSMs):
\item
  a poster child from industrial statistics' heyday, well before
  information technology became a dominant industry
\end{itemize}

\section{Applications of Surrogate
Models}\label{applications-of-surrogate-models}

The four most common usages of surrogate models are:

\begin{itemize}
\tightlist
\item
  Augmenting Expensive Simulations: Surrogate models act as a `curve
  fit' to approximate the results of expensive simulation codes,
  enabling predictions without rerunning the primary source. This
  provides significant speed improvements while maintaining useful
  accuracy.
\item
  Calibration of Predictive Codes: Surrogates bridge the gap between
  simpler, faster but less accurate models and more accurate, slower
  models. This multi-fidelity approach allows for improved accuracy
  without the full computational expense.
\item
  Handling Noisy or Missing Data: Surrogates smooth out random or
  systematic errors in experimental or computational data, filling gaps
  and revealing overall trends while filtering out extraneous details.
\item
  Data Mining and Insight Generation: Surrogates help identify
  functional relationships between variables and their impact on
  results. They enable engineers to focus on critical variables and
  visualize data trends effectively.
\end{itemize}

\section{The Curse of Dimensionality}\label{the-curse-of-dimensionality}

The ``curse of dimensionality'' refers to the exponential increase in
computational complexity and data requirements as the number of
dimensions (variables) in a problem grows. In high-dimensional spaces,
the amount of data needed to maintain the same level of accuracy or
coverage increases dramatically. For example, if a one-dimensional space
requires \(n\) samples for a certain accuracy, a \(k\)-dimensional space
would require \(n^k\) samples. This makes tasks like optimization,
sampling, and modeling computationally expensive and often impractical
in high-dimensional settings.

\section{Updating a Surrogate Model}\label{updating-a-surrogate-model}

A surrogate model is updated by incorporating new data points, known as
infill points, into the model to improve its accuracy and predictive
capabilities. This process is iterative and involves the following
steps:

\begin{itemize}
\tightlist
\item
  Identify Regions of Interest: The surrogate model is analyzed to
  determine areas where it is inaccurate or where further exploration is
  needed. This could be regions with high uncertainty or areas where the
  model predicts promising results (e.g., potential optima).
\item
  Select Infill Points: Infill points are new data points chosen based
  on specific criteria, such as:
\item
  Exploitation: Sampling near predicted optima to refine the solution.
  Exploration: Sampling in regions of high uncertainty to improve the
  model globally. Balanced Approach: Combining exploitation and
  exploration to ensure both local and global improvements.
\item
  Evaluate the True Function: The true function (e.g., a simulation or
  experiment) is evaluated at the selected infill points to obtain their
  corresponding outputs.
\item
  Update the Surrogate Model: The surrogate model is retrained or
  updated using the new data, including the infill points, to improve
  its accuracy.
\item
  Repeat: The process is repeated until the model meets predefined
  accuracy criteria or the computational budget is exhausted.
\end{itemize}

\begin{definition}[Infill
Points]\protect\hypertarget{def-infill-points}{}\label{def-infill-points}

Infill points are strategically chosen new data points added to the
surrogate model. They are selected to:

\begin{itemize}
\tightlist
\item
  Reduce uncertainty in the model.
\item
  Improve predictions in regions of interest.
\item
  Enhance the model's ability to identify optima or trends.
\end{itemize}

\end{definition}

The selection of infill points is often guided by infill criteria, such
as:

\begin{itemize}
\tightlist
\item
  Expected Improvement (EI): Maximizing the expected improvement over
  the current best solution.
\item
  Uncertainty Reduction: Sampling where the model's predictions have
  high variance.
\item
  Probability of Improvement (PI): Sampling where the probability of
  improving the current best solution is highest.
\end{itemize}

The iterative infill-points updating process ensures that the surrogate
model becomes increasingly accurate and useful for optimization or
decision-making tasks.

\section{Sampling Plans}\label{sampling-plans}

\subsection{Ideas and Concepts}\label{ideas-and-concepts}

Engineering design often requires the construction of a surrogate model
\(\hat{f}\) to approximate the expensive response of a black-box
function \(f\). The function \(f(x)\) represents a continuous metric
(e.g., quality, cost, or performance) defined over a design space
\(D \subset \mathbb{R}^k\), where \(x\) is a \(k\)-dimensional vector of
design variables. Since evaluating \(f\) is costly, only a sparse set of
samples is used to construct \(\hat{f}\), which can then provide
inexpensive predictions for any \(x \in D\).

The process involves:

\begin{itemize}
\tightlist
\item
  Sampling discrete observations:
\item
  Using these samples to construct an approximation \(\hat{f}\).
\item
  Ensuring the surrogate model is well-posed, meaning it is
  mathematically valid and can generalize predictions effectively.
\end{itemize}

A sampling plan

\[
X =
\left\{
  x^{(i)} \in D | i = 1, \ldots, n 
\right\}
\]

determines the spatial arrangement of observations. While some models
require a minimum number of data points \(n\), once this threshold is
met, a surrogate model can be constructed to approximate \(f\)
efficiently.

A well-posed model does not always perform well because its ability to
generalize depends heavily on the sampling plan used to collect data. If
the sampling plan is poorly designed, the model may fail to capture
critical behaviors in the design space. For example:

\begin{itemize}
\tightlist
\item
  Extreme Sampling: Measuring performance only at the extreme values of
  parameters may miss important behaviors in the center of the design
  space, leading to incomplete understanding.
\item
  Uneven Sampling: Concentrating samples in certain regions while
  neglecting others forces the model to extrapolate over unsampled
  areas, potentially resulting in inaccurate or misleading predictions.
  Additionally, in some cases, the data may come from external sources
  or be limited in scope, leaving little control over the sampling plan.
  This can further restrict the model's ability to generalize
  effectively.
\end{itemize}

\subsection{The `Curse of Dimensionality' and How to Avoid
It}\label{the-curse-of-dimensionality-and-how-to-avoid-it}

The curse of dimensionality refers to the exponential increase in
computational effort as the number of design variables grows. For a
one-dimensional space, sampling \(n\) locations may suffice for accurate
predictions, but for a \(k\)-dimensional space, the required number of
observations increases to \(n^k\). This makes high-dimensional problems
computationally expensive and often impractical.

\begin{example}[Example: Curse of
Dimensionality]\protect\hypertarget{exm-curse-of-dim}{}\label{exm-curse-of-dim}

Consider a simple example where we want to model the cost of a car tire
based on its wheel diameter. If we have one variable (wheel diameter),
we might need 10 simulations to get a good estimate of the cost. Now, if
we add 8 more variables (e.g., tread pattern, rubber type, etc.), the
number of simulations required increases to \(10^8\) (10 million). This
is because the number of combinations of design variables grows
exponentially with the number of dimensions. This means that the
computational budget required to evaluate all combinations of design
variables becomes infeasible. In this case, it would take 11,416 years
to complete the simulations, making it impractical to explore the design
space fully.

\end{example}

\subsection{Physical versus Computational
Experiments}\label{physical-versus-computational-experiments}

Physical experiments are prone to experimental errors from three main
sources:

\begin{itemize}
\tightlist
\item
  Human error: Mistakes made by the experimenter.
\item
  Random error: Measurement inaccuracies that vary unpredictably.
\item
  Systematic error: Consistent bias due to flaws in the experimental
  setup.
\end{itemize}

The key distinction is repeatability: systematic errors remain constant
across repetitions, while random errors vary.

Computational experiments, on the other hand, are deterministic and free
from random errors. However, they are still affected by:

\begin{itemize}
\tightlist
\item
  Human error: Bugs in code or incorrect boundary conditions.
\item
  Systematic error: Biases from model simplifications (e.g., inviscid
  flow approximations) or finite resolution (e.g., insufficient mesh
  resolution).
\end{itemize}

The term ``noise'' is used differently in physical and computational
contexts. In physical experiments, it refers to random errors, while in
computational experiments, it often refers to systematic errors.

Understanding these differences is crucial for designing experiments and
applying techniques like Gaussian process-based approximations. For
physical experiments, replication mitigates random errors, but this is
unnecessary for deterministic computational experiments.

\subsection{Designing Preliminary Experiments
(Screening)}\label{designing-preliminary-experiments-screening}

Minimizing the number of design variables \(x_1, x_2, \dots, x_k\) is
crucial before modeling the objective function \(f\). This process,
called screening, aims to reduce dimensionality without compromising the
analysis. If \(f\) is at least once differentiable over the design
domain \(D\), the partial derivative \(\frac{\partial f}{\partial x_i}\)
can be used to classify variables:

\begin{itemize}
\tightlist
\item
  Negligible Variables: If
  \(\frac{\partial f}{\partial x_i} = 0, \, \forall x \in D\), the
  variable \(x_i\) can be safely neglected.
\item
  Linear Additive Variables: If
  \(\frac{\partial f}{\partial x_i} = \text{constant} \neq 0, \, \forall x \in D\),
  the effect of \(x_i\) is linear and additive.
\item
  Nonlinear Variables: If
  \(\frac{\partial f}{\partial x_i} = g(x_i), \, \forall x \in D\),
  where \(g(x_i)\) is a non-constant function, \(f\) is nonlinear in
  \(x_i\).
\item
  Interactive Nonlinear Variables: If
  \(\frac{\partial f}{\partial x_i} = g(x_i, x_j, \dots), /, \forall x \in D\),
  where \(g(x_i, x_j, \dots)\) is a function involving interactions with
  other variables, \(f\) is nonlinear in \(x_i\) and interacts with
  \(x_j\).
\end{itemize}

Measuring \(\frac{\partial f}{\partial x_i}\) across the entire design
space is often infeasible due to limited budgets. The percentage of time
allocated to screening depends on the problem: If many variables are
expected to be inactive, thorough screening can significantly improve
model accuracy by reducing dimensionality. If most variables are
believed to impact the objective, focus should shift to modeling
instead. Screening is a trade-off between computational cost and model
accuracy, and its effectiveness depends on the specific problem context.

\subsubsection{Estimating the Distribution of Elementary
Effects}\label{estimating-the-distribution-of-elementary-effects}

In order to simplify the presentation of what follows, we make, without
loss of generality, the assumption that the design space
\(D = [0, 1]^k\); that is, we normalize all variables into the unit
cube. We shall adhere to this convention for the rest of the book and
strongly urge the reader to do likewise when implementing any algorithms
described here, as this step not only yields clearer mathematics in some
cases but also safeguards against scaling issues.

Before proceeding with the description of the Morris algorithm, we need
to define an important statistical concept. Let us restrict our design
space \(D\) to a \(k\)-dimensional, \(p\)-level full factorial grid,
that is,

\[
x_i \in \{0, \frac{1}{p-1}, \frac{2}{p-1}, \dots, 1\}, \quad \text{ for } i = 1, \dots, k.
\]

For a given baseline value \(x \in D\), let \(d_i(x)\) denote the
elementary effect of \(x_i\), where:

\begin{equation}\phantomsection\label{eq-eleffect}{
d_i(x) = \frac{f(x_1, \dots, x_i + \Delta, \dots, x_k) - f(x_1, \dots, x_i - \Delta, \dots, x_k)}{2\Delta}, \quad i = 1, \dots, k,
}\end{equation} where \(\Delta\) is the step size, which is defined as
the distance between two adjacent levels in the grid. In other words, we
have:

with
\[\Delta = \frac{\xi}{p-1}, \quad \xi \in \mathbb{N}^*, \quad \text{and} \quad x \in D , \text{ such that its components } x_i \leq 1 - \Delta.
\]

\(\Delta\) is the step size. The elementary effect \(d_i(x)\) measures
the sensitivity of the function \(f\) to changes in the variable \(x_i\)
at the point \(x\).

Morris's method aims to estimate the parameters of the distribution of
elementary effects associated with each variable. A large measure of
central tendency indicates that a variable has a significant influence
on the objective function across the design space, while a large measure
of spread suggests that the variable is involved in interactions or
contributes to the nonlinearity of \(f\). In practice, the sample mean
and standard deviation of a set of \(d_i(x)\) values, calculated in
different parts of the design space, are used for this estimation.

To ensure efficiency, the preliminary sampling plan \(X\) should be
designed so that each evaluation of the objective function \(f\)
contributes to the calculation of two elementary effects, rather than
just one (as would occur with a naive random spread of baseline \(x\)
values and adding \(\Delta\) to one variable). Additionally, the
sampling plan should provide a specified number (e.g., \(r\)) of
elementary effects for each variable, independently drawn with
replacement. For a detailed discussion on constructing such a sampling
plan, readers are encouraged to consult Morris's original paper (Morris,
1991). Here, we focus on describing the process itself.

The random orientation of the sampling plan \(B\) can be constructed as
follows: * Let \(B\) be a \((k+1) \times k\) matrix of 0s and 1s, where
for each column \(i\), two rows differ only in their \(i\)-th entries. *
Compute a random orientation of \(B\), denoted \(B^*\):

\[
B^* =
\left(
1_{k+1,k} x^* + (\Delta/2) 
\left[
(2B-1_{k+1,k})
D^* +
1_{k+1,k}
\right]
\right)
P^*,
\]

where:

\begin{itemize}
\tightlist
\item
  \(D^*\) is a \(k\)-dimensional diagonal matrix with diagonal elements
  \(\pm 1\) (equal probability),
\item
  \(\mathbf{1}\) is a matrix of 1s,
\item
  \(x^*\) is a randomly chosen point in the \(p\)-level design space
  (limited by \(\Delta\)),
\item
  \(P^*\) is a \(k \times k\) random permutation matrix with one 1 per
  column and row.
\end{itemize}

\texttt{spotpython} provides a \texttt{Python} implementation to compute
\(B^*\), see
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py}.

Here is the corresponding code:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\KeywordTok{def}\NormalTok{ randorient(k, p, xi, seed}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \CommentTok{\# Initialize random number generator with the provided seed}
    \ControlFlowTok{if}\NormalTok{ seed }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}

    \CommentTok{\# Step length}
\NormalTok{    Delta }\OperatorTok{=}\NormalTok{ xi }\OperatorTok{/}\NormalTok{ (p }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}

\NormalTok{    m }\OperatorTok{=}\NormalTok{ k }\OperatorTok{+} \DecValTok{1}

    \CommentTok{\# A truncated p{-}level grid in one dimension}
\NormalTok{    xs }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{0}\NormalTok{, }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ Delta, }\DecValTok{1} \OperatorTok{/}\NormalTok{ (p }\OperatorTok{{-}} \DecValTok{1}\NormalTok{))}
\NormalTok{    xsl }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(xs)}
    \ControlFlowTok{if}\NormalTok{ xsl }\OperatorTok{\textless{}} \DecValTok{1}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"xi = }\SpecialCharTok{\{}\NormalTok{xi}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p = }\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Delta = }\SpecialCharTok{\{}\NormalTok{Delta}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p {-} 1 = }\SpecialCharTok{\{}\NormalTok{p }\OperatorTok{{-}} \DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\SpecialStringTok{f"The number of levels xsl is }\SpecialCharTok{\{}\NormalTok{xsl}\SpecialCharTok{\}}\SpecialStringTok{, but it must be greater than 0."}\NormalTok{)}

    \CommentTok{\# Basic sampling matrix}
\NormalTok{    B }\OperatorTok{=}\NormalTok{ np.vstack((np.zeros((}\DecValTok{1}\NormalTok{, k)), np.tril(np.ones((k, k)))))}

    \CommentTok{\# Randomization}

    \CommentTok{\# Matrix with +1s and {-}1s on the diagonal with equal probability}
\NormalTok{    Dstar }\OperatorTok{=}\NormalTok{ np.diag(}\DecValTok{2} \OperatorTok{*}\NormalTok{ rng.integers(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, size}\OperatorTok{=}\NormalTok{k) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}

    \CommentTok{\# Random base value}
\NormalTok{    xstar }\OperatorTok{=}\NormalTok{ xs[rng.integers(}\DecValTok{0}\NormalTok{, xsl, size}\OperatorTok{=}\NormalTok{k)]}

    \CommentTok{\# Permutation matrix}
\NormalTok{    Pstar }\OperatorTok{=}\NormalTok{ np.zeros((k, k))}
\NormalTok{    rp }\OperatorTok{=}\NormalTok{ rng.permutation(k)}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
\NormalTok{        Pstar[i, rp[i]] }\OperatorTok{=} \DecValTok{1}

    \CommentTok{\# A random orientation of the sampling matrix}
\NormalTok{    Bstar }\OperatorTok{=}\NormalTok{ (np.ones((m, }\DecValTok{1}\NormalTok{)) }\OperatorTok{@}\NormalTok{ xstar.reshape(}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ (Delta }\OperatorTok{/} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\NormalTok{ ((}\DecValTok{2} \OperatorTok{*}\NormalTok{ B }\OperatorTok{{-}}\NormalTok{ np.ones((m, k))) }\OperatorTok{@}\NormalTok{ Dstar }\OperatorTok{+}\NormalTok{ np.ones((m, k)))) }\OperatorTok{@}\NormalTok{ Pstar}

    \ControlFlowTok{return}\NormalTok{ Bstar}
\end{Highlighting}
\end{Shaded}

The code following snippet generates a random orientation of a sampling
matrix \texttt{Bstar} using the \texttt{randorient()} function. The
input parameters are:

\begin{itemize}
\tightlist
\item
  k = 3: The number of design variables (dimensions).
\item
  p = 3: The number of levels in the grid for each variable.
\item
  xi = 1: A parameter used to calculate the step size Delta.
\end{itemize}

Step-size calculation is performed as follows:
\texttt{Delta\ =\ xi\ /\ (p\ -\ 1)\ =\ 1\ /\ (3\ -\ 1)\ =\ 0.5}, which
determines the spacing between levels in the grid.

Next, random sampling matrix construction is computed:

\begin{itemize}
\tightlist
\item
  A truncated grid is created with levels \texttt{{[}0,\ 0.5{]}} (based
  on Delta).
\item
  A basic sampling matrix B is constructed, which is a lower triangular
  matrix with 0s and 1s.
\end{itemize}

Then, randomization is applied:

\begin{itemize}
\tightlist
\item
  Dstar: A diagonal matrix with random entries of +1 or -1.
\item
  xstar: A random starting point from the grid.
\item
  Pstar: A random permutation matrix.
\end{itemize}

Random orientation is applied to the basic sampling matrix B to create
Bstar. This involves scaling, shifting, and permuting the rows and
columns of B.

The final output is the matrix \texttt{Bstar}, which represents a random
orientation of the sampling plan. Each row corresponds to a sampled
point in the design space, and each column corresponds to a design
variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example usage}
\NormalTok{k }\OperatorTok{=} \DecValTok{3}
\NormalTok{p }\OperatorTok{=} \DecValTok{3}
\NormalTok{xi }\OperatorTok{=} \DecValTok{1}
\NormalTok{Bstar }\OperatorTok{=}\NormalTok{ randorient(k, p, xi)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Random orientation of the sampling matrix:}\CharTok{\textbackslash{}n}\SpecialCharTok{\{}\NormalTok{Bstar}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Random orientation of the sampling matrix:
[[0.  0.5 0.5]
 [0.5 0.5 0.5]
 [0.5 0.5 0. ]
 [0.5 0.  0. ]]
\end{verbatim}

To obtain \(r\) elementary effects for each variable, the screening plan
is built from \(r\) random orientations:

\[
X = 
\begin{pmatrix}
B^*_1 \\
B^*_2 \\
\vdots \\
B^*_r
\end{pmatrix}
\]

The screening plan implementation in \texttt{Python} is as follows (see
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ screeningplan(k, p, xi, r):}
    \CommentTok{\# Empty list to accumulate screening plan rows}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ []}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(r):}
\NormalTok{        X.append(randorient(k, p, xi))}

    \CommentTok{\# Concatenate list of arrays into a single array}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.vstack(X)}

    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

The function \texttt{screeningplan()} generates a screening plan by
calling the \texttt{randorient()} function \texttt{r} times. It creates
a list of random orientations and then concatenates them into a single
array, which represents the screening plan. It works like follows:

\begin{itemize}
\item
  The value of the objective function \(f\) is computed for each row of
  the screening plan matrix \(X\). These values are stored in a column
  vector \(t\) of size \((r * (k + 1)) \times 1\), where:
\item
  r is the number of random orientations.
\item
  k is the number of design variables.
\end{itemize}

The elementary effects are calculated using the following formula:

\begin{itemize}
\tightlist
\item
  For each random orientation, adjacent rows of the screening plan
  matrix X and their corresponding function values from t are used.
\item
  These values are inserted into Equation~\ref{eq-eleffect} to compute
  elementary effects for each variable. An elementary effect measures
  the sensitivity of the objective function to changes in a specific
  variable.
\end{itemize}

Results can be used for a statistical analysis. After collecting a
sample of \(r\) elementary effects for each variable:

\begin{itemize}
\tightlist
\item
  The sample mean (central tendency) is computed to indicate the overall
  influence of the variable.
\item
  The sample standard deviation (spread) is computed to capture
  variability, which may indicate interactions or nonlinearity.
\end{itemize}

The results (sample means and standard deviations) are plotted on a
chart for comparison. This helps identify which variables have the most
significant impact on the objective function and whether their effects
are linear or involve interactions.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.effects }\ImportTok{import}\NormalTok{ screening\_plot, screeningplan}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical()}
\NormalTok{k }\OperatorTok{=} \DecValTok{10}
\NormalTok{p }\OperatorTok{=} \DecValTok{10}
\NormalTok{xi }\OperatorTok{=} \DecValTok{1}
\NormalTok{r }\OperatorTok{=} \DecValTok{25}
\NormalTok{X }\OperatorTok{=}\NormalTok{ screeningplan(k}\OperatorTok{=}\NormalTok{k, p}\OperatorTok{=}\NormalTok{p, xi}\OperatorTok{=}\NormalTok{xi, r}\OperatorTok{=}\NormalTok{r)  }\CommentTok{\# shape (r x (k+1), k)}
\NormalTok{value\_range }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{    [}\DecValTok{150}\NormalTok{, }\DecValTok{220}\NormalTok{,   }\DecValTok{6}\NormalTok{, }\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{16}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\DecValTok{1700}\NormalTok{, }\FloatTok{0.025}\NormalTok{],}
\NormalTok{    [}\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{,  }\DecValTok{10}\NormalTok{,  }\DecValTok{10}\NormalTok{, }\DecValTok{45}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{6.0}\NormalTok{, }\DecValTok{2500}\NormalTok{, }\FloatTok{0.08}\NormalTok{ ],}
\NormalTok{])}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}
    \StringTok{"S\_W"}\NormalTok{, }\StringTok{"W\_fw"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"Lambda"}\NormalTok{,}
    \StringTok{"q"}\NormalTok{,   }\StringTok{"lambda"}\NormalTok{, }\StringTok{"tc"}\NormalTok{, }\StringTok{"N\_z"}\NormalTok{,}
    \StringTok{"W\_dg"}\NormalTok{, }\StringTok{"W\_p"}
\NormalTok{]}
\NormalTok{screening\_plot(}
\NormalTok{    X}\OperatorTok{=}\NormalTok{X,}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{fun.fun\_wingwt,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{value\_range,}
\NormalTok{    xi}\OperatorTok{=}\NormalTok{xi,}
\NormalTok{    p}\OperatorTok{=}\NormalTok{p,}
\NormalTok{    labels}\OperatorTok{=}\NormalTok{labels,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_optimization_surrogate_files/figure-pdf/fig-forre08a-1-2-output-1.pdf}}

}

\caption{\label{fig-forre08a-1-2}Estimated means and standard deviations
of the elementary effects for the 10 design variables of the wing weight
function. Example based on Forrester, SÃ³bester, and Keane (2008).}

\end{figure}%

\section{Designing a Sampling Plan}\label{designing-a-sampling-plan}

\subsection{Stratification}\label{stratification}

A feature shared by all of the approximation models discussed in
Forrester, SÃ³bester, and Keane (2008) is that they are more accurate in
the vicinity of the points where we have evaluated the objective
function. In later chapters we will delve into the laws that quantify
our decaying trust in the model as we move away from a known, sampled
point, but for the purposes of the present discussion we shall merely
draw the intuitive conclusion that a uniform level of model accuracy
throughout the design space requires a uniform spread of points. A
sampling plan possessing this feature is said to be space-filling .

The most straightforward way of sampling a design space in a uniform
fashion is by means of a rectangular grid of points. This is the full
factorial sampling technique referred to in the section about the curse
of dimensionality.

Here is the simplified version of a \texttt{Python} function that will
sample the unit hypercube at all levels in all dimensions, with the
\(k\)-vector \(q\) containing the number of points required along each
dimension, see
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/sampling.py}.

The variable \texttt{Edges} specifies whether we want the points to be
equally spaced from edge to edge (\texttt{Edges=1}) or we want them to
be in the centres of \(n = q_1 \times q_2 \times \ldots \times q_k\)
bins filling the unit hypercube (for any other value of \texttt{Edges}).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Tuple, Optional}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}


\KeywordTok{def}\NormalTok{ fullfactorial(q, Edges}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""Generates a full factorial sampling plan in the unit cube.}

\CommentTok{    Args:}
\CommentTok{        q (list or np.ndarray):}
\CommentTok{            A list or array containing the number of points along each dimension (k{-}vector).}
\CommentTok{        Edges (int, optional):}
\CommentTok{            Determines spacing of points. If \textasciigrave{}Edges=1\textasciigrave{}, points are equally spaced from edge to edge (default).}
\CommentTok{            Otherwise, points will be in the centers of n = q[0]*q[1]*...*q[k{-}1] bins filling the unit cube.}

\CommentTok{    Returns:}
\CommentTok{        (np.ndarray): Full factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.}

\CommentTok{    Raises:}
\CommentTok{        ValueError: If any dimension in \textasciigrave{}q\textasciigrave{} is less than 2.}

\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} from spotpython.utils.sampling import fullfactorial}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} q = [3, 2]}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} X = fullfactorial(q, Edges=0)}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} print(X)}
\CommentTok{                [[0.         0.        ]}
\CommentTok{                [0.         0.75      ]}
\CommentTok{                [0.41666667 0.        ]}
\CommentTok{                [0.41666667 0.75      ]}
\CommentTok{                [0.83333333 0.        ]}
\CommentTok{                [0.83333333 0.75      ]]}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} X = fullfactorial(q, Edges=1)}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} print(X)}
\CommentTok{                [[0.  0. ]}
\CommentTok{                [0.  1. ]}
\CommentTok{                [0.5 0. ]}
\CommentTok{                [0.5 1. ]}
\CommentTok{                [1.  0. ]}
\CommentTok{                [1.  1. ]]}

\CommentTok{    """}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ np.array(q)}
    \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(q) }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"You must have at least two points per dimension."}\NormalTok{)}

    \CommentTok{\# Total number of points in the sampling plan}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ np.prod(q)}

    \CommentTok{\# Number of dimensions}
\NormalTok{    k }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(q)}

    \CommentTok{\# Pre{-}allocate memory for the sampling plan}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.zeros((n, k))}

    \CommentTok{\# Additional phantom element}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ np.append(q, }\DecValTok{1}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{if}\NormalTok{ Edges }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
\NormalTok{            one\_d\_slice }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, q[j])}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            one\_d\_slice }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ q[j]), }\DecValTok{1}\NormalTok{, q[j]) }\OperatorTok{{-}} \DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ q[j])}

\NormalTok{        column }\OperatorTok{=}\NormalTok{ np.array([])}

        \ControlFlowTok{while} \BuiltInTok{len}\NormalTok{(column) }\OperatorTok{\textless{}}\NormalTok{ n:}
            \ControlFlowTok{for}\NormalTok{ ll }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(q[j]):}
\NormalTok{                column }\OperatorTok{=}\NormalTok{ np.append(column, np.ones(np.prod(q[j }\OperatorTok{+} \DecValTok{1}\NormalTok{ : k])) }\OperatorTok{*}\NormalTok{ one\_d\_slice[ll])}

\NormalTok{        X[:, j] }\OperatorTok{=}\NormalTok{ column}

    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ fullfactorial}
\NormalTok{q }\OperatorTok{=}\NormalTok{ [}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{X }\OperatorTok{=}\NormalTok{ fullfactorial(q, Edges}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.         0.        ]
 [0.         0.75      ]
 [0.41666667 0.        ]
 [0.41666667 0.75      ]
 [0.83333333 0.        ]
 [0.83333333 0.75      ]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ fullfactorial(q, Edges}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.  0. ]
 [0.  1. ]
 [0.5 0. ]
 [0.5 1. ]
 [1.  0. ]
 [1.  1. ]]
\end{verbatim}

The full factorial sampling plan method generates a uniform sampling
design by creating a grid of points across all dimensions. For example,
calling fullfactorial({[}3, 4, 5{]}, 1) produces a three-dimensional
sampling plan with 3, 4, and 5 levels along each dimension,
respectively. While this approach satisfies the uniformity criterion, it
has two significant limitations:

\begin{itemize}
\item
  Restricted Design Sizes: The method only works for designs where the
  total number of points \(n\) can be expressed as the product of the
  number of levels in each dimension, i.e.,
  \(n = q_1 \times q_2 \times \cdots \times q_k\).
\item
  Overlapping Projections: When the sampling points are projected onto
  individual axes, sets of points may overlap, reducing the
  effectiveness of the sampling plan. This can lead to non-uniform
  coverage in the projections, which may not fully represent the design
  space.
\end{itemize}

\subsection{Latin Squares and Random Latin
Hypercubes}\label{latin-squares-and-random-latin-hypercubes}

To improve the uniformity of projections for any individual variable,
the range of that variable can be divided into a large number of
equal-sized bins, and random subsamples of equal size can be generated
within these bins. This method is called stratified random sampling.
Extending this idea to all dimensions results in a stratified sampling
plan, commonly implemented using Latin hypercube sampling.

For two-dimensional discrete variables, a Latin square ensures uniform
projections. An \((n \times n)\) Latin square is constructed by filling
each row and column with a permutation of \(\{1, 2, \dots, n\}\),
ensuring each number appears only once per row and column. For example,
for \$n = 4 \$, a Latin square might look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{2   1   3   4}
\NormalTok{3   2   4   1}
\NormalTok{1   4   2   3}
\NormalTok{4   3   1   2}
\end{Highlighting}
\end{Shaded}

Latin Hypercubes are the multidimensional extension of Latin squares.
The design space is divided into equal-sized hypercubes (bins), and one
point is placed in each bin. The placement ensures that moving along any
axis from an occupied bin does not encounter another occupied bin. This
guarantees uniform projections across all dimensions. To construct a
Latin hypercube:

Represent the sampling plan as an ( n \times k ) matrix ( X ), where ( n
) is the number of points and ( k ) is the number of dimensions. Fill
each column of ( X ) with random permutations of ( \{1, 2, \dots, n\} ).
Normalize the plan into the unit hypercube ({[}0, 1{]}\^{}k). This
approach ensures multidimensional stratification and uniformity in
projections. Here is the code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rlh(n: }\BuiltInTok{int}\NormalTok{, k: }\BuiltInTok{int}\NormalTok{, edges: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{\# Initialize array}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.zeros((n, k), dtype}\OperatorTok{=}\BuiltInTok{float}\NormalTok{)}

    \CommentTok{\# Fill with random permutations}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
\NormalTok{        X[:, i] }\OperatorTok{=}\NormalTok{ np.random.permutation(n)}

    \CommentTok{\# Adjust normalization based on the edges flag}
    \ControlFlowTok{if}\NormalTok{ edges }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
        \CommentTok{\# [X=0..n{-}1] {-}\textgreater{} [0..1]}
\NormalTok{        X }\OperatorTok{=}\NormalTok{ X }\OperatorTok{/}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# Points at true midpoints}
        \CommentTok{\# [X=0..n{-}1] {-}\textgreater{} [0.5/n..(n{-}0.5)/n]}
\NormalTok{        X }\OperatorTok{=}\NormalTok{ (X }\OperatorTok{+} \FloatTok{0.5}\NormalTok{) }\OperatorTok{/}\NormalTok{ n}

    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ rlh}
\CommentTok{\# Generate a 2D Latin hypercube with 5 points and edges=0}
\NormalTok{X }\OperatorTok{=}\NormalTok{ rlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, edges}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.9 0.3]
 [0.1 0.1]
 [0.3 0.5]
 [0.7 0.7]
 [0.5 0.9]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ rlh}
\CommentTok{\# Generate a 2D Latin hypercube with 5 points and edges=1}
\NormalTok{X }\OperatorTok{=}\NormalTok{ rlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, edges}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.75 0.25]
 [0.   1.  ]
 [0.5  0.5 ]
 [0.25 0.75]
 [1.   0.  ]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ rlh}

\CommentTok{\# Generate a 2D Latin hypercube with 5 points and edges=0}
\NormalTok{X }\OperatorTok{=}\NormalTok{ rlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, edges}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\CommentTok{\# Plot the points}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plt.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{50}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Hypercube Points\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}2D Latin Hypercube Sampling (5 Points, Edges=0)\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Dimension 1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Dimension 2\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{001_optimization_surrogate_files/figure-pdf/cell-12-output-1.pdf}}

\subsection{Space-filling Latin
Hypercubes}\label{space-filling-latin-hypercubes}

\subsubsection{\texorpdfstring{Die Funktion
\texttt{jd(X,p)}}{Die Funktion jd(X,p)}}\label{die-funktion-jdxp}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ jd(X: np.ndarray, p: }\BuiltInTok{float} \OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ Tuple[np.ndarray, np.ndarray]:}
    \CommentTok{"""}
\CommentTok{    Computes and counts the distinct p{-}norm distances between all pairs of points in X.}
\CommentTok{    It returns:}
\CommentTok{    1) A list of distinct distances (sorted), and}
\CommentTok{    2) A corresponding multiplicity array that indicates how often each distance occurs.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray):}
\CommentTok{            A 2D array of shape (n, d) representing n points in d{-}dimensional space.}
\CommentTok{        p (float, optional):}
\CommentTok{            The distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the}
\CommentTok{            Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).}

\CommentTok{    Returns:}
\CommentTok{        (np.ndarray, np.ndarray):}
\CommentTok{            A tuple (J, distinct\_d), where:}
\CommentTok{            {-} distinct\_d is a 1D float array of unique, sorted distances between points.}
\CommentTok{            {-} J is a 1D integer array that provides the multiplicity (occurrence count)}
\CommentTok{              of each distance in distinct\_d.}
\CommentTok{    """}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# Allocate enough space for all pairwise distances}
    \CommentTok{\# (n*(n{-}1))/2 pairs for an n{-}point set}
\NormalTok{    pair\_count }\OperatorTok{=}\NormalTok{ n }\OperatorTok{*}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{//} \DecValTok{2}
\NormalTok{    d }\OperatorTok{=}\NormalTok{ np.zeros(pair\_count, dtype}\OperatorTok{=}\BuiltInTok{float}\NormalTok{)}

    \CommentTok{\# Fill the distance array}
\NormalTok{    idx }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i }\OperatorTok{+} \DecValTok{1}\NormalTok{, n):}
            \CommentTok{\# Compute the p{-}norm distance}
\NormalTok{            d[idx] }\OperatorTok{=}\NormalTok{ np.linalg.norm(X[i] }\OperatorTok{{-}}\NormalTok{ X[j], }\BuiltInTok{ord}\OperatorTok{=}\NormalTok{p)}
\NormalTok{            idx }\OperatorTok{+=} \DecValTok{1}

    \CommentTok{\# Find unique distances and their multiplicities}
\NormalTok{    distinct\_d }\OperatorTok{=}\NormalTok{ np.unique(d)}
\NormalTok{    J }\OperatorTok{=}\NormalTok{ np.zeros\_like(distinct\_d, dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ i, val }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(distinct\_d):}
\NormalTok{        J[i] }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(d }\OperatorTok{==}\NormalTok{ val)}

    \ControlFlowTok{return}\NormalTok{ J, distinct\_d}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{Die Funktion
\texttt{mm(X1,\ X2,p)}}{Die Funktion mm(X1, X2,p)}}\label{die-funktion-mmx1-x2p}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ jd}
\CommentTok{\# A small 3{-}point set in 2D}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{],[}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{],[}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{]])}
\NormalTok{J, distinct\_d }\OperatorTok{=}\NormalTok{ jd(X, p}\OperatorTok{=}\FloatTok{2.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Distinct distances (d\_i):"}\NormalTok{, distinct\_d)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Occurrences (J\_i):"}\NormalTok{, J)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Distinct distances (d_i): [1.41421356 2.82842712]
Occurrences (J_i): [2 1]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mm(X1: np.ndarray, X2: np.ndarray, p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \CommentTok{"""}
\CommentTok{    Determines which of two sampling plans has better space{-}filling properties}
\CommentTok{    according to the Morris{-}Mitchell criterion.}

\CommentTok{    Args:}
\CommentTok{        X1 (np.ndarray): A 2D array representing the first sampling plan.}
\CommentTok{        X2 (np.ndarray): A 2D array representing the second sampling plan.}
\CommentTok{        p (float, optional): The distance metric. p=1 uses Manhattan (L1) distance,}
\CommentTok{            while p=2 uses Euclidean (L2). Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        int:}
\CommentTok{            {-} 0 if both plans are identical or equally space{-}filling}
\CommentTok{            {-} 1 if X1 is more space{-}filling}
\CommentTok{            {-} 2 if X2 is more space{-}filling}
\CommentTok{    """}
\NormalTok{    X1\_sorted }\OperatorTok{=}\NormalTok{ X1[np.lexsort(np.rot90(X1))]}
\NormalTok{    X2\_sorted }\OperatorTok{=}\NormalTok{ X2[np.lexsort(np.rot90(X2))]}
    \ControlFlowTok{if}\NormalTok{ np.array\_equal(X1\_sorted, X2\_sorted):}
        \ControlFlowTok{return} \DecValTok{0}  \CommentTok{\# Identical sampling plans}

    \CommentTok{\# Compute distance multiplicities for each plan}
\NormalTok{    J1, d1 }\OperatorTok{=}\NormalTok{ jd(X1, p)}
\NormalTok{    J2, d2 }\OperatorTok{=}\NormalTok{ jd(X2, p)}
\NormalTok{    m1, m2 }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(d1), }\BuiltInTok{len}\NormalTok{(d2)}

    \CommentTok{\# Construct V1 and V2: alternate distance and negative multiplicity}
\NormalTok{    V1 }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{*}\NormalTok{ m1)}
\NormalTok{    V1[}\DecValTok{0}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ d1}
\NormalTok{    V1[}\DecValTok{1}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{J1}

\NormalTok{    V2 }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{*}\NormalTok{ m2)}
\NormalTok{    V2[}\DecValTok{0}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ d2}
\NormalTok{    V2[}\DecValTok{1}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{J2}

    \CommentTok{\# Trim the longer vector to match the size of the shorter}
\NormalTok{    m }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(m1, m2)}
\NormalTok{    V1 }\OperatorTok{=}\NormalTok{ V1[:m]}
\NormalTok{    V2 }\OperatorTok{=}\NormalTok{ V2[:m]}

    \CommentTok{\# Compare element{-}by{-}element:}
    \CommentTok{\# c[i] = 1 if V1[i] \textgreater{} V2[i], 2 if V1[i] \textless{} V2[i], 0 otherwise.}
\NormalTok{    c }\OperatorTok{=}\NormalTok{ (V1 }\OperatorTok{\textgreater{}}\NormalTok{ V2).astype(}\BuiltInTok{int}\NormalTok{) }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ (V1 }\OperatorTok{\textless{}}\NormalTok{ V2).astype(}\BuiltInTok{int}\NormalTok{)}

    \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(c) }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \CommentTok{\# Equally space{-}filling}
        \ControlFlowTok{return} \DecValTok{0}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# The first non{-}zero entry indicates which plan is better}
\NormalTok{        idx }\OperatorTok{=}\NormalTok{ np.argmax(c }\OperatorTok{!=} \DecValTok{0}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ c[idx]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ mm}
\CommentTok{\# Create two 3{-}point sampling plans in 2D}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{],[}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{],[}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{], [}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{]])}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{],[}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.6}\NormalTok{],[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.9}\NormalTok{], [}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.9}\NormalTok{]])}
\CommentTok{\# plot the two plans}
\NormalTok{plt.scatter(X1[:, }\DecValTok{0}\NormalTok{], X1[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Plan 1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.scatter(X2[:, }\DecValTok{0}\NormalTok{], X2[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Plan 2\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Comparison of Two Sampling Plans\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Dimension 1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Dimension 2\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.show()}
\CommentTok{\# Compare which plan has better space{-}filling (Morris{-}Mitchell)}
\NormalTok{better }\OperatorTok{=}\NormalTok{ mm(X1, X2, p}\OperatorTok{=}\FloatTok{2.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(better)}
\CommentTok{\# Prints either 0, 1, or 2 depending on which plan is more space{-}filling.}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{001_optimization_surrogate_files/figure-pdf/cell-16-output-1.pdf}}

\begin{verbatim}
1
\end{verbatim}

\subsubsection{\texorpdfstring{Die Funktion
\texttt{mmphi(X,q,p)}}{Die Funktion mmphi(X,q,p)}}\label{die-funktion-mmphixqp}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mmphi(X: np.ndarray, q: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{2.0}\NormalTok{, p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{float}\NormalTok{:}
    \CommentTok{"""}
\CommentTok{    Calculates the Morris{-}Mitchell sampling plan quality criterion.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray):}
\CommentTok{            A 2D array representing the sampling plan, where each row is a point in}
\CommentTok{            d{-}dimensional space (shape: (n, d)).}
\CommentTok{        q (float, optional):}
\CommentTok{            Exponent used in the computation of the metric. Defaults to 2.0.}
\CommentTok{        p (float, optional):}
\CommentTok{            The distance norm to use. For example, p=1 is Manhattan (L1),}
\CommentTok{            p=2 is Euclidean (L2). Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        float:}
\CommentTok{            The space{-}fillingness metric Phiq. Larger values typically indicate a more}
\CommentTok{            space{-}filling plan according to the Morris{-}Mitchell criterion.}
\CommentTok{    """}
    \CommentTok{\# Compute the distance multiplicities: J, and unique distances: d}
\NormalTok{    J, d }\OperatorTok{=}\NormalTok{ jd(X, p)}

    \CommentTok{\# Summation of J[i] * d[i]\^{}({-}q), then raised to 1/q}
    \CommentTok{\# This follows the Morris{-}Mitchell definition.}
\NormalTok{    Phiq }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(J }\OperatorTok{*}\NormalTok{ (d }\OperatorTok{**}\NormalTok{ (}\OperatorTok{{-}}\NormalTok{q))) }\OperatorTok{**}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{/}\NormalTok{ q)}
    \ControlFlowTok{return}\NormalTok{ Phiq}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ mmphi}
\CommentTok{\# Two simple sampling plans from above}
\NormalTok{quality1 }\OperatorTok{=}\NormalTok{ mmphi(X1, q}\OperatorTok{=}\DecValTok{2}\NormalTok{, p}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{quality2 }\OperatorTok{=}\NormalTok{ mmphi(X2, q}\OperatorTok{=}\DecValTok{2}\NormalTok{, p}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Quality of sampling plan X1:  }\SpecialCharTok{\{}\NormalTok{quality1}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Quality of sampling plan X2:  }\SpecialCharTok{\{}\NormalTok{quality2}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Quality of sampling plan X1:  2.91547594742265
Quality of sampling plan X2:  3.917162046269215
\end{verbatim}

\paragraph{\texorpdfstring{Die Funktion
\texttt{mmsort(X3D,p)}}{Die Funktion mmsort(X3D,p)}}\label{die-funktion-mmsortx3dp}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mmsort(X3D: np.ndarray, p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Ranks multiple sampling plans stored in a 3D array according to the}
\CommentTok{    Morris{-}Mitchell criterion, using a simple bubble sort.}

\CommentTok{    Args:}
\CommentTok{        X3D (np.ndarray):}
\CommentTok{            A 3D NumPy array of shape (n, d, m), where m is the number of}
\CommentTok{            sampling plans, and each plan is an (n, d) matrix of points.}
\CommentTok{        p (float, optional):}
\CommentTok{            The distance metric to use. p=1 for Manhattan (L1), p=2 for}
\CommentTok{            Euclidean (L2). Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 1D integer array of length m that holds the plan indices in}
\CommentTok{            ascending order of space{-}filling quality. The first index in the}
\CommentTok{            returned array corresponds to the most space{-}filling plan.}

\CommentTok{    Notes:}
\CommentTok{        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:}
\CommentTok{        "This program is free software: you can redistribute it and/or modify  it}
\CommentTok{        under the terms of the GNU Lesser General Public License as published by}
\CommentTok{        the Free Software Foundation, either version 3 of the License, or any}
\CommentTok{        later version.}
\CommentTok{        This program is distributed in the hope that it will be useful, but}
\CommentTok{        WITHOUT ANY WARRANTY; without even the implied warranty of}
\CommentTok{        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser}
\CommentTok{        General Public License for more details.}
\CommentTok{        You should have received a copy of the GNU General Public License and GNU}
\CommentTok{        Lesser General Public License along with this program. If not, see}
\CommentTok{        \textless{}http://www.gnu.org/licenses/\textgreater{}."}
\CommentTok{    """}
    \CommentTok{\# Number of plans (m)}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ X3D.shape[}\DecValTok{2}\NormalTok{]}

    \CommentTok{\# Create index array (1{-}based to match original MATLAB convention)}
\NormalTok{    Index }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, m }\OperatorTok{+} \DecValTok{1}\NormalTok{)}

\NormalTok{    swap\_flag }\OperatorTok{=} \VariableTok{True}
    \ControlFlowTok{while}\NormalTok{ swap\_flag:}
\NormalTok{        swap\_flag }\OperatorTok{=} \VariableTok{False}
\NormalTok{        i }\OperatorTok{=} \DecValTok{0}
        \ControlFlowTok{while}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ m }\OperatorTok{{-}} \DecValTok{1}\NormalTok{:}
            \CommentTok{\# Compare plan at Index[i] vs. Index[i+1] using mm()}
            \CommentTok{\# Note: subtract 1 from each index to convert to 0{-}based array indexing}
            \ControlFlowTok{if}\NormalTok{ mm(X3D[:, :, Index[i] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], X3D[:, :, Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], p) }\OperatorTok{==} \DecValTok{2}\NormalTok{:}
                \CommentTok{\# Swap indices if the second plan is more space{-}filling}
\NormalTok{                Index[i], Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{], Index[i]}
\NormalTok{                swap\_flag }\OperatorTok{=} \VariableTok{True}
\NormalTok{            i }\OperatorTok{+=} \DecValTok{1}

    \ControlFlowTok{return}\NormalTok{ Index}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ mmsort}
\CommentTok{\# Suppose we have two 3{-}point sampling plans X1 and X1 from above}
\NormalTok{X3D }\OperatorTok{=}\NormalTok{ np.stack([X1, X2], axis}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\CommentTok{\# Sort them using the Morris{-}Mitchell criterion with p=2}
\NormalTok{ranking }\OperatorTok{=}\NormalTok{ mmsort(X3D, p}\OperatorTok{=}\FloatTok{2.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(ranking)}
\CommentTok{\# It prints [1, 2] indicating that X1 is more space{-}filling than X2.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1 2]
\end{verbatim}

\subsubsection{\texorpdfstring{Die Funktion
\texttt{perturb(X,PertNum=1)}}{Die Funktion perturb(X,PertNum=1)}}\label{die-funktion-perturbxpertnum1}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ perturb(X: np.ndarray, PertNum: Optional[}\BuiltInTok{int}\NormalTok{] }\OperatorTok{=} \DecValTok{1}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Performs a specified number of random element swaps on a sampling plan.}
\CommentTok{    If the plan is a Latin hypercube, the result remains a valid Latin hypercube.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray):}
\CommentTok{            A 2D array (sampling plan) of shape (n, k), where each row is a point}
\CommentTok{            and each column is a dimension.}
\CommentTok{        PertNum (int, optional):}
\CommentTok{            The number of element swaps (perturbations) to perform. Defaults to 1.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            The perturbed sampling plan, identical in shape to the input, with}
\CommentTok{            one or more random column swaps executed.}
\CommentTok{    """}
    \CommentTok{\# Get dimensions of the plan}
\NormalTok{    n, k }\OperatorTok{=}\NormalTok{ X.shape}
    \ControlFlowTok{if}\NormalTok{ n }\OperatorTok{\textless{}} \DecValTok{2} \KeywordTok{or}\NormalTok{ k }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes require at least 2 points and 2 dimensions"}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(PertNum):}
        \CommentTok{\# Pick a random column}
\NormalTok{        col }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(np.random.rand() }\OperatorTok{*}\NormalTok{ k))}

        \CommentTok{\# Pick two distinct row indices}
\NormalTok{        el1, el2 }\OperatorTok{=} \DecValTok{0}\NormalTok{, }\DecValTok{0}
        \ControlFlowTok{while}\NormalTok{ el1 }\OperatorTok{==}\NormalTok{ el2:}
\NormalTok{            el1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(np.random.rand() }\OperatorTok{*}\NormalTok{ n))}
\NormalTok{            el2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(np.random.rand() }\OperatorTok{*}\NormalTok{ n))}

        \CommentTok{\# Swap the two selected elements in the chosen column}
\NormalTok{        X[el1, col], X[el2, col] }\OperatorTok{=}\NormalTok{ X[el2, col], X[el1, col]}

    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ perturb}
\CommentTok{\# Create a simple 4x2 sampling plan}
\NormalTok{X\_original }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{],[}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{],[}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{],[}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original Sampling Plan:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_original)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Perturbed Sampling Plan:"}\NormalTok{)}
\NormalTok{X\_perturbed }\OperatorTok{=}\NormalTok{ perturb(X\_original, PertNum}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_perturbed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Original Sampling Plan:
[[1 3]
 [2 4]
 [3 1]
 [4 2]]
Perturbed Sampling Plan:
[[2 3]
 [1 4]
 [3 1]
 [4 2]]
\end{verbatim}

\subsubsection{\texorpdfstring{The function
\texttt{mmlhs(X\_start,\ population,\ iterations,\ q=2.0,\ plot=False)}}{The function mmlhs(X\_start, population, iterations, q=2.0, plot=False)}}\label{the-function-mmlhsx_start-population-iterations-q2.0-plotfalse}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mmlhs(X\_start: np.ndarray, population: }\BuiltInTok{int}\NormalTok{, iterations: }\BuiltInTok{int}\NormalTok{, q: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{2.0}\NormalTok{, plot}\OperatorTok{=}\VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Performs an evolutionary search (using perturbations) to find a Morris{-}Mitchell}
\CommentTok{    optimal Latin hypercube, starting from an initial plan X\_start.}

\CommentTok{    This function does the following:}
\CommentTok{      1. Initializes a "best" Latin hypercube (X\_best) from the provided X\_start.}
\CommentTok{      2. Iteratively perturbs X\_best to create offspring.}
\CommentTok{      3. Evaluates the space{-}fillingness of each offspring via the Morris{-}Mitchell}
\CommentTok{         metric (using mmphi).}
\CommentTok{      4. Updates the best plan whenever a better offspring is found.}

\CommentTok{    Args:}
\CommentTok{        X\_start (np.ndarray):}
\CommentTok{            A 2D array of shape (n, k) providing the initial Latin hypercube}
\CommentTok{            (n points in k dimensions).}
\CommentTok{        population (int):}
\CommentTok{            Number of offspring to create in each generation.}
\CommentTok{        iterations (int):}
\CommentTok{            Total number of generations to run the evolutionary search.}
\CommentTok{        q (float, optional):}
\CommentTok{            The exponent used by the Morris{-}Mitchell space{-}filling criterion.}
\CommentTok{            Defaults to 2.0.}
\CommentTok{        plot (bool, optional):}
\CommentTok{            If True, a simple scatter plot of the first two dimensions will be}
\CommentTok{            displayed at each iteration. Only if k \textgreater{}= 2. Defaults to False.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 2D array representing the most space{-}filling Latin hypercube found}
\CommentTok{            after all iterations, of the same shape as X\_start.}
\CommentTok{    """}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X\_start.shape[}\DecValTok{0}\NormalTok{]}
    \ControlFlowTok{if}\NormalTok{ n }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes require at least 2 points"}\NormalTok{)}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X\_start.shape[}\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes are not defined for dim k \textless{} 2"}\NormalTok{)}

    \CommentTok{\# Initialize best plan and its metric}
\NormalTok{    X\_best }\OperatorTok{=}\NormalTok{ X\_start.copy()}
\NormalTok{    Phi\_best }\OperatorTok{=}\NormalTok{ mmphi(X\_best, q}\OperatorTok{=}\NormalTok{q)}

    \CommentTok{\# After 85\% of iterations, reduce the mutation rate to 1}
\NormalTok{    leveloff }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(}\FloatTok{0.85} \OperatorTok{*}\NormalTok{ iterations))}

    \ControlFlowTok{for}\NormalTok{ it }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, iterations }\OperatorTok{+} \DecValTok{1}\NormalTok{):}
        \CommentTok{\# Decrease number of mutations over time}
        \ControlFlowTok{if}\NormalTok{ it }\OperatorTok{\textless{}}\NormalTok{ leveloff:}
\NormalTok{            mutations }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(}\BuiltInTok{round}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\NormalTok{ (}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ (leveloff }\OperatorTok{{-}}\NormalTok{ it) }\OperatorTok{/}\NormalTok{ (leveloff }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)))}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            mutations }\OperatorTok{=} \DecValTok{1}

\NormalTok{        X\_improved }\OperatorTok{=}\NormalTok{ X\_best.copy()}
\NormalTok{        Phi\_improved }\OperatorTok{=}\NormalTok{ Phi\_best}

        \CommentTok{\# Create offspring, evaluate, and keep the best}
        \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(population):}
\NormalTok{            X\_try }\OperatorTok{=}\NormalTok{ perturb(X\_best.copy(), mutations)}
\NormalTok{            Phi\_try }\OperatorTok{=}\NormalTok{ mmphi(X\_try, q}\OperatorTok{=}\NormalTok{q)}

            \ControlFlowTok{if}\NormalTok{ Phi\_try }\OperatorTok{\textless{}}\NormalTok{ Phi\_improved:}
\NormalTok{                X\_improved }\OperatorTok{=}\NormalTok{ X\_try}
\NormalTok{                Phi\_improved }\OperatorTok{=}\NormalTok{ Phi\_try}

        \CommentTok{\# Update the global best if we found a better plan}
        \ControlFlowTok{if}\NormalTok{ Phi\_improved }\OperatorTok{\textless{}}\NormalTok{ Phi\_best:}
\NormalTok{            X\_best }\OperatorTok{=}\NormalTok{ X\_improved}
\NormalTok{            Phi\_best }\OperatorTok{=}\NormalTok{ Phi\_improved}

        \CommentTok{\# Simple visualization of the first two dimensions}
        \ControlFlowTok{if}\NormalTok{ plot }\KeywordTok{and}\NormalTok{ (X\_best.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{\textgreater{}=} \DecValTok{2}\NormalTok{):}
\NormalTok{            plt.clf()}
\NormalTok{            plt.scatter(X\_best[:, }\DecValTok{0}\NormalTok{], X\_best[:, }\DecValTok{1}\NormalTok{], marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{)}
\NormalTok{            plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{            plt.title(}\SpecialStringTok{f"Iteration }\SpecialCharTok{\{}\NormalTok{it}\SpecialCharTok{\}}\SpecialStringTok{ {-} Current Best Plan"}\NormalTok{)}
\NormalTok{            plt.pause(}\FloatTok{0.01}\NormalTok{)}

    \ControlFlowTok{return}\NormalTok{ X\_best}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ mmlhs}
\CommentTok{\# Suppose we have an initial 4x2 plan}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.3}\NormalTok{],[}\FloatTok{.1}\NormalTok{, }\FloatTok{.4}\NormalTok{],[}\FloatTok{.2}\NormalTok{, }\FloatTok{.9}\NormalTok{],[}\FloatTok{.9}\NormalTok{, }\FloatTok{.2}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Initial plan:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_start)}
\CommentTok{\# Search for a more space{-}filling plan}
\NormalTok{X\_opt }\OperatorTok{=}\NormalTok{ mmlhs(X\_start, population}\OperatorTok{=}\DecValTok{10}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{100}\NormalTok{, q}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimized plan:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_opt)}
\CommentTok{\# Plot the initial and optimized plans}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.scatter(X\_start[:, }\DecValTok{0}\NormalTok{], X\_start[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Initial Plan\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.scatter(X\_opt[:, }\DecValTok{0}\NormalTok{], X\_opt[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Optimized Plan\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Comparison of Initial and Optimized Plans\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Dimension 1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Dimension 2\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\CommentTok{\# plot legend outside the plot}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper left\textquotesingle{}}\NormalTok{, bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{plt.xlim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Initial plan:
[[0.1 0.3]
 [0.1 0.4]
 [0.2 0.9]
 [0.9 0.2]]
Optimized plan:
[[0.1 0.9]
 [0.2 0.4]
 [0.1 0.2]
 [0.9 0.3]]
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{001_optimization_surrogate_files/figure-pdf/cell-24-output-2.pdf}}

\subsubsection{\texorpdfstring{Die Funktion
\texttt{bestlh()}}{Die Funktion bestlh()}}\label{die-funktion-bestlh}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ bestlh(n: }\BuiltInTok{int}\NormalTok{, k: }\BuiltInTok{int}\NormalTok{, population: }\BuiltInTok{int}\NormalTok{, iterations: }\BuiltInTok{int}\NormalTok{, p}\OperatorTok{=}\DecValTok{1}\NormalTok{, plot}\OperatorTok{=}\VariableTok{False}\NormalTok{, verbosity}\OperatorTok{=}\DecValTok{0}\NormalTok{, edges}\OperatorTok{=}\DecValTok{0}\NormalTok{, q\_list}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{]) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Generates an optimized Latin hypercube by evolving the Morris{-}Mitchell}
\CommentTok{    criterion across multiple exponents (q values) and selecting the best plan.}

\CommentTok{    Args:}
\CommentTok{        n (int):}
\CommentTok{            Number of points required in the Latin hypercube.}
\CommentTok{        k (int):}
\CommentTok{            Number of design variables (dimensions).}
\CommentTok{        population (int):}
\CommentTok{            Number of offspring in each generation of the evolutionary search.}
\CommentTok{        iterations (int):}
\CommentTok{            Number of generations for the evolutionary search.}
\CommentTok{        p (int, optional):}
\CommentTok{            The distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2).}
\CommentTok{            Defaults to 1 (faster than 2).}
\CommentTok{        plot (bool, optional):}
\CommentTok{            If True, a scatter plot of the optimized plan in the first two dimensions}
\CommentTok{            will be displayed. Only if k\textgreater{}=2.  Defaults to False.}
\CommentTok{        verbosity (int, optional):}
\CommentTok{            Verbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.}
\CommentTok{        edges (int, optional):}
\CommentTok{            If 1, places centers of the extreme bins at the domain edges ([0,1]).}
\CommentTok{            Otherwise, bins are fully contained within the domain, i.e. midpoints.}
\CommentTok{            Defaults to 0.}
\CommentTok{        q\_list (list, optional):}
\CommentTok{            A list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100].}
\CommentTok{            These values are used to evaluate the space{-}fillingness of the Latin}
\CommentTok{            hypercube. The best plan is selected based on the lowest mmphi value.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 2D array of shape (n, k) representing an optimized Latin hypercube.}

\CommentTok{    Notes:}
\CommentTok{        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:}
\CommentTok{        "This program is free software: you can redistribute it and/or modify  it}
\CommentTok{        under the terms of the GNU Lesser General Public License as published by}
\CommentTok{        the Free Software Foundation, either version 3 of the License, or any}
\CommentTok{        later version.}
\CommentTok{        This program is distributed in the hope that it will be useful, but}
\CommentTok{        WITHOUT ANY WARRANTY; without even the implied warranty of}
\CommentTok{        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser}
\CommentTok{        General Public License for more details.}
\CommentTok{        You should have received a copy of the GNU General Public License and GNU}
\CommentTok{        Lesser General Public License along with this program. If not, see}
\CommentTok{        \textless{}http://www.gnu.org/licenses/\textgreater{}."}
\CommentTok{    """}
    \ControlFlowTok{if}\NormalTok{ n }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes require at least 2 points"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes are not defined for dim k \textless{} 2"}\NormalTok{)}

    \CommentTok{\# A list of exponents (q) to optimize}

    \CommentTok{\# Start with a random Latin hypercube}
\NormalTok{    X\_start }\OperatorTok{=}\NormalTok{ rlh(n, k, edges}\OperatorTok{=}\NormalTok{edges)}

    \CommentTok{\# Allocate a 3D array to store the results for each q}
    \CommentTok{\# (shape: (n, k, number\_of\_q\_values))}
\NormalTok{    X3D }\OperatorTok{=}\NormalTok{ np.zeros((n, k, }\BuiltInTok{len}\NormalTok{(q\_list)))}

    \CommentTok{\# Evolve the plan for each q in q\_list}
    \ControlFlowTok{for}\NormalTok{ i, q\_val }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(q\_list):}
        \ControlFlowTok{if}\NormalTok{ verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Now optimizing for q=}\SpecialCharTok{\{}\NormalTok{q\_val}\SpecialCharTok{\}}\SpecialStringTok{..."}\NormalTok{)}
\NormalTok{        X3D[:, :, i] }\OperatorTok{=}\NormalTok{ mmlhs(X\_start, population, iterations, q\_val)}

    \CommentTok{\# Sort the set of evolved plans according to the Morris{-}Mitchell criterion}
\NormalTok{    index\_order }\OperatorTok{=}\NormalTok{ mmsort(X3D, p}\OperatorTok{=}\NormalTok{p)}

    \CommentTok{\# index\_order is a 1{-}based array of plan indices; the first element is the best}
\NormalTok{    best\_idx }\OperatorTok{=}\NormalTok{ index\_order[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
    \ControlFlowTok{if}\NormalTok{ verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best lh found using q=}\SpecialCharTok{\{}\NormalTok{q\_list[best\_idx]}\SpecialCharTok{\}}\SpecialStringTok{..."}\NormalTok{)}

    \CommentTok{\# The best plan in 3D array order}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ X3D[:, :, best\_idx]}

    \CommentTok{\# Plot the first two dimensions}
    \ControlFlowTok{if}\NormalTok{ plot }\KeywordTok{and}\NormalTok{ (k }\OperatorTok{\textgreater{}=} \DecValTok{2}\NormalTok{):}
\NormalTok{        plt.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\StringTok{"r"}\NormalTok{, marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{)}
\NormalTok{        plt.title(}\SpecialStringTok{f"Morris{-}Mitchell optimum plan found using q=}\SpecialCharTok{\{}\NormalTok{q\_list[best\_idx]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{        plt.xlabel(}\StringTok{"x\_1"}\NormalTok{)}
\NormalTok{        plt.ylabel(}\StringTok{"x\_2"}\NormalTok{)}
\NormalTok{        plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{        plt.show()}

    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ bestlh}
\NormalTok{Xbestlh}\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{5}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\CommentTok{\# plot the bestlh}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.scatter(Xbestlh[:, }\DecValTok{0}\NormalTok{], Xbestlh[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Best Latin Hypercube\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Best Latin Hypercube Sampling\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Dimension 1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Dimension 2\textquotesingle{}}\NormalTok{)}
\CommentTok{\# plot legend outside the plot}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper left\textquotesingle{}}\NormalTok{, bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{plt.xlim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{001_optimization_surrogate_files/figure-pdf/cell-26-output-1.pdf}}

\subsubsection{\texorpdfstring{Die Funktion
\texttt{phisort(X3D,q,p)}}{Die Funktion phisort(X3D,q,p)}}\label{die-funktion-phisortx3dqp}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ phisort(X3D: np.ndarray, q: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{2.0}\NormalTok{, p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Ranks multiple sampling plans stored in a 3D array by the Morris{-}Mitchell}
\CommentTok{    numerical quality metric (mmphi). Uses a simple bubble{-}sort:}
\CommentTok{    sampling plans with smaller mmphi values are placed first in the index array.}

\CommentTok{    Args:}
\CommentTok{        X3D (np.ndarray):}
\CommentTok{            A 3D array of shape (n, d, m), where m is the number of sampling plans.}
\CommentTok{        q (float, optional):}
\CommentTok{            Exponent for the mmphi metric. Defaults to 2.0.}
\CommentTok{        p (float, optional):}
\CommentTok{            Distance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 1D integer array of length m, giving the plan indices in ascending}
\CommentTok{            order of mmphi. The first index in the returned array corresponds}
\CommentTok{            to the numerically lowest mmphi value.}
\CommentTok{    """}
    \CommentTok{\# Number of 2D sampling plans}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ X3D.shape[}\DecValTok{2}\NormalTok{]}

    \CommentTok{\# Create a 1{-}based index array}
\NormalTok{    Index }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, m }\OperatorTok{+} \DecValTok{1}\NormalTok{)}

    \CommentTok{\# Bubble{-}sort: plan with lower mmphi() climbs toward the front}
\NormalTok{    swap\_flag }\OperatorTok{=} \VariableTok{True}
    \ControlFlowTok{while}\NormalTok{ swap\_flag:}
\NormalTok{        swap\_flag }\OperatorTok{=} \VariableTok{False}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(m }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
            \CommentTok{\# Retrieve mmphi values for consecutive plans}
\NormalTok{            val\_i }\OperatorTok{=}\NormalTok{ mmphi(X3D[:, :, Index[i] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p)}
\NormalTok{            val\_j }\OperatorTok{=}\NormalTok{ mmphi(X3D[:, :, Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p)}

            \CommentTok{\# Swap if the left plan\textquotesingle{}s mmphi is larger (i.e. \textquotesingle{}worse\textquotesingle{})}
            \ControlFlowTok{if}\NormalTok{ val\_i }\OperatorTok{\textgreater{}}\NormalTok{ val\_j:}
\NormalTok{                Index[i], Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{], Index[i]}
\NormalTok{                swap\_flag }\OperatorTok{=} \VariableTok{True}

    \ControlFlowTok{return}\NormalTok{ Index}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ phisort}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{5}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{15}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{25}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{30}\NormalTok{)}
\CommentTok{\# Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]}
\NormalTok{X3D }\OperatorTok{=}\NormalTok{ np.array([X1, X2])}
\BuiltInTok{print}\NormalTok{(phisort(X3D))}
\NormalTok{X3D }\OperatorTok{=}\NormalTok{ np.array([X3, X2])}
\BuiltInTok{print}\NormalTok{(phisort(X3D))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1 2]
[2 1]
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Goals}]

\begin{itemize}
\tightlist
\item
  How to choose models and optimizers for solving real-world problems
\item
  How to use simulation to understand and improve processes
\end{itemize}

\end{tcolorbox}

\section{Jupyter Notebook}\label{jupyter-notebook}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/001_optimization_surrogate.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Aircraft Wing Weight
Example}\label{aircraft-wing-weight-example}

\section{AWWE Equation}\label{awwe-equation}

\begin{itemize}
\tightlist
\item
  Example from Forrester et al.~
\item
  Understand the \textbf{weight} of an unpainted light aircraft wing as
  a function of nine design and operational parameters:
\end{itemize}

\[ W = 0.036 S_W^{0.758} \times W_{fw}^{0.0035} \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times  q^{0.006}  \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49}\]

\section{AWWE Parameters and Equations (Part
1)}\label{awwe-parameters-and-equations-part-1}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1392}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.5063}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1266}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1139}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1139}}@{}}
\caption{Aircraft Wing Weight Parameters}\label{tbl-awwe}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(S_W\) & Wing area (\(ft^2\)) & 174 & 150 & 200 \\
\(W_{fw}\) & Weight of fuel in wing (lb) & 252 & 220 & 300 \\
\(A\) & Aspect ratio & 7.52 & 6 & 10 \\
\(\Lambda\) & Quarter-chord sweep (deg) & 0 & -10 & 10 \\
\(q\) & Dynamic pressure at cruise (\(lb/ft^2\)) & 34 & 16 & 45 \\
\(\lambda\) & Taper ratio & 0.672 & 0.5 & 1 \\
\(R_{tc}\) & Aerofoil thickness to chord ratio & 0.12 & 0.08 & 0.18 \\
\(N_z\) & Ultimate load factor & 3.8 & 2.5 & 6 \\
\(W_{dg}\) & Flight design gross weight (lb) & 2000 & 1700 & 2500 \\
\(W_p\) & paint weight (lb/ft\^{}2) & 0.064 & 0.025 & 0.08 \\
\end{longtable}

The study begins with a baseline Cessna C172 Skyhawk Aircraft as its
reference point. It aims to investigate the impact of wing area and fuel
weight on the overall weight of the aircraft. Two crucial parameters in
this analysis are the aspect ratio (\(A\)), defined as the ratio of the
wing's length to the average chord (thickness of the airfoil), and the
taper ratio (\(\lambda\)), which represents the ratio of the maximum to
the minimum thickness of the airfoil or the maximum to minimum chord.

It's important to note that the equation used in this context is not a
computer simulation but will be treated as one for the purpose of
illustration. This approach involves employing a true mathematical
equation, even if it's considered unknown, as a useful tool for
generating realistic settings to test the methodology. The functional
form of this equation was derived by ``calibrating'' known physical
relationships to curves obtained from existing aircraft data, as
referenced in Raymer 2012. Essentially, it acts as a surrogate for
actual measurements of aircraft weight.

Examining the mathematical properties of the AWWE (Aircraft Weight With
Wing Area and Fuel Weight Equation), it is evident that the response is
highly nonlinear concerning its inputs. While it's common to apply the
logarithm to simplify equations with complex exponents, even when
modeling the logarithm, which transforms powers into slope coefficients
and products into sums, the response remains nonlinear due to the
presence of trigonometric terms. Given the combination of nonlinearity
and high input dimension, simple linear and quadratic response surface
approximations are likely to be inadequate for this analysis.

\section{Goals: Understanding and
Optimization}\label{goals-understanding-and-optimization}

The primary goals of this study revolve around understanding and
optimization:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Understanding}: One of the straightforward objectives is to
  gain a deep understanding of the input-output relationships in this
  context. Given the global perspective implied by this setting, it
  becomes evident that a more sophisticated model is almost necessary.
  At this stage, let's focus on this specific scenario to establish a
  clear understanding.
\item
  \textbf{Optimization}: Another application of this analysis could be
  optimization. There may be an interest in minimizing the weight of the
  aircraft, but it's likely that there will be constraints in place. For
  example, the presence of wings with a nonzero area is essential for
  the aircraft to be capable of flying. In situations involving
  (constrained) optimization, a global perspective and, consequently,
  the use of flexible modeling are vital.
\end{enumerate}

The provided Python code serves as a genuine computer implementation
that ``solves'' a mathematical model. It accepts arguments encoded in
the unit cube, with defaults used to represent baseline settings, as
detailed in the table labeled as Table~\ref{tbl-awwe}. To map values
from the interval \([a, b]\) to the interval \([0, 1]\), the following
formula can be employed:

\[y = f(x) = \frac{x - a}{b - a}.\]

To reverse this mapping and obtain the original values, the formula
\[g(y) = a + (b - a) y\] can be used.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ wingwt(Sw}\OperatorTok{=}\FloatTok{0.48}\NormalTok{, Wfw}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, A}\OperatorTok{=}\FloatTok{0.38}\NormalTok{, L}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, q}\OperatorTok{=}\FloatTok{0.62}\NormalTok{, l}\OperatorTok{=}\FloatTok{0.344}\NormalTok{,  Rtc}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, Nz}\OperatorTok{=}\FloatTok{0.37}\NormalTok{, Wdg}\OperatorTok{=}\FloatTok{0.38}\NormalTok{):}
    \CommentTok{\# put coded inputs back on natural scale}
\NormalTok{    Sw }\OperatorTok{=}\NormalTok{ Sw }\OperatorTok{*}\NormalTok{ (}\DecValTok{200} \OperatorTok{{-}} \DecValTok{150}\NormalTok{) }\OperatorTok{+} \DecValTok{150} 
\NormalTok{    Wfw }\OperatorTok{=}\NormalTok{ Wfw }\OperatorTok{*}\NormalTok{ (}\DecValTok{300} \OperatorTok{{-}} \DecValTok{220}\NormalTok{) }\OperatorTok{+} \DecValTok{220} 
\NormalTok{    A }\OperatorTok{=}\NormalTok{ A }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}} \DecValTok{6}\NormalTok{) }\OperatorTok{+} \DecValTok{6} 
\NormalTok{    L }\OperatorTok{=}\NormalTok{ (L }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}}\NormalTok{ (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{)) }\OperatorTok{{-}} \DecValTok{10}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.pi}\OperatorTok{/}\DecValTok{180}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ q }\OperatorTok{*}\NormalTok{ (}\DecValTok{45} \OperatorTok{{-}} \DecValTok{16}\NormalTok{) }\OperatorTok{+} \DecValTok{16} 
\NormalTok{    l }\OperatorTok{=}\NormalTok{ l }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+} \FloatTok{0.5}  
\NormalTok{    Rtc }\OperatorTok{=}\NormalTok{ Rtc }\OperatorTok{*}\NormalTok{ (}\FloatTok{0.18} \OperatorTok{{-}} \FloatTok{0.08}\NormalTok{) }\OperatorTok{+} \FloatTok{0.08}
\NormalTok{    Nz }\OperatorTok{=}\NormalTok{ Nz }\OperatorTok{*}\NormalTok{ (}\DecValTok{6} \OperatorTok{{-}} \FloatTok{2.5}\NormalTok{) }\OperatorTok{+} \FloatTok{2.5}
\NormalTok{    Wdg }\OperatorTok{=}\NormalTok{ Wdg}\OperatorTok{*}\NormalTok{(}\DecValTok{2500} \OperatorTok{{-}} \DecValTok{1700}\NormalTok{) }\OperatorTok{+} \DecValTok{1700}
    \CommentTok{\# calculation on natural scale}
\NormalTok{    W }\OperatorTok{=} \FloatTok{0.036} \OperatorTok{*}\NormalTok{ Sw}\OperatorTok{**}\FloatTok{0.758} \OperatorTok{*}\NormalTok{ Wfw}\OperatorTok{**}\FloatTok{0.0035} \OperatorTok{*}\NormalTok{ (A}\OperatorTok{/}\NormalTok{np.cos(L)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\FloatTok{0.6} \OperatorTok{*}\NormalTok{ q}\OperatorTok{**}\FloatTok{0.006} 
\NormalTok{    W }\OperatorTok{=}\NormalTok{ W }\OperatorTok{*}\NormalTok{ l}\OperatorTok{**}\FloatTok{0.04} \OperatorTok{*}\NormalTok{ (}\DecValTok{100}\OperatorTok{*}\NormalTok{Rtc}\OperatorTok{/}\NormalTok{np.cos(L))}\OperatorTok{**}\NormalTok{(}\OperatorTok{{-}}\FloatTok{0.3}\NormalTok{) }\OperatorTok{*}\NormalTok{ (Nz}\OperatorTok{*}\NormalTok{Wdg)}\OperatorTok{**}\NormalTok{(}\FloatTok{0.49}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{(W)}
\end{Highlighting}
\end{Shaded}

\section{Properties of the Python
``Solver''}\label{properties-of-the-python-solver}

The compute time required by the ``wingwt'' solver is extremely short
and can be considered trivial in terms of computational resources. The
approximation error is exceptionally small, effectively approaching
machine precision, which indicates the high accuracy of the solver's
results.

To simulate time-consuming evaluations, a deliberate delay is introduced
by incorporating a \texttt{sleep(3600)} command, which effectively
synthesizes a one-hour execution time for a particular evaluation.

Moving on to the AWWE visualization, plotting in two dimensions is
considerably simpler than dealing with nine dimensions. To aid in
creating visual representations, the code provided below establishes a
grid within the unit square to facilitate the generation of sliced
visuals. This involves generating a ``meshgrid'' as outlined in the
code.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zp }\OperatorTok{=} \BuiltInTok{zip}\NormalTok{(np.ravel(X), np.ravel(Y))}
\BuiltInTok{list}\NormalTok{(zp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[(np.float64(0.0), np.float64(0.0)),
 (np.float64(0.5), np.float64(0.0)),
 (np.float64(1.0), np.float64(0.0)),
 (np.float64(0.0), np.float64(0.5)),
 (np.float64(0.5), np.float64(0.5)),
 (np.float64(1.0), np.float64(0.5)),
 (np.float64(0.0), np.float64(1.0)),
 (np.float64(0.5), np.float64(1.0)),
 (np.float64(1.0), np.float64(1.0))]
\end{verbatim}

The coding used to transform inputs from natural units is largely a
matter of taste, so long as it's easy to undo for reporting back on
original scales

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{matplotlib inline}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\CommentTok{\# plt.style.use(\textquotesingle{}seaborn{-}white\textquotesingle{})}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Plot 1: Load Factor (\(N_z\)) and Aspect Ratio
(\(A\))}{Plot 1: Load Factor (N\_z) and Aspect Ratio (A)}}\label{plot-1-load-factor-n_z-and-aspect-ratio-a}

We will vary \(N_z\) and \(A\), with other inputs fixed at their
baseline values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(A }\OperatorTok{=}\NormalTok{ X, Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{7.}\NormalTok{, }\FloatTok{5.}\NormalTok{))}
\NormalTok{plt.contourf(X, Y, z, }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Load factor (Nz) vs. Aspect Ratio (A)"}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-5-output-1.pdf}}

Contour plots can be refined, e.g., by adding explicit contour lines as
shown in the following figure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-6-output-1.pdf}}

The interpretation of the AWWE plot can be summarized as follows:

\begin{itemize}
\tightlist
\item
  The figure displays the weight response as a function of two
  variables, \(N_z\) and \(A\), using an image-contour plot.
\item
  The slight curvature observed in the contours suggests an interaction
  between these two variables.
\item
  Notably, the range of outputs depicted in the figure, spanning from
  approximately 160 to 320, nearly encompasses the entire range of
  outputs observed from various input settings within the full
  9-dimensional input space.
\item
  The plot indicates that aircraft wings tend to be heavier when the
  aspect ratios (\(A\)) are high.
\item
  This observation aligns with the idea that wings are designed to
  withstand and accommodate high gravitational forces (\(g\)-forces,
  large \(N_z\)), and there may be a compounding effect where larger
  values of \(N_z\) contribute to increased wing weight.
\item
  It's plausible that this phenomenon is related to the design
  considerations of fighter jets, which cannot have the efficient and
  lightweight glider-like wings typically found in other types of
  aircraft.
\end{itemize}

\section{Plot 2: Taper Ratio and Fuel
Weight}\label{plot-2-taper-ratio-and-fuel-weight}

\begin{itemize}
\tightlist
\item
  The same experiment for two other inputs, e.g., taper ratio
  \(\lambda\) and fuel weight \(W_{fw}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(Wfw }\OperatorTok{=}\NormalTok{ X,  Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"WfW"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"l"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-7-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Interpretation of Taper Ratio (\(l\)) and Fuel Weight (\(W_{fw}\))

  \begin{itemize}
  \tightlist
  \item
    Apparently, neither input has much effect on wing weight:

    \begin{itemize}
    \tightlist
    \item
      with \(\lambda\) having a marginally greater effect, covering less
      than 4 percent of the span of weights observed in the
      \(A \times N_z\) plane
    \end{itemize}
  \item
    There's no interaction evident in \(\lambda \times W_{fw}\)
  \end{itemize}
\end{itemize}

\section{The Big Picture: Combining all
Variables}\label{the-big-picture-combining-all-variables}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pl }\OperatorTok{=}\NormalTok{ [}\StringTok{"Sw"}\NormalTok{, }\StringTok{"Wfw"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"q"}\NormalTok{, }\StringTok{"l"}\NormalTok{,  }\StringTok{"Rtc"}\NormalTok{, }\StringTok{"Nz"}\NormalTok{, }\StringTok{"Wdg"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\NormalTok{Z }\OperatorTok{=}\NormalTok{ []}
\NormalTok{Zlab }\OperatorTok{=}\NormalTok{ []}
\NormalTok{l }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(pl)}
\CommentTok{\# lc = math.comb(l,2)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(l):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, l):}
    \CommentTok{\# for j in range(l):}
        \CommentTok{\# print(pl[i], pl[j])}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ \{pl[i]: X, pl[j]: Y\}}
\NormalTok{        Z.append(wingwt(}\OperatorTok{**}\NormalTok{d))}
\NormalTok{        Zlab.append([pl[i],pl[j]])}
\end{Highlighting}
\end{Shaded}

Now we can generate all 36 combinations, e.g., our first example is
combination \texttt{p\ =\ 19}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{19}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['A', 'Nz']
\end{verbatim}

To help interpret outputs from experiments such as this one---to level
the playing field when comparing outputs from other pairs of
inputs---code below sets up a color palette that can be re-used from one
experiment to the next. We use the arguments \texttt{vmin=180} and
\texttt{vmax\ =360} to implement comparibility

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-11-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Let's plot the second example, taper ratio \(\lambda\) and fuel weight
  \(W_{fw}\)
\item
  This is combination \texttt{11}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{11}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['Wfw', 'l']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-13-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Using a global colormap indicates that these variables have minor
  effects on the wing weight.
\item
  Important factors can be detected by visual inspection
\item
  Plotting the Big Picture: we can plot all 36 combinations in one
  figure.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.axes\_grid1 }\ImportTok{import}\NormalTok{ ImageGrid}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{20.}\NormalTok{, }\FloatTok{20.}\NormalTok{))}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ ImageGrid(fig, }\DecValTok{111}\NormalTok{,  }\CommentTok{\# similar to subplot(111)}
\NormalTok{                 nrows\_ncols}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{),  }\CommentTok{\# creates 2x2 grid of axes}
\NormalTok{                 axes\_pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,  }\CommentTok{\# pad between axes in inch.}
\NormalTok{                 share\_all}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                 label\_mode}\OperatorTok{=}\StringTok{"all"}\NormalTok{,}
\NormalTok{                 ) }
\NormalTok{i }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ ax, im }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(grid, Z):}
    \CommentTok{\# Iterating over the grid returns the Axes.}
\NormalTok{    ax.set\_xlabel(Zlab[i][}\DecValTok{0}\NormalTok{])}
\NormalTok{    ax.set\_ylabel(Zlab[i][}\DecValTok{1}\NormalTok{])}
    \CommentTok{\# ax.set\_title(Zlab[i][1] + " vs. " + Zlab[i][0])}
\NormalTok{    ax.contourf(X, Y, im, }\DecValTok{30}\NormalTok{, cmap }\OperatorTok{=} \StringTok{"jet"}\NormalTok{,  vmin }\OperatorTok{=} \DecValTok{180}\NormalTok{, vmax }\OperatorTok{=} \DecValTok{360}\NormalTok{)}
\NormalTok{    i }\OperatorTok{=}\NormalTok{ i }\OperatorTok{+} \DecValTok{1}
       
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-14-output-1.pdf}}

\section{AWWE Landscape}\label{awwe-landscape}

\begin{itemize}
\tightlist
\item
  Our Observations

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The load factor \(N_z\), which determines the magnitude of the
    maximum aerodynamic load on the wing, is very active and involved in
    interactions with other variables.
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Classic example: the interaction of \(N_z\) with the aspect ratio
    \(A\) indicates a heavy wing for high aspect ratios and large
    \(g\)-forces
  \item
    This is the reaon why highly manoeuvrable fighter jets cannot have
    very efficient, glider wings)
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Aspect ratio \(A\) and airfoil thickness to chord ratio \(R_{tc}\)
    have nonlinear interactions.
  \item
    Most important variables:
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Ultimate load factor \(N_z\), wing area \(S_w\), and flight design
    gross weight\(W_{dg}\).
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Little impact: dynamic pressure \(q\), taper ratio \(l\), and
    quarter-chord sweep \(L\).
  \end{enumerate}
\item
  Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Aircraft designers know that the overall weight of the aircraft and
    the wing area must be kept to a minimum
  \item
    the latter usually dictated by constraints such as required stall
    speed, landing distance, turn rate, etc.
  \end{itemize}
\end{itemize}

\section{Summary of the First
Experiments}\label{summary-of-the-first-experiments}

\begin{itemize}
\tightlist
\item
  First, we considered two pairs of inputs, out of 36 total pairs
\item
  Then, the ``Big Picture'':

  \begin{itemize}
  \tightlist
  \item
    For each pair we evaluated \texttt{wingwt} 10,000 times
  \end{itemize}
\item
  Doing the same for all pairs would require 360K evaluations:

  \begin{itemize}
  \tightlist
  \item
    not a reasonable number with a real computer simulation that takes
    any non-trivial amount of time to evaluate
  \item
    Only 1s per evaluation: \(>100\) hours
  \end{itemize}
\item
  Many solvers take minutes/hours/days to execute a single run
\item
  And: three-way interactions?
\item
  Consequence: a different strategy is needed
\end{itemize}

\section{Exercise}\label{exercise}

\subsection{Adding Paint Weight}\label{adding-paint-weight}

\begin{itemize}
\tightlist
\item
  Paint weight is not considered.
\item
  Add Paint Weight \(W_p\) to formula (the updated formula is shown
  below) and update the functions and plots in the notebook.
\end{itemize}

\[ W = 0.036S_W^{0.758} \times W_{fw}^{0.0035} \times \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times q^{0.006} \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49} + S_w W_p\]

\section{Jupyter Notebook}\label{jupyter-notebook-1}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/002_awwe.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{\texorpdfstring{Introduction to
\texttt{scipy.optimize}}{Introduction to scipy.optimize}}\label{introduction-to-scipy.optimize}

\href{https://scipy.org}{SciPy} provides algorithms for optimization,
integration, interpolation, eigenvalue problems, algebraic equations,
differential equations, statistics and many other classes of problems.
SciPy is a collection of mathematical algorithms and convenience
functions built on NumPy. It adds significant power to Python by
providing the user with high-level commands and classes for manipulating
and visualizing data.

\href{https://docs.scipy.org/doc/scipy/reference/optimize.html\#module-scipy.optimize}{SciPy
optimize} provides functions for minimizing (or maximizing) objective
functions, possibly subject to constraints. It includes solvers for
nonlinear problems (with support for both local and global optimization
algorithms), linear programing, constrained and nonlinear least-squares,
root finding, and curve fitting.

In this notebook, we will learn how to use the \texttt{scipy.optimize}
module to solve optimization problems. See:
\url{https://docs.scipy.org/doc/scipy/tutorial/optimize.html}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  This content is based on information from the scipy.optimize package.
\item
  The \texttt{scipy.optimize} package provides several commonly used
  optimization algorithms. A detailed listing is available in
  \texttt{scipy.optimize} (can also be found by
  \texttt{help(scipy.optimize)}).
\end{itemize}

\end{tcolorbox}

Common functions and objects, shared across different SciPy optimize
solvers, are shown in Table~\ref{tbl-shared-functions}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\caption{Common functions and objects, shared across different SciPy
optimize solvers}\label{tbl-shared-functions}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function or Object
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function or Object
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html\#scipy.optimize.show_options}{show\_options({[}solver,
method, disp{]})} & Show documentation for additional options of
optimization solvers. \\
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html\#scipy.optimize.OptimizeResult}{OptimizeResult}
& Represents the optimization result. \\
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeWarning.html\#scipy.optimize.OptimizeWarning}{OptimizeWarning}
& Warning issued by solvers. \\
\end{longtable}

We will introduce unconstrained minimization of multivariate scalar
functions in this chapter. The
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\#scipy.optimize.minimize}{minimize}
function provides a common interface to unconstrained and constrained
minimization algorithms for multivariate scalar functions in
\texttt{scipy.optimize}. To demonstrate the minimization function,
consider the problem of minimizing the Rosenbrock function of \emph{N}
variables:

\[
f(J) = \sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2)^2 + (1 - x_i)^2
\]

The minimum value of this function is 0, which is achieved when (x\_i =
1).

Note that the Rosenbrock function and its derivatives are included in
\texttt{scipy.optimize}. The implementations shown in the following
sections provide examples of how to define an objective function as well
as its Jacobian and Hessian functions. Objective functions in
\texttt{scipy.optimize} expect a numpy array as their first parameter,
which is to be optimized and must return a float value. The exact
calling signature must be \texttt{f(x,\ *args)}, where \texttt{x}
represents a numpy array, and \texttt{args} is a tuple of additional
arguments supplied to the objective function.

\section{Derivative-free Optimization
Algorithms}\label{derivative-free-optimization-algorithms}

Section~\ref{sec-nelder-mead-simplex-algorithm} and
Section~\ref{sec-powells-method} present two approaches that do not need
gradient information to find the minimum. They use function evaluations
to find the minimum.

\subsection{Nelder-Mead Simplex
Algorithm}\label{sec-nelder-mead-simplex-algorithm}

The Nelder Mead is a simple local optimization algorithm. It requires
only function evaluations and is a good choice for simple minimization
problems. However, because it does not use any gradient evaluations, it
may take longer to find the minimum. It can be devided into the
following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize the simplex
\item
  Evaluate the function at each vertex of the simplex
\item
  Order the vertices by function value
\item
  \textbf{Reflect} the worst point through the centroid of the remaining
  points
\item
  If the reflected point is better than the second worst, replace the
  worst point with the reflected point
\item
  If the reflected point is worse than the worst point, try
  \textbf{contracting} the simplex
\item
  If the reflected point is better than the best point, try
  \textbf{expanding} the simplex
\item
  If none of the above steps improve the simplex, \textbf{shrink} the
  simplex towards the best point
\item
  Check for convergence
\end{enumerate}

\texttt{method=\textquotesingle{}Nelder-Mead\textquotesingle{}}: In the
example below, the \texttt{minimize} routine is used with the
\emph{Nelder-Mead} simplex algorithm (selected through the
\texttt{method} parameter):

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\KeywordTok{def}\NormalTok{ rosen(x):}
    \CommentTok{"""The Rosenbrock function"""}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(}\FloatTok{100.0} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{, }\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 339
         Function evaluations: 571
[1. 1. 1. 1. 1.]
\end{verbatim}

The simplex algorithm is probably the simplest way to minimize a
well-behaved function. It requires only function evaluations and is a
good choice for simple minimization problems. However, because it does
not use any gradient evaluations, it may take longer to find the
minimum.

\subsection{Powell's Method}\label{sec-powells-method}

Another optimization algorithm that needs only function calls to find
the minimum is \emph{Powell}'s method, which can be selected by setting
the \texttt{method} parameter to
\texttt{\textquotesingle{}powell\textquotesingle{}} in the
\texttt{minimize} function. This algorithm consists of a conjugate
direction method. It performs sequential one-dimensional minimizations
along each vector of the directions set, which is updated at each
iteration of the main minimization loop. It can be described by the
following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialization
\item
  Minimization along each direction
\item
  Create conjugate direction
\item
  Line search along the conjugate direction
\item
  Check for convergence
\end{enumerate}

\begin{example}[]\protect\hypertarget{exm-powells-method}{}\label{exm-powells-method}

To demonstrate how to supply additional arguments to an objective
function, let's consider minimizing the Rosenbrock function with an
additional scaling factor \(a\) and an offset \(b\):

\[
f(J, a, b) = \sum_{i=1}^{N-1} a (x_{i+1} - x_i^2)^2 + (1 - x_i)^2 + b
\]

You can achieve this using the \texttt{minimize} routine with the
example parameters \(a=0.5\) and \(b=1\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_with\_args(x, a, b):}
    \CommentTok{"""The Rosenbrock function with additional arguments"""}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(a }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ b}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen\_with\_args, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               args}\OperatorTok{=}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.}\NormalTok{), options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{, }\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 1.000000
         Iterations: 319
         Function evaluations: 525
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

As an alternative to using the \texttt{args} parameter of
\texttt{minimize}, you can wrap the objective function in a new function
that accepts only \texttt{x}. This approach is also useful when it is
necessary to pass additional parameters to the objective function as
keyword arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_with\_args(x, a, }\OperatorTok{*}\NormalTok{, b):  }\CommentTok{\# b is a keyword{-}only argument}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(a }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{:] }\OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\FloatTok{2.0}\NormalTok{)}\OperatorTok{**}\FloatTok{2.0} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}\OperatorTok{**}\FloatTok{2.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ b}

\KeywordTok{def}\NormalTok{ wrapped\_rosen\_without\_args(x):}
    \ControlFlowTok{return}\NormalTok{ rosen\_with\_args(x, }\FloatTok{0.5}\NormalTok{, b}\OperatorTok{=}\FloatTok{1.}\NormalTok{)  }\CommentTok{\# pass in \textasciigrave{}a\textasciigrave{} and \textasciigrave{}b\textasciigrave{}}

\NormalTok{x0 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{1.2}\NormalTok{])}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(wrapped\_rosen\_without\_args, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{,\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

Another alternative is to use \texttt{functools.partial}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ functools }\ImportTok{import}\NormalTok{ partial}

\NormalTok{partial\_rosen }\OperatorTok{=}\NormalTok{ partial(rosen\_with\_args, a}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, b}\OperatorTok{=}\FloatTok{1.}\NormalTok{)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(partial\_rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}nelder{-}mead\textquotesingle{}}\NormalTok{,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}xatol\textquotesingle{}}\NormalTok{: }\FloatTok{1e{-}8}\NormalTok{,\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.         1.         1.         1.         0.99999999]
\end{verbatim}

\end{example}

\section{Gradient-based Optimization
Algorithms}\label{gradient-based-optimization-algorithms}

\subsection{An Introductory Example: Broyden-Fletcher-Goldfarb-Shanno
Algorithm (BFGS)}\label{sec-bfgs-intro}

This section introduces an optimization algorithm that uses gradient
information to find the minimum. The Broyden-Fletcher-Goldfarb-Shanno
(BFGS) algorithm (selected by setting
\texttt{method=\textquotesingle{}BFGS\textquotesingle{}}) is an
optimization algorithm that aims to converge quickly to the solution.
This algorithm uses the gradient of the objective function. If the
gradient is not provided by the user, it is estimated using
first-differences. The BFGS method typically requires fewer function
calls compared to the simplex algorithm, even when the gradient needs to
be estimated.

\begin{example}[BFGS]\protect\hypertarget{exm-bfgs}{}\label{exm-bfgs}

To demonstrate the BFGS algorithm, let's use the Rosenbrock function
again. The gradient of the Rosenbrock function is a vector described by
the following mathematical expression:

\begin{align}
\frac{\partial f}{\partial x_j} = \sum_{i=1}^{N} 200(x_i - x_{i-1}^2)(\delta_{i,j} - 2x_{i-1}\delta_{i-1,j}) - 2(1 - x_{i-1})\delta_{i-1,j} \\
= 200(x_j - x_{j-1}^2) - 400x_j(x_{j+1} - x_j^2) - 2(1 - x_j)
\end{align}

This expression is valid for interior derivatives, but special cases
are:

\[
\frac{\partial f}{\partial x_0} = -400x_0(x_1 - x_0^2) - 2(1 - x_0)
\]

\[
\frac{\partial f}{\partial x_{N-1}} = 200(x_{N-1} - x_{N-2}^2)
\]

Here's a Python function that computes this gradient:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosen\_der(x):}
\NormalTok{    xm }\OperatorTok{=}\NormalTok{ x[}\DecValTok{1}\NormalTok{:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    xm\_m1 }\OperatorTok{=}\NormalTok{ x[:}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}
\NormalTok{    xm\_p1 }\OperatorTok{=}\NormalTok{ x[}\DecValTok{2}\NormalTok{:]}
\NormalTok{    der }\OperatorTok{=}\NormalTok{ np.zeros\_like(x)}
\NormalTok{    der[}\DecValTok{1}\NormalTok{:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{200}\OperatorTok{*}\NormalTok{(xm}\OperatorTok{{-}}\NormalTok{xm\_m1}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{400}\OperatorTok{*}\NormalTok{(xm\_p1 }\OperatorTok{{-}}\NormalTok{ xm}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{xm }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{xm)}
\NormalTok{    der[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{400}\OperatorTok{*}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\OperatorTok{*}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{])}
\NormalTok{    der[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{200}\OperatorTok{*}\NormalTok{(x[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{x[}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ der}
\end{Highlighting}
\end{Shaded}

You can specify this gradient information in the minimize function using
the jac parameter as illustrated below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OperatorTok{=}\NormalTok{ minimize(rosen, x0, method}\OperatorTok{=}\StringTok{\textquotesingle{}BFGS\textquotesingle{}}\NormalTok{, jac}\OperatorTok{=}\NormalTok{rosen\_der,}
\NormalTok{               options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{True}\NormalTok{\})}

\BuiltInTok{print}\NormalTok{(res.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 25
         Function evaluations: 30
         Gradient evaluations: 30
[1.00000004 1.0000001  1.00000021 1.00000044 1.00000092]
\end{verbatim}

\end{example}

\subsection{Background and Basics for Gradient-based
Optimization}\label{background-and-basics-for-gradient-based-optimization}

\subsection{Gradient}\label{gradient}

The gradient \(\nabla f(J)\) for a scalar function \(f(J)\) with \(n\)
different variables is defined by its partial derivatives:

\[
\nabla f(J) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right]
\]

\subsection{Jacobian Matrix}\label{jacobian-matrix}

The Jacobian matrix \(J(J)\) for a vector-valued function
\(F(J) = [f_1(J), f_2(J), \ldots, f_m(J)]\) is defined as:

\(J(J) = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \ldots & \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \ldots & \frac{\partial f_2}{\partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \ldots & \frac{\partial f_m}{\partial x_n} \end{bmatrix}\)

It consists of the first order partial derivatives and gives therefore
an overview about the gradients of a vector valued function.

\begin{example}[acobian
matrix]\protect\hypertarget{exm-jacobian}{}\label{exm-jacobian}

Consider a vector-valued function
\(f : \mathbb{R}^2 \rightarrow \mathbb{R}^3\) defined as follows:
\[f(J) = \begin{bmatrix} x_1^2 + 2x_2 \\ 3x_1 - \sin(x_2) \\ e^{x_1 + x_2} \end{bmatrix}\]

Let's compute the partial derivatives and construct the Jacobian matrix:

\(\frac{\partial f_1}{\partial x_1} = 2x_1, \quad \frac{\partial f_1}{\partial x_2} = 2\)

\(\frac{\partial f_2}{\partial x_1} = 3, \quad \frac{\partial f_2}{\partial x_2} = -\cos(x_2)\)

\(\frac{\partial f_3}{\partial x_1} = e^{x_1 + x_2}, \quad \frac{\partial f_3}{\partial x_2} = e^{x_1 + x_2}\)

So, the Jacobian matrix is:

\[J(J) = \begin{bmatrix} 2x_1 & 2 \\ 3 & -\cos(x_2) \\ e^{x_1 + x_2} & e^{x_1 + x_2} \end{bmatrix}\]

This Jacobian matrix provides information about how small changes in the
input variables \(x_1\) and \(x_2\) affect the corresponding changes in
each component of the output vector.

\end{example}

\subsection{Hessian Matrix}\label{hessian-matrix}

The Hessian matrix \(H(J)\) for a scalar function \(f(J)\) is defined
as:

\(H(J) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \ldots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}\)

The Hessian matrix consists of the second order derivatives of the
function. It provides information about the local curvature of the
function with respect to changes in the input variables.

\begin{example}[Hessian
matrix]\protect\hypertarget{exm-hessian}{}\label{exm-hessian}

Consider a scalar-valued function:
\[f(J) = x_1^2 + 2x_2^2 + \sin(x_1   x_2)\]

The Hessian matrix of this scalar-valued function is the matrix of its
second-order partial derivatives with respect to the input variables:
\[H(J) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} \end{bmatrix}\]

Let's compute the second-order partial derivatives and construct the
Hessian matrix:

\begin{align}
\frac{\partial^2 f}{\partial x_1^2} &= 2 + \cos(x_1 x_2) x_2^2\\
\frac{\partial^2 f}{\partial x_1 \partial x_2} &= 2x_1  x_2 \cos(x_1 x_2) - \sin(x_1  x_2)\\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &= 2x_1  x_2  \cos(x_1  x_2) - \sin(x_1  x_2)\\
\frac{\partial^2 f}{\partial x_2^2} &= 4x_2^2 + \cos(x_1  x_2) x_1^2
\end{align}

So, the Hessian matrix is:

\[H(J) = \begin{bmatrix} 2 + \cos(x_1   x_2)   x_2^2 & 2x_1   x_2   \cos(x_1   x_2) - \sin(x_1   x_2) \\ 2x_1   x_2   \cos(x_1   x_2) - \sin(x_1   x_2) & 4x_2^2 + \cos(x_1   x_2)   x_1^2 \end{bmatrix}\]

\end{example}

\subsection{Gradient Descent}\label{gradient-descent}

In optimization, the goal is to find the minimum or maximum of a
function. Gradient-based optimization methods utilize information about
the gradient (or derivative) of the function to guide the search for the
optimal solution. This is particularly useful when dealing with complex,
high-dimensional functions where an exhaustive search is impractical.

The gradient descent method can be divided in the following steps:

\begin{itemize}
\tightlist
\item
  \textbf{Initialize:} start with an initial guess for the parameters of
  the function to be optimized.
\item
  \textbf{Compute Gradient:} Calculate the gradient (partial
  derivatives) of the function with respect to each parameter at the
  current point. The gradient indicates the direction of the steepest
  increase in the function.
\item
  \textbf{Update Parameters:} Adjust the parameters in the opposite
  direction of the gradient, scaled by a learning rate. This step aims
  to move towards the minimum of the function:

  \begin{itemize}
  \tightlist
  \item
    \(x_{k+1} = x_k - \alpha \times \nabla f(x_{k})\)
  \item
    \(x_{x}\) is current parameter vector or point in the parameter
    space.
  \item
    \(\alpha\) is the learning rate, a positive scalar that determines
    the step size in each iteration.
  \item
    \(\nabla f(x)\) is the gradient of the objective function.
  \end{itemize}
\item
  \textbf{Iterate:} Repeat the above steps until convergence or a
  predefined number of iterations. Convergence is typically determined
  when the change in the function value or parameters becomes
  negligible.
\end{itemize}

\begin{example}[Gradient
Descent]\protect\hypertarget{exm-gradient-descent}{}\label{exm-gradient-descent}

We consider a simple quadratic function as an example: \[
f(x) = x^2 + 4x + y^2 + 2y + 4.
\]

We'll use gradient descent to find the minimum of this function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}

\CommentTok{\# Define the quadratic function}
\KeywordTok{def}\NormalTok{ quadratic\_function(x, y):}
    \ControlFlowTok{return}\NormalTok{ x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\NormalTok{ y}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{2}\OperatorTok{*}\NormalTok{y }\OperatorTok{+} \DecValTok{4}

\CommentTok{\# Define the gradient of the quadratic function}
\KeywordTok{def}\NormalTok{ gradient\_quadratic\_function(x, y):}
\NormalTok{    grad\_x }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{x }\OperatorTok{+} \DecValTok{4}
\NormalTok{    grad\_y }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{y }\OperatorTok{+} \DecValTok{2}
    \ControlFlowTok{return}\NormalTok{ np.array([grad\_x, grad\_y])}

\CommentTok{\# Gradient Descent for optimization in 2D}
\KeywordTok{def}\NormalTok{ gradient\_descent(initial\_point, learning\_rate, num\_iterations):}
\NormalTok{    points }\OperatorTok{=}\NormalTok{ [np.array(initial\_point)]}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_iterations):}
\NormalTok{        current\_point }\OperatorTok{=}\NormalTok{ points[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{        gradient }\OperatorTok{=}\NormalTok{ gradient\_quadratic\_function(}\OperatorTok{*}\NormalTok{current\_point)}
\NormalTok{        new\_point }\OperatorTok{=}\NormalTok{ current\_point }\OperatorTok{{-}}\NormalTok{ learning\_rate }\OperatorTok{*}\NormalTok{ gradient}
\NormalTok{        points.append(new\_point)}
    \ControlFlowTok{return}\NormalTok{ points}

\CommentTok{\# Visualization of optimization process with 3D surface and consistent arrow sizes}
\KeywordTok{def}\NormalTok{ plot\_optimization\_process\_3d\_consistent\_arrows(points):}
\NormalTok{    fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\NormalTok{    ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}

\NormalTok{    x\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{    y\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{    X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x\_vals, y\_vals)}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ quadratic\_function(X, Y)}

\NormalTok{    ax.plot\_surface(X, Y, Z, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}viridis\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{)}
\NormalTok{    ax.scatter(}\OperatorTok{*}\BuiltInTok{zip}\NormalTok{(}\OperatorTok{*}\NormalTok{points), [quadratic\_function(}\OperatorTok{*}\NormalTok{p) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ points], c}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Optimization Trajectory\textquotesingle{}}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(points) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):  }
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ points[i]}
\NormalTok{        dx, dy }\OperatorTok{=}\NormalTok{ points[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ points[i]}
\NormalTok{        dz }\OperatorTok{=}\NormalTok{ quadratic\_function(}\OperatorTok{*}\NormalTok{(points[i }\OperatorTok{+} \DecValTok{1}\NormalTok{])) }\OperatorTok{{-}}\NormalTok{ quadratic\_function(}\OperatorTok{*}\NormalTok{points[i])}
\NormalTok{        gradient\_length }\OperatorTok{=} \FloatTok{0.5}

\NormalTok{        ax.quiver(x, y, quadratic\_function(}\OperatorTok{*}\NormalTok{points[i]), dx, dy, dz, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, length}\OperatorTok{=}\NormalTok{gradient\_length, normalize}\OperatorTok{=}\VariableTok{False}\NormalTok{, arrow\_length\_ratio}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}

\NormalTok{    ax.set\_title(}\StringTok{\textquotesingle{}Gradient{-}Based Optimization with 2D Quadratic Function\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_zlabel(}\StringTok{\textquotesingle{}f(x, y)\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.legend()}
\NormalTok{    plt.show()}

\CommentTok{\# Initial guess and parameters}
\NormalTok{initial\_guess }\OperatorTok{=}\NormalTok{ [}\OperatorTok{{-}}\FloatTok{9.0}\NormalTok{, }\OperatorTok{{-}}\FloatTok{9.0}\NormalTok{]}
\NormalTok{learning\_rate }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{num\_iterations }\OperatorTok{=} \DecValTok{10}

\CommentTok{\# Run gradient descent in 2D and visualize the optimization process with 3D surface and consistent arrow sizes}
\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ gradient\_descent(initial\_guess, learning\_rate, num\_iterations)}
\NormalTok{plot\_optimization\_process\_3d\_consistent\_arrows(trajectory)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{003_scipy_optimize_intro_files/figure-pdf/cell-8-output-1.pdf}}

\end{example}

\subsection{Newton Method}\label{newton-method}

\textbf{Initialization:} Start with an initial guess for the optimal
solution: \(x_0\).

\textbf{Iteration:} Repeat the following three steps until convergence
or a predefined stopping criterion is met:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Calculate the gradient (\(\nabla\)) and the Hessian matrix
  (\(\nabla^2\)) of the objective function at the current point:
  \[\nabla f(x_k) \quad \text{and} \quad \nabla^2 f(x_k)\]
\item
  Update the current solution using the Newton-Raphson update formula \[
   x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k),
   \] where

\begin{verbatim}
* $\nabla f(x_k)$ is the gradient (first derivative) of the objective function with respect to the variable $x$, evaluated at the current solution $x_k$.
\end{verbatim}

  \begin{itemize}
  \tightlist
  \item
    \(\nabla^2 f(x_k)\): The Hessian matrix (second derivative) of the
    objective function with respect to \(x\), evaluated at the current
    solution \(x_k\).
  \item
    \(x_k\): The current solution or point in the optimization process.
  \item
    \(\nabla^2 f(x_k)]^{-1}\): The inverse of the Hessian matrix at the
    current point, representing the approximation of the curvature of
    the objective function.
  \item
    \(x_{k+1}\): The updated solution or point after applying the
    Newton-Raphson update.
  \end{itemize}
\item
  Check for convergence.
\end{enumerate}

\begin{example}[Newton
Method]\protect\hypertarget{exm-newton-method}{}\label{exm-newton-method}

We want to optimize the Rosenbrock function and use the Hessian and the
Jacobian (which is equal to the gradient vector for scalar objective
function) to the \texttt{minimize} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rosenbrock(x):}
    \ControlFlowTok{return} \DecValTok{100} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{])}\OperatorTok{**}\DecValTok{2}

\KeywordTok{def}\NormalTok{ rosenbrock\_gradient(x):}
\NormalTok{    dfdx0 }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{400} \OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{])}
\NormalTok{    dfdx1 }\OperatorTok{=} \DecValTok{200} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ np.array([dfdx0, dfdx1])}

\KeywordTok{def}\NormalTok{ rosenbrock\_hessian(x):}
\NormalTok{    d2fdx0 }\OperatorTok{=} \DecValTok{1200} \OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{400} \OperatorTok{*}\NormalTok{ x[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{2}
\NormalTok{    d2fdx1 }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{400} \OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ np.array([[d2fdx0, d2fdx1], [d2fdx1, }\DecValTok{200}\NormalTok{]])}

\KeywordTok{def}\NormalTok{ classical\_newton\_optimization\_2d(initial\_guess, tol}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{, max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ initial\_guess.copy()}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(max\_iter):}
\NormalTok{        gradient }\OperatorTok{=}\NormalTok{ rosenbrock\_gradient(x)}
\NormalTok{        hessian }\OperatorTok{=}\NormalTok{ rosenbrock\_hessian(x)}

        \CommentTok{\# Solve the linear system H * d = {-}g for d}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ np.linalg.solve(hessian, }\OperatorTok{{-}}\NormalTok{gradient)}

        \CommentTok{\# Update x}
\NormalTok{        x }\OperatorTok{+=}\NormalTok{ d}

        \CommentTok{\# Check for convergence}
        \ControlFlowTok{if}\NormalTok{ np.linalg.norm(gradient, }\BuiltInTok{ord}\OperatorTok{=}\NormalTok{np.inf) }\OperatorTok{\textless{}}\NormalTok{ tol:}
            \ControlFlowTok{break}

    \ControlFlowTok{return}\NormalTok{ x}

\CommentTok{\# Initial guess}
\NormalTok{initial\_guess\_2d }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{])}

\CommentTok{\# Run classical Newton optimization for the 2D Rosenbrock function}
\NormalTok{result\_2d }\OperatorTok{=}\NormalTok{ classical\_newton\_optimization\_2d(initial\_guess\_2d)}

\CommentTok{\# Print the result}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal solution:"}\NormalTok{, result\_2d)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Objective value:"}\NormalTok{, rosenbrock(result\_2d))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimal solution: [1. 1.]
Objective value: 0.0
\end{verbatim}

\end{example}

\subsection{BFGS-Algorithm}\label{bfgs-algorithm}

BFGS is an optimization algorithm designed for unconstrained
optimization problems. It belongs to the class of quasi-Newton methods
and is known for its efficiency in finding the minimum of a smooth,
unconstrained objective function.

\subsection{Procedure:}\label{procedure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initialization:}

  \begin{itemize}
  \tightlist
  \item
    Start with an initial guess for the parameters of the objective
    function.
  \item
    Initialize an approximation of the Hessian matrix (inverse) denoted
    by \(H\).\\
  \end{itemize}
\item
  \textbf{Iterative Update:}

  \begin{itemize}
  \tightlist
  \item
    At each iteration, compute the gradient vector at the current point.
  \item
    Update the parameters using the BFGS update formula, which involves
    the inverse Hessian matrix approximation, the gradient, and the
    difference in parameter vectors between successive iterations:
    \[x_{k+1} = x_k - H_k^{-1} \nabla f(x_k).\]
  \item
    Update the inverse Hessian approximation using the BFGS update
    formula for the inverse Hessian.
    \[H_{k+1} = H_k + \frac{\Delta x_k \Delta x_k^T}{\Delta x_k^T \Delta g_k} - \frac{H_k g_k g_k^T H_k}{g_k^T H_k g_k},\]
    where:
  \item
    \(x_k\) and \(x_{k+1}\) are the parameter vectors at the current and
    updated iterations, respectively.
  \item
    \(\nabla f(x_k)\) is the gradient vector at the current iteration.
  \item
    \(\Delta x_k = x_{k+1} - x_k\) is the change in parameter vectors.
  \item
    \(\Delta g_k = \nabla f(x_{k+1}) - \nabla f(x_k)\) is the change in
    gradient vectors.
  \end{itemize}
\item
  \textbf{Convergence:}

  \begin{itemize}
  \tightlist
  \item
    Repeat the iterative update until the optimization converges.
    Convergence is typically determined by reaching a sufficiently low
    gradient or parameter change.
  \end{itemize}
\end{enumerate}

\begin{example}[BFGS for
Rosenbrock]\protect\hypertarget{exm-bfgs}{}\label{exm-bfgs}

~

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\CommentTok{\# Define the 2D Rosenbrock function}
\KeywordTok{def}\NormalTok{ rosenbrock(x):}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{])}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{100} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\CommentTok{\# Initial guess}
\NormalTok{initial\_guess }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{])}

\CommentTok{\# Minimize the Rosenbrock function using BFGS}
\NormalTok{minimize(rosenbrock, initial\_guess, method}\OperatorTok{=}\StringTok{\textquotesingle{}BFGS\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: 2.8440052847381483e-11
        x: [ 1.000e+00  1.000e+00]
      nit: 19
      jac: [ 3.987e-06 -2.844e-06]
 hess_inv: [[ 4.948e-01  9.896e-01]
            [ 9.896e-01  1.984e+00]]
     nfev: 72
     njev: 24
\end{verbatim}

\end{example}

\subsection{Visualization BFGS for
Rosenbrock}\label{visualization-bfgs-for-rosenbrock}

A visualization of the BFGS search process on Rosenbrock's function can
be found here:
\url{https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif}

\section{Global Optimization}\label{global-optimization}

Global optimization aims to find the global minimum of a function within
given bounds, in the presence of potentially many local minima.
Typically, global minimizers efficiently search the parameter space,
while using a local minimizer (e.g., minimize) under the hood.

\subsection{Local vs Global
Optimization}\label{local-vs-global-optimization}

\subsubsection{Local Optimizater:}\label{local-optimizater}

\begin{itemize}
\tightlist
\item
  Seeks the optimum in a \textbf{specific region} of the search space
\item
  Tends to \textbf{exploit} the local environment, to find solutions in
  the immediate area
\item
  Highly \textbf{sensitive to initial conditions}; may converge to
  different local optima based on the starting point
\item
  Often \textbf{computationally efficient for low-dimensional problems}
  but may struggle with high-dimensional or complex search spaces
\item
  Commonly used in situations where the objective is to refine and
  improve existing solutions
\end{itemize}

\subsubsection{Global Optimizer:}\label{global-optimizer}

\begin{itemize}
\tightlist
\item
  Explores the \textbf{entire search space} to find the global optimum
\item
  Emphasize \textbf{exploration over exploitation}, aiming to search
  broadly and avoid premature convergence to local optima
\item
  Aim to \textbf{mitigate the risk of premature convergence} to local
  optima by employing strategies for global exploration
\item
  \textbf{Less sensitive to initial conditions}, designed to navigate
  diverse regions of the search space
\item
  Equipped to handle \textbf{high-dimensional} and \textbf{complex}
  problems, though computational demands may vary depending on the
  specific algorithm
\item
  Preferred for applications where a comprehensive search of the
  solution space is crucial, such as in parameter tuning, machine
  learning, and complex engineering design
\end{itemize}

\begin{example}[Global Optimizers in
SciPy]\protect\hypertarget{exm-global-optimization}{}\label{exm-global-optimization}

SciPy contains a number of good global optimizers. Here, we'll use those
on the same objective function, namely the (aptly named) eggholder
function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ eggholder(x):}
    \ControlFlowTok{return}\NormalTok{ (}\OperatorTok{{-}}\NormalTok{(x[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{47}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.sin(np.sqrt(}\BuiltInTok{abs}\NormalTok{(x[}\DecValTok{0}\NormalTok{]}\OperatorTok{/}\DecValTok{2} \OperatorTok{+}\NormalTok{ (x[}\DecValTok{1}\NormalTok{]  }\OperatorTok{+} \DecValTok{47}\NormalTok{))))}
            \OperatorTok{{-}}\NormalTok{x[}\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ np.sin(np.sqrt(}\BuiltInTok{abs}\NormalTok{(x[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ (x[}\DecValTok{1}\NormalTok{]  }\OperatorTok{+} \DecValTok{47}\NormalTok{)))))}

\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{512}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{512}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{513}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{512}\NormalTok{, }\DecValTok{513}\NormalTok{)}
\NormalTok{xgrid, ygrid }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{xy }\OperatorTok{=}\NormalTok{ np.stack([xgrid, ygrid])}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.view\_init(}\DecValTok{45}\NormalTok{, }\OperatorTok{{-}}\DecValTok{45}\NormalTok{)}
\NormalTok{ax.plot\_surface(xgrid, ygrid, eggholder(xy), cmap}\OperatorTok{=}\StringTok{\textquotesingle{}terrain\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}eggholder(x, y)\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{003_scipy_optimize_intro_files/figure-pdf/cell-12-output-1.pdf}}

We now use the global optimizers to obtain the minimum and the function
value at the minimum. We'll store the results in a dictionary so we can
compare different optimization results later.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ optimize}
\NormalTok{results }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{()}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.shgo(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -935.3379515605789
    funl: [-9.353e+02]
       x: [ 4.395e+02  4.540e+02]
      xl: [[ 4.395e+02  4.540e+02]]
     nit: 1
    nfev: 45
   nlfev: 40
   nljev: 10
   nlhev: 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.dual\_annealing(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: ['Maximum number of iteration reached']
 success: True
  status: 0
     fun: -894.5789003901064
       x: [-4.657e+02  3.857e+02]
     nit: 1000
    nfev: 4118
    njev: 39
    nhev: 0
\end{verbatim}

All optimizers return an \texttt{OptimizeResult}, which in addition to
the solution contains information on the number of function evaluations,
whether the optimization was successful, and more. For brevity, we won't
show the full output of the other optimizers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.differential\_evolution(eggholder, bounds)}
\NormalTok{results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             message: Optimization terminated successfully.
             success: True
                 fun: -955.2551521441337
                   x: [ 4.790e+02  4.345e+02]
                 nit: 33
                nfev: 1068
          population: [[ 4.788e+02  4.343e+02]
                       [ 4.769e+02  4.329e+02]
                       ...
                       [ 4.807e+02  4.365e+02]
                       [ 4.842e+02  4.398e+02]]
 population_energies: [-9.553e+02 -9.483e+02 ... -9.522e+02 -9.493e+02]
                 jac: [-5.684e-05  6.821e-05]
\end{verbatim}

\texttt{shgo} has a second method, which returns all local minima rather
than only what it thinks is the global minimum:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ optimize.shgo(eggholder, bounds, n}\OperatorTok{=}\DecValTok{200}\NormalTok{, iters}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                                      sampling\_method}\OperatorTok{=}\StringTok{\textquotesingle{}sobol\textquotesingle{}}\NormalTok{)}
\NormalTok{results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 message: Optimization terminated successfully.
 success: True
     fun: -959.640662720831
    funl: [-9.596e+02 -9.353e+02 ... -6.591e+01 -6.387e+01]
       x: [ 5.120e+02  4.042e+02]
      xl: [[ 5.120e+02  4.042e+02]
           [ 4.395e+02  4.540e+02]
           ...
           [ 3.165e+01 -8.523e+01]
           [ 5.865e+01 -5.441e+01]]
     nit: 5
    nfev: 3529
   nlfev: 2327
   nljev: 634
   nlhev: 0
\end{verbatim}

We'll now plot all found minima on a heatmap of the function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(eggholder(xy), interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}bilinear\textquotesingle{}}\NormalTok{, origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{               cmap}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}

\KeywordTok{def}\NormalTok{ plot\_point(res, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
\NormalTok{    ax.plot(}\DecValTok{512}\OperatorTok{+}\NormalTok{res.x[}\DecValTok{0}\NormalTok{], }\DecValTok{512}\OperatorTok{+}\NormalTok{res.x[}\DecValTok{1}\NormalTok{], marker}\OperatorTok{=}\NormalTok{marker, color}\OperatorTok{=}\NormalTok{color, ms}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}DE\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{)  }\CommentTok{\# differential\_evolution {-} cyan}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}DA\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{)  }\CommentTok{\# dual\_annealing.        {-} white}

\CommentTok{\# SHGO produces multiple minima, plot them all (with a smaller marker size)}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}shgo\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{)}
\NormalTok{plot\_point(results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl.shape[}\DecValTok{0}\NormalTok{]):}
\NormalTok{    ax.plot(}\DecValTok{512} \OperatorTok{+}\NormalTok{ results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl[i, }\DecValTok{0}\NormalTok{],}
            \DecValTok{512} \OperatorTok{+}\NormalTok{ results[}\StringTok{\textquotesingle{}shgo\_sobol\textquotesingle{}}\NormalTok{].xl[i, }\DecValTok{1}\NormalTok{],}
            \StringTok{\textquotesingle{}ro\textquotesingle{}}\NormalTok{, ms}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax.set\_xlim([}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{514}\OperatorTok{*}\DecValTok{2}\NormalTok{])}
\NormalTok{ax.set\_ylim([}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{514}\OperatorTok{*}\DecValTok{2}\NormalTok{])}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{003_scipy_optimize_intro_files/figure-pdf/cell-17-output-1.pdf}}

\end{example}

\subsection{Dual Annealing
Optimization}\label{dual-annealing-optimization}

This function implements the Dual-Annealing optimization, which is a
variant of the famous simulated annealing optimization.

Simulated Annealing is a \textbf{probabilistic} optimization algorithm
inspired by the annealing process in metallurgy. The algorithm is
designed to find a good or optimal \textbf{global} solution to a problem
by exploring the solution space in a controlled and adaptive manner.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Annealing in Metallurgy}]

Simulated Annealing draws inspiration from the physical process of
annealing in metallurgy. Just as metals are gradually cooled to achieve
a more stable state, Simulated Annealing uses a similar approach to
explore solution spaces in the digital world.

\end{tcolorbox}

\textbf{Heating Phase}: In metallurgy, a metal is initially heated to a
high temperature. At this elevated temperature, the atoms or molecules
in the material become more energetic and chaotic, allowing the material
to overcome energy barriers and defects.

\textbf{Analogy Simulated Annealing (Exploration Phase):} In Simulated
Annealing, the algorithm starts with a high ``temperature,'' which
encourages exploration of the solution space. At this stage, the
algorithm is more likely to accept solutions that are worse than the
current one, allowing it to escape local optima and explore a broader
region of the solution space.

\textbf{Cooling Phase:} The material is then gradually cooled at a
controlled rate. As the temperature decreases, the atoms or molecules
start to settle into more ordered and stable arrangements. The slow
cooling rate is crucial to avoid the formation of defects and to ensure
the material reaches a well-organized state.

\textbf{Analogy Simulated Annealing (Exploitation Phase):} As the
algorithm progresses, the temperature is gradually reduced over time
according to a cooling schedule. This reduction simulates the cooling
process in metallurgy. With lower temperatures, the algorithm becomes
more selective and tends to accept only better solutions, focusing on
refining and exploiting the promising regions discovered during the
exploration phase.

\subsubsection{Key Concepts}\label{key-concepts}

\textbf{Temperature:} The temperature is a parameter that controls the
likelihood of accepting worse solutions. We start with a high
temperature, allowing the algorithm to explore the solution space
braodly. The temperature decreases with the iterations of the algorithm.

\textbf{Cooling Schedule:} The temperature parameter is reduced
according to this schedule. The analogy to the annealing of metals: a
slower cooling rate allows the material to reach a more stable state.

\textbf{Neighborhood Exploration:} At each iteration, the algorithm
explores the neighborhood of the current solution. The neighborhood is
defined by small perturbations or changes to the current solution.

\textbf{Acceptance Probability:} The algorithm evaluates the objective
function for the new solution in the neighborhood. If the new solution
is better, it is accepted. If the new solution is worse, it may still be
accepted with a certain probability. This probability is determined by
both the difference in objective function values and the current
temperature.

\textbf{For minimization:} If: \[
f(x_{t}) > f(x_{t+1})
\] Then: \[
P(accept\_new\_point) = 1
\]

If: \[
f(x_{t}) < f(x_{t+1})
\] Then: \[
P(accept\_new\_point) = e^{-(\frac{f(x_{t+1}) - f(x_{t})}{Tt})}
\]

\textbf{Termination Criterion:} The algorithm continues iterations until
a termination condition is met. This could be a fixed number of
iterations, reaching a specific temperature threshold, or achieving a
satisfactory solution.

\subsubsection{Steps}\label{steps}

\textbf{1. Initialization:} Set an initial temperature (\(T_{0}\)) and
an initial solution (\(f(x_{0})\)). The temperature is typically set
high initially to encourage exploration.

\textbf{2. Generate a Neighbor:} Perturb the current solution to
generate a neighboring solution. The perturbation can be random or
follow a specific strategy.

\textbf{3. Evaluate the Neighbor:} Evaluate the objective function for
the new solution in the neighborhood.

\textbf{4. Accept or Reject the Neighbor:} + If the new solution is
better (lower cost for minimization problems or higher for maximization
problems), accept it as the new current solution. + If the new solution
is worse, accept it with a probability determined by an acceptance
probability function as mentioned above. The probability is influenced
by the difference in objective function values and the current
temperature.

\textbf{5. Cooling:} Reduce the temperature according to a cooling
schedule. The cooling schedule defines how fast the temperature
decreases over time. Common cooling schedules include exponential or
linear decay.

\textbf{6. Termination Criterion:} Repeat the iterations (2-5) until a
termination condition is met. This could be a fixed number of
iterations, reaching a specific temperature threshold, or achieving a
satisfactory solution.

\subsubsection{Scipy Implementation of the Dual Annealing
Algorithm}\label{scipy-implementation-of-the-dual-annealing-algorithm}

In Scipy, we utilize the Dual Annealing optimizer, an extension of the
simulated annealing algorithm that is versatile for both discrete and
continuous problems.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ dual\_annealing}

\KeywordTok{def}\NormalTok{ rastrigin\_function(x):}
    \ControlFlowTok{return} \DecValTok{20} \OperatorTok{+}\NormalTok{ x[}\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{10} \OperatorTok{*}\NormalTok{ np.cos(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{]) }\OperatorTok{+}\NormalTok{ x[}\DecValTok{1}\NormalTok{]}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{10} \OperatorTok{*}\NormalTok{ np.cos(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ x[}\DecValTok{1}\NormalTok{])}

\CommentTok{\# Define the Rastrigin function for visualization}
\KeywordTok{def}\NormalTok{ rastrigin\_visualization(x, y):}
    \ControlFlowTok{return} \DecValTok{20} \OperatorTok{+}\NormalTok{ x}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{10} \OperatorTok{*}\NormalTok{ np.cos(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ x) }\OperatorTok{+}\NormalTok{ y}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{10} \OperatorTok{*}\NormalTok{ np.cos(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ y)}

\CommentTok{\# Create a meshgrid for visualization}
\NormalTok{x\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{x\_mesh, y\_mesh }\OperatorTok{=}\NormalTok{ np.meshgrid(x\_vals, y\_vals)}
\NormalTok{z\_mesh }\OperatorTok{=}\NormalTok{ rastrigin\_visualization(x\_mesh, y\_mesh)}

\CommentTok{\# Visualize the Rastrigin function}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\NormalTok{contour }\OperatorTok{=}\NormalTok{ plt.contour(x\_mesh, y\_mesh, z\_mesh, levels}\OperatorTok{=}\DecValTok{50}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}viridis\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.colorbar(contour, label}\OperatorTok{=}\StringTok{\textquotesingle{}Rastrigin Function Value\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Visualization of the 2D Rastrigin Function\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Optimize the Rastrigin function using dual annealing}
\NormalTok{result }\OperatorTok{=}\NormalTok{ dual\_annealing(func }\OperatorTok{=}\NormalTok{ rastrigin\_function,}
\NormalTok{                        x0}\OperatorTok{=}\NormalTok{[}\FloatTok{5.0}\NormalTok{,}\FloatTok{3.0}\NormalTok{],                       }\CommentTok{\#Initial Guess}
\NormalTok{                        bounds}\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)],}
\NormalTok{                        initial\_temp }\OperatorTok{=} \DecValTok{5230}\NormalTok{,                }\CommentTok{\#Intial Value for temperature}
\NormalTok{                        restart\_temp\_ratio }\OperatorTok{=} \FloatTok{2e{-}05}\NormalTok{,         }\CommentTok{\#Temperature schedule}
\NormalTok{                        seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Plot the optimized point}
\NormalTok{optimal\_x, optimal\_y }\OperatorTok{=}\NormalTok{ result.x}
\NormalTok{plt.plot(optimal\_x, optimal\_y, }\StringTok{\textquotesingle{}ro\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Optimal Point\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Set labels and legend}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}

\CommentTok{\# Show the plot}
\NormalTok{plt.show()}

\CommentTok{\# Display the optimization result}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal parameters:"}\NormalTok{, result.x)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Minimum value of the Rastrigin function:"}\NormalTok{, result.fun)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{003_scipy_optimize_intro_files/figure-pdf/cell-18-output-1.pdf}}

\begin{verbatim}
Optimal parameters: [-4.60133247e-09 -4.31928660e-09]
Minimum value of the Rastrigin function: 7.105427357601002e-15
\end{verbatim}

\subsection{Differential Evolution}\label{differential-evolution}

Differential Evolution is an algorithm used for finding the global
minimum of multivariate functions. It is stochastic in nature (does not
use gradient methods), and can search large areas of candidate space,
but often requires larger numbers of function evaluations than
conventional gradient based techniques.

Differential Evolution (DE) is a versatile and global optimization
algorithm inspired by natural selection and evolutionary processes.
Introduced by Storn and Price in 1997, DE mimics the
survival-of-the-fittest principle by evolving a population of candidate
solutions through iterative mutation, crossover, and selection
operations. This nature-inspired approach enables DE to efficiently
explore complex and non-linear solution spaces, making it a widely
adopted optimization technique in diverse fields such as engineering,
finance, and machine learning.

\subsection{Procedure}\label{procedure-1}

The procedure boils down to the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initialization:}

  \begin{itemize}
  \tightlist
  \item
    Create a population of candidate solutions randomly within the
    specified search space.
  \end{itemize}
\item
  \textbf{Mutation:}

  \begin{itemize}
  \tightlist
  \item
    For each individual in the population, select three distinct
    individuals (vectors) randomly.
  \item
    Generate a mutant vector \texttt{V} by combining these three vectors
    with a scaling factor.
  \end{itemize}
\item
  \textbf{Crossover:}

  \begin{itemize}
  \tightlist
  \item
    Perform the crossover operation between the target vector \texttt{U}
    and the mutant vector \texttt{V}. Information from both vectors is
    used to create a trial vector \texttt{UÂ´}
  \end{itemize}
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Cross-Over Strategies in DE}]

\begin{itemize}
\tightlist
\item
  There are several crossover strategies in the literature. Two examples
  are:
\end{itemize}

\textbf{Binominal Crossover:}

In this strategy, each component of the trial vector is selected from
the mutant vector with a probability equal to the crossover rate
(\(CR\)). This means that each element of the trial vector has an
independent probability of being replaced by the corresponding element
of the mutant vector.

\[U'_i =
      \begin{cases}
      V_i, & \text{if a random number} \ \sim U(0, 1) \leq CR \ \text{(Crossover Rate)} \\
      U_i, & \text{otherwise}
      \end{cases}
\]

\textbf{Exponential Crossover:}

In exponential crossover, the trial vector is constructed by selecting a
random starting point and copying elements from the mutant vector with a
certain probability. The probability decreases exponentially with the
distance from the starting point. This strategy introduces a correlation
between neighboring elements in the trial vector.

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Selection:}

  \begin{itemize}
  \tightlist
  \item
    Evaluate the fitness of the trial vector obtained from the
    crossover.
  \item
    Replace the target vector with the trial vector if its fitness is
    better.
  \end{itemize}
\item
  \textbf{Termination:}

  \begin{itemize}
  \tightlist
  \item
    Repeat the mutation, crossover, and selection steps for a predefined
    number of generations or until convergence criteria are met.
  \end{itemize}
\item
  \textbf{Result:}

  \begin{itemize}
  \tightlist
  \item
    The algorithm returns the best-found solution after the specified
    number of iterations.
  \end{itemize}
\end{enumerate}

The key parameters in DE include the population size, crossover
probability, and the scaling factor. Tweak these parameters based on the
characteristics of the optimization problem for optimal performance.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\CommentTok{\# Define the Rastrigin function}
\KeywordTok{def}\NormalTok{ rastrigin(x):}
\NormalTok{    A }\OperatorTok{=} \DecValTok{10}
    \ControlFlowTok{return}\NormalTok{ A }\OperatorTok{*} \BuiltInTok{len}\NormalTok{(x) }\OperatorTok{+} \BuiltInTok{sum}\NormalTok{([(xi}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}}\NormalTok{ A }\OperatorTok{*}\NormalTok{ np.cos(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ xi)) }\ControlFlowTok{for}\NormalTok{ xi }\KeywordTok{in}\NormalTok{ x])}

\CommentTok{\# Create a grid for visualization}
\NormalTok{x\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{5.12}\NormalTok{, }\FloatTok{5.12}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y\_vals }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{5.12}\NormalTok{, }\FloatTok{5.12}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x\_vals, y\_vals)}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ rastrigin(np.vstack([X.ravel(), Y.ravel()]))}

\CommentTok{\# Reshape Z to match the shape of X and Y}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ Z.reshape(X.shape)}

\CommentTok{\# Plot the Rastrigin function}
\NormalTok{plt.contour(X, Y, Z, levels}\OperatorTok{=}\DecValTok{50}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}viridis\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Rastrigin Function\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Initial guess (starting point for the optimization)}
\NormalTok{initial\_guess }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Define the bounds for each variable in the Rastrigin function}
\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\FloatTok{5.12}\NormalTok{, }\FloatTok{5.12}\NormalTok{)] }\OperatorTok{*} \DecValTok{4}  \CommentTok{\# 4D problem, each variable has bounds ({-}5.12, 5.12)}

\CommentTok{\# Run the minimize function}
\NormalTok{result }\OperatorTok{=}\NormalTok{ minimize(rastrigin, initial\_guess, bounds}\OperatorTok{=}\NormalTok{bounds, method}\OperatorTok{=}\StringTok{\textquotesingle{}L{-}BFGS{-}B\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Extract the optimal solution}
\NormalTok{optimal\_solution }\OperatorTok{=}\NormalTok{ result.x}

\CommentTok{\# Plot the optimal solution}
\NormalTok{plt.scatter(optimal\_solution[}\DecValTok{0}\NormalTok{], optimal\_solution[}\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Optimal Solution\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Add labels and legend}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Optimization of Rastrigin Function with Minimize\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Variable 1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Variable 2\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}

\CommentTok{\# Show the plot}
\NormalTok{plt.show()}

\CommentTok{\# Print the optimization result}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal Solution:"}\NormalTok{, optimal\_solution)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimal Objective Value:"}\NormalTok{, result.fun)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{003_scipy_optimize_intro_files/figure-pdf/cell-19-output-1.pdf}}

\begin{verbatim}
Optimal Solution: [-2.52869119e-08 -2.07795060e-08 -2.52869119e-08 -1.62721002e-08]
Optimal Objective Value: 3.907985046680551e-13
\end{verbatim}

\subsection{Other global optimization
algorithms}\label{other-global-optimization-algorithms}

\subsection{DIRECT}\label{direct}

DIviding RECTangles (DIRECT) is a deterministic global optimization
algorithm capable of minimizing a black box function with its variables
subject to lower and upper bound constraints by sampling potential
solutions in the search space

\subsection{SHGO}\label{shgo}

SHGO stands for ``simplicial homology global optimization''. It is
considered appropriate for solving general purpose NLP and blackbox
optimization problems to global optimality (low-dimensional problems).

\subsection{Basin-hopping}\label{basin-hopping}

Basin-hopping is a two-phase method that combines a global stepping
algorithm with local minimization at each step. Designed to mimic the
natural process of energy minimization of clusters of atoms, it works
well for similar problems with ``funnel-like, but rugged'' energy
landscapes

\section{Project: One-Mass Oscillator
Optimization}\label{project-one-mass-oscillator-optimization}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}
\end{Highlighting}
\end{Shaded}

\subsection{Introduction}\label{introduction}

In this project, you will apply various optimization algorithms to fit a
one-mass oscillator model to real-world data. The objective is to
minimize the sum of the squared residuals between the model predictions
and the observed amplitudes of a one-mass oscillator system across
different frequencies.

\subsection{One-Mass Oscillator Model}\label{one-mass-oscillator-model}

The one-mass oscillator is characterized by the following equation,
representing the amplitudes of the system:

\[
V(\omega) = \frac{F}{\sqrt{(1 - \nu^2)^2 + 4D^2\nu^2}}
\]

Here, \(\omega\) represents the angular frequency of the system, \(\nu\)
is the ratio of the excitation frequency to the natural frequency, i.e.,
\[
\nu = \frac{\omega_{\text{err}}}{\omega_{\text{eig}}},
\] \(D\) is the damping ratio, and \(F\) is the force applied to the
system.

The goal of the project is to determine the optimal values for the
parameters \(\omega_{\text{eig}}\), \(D\), and \(F\) that result in the
best fit of the one-mass oscillator model to the observed amplitudes.

\subsection{The Real-World Data}\label{the-real-world-data}

There are two different measurements. \texttt{J} represents the measured
frequencies, and \texttt{N} represents the measured amplitudes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 }\OperatorTok{=}\NormalTok{ pd.read\_pickle(}\StringTok{"./data/Hcf.d/df1.pkl"}\NormalTok{)}
\NormalTok{df2 }\OperatorTok{=}\NormalTok{ pd.read\_pickle(}\StringTok{"./data/Hcf.d/df2.pkl"}\NormalTok{)}
\NormalTok{df1.describe()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& J & N \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
count & 33.000000 & 33.000000 \\
mean & 8148.750252 & 10.430887 \\
std & 6.870023 & 2.846469 \\
min & 8137.649210 & 4.698761 \\
25\% & 8143.799766 & 8.319253 \\
50\% & 8146.942295 & 10.152119 \\
75\% & 8153.934051 & 13.407260 \\
max & 8162.504002 & 14.382749 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& J & N \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
14999 & 8162.504002 & 5.527511 \\
15011 & 8156.384831 & 7.359789 \\
15016 & 8159.199238 & 6.532958 \\
15020 & 8159.200889 & 5.895933 \\
15025 & 8153.934051 & 9.326749 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the data, i.e., the measured amplitudes as a function of the measured frequencies}
\NormalTok{plt.scatter(df1[}\StringTok{"J"}\NormalTok{], df1[}\StringTok{"N"}\NormalTok{], color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Spektralpunkte"}\NormalTok{, zorder}\OperatorTok{=}\DecValTok{5}\NormalTok{, s}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Frequency [Hz]"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Amplitude"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{003_scipy_optimize_intro_files/figure-pdf/cell-23-output-1.pdf}}

Note: Low amplitudes distort the fit and are negligible therefore we
define a lower threshold for \texttt{N}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{threshold }\OperatorTok{=} \FloatTok{0.4}
\NormalTok{df1.sort\_values(}\StringTok{"N"}\NormalTok{)}
\NormalTok{max\_N }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(df1[}\StringTok{"N"}\NormalTok{])}
\NormalTok{df1 }\OperatorTok{=}\NormalTok{ df1[df1[}\StringTok{"N"}\NormalTok{]}\OperatorTok{\textgreater{}=}\NormalTok{threshold}\OperatorTok{*}\NormalTok{max\_N]}
\end{Highlighting}
\end{Shaded}

We extract the frequency value for maximum value of the amplitude. This
serves as the initial value for one decision variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_max}\OperatorTok{=}\NormalTok{df1[df1[}\StringTok{"N"}\NormalTok{]}\OperatorTok{==}\BuiltInTok{max}\NormalTok{(df1[}\StringTok{"N"}\NormalTok{])]}
\NormalTok{initial\_Oeig }\OperatorTok{=}\NormalTok{ df\_max[}\StringTok{"J"}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{max\_N }\OperatorTok{=}\NormalTok{ df\_max[}\StringTok{"N"}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

We also have to define the other two initial guesses for the damping
ratio and the force, e.g.,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial\_D }\OperatorTok{=} \FloatTok{0.006}
\NormalTok{initial\_F }\OperatorTok{=} \FloatTok{0.120}
\NormalTok{initial\_values }\OperatorTok{=}\NormalTok{ [initial\_Oeig, initial\_D, initial\_F]}
\end{Highlighting}
\end{Shaded}

Additionally, we define the bounds for the decision variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_Oerr }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(df1[}\StringTok{"J"}\NormalTok{])}
\NormalTok{max\_Oerr }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(df1[}\StringTok{"J"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(min\_Oerr, max\_Oerr), (}\DecValTok{0}\NormalTok{, }\FloatTok{0.03}\NormalTok{), (}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\subsection{Objective Function}\label{objective-function}

Then we define the objective function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ one\_mass\_oscillator(params, Oerr) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{\# returns amplitudes of the system}
    \CommentTok{\# Defines the model of a one mass oscilator }
\NormalTok{    Oeig, D, F }\OperatorTok{=}\NormalTok{ params}
\NormalTok{    nue }\OperatorTok{=}\NormalTok{ Oerr }\OperatorTok{/}\NormalTok{ Oeig}
\NormalTok{    V }\OperatorTok{=}\NormalTok{ F }\OperatorTok{/}\NormalTok{ (np.sqrt((}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ nue}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{**} \DecValTok{2} \OperatorTok{+}\NormalTok{ (}\DecValTok{4} \OperatorTok{*}\NormalTok{ D}\OperatorTok{**}\DecValTok{2} \OperatorTok{*}\NormalTok{ nue}\OperatorTok{**}\DecValTok{2}\NormalTok{)))}
    \ControlFlowTok{return}\NormalTok{ V}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ objective\_function(params, Oerr, amplitudes) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{\# objective function to compare calculated and real amplitudes}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((amplitudes }\OperatorTok{{-}}\NormalTok{ one\_mass\_oscillator(params, Oerr)) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We define the options for the optimzer and start the optimization
process:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{options }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"maxfun"}\NormalTok{: }\DecValTok{100000}\NormalTok{,}
    \StringTok{"ftol"}\NormalTok{: }\FloatTok{1e{-}9}\NormalTok{,}
    \StringTok{"xtol"}\NormalTok{: }\FloatTok{1e{-}9}\NormalTok{,}
    \StringTok{"stepmx"}\NormalTok{: }\DecValTok{10}\NormalTok{,}
    \StringTok{"eta"}\NormalTok{: }\FloatTok{0.25}\NormalTok{,}
    \StringTok{"gtol"}\NormalTok{: }\FloatTok{1e{-}5}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{J }\OperatorTok{=}\NormalTok{ np.array(df1[}\StringTok{"J"}\NormalTok{]) }\CommentTok{\# measured frequency}
\NormalTok{N }\OperatorTok{=}\NormalTok{ np.array(df1[}\StringTok{"N"}\NormalTok{]) }\CommentTok{\# measured amplitude}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OperatorTok{=}\NormalTok{ minimize(}
\NormalTok{    objective\_function,}
\NormalTok{    initial\_values,}
\NormalTok{    args}\OperatorTok{=}\NormalTok{(J, N),}
\NormalTok{    method}\OperatorTok{=}\StringTok{\textquotesingle{}Nelder{-}Mead\textquotesingle{}}\NormalTok{,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    options}\OperatorTok{=}\NormalTok{options)}
\end{Highlighting}
\end{Shaded}

\subsection{Results}\label{results}

We can observe the results:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# map optimized values to variables}
\NormalTok{resonant\_frequency }\OperatorTok{=}\NormalTok{ result.x[}\DecValTok{0}\NormalTok{]}
\NormalTok{D }\OperatorTok{=}\NormalTok{ result.x[}\DecValTok{1}\NormalTok{]}
\NormalTok{F }\OperatorTok{=}\NormalTok{ result.x[}\DecValTok{2}\NormalTok{]}
\CommentTok{\# predict the resonant amplitude with the fitted one mass oscillator.}
\NormalTok{X\_pred }\OperatorTok{=}\NormalTok{ np.linspace(min\_Oerr, max\_Oerr, }\DecValTok{1000}\NormalTok{)}
\NormalTok{ypred\_one\_mass\_oscillator }\OperatorTok{=}\NormalTok{ one\_mass\_oscillator(result.x, X\_pred)}
\NormalTok{resonant\_amplitude }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(ypred\_one\_mass\_oscillator)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"result: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
result:        message: Optimization terminated successfully.
       success: True
        status: 0
           fun: 53.54144061205875
             x: [ 8.148e+03  7.435e-04  2.153e-02]
           nit: 93
          nfev: 169
 final_simplex: (array([[ 8.148e+03,  7.435e-04,  2.153e-02],
                       [ 8.148e+03,  7.435e-04,  2.153e-02],
                       [ 8.148e+03,  7.435e-04,  2.153e-02],
                       [ 8.148e+03,  7.435e-04,  2.153e-02]]), array([ 5.354e+01,  5.354e+01,  5.354e+01,  5.354e+01]))
\end{verbatim}

Finally, we can plot the optimized fit and the real values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.scatter(}
\NormalTok{    df1[}\StringTok{"J"}\NormalTok{],}
\NormalTok{    df1[}\StringTok{"N"}\NormalTok{],}
\NormalTok{    color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\StringTok{"Spektralpunkte filtered"}\NormalTok{,}
\NormalTok{    zorder}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{    s}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{)}
\CommentTok{\# color the max amplitude point red}
\NormalTok{plt.scatter(}
\NormalTok{    initial\_Oeig,}
\NormalTok{    max\_N,}
\NormalTok{    color}\OperatorTok{=}\StringTok{"red"}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\StringTok{"Max Amplitude"}\NormalTok{,}
\NormalTok{    zorder}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{    s}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{)}

\NormalTok{plt.plot(}
\NormalTok{        X\_pred,}
\NormalTok{        ypred\_one\_mass\_oscillator,}
\NormalTok{        label}\OperatorTok{=}\StringTok{"Alpha"}\NormalTok{,}
\NormalTok{        color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{,}
\NormalTok{        linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    )}
\NormalTok{plt.scatter(}
\NormalTok{    resonant\_frequency,}
\NormalTok{    resonant\_amplitude,}
\NormalTok{    color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\StringTok{"Max Curve Fit"}\NormalTok{,}
\NormalTok{    zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    s}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{003_scipy_optimize_intro_files/figure-pdf/cell-35-output-1.pdf}}

\section{Exercises}\label{exercises}

\begin{exercise}[Nelder-Mead]\protect\hypertarget{exr-nelder-mead}{}\label{exr-nelder-mead}

~

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the steps of the Nelder-Mead algorithm?
\item
  What are the advantages and disadvantages of the Nelder-Mead
  algorithm?
\end{enumerate}

\end{exercise}

\begin{exercise}[Powell's
Method]\protect\hypertarget{exr-powell}{}\label{exr-powell}

~

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the steps of Powell's method?
\item
  What are the advantages and disadvantages of Powell's method?
\item
  What are similarities between the Nelder-Mead and Powell's methods?
\end{enumerate}

\end{exercise}

\begin{exercise}[Gradient
Descent]\protect\hypertarget{exr-cg}{}\label{exr-cg}

~

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the steps of the gradient descent algorithm?
\item
  What is the learning rate in the gradient descent algorithm?
\end{enumerate}

\end{exercise}

\begin{exercise}[Newton
Method]\protect\hypertarget{exr-newton}{}\label{exr-newton}

~

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the difference between the gradient descent and Newton method?
\item
  Which of the two methods converges faster?
\end{enumerate}

\end{exercise}

\begin{exercise}[BFGS]\protect\hypertarget{exr-bfgs}{}\label{exr-bfgs}

~

\begin{itemize}
\tightlist
\item
  In which situations is it possible to use algorithms like BFGS, but
  not the classical Newton method?
\item
  Would you choose Gradient Descent or BFGS for a large-scale
  optimization problem?
\end{itemize}

\end{exercise}

\begin{exercise}[Dual
Annealing]\protect\hypertarget{exr-simulated-annealing}{}\label{exr-simulated-annealing}

~

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When should you use Simulated Annealing or Dual Annealing over a local
  optimization algorithm?
\item
  Describe the Temperature parameter in Simulated Annealing.
\end{enumerate}

\end{exercise}

\begin{exercise}[Differential
Evolution]\protect\hypertarget{exr-global-optimization}{}\label{exr-global-optimization}

~

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the key steps in the Differential Evolution algorithm?
\item
  Explain the crossover operation in Differential Evolution.
\end{enumerate}

\end{exercise}

\section{Jupyter Notebook}\label{jupyter-notebook-2}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/003_scipy_optimize_intro.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{\texorpdfstring{Sequential Parameter Optimization: Using
\texttt{scipy}
Optimizers}{Sequential Parameter Optimization: Using scipy Optimizers}}\label{sec-scipy-optimizers}

As a default optimizer, \texttt{spotpython} uses
\texttt{differential\_evolution} from the \texttt{scipy.optimize}
package. Alternatively, any other optimizer from the
\texttt{scipy.optimize} package can be used. This chapter describes how
different optimizers from the \texttt{scipy\ optimize} package can be
used on the surrogate. The optimization algorithms are available from
\url{https://docs.scipy.org/doc/scipy/reference/optimize.html}

\phantomsection\label{imports}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ dual\_annealing}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ basinhopping}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, optimizer\_control\_init, surrogate\_control\_init}
\end{Highlighting}
\end{Shaded}

\section{The Objective Function
Branin}\label{the-objective-function-branin}

The \texttt{spotpython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula. Here we will use
the Branin function. The 2-dim Branin function is \[
y = a  (x_2 - b  x_1^2 + c  x_1 - r) ^2 + s  (1 - t)  \cos(x_1) + s,
\] where values of \(a\), \(b\), \(c\), \(r\), \(s\) and \(t\) are:
\(a = 1\), \(b = 5.1 / (4\pi^2)\), \(c = 5 / \pi\), \(r = 6\),
\(s = 10\) and \(t = 1 / (8\pi)\).

It has three global minima: \(f(x) = 0.397887\) at \((-\pi, 12.275)\),
\((\pi, 2.275)\), and \((9.42478, 2.475)\).

Input Domain: This function is usually evaluated on the square
\(x_1 \in  [-5, 10] \times x_2 \in  [0, 15]\).

\phantomsection\label{objective_function}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\end{Highlighting}
\end{Shaded}

\section{The Optimizer}\label{sec-optimizer}

Differential Evolution (DE) from the \texttt{scikit.optimize} package,
see
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html\#scipy.optimize.differential_evolution}
is the default optimizer for the search on the surrogate. Other
optimiers that are available in \texttt{spotpython}, see
\url{https://docs.scipy.org/doc/scipy/reference/optimize.html\#global-optimization}.

\begin{itemize}
\tightlist
\item
  \texttt{dual\_annealing}
\item
  \texttt{direct}
\item
  \texttt{shgo}
\item
  \texttt{basinhopping}
\end{itemize}

These optimizers can be selected as follows:

\phantomsection\label{optimizer_control}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ differential\_evolution}
\end{Highlighting}
\end{Shaded}

As noted above, we will use \texttt{differential\_evolution}. The
optimizer can use \texttt{1000} evaluations. This value will be passed
to the \texttt{differential\_evolution} method, which has the argument
\texttt{maxiter} (int). It defines the maximum number of generations
over which the entire differential evolution population is evolved, see
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html\#scipy.optimize.differential_evolution}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}]

Similar to the one-dimensional case, which is discussed in
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use a similar code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                    lower }\OperatorTok{=}\NormalTok{ lower,}
\NormalTok{                    upper }\OperatorTok{=}\NormalTok{ upper,}
\NormalTok{                    fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                    PREFIX }\OperatorTok{=} \StringTok{"04\_DE\_"}
\NormalTok{                    )}
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(}
\NormalTok{                    n\_theta}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(lower))}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_de.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.6961557165023953 [######----] 55.00% 
spotpython tuning: 3.483508988983779 [######----] 60.00% 
spotpython tuning: 3.0409502575998015 [######----] 65.00% 
spotpython tuning: 2.456350051306707 [#######---] 70.00% 
spotpython tuning: 2.4047922291552375 [########--] 75.00% 
spotpython tuning: 2.3739197901896123 [########--] 80.00% 
spotpython tuning: 2.355976671386384 [########--] 85.00% 
spotpython tuning: 2.348822784282971 [#########-] 90.00% 
spotpython tuning: 2.348822784282971 [##########] 95.00% 
spotpython tuning: 2.348822784282971 [##########] 100.00% Done...

Experiment saved to 04_DE__res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x107f917f0>
\end{verbatim}

\subsection{TensorBoard}\label{tensorboard-1}

If the \texttt{prefix} argument in \texttt{fun\_control\_init()}is not
\texttt{None} (as above, where the \texttt{prefix} was set to
\texttt{04\_DE\_}) , we can start TensorBoard in the background with the
following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotpython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/05_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotpython optimization
process and the surrogate model.}

\end{figure}%

\section{Print the Results}\label{print-the-results}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.348822784282971
x0: 3.1733035041022477
x1: 3.645429652665629
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(3.1733035041022477)], ['x1', np.float64(3.645429652665629)]]
\end{verbatim}

\section{Show the Progress}\label{show-the-progress}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-8-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_de.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-9-output-1.pdf}}

\section{Exercises}\label{exercises-1}

\subsection{\texorpdfstring{\texttt{dual\_annealing}}{dual\_annealing}}\label{dual_annealing}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm, see
  \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html}{scipy.optimize.dual\_annealing}.
\item
  Use the algorithm as an optimizer on the surrogate.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}]

We can run spotpython with the \texttt{dual\_annealing} optimizer as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_da }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{dual\_annealing,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_da.run()}
\NormalTok{spot\_da.print\_results()}
\NormalTok{spot\_da.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_da.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.696151364195096 [######----] 55.00% 
spotpython tuning: 3.483508760948294 [######----] 60.00% 
spotpython tuning: 3.0403979799851646 [######----] 65.00% 
spotpython tuning: 2.456351950973671 [#######---] 70.00% 
spotpython tuning: 2.4046755988763344 [########--] 75.00% 
spotpython tuning: 2.3764453024469283 [########--] 80.00% 
spotpython tuning: 2.362272749066122 [########--] 85.00% 
spotpython tuning: 2.361636390960074 [#########-] 90.00% 
spotpython tuning: 2.361636390960074 [##########] 95.00% 
spotpython tuning: 2.361636390960074 [##########] 100.00% Done...

Experiment saved to 04_DE__res.pkl
min y: 2.361636390960074
x0: 3.1767738440354223
x1: 3.646940192088105
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-10-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-10-output-3.pdf}}

\end{tcolorbox}

\subsection{\texorpdfstring{\texttt{direct}}{direct}}\label{direct-1}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}]

We can run spotpython with the \texttt{direct} optimizer as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_di }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{direct,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_di.run()}
\NormalTok{spot\_di.print\_results()}
\NormalTok{spot\_di.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_di.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.676976757805569 [######----] 55.00% 
spotpython tuning: 3.475991674101529 [######----] 60.00% 
spotpython tuning: 2.868437104503963 [######----] 65.00% 
spotpython tuning: 2.472089897497276 [#######---] 70.00% 
spotpython tuning: 2.3608759795941676 [########--] 75.00% 
spotpython tuning: 2.3020349459926672 [########--] 80.00% 
spotpython tuning: 2.2514916165157715 [########--] 85.00% 
spotpython tuning: 2.189644482495839 [#########-] 90.00% 
spotpython tuning: 2.04625864428877 [##########] 95.00% 
spotpython tuning: 1.642088997537579 [##########] 100.00% Done...

Experiment saved to 04_DE__res.pkl
min y: 1.642088997537579
x0: 3.076131687242797
x1: 3.4327846364883405
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-11-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-11-output-3.pdf}}

\end{tcolorbox}

\subsection{\texorpdfstring{\texttt{shgo}}{shgo}}\label{shgo-1}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}]

We can run spotpython with the \texttt{direct} optimizer as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_sh }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{shgo,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_sh.run()}
\NormalTok{spot\_sh.print\_results()}
\NormalTok{spot\_sh.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_sh.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.696158422862448 [######----] 55.00% 
spotpython tuning: 3.48351065347906 [######----] 60.00% 
spotpython tuning: 3.040368769129441 [######----] 65.00% 
spotpython tuning: 2.4564036280645185 [#######---] 70.00% 
spotpython tuning: 2.4047133036821 [########--] 75.00% 
spotpython tuning: 2.3763717663570914 [########--] 80.00% 
spotpython tuning: 2.3621941253656855 [########--] 85.00% 
spotpython tuning: 2.361459710666984 [#########-] 90.00% 
spotpython tuning: 2.361459710666984 [##########] 95.00% 
spotpython tuning: 2.361459710666984 [##########] 100.00% Done...

Experiment saved to 04_DE__res.pkl
min y: 2.361459710666984
x0: 3.176699445547513
x1: 3.6469433701030436
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-12-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-12-output-3.pdf}}

\end{tcolorbox}

\subsection{\texorpdfstring{\texttt{basinhopping}}{basinhopping}}\label{basinhopping}

\begin{itemize}
\tightlist
\item
  Describe the optimization algorithm
\item
  Use the algorithm as an optimizer on the surrogate
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: Selecting the Optimizer for the Surrogate}]

We can run spotpython with the \texttt{direct} optimizer as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_bh }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    optimizer}\OperatorTok{=}\NormalTok{basinhopping,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_bh.run()}
\NormalTok{spot\_bh.print\_results()}
\NormalTok{spot\_bh.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_bh.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.696197709339935 [######----] 55.00% 
spotpython tuning: 3.4835593225124644 [######----] 60.00% 
spotpython tuning: 3.0395966545106026 [######----] 65.00% 
spotpython tuning: 2.4563675121684057 [#######---] 70.00% 
spotpython tuning: 2.404442684372791 [########--] 75.00% 
spotpython tuning: 2.3759612272708743 [########--] 80.00% 
spotpython tuning: 2.361506968913666 [########--] 85.00% 
spotpython tuning: 2.3604186560858285 [#########-] 90.00% 
spotpython tuning: 2.3604186560858285 [##########] 95.00% 
spotpython tuning: 2.3604186560858285 [##########] 100.00% Done...

Experiment saved to 04_DE__res.pkl
min y: 2.3604186560858285
x0: 3.1764625539229465
x1: 3.646782335781278
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-13-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{004_spot_sklearn_optimization_files/figure-pdf/cell-13-output-3.pdf}}

\end{tcolorbox}

\subsection{Performance Comparison}\label{performance-comparison}

Compare the performance and run time of the 5 different optimizers:

\begin{itemize}
\tightlist
\item
  \texttt{differential\_evolution}
\item
  \texttt{dual\_annealing}
\item
  \texttt{direct}
\item
  \texttt{shgo}
\item
  \texttt{basinhopping}.
\end{itemize}

The Branin function has three global minima:

\begin{itemize}
\tightlist
\item
  \(f(x) = 0.397887\) at

  \begin{itemize}
  \tightlist
  \item
    \((-\pi, 12.275)\),
  \item
    \((\pi, 2.275)\), and
  \item
    \((9.42478, 2.475)\).\\
  \end{itemize}
\item
  Which optima are found by the optimizers?
\item
  Does the \texttt{seed} argument in
  \texttt{fun\ =\ Analytical(seed=123).fun\_branin} change this
  behavior?
\end{itemize}

\section{Jupyter Notebook}\label{jupyter-notebook-3}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this chapter is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/004_spot_sklearn_optimization.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\part{Numerical Methods}

\chapter{Introduction: Numerical
Methods}\label{introduction-numerical-methods}

This part deals with numerical implementations of optimization methods.
The goal is to understand the implementation of optimization methods and
to solve real-world problems numerically and efficiently. We will focus
on the implementation of surrogate models, because they are the most
efficient way to solve real-world problems.

Starting point is the well-established response surface methodology. It
will be extended to the design and analysis of computer experiments
(DACE). The DACE methodology is a modern extension of the response
surface methodology. It is based on the use of surrogate models, which
are used to replace the real-world problem with a simpler problem. The
simpler problem is then solved numerically. The solution of the simpler
problem is then used to solve the real-world problem.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Numerical methods: Goals}]

\begin{itemize}
\tightlist
\item
  Understand implementation of optimization methods
\item
  Solve real-world problems numerically and efficiently
\end{itemize}

\end{tcolorbox}

\section{Response Surface Methods: What is RSM?}\label{sec-rsm-intro}

Response Surface Methods (RSM) refer to a collection of statistical and
mathematical tools that are valuable for developing, improving, and
optimizing processes. The overarching theme of RSM involves studying how
input variables that control a product or process can potentially
influence a response that measures performance or quality
characteristics.

The advantages of RSM include a rich literature, well-established
methods often used in manufacturing, the importance of careful
experimental design combined with a well-understood model, and the
potential to add significant value to scientific inquiry, process
refinement, optimization, and more. However, there are also drawbacks to
RSM, such as the use of simple and crude surrogates, the hands-on nature
of the methods, and the limitation of local methods.

RSM is related to various fields, including Design of Experiments (DoE),
quality management, reliability, and productivity. Its applications are
widespread in industry and manufacturing, focusing on designing,
developing, and formulating new products and improving existing ones, as
well as from laboratory research. RSM is commonly applied in domains
such as materials science, manufacturing, applied chemistry, climate
science, and many others.

An example of RSM involves studying the relationship between a response
variable, such as yield (\(y\)) in a chemical process, and two process
variables: reaction time (\(\xi_1\)) and reaction temperature
(\(\xi_2\)). The provided code illustrates this scenario, following a
variation of the so-called ``banana function.''

In the context of visualization, RSM offers the choice between 3D plots
and contour plots. In a 3D plot, the independent variables \(\xi_1\) and
\(\xi_2\) are represented, with \(y\) as the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\KeywordTok{def}\NormalTok{ fun\_rosen(x1, x2):}
\NormalTok{    b }\OperatorTok{=} \DecValTok{10}
    \ControlFlowTok{return}\NormalTok{ (x1}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ b}\OperatorTok{*}\NormalTok{(x2}\OperatorTok{{-}}\NormalTok{x1}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zs }\OperatorTok{=}\NormalTok{ np.array(fun\_rosen(np.ravel(X), np.ravel(Y)))}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ zs.reshape(X.shape)}

\NormalTok{ax.plot\_surface(X, Y, Z)}

\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}X1\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}X2\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-2-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  contour plot example:

  \begin{itemize}
  \tightlist
  \item
    \(x_1\) and \(x_2\) are the independent variables
  \item
    \(y\) is the dependent variable
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_rosen(X1, X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y , }\DecValTok{50}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Rosenbrock\textquotesingle{}s Banana Function"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, "Rosenbrock's Banana Function")
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-3-output-2.pdf}}

\begin{itemize}
\tightlist
\item
  Visual inspection: yield is optimized near \((\xi_1. \xi_2)\)
\end{itemize}

\subsection{Visualization: Problems in
Practice}\label{visualization-problems-in-practice}

\begin{itemize}
\tightlist
\item
  True response surface is unknown in practice
\item
  When yield evaluation is not as simple as a toy banana function, but a
  process requiring care to monitor, reconfigure and run, it's far too
  expensive to observe over a dense grid
\item
  And, measuring yield may be a noisy/inexact process
\item
  That's where stats (RSM) comes in
\end{itemize}

\subsection{RSM: Strategies}\label{rsm-strategies}

\begin{itemize}
\item
  RSMs consist of experimental strategies for
\item
  \textbf{exploring} the space of the process (i.e., independent/input)
  variables (above \(\xi_1\) and \(\xi2)\)
\item
  empirical statistical \textbf{modeling} targeted toward development of
  an appropriate approximating relationship between the response (yield)
  and process variables local to a study region of interest
\item
  \textbf{optimization} methods for sequential refinement in search of
  the levels or values of process variables that produce desirable
  responses (e.g., that maximize yield or explain variation)
\item
  RSM used for fitting an Empirical Model
\item
  True response surface driven by an unknown physical mechanism
\item
  Observations corrupted by noise
\item
  Helpful: fit an empirical model to output collected under different
  process configurations
\item
  Consider response \(Y\) that depends on controllable input variables
  \(\xi_1, \xi_2, \ldots, \xi_m\)
\item
  RSM: Equations of the Empirical Model

  \begin{itemize}
  \tightlist
  \item
    \(Y=f(\xi_1, \xi_2, \ldots, \xi_m) + \epsilon\)
  \item
    \(\mathbb{E}\{Y\} = \eta = f(\xi1_1, \xi_2, \ldots, \xi_m)\)
  \item
    \(\epsilon\) is treated as zero mean idiosyncratic noise possibly
    representing

    \begin{itemize}
    \tightlist
    \item
      inherent variation, or
    \item
      the effect of other systems or
    \item
      variables not under our purview at this time
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{RSM: Noise in the Empirical
Model}\label{rsm-noise-in-the-empirical-model}

\begin{itemize}
\tightlist
\item
  Typical simplifying assumption: \(\epsilon \sim N(0,\sigma^2)\)
\item
  We seek estimates for \(f\) and \(\sigma^2\) from noisy observations
  \(Y\) at inputs \(\xi\)
\end{itemize}

\subsection{RSM: Natural and Coded
Variables}\label{rsm-natural-and-coded-variables}

\begin{itemize}
\tightlist
\item
  Inputs \(\xi_1, \xi_2, \ldots, \xi_m\) called \textbf{natural
  variables}:

  \begin{itemize}
  \tightlist
  \item
    expressed in natural units of measurement, e.g., degrees Celsius,
    pounds per square inch (psi), etc.
  \end{itemize}
\item
  Transformed to \textbf{coded variables} \(x_1, x_2, \ldots, x_m\):

  \begin{itemize}
  \tightlist
  \item
    to mitigate hassles and confusion that can arise when working with a
    multitude of scales of measurement
  \end{itemize}
\item
  Typical \textbf{Transformations} offering dimensionless inputs
  \(x_1, x_2, \ldots, x_m\)

  \begin{itemize}
  \tightlist
  \item
    in the unit cube, or
  \item
    scaled to have a mean of zero and standard deviation of one, are
    common choices.
  \end{itemize}
\item
  Empirical model becomes \(\eta = f(x_1, x_2, \ldots, x_m)\)
\end{itemize}

\subsection{RSM Low-order Polynomials}\label{rsm-low-order-polynomials}

\begin{itemize}
\tightlist
\item
  Low-order polynomial make the following simplifying Assumptions

  \begin{itemize}
  \tightlist
  \item
    Learning about \(f\) is lots easier if we make some simplifying
    approximations
  \item
    Appealing to \textbf{Taylor's theorem}, a low-order polynomial in a
    small, localized region of the input (\(x\)) space is one way
    forward
  \item
    Classical RSM:

    \begin{itemize}
    \tightlist
    \item
      disciplined application of \textbf{local analysis} and
    \item
      \textbf{sequential refinement} of locality through conservative
      extrapolation
    \end{itemize}
  \item
    Inherently a \textbf{hands-on process}
  \end{itemize}
\end{itemize}

\section{First-Order Models (Main Effects
Model)}\label{first-order-models-main-effects-model}

\begin{itemize}
\tightlist
\item
  \textbf{First-order model} (sometimes called main effects model)
  useful in parts of the input space where it's believed that there's
  little curvature in \(f\):
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \]
\item
  For example: \[\eta = 50 + 8 x_1 + 3x_2\]
\item
  In practice, such a surface would be obtained by fitting a model to
  the outcome of a designed experiment
\item
  First-Order Model in python Evaluated on a Grid
\item
  Evaluate model on a grid in a double-unit square centered at the
  origin
\item
  Coded units are chosen arbitrarily, although one can imagine deploying
  this approximating function nearby \(x^{(0)} = (0,0)\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_1(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_1(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model: $50 + 8x\_1 + 3x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model: $50 + 8x_1 + 3x_2$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-5-output-2.pdf}}

\subsection{First-Order Model
Properties}\label{first-order-model-properties}

\begin{itemize}
\tightlist
\item
  First-order model in 2d traces out a \textbf{plane} in
  \(y \times (x_1, x_2)\) space
\item
  Only be appropriate for the most trivial of response surfaces, even
  when applied in a highly localized part of the input space
\item
  Adding \textbf{curvature} is key to most applications:

  \begin{itemize}
  \tightlist
  \item
    First-order model with \textbf{interactions} induces limited degree
    of curvature via different rates of change of \(y\) as \(x_1\) is
    varied for fixed \(x_2\), and vice versa:
    \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_{12} \]
  \end{itemize}
\item
  For example \(\eta = 50+8x_1+3x_2-4x_1x_2\)
\end{itemize}

\subsection{First-order Model with Interactions in
python}\label{first-order-model-with-interactions-in-python}

\begin{itemize}
\tightlist
\item
  Code below facilitates evaluations for pairs \((x_1, x_2)\)
\item
  Responses may be observed over a mesh in the same double-unit square
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_11(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_11(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model with Interactions\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model with Interactions')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-7-output-2.pdf}}

\subsection{Observations: First-Order Model with
Interactions}\label{observations-first-order-model-with-interactions}

\begin{itemize}
\tightlist
\item
  Mean response \(\eta\) is increasing marginally in both \(x_1\) and
  \(x_2\), or conditional on a fixed value of the other until \(x_1\) is
  0.75
\item
  Rate of increase slows as both coordinates grow simultaneously since
  the coefficient in front of the interaction term \(x_1 x_2\) is
  negative
\item
  Compared to the first-order model (without interactions): surface is
  far more useful locally
\item
  Least squares regressions often flag up significant interactions when
  fit to data collected on a design far from local optima
\end{itemize}

\section{Second-Order Models}\label{second-order-models}

\begin{itemize}
\item
  Second-order model may be appropriate near local optima where \(f\)
  would have substantial curvature:
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \beta_{11}x_1^2 + \beta_{22}x^2 + \beta_{12} x_1 x_2\]
\item
  For example \[\eta = 50 + 8 x_1 + 3x_2 - 7x_1^2 - 3 x_2^2 - 4x_1x_2\]
\item
  Implementation of the Second-Order Model as \texttt{fun\_2()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_2(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{7} \OperatorTok{*}\NormalTok{ x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_2(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Second Order Model with Interactions. Maximum near about $(0.6,0.2)$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-9-output-2.pdf}}

\subsection{Second-Order Models:
Properties}\label{second-order-models-properties}

\begin{itemize}
\tightlist
\item
  Not all second-order models would have a single stationary point (in
  RSM jargon called ``a simple maximum'')
\item
  In ``yield maximizing'' setting we're presuming response surface is
  \textbf{concave} down from a global viewpoint

  \begin{itemize}
  \tightlist
  \item
    even though local dynamics may be more nuanced
  \end{itemize}
\item
  Exact criteria depend upon the eigenvalues of a certain matrix built
  from those coefficients
\item
  Box and Draper (2007) provide a diagram categorizing all of the kinds
  of second-order surfaces in RSM analysis, where finding local maxima
  is the goal
\end{itemize}

\subsection{Example: Stationary Ridge}\label{example-stationary-ridge}

\begin{itemize}
\tightlist
\item
  Example set of coefficients describing what's called a
  \textbf{stationary ridge} is provided by the code below
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Example of a stationary ridge\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Example of a stationary ridge')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-11-output-2.pdf}}

\subsection{Observations: Second-Order Model
(Ridge)}\label{observations-second-order-model-ridge}

\begin{itemize}
\tightlist
\item
  \textbf{Ridge}: a whole line of stationary points corresponding to
  maxima
\item
  Situation means that the practitioner has some flexibility when it
  comes to optimizing:

  \begin{itemize}
  \tightlist
  \item
    can choose the precise setting of \((x_1, x_2)\) either arbitrarily
    or (more commonly) by consulting some tertiary criteria
  \end{itemize}
\end{itemize}

\subsection{Example: Rising Ridge}\label{example-rising-ridge}

\begin{itemize}
\tightlist
\item
  An example of a rising ridge is implemented by the code below.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge\_rise(x1, x2):}
     \ControlFlowTok{return} \DecValTok{80} \OperatorTok{{-}} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{12}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge\_rise(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Rising ridge: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 3x\_1\^{}2 {-} 12x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Rising ridge: $\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-13-output-2.pdf}}

\subsection{Summary: Rising Ridge}\label{summary-rising-ridge}

\begin{itemize}
\tightlist
\item
  The stationary point is remote to the study region
\item
  Ccontinuum of (local) stationary points along any line going through
  the 2d space, excepting one that lies directly on the ridge
\item
  Although estimated response will increase while moving along the axis
  of symmetry toward its stationary point, this situation indicates

  \begin{itemize}
  \tightlist
  \item
    either a poor fit by the approximating second-order function, or
  \item
    that the study region is not yet precisely in the vicinity of a
    local optima---often both.
  \end{itemize}
\end{itemize}

\subsection{Falling Ridge}\label{falling-ridge}

\begin{itemize}
\tightlist
\item
  Inversion of a rising ridge is a falling ridge
\item
  Similarly indicating one is far from local optima, except that the
  response decreases as you move toward the stationary point
\item
  Finding a falling ridge system can be a back-to-the-drawing-board
  affair.
\end{itemize}

\subsection{Saddle Point}\label{saddle-point}

\begin{itemize}
\tightlist
\item
  Finally, we can get what's called a saddle or minimax system.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_saddle(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_saddle(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Saddle Point: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 2x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Saddle Point: $\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-15-output-2.pdf}}

\subsection{Interpretation: Saddle
Points}\label{interpretation-saddle-points}

\begin{itemize}
\tightlist
\item
  Likely further data collection, and/or outside expertise, is needed
  before determining a course of action in this situation
\end{itemize}

\subsection{Summary: Ridge Analysis}\label{summary-ridge-analysis}

\begin{itemize}
\tightlist
\item
  Finding a simple maximum, or stationary ridge, represents ideals in
  the spectrum of second-order approximating functions
\item
  But getting there can be a bit of a slog
\item
  Using models fitted from data means uncertainty due to noise, and
  therefore uncertainty in the type of fitted second-order model
\item
  A ridge analysis attempts to offer a principled approach to navigating
  uncertainties when one is seeking local maxima
\item
  The two-dimensional setting exemplified above is convenient for
  visualization, but rare in practice
\item
  Complications compound when studying the effect of more than two
  process variables
\end{itemize}

\section{General RSM Models}\label{general-rsm-models}

\begin{itemize}
\tightlist
\item
  General \textbf{first-order model} on \(m\) process variables
  \(x_1, x_2, \cdots, x_m\) is
  \[\eta = \beta_0 + \beta_1x_1 + \cdots + \beta_m x_m\]
\item
  General \textbf{second-order model} on \(m\) process variables \[
  \eta= \beta_0 + \sum_{j=1}^m + \sum_{j=1}^m x_j^2 + \sum_{j=2}^m \sum_{k=1}^j \beta_{kj}x_k x_j.
  \]
\end{itemize}

\subsection{Ordinary Least Squares}\label{ordinary-least-squares}

\begin{itemize}
\tightlist
\item
  Inference from data is carried out by \textbf{ordinary least squares}
  (OLS)
\item
  For an excellent review including R examples, see Sheather (2009)
\item
  OLS and maximum likelihood estimators (MLEs) are in the typical
  Gaussian linear modeling setup basically equivalent
\end{itemize}

\section{General Linear Regression}\label{general-linear-regression}

We are considering a model, which can be written in the form

\[
Y = X \beta + \epsilon,
\] where \(Y\) is an \((n \times 1)\) vector of observations
(responses), \(X\) is an \((n \times p)\) matrix of known form,
\(\beta\) is a \((1 \times p)\) vector of unknown parameters, and
\(\epsilon\) is an \((n \times 1)\) vector of errors. Furthermore,
\(E(\epsilon) = 0\), \(Var(\epsilon) = \sigma^2 I\) and the
\(\epsilon_i\) are uncorrelated.

Using the normal equations \[
(X'X)b = X'Y,
\]

the solution is given by

\[
b = (X'X)^{-1}X'Y.
\]

\begin{example}[Linear
Regression]\protect\hypertarget{exm-ols}{}\label{exm-ols}

~

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{n }\OperatorTok{=} \DecValTok{8}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(X, }\DecValTok{2}\NormalTok{))}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(X)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(y, }\DecValTok{2}\NormalTok{))}
\CommentTok{\# fit an OLS model to the data, predict the response based on the 1ÃÃ x values}
\NormalTok{m }\OperatorTok{=} \DecValTok{100}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, m, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{model.fit(X, y)}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model.predict(x)}
\CommentTok{\# visualize the data and the fitted model}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.scatter(X, y, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(x, y\_pred, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\CommentTok{\# add the ground truth (sine function) in orange}
\NormalTok{plt.plot(x, np.sin(x), color}\OperatorTok{=}\StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.  ]
 [0.79]
 [1.57]
 [2.36]
 [3.14]
 [3.93]
 [4.71]
 [5.5 ]]
[[ 0.  ]
 [ 0.71]
 [ 1.  ]
 [ 0.71]
 [ 0.  ]
 [-0.71]
 [-1.  ]
 [-0.71]]
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/linreg-example-output-2.pdf}}

\end{example}

\section{Designs}\label{designs}

\begin{itemize}
\tightlist
\item
  Important: Organize the data collection phase of a response surface
  study carefully
\item
  \textbf{Design}: choice of \(x\)'s where we plan to observe \(y\)'s,
  for the purpose of approximating \(f\)
\item
  Analyses and designs need to be carefully matched
\item
  When using a first-order model, some designs are preferred over others
\item
  When using a second-order model to capture curvature, a different sort
  of design is appropriate
\item
  Design choices often contain features enabling modeling assumptions to
  be challenged

  \begin{itemize}
  \tightlist
  \item
    e.g., to check if initial impressions are supported by the data
    ultimately collected
  \end{itemize}
\end{itemize}

\subsection{Different Designs}\label{different-designs}

\begin{itemize}
\tightlist
\item
  \textbf{Screening desings}: determine which variables matter so that
  subsequent experiments may be smaller and/or more focused
\item
  Then there are designs tailored to the form of model (first- or
  second-order, say) in the screened variables
\item
  And then there are more designs still
\end{itemize}

\section{RSM Experimentation}\label{rsm-experimentation}

\subsection{First Step}\label{first-step}

\begin{itemize}
\tightlist
\item
  RSM-based experimentation begins with a \textbf{first-order model},
  possibly with interactions
\item
  Presumption: current process operating \textbf{far from optimal}
  conditions
\item
  Collect data and apply \textbf{method of steepest ascent} (gradient)
  on fitted surfaces to move to the optimum
\end{itemize}

\subsection{Second Step}\label{second-step}

\begin{itemize}
\tightlist
\item
  Eventually, if all goes well after several such carefully iterated
  refinements, \textbf{second-order models} are used on appropriate
  designs in order to zero-in on ideal operating conditions
\item
  Careful analysis of the fitted surface:

  \begin{itemize}
  \tightlist
  \item
    Ridge analysis with further refinement using gradients of, and
  \item
    standard errors associated with, the fitted surfaces, and so on
  \end{itemize}
\end{itemize}

\subsection{Third Step}\label{third-step}

\begin{itemize}
\tightlist
\item
  Once the practitioner is satisfied with the full arc of

  \begin{itemize}
  \tightlist
  \item
    design(s),
  \item
    fit(s), and
  \item
    decision(s):
  \end{itemize}
\item
  A small experiment called \textbf{confirmation test} may be performed
  to check if the predicted optimal settings are realizable in practice
\end{itemize}

\section{RSM: Review and General
Considerations}\label{rsm-review-and-general-considerations}

\begin{itemize}
\item
  First Glimpse, RSM seems sensible, and pretty straightforward as
  quantitative statistics-based analysis goes
\item
  But: RSM can get complicated, especially when input dimensions are not
  very low
\item
  Design considerations are particularly nuanced, since the goal is to
  obtain reliable estimates of main effects, interaction, and curvature
  while minimizing sampling effort/expense
\item
  RSM Downside: Inefficiency

  \begin{itemize}
  \tightlist
  \item
    Despite intuitive appeal, several RSM downsides become apparent upon
    reflection
  \item
    Problems in practice
  \item
    Stepwise nature of sequential decision making is inefficient:

    \begin{itemize}
    \tightlist
    \item
      Not obvious how to re-use or update analysis from earlier phases,
      or couple with data from other sources/related experiments
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Locality

  \begin{itemize}
  \tightlist
  \item
    In addition to being local in experiment-time (stepwise approach),
    it's local in experiment-space
  \item
    Balance between

    \begin{itemize}
    \tightlist
    \item
      exploration (maybe we're barking up the wrong tree) and
    \item
      exploitation (let's make things a little better) is modest at best
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Interjection of expert knowledge is limited to hunches about
    relevant variables (i.e., the screening phase), where to initialize
    search, how to design the experiments
  \item
    Yet at the same time classical RSMs rely heavily on constant
    examination throughout stages of modeling and design and on the
    instincts of seasoned practitioners
  \end{itemize}
\item
  RSM Downside: Replicability

  \begin{itemize}
  \tightlist
  \item
    Parallel analyses, conducted according to the same best intentions,
    rarely lead to the same designs, model fits and so on
  \item
    Sometimes that means they lead to different conclusions, which can
    be cause for concern
  \end{itemize}
\end{itemize}

\subsection{Historical Considerations about
RSM}\label{historical-considerations-about-rsm}

\begin{itemize}
\tightlist
\item
  In spite of those criticisms, however, there was historically little
  impetus to revise the status quo
\item
  Classical RSM was comfortable in its skin, consistently led to
  improvements or compelling evidence that none can reasonably be
  expected
\item
  But then in the late 20th century came an explosive expansion in
  computational capability, and with it a means of addressing many of
  those downsides
\end{itemize}

\subsection{Status Quo}\label{status-quo}

\begin{itemize}
\tightlist
\item
  Nowadays, field experiments and statistical models, designs and
  optimizations are coupled with with mathematical models
\item
  Simple equations are not regarded as sufficient to describe real-world
  systems anymore
\item
  Physicists figured that out fifty years ago; industrial engineers
  followed, biologists, social scientists, climate scientists and
  weather forecasters, etc.
\item
  Systems of equations are required, solved over meshes (e.g., finite
  elements), or stochastically interacting agents
\item
  Goals for those simulation experiments are as diverse as their
  underlying dynamics
\item
  Optimization of systems is common, e.g., to identify worst-case
  scenarios
\end{itemize}

\subsection{The Role of Statistics}\label{the-role-of-statistics}

\begin{itemize}
\tightlist
\item
  Solving systems of equations, or interacting agents, requires
  computing
\item
  Statistics involved at various stages:

  \begin{itemize}
  \tightlist
  \item
    choosing the mathematical model
  \item
    solving by stochastic simulation (Monte Carlo)
  \item
    designing the computer experiment
  \item
    smoothing over idiosyncrasies or noise
  \item
    finding optimal conditions, or
  \item
    calibrating mathematical/computer models to data from field
    experiments
  \end{itemize}
\end{itemize}

\subsection{New RSM is needed: DACE}\label{new-rsm-is-needed-dace}

\begin{itemize}
\tightlist
\item
  Classical RSMs are not well-suited to any of those tasks, because

  \begin{itemize}
  \tightlist
  \item
    they lack the fidelity required to model these data
  \item
    their intended application is too local
  \item
    they're also too hands-on.
  \end{itemize}
\item
  Once computers are involved, a natural inclination is to automate---to
  remove humans from the loop and set the computer running on the
  analysis in order to maximize computing throughput, or minimize idle
  time
\item
  \textbf{Design and Analysis of Computer Experiments} as a modern
  extension of RSM
\item
  Experimentation is changing due to advances in machine learning
\item
  \textbf{Gaussian process} (GP) regression is the canonical surrogate
  model
\item
  Origins in geostatistics (gold mining)
\item
  Wide applicability in contexts where prediction is king
\item
  Machine learners exposed GPs as powerful predictors for all sorts of
  tasks:
\item
  from regression to classification,
\item
  active learning/sequential design,
\item
  reinforcement learning and optimization,
\item
  latent variable modeling, and so on
\end{itemize}

\section{Exercises}\label{exercises-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate 3d Plots for the Contour Plots in this notebook.
\item
  Write a \texttt{plot\_3d} function, that takes the objective function
  \texttt{fun} as an argument.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface: \texttt{plot\_3d(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Write a \texttt{plot\_contour} function, that takes the objective
  function \texttt{fun} as an argument:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface:
  \texttt{plot\_contour(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Consider further arguments that might be useful for both function,
  e.g., ranges, size, etc.
\end{enumerate}

\section{Jupyter Notebook}\label{jupyter-notebook-4}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/005_num_rsm.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Kriging (Gaussian Process
Regression)}\label{kriging-gaussian-process-regression}

\section{DACE and RSM}\label{dace-and-rsm}

Mathematical models implemented in computer codes are used to circumvent
the need for expensive field data collection. These models are
particularly useful when dealing with highly nonlinear response
surfaces, high signal-to-noise ratios (which often involve deterministic
evaluations), and a global scope. As a result, a new approach is
required in comparison to Response Surface Methodology (RSM), which was
discussed in Section~\ref{sec-rsm-intro}.

With the improvement in computing power and simulation fidelity,
researchers gain higher confidence and a better understanding of the
dynamics in physical, biological, and social systems. However, the
expansion of configuration spaces and increasing input dimensions
necessitates more extensive designs. High-performance computing (HPC)
allows for thousands of runs, whereas previously only tens were
possible. This shift towards larger models and training data presents
new computational challenges.

Research questions for DACE (Design and Analysis of Computer
Experiments) include how to design computer experiments that make
efficient use of computation and how to meta-model computer codes to
save on simulation effort. The choice of surrogate model for computer
codes significantly impacts the optimal experiment design, and the
preferred model-design pairs can vary depending on the specific goal.

The combination of computer simulation, design, and modeling with field
data from similar real-world experiments introduces a new category of
computer model tuning problems. The ultimate goal is to automate these
processes to the greatest extent possible, allowing for the deployment
of HPC with minimal human intervention.

One of the remaining differences between RSM and DACE lies in how they
handle noise. DACE employs replication, a technique that would not be
used in a deterministic setting, to separate signal from noise.
Traditional RSM is best suited for situations where a substantial
proportion of the variability in the data is due to noise, and where the
acquisition of data values can be severely limited. Consequently, RSM is
better suited for a different class of problems, aligning with its
intended purposes.

Two very good texts on computer experiments and surrogate modeling are
Santner, Williams, and Notz (2003) and Forrester, SÃ³bester, and Keane
(2008). The former is the canonical reference in the statistics
literature and the latter is perhaps more popular in engineering.

\begin{example}[Example: DACE and
RSM]\protect\hypertarget{exm-dace-rsm}{}\label{exm-dace-rsm}

Imagine you are a chemical engineer tasked with optimizing a chemical
process to maximize yield. You can control temperature and pressure, but
repeated experiments show variability in yield due to inconsistencies in
raw materials.

\begin{itemize}
\item
  Using RSM: You would use RSM to design a series of experiments varying
  temperature and pressure. You would then fit a response surface (a
  mathematical model) to the data, helping you understand how changes in
  temperature and pressure affect yield. Using this model, you can
  identify optimal conditions for maximizing yield despite the noise.
\item
  Using DACE: If instead you use a computational model to simulate the
  chemical process and want to account for numerical noise or
  uncertainty in model parameters, you might use DACE. You would run
  simulations at different conditions, possibly repeating them to assess
  variability and build a surrogate model that accurately predicts
  yields, which can be optimized to find the best conditions.
\end{itemize}

\end{example}

\subsection{Noise Handling in RSM and
DACE}\label{noise-handling-in-rsm-and-dace}

Noise in RSM: In experimental settings, noise often arises due to
variability in experimental conditions, measurement errors, or other
uncontrollable factors. This noise can significantly affect the response
variable, \(Y\). Replication is a standard procedure for handling noise
in RSM. In the context of computer experiments, noise might not be
present in the traditional sense since simulations can be deterministic.
However, variability can arise from uncertainty in input parameters or
model inaccuracies. DACE predominantly utilizes advanced interpolation
to construct accurate models of deterministic data, sometimes
considering statistical noise modeling if needed.

\section{Data Types and Precision in
Python}\label{data-types-and-precision-in-python}

The float16 data type in numpy represents a half-precision floating
point number. It uses 16 bits of memory, which gives it a precision of
about 3 decimal digits.

The float32 data type in numpy represents a single-precision floating
point number. It uses 32 bits of memory, which gives it a precision of
about 7 decimal digits. On the other hand, float64 represents a
double-precision floating point number. It uses 64 bits of memory, which
gives it a precision of about 15 decimal digits.

The reason float16 and float32 show fewer digits is because it has less
precision due to using less memory. The bits of memory are used to store
the sign, exponent, and fraction parts of the floating point number, and
with fewer bits, you can represent fewer digits accurately.

\begin{example}[16 versus 32 versus 64
bit]\protect\hypertarget{exm-float}{}\label{exm-float}

~

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a number}
\NormalTok{num }\OperatorTok{=} \FloatTok{0.123456789123456789}

\NormalTok{num\_float16 }\OperatorTok{=}\NormalTok{ np.float16(num)}
\NormalTok{num\_float32 }\OperatorTok{=}\NormalTok{ np.float32(num)}
\NormalTok{num\_float64 }\OperatorTok{=}\NormalTok{ np.float64(num)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"float16: "}\NormalTok{, num\_float16) }
\BuiltInTok{print}\NormalTok{(}\StringTok{"float32: "}\NormalTok{, num\_float32)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"float64: "}\NormalTok{, num\_float64)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
float16:  0.1235
float32:  0.12345679
float64:  0.12345678912345678
\end{verbatim}

\end{example}

\section{Cholesky Decomposition and Positive Definite
Matrices}\label{cholesky-decomposition-and-positive-definite-matrices}

We consider the definiteness of a matrix, before discussing the Cholesky
decomposition.

\begin{definition}[Positive Definite
Matrix]\protect\hypertarget{def-positive-definite}{}\label{def-positive-definite}

A symmetric matrix \(A\) is positive definite if all its eigenvalues are
positive.

\end{definition}

\begin{example}[Positive Definite
Matrix]\protect\hypertarget{exm-positive-definite}{}\label{exm-positive-definite}

Given a symmetric matrix
\(A = \begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix}\), the eigenvalues of
\(A\) are \(\lambda_1 = 13\) and \(\lambda_2 = 5\). Since both
eigenvalues are positive, the matrix \(A\) is positive definite.

\end{example}

\begin{definition}[Negative Definite, Positive Semidefinite, and
Negative Semidefinite
Matrices]\protect\hypertarget{def-negative-definite}{}\label{def-negative-definite}

Similarily, a symmetric matrix \(A\) is negative definite if all its
eigenvalues are negative. It is positive semidefinite if all its
eigenvalues are non-negative, and negative semidefinite if all its
eigenvalues are non-positive.

\end{definition}

The covariance matrix must be positive definite for a multivariate
normal distribution for a couple of reasons:

\begin{itemize}
\tightlist
\item
  Semidefinite vs Definite: A covariance matrix is always symmetric and
  positive semidefinite. However, for a multivariate normal
  distribution, it must be positive definite, not just semidefinite.
  This is because a positive semidefinite matrix can have zero
  eigenvalues, which would imply that some dimensions in the
  distribution have zero variance, collapsing the distribution in those
  dimensions. A positive definite matrix has all positive eigenvalues,
  ensuring that the distribution has positive variance in all
  dimensions.
\item
  Invertibility: The multivariate normal distribution's probability
  density function involves the inverse of the covariance matrix. If the
  covariance matrix is not positive definite, it may not be invertible,
  and the density function would be undefined.
\end{itemize}

In summary, the covariance matrix being positive definite ensures that
the multivariate normal distribution is well-defined and has positive
variance in all dimensions.

The definiteness of a matrix can be checked by examining the eigenvalues
of the matrix. If all eigenvalues are positive, the matrix is positive
definite.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ is\_positive\_definite(matrix):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{all}\NormalTok{(np.linalg.eigvals(matrix) }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(is\_positive\_definite(matrix))  }\CommentTok{\# Outputs: True}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

However, a more efficient way to check the definiteness of a matrix is
through the Cholesky decomposition.

\begin{definition}[Cholesky
Decomposition]\protect\hypertarget{def-cholesky-decomposition}{}\label{def-cholesky-decomposition}

For a given symmetric positive-definite matrix
\(A \in \mathbb{R}^{n \times n}\), there exists a unique lower
triangular matrix \(L \in \mathbb{R}^{n \times n}\) with positive
diagonal elements such that:

\[
A = L L^T.
\]

Here, \(L^T\) denotes the transpose of \(L\).

\end{definition}

\begin{example}[Cholesky decomposition using
\texttt{numpy}]\protect\hypertarget{exm-cholesky-decomposition}{}\label{exm-cholesky-decomposition}

\texttt{linalg.cholesky} computes the Cholesky decomposition of a
matrix, i.e., it computes a lower triangular matrix \(L\) such that
\(LL^T = A\). If the matrix is not positive definite, an error
(\texttt{LinAlgError}) is raised.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a Hermitian, positive{-}definite matrix}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]]) }

\CommentTok{\# Compute the Cholesky decomposition}
\NormalTok{L }\OperatorTok{=}\NormalTok{ np.linalg.cholesky(A)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"L = }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, L)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L*LT = }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.dot(L, L.T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
L = 
 [[3.         0.        ]
 [1.33333333 2.68741925]]
L*LT = 
 [[9. 4.]
 [4. 9.]]
\end{verbatim}

\end{example}

\begin{example}[Cholesky
Decomposition]\protect\hypertarget{exm-cholesky-decomposition}{}\label{exm-cholesky-decomposition}

Given a symmetric positive-definite matrix
\(A = \begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix}\), the Cholesky
decomposition computes the lower triangular matrix \(L\) such that
\(A = L L^T\). The matrix \(L\) is computed as: \[
L = \begin{pmatrix} 3 & 0 \\ 4/3 & 2 \end{pmatrix},
\] so that \[
L L^T = \begin{pmatrix} 3 & 0 \\ 4/3 & \sqrt{65}/3 \end{pmatrix} \begin{pmatrix} 3 & 4/3 \\ 0 & \sqrt{65}/3 \end{pmatrix} = \begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix} = A.
\]

\end{example}

An efficient implementation of the definiteness-check based on Cholesky
is already available in the \texttt{numpy} library. It provides the
\texttt{np.linalg.cholesky} function to compute the Cholesky
decomposition of a matrix. This more efficient \texttt{numpy}-approach
can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ is\_pd(K):}
    \ControlFlowTok{try}\NormalTok{:}
\NormalTok{        np.linalg.cholesky(K)}
        \ControlFlowTok{return} \VariableTok{True}
    \ControlFlowTok{except}\NormalTok{ np.linalg.linalg.LinAlgError }\ImportTok{as}\NormalTok{ err:}
        \ControlFlowTok{if} \StringTok{\textquotesingle{}Matrix is not positive definite\textquotesingle{}} \KeywordTok{in}\NormalTok{ err.message:}
            \ControlFlowTok{return} \VariableTok{False}
        \ControlFlowTok{else}\NormalTok{:}
            \ControlFlowTok{raise}
\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(is\_pd(matrix))  }\CommentTok{\# Outputs: True}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\section{Constructing a Surrogate}\label{constructing-a-surrogate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

This section is based on chapter 2 in Forrester, SÃ³bester, and Keane
(2008).

\end{tcolorbox}

\begin{definition}[Black Box
Problem]\protect\hypertarget{def-black-box}{}\label{def-black-box}

We are trying to learn a mapping that converts the vector \(\mathbf{x}\)
into a scalar output \(y\), i.e., we are trying to learn a function \[
y = f(x).
\] If function is hidden (``lives in a black box''), so that the physics
of the problem is not known, the problem is called a black box problem.

\end{definition}

This black box could take the form of either a physical or computer
experiment, for example, a finite element code, which calculates the
maximum stress (\(\sigma\)) for given product dimensions
(\(\mathbf{x}\)).

\begin{definition}[Generic
Solution]\protect\hypertarget{def-generic-solution}{}\label{def-generic-solution}

The generic solution method is to collect the output values \(y^{(1)}\),
\(y^{(2)}\), \ldots, \(y^{(n)}\) that result from a set of inputs
\(\mathbf{x}^{(1)}\), \(\mathbf{x}^{(2)}\), \ldots, \(\mathbf{x}^{(n)}\)
and find a best guess \(\hat{f}(\mathbf{x})\) for the black box mapping
\(f\), based on these known observations.

\end{definition}

\subsection{Stage One: Preparing the Data and Choosing a Modelling
Approach}\label{stage-one-preparing-the-data-and-choosing-a-modelling-approach}

The first step is the identification, through a small number of
observations, of the inputs that have a significant impact on \(f\);
that is the determination of the shortest design variable vector
\(\mathbf{x} = \{x_1, x_2, \ldots, x_k\}^T\) that, by sweeping the
ranges of all of its variables, can still elicit most of the behavior
the black box is capable of. The ranges of the various design variables
also have to be established at this stage.

The second step is to recruit \(n\) of these \(k\)-vectors into a list
\[
\mathbf{X} = \{ \mathbf{x}^{(1)},\mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(n)} \}^T,
\] where each \(\mathbf{x}^{(i)}\) is a \(k\)-vector. The corresponding
responses are collected in a vector such that this represents the design
space as thoroughly as possible.

In the surrogate modeling process, the number of samples \(n\) is often
limited, as it is constrained by the computational cost (money and/or
time) associated with obtaining each observation.

It is advisable to scale \(\mathbf{x}\) at this stage into the unit cube
\([0, 1]^k\), a step that can simplify the subsequent mathematics and
prevent multidimensional scaling issues.

We now focus on the attempt to learn \(f\) through data pairs \[
\{ (\mathbf{x}^{(1)}, y^{(1)}), (\mathbf{x}^{(2)}, y^{(2)}), \ldots, (\mathbf{x}^{(n)}, y^{(n)}) \}.
\]

This supervised learning process essentially involves searching across
the space of possible functions \(\hat{f}\) that would replicate
observations of \(f\). This space of functions is infinite. Any number
of hypersurfaces could be drawn to pass through or near the known
observations, accounting for experimental error. However, most of these
would generalize poorly; they would be practically useless at predicting
responses at new sites, which is the ultimate goal.

\begin{example}[The Needle(s) in the Haystack
Function]\protect\hypertarget{exm-needle-haystack}{}\label{exm-needle-haystack}

An extreme example is the `needle(s) in the haystack' function:

\[
f(x) = \begin{cases} 
y^{(1)}, & \text{if } x = \mathbf{x}^{(1)} \\
y^{(2)}, & \text{if } x = \mathbf{x}^{(2)} \\
\vdots & \\
y^{(n)}, & \text{if } x = \mathbf{x}^{(n)} \\
0, & \text{otherwise.}
\end{cases}
\]

While this predictor reproduces all training data, it seems
counter-intuitive and unsettling to predict 0 everywhere else for most
engineering functions. Although there is a small chance that the
function genuinely resembles the equation above and we sampled exactly
where the needles are, it is highly unlikely.

\end{example}

There are countless other configurations, perhaps less contrived, that
still generalize poorly. This suggests a need for systematic means to
filter out nonsensical predictors. In our approach, we embed the
structure of \(f\) into the model selection algorithm and search over
its parameters to fine-tune the approximation to observations. For
instance, consider one of the simplest models,
\begin{equation}\phantomsection\label{eq-linear-model-simple}{
f(x, \mathbf{w}) = \mathbf{w}^T\mathbf{x} + v.
}\end{equation} Learning \(f\) with this model implies that its
structure---a hyperplane---is predetermined, and the fitting process
involves finding the \(k + 1\) parameters (the slope vector
\(\mathbf{w}\) and the intercept \(v\)) that best fit the data. This
will be accomplished in Stage Two.

Complicating this further is the noise present in observed responses (we
assume design vectors \(\mathbf{x}\) are not corrupted). Here, we focus
on learning from such data, which sometimes risks overfitting.

\begin{definition}[Overfitting]\protect\hypertarget{def-overfitting}{}\label{def-overfitting}

Overfitting occurs when the model becomes too flexible and captures not
only the underlying trend but also the noise in the data.

\end{definition}

In the surrogate modeling process, the second stage as described in
Section~\ref{sec-stage-two}, addresses this issue of complexity control
by estimating the parameters of the fixed structure model. However,
foresight is necessary even at the model type selection stage.

Model selection often involves physics-based considerations, where the
modeling technique is chosen based on expected underlying responses.

\begin{example}[Model
Selection]\protect\hypertarget{exm-model-selection}{}\label{exm-model-selection}

Modeling stress in an elastically deformed solid due to small strains
may justify using a simple linear approximation. Without insights into
the physics, and if one fails to account for the simplicity of the data,
a more complex and excessively flexible model may be incorrectly chosen.
Although parameter estimation might still adjust the approximation to
become linear, an opportunity to develop a simpler and robust model may
be lost.

\begin{itemize}
\tightlist
\item
  Simple linear (or polynomial) models, despite their lack of
  flexibility, have advantages like applicability in further symbolic
  computations.
\item
  Conversely, if we incorrectly assume a quadratic process when multiple
  peaks and troughs exist, the parameter estimation stage will not
  compensate for an unsuitable model choice. A quadratic model is too
  rigid to fit a multimodal function, regardless of parameter
  adjustments.
\end{itemize}

\end{example}

\subsection{Stage Two: Parameter Estimation and
Training}\label{sec-stage-two}

Assuming that Stage One helped identify the \(k\) critical design
variables, acquire the learning data set, and select a generic model
structure \(f(\mathbf{x}, \mathbf{w})\), the task now is to estimate
parameters \(\mathbf{w}\) to ensure the model fits the data optimally.
Among several estimation criteria, we will discuss two methods here.

\begin{definition}[Maximum Likelihood
Estimation]\protect\hypertarget{def-mle}{}\label{def-mle}

Given a set of parameters \(\mathbf{w}\), the model
\(f(\mathbf{x}, \mathbf{w})\) allows computation of the probability of
the data set \[
\{(\mathbf{x}^{(1)}, y^{(1)} \pm \epsilon), (\mathbf{x}^{(2)}, y^{(2)} \pm \epsilon), \ldots, (\mathbf{x}^{(n)}, y^{(n)} \pm \epsilon)\}
\] resulting from \(f\) (where \(\epsilon\) is a small error margin
around each data point).

Taking Equation~\ref{eq-likelihood-mvn} and assuming errors \(\epsilon\)
are independently and normally distributed with standard deviation
\(\sigma\), the probability of the data set is given by:

\[
P = \frac{1}{(2\pi \sigma^2)^{n/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^{n} \left( y^{(i)} - f(\mathbf{x}^{(i)}, \mathbf{w}) \right)^2 \epsilon \right].
\]

Intuitively, this is equivalent to the likelihood of the parameters
given the data. Accepting this intuitive relationship as a mathematical
one aids in model parameter estimation. This is achieved by maximizing
the likelihood or, more conveniently, minimizing the negative of its
natural logarithm:

\begin{equation}\phantomsection\label{eq-forr23}{ 
\min_{\mathbf{w}} \sum_{i=1}^{n} \frac{[y^{(i)} - f(\mathbf{x}^{(i)}, \mathbf{w})]^2}{2\sigma^2} + \frac{n}{2} \ln \epsilon .
}\end{equation}

\end{definition}

If we assume \(\sigma\) and \(\epsilon\) are constants,
Equation~\ref{eq-forr23} simplifies to the well-known least squares
criterion:

\[ 
\min_{\mathbf{w}} \sum_{i=1}^{n} [y^{(i)} - f(\mathbf{x}^{(i)}, \mathbf{w})]^2 . 
\]

Cross-validation is another method used to estimate model performance.

\begin{definition}[Cross-Validation]\protect\hypertarget{def-cross-validation}{}\label{def-cross-validation}

Cross-validation splits the data randomly into \(q\) roughly equal
subsets, and then cyclically removing each subset and fitting the model
to the remaining \(q - 1\) subsets. A loss function \(L\) is then
computed to measure the error between the predictor and the withheld
subset for each iteration, with contributions summed over all \(q\)
iterations. More formally, if a mapping
\(\theta: \{1, \ldots, n\} \to \{1, \ldots, q\}\) describes the
allocation of the \(n\) training points to one of the \(q\) subsets and
\(f^{(-\theta(i))}(\mathbf{x})\) is the predicted value by removing the
subset \(\theta(i)\) (i.e., the subset where observation \(i\) belongs),
the cross-validation measure, used as an estimate of prediction error,
is:

\begin{equation}\phantomsection\label{eq-cv-basis}{ 
CV = \frac{1}{n} \sum_{i=1}^{n} L(y^{(i)}, f^{(-\theta(i))}(\mathbf{x}^{(i)})) . 
}\end{equation}

\end{definition}

Introducing the squared error as the loss function and considering our
generic model \(f\) still dependent on undetermined parameters, we write
Equation~\ref{eq-cv-basis} as:

\begin{equation}\phantomsection\label{eq-cv-sse}{ 
CV = \frac{1}{n} \sum_{i=1}^{n} [y^{(i)} - f^{(-\theta(i))}(\mathbf{x}^{(i)})]^2 .
}\end{equation}

The extent to which Equation~\ref{eq-cv-sse} is an unbiased estimator of
true risk depends on \(q\). It is shown that if \(q = n\), the
leave-one-out cross-validation (LOOCV) measure is almost unbiased.
However, LOOCV can have high variance because subsets are very similar.
Hastie, Tibshirani, and Friedman (2017)) suggest using compromise values
like \(q = 5\) or \(q = 10\). Using fewer subsets also reduces the
computational cost of the cross-validation process, see also Arlot,
Celisse, et al. (2010) and Kohavi (1995).

\subsection{Stage Three: Model Testing}\label{stage-three-model-testing}

If there is a sufficient amount of observational data, a random subset
should be set aside initially for model testing. Hastie, Tibshirani, and
Friedman (2017) recommend setting aside approximately \(0.25n\) of
\(\mathbf{x} \rightarrow y\) pairs for testing purposes. These
observations must remain untouched during Stages One and Two, as their
sole purpose is to evaluate the testing error---the difference between
true and approximated function values at the test sites---once the model
has been built. Interestingly, if the main goal is to construct an
initial surrogate for seeding a global refinement criterion-based
strategy (as discussed in Section 3.2 in Forrester, SÃ³bester, and Keane
(2008)), the model testing phase might be skipped.

It is noted that, ideally, parameter estimation (Stage Two) should also
rely on a separate subset. However, observational data is rarely
abundant enough to afford this luxury (if the function is cheap to
evaluate and evaluation sites are selectable, a surrogate model might
not be necessary).

When data are available for model testing and the primary objective is a
globally accurate model, using either a root mean square error (RMSE)
metric or the correlation coefficient (\(r^2\)) is recommended. To test
the model, a test data set of size \(n_t\) is used alongside predictions
at the corresponding locations to calculate these metrics.

The RMSE is defined as follows:

\begin{definition}[Root Mean Square Error
(RMSE)]\protect\hypertarget{def-rmse}{}\label{def-rmse}

\[
\text{RMSE} = \sqrt{\frac{1}{n_t} \sum_{i=1}^{n_t} (y^{(i)} - \hat{y}^{(i)})^2}, 
\]

\end{definition}

Ideally, the RMSE should be minimized, acknowledging its limitation by
errors in the objective function \(f\) calculation. If the error level
is known, like a standard deviation, the aim might be to achieve an RMSE
within this value. Often, the target is an RMSE within a specific
percentage of the observed data's objective value range.

The squared correlation coefficient \(r\), see
Equation~\ref{eq-pears-corr}, between the observed \(y\) and predicted
\(\hat{y}\) values can be computed as:

\begin{equation}\phantomsection\label{eq-r2}{ 
r^2 = \left( \frac{\text{cov}(y, \hat{y})}{\sqrt{\text{var}(y)\text{var}(\hat{y})}} \right)^2, 
}\end{equation}

Equation~\ref{eq-r2} and can be expanded as:

\[ 
r^2 = 
\left(
\frac{n_t \sum_{i=1}^{n_t} y^{(i)} \hat{y}^{(i)} - \sum_{i=1}^{n_t} y^{(i)} \sum_{i=1}^{n_t} \hat{y}^{(i)}}{ \sqrt{\left( n_t \sum_{i=1}^{n_t} (y^{(i)})^2 - \left(\sum_{i=1}^{n_t} y^{(i)}\right)^2 \right) \left( n_t \sum_{i=1}^{n_t} (\hat{y}^{(i)})^2 - \left(\sum_{i=1}^{n_t} \hat{y}^{(i)}\right)^2 \right)}}
\right)^2.
\]

The correlation coefficient \(r^2\) does not require scaling the data
sets and only compares landscape shapes, not values. An \(r^2 > 0.8\)
typically indicates a surrogate with good predictive capability.

The methods outlined provide quantitative assessments of model accuracy,
yet visual evaluations can also be insightful. In general, the RMSE will
not reach zero but will stabilize around a low value. At this point, the
surrogate model is saturated with data, and further additions do not
enhance the model globally (though local improvements can occur at newly
added points if using an interpolating model).

\begin{example}[The Tea and Sugar
Analogy]\protect\hypertarget{exm-tea-sugar}{}\label{exm-tea-sugar}

Forrester, SÃ³bester, and Keane (2008) illustrates this saturation point
using a comparision with a cup of tea and sugar. The tea represents the
surrogate model, and sugar represents data. Initially, the tea is
unsweetened, and adding sugar increases its sweetness. Eventually, a
saturation point is reached where no more sugar dissolves, and the tea
cannot get any sweeter. Similarly, a more flexible model, like one with
additional parameters or employing interpolation rather than regression,
can increase the saturation point---akin to making a hotter cup of tea
for dissolving more sugar.

\end{example}

\section{Sampling Plans}\label{sampling-plans-1}

\begin{definition}[Sampling
Plan]\protect\hypertarget{def-sampling-plan}{}\label{def-sampling-plan}

In the context of computer experiments, the term ``sampling plan''
refers to the set of input values at which the computer code is
evaluated.

\end{definition}

The goal of a sampling plan is to efficiently explore the input space to
understand the behavior of the computer code and build a surrogate model
that accurately represents the code's behavior. Traditionally, Response
Surface Methodology (RSM) has been used to design sampling plans for
computer experiments. These sampling plans are based on procedures that
generate points by means of a rectangiular grid or a factorial design.
However, more recently, Design and Analysis of Computer Experiments
(DACE) has emerged as a more flexible and powerful approach for
designing sampling plans. \texttt{spotpython} uses a class for
generating space-filling designs using Latin Hypercube Sampling (LHS)
and maximin distance criteria. It is based on \texttt{scipy}'s
\texttt{LatinHypercube} class. The following example demonstrates how to
generate a Latin Hypercube Sampling design using \texttt{spotpython}.
The result is shown in Figure~\ref{fig-lhs-spotpython}. As can seen in
the figure, a Latin hypercube sample generates \(n\) points in
\([0,1)^{d}\). Each univariate marginal distribution is stratified,
placing exactly one point in \([j/n, (j+1)/n)\) for \(j=0,1,...,n-1\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\NormalTok{lhd }\OperatorTok{=}\NormalTok{ SpaceFilling(k}\OperatorTok{=}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ lhd.scipy\_lhd(n}\OperatorTok{=}\DecValTok{10}\NormalTok{, repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{, lower}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]), upper}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{]))}
\NormalTok{plt.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{])}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-lhs-spotpython-output-1.pdf}}

}

\caption{\label{fig-lhs-spotpython}Latin Hypercube Sampling design
(sampling plan)}

\end{figure}%

\section{Kriging}\label{kriging}

\subsection{The Kriging Idea in a
Nutshell}\label{the-kriging-idea-in-a-nutshell}

Kriging can be applied to planned experiments, where the design is based
on a sampling plan as shown in Figure~\ref{fig-lhs-spotpython}, as well
as to computer experiments, where the design is based on the computer
code's input space, as shown in Figure~\ref{fig-unknownf}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-unknownf-output-1.pdf}}

}

\caption{\label{fig-unknownf}Eight measurements of an unknown function.
No sampling plan was used.}

\end{figure}%

Kriging can be explained using the concept of radial basis functions.

Radial basis functions (RBFs) are a class of functions used in various
types of interpolation and approximation tasks. An RBF is a real-valued
function whose value depends only on the distance from a certain point,
called the center, usually in a multidimensional space. This distance is
typically measured using the Euclidean distance.

\subsection{Radial Basis Function
(RBF)}\label{radial-basis-function-rbf}

Mathematically, a radial basis function \(\phi\) can be expressed as:

\[
\phi(\mathbf{x}) = \phi(\|\mathbf{x} - \mathbf{c}\|),
\] where \(\mathbf{x}\) is the input vector, \(\mathbf{c}\) is the
center of the function, and \(\|\mathbf{x} - \mathbf{c}\|\) denotes the
Euclidean distance between \(\mathbf{x}\) and \(\mathbf{c}\).

Common types of radial basis functions include:

\begin{itemize}
\tightlist
\item
  Linear: \(\phi(r) = r\).
\item
  Cubic: \(\phi(r) = r^3\),
\item
  Gaussian: \(\phi(r) = e^{-(\epsilon r)^2}\),
\end{itemize}

where \(r\) is the distance and \(\epsilon\) is a parameter that
determines the width of the Gaussian function.

Radial basis functions are widely used in interpolation for scattered
data points in multiple dimensions, in machine learning models such as
radial basis function networks, and in numerical solutions to partial
differential equations due to their smooth approximation properties.

In general, we onsider observed data of an unknown function \(f\) at
\(n\) points \(x_1, \ldots, x_n\). These measurements a considered as
realizations of MVN random variables \(Y_1, \ldots, Y_n\) with mean
\(\mu\) and covariance matrix \(\Sigma_n\) as shown in
Figure~\ref{fig-mvn1-3}, Figure~\ref{fig-mvn2} or Figure~\ref{fig-mvn5}.

In Kriging, a more general covariance matrix (or equivalently, a
correlation matrix \(\Psi\)) is used, see Equation~\ref{eq-krigingbase}.
Using a maximum likelihood approach, we can estimate the unknown
parameters \(\mu\) and \(\Sigma_n\) from the data so that the likelihood
function is maximized.

\begin{definition}[The Kriging Basis
Functions]\protect\hypertarget{def-kriging-basis-function}{}\label{def-kriging-basis-function}

Kriging uses \(k\)-dimensional basis functions of the form
\begin{equation}\phantomsection\label{eq-krigingbase}{
\psi(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp \left( - \sum_{l=1}^k \theta_l | x_{l}^{(i)} - x_{l}^{(j)} | ^{p_l} \right),
}\end{equation} where \(\vec{x}^{(i)}\) denotes the \(k\)-dim vector
\(\vec{x}^{(i)}= (x_1^{(i)}, \ldots, x_k^{(i)})^T\).

\end{definition}

\subsection{The Kriging Model}\label{the-kriging-model}

Consider sample data \(\vec{X}\) and \(\vec{y}\) from \(n\) locations
that are available in matrix form: \(\vec{X}\) is a \((n \times k)\)
matrix, where \(k\) denotes the problem dimension and \(\vec{y}\) is a
\((n\times 1)\) vector. We want to find an expression for a predicted
values at a new point \(\vec{x}\), denoted as \(\hat{y}\).

We start with an abstract, not really intuitive concept: The observed
responses \(\vec{y}\) are considered as if they are from a stochastic
process, which will be denoted as
\begin{equation}\phantomsection\label{eq-yvec-51}{
\begin{pmatrix}
\vec{Y}(\vec{x}^{(1)})\\
\vdots\\
\vec{Y}(\vec{x}^{(n)})\\
\end{pmatrix}.
}\end{equation}

The set of random vectors from Equation~\ref{eq-yvec-51} (also referred
to as a \emph{random field}) has a mean of \(\vec{1} \mu\), which is a
\((n\times 1)\) vector. The random vectors are correlated with each
other using the basis function expression from
Equation~\ref{eq-krigingbase}:
\begin{equation}\phantomsection\label{eq-corr-kriging-51}{
\text{cor} \left(\vec{Y}(\vec{x}^{(i)}),\vec{Y}(\vec{x}^{(l)}) \right) = \exp\left\{ - \sum_{j=1}^k \theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\right\}.
}\end{equation} Using Equation~\ref{eq-corr-kriging-51}, we can compute
the \((n \times n)\) correlation matrix \(\vec{\Psi}\) of the observed
sample data as shown in Equation~\ref{eq-corr-matrix-kriging-51},

\begin{equation}\phantomsection\label{eq-corr-matrix-kriging-51}{
\vec{\Psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(1)}),
\vec{Y}(\vec{x}^{(1)}) 
\right) & \ldots &
\text{cor}\left(
\vec{Y}(\vec{x}^{(1)}),
\vec{Y}(\vec{x}^{(n)}) 
\right)\\
\vdots  & \vdots &  \vdots\\
 \text{cor}\left(
\vec{Y}(\vec{x}^{(n)}),
\vec{Y}(\vec{x}^{(1)}) 
\right)&
\ldots &
\text{cor}\left(
\vec{Y}(\vec{x}^{(n)}),
\vec{Y}(\vec{x}^{(n)}) 
\right)
\end{pmatrix},
}\end{equation}

and a covariance matrix as shown in
Equation~\ref{eq-cov-matrix-kriging-52},

\begin{equation}\phantomsection\label{eq-cov-matrix-kriging-52}{
\text{Cov}(\mathbf{Y}, \mathbf{Y} ) = \sigma^2 \mathbf{\Psi}.
}\end{equation}

This assumed correlation between the sample data reflects our
expectation that an engineering function will behave in a certain way
and it will be smoothly and continuous.

We now have a set of \(n\) random variables (\(\mathbf{Y}\)) that are
correlated with each other as described in the \((n \times n)\)
correlation matrix \(\mathbf{\Psi}\), see
Equation~\ref{eq-corr-matrix-kriging-51}. The correlations depend on the
absolute distances in dimension \(j\) between the \(i\)-th and the
\(l\)-th sample point \(|x_j^{(i)} - x_j^{(l)}|\) and the corresponding
parameters \(p_j\) and \(\theta_j\) for dimension \(j\). The correlation
is intuitive, because when

\begin{itemize}
\tightlist
\item
  two points move close together, then \(|x_j^{(i)} - x_j| \to 0\) and
  \(\exp \left(-|x_j^{(i)} - x_j|^{p_j} \right) \to 1\) (these points
  show very close correlation and \(Y(x_j^{(i)}) = Y(x_j)\)).
\item
  two points move far apart, then \(|x_j^{(i)} - x_j| \to \infty\) and
  \(\exp \left(-|x_j^{(i)} - x_j|^{p_j} \right) \to 0\) (these points
  show very low correlation).
\end{itemize}

\begin{example}[Correlations for Different
\(p_j\)]\protect\hypertarget{exm-kriging-corr-1}{}\label{exm-kriging-corr-1}

Three different correlations are shown in Figure~\ref{fig-pval12}:
\(p_j= 0.1, 1, 2\). The smoothness parameter \(p_j\) affects the
correlation:

\begin{itemize}
\tightlist
\item
  With \(p_j=0.1\), there is basicaly no immediate correlation between
  the points and there is a near discontinuity between the points
  \(Y(\vec{x}_j^{(i)})\) and \(Y(\vec{x}_j)\).
\item
  With \(p_j=2\), the correlation is more smooth and we have a
  continuous gradient through \(x_j^{(i)} - x_j\).
\end{itemize}

Reducing \(p_j\) increases the rate at which the correlation initially
drops with distance. This is shown in Figure~\ref{fig-pval12}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-pval12-output-1.pdf}}

}

\caption{\label{fig-pval12}Correlations with varying \(\theta\).
\(\theta\) set to 1/10, 1, and 10.}

\end{figure}%

\end{example}

\begin{example}[Correlations for Different
\(\theta\)]\protect\hypertarget{exm-kriging-corr-2}{}\label{exm-kriging-corr-2}

Figure~\ref{fig-theta12} visualizes the correlation between two points
\(Y(\vec{x}_j^{(i)})\) and \(Y(\vec{x}_j)\) for different values of
\(\theta\). The parameter \(\theta\) can be seen as a width parameter:

\begin{itemize}
\tightlist
\item
  low \(\theta_j\) means that all points will have a high correlation,
  with \(Y(x_j)\) being similar across the sample.
\item
  high \(\theta_j\) means that there is a significant difference between
  the \(Y(x_j)\)'s.
\item
  \(\theta_j\) is a measure of how active the function we are
  approximating is.
\item
  High \(\theta_j\) indicate important parameters, see
  Figure~\ref{fig-theta12}.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-theta12-output-1.pdf}}

}

\caption{\label{fig-theta12}Correlations with varying \(\theta\).
\(\theta\) set to 1/10, 1, and 10.}

\end{figure}%

\end{example}

Considering the activity parameter \(\theta\) is useful in
high-dimensional problems where it is difficult to visualize the design
landscape and the effect of the variable is unknown. By examining the
elements of the vector \(\vec{\theta}\), we can identify the most
important variables and focus on them. This is a crucial step in the
optimization process, as it allows us to reduce the dimensionality of
the problem and focus on the most important variables.

\begin{example}[Example: The Correlation Matrix (Detailed
Computation)]\protect\hypertarget{exm-corr-matrix-detailed}{}\label{exm-corr-matrix-detailed}

Let \(n=4\) and \(k=3\). The sample plan is represented by the following
matrix \(X\): \[
X = \begin{pmatrix} x_{11} & x_{12} & x_{13}\\
x_{21} & x_{22} & x_{23}\\
x_{31} & x_{32} & x_{33}\\
x_{41} & x_{42} & x_{43}\\ 
\end{pmatrix}
\]

To compute the elements of the matrix \(\Psi\), the following \(k\) (one
for each of the \(k\) dimensions) \((n,n)\)-matrices have to be
computed: \[
D_1 = \begin{pmatrix} x_{11} - x_{11} & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  x_{21} - x_{11} & x_{21} - x_{21} & x_{21} -x_{31} & x_{21} - x_{41} \\ x_{31} - x_{11} & x_{31} - x_{21} & x_{31} -x_{31} & x_{31} - x_{41} \\ x_{41} - x_{11} & x_{41} - x_{21} & x_{41} -x_{31} & x_{41} - x_{41} \\
\end{pmatrix}
\]

\[
D_2 = \begin{pmatrix} x_{12} - x_{12} & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  x_{22} - x_{12} & x_{22} - x_{22} & x_{22} -x_{32} & x_{22} - x_{42} \\ x_{32} - x_{12} & x_{32} - x_{22} & x_{32} -x_{32} & x_{32} - x_{42} \\ x_{42} - x_{12} & x_{42} - x_{22} & x_{42} -x_{32} & x_{42} - x_{42} \\
\end{pmatrix}
\]

\[
D_3 = \begin{pmatrix} x_{13} - x_{13} & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  x_{23} - x_{13} & x_{23} - x_{23} & x_{23} -x_{33} & x_{23} - x_{43} \\ x_{33} - x_{13} & x_{33} - x_{23} & x_{33} -x_{33} & x_{33} - x_{43} \\ x_{43} - x_{13} & x_{43} - x_{23} & x_{43} -x_{33} & x_{43} - x_{43} \\\end{pmatrix}
\]

Since the matrices are symmetric and the main diagonals are zero, it is
sufficient to compute the following matrices: \[
D_1 = \begin{pmatrix} 0 & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  0 &  0 & x_{21} -x_{31} & x_{21} - x_{41} \\ 0 & 0 & 0 & x_{31} - x_{41} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\] \[
D_2 = \begin{pmatrix} 0 & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  0 & 0 & x_{22} -x_{32} & x_{22} - x_{42} \\ 0 & 0 & 0 & x_{32} - x_{42} \\ 0 & 0 & 0 & 0 \\
\end{pmatrix}
\]

\[
D_3 = \begin{pmatrix} 0 & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  0 & 0 & x_{23} -x_{33} & x_{23} - x_{43} \\ 0 & 0 & 0 & x_{33} - x_{43} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

We will consider \(p_l=2\). The differences will be squared and
multiplied by \(\theta_i\), i.e.:

\[
D_1 = \theta_1 \begin{pmatrix} 0 & (x_{11} - x_{21})^2 & (x_{11} -x_{31})^2 & (x_{11} - x_{41})^2 \\  0 &  0 & (x_{21} -x_{31})^2 & (x_{21} - x_{41})^2 \\ 0 & 0 & 0 & (x_{31} - x_{41})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

\[
D_2 = \theta_2 \begin{pmatrix} 0 & (x_{12} - x_{22})^2 & (x_{12} -x_{32})^2 & (x_{12} - x_{42})^2 \\  0 & 0 & (x_{22} -x_{32})^2 & (x_{22} - x_{42})^2 \\ 0 & 0 & 0 & (x_{32} - x_{42})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

\[
D_3 = \theta_3 \begin{pmatrix} 0 & (x_{13} - x_{23})^2 & (x_{13} -x_{33})^2 & (x_{13} - x_{43})^2 \\  0 & 0 & (x_{23} -x_{33})^2 & (x_{23} - x_{43})^2 \\ 0 & 0 & 0 & (x_{33} - x_{43})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

The sum of the three matrices \(D=D_1+ D_2 + D_3\) will be calculated
next:

\[
\begin{pmatrix} 0 & 
\theta_1  (x_{11} - x_{21})^2 + \theta_2 (x_{12} - x_{22})^2 + \theta_3  (x_{13} - x_{23})^2  &
\theta_1 (x_{11} -x_{31})^2 + \theta_2  (x_{12} -x_{32})^2 + \theta_3  (x_{13} -x_{33})^2 &
\theta_1  (x_{11} - x_{41})^2 + \theta_2  (x_{12} - x_{42})^2 + \theta_3 (x_{13} - x_{43})^2
\\  0 &  0 & 
\theta_1  (x_{21} -x_{31})^2 + \theta_2 (x_{22} -x_{32})^2 + \theta_3  (x_{23} -x_{33})^2 &
\theta_1  x_{21} - x_{41})^2 + \theta_2  (x_{22} - x_{42})^2 + \theta_3 (x_{23} - x_{43})^2
\\ 0 & 0 & 0 & 
\theta_1 (x_{31} - x_{41})^2 + \theta_2 (x_{32} - x_{42})^2 + \theta_3 (x_{33} - x_{43})^2
\\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

Finally, \[ \Psi = \exp(-D)\] is computed.

Next, we will demonstrate how this computation can be implemented in
Python. We will consider four points in three dimensions and compute the
correlation matrix \(\Psi\) using the basis function from
Equation~\ref{eq-krigingbase}. These points are placed at the origin, at
the unit vectors, and at the points \((100, 100, 100)\) and
\((101, 100, 100)\). So, they form two clusters: one at the origin and
one at \((100, 100, 100)\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ (array, zeros, power, ones, exp, multiply,}
\NormalTok{                    eye, linspace, spacing, sqrt, arange,}
\NormalTok{                    append, ravel)}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([ [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{], [}\DecValTok{101}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{]])}
\NormalTok{X}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[  1,   0,   0],
       [  0,   1,   0],
       [100, 100, 100],
       [101, 100, 100]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_Psi(X, theta):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=}\NormalTok{ theta[l]}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])
\end{verbatim}

\end{example}

\begin{example}[Example: The Correlation Matrix (Using Existing
Functions)]\protect\hypertarget{exm-corr-matrix-existing}{}\label{exm-corr-matrix-existing}

The same result as computed in the previous example can be obtained with
existing python functions, e.g., from the package \texttt{scipy}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ squareform}
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ pdist}

\KeywordTok{def}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{ squareform(pdist(X,}
\NormalTok{                            metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{,}
\NormalTok{                            out}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{                            w}\OperatorTok{=}\NormalTok{theta))) }\OperatorTok{+}\NormalTok{  multiply(eye(X.shape[}\DecValTok{0}\NormalTok{]),}
\NormalTok{                                                   eps)}

\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\FloatTok{.0}\NormalTok{)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])
\end{verbatim}

\end{example}

\subsection{The Condition Number}\label{the-condition-number}

A small value, \texttt{eps}, can be passed to the function
\texttt{build\_Psi} to improve the condition number. For example,
\texttt{eps=sqrt(spacing(1))} can be used. The numpy function
\texttt{spacing()} returns the distance between a number and its nearest
adjacent number.

The condition number of a matrix is a measure of its sensitivity to
small changes in its elements. It is used to estimate how much the
output of a function will change if the input is slightly altered.

A matrix with a low condition number is well-conditioned, which means
its behavior is relatively stable, while a matrix with a high condition
number is ill-conditioned, meaning its behavior is unstable with respect
to numerical precision.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a well{-}conditioned matrix (low condition number)}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{], [}\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Condition number of A: "}\NormalTok{, np.linalg.cond(A))}

\CommentTok{\# Define an ill{-}conditioned matrix (high condition number)}
\NormalTok{B }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\FloatTok{0.99999999}\NormalTok{], [}\FloatTok{0.99999999}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Condition number of B: "}\NormalTok{, np.linalg.cond(B))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Condition number of A:  1.2222222222222225
Condition number of B:  200000000.57495335
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.linalg.cond(Psi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(2.163953413738652)
\end{verbatim}

\subsection{\texorpdfstring{MLE to estimate \(\theta\) and
\(p\)}{MLE to estimate \textbackslash theta and p}}\label{mle-to-estimate-theta-and-p}

Until now, the observed data \(\vec{y}\) was not used. We know what the
correlations mean, but how do we estimate the values of \(\theta_j\) and
where does our observed data \(y\) come in? To estimate the values of
\(\vec{\theta}\) and \(\vec{p}\), they are chosen to maximize the
likelihood of \(\vec{y}\), which can be expressed in terms of the sample
data
\[L\left(\vec{Y}(\vec{x}^{(1)}), \ldots, \vec{Y}(\vec{x}^{(n)}) | \mu, \sigma \right) = \frac{1}{(2\pi \sigma)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left\{ \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}\right\},\]
and formulated as the log-likelihood:
\begin{equation}\phantomsection\label{eq-loglikelihood-55}{
\ln(L) = - \frac{n}{2} \ln(2\pi \sigma) - \frac{1}{2} \ln |\vec{\Psi}| \frac{-(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}.
}\end{equation}

Optimization of the log-likelihood by taking derivatives with respect to
\(\mu\) and \(\sigma\) results in
\begin{equation}\phantomsection\label{eq-muhat-55}{
\hat{\mu} = \frac{\vec{1}^T \vec{\Psi}^{-1} \vec{y}^T}{\vec{1}^T \vec{\Psi}^{-1} \vec{1}^T}
}\end{equation} and
\begin{equation}\phantomsection\label{eq-sigmahat-55}{
\hat{\sigma}^2 = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{n}.
}\end{equation}

Combining the equations, i.e., substituting Equation~\ref{eq-muhat-55}
and Equation~\ref{eq-sigmahat-55} inti
Equation~\ref{eq-loglikelihood-55} leads to the concentrated
log-likelihood function:
\begin{equation}\phantomsection\label{eq-concentrated-loglikelihood}{
\ln(L) = - \frac{n}{2} \ln(\hat{\sigma}) - \frac{1}{2} \ln |\vec{\Psi}|.
}\end{equation}

\begin{refremark}[The Concentrated Log-Likelihood]
\leavevmode

\begin{itemize}
\tightlist
\item
  The first term in Equation~\ref{eq-concentrated-loglikelihood}
  requires information about the measured point (observations) \(y_i\).
\item
  To maximize \(\ln(L)\), optimal values of \(\vec{\theta}\) and
  \(\vec{p}\) are determined numerically, because the function
  (Equation~\ref{eq-concentrated-loglikelihood}) is not differentiable.
\end{itemize}

\label{rem-concentrated-loglikelihood}

\end{refremark}

The concentrated log-likelihood function is very quick to compute. We do
not need a statistical model, because we are only interested in the
maximum likelihood estimate (MLE) of \(\theta\) and \(p\). Optimizers
such as Nelder-Mead, Conjugate Gradient, or Simulated Annealing can be
used to determine optimal values for \(\theta\) and \(p\). After the
optimization, the correlation matrix \(\Psi\) is build with the
optimized \(\theta\) and \(p\) values. This is best (most likely)
Kriging model for the given data \(y\).

Observing Figure~\ref{fig-theta12}, there's significant change between
\(\theta = 0.1\) and \(\theta = 1\), just as there is between
\(\theta = 1\) and \(\theta = 10\). Hence, it is sensible to search for
\(\theta\) on a logarithmic scale. Suitable search bounds typically
range from \(10^{-3}\) to \(10^2\), although this is not a stringent
requirement. Importantly, the scaling of the observed data does not
affect the values of \(\hat{\theta}\), but the scaling of the design
space does. Therefore, it is advisable to consistently scale variable
ranges between zero and one to ensure consistency in the degree of
activity \(\hat{\theta}_j\) represents across different problems.

Optimizing \(\hat{\phi}\) can enhance prediction accuracy across various
problems.

\subsection{Implementing an MLE of the Model
Parameters}\label{implementing-an-mle-of-the-model-parameters}

The matrix algebra necessary for calculating the likelihood is the most
computationally intensive aspect of the Kriging process. It's crucial to
ensure that the code implementation is as efficient as possible.

Given that \(\Psi\) (our correlation matrix) is symmetric, only half of
the matrix needs to be computed before adding it to its transpose. When
calculating the log-likelihood, several matrix inversions are required.
The fastest approach is to conduct one Cholesky factorization and then
apply backward and forward substitution for each inverse.

The Cholesky factorization is applicable only to positive-definite
matrices, which \(\Psi\) generally is. However, if \(\Psi\) becomes
nearly singular, such as when the \(\mathbf{x}^{(i)}\)'s are densely
packed, the Cholesky factorization might fail. In these cases, one could
employ an LU-decomposition, though the result might be unreliable. When
\(\Psi\) is near singular, the best course of action is to either use
regression techniques or, as we do here, assign a poor likelihood value
to parameters generating the near singular matrix, thus diverting the
MLE search towards better-conditioned \(\Psi\) matrices.

Another consideration in calculating the concentrated log-likelihood is
that \(\det(\Psi) \rightarrow 0\) for poorly conditioned matrices, so it
is advisable to use twice the sum of the logarithms of the diagonal of
the Cholesky factorization when calculating \(\ln(\lvert\Psi\rvert)\) in
Equation~\ref{eq-concentrated-loglikelihood}.

\subsection{Kriging Prediction}\label{kriging-prediction}

We will use the Kriging correlation \(\Psi\) to predict new values based
on the observed data. The matrix algebra involved for calculating the
likelihood is the most computationally intensive part of the Kriging
process. Care must be taken that the computer code is as efficient as
possible.

Basic elements of the Kriging based surrogate optimization such as
interpolation, expected improvement, and regression are presented. The
presentation follows the approach described in Forrester, SÃ³bester, and
Keane (2008) and Bartz et al. (2022).

Main idea for prediction is that the new \(Y(\vec{x})\) should be
consistent with the old sample data \(X\). For a new prediction
\(\hat{y}\) at \(\vec{x}\), the value of \(\hat{y}\) is chosen so that
it maximizes the likelihood of the sample data \(\vec{X}\) and the
prediction, given the (optimized) correlation parameter \(\vec{\theta}\)
and \(\vec{p}\) from above. The observed data \(\vec{y}\) is augmented
with the new prediction \(\hat{y}\) which results in the augmented
vector \(\vec{\tilde{y}} = ( \vec{y}^T, \hat{y})^T\). A vector of
correlations between the observed data and the new prediction is defined
as

\[ \vec{\psi} = \begin{pmatrix}
\text{cor}\left(
\vec{Y}(\vec{x}^{(1)}),
\vec{Y}(\vec{x}) 
\right) \\
\vdots  \\
\text{cor}\left(
\vec{Y}(\vec{x}^{(n)}),
\vec{Y}(\vec{x}) 
\right)
\end{pmatrix}
=
\begin{pmatrix}
\vec{\psi}^{(1)}\\
\vdots\\
\vec{\psi}^{(n)}
\end{pmatrix}.
\]

\begin{definition}[The Augmented Correlation
Matrix]\protect\hypertarget{def-augmented-correlation-matrix}{}\label{def-augmented-correlation-matrix}

The augmented correlation matrix is constructed as
\[ \tilde{\vec{\Psi}} =
\begin{pmatrix}
\vec{\Psi} & \vec{\psi} \\
\vec{\psi}^T & 1
\end{pmatrix}.
\]

\end{definition}

The log-likelihood of the augmented data is
\begin{equation}\phantomsection\label{eq-loglikelihood-augmented}{
\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\hat{\sigma}^2) - \frac{1}{2} \ln |\vec{\hat{\Psi}}| -  \frac{(\vec{\tilde{y}} - \vec{1}\hat{\mu})^T \vec{\tilde{\Psi}}^{-1}(\vec{\tilde{y}} - \vec{1}\hat{\mu})}{2 \hat{\sigma}^2},
}\end{equation}

where \(\vec{1}\) is a vector of ones and \(\hat{\mu}\) and
\(\hat{\sigma}^2\) are the MLEs from Equation~\ref{eq-muhat-55} and
Equation~\ref{eq-sigmahat-55}. Only the last term in
Equation~\ref{eq-loglikelihood-augmented} depends on \(\hat{y}\), so we
need only consider this term in the maximization. Details cen be found
in Forrester, SÃ³bester, and Keane (2008). Finally, the MLE for
\(\hat{y}\) can be calculated as
\begin{equation}\phantomsection\label{eq-mle-yhat}{
\hat{y}(\vec{x}) = \hat{\mu} + \vec{\psi}^T \vec{\tilde{\Psi}}^{-1} (\vec{y} - \vec{1}\hat{\mu}).
}\end{equation}

Equation~\ref{eq-mle-yhat} reveals two important properties of the
Kriging predictor:

\begin{itemize}
\tightlist
\item
  Basis functions: The basis function impacts the vector \(\vec{\psi}\),
  which contains the \(n\) correlations between the new point
  \(\vec{x}\) and the observed locations. Values from the \(n\) basis
  functions are added to a mean base term \(\mu\) with weightings \[
  \vec{w} = \vec{\tilde{\Psi}}^{(-1)} (\vec{y} - \vec{1}\hat{\mu}).
  \]
\item
  Interpolation: The predictions interpolate the sample data. When
  calculating the prediction at the \(i\)th sample point,
  \(\vec{x}^{(i)}\), the \(i\)th column of \(\vec{\Psi}^{-1}\) is
  \(\vec{\psi}\), and \(\vec{\psi}  \vec{\Psi}^{-1}\) is the \(i\)th
  unit vector. Hence,
\end{itemize}

\[
\hat{y}(\vec{x}^{(i)}) = y^{(i)}.
\]

\section{Kriging Example: Sinusoid
Function}\label{kriging-example-sinusoid-function}

Toy example in 1d where the response is a simple sinusoid measured at
eight equally spaced \(x\)-locations in the span of a single period of
oscillation.

\subsection{\texorpdfstring{Calculating the Correlation Matrix
\(\Psi\)}{Calculating the Correlation Matrix \textbackslash Psi}}\label{calculating-the-correlation-matrix-psi}

The correlation matrix \(\Psi\) is based on the pairwise squared
distances between the input locations. Here we will use \(n=8\) sample
locations and \(\theta\) is set to 1.0.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{8}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(X, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.  ]
 [0.79]
 [1.57]
 [2.36]
 [3.14]
 [3.93]
 [4.71]
 [5.5 ]]
\end{verbatim}

Evaluate at sample points

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(X)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(y, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.  ]
 [ 0.71]
 [ 1.  ]
 [ 0.71]
 [ 0.  ]
 [-0.71]
 [-1.  ]
 [-0.71]]
\end{verbatim}

We have the data points shown in Table~\ref{tbl-sin-data}.

\begin{longtable}[]{@{}rr@{}}
\caption{Data points for the sinusoid
function}\label{tbl-sin-data}\tabularnewline
\toprule\noalign{}
\(x\) & \(y\) \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\(x\) & \(y\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.0 & 0.0 \\
0.79 & 0.71 \\
1.57 & 1.0 \\
2.36 & 0.71 \\
3.14 & 0.0 \\
3.93 & -0.71 \\
4.71 & -1.0 \\
5.5 & -0.71 \\
\end{longtable}

The data points are visualized in Figure~\ref{fig-sin-data}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{)}
\NormalTok{plt.title(}\SpecialStringTok{f"Sin(x) evaluated at }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ points"}\NormalTok{)}
\NormalTok{plt.grid()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-sin-data-output-1.pdf}}

}

\caption{\label{fig-sin-data}Sin(x) evaluated at 8 points.}

\end{figure}%

\subsection{\texorpdfstring{Computing the \(\Psi\)
Matrix}{Computing the \textbackslash Psi Matrix}}\label{computing-the-psi-matrix}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# theta should be an array (of one value, for the moment, will be changed later)}
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(Psi, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1.   0.54 0.08 0.   0.   0.   0.   0.  ]
 [0.54 1.   0.54 0.08 0.   0.   0.   0.  ]
 [0.08 0.54 1.   0.54 0.08 0.   0.   0.  ]
 [0.   0.08 0.54 1.   0.54 0.08 0.   0.  ]
 [0.   0.   0.08 0.54 1.   0.54 0.08 0.  ]
 [0.   0.   0.   0.08 0.54 1.   0.54 0.08]
 [0.   0.   0.   0.   0.08 0.54 1.   0.54]
 [0.   0.   0.   0.   0.   0.08 0.54 1.  ]]
\end{verbatim}

\subsection{Selecting the New
Locations}\label{selecting-the-new-locations}

We would like to predict at \(m = 100\) new locations (or testign
locations) in the interval \([0, 2\pi]\). The new locations are stored
in the variable \texttt{x}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=} \DecValTok{100}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, m, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Computing the \(\psi\)
Vector}{Computing the \textbackslash psi Vector}}\label{computing-the-psi-vector}

Distances between testing locations \(x\) and training data locations
\(X\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ cdist}

\KeywordTok{def}\NormalTok{ build\_psi(X, x, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ x.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    theta }\OperatorTok{=}\NormalTok{ theta }\OperatorTok{*}\NormalTok{ ones(k)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ cdist(x.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{,}
\NormalTok{              out}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{              w}\OperatorTok{=}\NormalTok{theta)    }
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}
    \CommentTok{\# return psi transpose to be consistent with the literature}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dimensions of psi: }\SpecialCharTok{\{}\NormalTok{psi}\SpecialCharTok{.}\NormalTok{T}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{(psi.T)}

\NormalTok{psi }\OperatorTok{=}\NormalTok{ build\_psi(X, x, theta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Dimensions of psi: (8, 100)
\end{verbatim}

\subsection{Predicting at New
Locations}\label{predicting-at-new-locations}

Computation of the predictive equations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U }\OperatorTok{=}\NormalTok{ cholesky(Psi).T}
\NormalTok{one }\OperatorTok{=}\NormalTok{ np.ones(n).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ (one.T.dot(solve(U, solve(U.T, y)))) }\OperatorTok{/}\NormalTok{ one.T.dot(solve(U, solve(U.T, one)))}
\NormalTok{f }\OperatorTok{=}\NormalTok{ mu }\OperatorTok{*}\NormalTok{ ones(m).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ psi.T.dot(solve(U, solve(U.T, y }\OperatorTok{{-}}\NormalTok{ one }\OperatorTok{*}\NormalTok{ mu)))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dimensions of f: }\SpecialCharTok{\{}\NormalTok{f}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Dimensions of f: (100, 1)
\end{verbatim}

To compute \(f\), Equation~\ref{eq-mle-yhat} is used.

\subsection{Visualization}\label{visualization}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(x, f, color }\OperatorTok{=} \StringTok{"orange"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Fitted"}\NormalTok{)}
\NormalTok{plt.plot(x, np.sin(x), color }\OperatorTok{=} \StringTok{"grey"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Original"}\NormalTok{)}
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Measurements"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Kriging prediction of sin(x) with }\SpecialCharTok{\{\}}\StringTok{ points.}\CharTok{\textbackslash{}n}\StringTok{ theta: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(n, theta[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper right\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/cell-24-output-1.pdf}}

\section{Cholesky Decomposition}\label{cholesky-decomposition-2}

\subsection{Example of Cholesky
Decomposition}\label{example-of-cholesky-decomposition}

We consider dimension \(k=1\) and \(n=2\) sample points. The sample
points are located at \(x_1=1\) and \(x_2=5\). The response values are
\(y_1=2\) and \(y_2=10\). The correlation parameter is \(\theta=1\) and
\(p\) is set to \(1\). Using Equation~\ref{eq-krigingbase}, we can
compute the correlation matrix \(\Psi\):

\[
\Psi = \begin{pmatrix}
1 & e^{-1}\\
e^{-1} & 1
\end{pmatrix}.
\]

To determine MLE as in Equation~\ref{eq-mle-yhat}, we need to compute
\(\Psi^{-1}\):

\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e & -1\\
-1 & e
\end{pmatrix}.
\]

Cholesky-decomposition of \(\Psi\) is recommended to compute
\(\Psi^{-1}\). Cholesky decomposition is a decomposition of a positive
definite symmetric matrix into the product of a lower triangular matrix
\(L\), a diagonal matrix \(D\) and the transpose of \(L\), which is
denoted as \(L^T\). Consider the following example:

\[
LDL^T=
\begin{pmatrix}
1 & 0 \\
l_{21} & 1
\end{pmatrix}
\begin{pmatrix}
d_{11} & 0 \\
0 & d_{22}
\end{pmatrix}
\begin{pmatrix}
1 & l_{21} \\
0 & 1
\end{pmatrix}=
\]

\begin{equation}\phantomsection\label{eq-cholex}{
\begin{pmatrix}
d_{11} & 0 \\
d_{11} l_{21} & d_{22}
\end{pmatrix}
\begin{pmatrix}
1 & l_{21} \\
0 & 1
\end{pmatrix}
=
\begin{pmatrix}
d_{11} & d_{11} l_{21} \\
d_{11} l_{21} & d_{11} l_{21}^2 + d_{22}
\end{pmatrix}.
}\end{equation}

Using Equation~\ref{eq-cholex}, we can compute the Cholesky
decomposition of \(\Psi\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(d_{11} = 1\),
\item
  \(l_{21}d_{11} = e^{-1} \Rightarrow l_{21} = e^{-1}\), and
\item
  \(d_{11} l_{21}^2 + d_{22} = 1 \Rightarrow d_{22} = 1 - e^{-2}\).
\end{enumerate}

The Cholesky decomposition of \(\Psi\) is \[
\Psi = \begin{pmatrix}
1 & 0\\
e^{-1} & 1\\
\end{pmatrix}
\begin{pmatrix}
1 & 0\\
0 & 1 - e^{-2}\\
\end{pmatrix}
\begin{pmatrix}
1 & e^{-1}\\
0 & 1\\
\end{pmatrix}
= LDL^T\]

Some programs use \(U\) instead of \(L\). The Cholesky decomposition of
\(\Psi\) is \[
\Psi = LDL^T = U^TDU.
\]

Using \[
\sqrt{D} =\begin{pmatrix}
1 & 0\\
0 & \sqrt{1 - e^{-2}}\\
\end{pmatrix},
\] we can write the Cholesky decomposition of \(\Psi\) without a
diagonal matrix \(D\) as \[
\Psi = \begin{pmatrix}
1 & 0\\
e^{-1} & \sqrt{1 - e^{-2}}\\
\end{pmatrix}
\begin{pmatrix}
1 & e^{-1}\\
0 & \sqrt{1 - e^{-2}}\\
\end{pmatrix}
= U^TU.
\]

\subsection{Inverse Matrix Using Cholesky
Decomposition}\label{inverse-matrix-using-cholesky-decomposition}

To compute the inverse of a matrix using the Cholesky decomposition, you
can follow these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decompose the matrix \(A\) into \(L\) and \(L^T\), where \(L\) is a
  lower triangular matrix and \(L^T\) is the transpose of \(L\).
\item
  Compute \(L^{-1}\), the inverse of \(L\).
\item
  The inverse of \(A\) is then \((L^{-1})^T  L^-1\).
\end{enumerate}

Please note that this method only applies to symmetric,
positive-definite matrices.

The inverse of the matrix \(\Psi\) from above is:

\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e & -1\\
-1 & e
\end{pmatrix}.
\]

Here's an example of how to compute the inverse of a matrix using
Cholesky decomposition in Python:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ cholesky, inv}
\NormalTok{E }\OperatorTok{=}\NormalTok{ np.exp(}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Psi is a symmetric, positive{-}definite matrix }
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\NormalTok{E], [}\DecValTok{1}\OperatorTok{/}\NormalTok{E, }\DecValTok{1}\NormalTok{]])}
\NormalTok{L }\OperatorTok{=}\NormalTok{ cholesky(Psi, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{L\_inv }\OperatorTok{=}\NormalTok{ inv(L)}
\CommentTok{\# The inverse of A is (L\^{}{-}1)\^{}T * L\^{}{-}1}
\NormalTok{Psi\_inv }\OperatorTok{=}\NormalTok{ np.dot(L\_inv.T, L\_inv)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Psi:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Psi)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Psi Inverse:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Psi\_inv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Psi:
 [[1.         0.36787944]
 [0.36787944 1.        ]]
Psi Inverse:
 [[ 1.15651764 -0.42545906]
 [-0.42545906  1.15651764]]
\end{verbatim}

\section{Gaussian Processes---Some Background
Information}\label{gaussian-processessome-background-information}

The concept of GP (Gaussian Process) regression can be understood as a
simple extension of linear modeling. It is worth noting that this
approach goes by various names and acronyms, including ``kriging,'' a
term derived from geostatistics, as introduced by Matheron in 1963.
Additionally, it is referred to as Gaussian spatial modeling or a
Gaussian stochastic process, and machine learning (ML) researchers often
use the term Gaussian process regression (GPR). In all of these
instances, the central focus is on regression. This involves training on
both inputs and outputs, with the ultimate objective of making
predictions and quantifying uncertainty (referred to as uncertainty
quantification or UQ).

However, it's important to emphasize that GPs are not a universal
solution for every problem. Specialized tools may outperform GPs in
specific, non-generic contexts, and GPs have their own set of
limitations that need to be considered.

\subsection{Gaussian Process Prior}\label{gaussian-process-prior}

In the context of GP, any finite collection of realizations, which is
represented by \(n\) observations, is modeled as having a multivariate
normal (MVN) distribution. The characteristics of these realizations can
be fully described by two key parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Their mean, denoted as an \(n\)-vector \(\mu\).
\item
  The covariance matrix, denoted as an \(n \times n\) matrix \(\Sigma\).
  This covariance matrix encapsulates the relationships and variability
  between the individual realizations within the collection.
\end{enumerate}

\subsection{Covariance Function}\label{covariance-function}

The covariance function is defined by inverse exponentiated squared
Euclidean distance: \[
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 \},
\] where \(\vec{x}\) and \(\vec{x}'\) are two points in the
\(k\)-dimensional input space and \(\| \cdot \|\) denotes the Euclidean
distance, i.e., \[
|| \vec{x} - \vec{x}'||^2 = \sum_{i=1}^k (x_i - x_i')^2.
\]

An 1-d example is shown in Figure~\ref{fig-exp2euclid}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-exp2euclid-output-1.pdf}}

}

\caption{\label{fig-exp2euclid}One-dim inverse exponentiated squared
Euclidean distance}

\end{figure}%

The covariance function is also referred to as the kernel function. The
\emph{Gaussian} kernel uses an additional parameter, \(\sigma^2\), to
control the rate of decay. This parameter is referred to as the length
scale or the characteristic length scale. The covariance function is
then defined as

\begin{equation}\phantomsection\label{eq-Sigma}{
\Sigma(\vec{x}, \vec{x}') = \exp\{ - || \vec{x} - \vec{x}'||^2 / (2 \sigma^2) \}.
}\end{equation}

The covariance decays exponentially fast as \(\vec{x}\) and \(\vec{x}'\)
become farther apart. Observe that

\[
\Sigma(\vec{x},\vec{x}) = 1
\] and

\[
\Sigma(\vec{x}, \vec{x}') < 1
\] for \(\vec{x} \neq \vec{x}'\). The function
\(\Sigma(\vec{x},\vec{x}')\) must be positive definite.

\begin{refremark}[Kriging and Gaussian Basis Functions]
The Kriging basis function (Equation~\ref{eq-krigingbase}) is related to
the 1-dim Gaussian basis function (Equation~\ref{eq-Sigma}), which is
defined as \begin{equation}\phantomsection\label{eq-Sigma2}{
\Sigma(\vec{x}^{(i)}, \vec{x}^{(j)}) = \exp\{ - || \vec{x}^{(i)} - \vec{x}^{(j)}||^2 / (2\sigma^2) \}.
}\end{equation}

There are some differences between Gaussian basis functions and Kriging
basis functions:

\begin{itemize}
\tightlist
\item
  Where the Gaussian basis function has \(1/(2\sigma^2)\), the Kriging
  basis has a vector
  \(\theta = [\theta_1, \theta_2, \ldots, \theta_k]^T\).
\item
  The \(\theta\) vector allows the width of the basis function to vary
  from dimension to dimension.
\item
  In the Gaussian basis function, the exponent is fixed at 2, Kriging
  allows this exponent \(p_l\) to vary (typically from 1 to 2).
\end{itemize}

\label{rem-krigingbase-gauss}

\end{refremark}

\subsubsection{Positive Definiteness}\label{positive-definiteness}

Positive definiteness in the context of the covariance matrix
\(\Sigma_n\) is a fundamental requirement. It is determined by
evaluating \(\Sigma(x_i, x_j)\) at pairs of \(n\) \(\vec{x}\)-values,
denoted as \(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\). The condition
for positive definiteness is that for all \(\vec{x}\) vectors that are
not equal to zero, the expression \(\vec{x}^\top \Sigma_n \vec{x}\) must
be greater than zero. This property is essential when intending to use
\(\Sigma_n\) as a covariance matrix in multivariate normal (MVN)
analysis. It is analogous to the requirement in univariate Gaussian
distributions where the variance parameter, \(\sigma^2\), must be
positive.

Gaussian Processes (GPs) can be effectively utilized to generate random
data that follows a smooth functional relationship. The process involves
the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select a set of \(\vec{x}\)-values, denoted as
  \(\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n\).
\item
  Define the covariance matrix \(\Sigma_n\) by evaluating
  \(\Sigma_n^{ij} = \Sigma(\vec{x}_i, \vec{x}_j)\) for
  \(i, j = 1, 2, \ldots, n\).
\item
  Generate an \(n\)-variate realization \(Y\) that follows a
  multivariate normal distribution with a mean of zero and a covariance
  matrix \(\Sigma_n\), expressed as
  \(Y \sim \mathcal{N}_n(0, \Sigma_n)\).
\item
  Visualize the result by plotting it in the \(x\)-\(y\) plane.
\end{enumerate}

\subsection{Construction of the Covariance
Matrix}\label{construction-of-the-covariance-matrix}

Here is an one-dimensional example. The process begins by creating an
input grid using \(\vec{x}\)-values. This grid consists of 100 elements,
providing the basis for further analysis and visualization.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{n }\OperatorTok{=} \DecValTok{100}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the context of this discussion, the construction of the covariance
matrix, denoted as \(\Sigma_n\), relies on the concept of inverse
exponentiated squared Euclidean distances. However, it's important to
note that a modification is introduced later in the process.
Specifically, the diagonal of the covariance matrix is augmented with a
small value, represented as ``eps'' or \(\epsilon\).

The reason for this augmentation is that while inverse exponentiated
distances theoretically ensure the covariance matrix's positive
definiteness, in practical applications, the matrix can sometimes become
numerically ill-conditioned. By adding a small value to the diagonal,
such as \(\epsilon\), this ill-conditioning issue is mitigated. In this
context, \(\epsilon\) is often referred to as ``jitter.''

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ array, zeros, power, ones, exp, multiply, eye, linspace, spacing, sqrt, arange, append, ravel}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ multivariate\_normal}
\KeywordTok{def}\NormalTok{ build\_Sigma(X, sigma2):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=} \DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{sigma2[l])}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigma2 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Sigma }\OperatorTok{=}\NormalTok{ build\_Sigma(X, sigma2)}
\NormalTok{np.}\BuiltInTok{round}\NormalTok{(Sigma[:}\DecValTok{3}\NormalTok{,:], }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783, 0.726,
        0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278, 0.236,
        0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044, 0.034,
        0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003, 0.002,
        0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835, 0.783,
        0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325, 0.278,
        0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056, 0.044,
        0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004, 0.003,
        0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ],
       [0.98 , 0.995, 1.   , 0.995, 0.98 , 0.956, 0.923, 0.882, 0.835,
        0.783, 0.726, 0.667, 0.607, 0.546, 0.487, 0.43 , 0.375, 0.325,
        0.278, 0.236, 0.198, 0.164, 0.135, 0.11 , 0.089, 0.071, 0.056,
        0.044, 0.034, 0.026, 0.02 , 0.015, 0.011, 0.008, 0.006, 0.004,
        0.003, 0.002, 0.002, 0.001, 0.001, 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,
        0.   ]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.imshow(Sigma, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}hot\textquotesingle{}}\NormalTok{, interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}nearest\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/cell-31-output-1.pdf}}

\subsection{Generation of Random Samples and Plotting the Realizations
of the Random
Function}\label{generation-of-random-samples-and-plotting-the-realizations-of-the-random-function}

In the context of the multivariate normal distribution, the next step is
to utilize the previously constructed covariance matrix denoted as
\texttt{Sigma}. It is used as an essential component in generating
random samples from the multivariate normal distribution.

The function \texttt{multivariate\_normal} is employed for this purpose.
It serves as a random number generator specifically designed for the
multivariate normal distribution. In this case, the mean of the
distribution is set equal to \texttt{mean}, and the covariance matrix is
provided as \texttt{Psi}. The argument \texttt{size} specifies the
number of realizations, which, in this specific scenario, is set to one.

By default, the mean vector is initialized to zero. To match the number
of samples, which is equivalent to the number of rows in the \texttt{X}
and \texttt{Sigma} matrices, the argument \texttt{zeros(n)} is used,
where \texttt{n} represents the number of samples (here taken from the
size of the matrix, e.g.,: \texttt{Sigma.shape{[}0{]}}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{12345}\NormalTok{)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(zeros(Sigma.shape[}\DecValTok{0}\NormalTok{]), Sigma, size }\OperatorTok{=} \DecValTok{1}\NormalTok{, check\_valid}\OperatorTok{=}\StringTok{"raise"}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Y.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(100, 1)
\end{verbatim}

Now we can plot the results, i.e., a finite realization of the random
function \(Y()\) under a GP prior with a particular covariance
structure. We will plot those \texttt{X} and \texttt{Y} pairs as
connected points on an \(x\)-\(y\) plane.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.plot(X, Y)}
\NormalTok{plt.title(}\StringTok{"Realization of Random Functions under a GP prior.}\CharTok{\textbackslash{}n}\StringTok{ sigma2: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sigma2[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-mvn1-1-output-1.pdf}}

}

\caption{\label{fig-mvn1-1}Realization of one random function under a GP
prior. sigma2: 1.0}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{12345}\NormalTok{)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(zeros(Sigma.shape[}\DecValTok{0}\NormalTok{]), Sigma, size }\OperatorTok{=} \DecValTok{3}\NormalTok{, check\_valid}\OperatorTok{=}\StringTok{"raise"}\NormalTok{)}
\NormalTok{plt.plot(X, Y.T)}
\NormalTok{plt.title(}\StringTok{"Realization of Three Random Functions under a GP prior.}\CharTok{\textbackslash{}n}\StringTok{ sigma2: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sigma2[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-mvn1-3-output-1.pdf}}

}

\caption{\label{fig-mvn1-3}Realization of three random functions under a
GP prior. sigma2: 1.0}

\end{figure}%

\subsection{Properties of the 1d
Example}\label{properties-of-the-1d-example}

\subsubsection{Several Bumps:}\label{several-bumps}

In this analysis, we observe several bumps in the \(x\)-range of
\([0,10]\). These bumps in the function occur because shorter distances
exhibit high correlation, while longer distances tend to be essentially
uncorrelated. This leads to variations in the function's behavior:

\begin{itemize}
\tightlist
\item
  When \(x\) and \(x'\) are one \(\sigma\) unit apart, the correlation
  is
  \(\exp\left(-\sigma^2 / (2\sigma^2)\right) = \exp(-1/2) \approx 0.61\),
  i.e., a relative high correlation.
\item
  \(2\sigma\) apart means correlation \(\exp(â 2^2 /2) \approx 0.14\),
  i.e., only small correlation.
\item
  \(4\sigma\) apart means correlation \(\exp(â 4^2 /2) \approx 0.0003\),
  i.e., nearly no correlation---variables are considered independent for
  almost all practical application.
\end{itemize}

\subsubsection{Smoothness:}\label{smoothness}

The function plotted in Figure~\ref{fig-mvn1-1} represents only a finite
realization, which means that we have data for a limited number of
pairs, specifically 100 points. These points appear smooth in a tactile
sense because they are closely spaced, and the plot function connects
the dots with lines to create the appearance of smoothness. The complete
surface, which can be conceptually extended to an infinite realization
over a compact domain, is exceptionally smooth in a calculus sense due
to the covariance function's property of being infinitely
differentiable.

\subsubsection{Scale of Two:}\label{scale-of-two}

Regarding the scale of the \(Y\) values, they have a range of
approximately \([-2,2]\), with a 95\% probability of falling within this
range. In standard statistical terms, 95\% of the data points typically
fall within two standard deviations of the mean, which is a common
measure of the spread or range of data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ array, zeros, power, ones, exp, multiply, eye, linspace, spacing, sqrt, arange, append, ravel}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ multivariate\_normal}

\KeywordTok{def}\NormalTok{ build\_Sigma(X, sigma2):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=} \DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{sigma2[l])}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}

\KeywordTok{def}\NormalTok{ plot\_mvn( a}\OperatorTok{=}\DecValTok{0}\NormalTok{, b}\OperatorTok{=}\DecValTok{10}\NormalTok{, sigma2}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, size}\OperatorTok{=}\DecValTok{1}\NormalTok{, n}\OperatorTok{=}\DecValTok{100}\NormalTok{, show}\OperatorTok{=}\VariableTok{True}\NormalTok{):    }
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.linspace(a, b, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{    sigma2 }\OperatorTok{=}\NormalTok{ np.array([sigma2])}
\NormalTok{    Sigma }\OperatorTok{=}\NormalTok{ build\_Sigma(X, sigma2)}
\NormalTok{    rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{12345}\NormalTok{)}
\NormalTok{    Y }\OperatorTok{=}\NormalTok{ rng.multivariate\_normal(zeros(Sigma.shape[}\DecValTok{0}\NormalTok{]), Sigma, size }\OperatorTok{=}\NormalTok{ size, check\_valid}\OperatorTok{=}\StringTok{"raise"}\NormalTok{)}
\NormalTok{    plt.plot(X, Y.T)}
\NormalTok{    plt.title(}\StringTok{"Realization of Random Functions under a GP prior.}\CharTok{\textbackslash{}n}\StringTok{ sigma2: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sigma2[}\DecValTok{0}\NormalTok{]))}
    \ControlFlowTok{if}\NormalTok{ show:}
\NormalTok{        plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_mvn(a}\OperatorTok{=}\DecValTok{0}\NormalTok{, b}\OperatorTok{=}\DecValTok{10}\NormalTok{, sigma2}\OperatorTok{=}\FloatTok{10.0}\NormalTok{, size}\OperatorTok{=}\DecValTok{3}\NormalTok{, n}\OperatorTok{=}\DecValTok{250}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-mvn2-output-1.pdf}}

}

\caption{\label{fig-mvn2}Realization of Random Functions under a GP
prior. sigma2: 10}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_mvn(a}\OperatorTok{=}\DecValTok{0}\NormalTok{, b}\OperatorTok{=}\DecValTok{10}\NormalTok{, sigma2}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, size}\OperatorTok{=}\DecValTok{3}\NormalTok{, n}\OperatorTok{=}\DecValTok{250}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-mvn5-output-1.pdf}}

}

\caption{\label{fig-mvn5}Realization of Random Functions under a GP
prior. sigma2: 0.1}

\end{figure}%

\section{Jupyter Notebook}\label{jupyter-notebook-5}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_num_gp.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Introduction to spotpython}\label{sec-spot}

Surrogate model based optimization methods are common approaches in
simulation and optimization. SPOT was developed because there is a great
need for sound statistical analysis of simulation and optimization
algorithms. SPOT includes methods for tuning based on classical
regression and analysis of variance techniques. It presents tree-based
models such as classification and regression trees and random forests as
well as Bayesian optimization (Gaussian process models, also known as
Kriging). Combinations of different meta-modeling approaches are
possible. SPOT comes with a sophisticated surrogate model based
optimization method, that can handle discrete and continuous inputs.
Furthermore, any model implemented in \texttt{scikit-learn} can be used
out-of-the-box as a surrogate in \texttt{spotpython}.

SPOT implements key techniques such as exploratory fitness landscape
analysis and sensitivity analysis. It can be used to understand the
performance of various algorithms, while simultaneously giving insights
into their algorithmic behavior.

The \texttt{spot} loop consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Init: Build initial design \(X\)
\item
  Evaluate initial design on real objective \(f\): \(y = f(X)\)
\item
  Build surrogate: \(S = S(X,y)\)
\item
  Optimize on surrogate: \(X_0 =  \text{optimize}(S)\)
\item
  Evaluate on real objective: \(y_0 = f(X_0)\)
\item
  Impute (Infill) new points: \(X = X \cup X_0\), \(y = y \cup y_0\).
\item
  Goto 3.
\end{enumerate}

\section{Advantages of the spotpython
approach}\label{advantages-of-the-spotpython-approach}

\begin{itemize}
\item
  Neural networks and many ML algorithms are non-deterministic, so
  results are noisy (i.e., depend on the the initialization of the
  weights). Enhanced noise handling strategies, OCBA (description from
  HPT-book).
\item
  Optimal Computational Budget Allocation (OCBA) is a very efficient
  solution to solve the ``general ranking and selection problem'' if the
  objective function is noisy. It allocates function evaluations in an
  uneven manner to identify the best solutions and to reduce the total
  optimization costs. {[}Chen10a, Bart11b{]} Given a total number of
  optimization samples \(N\) to be allocated to \(k\) competing
  solutions whose performance is depicted by random variables with means
  \(\bar{y}_i\) (\(i=1, 2, \ldots, k\)), and finite variances
  \(\sigma_i^2\), respectively, as \(N \to \infty\), the \gls{APCS} can
  be asymptotically maximized when \begin{align}
  \frac{N_i}{N_j} & = \left( \frac{ \sigma_i / \delta_{b,i}}{\sigma_j/ \delta_{b,j}} \right)^2, i,j \in \{ 1, 2, \ldots, k\}, \text{ and }
  i \neq j \neq b,\\
  N_b &= \sigma_b \sqrt{ 
  \sum_{i=1, i\neq b}^k \frac{N_i^2}{\sigma_i^2}
  },
  \end{align} where \(N_i\) is the number of replications allocated to
  solution \(i\), \(\delta_{b,i} = \bar{y}_b - \bar{y}_i\), and
  \(\bar{y}_b \leq \min_{i\neq b} \bar{y}_i\) Bartz-Beielstein and
  Friese (2011).
\item
  Surrogate-based optimization: Better than grid search and random
  search (Reference to HPT-book)
\item
  Visualization
\item
  Importance based on the Kriging model
\item
  Sensitivity analysis. Exploratory fitness landscape analysis. Provides
  XAI methods (feature importance, integrated gradients, etc.)
\item
  Uncertainty quantification
\item
  Flexible, modular meta-modeling handling. spotpython come with a
  Kriging model, which can be replaced by any model implemented in
  \texttt{scikit-learn}.
\item
  Enhanced metric handling, especially for categorical hyperparameters
  (any sklearn metric can be used). Default is..
\item
  Integration with TensorBoard: Visualization of the hyperparameter
  tuning process, of the training steps, the model graph. Parallel
  coordinates plot, scatter plot matrix, and more.
\item
  Reproducibility. Results are stored as pickle files. The results can
  be loaded and visualized at any time and be transferred between
  different machines and operating systems.
\item
  Handles scikit-learn models and pytorch models out-of-the-box. The
  user has to add a simple wrapper for passing the hyperparemeters to
  use a pytorch model in spotpython.
\item
  Compatible with Lightning.
\item
  User can add own models as plain python code.
\item
  User can add own data sets in various formats.
\item
  Flexible data handling and data preprocessing.
\item
  Many examples online (hyperparameter-tuning-cookbook).
\item
  spotpython uses a robust optimizer that can even deal with
  hyperparameter-settings that cause crashes of the algorithms to be
  tuned.
\item
  even if the optimum is not found, HPT with spotpython prevents the
  user from choosing bad hyperparameters in a systematic way (design of
  experiments).
\end{itemize}

\section{Disadvantages of the spotpython
approach}\label{disadvantages-of-the-spotpython-approach}

\begin{itemize}
\tightlist
\item
  Time consuming
\item
  Surrogate can be misguiding
\item
  no parallelization implement yet
\end{itemize}

Central Idea: Evaluation of the surrogate model \texttt{S} is much
cheaper (or / and much faster) than running the real-world experiment
\(f\). We start with a small example.

\section{\texorpdfstring{Example: \texttt{Spot} and the Sphere
Function}{Example: Spot and the Sphere Function}}\label{example-spot-and-the-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function:
Sphere}\label{the-objective-function-sphere}

The \texttt{spotpython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[
f(x) = x^2
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

We can apply the function \texttt{fun} to input values and plot the
result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x, y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{007_num_spot_intro_files/figure-pdf/cell-4-output-1.pdf}}

\subsection{\texorpdfstring{The \texttt{Spot} Method as an Optimization
Algorithm Using a Surrogate
Model}{The Spot Method as an Optimization Algorithm Using a Surrogate Model}}\label{the-spot-method-as-an-optimization-algorithm-using-a-surrogate-model}

We initialize the \texttt{fun\_control} dictionary. The
\texttt{fun\_control} dictionary contains the parameters for the
objective function. The \texttt{fun\_control} dictionary is passed to
the \texttt{Spot} method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                     upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]))}
\NormalTok{spot\_0 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{spot\_0.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 6.690918515799129e-09 [#######---] 73.33% 
spotpython tuning: 6.719979618922052e-11 [########--] 80.00% 
spotpython tuning: 6.719979618922052e-11 [#########-] 86.67% 
spotpython tuning: 6.719979618922052e-11 [#########-] 93.33% 
spotpython tuning: 6.719979618922052e-11 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x15a7b73e0>
\end{verbatim}

The method \texttt{print\_results()} prints the results, i.e., the best
objective function value (``min y'') and the corresponding input value
(``x0'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 6.719979618922052e-11
x0: 8.197548181573593e-06
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(8.197548181573593e-06)]]
\end{verbatim}

To plot the search progress, the method \texttt{plot\_progress()} can be
used. The parameter \texttt{log\_y} is used to plot the objective
function values on a logarithmic scale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{007_num_spot_intro_files/figure-pdf/fig-spot-progress-output-1.pdf}}

}

\caption{\label{fig-spot-progress}Visualization of the search progress
of the \texttt{Spot} method. The black elements (points and line)
represent the initial design, before the surrogate is build. The red
elements represent the search on the surrogate.}

\end{figure}%

If the dimension of the input space is one, the method
\texttt{plot\_model()} can be used to visualize the model and the
underlying objective function values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_0.plot\_model()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{007_num_spot_intro_files/figure-pdf/fig-spot-model-1d-output-1.pdf}}

}

\caption{\label{fig-spot-model-1d}Visualization of the model and the
underlying objective function values.}

\end{figure}%

\section{\texorpdfstring{\texttt{Spot} Parameters: \texttt{fun\_evals},
\texttt{init\_size} and
\texttt{show\_models}}{Spot Parameters: fun\_evals, init\_size and show\_models}}\label{spot-parameters-fun_evals-init_size-and-show_models}

We will modify three parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The number of function evaluations (\texttt{fun\_evals}) will be set
  to \texttt{10} (instead of 15, which is the default value) in the
  \texttt{fun\_control} dictionary.
\item
  The parameter \texttt{show\_models}, which visualizes the search
  process for each single iteration for 1-dim functions, in the
  \texttt{fun\_control} dictionary.
\item
  The size of the initial design (\texttt{init\_size}) in the
  \texttt{design\_control} dictionary.
\end{enumerate}

The full list of the \texttt{Spot} parameters is shown in code reference
on GitHub, see
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/spot/spot/\#spotpython.Spot}{Spot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                     upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                     fun\_evals }\OperatorTok{=} \DecValTok{10}\NormalTok{,}
\NormalTok{                     show\_models }\OperatorTok{=} \VariableTok{True}\NormalTok{)               }
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{007_num_spot_intro_files/figure-pdf/cell-9-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{007_num_spot_intro_files/figure-pdf/cell-9-output-2.pdf}}

\begin{verbatim}
spotpython tuning: 9.7097578737121e-06 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\section{Print the Results}\label{print-the-results-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 9.7097578737121e-06
x0: 0.0031160484389226206
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.0031160484389226206)]]
\end{verbatim}

\section{Show the Progress}\label{show-the-progress-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{007_num_spot_intro_files/figure-pdf/cell-11-output-1.pdf}}

\section{Visualizing the Optimization and Hyperparameter Tuning Process
with TensorBoard}\label{sec-visualizing-tensorboard-01}

\texttt{spotpython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotpython}.

First, we define an ``PREFIX'' to identify the hyperparameter tuning
process. The PREFIX is used to create a directory for the TensorBoard
files.

\phantomsection\label{code-spot-tensorboard}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX }\OperatorTok{=} \StringTok{"01"}\NormalTok{,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_04_06_0
Created spot_tensorboard_path: runs/spot_logs/01_maans08_2025-05-06_10-04-06 for SummaryWriter()
\end{verbatim}

Since the \texttt{tensorboard\_log} is \texttt{True},
\texttt{spotpython} will log the optimization process in the TensorBoard
files. The argument \texttt{TENSORBOARD\_CLEAN=True} will move the
TensorBoard files from the previous run to a backup folder, so that
TensorBoard files from previous runs are not overwritten and a clean
start in the \texttt{runs} folder is guaranteed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,                   }
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_tuner.run()}
\NormalTok{spot\_tuner.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0005401959712392311 [#---------] 6.00% 
spotpython tuning: 0.00017194734221844825 [#---------] 7.00% 
spotpython tuning: 0.00012951639447619488 [#---------] 8.00% 
spotpython tuning: 0.00011593270967604635 [#---------] 9.00% 
spotpython tuning: 8.960832462042882e-05 [#---------] 10.00% 
spotpython tuning: 1.1087652041635331e-05 [#---------] 11.00% 
spotpython tuning: 1.5733042163261654e-06 [#---------] 12.00% 
spotpython tuning: 1.1001891458836728e-08 [#---------] 13.00% 
spotpython tuning: 1.1001891458836728e-08 [#---------] 14.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 15.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 16.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 17.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 18.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 19.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 20.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 21.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 22.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 23.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 24.00% 
spotpython tuning: 1.1001891458836728e-08 [##--------] 25.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 26.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 27.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 28.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 29.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 30.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 31.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 32.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 33.00% 
spotpython tuning: 1.1001891458836728e-08 [###-------] 34.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 35.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 36.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 37.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 38.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 39.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 40.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 41.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 42.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 43.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 44.00% 
spotpython tuning: 1.1001891458836728e-08 [####------] 45.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 46.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 47.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 48.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 49.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 50.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 51.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 52.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 53.00% 
spotpython tuning: 1.1001891458836728e-08 [#####-----] 54.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 55.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 56.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 57.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 58.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 59.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 60.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 61.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 62.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 63.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 64.00% 
spotpython tuning: 1.1001891458836728e-08 [######----] 65.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 66.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 67.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 68.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 69.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 70.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 71.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 72.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 73.00% 
spotpython tuning: 1.1001891458836728e-08 [#######---] 74.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 75.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 76.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 77.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 78.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 79.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 80.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 81.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 82.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 83.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 84.00% 
spotpython tuning: 1.1001891458836728e-08 [########--] 85.00% 
spotpython tuning: 1.1001891458836728e-08 [#########-] 86.00% 
spotpython tuning: 1.1001891458836728e-08 [#########-] 87.00% 
spotpython tuning: 1.1001891458836728e-08 [#########-] 88.00% 
spotpython tuning: 1.386815256673593e-09 [#########-] 89.00% 
spotpython tuning: 1.386815256673593e-09 [#########-] 90.00% 
spotpython tuning: 1.386815256673593e-09 [#########-] 91.00% 
spotpython tuning: 1.386815256673593e-09 [#########-] 92.00% 
spotpython tuning: 1.386815256673593e-09 [#########-] 93.00% 
spotpython tuning: 1.386815256673593e-09 [#########-] 94.00% 
spotpython tuning: 1.386815256673593e-09 [##########] 95.00% 
spotpython tuning: 1.386815256673593e-09 [##########] 96.00% 
spotpython tuning: 1.386815256673593e-09 [##########] 97.00% 
spotpython tuning: 3.952064321854671e-11 [##########] 98.00% 
spotpython tuning: 3.952064321854671e-11 [##########] 99.00% 
spotpython tuning: 3.952064321854671e-11 [##########] 100.00% Done...

Experiment saved to 01_res.pkl
min y: 3.952064321854671e-11
x0: -6.2865446167625905e-06
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(-6.2865446167625905e-06)]]
\end{verbatim}

Now we can start TensorBoard in the background. The TensorBoard process
will read the TensorBoard files and visualize the hyperparameter tuning
process. From the terminal, we can start TensorBoard with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\texttt{logdir} is the directory where the TensorBoard files are stored.
In our case, the TensorBoard files are stored in the directory
\texttt{./runs}.

TensorBoard will start a web server on port 6006. We can access the
TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The first TensorBoard visualization shows the objective function values
plotted against the wall time. The wall time is the time that has passed
since the start of the hyperparameter tuning process. The five initial
design points are shown in the upper left region of the plot. The line
visualizes the optimization process.
\pandocbounded{\includegraphics[keepaspectratio]{figures_static/01_tensorboard_01.png}}

The second TensorBoard visualization shows the input values, i.e.,
\(x_0\), plotted against the wall time.
\pandocbounded{\includegraphics[keepaspectratio]{figures_static/01_tensorboard_02.png}}

The third TensorBoard plot illustrates how \texttt{spotpython} can be
used as a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{figures_static/01_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotpython process.}

\end{figure}%

\section{Jupyter Notebook}\label{jupyter-notebook-6}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/007_num_spot_intro.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Multi-dimensional Functions}\label{sec-multi-dim}

This chapter illustrates how high-dimensional functions can be optimized
and analyzed. For reasons of illustration, we will use the
three-dimensional Sphere function, which is a simple and well-known
function. The problem dimension is \(k=3\), but can be easily adapted to
other, higher dimensions.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init, design\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\end{Highlighting}
\end{Shaded}

\section{The Objective Function: 3-dim
Sphere}\label{the-objective-function-3-dim-sphere}

The \texttt{spotpython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[
f(x) = \sum_i^k x_i^2.
\]

The Sphere function is continuous, convex and unimodal. The plot shows
its two-dimensional form. \index{Sphere function} The global minimum is
\[
f(x) = 0, \text{at } x = (0,0, \ldots, 0).
\]

It is available as \texttt{fun\_sphere} in the \texttt{Analytical} class
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/objectivefunctions.py}{{[}SOURCE{]}}.
\index{fun\_sphere}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

Here we will use problem dimension \(k=3\), which can be specified by
the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
array determines the problem dimension. If we select
\texttt{-1.0\ *\ np.ones(3)}, a three-dimensional function is created.

In contrast to the one-dimensional case
(Section~\ref{sec-visualizing-tensorboard-01}), where only one
\texttt{theta} value was used, we will use three different
\texttt{theta} values (one for each dimension), i.e., we set
\texttt{n\_theta=3} in the \texttt{surrogate\_control}. As default,
\texttt{spotpython} sets the \texttt{n\_theta} to the problem dimension.
Therefore, the \texttt{n\_theta} parameter can be omitted in this case.
More specifically, if \texttt{n\_theta} is larger than 1 or set to the
string ``anisotropic'', then the \(k\) theta values are used, where
\(k\) is the problem dimension. The meaning of ``anisotropic'' is
explained in @\#sec-iso-aniso-kriging.

The prefix is set to \texttt{"03"} to distinguish the results from the
one-dimensional case. Again, TensorBoard can be used to monitor the
progress of the optimization.

We can also add interpretable labels to the dimensions, which will be
used in the plots. Therefore, we set
\texttt{var\_name={[}"Pressure",\ "Temp",\ "Lambda"{]}} instead of the
default \texttt{var\_name=None}, which would result in the labels
\texttt{x\_0}, \texttt{x\_1}, and \texttt{x\_2}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"03"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{1.0}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{              var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{"Pressure"}\NormalTok{, }\StringTok{"Temp"}\NormalTok{, }\StringTok{"Lambda"}\NormalTok{],}
\NormalTok{              TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{              tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{spot\_3 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_3.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_05_34_0
Created spot_tensorboard_path: runs/spot_logs/03_maans08_2025-05-06_10-05-34 for SummaryWriter()
spotpython tuning: 0.04499639370770697 [#######---] 73.33% 
spotpython tuning: 0.02879181572787603 [########--] 80.00% 
spotpython tuning: 0.0022744029851669532 [#########-] 86.67% 
spotpython tuning: 0.0014897478896309436 [#########-] 93.33% 
spotpython tuning: 0.0012759691122765592 [##########] 100.00% Done...

Experiment saved to 03_res.pkl
\end{verbatim}

\phantomsection\label{spot-run}
\begin{verbatim}
<spotpython.spot.spot.Spot at 0x14e033d10>
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{Results}\label{results-1}

\subsubsection{Best Objective Function
Values}\label{best-objective-function-values}

The best objective function value and its corresponding input values are
printed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_3.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.0012759691122765592
Pressure: 0.018303752441652287
Temp: 0.01971734567381022
Lambda: -0.023498256071690607
\end{verbatim}

The method \texttt{plot\_progress()} plots current and best found
solutions versus the number of iterations as shown in
Figure~\ref{fig-008-spot-plot-progress}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/fig-008-spot-plot-progress-output-1.pdf}}

}

\caption{\label{fig-008-spot-plot-progress}Progress of the optimization
process for the 3-dim Sphere function. The initial design points are
shown in black, whereas the points that were found by the search on the
surrogate are plotted in red.}

\end{figure}%

\subsubsection{A Contour Plot}\label{a-contour-plot}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows. Note, we have specified identical
\texttt{min\_z} and \texttt{max\_z} values to generate comparable plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-7-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  In a similar manner, we can plot dimension \(i=0\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-8-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The final combination is \(i=1\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_contour(i}\OperatorTok{=}\DecValTok{1}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-9-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The three plots look very similar, because the \texttt{fun\_sphere} is
  symmetric.
\item
  This can also be seen from the variable importance:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_3.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pressure:  89.07395721056233
Temp:  100.0
Lambda:  80.2593685337925
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_3.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-11-output-1.pdf}}

\subsection{TensorBoard}\label{tensorboard-2}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{figures_static/02_tensorboard_01.png}}

}

\caption{TensorBoard visualization of the spotpython process. Objective
function values plotted against wall time.}

\end{figure}%

The second TensorBoard visualization shows the input values, i.e.,
\(x_0, \ldots, x_2\), plotted against the wall time.
\pandocbounded{\includegraphics[keepaspectratio]{figures_static/02_tensorboard_02.png}}

The third TensorBoard plot illustrates how \texttt{spotpython} can be
used as a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/02_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotpython surrogate model.}

\end{figure}%

\subsection{Conclusion}\label{conclusion}

Based on this quick analysis, we can conclude that all three dimensions
are equally important (as expected, because the Analytical function is
known).

\section{Exercises}\label{exercises-3}

\begin{exercise}[The Three Dimensional
\texttt{fun\_cubed}]\protect\hypertarget{exr-fun-cubed}{}\label{exr-fun-cubed}

The \texttt{spotpython} package provides several classes of objective
functions.

We will use the \texttt{fun\_cubed} in the \texttt{Analytical} class
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/objectivefunctions.py}{{[}SOURCE{]}}.
The input dimension is \texttt{3}. The search range is
\(-1 \leq x \leq 1\) for all dimensions.

Tasks: * Generate contour plots * Calculate the variable importance. *
Discuss the variable importance: * Are all variables equally important?
* If not: * Which is the most important variable? * Which is the least
important variable?

\end{exercise}

\begin{exercise}[The Ten Dimensional
\texttt{fun\_wing\_wt}]\protect\hypertarget{exr-fun-wing-wt}{}\label{exr-fun-wing-wt}

~

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{10}. The search range is
  \(0 \leq x \leq 1\) for all dimensions.
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \item
    Generate contour plots for the three most important variables. Do
    they confirm your selection?
  \end{itemize}
\end{itemize}

\end{exercise}

\begin{exercise}[The Three Dimensional
\texttt{fun\_runge}]\protect\hypertarget{exr-fun-runge}{}\label{exr-fun-runge}

~

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-5 \leq x \leq 5\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\end{exercise}

\begin{exercise}[The Three Dimensional
\texttt{fun\_linear}]\protect\hypertarget{exr-fun-linear}{}\label{exr-fun-linear}

~

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{3}. The search range is
  \(-5 \leq x \leq 5\) for all dimensions.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\end{exercise}

\begin{exercise}[The Two Dimensional Rosenbrock Function
\texttt{fun\_rosen}]\protect\hypertarget{exr-fun-rosen}{}\label{exr-fun-rosen}

~

\begin{itemize}
\tightlist
\item
  The input dimension is \texttt{2}. The search range is
  \(-5 \leq x \leq 10\) for all dimensions.
\item
  See
  \href{https://en.wikipedia.org/wiki/Rosenbrock_function}{Rosenbrock
  function} and
  \href{https://www.sfu.ca/~ssurjano/rosen.html}{Rosenbrock Function}
  for details.
\item
  Generate contour plots
\item
  Calculate the variable importance.
\item
  Discuss the variable importance:

  \begin{itemize}
  \tightlist
  \item
    Are all variables equally important?
  \item
    If not:

    \begin{itemize}
    \tightlist
    \item
      Which is the most important variable?
    \item
      Which is the least important variable?
    \end{itemize}
  \end{itemize}
\end{itemize}

\end{exercise}

\section{Selected Solutions}\label{selected-solutions}

\begin{refsolution}[Solution to Exercise~\ref{exr-fun-cubed}: The
Three-dimensional Cubed Function \texttt{fun\_cubed}]
We instanciate the \texttt{fun\_cubed} function from the
\texttt{Analytical} class.

\phantomsection\label{spot-fun-cubed-instanciation}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun\_cubed }\OperatorTok{=}\NormalTok{ Analytical().fun\_cubed}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Here we will use problem dimension \(k=3\), which can be specified by
  the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
  array determines the problem dimension. If we select
  \texttt{-1.0\ *\ np.ones(3)}, a three-dimensional function is created.
\item
  In contrast to the one-dimensional case, where only one \texttt{theta}
  value was used, we will use three different \texttt{theta} values (one
  for each dimension), i.e., we can set \texttt{n\_theta=3} in the
  \texttt{surrogate\_control}. However, this is not necessary, because
  by default, \texttt{n\_theta} is set to the number of dimensions.
\item
  The prefix is set to \texttt{"03"} to distinguish the results from the
  one-dimensional case.
\item
  We will set the \texttt{fun\_evals=20} to limit the number of function
  evaluations to 20 for this example.
\item
  The size of the initial design is set to \texttt{10} by default. It
  can be changed by setting \texttt{init\_size=10} via
  \texttt{design\_control\_init} in the \texttt{design\_control}
  dictionary.
\item
  Again, TensorBoard can be used to monitor the progress of the
  optimization.
\item
  We can also add interpretable labels to the dimensions, which will be
  used in the plots. Therefore, we set
  \texttt{var\_name={[}"Pressure",\ "Temp",\ "Lambda"{]}} instead of the
  default \texttt{var\_name=None}, which would result in the labels
  \texttt{x\_0}, \texttt{x\_1}, and \texttt{x\_2}.
\end{itemize}

Here is the link to the documentation of the fun\_control\_init
function:
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/utils/init/\#spotpython.utils.init.fun_control_init}{{[}DOC{]}}.
The documentation of the \texttt{design\_control\_init} function can be
found here:
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/utils/init/\#spotpython.utils.init.design_control_init}{{[}DOC{]}}.

The setup can be done as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"cubed"}\NormalTok{,}
\NormalTok{              fun\_evals}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{1.0}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.ones(}\DecValTok{3}\NormalTok{),}
\NormalTok{              var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{"Pressure"}\NormalTok{, }\StringTok{"Temp"}\NormalTok{, }\StringTok{"Lambda"}\NormalTok{],}
\NormalTok{              TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{              tensorboard\_log}\OperatorTok{=}\VariableTok{True}
\NormalTok{              )}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_05_38_0
Created spot_tensorboard_path: runs/spot_logs/cubed_maans08_2025-05-06_10-05-38 for SummaryWriter()
\end{verbatim}

\begin{itemize}
\tightlist
\item
  After the setup, we can pass the dictionaries to the \texttt{Spot}
  class and run the optimization process.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_cubed }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun\_cubed,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_cubed.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: -1.4662953667287653 [######----] 55.00% 
spotpython tuning: -1.4662953667287653 [######----] 60.00% 
spotpython tuning: -1.4662953667287653 [######----] 65.00% 
spotpython tuning: -2.1232188292307277 [#######---] 70.00% 
spotpython tuning: -2.1232188292307277 [########--] 75.00% 
spotpython tuning: -2.2387452533097423 [########--] 80.00% 
spotpython tuning: -3.0 [########--] 85.00% 
spotpython tuning: -3.0 [#########-] 90.00% 
spotpython tuning: -3.0 [##########] 95.00% 
spotpython tuning: -3.0 [##########] 100.00% Done...

Experiment saved to cubed_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x1530fcf20>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_cubed.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: -3.0
Pressure: -1.0
Temp: -1.0
Lambda: -1.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_cubed.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-16-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Contour Plots
\end{itemize}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

We can specify identical \texttt{min\_z} and \texttt{max\_z} values to
generate comparable plots. The default values are \texttt{min\_z=None}
and \texttt{max\_z=None}, which will be replaced by the minimum and
maximum values of the objective function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_z }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{3}
\NormalTok{max\_z }\OperatorTok{=} \DecValTok{1}
\NormalTok{spot\_cubed.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-17-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  In a similar manner, we can plot dimension \(i=0\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_cubed.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-18-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The final combination is \(i=1\) and \(j=2\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_cubed.plot\_contour(i}\OperatorTok{=}\DecValTok{1}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-19-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The variable importance can be printed and visualized as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_cubed.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pressure:  100.00000000000001
Temp:  15.132821048272005
Lambda:  5.452153854543634
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_cubed.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-21-output-1.pdf}}

\label{sol-fun-cubed}

\end{refsolution}

\begin{refsolution}[Solution to Exercise~\ref{exr-fun-rosen}: The
Two-dimensional Rosenbrock Function \texttt{fun\_rosen}]
\leavevmode

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The Objective Function: 2-dim \texttt{fun\_rosen}
\end{itemize}

The \texttt{spotpython} package provides several classes of objective
functions. We will use the \texttt{fun\_rosen} in the
\texttt{Analytical} class
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/objectivefunctions.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_rosen }\OperatorTok{=}\NormalTok{ Analytical().fun\_rosen}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Here we will use problem dimension \(k=2\), which can be specified by
  the \texttt{lower} bound arrays.
\item
  The size of the \texttt{lower} bound array determines the problem
  dimension. If we select \texttt{-5.0\ *\ np.ones(2)}, a
  two-dimensional function is created.
\item
  In contrast to the one-dimensional case, where only one \texttt{theta}
  value is used, we will use \(k\) different \texttt{theta} values (one
  for each dimension), i.e., we set \texttt{n\_theta=3} in the
  \texttt{surrogate\_control}.
\item
  The prefix is set to \texttt{"ROSEN"}.
\item
  Again, TensorBoard can be used to monitor the progress of the
  optimization.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"ROSEN"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{5.0}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{2}\NormalTok{),}
\NormalTok{              upper }\OperatorTok{=} \DecValTok{10}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{2}\NormalTok{),}
\NormalTok{              fun\_evals}\OperatorTok{=}\DecValTok{25}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{spot\_rosen }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun\_rosen,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_rosen.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 65.06022753421351 [####------] 44.00% 
spotpython tuning: 1.25687656241917 [#####-----] 48.00% 
spotpython tuning: 1.25687656241917 [#####-----] 52.00% 
spotpython tuning: 1.25687656241917 [######----] 56.00% 
spotpython tuning: 1.25687656241917 [######----] 60.00% 
spotpython tuning: 1.25687656241917 [######----] 64.00% 
spotpython tuning: 1.25687656241917 [#######---] 68.00% 
spotpython tuning: 0.9617179972268041 [#######---] 72.00% 
spotpython tuning: 0.961504528152653 [########--] 76.00% 
spotpython tuning: 0.9608396537747581 [########--] 80.00% 
spotpython tuning: 0.9600173239788774 [########--] 84.00% 
spotpython tuning: 0.9593146019130889 [#########-] 88.00% 
spotpython tuning: 0.958637527629652 [#########-] 92.00% 
spotpython tuning: 0.9578239644646188 [##########] 96.00% 
spotpython tuning: 0.956940169960332 [##########] 100.00% Done...

Experiment saved to ROSEN_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x15448dbe0>
\end{verbatim}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.956940169960332
x0: 0.02384561040687396
x1: 0.020724946230769436
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-26-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  A Contour Plot: We can select two dimensions, say \(i=0\) and \(j=1\),
  and generate a contour plot as follows.
\item
  Note: For higher dimensions, it might be useful to have identical
  \texttt{min\_z} and \texttt{max\_z} values to generate comparable
  plots. The default values are \texttt{min\_z=None} and
  \texttt{max\_z=None}, which will be replaced by the minimum and
  maximum values of the objective function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{max\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{spot\_rosen.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-27-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The variable importance can be calculated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0:  100.0
x1:  1.413709918826025
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{008_num_spot_multidim_files/figure-pdf/cell-29-output-1.pdf}}

\label{sol-fun-rosen}

\end{refsolution}

\section{Jupyter Notebook}\label{jupyter-notebook-7}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/008_num_spot_multidim.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Isotropic and Anisotropic Kriging}\label{sec-iso-aniso-kriging}

This chapter illustrates the difference between isotropic and
anisotropic Kriging models. The difference is illustrated with the help
of the \texttt{spotpython} package. Isotropic Kriging models use the
same \texttt{theta} value for every dimension. Anisotropic Kriging
models use different \texttt{theta} values for each dimension.

\section{\texorpdfstring{Example: Isotropic \texttt{Spot} Surrogate and
the 2-dim Sphere
Function}{Example: Isotropic Spot Surrogate and the 2-dim Sphere Function}}\label{sec-spot-2d-sphere-iso}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"003"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: 2-dim
Sphere}\label{the-objective-function-2-dim-sphere}

The \texttt{spotpython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula:

\[
f(x, y) = x^2 + y^2
\] The size of the \texttt{lower} bound vector determines the problem
dimension. Here we will use \texttt{np.array({[}-1,\ -1{]})}, i.e., a
two-dimensional function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{                               lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                               upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

Although the default \texttt{spot} surrogate model is an isotropic
Kriging model, we will explicitly set the \texttt{n\_theta} parameter to
a value of \texttt{1}, so that the same theta value is used for both
dimensions. This is done to illustrate the difference between isotropic
and anisotropic Kriging models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}

\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0013050614212698486 [#######---] 73.33% 
spotpython tuning: 0.0003479187873901382 [########--] 80.00% 
spotpython tuning: 0.00022767416623665655 [#########-] 86.67% 
spotpython tuning: 0.00020787497784734184 [#########-] 93.33% 
spotpython tuning: 0.00020393508736265477 [##########] 100.00% Done...

Experiment saved to 003_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x133ba18e0>
\end{verbatim}

\subsection{Results}\label{results-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.00020393508736265477
x0: 0.014161858193549292
x1: 0.0018376234294478113
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.014161858193549292)],
 ['x1', np.float64(0.0018376234294478113)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-7-output-1.pdf}}

\section{Example With Anisotropic
Kriging}\label{example-with-anisotropic-kriging}

As described in Section~\ref{sec-spot-2d-sphere-iso}, the default
parameter setting of \texttt{spotpython}'s Kriging surrogate uses the
same \texttt{theta} value for every dimension. This is referred to as
``using an isotropic kernel''. If different \texttt{theta} values are
used for each dimension, then an anisotropic kernel is used. To enable
anisotropic models in \texttt{spotpython}, the number of \texttt{theta}
values should be larger than one. We can use
\texttt{surrogate\_control=surrogate\_control\_init(n\_theta=2)} to
enable this behavior (\texttt{2} is the problem dimension).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{spot\_2\_anisotropic }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_anisotropic.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0013050614212698486 [#######---] 73.33% 
spotpython tuning: 0.0003479187873901382 [########--] 80.00% 
spotpython tuning: 0.00022767416623665655 [#########-] 86.67% 
spotpython tuning: 0.00020787497784734184 [#########-] 93.33% 
spotpython tuning: 0.00020393508736265477 [##########] 100.00% Done...

Experiment saved to 003_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x15c7f18e0>
\end{verbatim}

The search progress of the optimization with the anisotropic model can
be visualized:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-9-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.00020393508736265477
x0: 0.014161858193549292
x1: 0.0018376234294478113
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.014161858193549292)],
 ['x1', np.float64(0.0018376234294478113)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-11-output-1.pdf}}

\subsection{\texorpdfstring{Taking a Look at the \texttt{theta}
Values}{Taking a Look at the theta Values}}\label{taking-a-look-at-the-theta-values}

\subsubsection{\texorpdfstring{\texttt{theta} Values from the
\texttt{spot}
Model}{theta Values from the spot Model}}\label{theta-values-from-the-spot-model}

We can check, whether one or several \texttt{theta} values were used.
The \texttt{theta} values from the surrogate can be printed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_anisotropic.surrogate.theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-0.40325798, -0.43218074])
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the surrogate from the isotropic setting was stored as
  \texttt{spot\_2}, we can also take a look at the \texttt{theta} value
  from this model:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-0.40325798, -0.43218074])
\end{verbatim}

\subsubsection{TensorBoard}\label{tensorboard-3}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotpython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/03_tensorboard_03.png}

}

\caption{TensorBoard visualization of the spotpython surrogate model.}

\end{figure}%

\section{Exercises}\label{exercises-4}

\subsection{\texorpdfstring{1. The Branin Function
\texttt{fun\_branin}}{1. The Branin Function fun\_branin}}\label{the-branin-function-fun_branin}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 10\) and \(0 \leq x_2 \leq 15\).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion: instead of the number of evaluations
  (which is specified via \texttt{fun\_evals}), the time should be used
  as the termination criterion. This can be done as follows
  (\texttt{max\_time=1} specifies a run time of one minute):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{              max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{2. The Two-dimensional Sin-Cos Function
\texttt{fun\_sin\_cos}}{2. The Two-dimensional Sin-Cos Function fun\_sin\_cos}}\label{the-two-dimensional-sin-cos-function-fun_sin_cos}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-2\pi \leq x_1 \leq 2\pi\) and \(-2\pi \leq x_2 \leq 2\pi\).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{3. The Two-dimensional Runge Function
\texttt{fun\_runge}}{3. The Two-dimensional Runge Function fun\_runge}}\label{the-two-dimensional-runge-function-fun_runge}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 5\) and \(-5 \leq x_2 \leq 5\).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{4. The Ten-dimensional Wing-Weight Function
\texttt{fun\_wingwt}}{4. The Ten-dimensional Wing-Weight Function fun\_wingwt}}\label{the-ten-dimensional-wing-weight-function-fun_wingwt}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{10}. The search ranges are between 0
    and 1 (values are mapped internally to their natural bounds).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{5. The Two-dimensional Rosenbrock Function
\texttt{fun\_rosen}}{5. The Two-dimensional Rosenbrock Function fun\_rosen}}\label{sec-09-exercise-rosen}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search ranges are between -5
    and 10.
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\section{Selected Solutions}\label{selected-solutions-1}

\subsection{\texorpdfstring{Solution to Exercise
Section~\ref{sec-09-exercise-rosen}: The Two-dimensional Rosenbrock
Function
\texttt{fun\_rosen}}{Solution to Exercise Section~: The Two-dimensional Rosenbrock Function fun\_rosen}}\label{solution-to-exercise-sec-09-exercise-rosen-the-two-dimensional-rosenbrock-function-fun_rosen}

\subsubsection{\texorpdfstring{The Two Dimensional \texttt{fun\_rosen}:
The Isotropic
Case}{The Two Dimensional fun\_rosen: The Isotropic Case}}\label{the-two-dimensional-fun_rosen-the-isotropic-case}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\end{Highlighting}
\end{Shaded}

The \texttt{spotpython} package provides several classes of objective
functions. We will use the \texttt{fun\_rosen} in the
\texttt{analytical} class
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/objectivefunctions.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_rosen }\OperatorTok{=}\NormalTok{ Analytical().fun\_rosen}
\end{Highlighting}
\end{Shaded}

Here we will use problem dimension \(k=2\), which can be specified by
the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
array determines the problem dimension.

The prefix is set to \texttt{"ROSEN"} to distinguish the results from
the one-dimensional case. Again, TensorBoard can be used to monitor the
progress of the optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"ROSEN"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\OperatorTok{{-}}\DecValTok{5}\NormalTok{]),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{]),}
\NormalTok{              show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{spot\_rosen }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun\_rosen,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_rosen.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 65.06022753421351 [#######---] 73.33% 
spotpython tuning: 1.25687656241917 [########--] 80.00% 
spotpython tuning: 1.25687656241917 [#########-] 86.67% 
spotpython tuning: 1.25687656241917 [#########-] 93.33% 
spotpython tuning: 1.25687656241917 [##########] 100.00% Done...

Experiment saved to ROSEN_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x15cd58890>
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\paragraph{Results}\label{results-3}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.25687656241917
x0: 0.056952323626011235
x1: -0.18846914563710274
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-19-output-1.pdf}}

\paragraph{A Contour Plot}\label{a-contour-plot-1}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{max\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{spot\_rosen.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-20-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The variable importance cannot be calculated, because only one
  \texttt{theta} value was used.
\end{itemize}

\paragraph{TensorBoard}\label{tensorboard-4}

TBD

\subsubsection{\texorpdfstring{The Two Dimensional \texttt{fun\_rosen}:
The Anisotropic
Case}{The Two Dimensional fun\_rosen: The Anisotropic Case}}\label{the-two-dimensional-fun_rosen-the-anisotropic-case}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\end{Highlighting}
\end{Shaded}

The \texttt{spotpython} package provides several classes of objective
functions. We will use the \texttt{fun\_rosen} in the
\texttt{analytical} class
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/objectivefunctions.py}{{[}SOURCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_rosen }\OperatorTok{=}\NormalTok{ Analytical().fun\_rosen}
\end{Highlighting}
\end{Shaded}

Here we will use problem dimension \(k=2\), which can be specified by
the \texttt{lower} bound arrays. The size of the \texttt{lower} bound
array determines the problem dimension.

We can also add interpreable labels to the dimensions, which will be
used in the plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"ROSEN"}\NormalTok{,}
\NormalTok{              lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\OperatorTok{{-}}\DecValTok{5}\NormalTok{]),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{]),}
\NormalTok{              show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{spot\_rosen }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun\_rosen,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                  surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_rosen.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 65.06022753421351 [#######---] 73.33% 
spotpython tuning: 1.25687656241917 [########--] 80.00% 
spotpython tuning: 1.25687656241917 [#########-] 86.67% 
spotpython tuning: 1.25687656241917 [#########-] 93.33% 
spotpython tuning: 1.25687656241917 [##########] 100.00% Done...

Experiment saved to ROSEN_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x15fe5b740>
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

and can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\paragraph{Results}\label{results-4}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.25687656241917
x0: 0.056952323626011235
x1: -0.18846914563710274
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-25-output-1.pdf}}

\paragraph{A Contour Plot}\label{a-contour-plot-2}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{max\_z }\OperatorTok{=} \VariableTok{None}
\NormalTok{spot\_rosen.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z}\OperatorTok{=}\NormalTok{max\_z)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-26-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The variable importance can be calculated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_rosen.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
x0:  100.0
x1:  1.98917702191357
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_rosen.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{009_num_spot_anisotropic_files/figure-pdf/cell-28-output-1.pdf}}

\paragraph{TensorBoard}\label{tensorboard-5}

TBD

\section{Jupyter Notebook}\label{jupyter-notebook-8}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/009_num_spot_anisotropic.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{\texorpdfstring{Using \texttt{sklearn} Surrogates in
\texttt{spotpython}}{Using sklearn Surrogates in spotpython}}\label{sec-sklearn-surrogates}

Besides the internal kriging surrogate, which is used as a default by
\texttt{spotpython}, any surrogate model from \texttt{scikit-learn} can
be used as a surrogate in \texttt{spotpython}. This chapter explains how
to use \texttt{scikit-learn} surrogates in \texttt{spotpython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Example: Branin Function with
\texttt{spotpython}'s Internal Kriging
Surrogate}{Example: Branin Function with spotpython's Internal Kriging Surrogate}}\label{example-branin-function-with-spotpythons-internal-kriging-surrogate}

\subsection{The Objective Function
Branin}\label{the-objective-function-branin-1}

\begin{itemize}
\item
  The \texttt{spotpython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula.
\item
  Here we will use the Branin function:

\begin{verbatim}
  y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,
  where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2),
  c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).
\end{verbatim}
\item
  It has three global minima:

\begin{verbatim}
  f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).
\end{verbatim}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"04"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf)}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{\texorpdfstring{Running the surrogate model based optimizer
\texttt{Spot}:}{Running the surrogate model based optimizer Spot:}}\label{running-the-surrogate-model-based-optimizer-spot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.6961557165023953 [######----] 55.00% 
spotpython tuning: 3.483508988983779 [######----] 60.00% 
spotpython tuning: 3.0409502575998015 [######----] 65.00% 
spotpython tuning: 2.456350051306707 [#######---] 70.00% 
spotpython tuning: 2.4047922291552375 [########--] 75.00% 
spotpython tuning: 2.3739197901896123 [########--] 80.00% 
spotpython tuning: 2.355976671386384 [########--] 85.00% 
spotpython tuning: 2.348822784282971 [#########-] 90.00% 
spotpython tuning: 2.348822784282971 [##########] 95.00% 
spotpython tuning: 2.348822784282971 [##########] 100.00% Done...

Experiment saved to 04_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x1554b65d0>
\end{verbatim}

\subsection{TensorBoard}\label{tensorboard-7}

Now we can start TensorBoard in the background with the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotpython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate is plotted against the number of
optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/04_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotpython optimization
process and the surrogate model.}

\end{figure}%

\subsection{Print the Results}\label{print-the-results-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.348822784282971
x0: 3.1733035041022477
x1: 3.645429652665629
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(3.1733035041022477)], ['x1', np.float64(3.645429652665629)]]
\end{verbatim}

\subsection{Show the Progress and the
Surrogate}\label{show-the-progress-and-the-surrogate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-8-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-9-output-1.pdf}}

\section{Example: Using Surrogates From
scikit-learn}\label{example-using-surrogates-from-scikit-learn}

\begin{itemize}
\tightlist
\item
  Default is the \texttt{spotpython} (i.e., the internal)
  \texttt{kriging} surrogate.
\item
  It can be called explicitely and passed to \texttt{Spot}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\NormalTok{S\_0 }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Alternatively, models from \texttt{scikit-learn} can be selected,
  e.g., Gaussian Process, RBFs, Regression Trees, etc.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Here are some additional models that might be useful later:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_Tree }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{S\_LM }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_Ridge }\OperatorTok{=}\NormalTok{ linear\_model.Ridge()}
\NormalTok{S\_RF }\OperatorTok{=}\NormalTok{ RandomForestRegressor(max\_depth}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{GaussianProcessRegressor as a
Surrogate}\label{gaussianprocessregressor-as-a-surrogate}

\begin{itemize}
\tightlist
\item
  To use a Gaussian Process model from \texttt{sklearn}, that is similar
  to \texttt{spotpython}'s \texttt{Kriging}, we can proceed as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  The scikit-learn GP model \texttt{S\_GP} is selected for \texttt{Spot}
  as follows:

  \texttt{surrogate\ =\ S\_GP}
\item
  We can check the kind of surogate model with the command
  \texttt{isinstance}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S\_GP, GaussianProcessRegressor) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S\_0, Kriging)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Similar to the \texttt{Spot} run with the internal \texttt{Kriging}
  model, we can call the run with the \texttt{scikit-learn} surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\NormalTok{spot\_2\_GP }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                     surrogate }\OperatorTok{=}\NormalTok{ S\_GP)}
\NormalTok{spot\_2\_GP.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 18.865129821249617 [######----] 55.00% 
spotpython tuning: 4.066961682805861 [######----] 60.00% 
spotpython tuning: 3.4619112320780285 [######----] 65.00% 
spotpython tuning: 3.4619112320780285 [#######---] 70.00% 
spotpython tuning: 1.3283123221495199 [########--] 75.00% 
spotpython tuning: 0.9548698218896146 [########--] 80.00% 
spotpython tuning: 0.9356616728510581 [########--] 85.00% 
spotpython tuning: 0.39968125707661706 [#########-] 90.00% 
spotpython tuning: 0.3983050744842078 [##########] 95.00% 
\end{verbatim}

\begin{verbatim}
spotpython tuning: 0.39821610604643354 [##########] 100.00% Done...

Experiment saved to 04_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x157541f40>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_GP.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-17-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.39821610604643354
x0: 3.1496411777654334
x1: 2.272943969041002
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(3.1496411777654334)], ['x1', np.float64(2.272943969041002)]]
\end{verbatim}

\section{\texorpdfstring{Example: One-dimensional Sphere Function With
\texttt{spotpython}'s
Kriging}{Example: One-dimensional Sphere Function With spotpython's Kriging}}\label{example-one-dimensional-sphere-function-with-spotpythons-kriging}

\begin{itemize}
\tightlist
\item
  In this example, we will use an one-dimensional function, which allows
  us to visualize the optimization process.

  \begin{itemize}
  \tightlist
  \item
    \texttt{show\_models=\ True} is added to the argument list.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-2.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475485848264724 [####------] 40.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-4.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475485848264724 [#####-----] 50.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-6.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475481964033922 [######----] 60.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-8.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475479804192463 [#######---] 70.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-10.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475446167786181 [########--] 80.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-12.pdf}}

\begin{verbatim}
spotpython tuning: 0.03474447900832713 [#########-] 90.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-20-output-14.pdf}}

\begin{verbatim}
spotpython tuning: 0.034515878522411086 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\subsection{Results}\label{results-5}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.034515878522411086
x0: 0.1857844948385389
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.1857844948385389)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-22-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The method \texttt{plot\_model} plots the final surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_model()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-23-output-1.pdf}}

\section{\texorpdfstring{Example: \texttt{Sklearn} Model
GaussianProcess}{Example: Sklearn Model GaussianProcess}}\label{example-sklearn-model-gaussianprocess}

\begin{itemize}
\tightlist
\item
  This example visualizes the search process on the
  \texttt{GaussianProcessRegression} surrogate from \texttt{sklearn}.
\item
  Therefore \texttt{surrogate\ =\ S\_GP} is added to the argument list.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{spot\_1\_GP }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                      fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                      design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                      surrogate }\OperatorTok{=}\NormalTok{ S\_GP)}
\NormalTok{spot\_1\_GP.run()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-2.pdf}}

\begin{verbatim}
spotpython tuning: 0.004925671418704527 [####------] 40.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-4.pdf}}

\begin{verbatim}
spotpython tuning: 0.002612062398164981 [#####-----] 50.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-6.pdf}}

\begin{verbatim}
spotpython tuning: 5.609944300870913e-07 [######----] 60.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-8.pdf}}

\begin{verbatim}
spotpython tuning: 3.399776625316493e-08 [#######---] 70.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-10.pdf}}

\begin{verbatim}
spotpython tuning: 2.8303204876737398e-08 [########--] 80.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-12.pdf}}

\begin{verbatim}
spotpython tuning: 2.8303204876737398e-08 [#########-] 90.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-24-output-14.pdf}}

\begin{verbatim}
spotpython tuning: 2.2894458385368016e-08 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.2894458385368016e-08
x0: 0.0001513091483862361
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.0001513091483862361)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-26-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_GP.plot\_model()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-27-output-1.pdf}}

\section{Exercises}\label{exercises-5}

\subsection{\texorpdfstring{1. A decision tree regressor:
\texttt{DecisionTreeRegressor}}{1. A decision tree regressor: DecisionTreeRegressor}}\label{sec-10-exercise-01}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{2. A random forest regressor:
\texttt{RandomForestRegressor}}{2. A random forest regressor: RandomForestRegressor}}\label{sec-10-exercise-02}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{3. Ordinary least squares Linear Regression:
\texttt{LinearRegression}}{3. Ordinary least squares Linear Regression: LinearRegression}}\label{sec-10-exercise-03}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{4. Linear least squares with l2
regularization:
\texttt{Ridge}}{4. Linear least squares with l2 regularization: Ridge}}\label{sec-10-exercise-04}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{\texorpdfstring{5. Gradient Boosting:
\texttt{HistGradientBoostingRegressor}}{5. Gradient Boosting: HistGradientBoostingRegressor}}\label{sec-10-exercise-05}

\begin{itemize}
\tightlist
\item
  Describe the surrogate model. Use the information from the
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html\#sklearn.ensemble.HistGradientBoostingRegressor}{scikit-learn
  documentation}.
\item
  Use the surrogate as the model for optimization.
\end{itemize}

\subsection{6. Comparison of Surrogates}\label{sec-10-exercise-06}

\begin{itemize}
\item
  Use the following two objective functions

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    the 1-dim sphere function
    \href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/objectivefunctions.py}{\texttt{fun\_sphere}}
    and
  \item
    the two-dim Branin function
    \href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/objectivefunctions.py}{\texttt{fun\_branin}}:
  \end{enumerate}

  for a comparison of the performance of the five different surrogates:

  \begin{itemize}
  \tightlist
  \item
    \texttt{spotpython}'s internal Kriging
  \item
    \texttt{DecisionTreeRegressor}
  \item
    \texttt{RandomForestRegressor}
  \item
    \texttt{linear\_model.LinearRegression}
  \item
    \texttt{linear\_model.Ridge}.
  \end{itemize}
\item
  Generate a table with the results (number of function evaluations,
  best function value, and best parameter vector) for each surrogate and
  each function as shown in Table~\ref{tbl-results}.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1529}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0824}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1647}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1412}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.2235}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1176}}@{}}
\caption{Result table}\label{tbl-results}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\texttt{surrogate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{fun}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{fun\_evals}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{max\_time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{x\_0}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{min\_y}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\texttt{surrogate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{fun}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{fun\_evals}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{max\_time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{x\_0}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\texttt{min\_y}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{Kriging} & \texttt{fun\_sphere} & 10 & \texttt{inf} & & & \\
\texttt{Kriging} & \texttt{fun\_branin} & 10 & \texttt{inf} & & & \\
\texttt{DecisionTreeRegressor} & \texttt{fun\_sphere} & 10 &
\texttt{inf} & & & \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & & & \\
\texttt{Ridge} & \texttt{fun\_branin} & 10 & \texttt{inf} & & & \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  Discuss the results. Which surrogate is the best for which function?
  Why?
\end{itemize}

\section{Selected Solutions}\label{selected-solutions-2}

\subsection{\texorpdfstring{Solution to Exercise
Section~\ref{sec-10-exercise-05}: Gradient
Boosting}{Solution to Exercise Section~: Gradient Boosting}}\label{solution-to-exercise-sec-10-exercise-05-gradient-boosting}

\subsubsection{Branin: Using SPOT}\label{branin-using-spot}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The Objective Function Branin
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"BRANIN"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf)}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Running the surrogate model based optimizer \texttt{Spot}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.6961557165023953 [######----] 55.00% 
spotpython tuning: 3.483508988983779 [######----] 60.00% 
spotpython tuning: 3.0409502575998015 [######----] 65.00% 
spotpython tuning: 2.456350051306707 [#######---] 70.00% 
spotpython tuning: 2.4047922291552375 [########--] 75.00% 
spotpython tuning: 2.3739197901896123 [########--] 80.00% 
spotpython tuning: 2.355976671386384 [########--] 85.00% 
spotpython tuning: 2.348822784282971 [#########-] 90.00% 
spotpython tuning: 2.348822784282971 [##########] 95.00% 
spotpython tuning: 2.348822784282971 [##########] 100.00% Done...

Experiment saved to BRANIN_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x169465130>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Print the results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2.348822784282971
x0: 3.1733035041022477
x1: 3.645429652665629
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(3.1733035041022477)], ['x1', np.float64(3.645429652665629)]]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Show the optimization progress:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-32-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Generate a surrogate model plot:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-33-output-1.pdf}}

\subsubsection{Branin: Using Surrogates From
scikit-learn}\label{branin-using-surrogates-from-scikit-learn}

\begin{itemize}
\tightlist
\item
  The \texttt{HistGradientBoostingRegressor} model from
  \texttt{scikit-learn} is selected:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ HistGradientBoostingRegressor}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{S\_XGB }\OperatorTok{=}\NormalTok{ HistGradientBoostingRegressor()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The scikit-learn XGB model \texttt{S\_XGB} is selected for
  \texttt{Spot} as follows: \texttt{surrogate\ =\ S\_XGB}.
\item
  Similar to the \texttt{Spot} run with the internal \texttt{Kriging}
  model, we can call the run with the \texttt{scikit-learn} surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_branin}
\NormalTok{spot\_2\_XGB }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                     surrogate }\OperatorTok{=}\NormalTok{ S\_XGB)}
\NormalTok{spot\_2\_XGB.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 30.69410528614059 [######----] 55.00% 
spotpython tuning: 30.69410528614059 [######----] 60.00% 
spotpython tuning: 30.69410528614059 [######----] 65.00% 
spotpython tuning: 30.69410528614059 [#######---] 70.00% 
spotpython tuning: 1.3263745845108854 [########--] 75.00% 
spotpython tuning: 1.3263745845108854 [########--] 80.00% 
spotpython tuning: 1.3263745845108854 [########--] 85.00% 
spotpython tuning: 1.3263745845108854 [#########-] 90.00% 
spotpython tuning: 1.3263745845108854 [##########] 95.00% 
spotpython tuning: 1.3263745845108854 [##########] 100.00% Done...

Experiment saved to BRANIN_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x16a74acf0>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Print the Results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_XGB.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.3263745845108854
x0: -2.872730773493426
x1: 10.874313833535739
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(-2.872730773493426)],
 ['x1', np.float64(10.874313833535739)]]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Show the Progress
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_XGB.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-37-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Since the \texttt{sklearn} model does not provide a \texttt{plot}
  method, we cannot generate a surrogate model plot.
\end{itemize}

\subsubsection{\texorpdfstring{One-dimensional Sphere Function With
\texttt{spotpython}'s
Kriging}{One-dimensional Sphere Function With spotpython's Kriging}}\label{one-dimensional-sphere-function-with-spotpythons-kriging}

\begin{itemize}
\tightlist
\item
  In this example, we will use an one-dimensional function, which allows
  us to visualize the optimization process.

  \begin{itemize}
  \tightlist
  \item
    \texttt{show\_models=\ True} is added to the argument list.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-2.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475485848264724 [####------] 40.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-4.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475485848264724 [#####-----] 50.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-6.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475481964033922 [######----] 60.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-8.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475479804192463 [#######---] 70.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-10.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475446167786181 [########--] 80.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-12.pdf}}

\begin{verbatim}
spotpython tuning: 0.03474447900832713 [#########-] 90.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-39-output-14.pdf}}

\begin{verbatim}
spotpython tuning: 0.034515878522411086 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Print the Results
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.034515878522411086
x0: 0.1857844948385389
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.1857844948385389)]]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Show the Progress
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-41-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The method \texttt{plot\_model} plots the final surrogate:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_model()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-42-output-1.pdf}}

\subsubsection{\texorpdfstring{One-dimensional Sphere Function With
\texttt{Sklearn} Model
HistGradientBoostingRegressor}{One-dimensional Sphere Function With Sklearn Model HistGradientBoostingRegressor}}\label{one-dimensional-sphere-function-with-sklearn-model-histgradientboostingregressor}

\begin{itemize}
\tightlist
\item
  This example visualizes the search process on the
  \texttt{HistGradientBoostingRegressor} surrogate from
  \texttt{sklearn}.
\item
  Therefore \texttt{surrogate\ =\ S\_XGB} is added to the argument list.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    show\_models}\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{spot\_1\_XGB }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                      fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                      design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                      surrogate }\OperatorTok{=}\NormalTok{ S\_XGB)}
\NormalTok{spot\_1\_XGB.run()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-2.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475493366922229 [####------] 40.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-4.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475493366922229 [#####-----] 50.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-6.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475493366922229 [######----] 60.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-8.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475493366922229 [#######---] 70.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-10.pdf}}

\begin{verbatim}
spotpython tuning: 0.008730885505764131 [########--] 80.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-12.pdf}}

\begin{verbatim}
spotpython tuning: 0.008730885505764131 [#########-] 90.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-43-output-14.pdf}}

\begin{verbatim}
spotpython tuning: 0.008730885505764131 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_XGB.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.008730885505764131
x0: 0.09343920754032609
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.09343920754032609)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_XGB.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-45-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_XGB.plot\_model()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{010_num_spot_sklearn_surrogate_files/figure-pdf/cell-46-output-1.pdf}}

\section{Jupyter Notebook}\label{jupyter-notebook-9}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/010_num_spot_sklearn_surrogate.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Sequential Parameter Optimization: Gaussian Process
Models}\label{sec-gaussian-process-models}

This chapter analyzes differences between the \texttt{Kriging}
implementation in \texttt{spotpython} and the
\texttt{GaussianProcessRegressor} in \texttt{scikit-learn}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ m}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Gaussian Processes Regression: Basic
Introductory \texttt{scikit-learn}
Example}{Gaussian Processes Regression: Basic Introductory scikit-learn Example}}\label{gaussian-processes-regression-basic-introductory-scikit-learn-example}

\begin{itemize}
\item
  This is the example from
  \href{https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html}{scikit-learn:
  https://scikit-learn.org/stable/auto\_examples/gaussian\_process/plot\_gpr\_noisy\_targets.html}
\item
  After fitting our model, we see that the hyperparameters of the kernel
  have been optimized.
\item
  Now, we will use our kernel to compute the mean prediction of the full
  dataset and plot the 95\% confidence interval.
\end{itemize}

\subsection{Train and Test Data}\label{train-and-test-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Building the Surrogate With
\texttt{Sklearn}}{Building the Surrogate With Sklearn}}\label{building-the-surrogate-with-sklearn}

\begin{itemize}
\tightlist
\item
  The model building with \texttt{sklearn} consisits of three steps:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Instantiating the model, then
  \item
    fitting the model (using \texttt{fit}), and
  \item
    making predictions (using \texttt{predict})
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{gaussian\_process }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{gaussian\_process.fit(X\_train, y\_train)}
\NormalTok{mean\_prediction, std\_prediction }\OperatorTok{=}\NormalTok{ gaussian\_process.predict(X, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Plotting the
\texttt{Sklearn}Model}{Plotting the SklearnModel}}\label{plotting-the-sklearnmodel}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"sk{-}learn Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-5-output-1.pdf}}

\subsection{\texorpdfstring{The \texttt{spotpython}
Version}{The spotpython Version}}\label{the-spotpython-version}

\begin{itemize}
\tightlist
\item
  The \texttt{spotpython} version is very similar:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Instantiating the model, then
  \item
    fitting the model and
  \item
    making predictions (using \texttt{predict}).
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}
\NormalTok{S\_mean\_prediction, S\_std\_prediction, S\_ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, S\_mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    S\_mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ S\_std\_prediction,}
\NormalTok{    S\_mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ S\_std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"spotpython Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-7-output-1.pdf}}

\subsection{\texorpdfstring{Visualizing the Differences Between the
\texttt{spotpython} and the \texttt{sklearn} Model
Fits}{Visualizing the Differences Between the spotpython and the sklearn Model Fits}}\label{visualizing-the-differences-between-the-spotpython-and-the-sklearn-model-fits}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, S\_mean\_prediction, label}\OperatorTok{=}\StringTok{"spotpython Mean prediction"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Sklearn Mean Prediction"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Comparing Mean Predictions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{011_num_spot_sklearn_gaussian_files/figure-pdf/cell-8-output-1.pdf}}

\section{Exercises}\label{exercises-6}

\subsection{\texorpdfstring{\texttt{Schonlau\ Example\ Function}}{Schonlau Example Function}}\label{schonlau-example-function}

\begin{itemize}
\tightlist
\item
  The Schonlau Example Function is based on sample points only (there is
  no analytical function description available):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{, }\FloatTok{2.}\NormalTok{, }\FloatTok{3.}\NormalTok{, }\FloatTok{4.}\NormalTok{, }\FloatTok{12.}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.75}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotpython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Since there is no analytical function available, you might be
  interested in adding some points and describe the effects.
\end{itemize}

\subsection{\texorpdfstring{\texttt{Forrester\ Example\ Function}}{Forrester Example Function}}\label{forrester-example-function}

\begin{itemize}
\item
  The Forrester Example Function is defined as follows:

  \texttt{f(x)\ =\ (6x-\ 2)\^{}2\ sin(12x-4)\ for\ x\ in\ {[}0,1{]}.}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\FloatTok{0.5}\NormalTok{, stop}\OperatorTok{=}\FloatTok{1.5}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.175}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.375}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X\_train, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotpython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.2}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_runge\ Function\ (1-dim)}}{fun\_runge Function (1-dim)}}\label{fun_runge-function-1-dim}

\begin{itemize}
\item
  The Runge function is defined as follows:

  \texttt{f(x)\ =\ 1/\ (1\ +\ sum(x\_i))\^{}2}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.025}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotpython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.05}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_cubed\ (1-dim)}}{fun\_cubed (1-dim)}}\label{fun_cubed-1-dim}

\begin{itemize}
\item
  The Cubed function is defined as follows:

  \texttt{np.sum(X{[}i{]}**\ 3)}
\item
  Data points are generated as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.025}\NormalTok{,}
\NormalTok{                lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{]))}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_cubed}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Describe the function.
\item
  Compare the two models that were build using the \texttt{spotpython}
  and the \texttt{sklearn} surrogate.
\item
  Note: Modify the noise level (\texttt{"sigma"}), e.g., use a value of
  \texttt{0.05}, and compare the two models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma }\OperatorTok{=} \FloatTok{0.025}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{The Effect of Noise}\label{the-effect-of-noise}

How does the behavior of the \texttt{spotpython} fit changes when the
argument \texttt{noise} is set to \texttt{True}, i.e.,

\texttt{S\ =\ Kriging(name=\textquotesingle{}kriging\textquotesingle{},\ \ seed=123,\ n\_theta=1,\ method="regression")}

is used?

\chapter{Expected Improvement}\label{sec-expected-improvement}

This chapter describes, analyzes, and compares different infill
criterion. An infill criterion defines how the next point \(x_{n+1}\) is
selected from the surrogate model \(S\). Expected improvement is a
popular infill criterion in Bayesian optimization.

\section{\texorpdfstring{Example: \texttt{Spot} and the 1-dim Sphere
Function}{Example: Spot and the 1-dim Sphere Function}}\label{example-spot-and-the-1-dim-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init, design\_control\_init}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: 1-dim
Sphere}\label{the-objective-function-1-dim-sphere}

\begin{itemize}
\tightlist
\item
  The \texttt{spotpython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula: \[f(x) = x^2 \]
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The size of the \texttt{lower} bound vector determines the problem
  dimension.
\item
  Here we will use \texttt{np.array({[}-1{]})}, i.e., a one-dim
  function.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TensorBoard}]

Similar to the one-dimensional case, which was introduced in Section
Section~\ref{sec-visualizing-tensorboard-01}, we can use TensorBoard to
monitor the progress of the optimization. We will use the same code,
only the prefix is different:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_Y"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),)}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ Spot(}
\NormalTok{            fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{            fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{            design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 6.690918515799129e-09 [####------] 44.00% 
spotpython tuning: 6.719979618922052e-11 [#####-----] 48.00% 
spotpython tuning: 6.719979618922052e-11 [#####-----] 52.00% 
spotpython tuning: 6.719979618922052e-11 [######----] 56.00% 
spotpython tuning: 6.719979618922052e-11 [######----] 60.00% 
spotpython tuning: 6.719979618922052e-11 [######----] 64.00% 
spotpython tuning: 6.719979618922052e-11 [#######---] 68.00% 
spotpython tuning: 6.719979618922052e-11 [#######---] 72.00% 
spotpython tuning: 6.719979618922052e-11 [########--] 76.00% 
spotpython tuning: 6.719979618922052e-11 [########--] 80.00% 
spotpython tuning: 6.719979618922052e-11 [########--] 84.00% 
spotpython tuning: 6.719979618922052e-11 [#########-] 88.00% 
spotpython tuning: 6.719979618922052e-11 [#########-] 92.00% 
spotpython tuning: 6.719979618922052e-11 [##########] 96.00% 
spotpython tuning: 6.719979618922052e-11 [##########] 100.00% Done...

Experiment saved to 07_Y_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x14f8ba5a0>
\end{verbatim}

\subsection{Results}\label{results-6}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 6.719979618922052e-11
x0: 8.197548181573593e-06
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(8.197548181573593e-06)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-7-output-1.pdf}}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/07_tensorboard_Y.png}

}

\caption{TensorBoard visualization of the spotpython optimization
process and the surrogate model.}

\end{figure}%

\section{Same, but with EI as
infill\_criterion}\label{same-but-with-ei-as-infill_criterion}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_EI\_ISO"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{    infill\_criterion }\OperatorTok{=} \StringTok{"ei"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{spot\_1\_ei.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 6.690918515799129e-09 [####------] 44.00% 
spotpython tuning: 6.719979618922052e-11 [#####-----] 48.00% 
spotpython tuning: 6.719979618922052e-11 [#####-----] 52.00% 
spotpython tuning: 6.719979618922052e-11 [######----] 56.00% 
spotpython tuning: 6.719979618922052e-11 [######----] 60.00% 
spotpython tuning: 6.719979618922052e-11 [######----] 64.00% 
spotpython tuning: 6.719979618922052e-11 [#######---] 68.00% 
spotpython tuning: 6.719979618922052e-11 [#######---] 72.00% 
spotpython tuning: 6.719979618922052e-11 [########--] 76.00% 
spotpython tuning: 6.719979618922052e-11 [########--] 80.00% 
spotpython tuning: 6.719979618922052e-11 [########--] 84.00% 
spotpython tuning: 6.719979618922052e-11 [#########-] 88.00% 
spotpython tuning: 6.719979618922052e-11 [#########-] 92.00% 
spotpython tuning: 6.719979618922052e-11 [##########] 96.00% 
spotpython tuning: 6.719979618922052e-11 [##########] 100.00% Done...

Experiment saved to 07_EI_ISO_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x153b7e300>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-10-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_ei.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 6.719979618922052e-11
x0: 8.197548181573593e-06
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(8.197548181573593e-06)]]
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/07_tensorboard_EI_ISO.png}

}

\caption{TensorBoard visualization of the spotpython optimization
process and the surrogate model. Expected improvement, isotropic
Kriging.}

\end{figure}%

\section{Non-isotropic Kriging}\label{non-isotropic-kriging}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"07\_EI\_NONISO"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{    infill\_criterion }\OperatorTok{=} \StringTok{"ei"}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(}
\NormalTok{    n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_ei\_noniso.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.001300040921033532 [####------] 44.00% 
spotpython tuning: 0.00032084547313593353 [#####-----] 48.00% 
spotpython tuning: 0.00019406280738811914 [#####-----] 52.00% 
spotpython tuning: 0.00015939359695595043 [######----] 56.00% 
spotpython tuning: 0.00014737851302527218 [######----] 60.00% 
spotpython tuning: 0.0001458874607475838 [######----] 64.00% 
spotpython tuning: 0.0001458874607475838 [#######---] 68.00% 
spotpython tuning: 0.0001458874607475838 [#######---] 72.00% 
spotpython tuning: 0.00014498403599255805 [########--] 76.00% 
spotpython tuning: 0.0001393597102409414 [########--] 80.00% 
spotpython tuning: 0.00010625540134140391 [########--] 84.00% 
spotpython tuning: 5.864391313750128e-05 [#########-] 88.00% 
spotpython tuning: 4.049967051189472e-05 [#########-] 92.00% 
spotpython tuning: 2.5450459678851107e-05 [##########] 96.00% 
spotpython tuning: 1.6373659109908014e-05 [##########] 100.00% Done...

Experiment saved to 07_EI_NONISO_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x151d52ba0>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-14-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.6373659109908014e-05
x0: 0.004003772156272264
x1: 0.0005860611150442899
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.004003772156272264)],
 ['x1', np.float64(0.0005860611150442899)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_ei\_noniso.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-16-output-1.pdf}}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/07_tensorboard_EI_NONISO.png}

}

\caption{TensorBoard visualization of the spotpython optimization
process and the surrogate model. Expected improvement, isotropic
Kriging.}

\end{figure}%

\section{\texorpdfstring{Using \texttt{sklearn}
Surrogates}{Using sklearn Surrogates}}\label{using-sklearn-surrogates}

\subsection{The spot Loop}\label{the-spot-loop}

The \texttt{spot} loop consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Init: Build initial design \(X\)
\item
  Evaluate initial design on real objective \(f\): \(y = f(X)\)
\item
  Build surrogate: \(S = S(X,y)\)
\item
  Optimize on surrogate: \(X_0 =  \text{optimize}(S)\)
\item
  Evaluate on real objective: \(y_0 = f(X_0)\)
\item
  Impute (Infill) new points: \(X = X \cup X_0\), \(y = y \cup y_0\).
\item
  Got 3.
\end{enumerate}

The \texttt{spot} loop is implemented in \texttt{R} as follows:

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{figures_static/spotModel.png}}

}

\caption{Visual representation of the model based search with SPOT.
Taken from: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter
tuning approaches. In Hyperparameter Tuning for Machine and Deep
Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M.
Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch.~4, pp.~67--114.}

\end{figure}%

\subsection{spot: The Initial Model}\label{spot-the-initial-model}

\subsubsection{Example: Modifying the initial design
size}\label{example-modifying-the-initial-design-size}

This is the ``Example: Modifying the initial design size'' from Chapter
4.5.1 in {[}bart21i{]}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_ei }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                upper}\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])), }
\NormalTok{                design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{))}
\NormalTok{spot\_ei.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.004169879899427079 [####------] 40.00% 
spotpython tuning: 0.004169879899427079 [#####-----] 46.67% 
spotpython tuning: 0.0008971492855200217 [#####-----] 53.33% 
spotpython tuning: 0.00047732364387283296 [######----] 60.00% 
spotpython tuning: 0.00047732364387283296 [#######---] 66.67% 
spotpython tuning: 0.00047732364387283296 [#######---] 73.33% 
spotpython tuning: 0.00043930823984539394 [########--] 80.00% 
spotpython tuning: 0.00040531992496737674 [#########-] 86.67% 
spotpython tuning: 0.00038005812225023545 [#########-] 93.33% 
spotpython tuning: 0.00036548617270809396 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x153c7ec30>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_ei.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-18-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.}\BuiltInTok{min}\NormalTok{(spot\_1.y), np.}\BuiltInTok{min}\NormalTok{(spot\_ei.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(np.float64(6.719979618922052e-11), np.float64(0.00036548617270809396))
\end{verbatim}

\subsection{Init: Build Initial Design}\label{init-build-initial-design}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 8.97647221 13.41926847]
 [ 0.66946019  1.22344228]
 [ 5.23614115 13.78185824]
 [ 5.6149825  11.5851384 ]
 [-1.72963184  1.66516096]
 [-4.26945568  7.1325531 ]
 [ 1.26363761 10.17935555]
 [ 2.88779942  8.05508969]
 [-3.39111089  4.15213772]
 [ 7.30131231  5.22275244]]
[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975
  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{S.fit(X, y)}
\NormalTok{S.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-21-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{345}\NormalTok{)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X0, X1, X2, X3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]),
 array([[0.78373509, 0.86811887],
        [0.06692621, 0.6058029 ],
        [0.41374778, 0.00525456]]),
 array([[0.121357  , 0.69043832],
        [0.41906219, 0.32838498],
        [0.86742658, 0.52910374]]),
 array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]))
\end{verbatim}

\subsection{Evaluate}\label{evaluate}

\subsection{Build Surrogate}\label{build-surrogate}

\subsection{A Simple Predictor}\label{a-simple-predictor}

The code below shows how to use a simple model for prediction.

\begin{itemize}
\item
  Assume that only two (very costly) measurements are available:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    f(0) = 0.5
  \item
    f(2) = 2.5
  \end{enumerate}
\item
  We are interested in the value at \(x_0 = 1\), i.e., \(f(x_0 = 1)\),
  but cannot run an additional, third experiment.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{], [}\DecValTok{2}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{])}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ S\_lm.fit(X, y)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{]])}
\NormalTok{y0 }\OperatorTok{=}\NormalTok{ S\_lm.predict(X0)}
\BuiltInTok{print}\NormalTok{(y0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.5]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Central Idea:

  \begin{itemize}
  \tightlist
  \item
    Evaluation of the surrogate model \texttt{S\_lm} is much cheaper (or
    / and much faster) than running the real-world experiment \(f\).
  \end{itemize}
\end{itemize}

\section{Gaussian Processes regression: basic introductory
example}\label{gaussian-processes-regression-basic-introductory-example}

This example was taken from
\href{https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html}{scikit-learn}.
After fitting our model, we see that the hyperparameters of the kernel
have been optimized. Now, we will use our kernel to compute the mean
prediction of the full dataset and plot the 95\% confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ m}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}

\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{gaussian\_process }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{gaussian\_process.fit(X\_train, y\_train)}
\NormalTok{gaussian\_process.kernel\_}

\NormalTok{mean\_prediction, std\_prediction }\OperatorTok{=}\NormalTok{ gaussian\_process.predict(X, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"sk{-}learn Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-24-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{1\_000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.squeeze(X }\OperatorTok{*}\NormalTok{ np.sin(X))}
\NormalTok{training\_indices }\OperatorTok{=}\NormalTok{ rng.choice(np.arange(y.size), size}\OperatorTok{=}\DecValTok{6}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{X\_train, y\_train }\OperatorTok{=}\NormalTok{ X[training\_indices], y[training\_indices]}


\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{std\_prediction}

\NormalTok{plt.plot(X, y, label}\OperatorTok{=}\VerbatimStringTok{r"$f(x) = x \textbackslash{}sin(x)$"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"dotted"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\NormalTok{plt.fill\_between(}
\NormalTok{    X.ravel(),}
\NormalTok{    mean\_prediction }\OperatorTok{{-}} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    mean\_prediction }\OperatorTok{+} \FloatTok{1.96} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"spotpython Version: Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-25-output-1.pdf}}

\section{The Surrogate: Using scikit-learn
models}\label{the-surrogate-using-scikit-learn-models}

Default is the internal \texttt{kriging} surrogate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_0 }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Models from \texttt{scikit-learn} can be selected, e.g., Gaussian
Process:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  and many more:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_Tree }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{S\_LM }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_Ridge }\OperatorTok{=}\NormalTok{ linear\_model.Ridge()}
\NormalTok{S\_RF }\OperatorTok{=}\NormalTok{ RandomForestRegressor(max\_depth}\OperatorTok{=}\DecValTok{2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The scikit-learn GP model \texttt{S\_GP} is selected.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ S\_GP}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{isinstance}\NormalTok{(S, GaussianProcessRegressor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{15}\NormalTok{)    }
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{spot\_GP }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun, }
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate}\OperatorTok{=}\NormalTok{S, }
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_GP.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 24.51465459019188 [####------] 40.00% 
spotpython tuning: 11.003092545432404 [#####-----] 46.67% 
spotpython tuning: 11.003092545432404 [#####-----] 53.33% 
spotpython tuning: 7.281405479109784 [######----] 60.00% 
spotpython tuning: 7.281405479109784 [#######---] 66.67% 
spotpython tuning: 7.281405479109784 [#######---] 73.33% 
spotpython tuning: 2.9520033012954237 [########--] 80.00% 
spotpython tuning: 2.9520033012954237 [#########-] 86.67% 
spotpython tuning: 2.1049818033904044 [#########-] 93.33% 
spotpython tuning: 1.9431597967021723 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x151d52d80>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([ 69.32459936, 152.38491454, 107.92560483,  24.51465459,
        76.73500031,  86.30426863,  11.00309255,  16.11758333,
         7.28140548,  21.82343562,  10.96088904,   2.9520033 ,
         3.02912616,   2.1049818 ,   1.9431598 ])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-34-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_GP.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 1.9431597967021723
x0: 10.0
x1: 2.99858238342458
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(10.0)], ['x1', np.float64(2.99858238342458)]]
\end{verbatim}

\section{Additional Examples}\label{additional-examples}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for the sklearn surrogates:}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process }\ImportTok{import}\NormalTok{ GaussianProcessRegressor}
\ImportTok{from}\NormalTok{ sklearn.gaussian\_process.kernels }\ImportTok{import}\NormalTok{ RBF}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ tree}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel }\OperatorTok{=} \DecValTok{1} \OperatorTok{*}\NormalTok{ RBF(length\_scale}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, length\_scale\_bounds}\OperatorTok{=}\NormalTok{(}\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e2}\NormalTok{))}
\NormalTok{S\_GP }\OperatorTok{=}\NormalTok{ GaussianProcessRegressor(kernel}\OperatorTok{=}\NormalTok{kernel, n\_restarts\_optimizer}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotpython}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}

\NormalTok{S\_K }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{              seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{              log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{              infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{              n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{              method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{,}
\NormalTok{              cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=} \DecValTok{25}\NormalTok{)}

\NormalTok{spot\_S\_K }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     surrogate}\OperatorTok{=}\NormalTok{S\_K,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                     surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_S\_K.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.09898480231445653 [##--------] 24.00% 
spotpython tuning: 0.011600518140914177 [###-------] 28.00% 
spotpython tuning: 0.003049615554913117 [###-------] 32.00% 
spotpython tuning: 0.003049615554913117 [####------] 36.00% 
spotpython tuning: 0.0005799644213399475 [####------] 40.00% 
spotpython tuning: 0.00032701651343083606 [####------] 44.00% 
spotpython tuning: 0.00026223112073428154 [#####-----] 48.00% 
spotpython tuning: 0.00025012027991100133 [#####-----] 52.00% 
spotpython tuning: 0.00025012027991100133 [######----] 56.00% 
spotpython tuning: 0.00025012027991100133 [######----] 60.00% 
spotpython tuning: 0.00025012027991100133 [######----] 64.00% 
spotpython tuning: 0.00025012027991100133 [#######---] 68.00% 
spotpython tuning: 0.0002490731534486115 [#######---] 72.00% 
spotpython tuning: 0.00024350109120356873 [########--] 76.00% 
spotpython tuning: 0.00019117601867302202 [########--] 80.00% 
spotpython tuning: 8.697745687224928e-05 [########--] 84.00% 
spotpython tuning: 7.454180600841285e-05 [#########-] 88.00% 
spotpython tuning: 7.393231742566199e-05 [#########-] 92.00% 
spotpython tuning: 7.393231742566199e-05 [##########] 96.00% 
spotpython tuning: 7.393231742566199e-05 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x1625301d0>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-39-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-40-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_S\_K.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 7.393231742566199e-05
x0: -0.00047899231068674524
x1: 0.00858503836869498
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(-0.00047899231068674524)],
 ['x1', np.float64(0.00858503836869498)]]
\end{verbatim}

\subsection{Optimize on Surrogate}\label{optimize-on-surrogate}

\subsection{Evaluate on Real
Objective}\label{evaluate-on-real-objective}

\subsection{Impute / Infill new Points}\label{impute-infill-new-points}

\section{Tests}\label{tests}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}

\NormalTok{fun\_sphere }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{                    lower}\OperatorTok{=}\NormalTok{np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                    upper}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{                    n\_points }\OperatorTok{=} \DecValTok{2}\NormalTok{)}
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ Spot(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{fun\_sphere,}
\NormalTok{    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{)}

\CommentTok{\# (S{-}2) Initial Design:}
\NormalTok{spot\_1.X }\OperatorTok{=}\NormalTok{ spot\_1.design.scipy\_lhd(}
\NormalTok{    spot\_1.design\_control[}\StringTok{"init\_size"}\NormalTok{], lower}\OperatorTok{=}\NormalTok{spot\_1.lower, upper}\OperatorTok{=}\NormalTok{spot\_1.upper}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(spot\_1.X)}

\CommentTok{\# (S{-}3): Eval initial design:}
\NormalTok{spot\_1.y }\OperatorTok{=}\NormalTok{ spot\_1.fun(spot\_1.X)}
\BuiltInTok{print}\NormalTok{(spot\_1.y)}

\NormalTok{spot\_1.fit\_surrogate()}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ spot\_1.suggest\_new\_X()}
\BuiltInTok{print}\NormalTok{(X0)}
\ControlFlowTok{assert}\NormalTok{ X0.size }\OperatorTok{==}\NormalTok{ spot\_1.n\_points }\OperatorTok{*}\NormalTok{ spot\_1.k}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.86352963  0.7892358 ]
 [-0.24407197 -0.83687436]
 [ 0.36481882  0.8375811 ]
 [ 0.415331    0.54468512]
 [-0.56395091 -0.77797854]
 [-0.90259409 -0.04899292]
 [-0.16484832  0.35724741]
 [ 0.05170659  0.07401196]
 [-0.78548145 -0.44638164]
 [ 0.64017497 -0.30363301]]
[1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764
 0.15480068 0.00815134 0.81623768 0.502017  ]
[[0.02672918 0.02430251]
 [0.02673036 0.02430322]]
\end{verbatim}

\section{EI: The Famous Schonlau
Example}\label{ei-the-famous-schonlau-example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_train0 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{12}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{10}\NormalTok{, num}\OperatorTok{=}\DecValTok{5}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{, }\FloatTok{2.}\NormalTok{, }\FloatTok{3.}\NormalTok{, }\FloatTok{4.}\NormalTok{, }\FloatTok{12.}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.75}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{5.}\NormalTok{])}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\ControlFlowTok{if} \VariableTok{True}\NormalTok{:}
\NormalTok{    plt.fill\_between(}
\NormalTok{        X.ravel(),}
\NormalTok{        mean\_prediction }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        mean\_prediction }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-44-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\CommentTok{\# plt.scatter(X\_train, y\_train, label="Observations")}
\NormalTok{plt.plot(X, }\OperatorTok{{-}}\NormalTok{ei, label}\OperatorTok{=}\StringTok{"Expected Improvement"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-45-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.get\_model\_params()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'log_theta_lambda': array([-0.96348969]),
 'U': array([[1.00000001e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
         0.00000000e+00],
        [8.96936418e-01, 4.42159561e-01, 0.00000000e+00, 0.00000000e+00,
         0.00000000e+00],
        [6.49160127e-01, 7.11691192e-01, 2.68489834e-01, 0.00000000e+00,
         0.00000000e+00],
        [3.79752007e-01, 6.97817607e-01, 5.72781159e-01, 2.01892941e-01,
         0.00000000e+00],
        [2.62778955e-06, 4.93854753e-05, 5.35595721e-04, 3.72289632e-03,
         9.99992933e-01]]),
 'X': array([[ 1.],
        [ 2.],
        [ 3.],
        [ 4.],
        [12.]]),
 'y': array([ 0.  , -1.75, -2.  , -0.5 ,  5.  ]),
 'negLnLike': np.float64(1.3376589344577305)}
\end{verbatim}

\section{EI: The Forrester Example}\label{ei-the-forrester-example}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ spotpython}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}

\CommentTok{\# exact x locations are unknown:}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.175}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.375}\NormalTok{, }\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_EI\_FORRESTER"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{1.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ fun(X\_train, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{, cod\_type}\OperatorTok{=}\StringTok{"norm"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, stop}\OperatorTok{=}\DecValTok{1}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X, mean\_prediction, label}\OperatorTok{=}\StringTok{"Mean prediction"}\NormalTok{)}
\ControlFlowTok{if} \VariableTok{True}\NormalTok{:}
\NormalTok{    plt.fill\_between(}
\NormalTok{        X.ravel(),}
\NormalTok{        mean\_prediction }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        mean\_prediction }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ std\_prediction,}
\NormalTok{        alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\VerbatimStringTok{r"95}\SpecialCharTok{\% c}\VerbatimStringTok{onfidence interval"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-47-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\CommentTok{\# plt.scatter(X\_train, y\_train, label="Observations")}
\NormalTok{plt.plot(X, }\OperatorTok{{-}}\NormalTok{ei, label}\OperatorTok{=}\StringTok{"Expected Improvement"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noise{-}free dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-48-output-1.pdf}}

\section{Noise}\label{noise}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotpython}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_Y"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[-1.57464135 16.13714981  2.77008442 93.14904827 71.59322218 14.28895359
 15.9770567  12.96468767 39.82265329 59.88028242]
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-49-output-2.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.get\_model\_params()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'log_theta_lambda': array([-2.8650639]),
 'U': array([[ 1.00000001e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.70233664e-01,  2.42170709e-01,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.98422699e-01,  5.51288247e-02,  1.06274479e-02,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 8.97612652e-01, -3.83100587e-01, -1.48544680e-01,
          1.59561775e-01,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 8.97802437e-01,  4.33533096e-01, -4.95341501e-02,
          2.65097408e-02,  5.33240574e-02,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.75897078e-01,  2.18154520e-01,  3.80893163e-03,
         -1.06623799e-03, -1.89880937e-03,  3.77658329e-03,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.83463162e-01, -1.70629980e-01, -4.39109597e-02,
          3.68970891e-02,  7.81794822e-04,  6.46676677e-05,
          1.98859254e-02,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.89382452e-01, -1.37921302e-01, -3.34099058e-02,
          2.61135398e-02,  5.33523731e-04,  4.66725261e-05,
          1.66561653e-02,  4.92346777e-03,  0.00000000e+00,
          0.00000000e+00],
        [ 9.57003753e-01, -2.65908685e-01, -8.19094811e-02,
          7.91152594e-02,  1.32031981e-03,  1.17850785e-04,
          1.81285930e-02, -3.34173608e-03,  1.12448081e-02,
          0.00000000e+00],
        [ 9.09754374e-01,  4.10161726e-01, -4.09836684e-02,
          2.16163708e-02,  4.40634520e-02, -7.94485222e-04,
         -3.31779455e-04,  4.06237708e-05, -1.78780589e-04,
          5.00610297e-03]]),
 'X': array([[ 0.63529627],
        [-4.10764204],
        [-0.44071975],
        [ 9.63125638],
        [-8.3518118 ],
        [-3.62418901],
        [ 4.15331   ],
        [ 3.4468512 ],
        [ 6.36049088],
        [-7.77978539]]),
 'y': array([-1.57464135, 16.13714981,  2.77008442, 93.14904827, 71.59322218,
        14.28895359, 15.9770567 , 12.96468767, 39.82265329, 59.88028242]),
 'negLnLike': np.float64(23.978516035724688)}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\CommentTok{\#plt.plot(X, y, label=r"$f(x) = x \textbackslash{}sin(x)$", linestyle="dotted")}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-51-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.get\_model\_params()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'log_theta_lambda': array([-2.62131641, -3.75862539]),
 'U': array([[ 1.00008716e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.48327184e-01,  3.17568707e-01,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.97149951e-01,  7.28390322e-02,  2.36808169e-02,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 8.27434508e-01, -4.42368906e-01, -1.65566198e-01,
          3.04013241e-01,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 8.27741190e-01,  5.46207104e-01, -6.50155056e-02,
          5.81284294e-02,  9.52403399e-02,  0.00000000e+00,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.58051531e-01,  2.86208289e-01,  6.68282049e-03,
         -3.36541069e-03,  1.53206920e-04,  1.84499595e-02,
          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.71109280e-01, -2.16060531e-01, -4.64176726e-02,
          8.25053371e-02,  8.45582501e-03, -4.90044955e-03,
          3.71712011e-02,  0.00000000e+00,  0.00000000e+00,
          0.00000000e+00],
        [ 9.81377837e-01, -1.75909975e-01, -3.36315238e-02,
          6.14634924e-02,  6.93957968e-03, -4.36266056e-03,
          2.84741202e-02,  1.84593200e-02,  0.00000000e+00,
          0.00000000e+00],
        [ 9.25777067e-01, -3.26826942e-01, -9.19064015e-02,
          1.61897427e-01,  1.02726000e-02, -4.96607986e-03,
          3.03391347e-02,  4.64846247e-03,  2.37798355e-02,
          0.00000000e+00],
        [ 8.47153093e-01,  5.20486762e-01, -5.38733186e-02,
          4.83017604e-02,  7.75510320e-02,  1.55114657e-03,
         -6.49927513e-04, -1.94890748e-04, -8.83882576e-05,
          1.86297412e-02]]),
 'X': array([[ 0.63529627],
        [-4.10764204],
        [-0.44071975],
        [ 9.63125638],
        [-8.3518118 ],
        [-3.62418901],
        [ 4.15331   ],
        [ 3.4468512 ],
        [ 6.36049088],
        [-7.77978539]]),
 'y': array([-1.57464135, 16.13714981,  2.77008442, 93.14904827, 71.59322218,
        14.28895359, 15.9770567 , 12.96468767, 39.82265329, 59.88028242]),
 'negLnLike': np.float64(22.35624628588101)}
\end{verbatim}

\section{Cubic Function}\label{cubic-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotpython}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_Y"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{10.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Cubed: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[  -9.63480707  -72.98497325   12.7936499   895.34567477 -573.35961837
  -41.83176425   65.27989461   46.37081417  254.1530734  -474.09587355]
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-53-output-2.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{0}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Cubed: Gaussian process with nugget regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-54-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotpython}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"07\_Y"}\NormalTok{,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\NormalTok{y.shape}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{, method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.63529627]
 [-4.10764204]
 [-0.44071975]
 [ 9.63125638]
 [-8.3518118 ]
 [-3.62418901]
 [ 4.15331   ]
 [ 3.4468512 ]
 [ 6.36049088]
 [-7.77978539]]
[ 0.46517267 -0.03599548  1.15933822  0.05915901  0.24419145  0.21502359
 -0.10432134  0.21312309 -0.05502681 -0.06434374]
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-55-output-2.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-56-output-1.pdf}}

\section{Modifying Lambda Search
Space}\label{modifying-lambda-search-space}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{,}
\NormalTok{            min\_Lambda}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{            max\_Lambda}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Lambda: }\SpecialCharTok{\{}\NormalTok{S}\SpecialCharTok{.}\NormalTok{Lambda}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Lambda: -0.0698939568220746
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\CommentTok{\#plt.plot(X, ei, label="Expected Improvement")}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Gaussian process regression with nugget on noisy dataset. Modified Lambda search space."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{012_num_spot_ei_files/figure-pdf/cell-58-output-1.pdf}}

\chapter{Handling Noise}\label{sec-noise}

This chapter demonstrates how noisy functions can be handled by
\texttt{Spot} and how noise can be simulated, i.e., added to the
objective function.

\section{\texorpdfstring{Example: \texttt{Spot} and the Noisy Sphere
Function}{Example: Spot and the Noisy Sphere Function}}\label{example-spot-and-the-noisy-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"08"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: Noisy
Sphere}\label{the-objective-function-noisy-sphere}

The \texttt{spotpython} package provides several classes of objective
functions, which return a one-dimensional output \(y=f(x)\) for a given
input \(x\) (independent variable). Several objective functions allow
one- or multidimensional input, some also combinations of real-valued
and categorial input values.

An objective function is considered as ``analytical'' if it can be
described by a closed mathematical formula, e.g., \[
f(x, y) = x^2 + y^2.
\]

To simulate measurement errors, adding artificial noise to the function
value \(y\) is a common practice, e.g.,:

\[
f(x, y) = x^2 + y^2 + \epsilon.
\]

Usually, noise is assumed to be normally distributed with mean \(\mu=0\)
and standard deviation \(\sigma\). spotpython uses numpy's
\texttt{scale} parameter, which specifies the standard deviation (spread
or ``width'') of the distribution is used. This must be a non-negative
value, see
\url{https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The sphere function without noise}]

The default setting does not use any noise.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-3-output-1.pdf}}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The sphere function with noise}]

Noise can be added to the sphere function as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{).fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-4-output-1.pdf}}

\end{tcolorbox}

\subsection{Reproducibility: Noise Generation and Seed
Handling}\label{reproducibility-noise-generation-and-seed-handling}

spotpython provides two mechanisms for generating random noise:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The seed is initialized once, i.e., when the objective function is
  instantiated. This can be done using the following call:
  \texttt{fun\ =\ Analytical(sigma=0.02,\ seed=123).fun\_sphere}.
\item
  The seed is set every time the objective function is called. This can
  be done using the following call:
  \texttt{y\ =\ fun(x,\ sigma=0.02,\ seed=123)}.
\end{enumerate}

These two different ways lead to different results as explained in the
following tables:

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Noise added to the sphere function}]

Since \texttt{sigma} is set to \texttt{0.02}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{fun(x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0: [0.98021757]
1: [0.98021757]
2: [0.98021757]
\end{verbatim}

The seed is set once. Every call to \texttt{fun()} results in a
different value. The whole experiment can be repeated, the initial seed
is used to generate the same sequence as shown below:

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: Noise added to the sphere function}]

Since \texttt{sigma} is set to \texttt{0.02}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical(sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{).fun\_sphere}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{fun(x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0: [0.98021757]
1: [0.98021757]
2: [0.98021757]
\end{verbatim}

\end{tcolorbox}

If \texttt{spotpython} is used as a hyperparameter tuner, it is
important that only one realization of the noise function is optimized.
This behaviour can be accomplished by passing the same seed via the
dictionary \texttt{fun\_control} to every call of the objective function
\texttt{fun} as shown below:

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Example: The same noise added to the sphere function}]

Since \texttt{sigma} is set to \texttt{0.02}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{fun(x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0: [0.98021757]
1: [0.98021757]
2: [0.98021757]
\end{verbatim}

\end{tcolorbox}

\section{spotpython's Noise Handling
Approaches}\label{spotpythons-noise-handling-approaches}

The following setting will be used for the next steps:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.02}\NormalTok{,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{spotpython} is adopted as follows to cope with noisy functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_repeats} is set to a value larger than 1 (here: 2)
\item
  \texttt{noise} is set to \texttt{true}. Therefore, a nugget
  (\texttt{Lambda}) term is added to the correlation matrix
\item
  \texttt{init\ size} (of the \texttt{design\_control} dictionary) is
  set to a value larger than 1 (here: 3)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                                    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                                    upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                                    fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                                    fun\_repeats }\OperatorTok{=} \DecValTok{2}\NormalTok{,}
\NormalTok{                                    noise }\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                                    show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{3}\NormalTok{, repeats}\OperatorTok{=}\DecValTok{2}\NormalTok{),}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.run()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-2.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475488114202547 [####------] 40.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-4.pdf}}

\begin{verbatim}
spotpython tuning: 0.034754842384453005 [#####-----] 50.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-6.pdf}}

\begin{verbatim}
spotpython tuning: 0.0347547994840775 [######----] 60.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-8.pdf}}

\begin{verbatim}
spotpython tuning: 0.034754767383189285 [#######---] 70.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-10.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475472191365096 [########--] 80.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-12.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475462984552639 [#########-] 90.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-10-output-14.pdf}}

\begin{verbatim}
spotpython tuning: 0.03475448487120397 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\section{Print the Results}\label{print-the-results-3}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.03475448487120397
min mean y: 0.03475448487120397
x0: 0.18642554779644332
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.18642554779644332)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+} \StringTok{"\_progress.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-12-output-1.pdf}}

}

\caption{Progress plot. \emph{Black} dots denote results from the
initial design. \emph{Red} dots illustrate the improvement found by the
surrogate model based optimization.}

\end{figure}%

\section{Noise and Surrogates: The Nugget
Effect}\label{noise-and-surrogates-the-nugget-effect}

\subsection{The Noisy Sphere}\label{the-noisy-sphere}

\subsubsection{The Data}\label{the-data}

\begin{itemize}
\tightlist
\item
  We prepare some data first:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotpython}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  A surrogate without nugget is fitted to these data:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-14-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  In comparison to the surrogate without nugget, we fit a surrogate with
  nugget to the data:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{)}
\NormalTok{S\_nug.fit(X\_train, y\_train)}
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S\_nug.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{013_num_spot_noisy_files/figure-pdf/cell-15-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The value of the nugget term can be extracted from the model as
  follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(-2.983429108510847)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We see:

  \begin{itemize}
  \tightlist
  \item
    the first model \texttt{S} has no nugget,
  \item
    whereas the second model has a nugget value (\texttt{Lambda}) larger
    than zero.
  \end{itemize}
\end{itemize}

\section{Exercises}\label{exercises-7}

\subsection{\texorpdfstring{Noisy
\texttt{fun\_cubed}}{Noisy fun\_cubed}}\label{noisy-fun_cubed}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_cubed} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_runge} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_forrester}}{fun\_forrester}}\label{fun_forrester}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_forrester} function
  with the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_xsin}}{fun\_xsin}}\label{fun_xsin}

\begin{itemize}
\tightlist
\item
  Analyse the effect of noise on the \texttt{fun\_xsin} function with
  the following settings:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\FloatTok{1.}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_xsin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\chapter{Optimal Computational Budget Allocation in
spotpython}\label{sec-ocba}

This chapter demonstrates how noisy functions can be handled
\texttt{spotpython}:

\begin{itemize}
\tightlist
\item
  First, Optimal Computational Budget Allocation (OCBA) is introduced in
  Chapter~\ref{sec-ocba}.
\item
  Then, the nugget effect is explained in Section~\ref{sec-nugget}.
\end{itemize}

\section*{Citation}\label{citation-1}
\addcontentsline{toc}{section}{Citation}

\markright{Citation}

If this document has been useful to you and you wish to cite it in a
scientific publication, please refer to the following paper, which can
be found on arXiv: \url{https://arxiv.org/abs/2307.10262}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{@ARTICLE\{bart23iArXiv,}
\NormalTok{      author = \{\{Bartz{-}Beielstein\}, Thomas\},}
\NormalTok{      title = "\{Hyperparameter Tuning Cookbook:}
\NormalTok{          A guide for scikit{-}learn, PyTorch, river, and spotpython\}",}
\NormalTok{     journal = \{arXiv e{-}prints\},}
\NormalTok{    keywords = \{Computer Science {-} Machine Learning,}
\NormalTok{      Computer Science {-} Artificial Intelligence, 90C26, I.2.6, G.1.6\},}
\NormalTok{         year = 2023,}
\NormalTok{        month = jul,}
\NormalTok{          eid = \{arXiv:2307.10262\},}
\NormalTok{          doi = \{10.48550/arXiv.2307.10262\},}
\NormalTok{archivePrefix = \{arXiv\},}
\NormalTok{       eprint = \{2307.10262\},}
\NormalTok{ primaryClass = \{cs.LG\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Example: \texttt{spotpython}, OCBA, and the
Noisy Sphere
Function}{Example: spotpython, OCBA, and the Noisy Sphere Function}}\label{example-spotpython-ocba-and-the-noisy-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, get\_spot\_tensorboard\_path}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init}

\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"14"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: Noisy
Sphere}\label{sec-noisy-sphere}

The \texttt{spotpython} package provides several classes of objective
functions. We will use an analytical objective function with noise,
i.e., a function that can be described by a (closed) formula:
\[f(x) = x^2 + \epsilon\]

Since \texttt{sigma} is set to \texttt{0.1}, noise is added to the
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A plot (Figure~\ref{fig-noisy-sphere-14}) illustrates the noise:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/fig-noisy-sphere-14-output-1.pdf}}

}

\caption{\label{fig-noisy-sphere-14}The noisy sphere function with
noise.}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Noise Handling in \texttt{spotpython}}]

\texttt{spotpython} has several options to cope with noisy functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_repeats} is set to a value larger than 1, e.g., 2, which
  means every function evaluation during the search on the surrogate is
  repeated twice. The mean of the two evaluations is used as the
  function value.
\item
  \texttt{init\ size} (of the \texttt{design\_control} dictionary) is
  set to a value larger than 1 (here: 2).
\item
  \texttt{ocba\_delta} is set to a value larger than 1 (here: 2). This
  means that the OCBA algorithm is used to allocate the computational
  budget optimally. An example is given in \textbf{?@sec-ocb-example}.
\item
  Using a nugget term in the surrogate model. This is done by setting
  \texttt{method="regression"} in the \texttt{surrogate\_control}
  dictionary. An example is given in Section~\ref{sec-nugget}.
\end{enumerate}

\end{tcolorbox}

\section{Using Optimal Computational Budget Allocation
(OCBA)}\label{sec-ocba-example}

The
\href{https://en.wikipedia.org/wiki/Optimal_computing_budget_allocation}{Optimal
Computational Budget Allocation} (OCBA) algorithm is a powerful tool for
efficiently distributing computational resources (C. H. Chen 2010). It
is specifically designed to maximize the Probability of Correct
Selection (PCS) while minimizing computational costs. By strategically
allocating more simulation effort to design alternatives that are either
more challenging to evaluate or more likely to yield optimal results,
OCBA ensures an efficient use of resources. This approach enables
researchers and decision-makers to achieve accurate outcomes more
quickly and with fewer computational demands, making it an invaluable
method for simulation optimization.

The OCBA algorithm is implemented in \texttt{spotpython} and can be used
by setting \texttt{ocba\_delta} to a value larger than \texttt{0}. The
source code is available in the \texttt{spotpython} package, see
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/budget/ocba/}{{[}DOC{]}}.
See also Bartz-Beielstein et al. (2011).

\begin{example}[]\protect\hypertarget{exm-ocba}{}\label{exm-ocba}

To reproduce the example from p.49 in C. H. Chen (2010), the following
\texttt{spotpython} code can be used:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.budget.ocba }\ImportTok{import}\NormalTok{ get\_ocba}
\NormalTok{mean\_y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{])}
\NormalTok{var\_y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{get\_ocba(mean\_y, var\_y, }\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([11,  9, 19,  9,  2])
\end{verbatim}

\end{example}

\subsection{The Noisy Sphere}\label{the-noisy-sphere-1}

We will demonstrate the OCBA algorithm on the noisy sphere function
defined in Section~\ref{sec-noisy-sphere}. The OCBA algorithm is used to
allocate the computational budget optimally. This means that the
function evaluations are repeated several times, and the best function
value is used for the next iteration.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Visualizing the Search of the OCBA Algorithm}]

\begin{itemize}
\tightlist
\item
  The \texttt{show\_models} parameter in the \texttt{fun\_control}
  dictionary is set to \texttt{True}. This means that the surrogate
  model is shown during the search.
\item
  To keep the visualization simple, only the ground truth and the
  surrogate model are shown. The surrogate model is shown in blue, and
  the ground truth is shown in orange. The noisy function was shown in
  Figure~\ref{fig-noisy-sphere-14}.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init( }
\NormalTok{                   lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{                   fun\_evals }\OperatorTok{=} \DecValTok{20}\NormalTok{,}
\NormalTok{                   fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                   noise }\OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                   tolerance\_x}\OperatorTok{=}\FloatTok{0.0}\NormalTok{,}
\NormalTok{                   ocba\_delta }\OperatorTok{=} \DecValTok{2}\NormalTok{,                   }
\NormalTok{                   show\_models}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, repeats}\OperatorTok{=}\DecValTok{2}\NormalTok{),}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.run()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-2.pdf}}

\begin{verbatim}
spotpython tuning: 0.005320352324811128 [######----] 55.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-4.pdf}}

\begin{verbatim}
spotpython tuning: 0.000565644894785201 [######----] 60.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-6.pdf}}

\begin{verbatim}
spotpython tuning: 1.591451674748005e-05 [######----] 65.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-8.pdf}}

\begin{verbatim}
spotpython tuning: 6.91090859790612e-07 [#######---] 70.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-10.pdf}}

\begin{verbatim}
spotpython tuning: 1.8899294735019857e-07 [########--] 75.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-12.pdf}}

\begin{verbatim}
spotpython tuning: 1.2178603905219397e-07 [########--] 80.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-14.pdf}}

\begin{verbatim}
spotpython tuning: 1.0212580354992866e-07 [########--] 85.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-16.pdf}}

\begin{verbatim}
spotpython tuning: 9.357713991612833e-08 [#########-] 90.00% 
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-7-output-18.pdf}}

\begin{verbatim}
spotpython tuning: 9.357713991612833e-08 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\subsection{Print the Results}\label{print-the-results-4}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 9.357713991612833e-08
min mean y: 9.357713991612833e-08
x0: 0.0003059038082733334
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.0003059038082733334)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_1\_noisy.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-9-output-1.pdf}}

\section{Noise and Surrogates: The Nugget Effect}\label{sec-nugget}

In the previous example, we have seen that the \texttt{fun\_repeats}
parameter can be used to repeat function evaluations. This is useful
when the function is noisy. However, it is not always possible to repeat
function evaluations, e.g., when the function is expensive to evaluate.
In this case, we can use a surrogate model with a nugget term. The
nugget term is a small value that is added to the diagonal of the
covariance matrix. This allows the surrogate model to fit the data
better, even if the data is noisy. The nugget term is added, if
\texttt{method="regression"} is set in the \texttt{surrogate\_control}
dictionary.

\subsection{The Noisy Sphere}\label{the-noisy-sphere-2}

\subsubsection{The Data}\label{the-data-1}

We prepare some data first:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ spotpython}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{1}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{125}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

A surrogate without nugget is fitted to these data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{)}
\NormalTok{S.fit(X\_train, y\_train)}

\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}

\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-11-output-1.pdf}}

In comparison to the surrogate without nugget, we fit a surrogate with
nugget to the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S\_nug }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{)}
\NormalTok{S\_nug.fit(X\_train, y\_train)}
\NormalTok{X\_axis }\OperatorTok{=}\NormalTok{ np.linspace(start}\OperatorTok{={-}}\DecValTok{13}\NormalTok{, stop}\OperatorTok{=}\DecValTok{13}\NormalTok{, num}\OperatorTok{=}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mean\_prediction, std\_prediction, ei }\OperatorTok{=}\NormalTok{ S\_nug.predict(X\_axis, return\_val}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\NormalTok{plt.scatter(X\_train, y\_train, label}\OperatorTok{=}\StringTok{"Observations"}\NormalTok{)}
\NormalTok{plt.plot(X\_axis, mean\_prediction, label}\OperatorTok{=}\StringTok{"mue"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.xlabel(}\StringTok{"$x$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"$f(x)$"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{"Sphere: Gaussian process regression with nugget on noisy dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{014_num_spot_ocba_files/figure-pdf/cell-12-output-1.pdf}}

The value of the nugget term can be extracted from the model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10}\OperatorTok{**}\NormalTok{S\_nug.Lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(0.0002592770433217388)
\end{verbatim}

We see:

\begin{itemize}
\tightlist
\item
  the first model \texttt{S} has no nugget,
\item
  whereas the second model has a nugget value (\texttt{Lambda}) larger
  than zero.
\end{itemize}

\section{Exercises}\label{exercises-8}

\subsection{\texorpdfstring{Noisy
\texttt{fun\_cubed}}{Noisy fun\_cubed}}\label{noisy-fun_cubed-1}

Analyse the effect of noise on the \texttt{fun\_cubed} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_cubed}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge-1}

Analyse the effect of noise on the \texttt{fun\_runge} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_runge}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_forrester}}{fun\_forrester}}\label{fun_forrester-1}

Analyse the effect of noise on the \texttt{fun\_forrester} function with
the following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_forrester}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ \{}\StringTok{"sigma"}\NormalTok{: }\DecValTok{5}\NormalTok{,}
               \StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_xsin}}{fun\_xsin}}\label{fun_xsin-1}

Analyse the effect of noise on the \texttt{fun\_xsin} function with the
following settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\FloatTok{1.}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_xsin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(    }
\NormalTok{    sigma}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Jupyter Notebook}\label{jupyter-notebook-10}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this chapter is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/014_num_spot_ocba.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Kriging with Varying
Correlation-p}\label{sec-num-spot-correlation-p}

This chapter illustrates the difference between Kriging models with
varying p.~The difference is illustrated with the help of the
\texttt{spotpython} package.

\section{\texorpdfstring{Example: \texttt{Spot} Surrogate and the 2-dim
Sphere
Function}{Example: Spot Surrogate and the 2-dim Sphere Function}}\label{example-spot-surrogate-and-the-2-dim-sphere-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"015"}
\end{Highlighting}
\end{Shaded}

\subsection{The Objective Function: 2-dim
Sphere}\label{the-objective-function-2-dim-sphere-1}

\begin{itemize}
\tightlist
\item
  The \texttt{spotpython} package provides several classes of objective
  functions.
\item
  We will use an analytical objective function, i.e., a function that
  can be described by a (closed) formula: \[f(x, y) = x^2 + y^2\]
\item
  The size of the \texttt{lower} bound vector determines the problem
  dimension.
\item
  Here we will use \texttt{np.array({[}-1,\ -1{]})}, i.e., a two-dim
  function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{                               lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                               upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Although the default \texttt{spot} surrogate model is an isotropic
  Kriging model, we will explicitly set the \texttt{theta} parameter to
  a value of \texttt{1} for both dimensions. This is done to illustrate
  the difference between isotropic and anisotropic Kriging models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                         p\_val}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}

\NormalTok{spot\_2.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0013050614212698486 [#######---] 73.33% 
spotpython tuning: 0.0003479187873901382 [########--] 80.00% 
spotpython tuning: 0.00022767416623665655 [#########-] 86.67% 
spotpython tuning: 0.00020787497784734184 [#########-] 93.33% 
spotpython tuning: 0.00020393508736265477 [##########] 100.00% Done...

Experiment saved to 015_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x152bba660>
\end{verbatim}

\subsection{Results}\label{results-7}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.00020393508736265477
x0: 0.014161858193549292
x1: 0.0018376234294478113
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.014161858193549292)],
 ['x1', np.float64(0.0018376234294478113)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-7-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-8-output-1.pdf}}

\section{Example With Modified p}\label{example-with-modified-p}

\begin{itemize}
\tightlist
\item
  We can use set \texttt{p} to a value other than \texttt{2} to obtain a
  different Kriging model.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                           p\_val}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\NormalTok{spot\_2\_p1}\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_p1.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0013050614212698486 [#######---] 73.33% 
spotpython tuning: 0.0003479187873901382 [########--] 80.00% 
spotpython tuning: 0.00022767416623665655 [#########-] 86.67% 
spotpython tuning: 0.00020787497784734184 [#########-] 93.33% 
spotpython tuning: 0.00020393508736265477 [##########] 100.00% Done...

Experiment saved to 015_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x15705a600>
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The search progress of the optimization with the anisotropic model can
  be visualized:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-10-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.00020393508736265477
x0: 0.014161858193549292
x1: 0.0018376234294478113
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.014161858193549292)],
 ['x1', np.float64(0.0018376234294478113)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-12-output-1.pdf}}

\subsection{\texorpdfstring{Taking a Look at the \texttt{p}
Values}{Taking a Look at the p Values}}\label{taking-a-look-at-the-p-values}

\subsubsection{\texorpdfstring{\texttt{p} Values from the \texttt{spot}
Model}{p Values from the spot Model}}\label{p-values-from-the-spot-model}

\begin{itemize}
\tightlist
\item
  We can check, which \texttt{p} values the \texttt{spot} model has
  used:
\item
  The \texttt{p} values from the surrogate can be printed as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_p1.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the surrogate from the isotropic setting was stored as
  \texttt{spot\_2}, we can also take a look at the \texttt{theta} value
  from this model:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2
\end{verbatim}

\section{\texorpdfstring{Optimization of the \texttt{p}
Values}{Optimization of the p Values}}\label{optimization-of-the-p-values}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                           optim\_p}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_2\_pm}\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_pm.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0013050614212698486 [#######---] 73.33% 
spotpython tuning: 0.0003479187873901382 [########--] 80.00% 
spotpython tuning: 0.00022767416623665655 [#########-] 86.67% 
spotpython tuning: 0.00020787497784734184 [#########-] 93.33% 
spotpython tuning: 0.00020393508736265477 [##########] 100.00% Done...

Experiment saved to 015_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x159bb3c80>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-16-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.00020393508736265477
x0: 0.014161858193549292
x1: 0.0018376234294478113
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.014161858193549292)],
 ['x1', np.float64(0.0018376234294478113)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-18-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pm.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2
\end{verbatim}

\section{\texorpdfstring{Optimization of Multiple \texttt{p}
Values}{Optimization of Multiple p Values}}\label{optimization-of-multiple-p-values}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_p}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{                                           optim\_p}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_2\_pmo}\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_2\_pmo.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0013050614212698486 [#######---] 73.33% 
spotpython tuning: 0.0003479187873901382 [########--] 80.00% 
spotpython tuning: 0.00022767416623665655 [#########-] 86.67% 
spotpython tuning: 0.00020787497784734184 [#########-] 93.33% 
spotpython tuning: 0.00020393508736265477 [##########] 100.00% Done...

Experiment saved to 015_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x159e52f30>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-21-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.00020393508736265477
x0: 0.014161858193549292
x1: 0.0018376234294478113
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.014161858193549292)],
 ['x1', np.float64(0.0018376234294478113)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{015_num_spot_correlation_p_files/figure-pdf/cell-23-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_2\_pmo.surrogate.p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2
\end{verbatim}

\section{Exercises}\label{exercises-9}

\subsection{\texorpdfstring{\texttt{fun\_branin}}{fun\_branin}}\label{fun_branin}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 10\) and \(0 \leq x_2 \leq 15\).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} runs with different
  options for \texttt{p}.
\item
  Modify the termination criterion: instead of the number of evaluations
  (which is specified via \texttt{fun\_evals}), the time should be used
  as the termination criterion. This can be done as follows
  (\texttt{max\_time=1} specifies a run time of one minute):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{fun\_sin\_cos}}{fun\_sin\_cos}}\label{fun_sin_cos}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-2\pi \leq x_1 \leq 2\pi\) and \(-2\pi \leq x_2 \leq 2\pi\).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} run a) with isotropic and
  b) anisotropic surrogate models.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{\texttt{fun\_runge}}{fun\_runge}}\label{fun_runge-2}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{2}. The search range is
    \(-5 \leq x_1 \leq 5\) and \(-5 \leq x_2 \leq 5\).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} runs with different
  options for \texttt{p}.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\subsection{\texorpdfstring{\texttt{fun\_wingwt}}{fun\_wingwt}}\label{fun_wingwt}

\begin{itemize}
\tightlist
\item
  Describe the function.

  \begin{itemize}
  \tightlist
  \item
    The input dimension is \texttt{10}. The search ranges are between 0
    and 1 (values are mapped internally to their natural bounds).
  \end{itemize}
\item
  Compare the results from \texttt{spotpython} runs with different
  options for \texttt{p}.
\item
  Modify the termination criterion (\texttt{max\_time} instead of
  \texttt{fun\_evals}) as described for \texttt{fun\_branin}.
\end{itemize}

\section{Jupyter Notebook}\label{jupyter-notebook-11}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/015_num_spot_correlation_p.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Factorial Variables}\label{sec-factorial}

Until now, we have considered continuous variables. However, in many
applications, the variables are not continuous, but rather discrete or
categorical. For example, the number of layers in a neural network, the
number of trees in a random forest, or the type of kernel in a support
vector machine are all discrete variables. In the following, we will
consider a simple example with two numerical variables and one
categorical variable.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\end{Highlighting}
\end{Shaded}

First, we generate the test data set for fitting the Kriging model. We
use the \texttt{SpaceFilling} class to generate the first two diemnsion
of \(n=30\) design points. The third dimension is a categorical
variable, which can take the values \(0\), \(1\), or \(2\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{n }\OperatorTok{=} \DecValTok{30}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun\_orig }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin\_factor}

\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(n, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ np.random.randint(low}\OperatorTok{=}\DecValTok{0}\NormalTok{, high}\OperatorTok{=}\DecValTok{3}\NormalTok{, size}\OperatorTok{=}\NormalTok{(n,))}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.c\_[X0, X1]}
\BuiltInTok{print}\NormalTok{(X[:}\DecValTok{5}\NormalTok{,:])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[-2.84117593  5.97308949  0.        ]
 [-3.61017994  6.90781409  0.        ]
 [ 9.91204705  5.09395275  0.        ]
 [-4.4616725   1.3617128   2.        ]
 [-2.40987728  8.05505365  2.        ]]
\end{verbatim}

The objective function is the \texttt{fun\_branin\_factor} in the
\texttt{analytical} class
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/fun/objectivefunctions/\#spotpython.fun.objectivefunctions.analytical.fun_branin_factor}{{[}SOURCE{]}}.
It calculates the Branin function of \((x_1, x_2)\) with an additional
factor based on the value of \(x_3\). If \(x_3 = 1\), the value of the
Branin function is increased by 10. If \(x_3 = 2\), the value of the
Branin function is decreased by 10. Otherwise, the value of the Branin
function is not changed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X)}
\NormalTok{y\_orig }\OperatorTok{=}\NormalTok{ fun\_orig(X0)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ np.c\_[X, y\_orig, y]}
\BuiltInTok{print}\NormalTok{(data[:}\DecValTok{5}\NormalTok{,:])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ -2.84117593   5.97308949   0.          32.09388125  32.09388125]
 [ -3.61017994   6.90781409   0.          43.965223    43.965223  ]
 [  9.91204705   5.09395275   0.           6.25588575   6.25588575]
 [ -4.4616725    1.3617128    2.         212.41884106 202.41884106]
 [ -2.40987728   8.05505365   2.           9.25981051  -0.74018949]]
\end{verbatim}

We fit two Kriging models, one with three numerical variables and one
with two numerical variables and one categorical variable. We then
compare the predictions of the two models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{, method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{, var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{])}
\NormalTok{S.fit(X, y)}
\NormalTok{Sf }\OperatorTok{=}\NormalTok{ Kriging(name}\OperatorTok{=}\StringTok{\textquotesingle{}kriging\textquotesingle{}}\NormalTok{,  seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{, n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{, method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{, var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{, }\StringTok{"factor"}\NormalTok{])}
\NormalTok{Sf.fit(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kriging(eps=np.float64(1.4901161193847656e-08), method='interpolation',
        model_fun_evals=100,
        model_optimizer=<function differential_evolution at 0x10e7b0540>,
        n_theta=3, name='kriging', seed=123, var_type=['num', 'num', 'factor'])
\end{verbatim}

We can now compare the predictions of the two models. We generate a new
test data set and calculate the sum of the absolute differences between
the predictions of the two models and the true values of the objective
function. If the categorical variable is important, the sum of the
absolute differences should be smaller than if the categorical variable
is not important.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{100}
\NormalTok{k }\OperatorTok{=} \DecValTok{100}
\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ np.zeros(n}\OperatorTok{*}\NormalTok{k)}
\NormalTok{y\_pred}\OperatorTok{=}\NormalTok{ np.zeros(n}\OperatorTok{*}\NormalTok{k)}
\NormalTok{y\_factor\_pred}\OperatorTok{=}\NormalTok{ np.zeros(n}\OperatorTok{*}\NormalTok{k)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
\NormalTok{  X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(n, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\NormalTok{  X1 }\OperatorTok{=}\NormalTok{ np.random.randint(low}\OperatorTok{=}\DecValTok{0}\NormalTok{, high}\OperatorTok{=}\DecValTok{3}\NormalTok{, size}\OperatorTok{=}\NormalTok{(n,))}
\NormalTok{  X }\OperatorTok{=}\NormalTok{ np.c\_[X0, X1]}
\NormalTok{  a }\OperatorTok{=}\NormalTok{ i}\OperatorTok{*}\NormalTok{n}
\NormalTok{  b }\OperatorTok{=}\NormalTok{ (i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{n}
\NormalTok{  y\_true[a:b] }\OperatorTok{=}\NormalTok{ fun(X)}
\NormalTok{  y\_pred[a:b] }\OperatorTok{=}\NormalTok{ S.predict(X)}
\NormalTok{  y\_factor\_pred[a:b] }\OperatorTok{=}\NormalTok{ Sf.predict(X)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"y"}\NormalTok{:y\_true, }\StringTok{"Prediction"}\NormalTok{:y\_pred, }\StringTok{"Prediction\_factor"}\NormalTok{:y\_factor\_pred\})}
\NormalTok{df.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& y & Prediction & Prediction\_factor \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 16.684749 & 4.776841 & 4.776777 \\
1 & 85.865258 & 85.281473 & 85.281330 \\
2 & 49.811774 & 51.393775 & 51.393551 \\
3 & 18.177150 & 9.367810 & 9.368369 \\
4 & -9.031623 & -28.288469 & -28.289330 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.tail()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& y & Prediction & Prediction\_factor \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
9995 & 83.620503 & 85.123565 & 85.123699 \\
9996 & 76.187178 & 79.210518 & 79.210455 \\
9997 & 29.494401 & 37.567762 & 37.567337 \\
9998 & 15.390268 & 50.205998 & 50.203208 \\
9999 & 26.261264 & 30.671347 & 30.670934 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(y\_pred }\OperatorTok{{-}}\NormalTok{ y\_true))}
\NormalTok{sf}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(y\_factor\_pred }\OperatorTok{{-}}\NormalTok{ y\_true))}
\NormalTok{res }\OperatorTok{=}\NormalTok{ (sf }\OperatorTok{{-}}\NormalTok{ s)}
\BuiltInTok{print}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
-2.678237715663272
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.validation }\ImportTok{import}\NormalTok{ plot\_actual\_vs\_predicted}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df[}\StringTok{"y"}\NormalTok{], y\_pred}\OperatorTok{=}\NormalTok{df[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"Default"}\NormalTok{)}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df[}\StringTok{"y"}\NormalTok{], y\_pred}\OperatorTok{=}\NormalTok{df[}\StringTok{"Prediction\_factor"}\NormalTok{], title}\OperatorTok{=}\StringTok{"Factor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{016_num_spot_factorial_files/figure-pdf/cell-10-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{016_num_spot_factorial_files/figure-pdf/cell-10-output-2.pdf}}

\section{Jupyter Notebook}\label{jupyter-notebook-12}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/016_num_spot_factorial.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{User-Specified Functions: Extending the Analytical
Class}\label{sec-user-function}

This chapter illustrates how user-specified functions can be optimized
and analyzed. It covers singe-objective function in
Section~\ref{sec-single-objective} and multi-objective functions in
Section~\ref{sec-multi-objective}, and how to use the
\texttt{spotpython} package to optimize them. It shows a simple approach
to define a user-specified function, both for single- and
multi-objective optimization, and how to extend the \texttt{Analytical}
class to create a custom function.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Citation}]

\begin{itemize}
\tightlist
\item
  If this document has been useful to you and you wish to cite it in a
  scientific publication, please refer to the following paper, which can
  be found on arXiv: \url{https://arxiv.org/abs/2307.10262}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{@ARTICLE\{bart23iArXiv,}
\NormalTok{      author = \{\{Bartz{-}Beielstein\}, Thomas\},}
\NormalTok{      title = "\{Hyperparameter Tuning Cookbook:}
\NormalTok{          A guide for scikit{-}learn, PyTorch, river, and spotpython\}",}
\NormalTok{     journal = \{arXiv e{-}prints\},}
\NormalTok{    keywords = \{Computer Science {-} Machine Learning,}
\NormalTok{      Computer Science {-} Artificial Intelligence, 90C26, I.2.6, G.1.6\},}
\NormalTok{         year = 2023,}
\NormalTok{        month = jul,}
\NormalTok{          eid = \{arXiv:2307.10262\},}
\NormalTok{          doi = \{10.48550/arXiv.2307.10262\},}
\NormalTok{archivePrefix = \{arXiv\},}
\NormalTok{       eprint = \{2307.10262\},}
\NormalTok{ primaryClass = \{cs.LG\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Software Requirements}\label{software-requirements}

\begin{itemize}
\tightlist
\item
  The code examples in this chapter require the \texttt{spotpython}
  package, which can be installed via \texttt{pip}.
\item
  Furthermore, the following Python packages are required:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, surrogate\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\end{Highlighting}
\end{Shaded}

\section{The Single-Objective Function: User
Specified}\label{sec-single-objective}

We will use an analytical objective function, i.e., a function that can
be described by a (closed) formula: \[
f(x) = \sum_i^k x_i^4.
\]

This function is continuous, convex and unimodal. Since it returns one
value for each input vector, it is a single-objective function.
Multiple-objective functions can also be handled by \texttt{spotpython}.
They are covered in Section~\ref{sec-multi-objective}.

The global minimum of the single-objective function is \[
f(x) = 0, \text{at } x = (0,0, \ldots, 0).
\]

It can be implemented in Python as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ user\_fun(X):}
    \ControlFlowTok{return}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{((X) }\OperatorTok{**}\DecValTok{4}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

For example, if we have \(X = (1, 2, 3)\), then \[
f(x) = 1^4 + 2^4 + 3^4 = 1 + 16 + 81 = 98,
\] and if we have \(X = (4, 5, 6)\), then \[
f(x) = 4^4 + 5^4 + 6^4 = 256 + 625 + 1296 = 2177.
\]

We can pass a 2D array to the function, and it will return a 1D array
with the results for each row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{user\_fun(np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{]]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([  98, 2177])
\end{verbatim}

To make \texttt{user\_fun} compatible with the \texttt{spotpython}
package, we need to extend its argument list, so that it can handle the
\texttt{fun\_control} dictionary.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ user\_fun(X, fun\_control}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \ControlFlowTok{return}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{((X) }\OperatorTok{**}\DecValTok{4}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Alternatively, you can add the \texttt{**kwargs} argument to the
function, which will allow you to pass any additional keyword arguments:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ user\_fun(X, }\OperatorTok{**}\NormalTok{kwargs):}
    \ControlFlowTok{return}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{((X) }\OperatorTok{**}\DecValTok{4}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              lower }\OperatorTok{=}\NormalTok{ np.array( [}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{)}
\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{user\_fun,}
\NormalTok{                 fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{S.run()}
\NormalTok{S.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.715394917589437e-05 [#######---] 73.33% 
spotpython tuning: 3.715394917589437e-05 [########--] 80.00% 
spotpython tuning: 3.715394917589437e-05 [#########-] 86.67% 
spotpython tuning: 3.715394917589437e-05 [#########-] 93.33% 
spotpython tuning: 3.715394917589437e-05 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{017_num_spot_user_function_files/figure-pdf/cell-7-output-2.pdf}}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Using \texttt{spotpython} with Single-Objective User-Specified
Functions}]

\begin{itemize}
\tightlist
\item
  \texttt{spotpython} accepts user-specified functions that can be
  defined in Python.
\item
  The function should accept a 2D array as input and return a 1D array
  as output.
\item
  The function can be defined with an additional argument
  \texttt{fun\_control} to handle control parameters.
\item
  The \texttt{fun\_control} dictionary can be initialized with the
  \texttt{fun\_control\_init} function, which allows you to specify the
  bounds of the input variables.
\end{itemize}

\end{tcolorbox}

\section{The Objective Function: Extending the Analytical
Class}\label{the-objective-function-extending-the-analytical-class}

\begin{itemize}
\tightlist
\item
  The \texttt{Analytical} class is a base class for analytical functions
  in the \texttt{spotpython} package.
\item
  It provides a framework for defining and evaluating analytical
  functions, including the ability to add noise to the output.
\item
  The \texttt{Analytical} class can be extended as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Optional, Dict}

\KeywordTok{class}\NormalTok{ UserAnalytical(Analytical):}
    \KeywordTok{def}\NormalTok{ fun\_user\_function(}\VariableTok{self}\NormalTok{, X: np.ndarray, fun\_control: Optional[Dict] }\OperatorTok{=} \VariableTok{None}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
        \CommentTok{"""}
\CommentTok{        Custom new function: f(x) = x\^{}4}
\CommentTok{        }
\CommentTok{        Args:}
\CommentTok{            X (np.ndarray): Input data as a 2D array.}
\CommentTok{            fun\_control (Optional[Dict]): Control parameters for the function.}
\CommentTok{        }
\CommentTok{        Returns:}
\CommentTok{            np.ndarray: Computed values with optional noise.}
\CommentTok{        }
\CommentTok{        Examples:}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} import numpy as np}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} X = np.array([[1, 2, 3], [4, 5, 6]])}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} fun = UserAnalytical()}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} fun.fun\_user\_function(X)}
\CommentTok{        """}
\NormalTok{        X }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_prepare\_input\_data(X, fun\_control)}
     
\NormalTok{        offset }\OperatorTok{=}\NormalTok{ np.ones(X.shape[}\DecValTok{1}\NormalTok{]) }\OperatorTok{*} \VariableTok{self}\NormalTok{.offset}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((X }\OperatorTok{{-}}\NormalTok{ offset) }\OperatorTok{**}\DecValTok{4}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) }

        \CommentTok{\# Add noise if specified in fun\_control}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.\_add\_noise(y)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  In comparison to the \texttt{user\_fun} function, the
  \texttt{UserAnalytical} class provides additional functionality, such
  as adding noise to the output and preparing the input data.
\item
  First, we use the \texttt{user\_fun} function as above.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{user\_fun }\OperatorTok{=}\NormalTok{ UserAnalytical()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\NormalTok{results }\OperatorTok{=}\NormalTok{ user\_fun.fun\_user\_function(X)}
\BuiltInTok{print}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0. 3.]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Then we can add an offset to the function, which will shift the
  function by a constant value. This is useful for testing the
  optimization algorithm's ability to find the global minimum.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{user\_fun }\OperatorTok{=}\NormalTok{ UserAnalytical(offset}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\NormalTok{results }\OperatorTok{=}\NormalTok{ user\_fun.fun\_user\_function(X)}
\BuiltInTok{print}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[3. 0.]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  And, we can add noise to the function, which will add a random value
  to the output. This is useful for testing the optimization algorithm's
  ability to find the global minimum in the presence of noise.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{user\_fun }\OperatorTok{=}\NormalTok{ UserAnalytical(sigma}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\NormalTok{results }\OperatorTok{=}\NormalTok{ user\_fun.fun\_user\_function(X)}
\BuiltInTok{print}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0.06691138 3.11495313]
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Here is an example of how to use the \texttt{UserAnalytical} class
  with the \texttt{spotpython} package:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{user\_fun }\OperatorTok{=}\NormalTok{ UserAnalytical().fun\_user\_function}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              PREFIX}\OperatorTok{=}\StringTok{"USER"}\NormalTok{,              }
\NormalTok{              lower }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{1.0}\OperatorTok{*}\NormalTok{np.ones(}\DecValTok{2}\NormalTok{),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.ones(}\DecValTok{2}\NormalTok{),}
\NormalTok{              var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{"User Pressure"}\NormalTok{, }\StringTok{"User Temp"}\NormalTok{],}
\NormalTok{              TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{              tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{spot\_user }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{user\_fun,}
\NormalTok{                  fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{spot\_user.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_12_52_0
Created spot_tensorboard_path: runs/spot_logs/USER_maans08_2025-05-06_10-12-52 for SummaryWriter()
spotpython tuning: 3.715394917589437e-05 [#######---] 73.33% 
spotpython tuning: 3.715394917589437e-05 [########--] 80.00% 
spotpython tuning: 3.715394917589437e-05 [#########-] 86.67% 
spotpython tuning: 3.715394917589437e-05 [#########-] 93.33% 
spotpython tuning: 3.715394917589437e-05 [##########] 100.00% Done...

Experiment saved to USER_res.pkl
\end{verbatim}

\phantomsection\label{spot-run}
\begin{verbatim}
<spotpython.spot.spot.Spot at 0x14e8be1e0>
\end{verbatim}

\section{Results}\label{results-8}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_user.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 3.715394917589437e-05
User Pressure: 0.05170658955305796
User Temp: 0.07401195908206382
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_user.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{017_num_spot_user_function_files/figure-pdf/cell-14-output-1.pdf}}

\section{A Contour Plot}\label{a-contour-plot-3}

We can select two dimensions, say \(i=0\) and \(j=1\), and generate a
contour plot as follows.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note:}]

We have specified identical \texttt{min\_z} and \texttt{max\_z} values
to generate comparable plots.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_user.plot\_contour(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, min\_z}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_z}\OperatorTok{=}\FloatTok{2.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{017_num_spot_user_function_files/figure-pdf/cell-15-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  The variable importance:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ spot\_user.print\_importance()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
User Pressure:  62.089314080820216
User Temp:  100.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_user.plot\_importance()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{017_num_spot_user_function_files/figure-pdf/cell-17-output-1.pdf}}

\section{Multi-Objective Functions}\label{sec-multi-objective}

\begin{itemize}
\tightlist
\item
  The \texttt{spotpython} package can also handle multi-objective
  functions, which are functions that return multiple values for each
  input vector.
\item
  As noted in Section~\ref{sec-single-objective}, in the
  single-objective case, the function returns one value for each input
  vector and \texttt{spotpython} expects a 1D array as output.
\item
  If the function returns a 2D array as output, \texttt{spotpython} will
  treat it as a multi-objective function result.
\end{itemize}

\subsection{Response Surface
Experiment}\label{response-surface-experiment}

Myers, Montgomery, and Anderson-Cook (2016) describe a response surface
experiment where three input variables (\texttt{reaction\ time},
\texttt{reaction\ temperature}, and \texttt{percent\ catalyst}) were
used to model two characteristics of a chemical reaction:
\texttt{percent\ conversion} and \texttt{thermal\ activity}. Their model
is based on the following equations:

\begin{align*}
f_{\text{con}}(x) =
&
 81.09
+
1.0284 \cdot x_1
+
4.043 \cdot x_2
+
6.2037 \cdot x_3
+
1.8366 \cdot x_1^2
+
2.9382 \cdot x_2^2 \\
&
+
5.1915 \cdot x_3^2
+
2.2150 \cdot x_1 \cdot x_2
+
11.375 \cdot x_1 \cdot x_3
+
3.875 \cdot x_2 \cdot x_3
\end{align*} and \begin{align*}
f_{\text{act}}(x) = 
 & 
 59.85
+ 3.583 \cdot x_1
+ 0.2546 \cdot x_2
+ 2.2298 \cdot x_3
+ 0.83479 \cdot x_1^2
+ 0.07484 \cdot x_2^2
\\
&
+ 0.05716 \cdot x_3^2
+ 0.3875 \cdot x_1 \cdot x_2
+ 0.375 \cdot x_1 \cdot x_3
+ 0.3125 \cdot x_2 \cdot x_3. 
\end{align*}

\subsubsection{\texorpdfstring{Defining the Multi-Objective Function
\texttt{myer16a}}{Defining the Multi-Objective Function myer16a}}\label{defining-the-multi-objective-function-myer16a}

\begin{itemize}
\tightlist
\item
  The multi-objective function \texttt{myer16a} combines the results of
  two single-objective functions: conversion and activity.
\item
  It is implemented in \texttt{spotpython} as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ conversion\_pred(X):}
    \CommentTok{"""}
\CommentTok{    Compute conversion predictions for each row in the input array.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray): 2D array where each row is a configuration.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray: 1D array of conversion predictions.}
\CommentTok{    """}
    \ControlFlowTok{return}\NormalTok{ (}
        \FloatTok{81.09}
        \OperatorTok{+} \FloatTok{1.0284} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}
        \OperatorTok{+} \FloatTok{4.043} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
        \OperatorTok{+} \FloatTok{6.2037} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}
        \OperatorTok{{-}} \FloatTok{1.8366} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}
        \OperatorTok{+} \FloatTok{2.9382} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}\OperatorTok{**}\DecValTok{2}
        \OperatorTok{{-}} \FloatTok{5.1915} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}\OperatorTok{**}\DecValTok{2}
        \OperatorTok{+} \FloatTok{2.2150} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
        \OperatorTok{+} \FloatTok{11.375} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}
        \OperatorTok{{-}} \FloatTok{3.875} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{] }\OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}
\NormalTok{    )}

\KeywordTok{def}\NormalTok{ activity\_pred(X):}
    \CommentTok{"""}
\CommentTok{    Compute activity predictions for each row in the input array.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray): 2D array where each row is a configuration.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray: 1D array of activity predictions.}
\CommentTok{    """}
    \ControlFlowTok{return}\NormalTok{ (}
        \FloatTok{59.85}
        \OperatorTok{+} \FloatTok{3.583} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}
        \OperatorTok{+} \FloatTok{0.2546} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
        \OperatorTok{+} \FloatTok{2.2298} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}
        \OperatorTok{+} \FloatTok{0.83479} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}
        \OperatorTok{+} \FloatTok{0.07484} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}\OperatorTok{**}\DecValTok{2}
        \OperatorTok{+} \FloatTok{0.05716} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}\OperatorTok{**}\DecValTok{2}
        \OperatorTok{{-}} \FloatTok{0.3875} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
        \OperatorTok{{-}} \FloatTok{0.375} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}
        \OperatorTok{+} \FloatTok{0.3125} \OperatorTok{*}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{] }\OperatorTok{*}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}
\NormalTok{    )}

\KeywordTok{def}\NormalTok{ fun\_myer16a(X, fun\_control}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Compute both conversion and activity predictions for each row in the input array.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray): 2D array where each row is a configuration.}
\CommentTok{        fun\_control (dict, optional): Additional control parameters (not used here).}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray: 2D array where each row contains [conversion\_pred, activity\_pred].}
\CommentTok{    """}
    \ControlFlowTok{return}\NormalTok{ np.column\_stack((conversion\_pred(X), activity\_pred(X)))}
\end{Highlighting}
\end{Shaded}

Now the function returns a 2D array with two columns, one for each
objective function. The first column corresponds to the conversion
prediction, and the second column corresponds to the activity
prediction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{]])}
\NormalTok{results }\OperatorTok{=}\NormalTok{ fun\_myer16a(X)}
\BuiltInTok{print}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 87.3132   72.25519]
 [200.8662   98.7442 ]]
\end{verbatim}

\subsubsection{Using a Weighted Sum}\label{using-a-weighted-sum}

\begin{itemize}
\tightlist
\item
  The \texttt{spotpython} package can also handle multi-objective
  functions, which are functions that return multiple values for each
  input vector.
\item
  In this case, we can use a weighted sum to combine the two objectives
  into a single objective function.
\item
  The function \texttt{aggergate} takes the two objectives and combines
  them into a single objective function by applying weights to each
  objective.
\item
  The weights can be adjusted to give more importance to one objective
  over the other.
\item
  For example, if we want to give more importance to the conversion
  prediction, we can set the weight for the conversion prediction to 2
  and the weight for the activity prediction to 0.1.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Weight first objective with 2, second with 1/10}
\KeywordTok{def}\NormalTok{ aggregate(y):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(y}\OperatorTok{*}\NormalTok{np.array([}\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{]), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{aggregate} function object is passed to the
\texttt{fun\_control} dictionary aas the \texttt{fun\_mo2so} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              lower }\OperatorTok{=}\NormalTok{ np.array( [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]),}
\NormalTok{              fun\_mo2so}\OperatorTok{=}\NormalTok{aggregate)}
\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun\_myer16a,}
\NormalTok{        fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{S.run()}
\NormalTok{S.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 171.91645662554126 [#######---] 73.33% 
spotpython tuning: 168.29206829613202 [########--] 80.00% 
spotpython tuning: 168.29206829613202 [#########-] 86.67% 
spotpython tuning: 168.25685825618206 [#########-] 93.33% 
spotpython tuning: 168.16500000000002 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{017_num_spot_user_function_files/figure-pdf/cell-21-output-2.pdf}}

If no \texttt{fun\_mo2so} function is specified, the \texttt{spotpython}
package will use the first return value of the multi-objective function
as the single objective function.

\texttt{spotpython} allows access to the complete history of
multi-objective return values. They are stored in the \texttt{y\_mo}
attribute of the \texttt{Spot} object. The \texttt{y\_mo} attribute is a
2D array where each row corresponds to a configuration and each column
corresponds to an objective function. These values can be visualized as
shown in Figure~\ref{fig-017-spot-single-multi-objective}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_mo }\OperatorTok{=}\NormalTok{ S.y\_mo}
\NormalTok{y }\OperatorTok{=}\NormalTok{ S.y}
\NormalTok{plt.xlim(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(y\_mo))}
\NormalTok{plt.ylim(}\FloatTok{0.9} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(y\_mo), }\FloatTok{1.1}\OperatorTok{*}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(y))}
\NormalTok{plt.scatter(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(y\_mo)), y\_mo[:, }\DecValTok{0}\NormalTok{], label}\OperatorTok{=}\StringTok{\textquotesingle{}Conversion\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.scatter(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(y\_mo)), y\_mo[:, }\DecValTok{1}\NormalTok{], label}\OperatorTok{=}\StringTok{\textquotesingle{}Activity\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(np.minimum.accumulate(y\_mo[:, }\DecValTok{0}\NormalTok{]), label}\OperatorTok{=}\StringTok{\textquotesingle{}Cum. Min Conversion\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(np.minimum.accumulate(y\_mo[:, }\DecValTok{1}\NormalTok{]), label}\OperatorTok{=}\StringTok{\textquotesingle{}Cum. Min Activity\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.scatter(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(y)), y, label}\OperatorTok{=}\StringTok{\textquotesingle{}Agg. Result\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}D\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(np.minimum.accumulate(y), label}\OperatorTok{=}\StringTok{\textquotesingle{}Cum. Min Agg. Res.\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Iteration\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Objective Function Value\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Single{-} and Multi{-}Obj. Function Values\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper left\textquotesingle{}}\NormalTok{, bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{017_num_spot_user_function_files/figure-pdf/fig-017-spot-single-multi-objective-output-1.pdf}}

}

\caption{\label{fig-017-spot-single-multi-objective}Single- and
Multi-Objective Function Values. The red line shows the optimization
progress based on the aggregated objective function. The blue lines show
the progress of the conversion objective, the orange line the progress
of the activity objective. Points denote individual evaluations, lines
the cumulative minimum of the respective objective function.}

\end{figure}%

Since all values from the multi-objective functions can be accessed,
more sophisticated multi-objective optimization methods can be
implemented. For example, the \texttt{spotpython} package provides a
\texttt{pareto\_front} function that can be used to compute the Pareto
front of the multi-objective function values, see
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/mo/pareto/}{pareto}.
The Pareto front is a set of solutions that are not dominated by any
other solution in the objective space.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Using \texttt{spotpython} with Multi-Objective User-Specified
Functions}]

\begin{itemize}
\tightlist
\item
  \texttt{spotpython} accepts user-specified multi-objective functions
  that can be defined in Python.
\item
  The function should accept a 2D array as input and return a 2D array
  as output.
\item
  An \texttt{aggregate} function can be used to combine multiple
  objectives into a single objective function.
\end{itemize}

\end{tcolorbox}

\section{Jupyter Notebook}\label{jupyter-notebook-13}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/017_num_spot_user_function.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\part{Data-Driven Modeling and Optimization}

\chapter{Basic Statistics and Data
Analysis}\label{basic-statistics-and-data-analysis}

This chapter covers basic statistical concepts, namely descriptive
statistics, probability distributions, and hypothesis testing. These
concepts are fundamental to understanding data and making informed
decisions based on data analysis. The chapter also introduces the
concept of exploratory data analysis (EDA), data preprocessing
(Principal Component Analysis), and data visualization techniques.

\section{Exploratory Data Analysis}\label{exploratory-data-analysis}

\subsection{Histograms}\label{histograms}

Creating a histogram and calculating the probabilities from a dataset
can be approached with scientific precision

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Data Collection: Obtain the dataset you wish to analyze. This dataset
  could represent any quantitative measure, such to examine its
  distribution.
\item
  Decide on the Number of Bins: The number of bins influences the
  histogram's granularity. There are several statistical rules to
  determine an optimal number of bins:

  \begin{itemize}
  \tightlist
  \item
    Square-root rule: suggests using the square root of the number of
    data points as the number of bins.
  \item
    Sturges' formula: \(k = 1 + 3.322 \log_{10}(n)\), where \(n\) is the
    number of data points and \(k\) is the suggested number of bins.
  \item
    Freedman-Diaconis rule: uses the interquartile range (IQR) and the
    cube root of the number of data points \(n\) to calculate bin width
    as \(2 \dfrac{IQR}{n^{1/3}}\).
  \end{itemize}
\item
  Determine Range and Bin Width: Calculate the range of data by
  subtracting the minimum data point value from the maximum. Divide this
  range by the number of bins to determine the width of each bin.
\item
  Allocate Data Points to Bins: Iterate through the data, sorting each
  data point into the appropriate bin based on its value.
\item
  Draw the Histogram: Use a histogram to visualize the frequency or
  relative frequency (probability) of data points within each bin.
\item
  Calculate Probabilities: The relative frequency of data within each
  bin represents the probability of a randomly selected data point
  falling within that bin's range.
\end{enumerate}

Below is a Python script that demonstrates how to generate a histogram
and compute probabilities using the \texttt{matplotlib} library for
visualization and \texttt{numpy} for data manipulation.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Sample data: Randomly generated for demonstration}
\NormalTok{data }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{)  }\CommentTok{\# 1000 data points with a normal distribution}

\CommentTok{\# Step 2: Decide on the number of bins}
\NormalTok{num\_bins }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.ceil(}\DecValTok{1} \OperatorTok{+} \FloatTok{3.322} \OperatorTok{*}\NormalTok{ np.log10(}\BuiltInTok{len}\NormalTok{(data))))  }\CommentTok{\# Sturges\textquotesingle{} formula}

\CommentTok{\# Step 3: Determine range and bin width {-}{-} handled internally by matplotlib}

\CommentTok{\# Steps 4 \& 5: Sort data into bins and draw the histogram}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{n, bins, patches }\OperatorTok{=}\NormalTok{ ax.hist(data, bins}\OperatorTok{=}\NormalTok{num\_bins, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.75}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Calculate probabilities (relative frequencies) manually, if needed}
\NormalTok{bin\_width }\OperatorTok{=}\NormalTok{ np.diff(bins)  }\CommentTok{\# np.diff finds the difference between adjacent bin boundaries}
\NormalTok{probabilities }\OperatorTok{=}\NormalTok{ n }\OperatorTok{*}\NormalTok{ bin\_width  }\CommentTok{\# n is already normalized to form a probability density if \textasciigrave{}density=True\textasciigrave{}}

\CommentTok{\# Adding labels and title for clarity}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}Data Value\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}Probability Density\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Histogram with Probability Density\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\centering{

\begin{verbatim}
Text(0.5, 1.0, 'Histogram with Probability Density')
\end{verbatim}

}

\subcaption{\label{fig-histogram-1}Histogram with Probability Density}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-histogram-output-2.pdf}}

}

\subcaption{\label{fig-histogram-2}}

}

\caption{\label{fig-histogram}}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i, prob }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(probabilities):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Bin }\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{ Probability: }\SpecialCharTok{\{}\NormalTok{prob}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Ensure probabilities sum to 1 (or very close, due to floating{-}point arithmetic)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Sum of probabilities: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\BuiltInTok{sum}\NormalTok{(probabilities)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Bin 1 Probability: 0.0010
Bin 2 Probability: 0.0010
Bin 3 Probability: 0.0110
Bin 4 Probability: 0.0470
Bin 5 Probability: 0.1290
Bin 6 Probability: 0.1970
Bin 7 Probability: 0.2370
Bin 8 Probability: 0.1950
Bin 9 Probability: 0.1290
Bin 10 Probability: 0.0430
Bin 11 Probability: 0.0100
Sum of probabilities: 1.0
\end{verbatim}

This code segment goes through the necessary steps to generate a
histogram and calculate probabilities for a synthetic dataset. It
demonstrates important scientific and computational practices including
binning, visualization, and probability calculation in Python.

Key Points:

\begin{itemize}
\item
  The histogram represents the distribution of data, with the
  histogram's bins outlining the data's spread and density.
\item
  The option \texttt{density=True} in \texttt{ax.hist()} normalizes the
  histogram so that the total area under the histogram sums to 1,
  thereby converting frequencies to probability densities.
\item
  The choice of bin number and width has a significant influence on the
  histogram's shape and the insights that can be drawn from it,
  highlighting the importance of selecting appropriate binning
  strategies based on the dataset's characteristics and the analysis
  objectives.
\item
  Video: \href{https://youtu.be/qBigTkBLU6g}{Histograms, Clearly
  Explained}
\end{itemize}

\subsection{Boxplots}\label{boxplots}

\begin{itemize}
\tightlist
\item
  Video:
  \href{https://youtu.be/fHLhBnmwUM0?si=QB5ccKIxL1FaIc0M}{Boxplots are
  Awesome}
\end{itemize}

\section{Probability Distributions}\label{probability-distributions}

What happens when we use smaller bins in a histogram? The histogram
becomes more detailed, revealing the distribution of data points with
greater precision. However, as the bin size decreases, the number of
data points within each bin may decrease, leading to sparse or empty
bins. This sparsity can make it challenging to estimate probabilities
accurately, especially for data points that fall within these empty
bins.

Advantages, when using a probability distribution, include:

\begin{itemize}
\tightlist
\item
  Blanks can be filled
\item
  Probabilities can be calculated
\item
  Parameters are sufficient to describe the distribution, e.g., mean and
  variance for the normal distribution
\end{itemize}

Probability distributions offer a powerful solution to the challenges
posed by limited data in estimating probabilities. When data is scarce,
constructing a histogram to determine the probability of certain
outcomes can lead to inaccurate or unreliable results due to the lack of
detail in the dataset. However, collecting vast amounts of data to
populate a histogram for more precise estimates can often be
impractical, time-consuming, and expensive.

A probability distribution is a mathematical function that provides the
probabilities of occurrence of different possible outcomes for an
experiment. It is a more efficient approach to understanding the
likelihood of various outcomes than relying solely on extensive data
collection. For continuous data, this is often represented graphically
by a smooth curve.

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/oI3hZJqXJuc}{The Main Ideas behind
  Probability Distributions}
\end{itemize}

\subsection{Sampling from a
Distribution}\label{sampling-from-a-distribution}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/XLCWeSVzHUU}{Sampling from a
  Distribution, Clearly Explained!!!}
\end{itemize}

\section{Discrete Distributions}\label{discrete-distributions}

Discrete probability distributions are essential tools in statistics,
providing a mathematical foundation to model and analyze situations with
discrete outcomes. Histograms, which can be seen as discrete
distributions with data organized into bins, offer a way to visualize
and estimate probabilities based on the collected data. However, they
come with limitations, especially when data is scarce or when we
encounter gaps in the data (blank spaces in histograms). These gaps can
make it challenging to accurately estimate probabilities.

A more efficient approach, especially for discrete data, is to use
mathematical equations---particularly those defining discrete
probability distributions---to calculate probabilities directly, thus
bypassing the intricacies of data collection and histogram
interpretation.

\subsection{Bernoulli Distribution}\label{bernoulli-distribution}

The Bernoulli distribution, named after Swiss scientist Jacob Bernoulli,
is a discrete probability distribution, which takes value \(1\) with
success probability \(p\) and value \(0\) with failure probability
\(q = 1-p\). So if \(X\) is a random variable with this distribution, we
have: \[
P(X=1) = 1-P(X=0) = p = 1-q.
\]

\subsection{Binomial Distribution}\label{binomial-distribution}

The Binomial Distribution is a prime example of a discrete probability
distribution that is particularly useful for binary outcomes (e.g.,
success/failure, yes/no, pumpkin pie/blueberry pie). It leverages simple
mathematical principles to calculate the probability of observing a
specific number of successes (preferred outcomes) in a fixed number of
trials, given the probability of success in each trial.

\begin{example}[Pie
Preference]\protect\hypertarget{exm-binom}{}\label{exm-binom}

Consider a scenario from ``StatLand'' where 70\% of people prefer
pumpkin pie over blueberry pie. The question is: What is the probability
that, out of three people asked, the first two prefer pumpkin pie and
the third prefers blueberry pie?

Using the concept of the Binomial Distribution, the probability of such
an outcome can be calculated without the need to layout every possible
combination by hand. This process not only simplifies calculations but
also provides a clear and precise method to determine probabilities in
scenarios involving discrete choices. We will use Python to calculate
the probability of observing exactly two out of three people prefer
pumpkin pie, given the 70\% preference rate:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ binom}
\NormalTok{n }\OperatorTok{=} \DecValTok{3}  \CommentTok{\# Number of trials (people asked)}
\NormalTok{p }\OperatorTok{=} \FloatTok{0.7}  \CommentTok{\# Probability of success (preferring pumpkin pie)}
\NormalTok{x }\OperatorTok{=} \DecValTok{2}  \CommentTok{\# Number of successes (people preferring pumpkin pie)}
\CommentTok{\# Probability calculation using Binomial Distribution}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ binom.pmf(x, n, p)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The probability that exactly 2 out of 3 people prefer pumpkin pie is: }\SpecialCharTok{\{}\NormalTok{prob}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The probability that exactly 2 out of 3 people prefer pumpkin pie is: 0.441
\end{verbatim}

This code uses the \texttt{binom.pmf()} function from
\texttt{scipy.stats} to calculate the probability mass function (PMF) of
observing exactly \texttt{x} successes in \texttt{n} trials, where each
trial has a success probability of \texttt{p}.

\end{example}

A Binomial random variable is the sum of \(n\) independent, identically
distributed Bernoulli random variables, each with probability \(p\) of
success. We may indicate a random variable \(X\) with Bernoulli
distribution using the notation \(X \sim \mathrm{Bi}(1,\theta)\). Then,
the notation for the Binomial is \(X \sim \mathrm{Bi}(n,\theta)\). Its
probability and distribution functions are, respectively, \[
p_X(x) = {n\choose x}\theta^x(1-\theta)^{n-x}, \qquad F_X(x) = \Pr\{X \le x\} = \sum_{i=0}^{x} {n\choose i}\theta^i(1-\theta)^{n-i}.
\]

The mean of the binomial distribution is \(\text{E}[X] = n\theta\). The
variance of the distribution is \(\text{Var}[X] = n\theta(1-\theta)\)
(see next section).

A process consists of a sequence of \(n\) independent trials, i.e., the
outcome of each trial does not depend on the outcome of previous trials.
The outcome of each trial is either a success or a failure. The
probability of success is denoted as \(p\), and \(p\) is constant for
each trial. Coin tossing is a classical example for this setting.

The binomial distribution is a statistical distribution giving the
probability of obtaining a specified number of successes in a binomial
experiment; written Binomial(n, p), where \(n\) is the number of trials,
and \(p\) the probability of success in each.

\begin{definition}[Binomial
Distribution]\protect\hypertarget{def-binom}{}\label{def-binom}

The binomial distribution with parameters \(n\) and \(p\), where \(n\)
is the number of trials, and \(p\) the probability of success in each,
is \begin{equation}
p(x) = { n \choose k } p^x(1-p)^{n-x} \qquad x = 0,1, \ldots, n.
\end{equation} The mean \(\mu\) and the variance \(\sigma^2\) of the
binomial distribution are \begin{equation}
\mu = np
\end{equation} and \begin{equation}
\sigma^2 = np(1-p).
\end{equation}

\end{definition}

Note, the Bernoulli distribution is simply Binomial(1,p).

\section{Continuous Distributions}\label{continuous-distributions}

Our considerations regarding probability distributions, expectations,
and standard deviations will be extended from discrete distributions to
continuous distributions. One simple example of a continuous
distribution is the uniform distribution. Continuous distributions are
defined by probability density functions.

\subsection{Distribution functions: PDFs and
CDFs}\label{distribution-functions-pdfs-and-cdfs}

The density for a continuous distribution is a measure of the relative
probability of ``getting a value close to \(x\).'' Probability density
functions \(f\) and cumulative distribution function \(F\) are related
as follows. \begin{equation}
f(x) = \frac{d}{dx} F(x)
\end{equation}

\section{Background: Expectation, Mean, Standard
Deviation}\label{background-expectation-mean-standard-deviation}

The distribution of a random vector is characterized by some indexes.
These are the expectation, the mean, and the standard deviation. The
expectation is a measure of the central tendency of a random variable,
while the standard deviation quantifies the spread of the distribution.
These indexes are essential for understanding the behavior of random
variables and making predictions based on them.

\begin{definition}[Random
Variable]\protect\hypertarget{def-random-variable}{}\label{def-random-variable}

A random variable \(X\) is a mapping from the sample space of a random
experiment to the real numbers. It assigns a numerical value to each
outcome of the experiment. Random variables can be either:

\begin{itemize}
\tightlist
\item
  Discrete: If \(X\) takes on a countable number of distinct values.
\item
  Continuous: If \(X\) takes on an uncountable number of values.
\end{itemize}

Mathematically, a random variable is a function
\(X: \Omega \rightarrow \mathbb{R}\), where \(\Omega\) is the sample
space.

\end{definition}

\begin{definition}[Probability
Distribution]\protect\hypertarget{def-probability-distribution}{}\label{def-probability-distribution}

A probability distribution describes how the values of a random variable
are distributed. It is characterized for a discrete random variable
\(X\) by the probability mass function (PMF) \(p_X(x)\) and for a
continuous random variable \(X\) by the probability density function
(PDF) \(f_X(x)\).

\end{definition}

\begin{definition}[Probability Mass Function
(PMF)]\protect\hypertarget{def-probability-mass-function}{}\label{def-probability-mass-function}

\(p_X(x) = P(X = x)\) gives the probability that \(X\) takes the value
\(x\).

\end{definition}

\begin{definition}[Probability Density Function
(PDF):]\protect\hypertarget{def-probability-density-function}{}\label{def-probability-density-function}

\(f_X(x)\) is a function such that for any interval \([a, b]\), the
probability that \(X\) falls within this interval is given by the
integral \(\int_a^b f_X(x) \mathrm{d}x\).

\end{definition}

The distribution function must satisfy: \[
\sum_{x \in D_X} p_X(x) = 1
\] for discrete random variables, where \(D_X\) is the domain of \(X\)
and \[
\int_{-\infty}^{\infty} f_X(x) \mathrm{d}x = 1
\] for continuous random variables.

With these definitions in place, we can now introduce the definition of
the expectation, which is a fundamental measure of the central tendency
of a random variable.

\begin{definition}[Expectation]\protect\hypertarget{def-expectation}{}\label{def-expectation}

The expectation or expected value of a random variable \(X\), denoted
\(E[X]\), is defined as follows:

For a discrete random variable \(X\): \[
E[X] = \sum_{x \in D_X} x p_X(x) \quad \text{if $X$ is discrete}.
\]

For a continuous random variable \(X\): \[
E[X] = \int_{x \in D_X} x f_X(x) \mathrm{d}x \quad \text{if $X$ is continuous.}
\]

\end{definition}

The mean, \(\mu\), of a probability distribution is a measure of its
central tendency or location. That is, \(E(X)\) is defined as the
average of all possible values of \(X\), weighted by their
probabilities.

\begin{example}[Expectation]\protect\hypertarget{exm-expectation}{}\label{exm-expectation}

Let \(X\) denote the number produced by rolling a fair die. Then \[
E(X) = 1 \times 1/6 + 2 \times 1/6 + 3 \times 1/6 + 4 \times 1/6 + 5 \times 1/6 + 6\times 1/6 = 3.5.
\]

\end{example}

\begin{definition}[Sample
Mean]\protect\hypertarget{def-sample-mean}{}\label{def-sample-mean}

The sample mean is an important estimate of the population mean. The
sample mean of a sample \(\{x_i\}\) (\(i=1,2,\ldots,n\)) is defined as
\[
\overline{x}  = \frac{1}{n} \sum_i x_i.
\]

\end{definition}

While both the expectation of a random variable and the sample mean
provide measures of central tendency, they differ in their context,
calculation, and interpretation.

\begin{itemize}
\tightlist
\item
  The expectation is a theoretical measure that characterizes the
  average value of a random variable over an infinite number of
  repetitions of an experiment. The expectation is calculated using a
  probability distribution and provides a parameter of the entire
  population or distribution. It reflects the long-term average or
  central value of the outcomes generated by the random process.
\item
  The sample mean is a statistic. It provides an estimate of the
  population mean based on a finite sample of data. It is computed
  directly from the data sample, and its value can vary between
  different samples from the same population. It serves as an
  approximation or estimate of the population mean. It is used in
  statistical inference to make conclusions about the population mean
  based on sample data.
\end{itemize}

If we are trying to predict the value of a random variable \(X\) by its
mean \(\mu = E(X)\), the error will be \(X-\mu\). In many situations it
is useful to have an idea how large this deviation or error is. Since
\(E(X-\mu) = E(X) -\mu = 0\), it is necessary to use the absolute value
or the square of (\(X-\mu\)). The squared error is the first choice,
because the derivatives are easier to calculate. These considerations
motivate the definition of the variance:

\begin{definition}[Variance]\protect\hypertarget{def-variance}{}\label{def-variance}

The variance of a random variable \(X\) is the mean squared deviation of
\(X\) from its expected value \(\mu = E(X)\). \begin{equation}
Var(X) = E[ (X-\mu)^2].
\end{equation}

\end{definition}

The variance is a measure of the spread of a distribution. It quantifies
how much the values of a random variable differ from the mean. A high
variance indicates that the values are spread out over a wide range,
while a low variance indicates that the values are clustered closely
around the mean.

\begin{definition}[Standard
Deviation]\protect\hypertarget{def-standard-deviation}{}\label{def-standard-deviation}

Taking the square root of the variance to get back to the same scale of
units as \(X\) gives the standard deviation. The standard deviation of
\(X\) is the square root of the variance of \(X\). \begin{equation}
sd(X) = \sqrt{Var(X)}.
\end{equation}

\end{definition}

\section{Distributions and Random Numbers in
Python}\label{distributions-and-random-numbers-in-python}

Results from computers are deterministic, so it sounds like a
contradiction in terms to generate random numbers on a computer.
Standard computers generate pseudo-randomnumbers, i.e., numbers that
behave as if they were drawn randomly.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Deterministic Random Numbers}]

\begin{itemize}
\tightlist
\item
  Idea: Generate deterministically numbers that \textbf{look} (behave)
  as if they were drawn randomly.
\end{itemize}

\end{tcolorbox}

\subsection{Calculation of the Standard Deviation with
Python}\label{calculation-of-the-standard-deviation-with-python}

The function \texttt{numpy.std} returns the standard deviation, a
measure of the spread of a distribution, of the array elements. The
argument \texttt{ddof} specifies the Delta Degrees of Freedom. The
divisor used in calculations is \texttt{N\ -\ ddof}, where \texttt{N}
represents the number of elements. By default \texttt{ddof} is zero,
i.e., \texttt{std} uses the formula \[
\sqrt{  \frac{1}{N} \sum_i \left( x_i - \bar{x} \right)^2  } \qquad \text{with } \quad \bar{x} = \sum_{i=1}^N x_i /N.
\]

\begin{example}[Standard Deviation with
Python]\protect\hypertarget{exm-std-python}{}\label{exm-std-python}

Consider the array \([1,2,3]\): Since \(\bar{x} = 2\), the following
value is computed:
\[ \sqrt{1/3 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/3}.\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]])}
\NormalTok{np.std(a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(0.816496580927726)
\end{verbatim}

\end{example}

The empirical standard deviation (which uses \(N-1\)),
\(\sqrt{1/2 \times \left( (1-2)^2 + (2-2)^2 + (3-2)^2  \right)} = \sqrt{2/2}\),
can be calculated in Python as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(a, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(1.0)
\end{verbatim}

\subsection{The Argument ``axis''}\label{the-argument-axis}

When you compute \texttt{np.std} with \texttt{axis=0}, it calculates the
standard deviation along the vertical axis, meaning it computes the
standard deviation for each column of the array. On the other hand, when
you compute \texttt{np.std} with \texttt{axis=1}, it calculates the
standard deviation along the horizontal axis, meaning it computes the
standard deviation for each row of the array. If the axis parameter is
not specified, \texttt{np.std} computes the standard deviation of the
flattened array, i.e., it calculates the standard deviation of all the
elements in the array.

\begin{example}[Axes along which the standard deviation is
computed]\protect\hypertarget{exm-std-axis}{}\label{exm-std-axis}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]])}
\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1, 2],
       [3, 4]])
\end{verbatim}

First, we calculate the standard deviation of all elements in the array:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(1.118033988749895)
\end{verbatim}

Setting \texttt{axis=0} calculates the standard deviation along the
vertical axis (column-wise):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1., 1.])
\end{verbatim}

Finally, setting \texttt{axis=1} calculates the standard deviation along
the horizontal axis (row-wise):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.std(A, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.5, 0.5])
\end{verbatim}

\end{example}

\section{Expectation (Continuous)}\label{expectation-continuous}

\begin{definition}[Expectation
(Continuous)]\protect\hypertarget{def-expectation}{}\label{def-expectation}

\begin{equation}
  \text{E}(X) = \int_{-\infty}^\infty x f(x) \, dx
  \end{equation}

\end{definition}

\section{Variance and Standard Deviation
(Continuous)}\label{variance-and-standard-deviation-continuous}

\begin{definition}[Variance
(Continuous)]\protect\hypertarget{def-variance}{}\label{def-variance}

Variance can be calculated with \(\text{E}(X)\) and \begin{equation}
  \text{E}(X^2) = \int_{-\infty}^\infty x^2 f(x) \, dx
\end{equation} as \begin{equation*}
  \text{Var}(X) = \text{E}(X^2) - [ E(X)]^2.
  \end{equation*} \hfill \(\Box\)

\end{definition}

\begin{definition}[Standard Deviation
(Continuous)]\protect\hypertarget{def-standard-deviation}{}\label{def-standard-deviation}

Standard deviation can be calculated as \begin{equation*}
  \text{sd}(X) = \sqrt{\text{Var}(X)}.
  \end{equation*} \hfill \(\Box\)

\end{definition}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/vikkiwjQqfU}{Population and Estimated
  Parameters, Clearly Explained}
\item
  Video: \href{https://youtu.be/SzZ6GpcfoQY}{Calculating the Mean,
  Variance, and Standard Deviation}
\end{itemize}

\subsection{The Uniform Distribution}\label{the-uniform-distribution}

\begin{definition}[The Uniform
Distribution]\protect\hypertarget{def-uniform-distribution}{}\label{def-uniform-distribution}

The probability density function of the uniform distribution is defined
as: \[
f_X(x) = \frac{1}{b-a} \qquad \text{for $x \in [a,b]$}.
\]

\end{definition}

Generate 10 random numbers from a uniform distribution between \(a=0\)
and \(b=1\):

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\CommentTok{\# Initialize the random number generator}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{123456789}\NormalTok{)}
\NormalTok{n }\OperatorTok{=} \DecValTok{10}
\NormalTok{x }\OperatorTok{=}\NormalTok{ rng.uniform(low}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, high}\OperatorTok{=}\FloatTok{1.0}\NormalTok{, size}\OperatorTok{=}\NormalTok{n)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.02771274, 0.90670006, 0.88139355, 0.62489728, 0.79071481,
       0.82590801, 0.84170584, 0.47172795, 0.95722878, 0.94659153])
\end{verbatim}

Generate 10,000 random numbers from a uniform distribution between 0 and
10 and plot a histogram of the numbers:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Initialize the random number generator}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed}\OperatorTok{=}\DecValTok{123456789}\NormalTok{)}

\CommentTok{\# Generate random numbers from a uniform distribution}
\NormalTok{x }\OperatorTok{=}\NormalTok{ rng.uniform(low}\OperatorTok{=}\DecValTok{0}\NormalTok{, high}\OperatorTok{=}\DecValTok{10}\NormalTok{, size}\OperatorTok{=}\DecValTok{10000}\NormalTok{)}

\CommentTok{\# Plot a histogram of the numbers}
\NormalTok{plt.hist(x, bins}\OperatorTok{=}\DecValTok{50}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Uniform Distribution [0,10]\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Value\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/cell-12-output-1.pdf}}

\section{The Uniform Distribution}\label{the-uniform-distribution-2}

This variable is defined in the interval \([a,b]\). We write it as
\(X \sim U[a,b]\). Its density and cumulative distribution functions
are, respectively, \[
f_X(x) = \frac{I_{[a,b]}(x)}{b-a},  \quad\quad F_X(x) = \frac{1}{b-a}\int\limits_{-\infty}\limits^x I_{[a,b]}(t) \mathrm{d}t = \frac{x-a}{b-a},
\] where \(I_{[a,b]}(\cdot)\) is the indicator function of the interval
\([a,b]\). Note that, if we set \(a=0\) and \(b=1\), we obtain
\(F_X(x) = x\), \(x\) \(\in\) \([0,1]\).

A typical example is the following: the cdf of a continuous r.v. is
uniformly distributed in \([0,1]\). The proof of this statement is as
follows: For \(u\) \(\in\) \([0,1]\), we have \begin{eqnarray*}
\Pr\{F_X(X) \leq u\} &=& \Pr\{F_X^{-1}(F_X(X)) \leq F_X^{-1}(u)\} = \Pr\{X \leq F_X^{-1}(u)\} \\
                      &=& F_X(F_X^{-1}(u)) = u.     
\end{eqnarray*} This means that, when \(X\) is continuous, there is a
one-to-one relationship (given by the cdf) between \(x\) \(\in\) \(D_X\)
and \(u\) \(\in\) \([0,1]\).

The \emph{uniform distribution} has a constant density over a specified
interval, say \([a,b]\). The uniform \(U(a,b)\) distribution has density
\begin{equation}
 f(x) = 
 \left\{
  \begin{array}{ll}
  1/(b-a) & \textrm{ if } a < x < b,\\
  0 & \textrm{ otherwise}
  \end{array}
  \right.
  \end{equation}

\subsection{The Normal Distribution}\label{the-normal-distribution}

A normally distributed random variable is a random variable whose
associated probability distribution is the normal (or Gaussian)
distribution. The normal distribution is a continuous probability
distribution characterized by a symmetric bell-shaped curve.

The distribution is defined by two parameters: the mean \(\mu\) and the
standard deviation \(\sigma\). The mean indicates the center of the
distribution, while the standard deviation measures the spread or
dispersion of the distribution.

This distribution is widely used in statistics and the natural and
social sciences as a simple model for random variables with unknown
distributions.

\begin{definition}[The Normal
Distribution]\protect\hypertarget{def-normal-distribution}{}\label{def-normal-distribution}

The probability density function of the normal distribution is defined
as: \begin{equation}\phantomsection\label{eq-normal-one}{
f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2\right),
}\end{equation} where: \(\mu\) is the mean; \(\sigma\) is the standard
deviation.

\end{definition}

To generate ten random numbers from a normal distribution, the following
command can be used.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}
\NormalTok{n }\OperatorTok{=} \DecValTok{10}
\NormalTok{mu, sigma }\OperatorTok{=} \DecValTok{2}\NormalTok{, }\FloatTok{0.1}
\NormalTok{x }\OperatorTok{=}\NormalTok{ rng.normal(mu, sigma, n)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{gen-normal-10}
\begin{verbatim}
array([2.04449435, 1.88301603, 1.88234727, 1.76008354, 1.8511215 ,
       1.88517921, 2.03698968, 2.20128047, 2.1971614 , 2.09428731])
\end{verbatim}

Verify the mean:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(mu }\OperatorTok{{-}}\NormalTok{ np.mean(x))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(0.016403925204067304)
\end{verbatim}

Note: To verify the standard deviation, we use \texttt{ddof\ =\ 1}
(empirical standard deviation):

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{abs}\NormalTok{(sigma }\OperatorTok{{-}}\NormalTok{ np.std(x, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(0.0525230690183848)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_normal\_distribution(mu}\OperatorTok{=}\DecValTok{0}\NormalTok{, sigma}\OperatorTok{=}\DecValTok{1}\NormalTok{, num\_samples}\OperatorTok{=}\DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/cell-17-output-1.pdf}}

\subsection{Visualization of the Standard
Deviation}\label{visualization-of-the-standard-deviation}

The standard deviation of normal distributed can be visualized in terms
of the histogram of \(X\):

\begin{itemize}
\tightlist
\item
  about 68\% of the values will lie in the interval within one standard
  deviation of the mean
\item
  95\% lie within two standard deviation of the mean
\item
  and 99.9\% lie within 3 standard deviations of the mean.
\end{itemize}

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/cell-18-output-1.pdf}}

\subsection{Realizations of a Normal
Distribution}\label{realizations-of-a-normal-distribution}

Realizations of a normal distribution refers to the actual values that
you get when you draw samples from a normal distribution. Each sample
drawn from the distribution is a realization of that distribution.

\begin{example}[Realizations of a Normal
Distribution]\protect\hypertarget{exm-realizations}{}\label{exm-realizations}

If you have a normal distribution with a mean of 0 and a standard
deviation of 1, each number you draw from that distribution is a
realization. Here is a Python example that generates 10 realizations of
a normal distribution with a mean of 0 and a standard deviation of 1:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{mu }\OperatorTok{=} \DecValTok{0}
\NormalTok{sigma }\OperatorTok{=} \DecValTok{1}
\NormalTok{realizations }\OperatorTok{=}\NormalTok{ np.random.normal(mu, sigma, }\DecValTok{10}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(realizations)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[ 0.48951662  0.23879586 -0.44811181 -0.610795   -2.02994507  0.60794659
 -0.35410888  0.15258149  0.50127485 -0.78640277]
\end{verbatim}

In this code, \texttt{np.random.normal} generates ten realizations of a
normal distribution with a mean of 0 and a standard deviation of 1. The
realizations array contains the actual values drawn from the
distribution.

\end{example}

\section{The Normal Distribution}\label{the-normal-distribution-2}

A commonly encountered probability distribution is the normal
distribution, known for its characteristic bell-shaped curve. This curve
represents how the values of a variable are distributed: most of the
observations cluster around the mean (or center) of the distribution,
with frequencies gradually decreasing as values move away from the mean.

The normal distribution is particularly useful because of its defined
mathematical properties. It is determined entirely by its mean (mu,
\(\mu\)) and its standard deviation (sigma, \(\sigma\)). The area under
the curve represents probability, making it possible to calculate the
likelihood of a random variable falling within a specific range.

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/rzFX5NWojp0}{The Normal Distribution,
  Clearly Explained!!!}
\end{itemize}

\begin{example}[Normal Distribution: Estimating
Probabilities]\protect\hypertarget{exm-estimat-prob}{}\label{exm-estimat-prob}

Consider we are interested in the heights of adults in a population.
Instead of measuring the height of every adult (which would be
impractical), we can use the normal distribution to estimate the
probability of adults' heights falling within certain intervals,
assuming we know the mean and standard deviation of the heights.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\NormalTok{mu }\OperatorTok{=} \DecValTok{170}  \CommentTok{\# e.g., mu height of adults in cm}
\NormalTok{sd }\OperatorTok{=} \DecValTok{10}  \CommentTok{\# e.g., standard deviation of heights in cm}
\NormalTok{heights }\OperatorTok{=}\NormalTok{ np.linspace(mu }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{sd, mu }\OperatorTok{+} \DecValTok{3}\OperatorTok{*}\NormalTok{sd, }\DecValTok{1000}\NormalTok{)}
\CommentTok{\# Calculate the probability density function for the normal distribution}
\NormalTok{pdf }\OperatorTok{=}\NormalTok{ norm.pdf(heights, mu, sd)}
\CommentTok{\# Plot the normal distribution curve}
\NormalTok{plt.plot(heights, pdf, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{plt.fill\_between(heights, pdf, where}\OperatorTok{=}\NormalTok{(heights }\OperatorTok{\textgreater{}=}\NormalTok{ mu }\OperatorTok{{-}} \DecValTok{2} \OperatorTok{*}\NormalTok{ sd) }\OperatorTok{\&}\NormalTok{ (heights }\OperatorTok{\textless{}=}\NormalTok{ mu }\OperatorTok{+} \DecValTok{2}\OperatorTok{*}\NormalTok{sd), color}\OperatorTok{=}\StringTok{\textquotesingle{}grey\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Height (cm)\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Probability Density\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-normal-distribution-output-1.pdf}}

}

\caption{\label{fig-normal-distribution}Normal Distribution Curve with
Highlighted Probability Area. 95 percent of the data falls within two
standard deviations of the mean.}

\end{figure}%

This Python code snippet generates a plot of the normal distribution for
adult heights, with a mean of 170 cm and a standard deviation of 10 cm.
It visually approximates a histogram with a blue bell-shaped curve, and
highlights (in grey) the area under the curve between
\(\mu \pm 2 \times \sigma\). This area corresponds to the probability of
randomly selecting an individual whose height falls within this range.

By using the area under the curve, we can efficiently estimate
probabilities without needing to collect and analyze a vast amount of
data. This method not only saves time and resources but also provides a
clear and intuitive way to understand and communicate statistical
probabilities.

\end{example}

\begin{definition}[Normal
Distribution]\protect\hypertarget{def-normal}{}\label{def-normal}

This variable is defined on the support \(D_X = \mathbb{R}\) and its
density function is given by \[
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left \{-\frac{1}{2\sigma^2}(x-\mu)^2 \right \}.
\] The density function is identified by the pair of parameters
\((\mu,\sigma^2)\), where \(\mu\) \(\in\) \(\mathbb{R}\) is the mean (or
location parameter) and \(\sigma^2 > 0\) is the variance (or dispersion
parameter) of \(X\). \hfill \(\Box\)

\end{definition}

The density function is symmetric around \(\mu\). The normal
distribution belongs to the location-scale family distributions. This
means that, if \(Z \sim N(0,1)\) (read, \(Z\) has a standard normal
distribution; i.e., with \(\mu=0\) and \(\sigma^2=1\)), and we consider
the linear transformation \(X = \mu + \sigma Z\), then
\(X \sim N(\mu,\sigma^2)\) (read, \(X\) has a normal distribution with
mean \(\mu\) and variance \(\sigma^2\)). This means that one can obtain
the probability of any interval \((-\infty,x]\), \(x\) \(\in\) \(R\) for
any normal distribution (i.e., for any pair of the parameters \(\mu\)
and \(\sigma\)) once the quantiles of the standard normal distribution
are known. Indeed \begin{eqnarray*}
F_X(x) &=& \Pr\left\{X \leq x \right\} = \Pr\left\{\frac{X-\mu}{\sigma} \leq \frac{x-\mu}{\sigma} \right\} \\
           &=& \Pr\left\{Z \leq \frac{x-\mu}{\sigma}\right\}  = F_Z\left(\frac{x-\mu}{\sigma}\right)    \qquad x \in \mathbb{R}.
\end{eqnarray*} The quantiles of the standard normal distribution are
available in any statistical program. The density and cumulative
distribution function of the standard normal r.v.\textasciitilde at
point \(x\) are usually denoted by the symbols \(\phi(x)\) and
\(\Phi(x)\).

The standard normal distribution is based on the
\emph{standard normal density function}
\begin{equation}\phantomsection\label{eq-standardization}{
 \varphi(z) = \frac{1}{\sqrt{2\pi}} \exp \left(- \frac{z^2}{2} \right).
}\end{equation}

An important application of the standardization introduced in
Equation~\ref{eq-standardization} reads as follows. In case the
distribution of \(X\) is approximately normal, the distribution of
X\^{}\{*\} is approximately standard normal. That is \begin{equation*}
  P(X\leq b) = P( \frac{X-\mu}{\sigma} \leq \frac{b-\mu}{\sigma}) = P(X^{*} \leq \frac{b-\mu}{\sigma})
\end{equation*} The probability \(P(X\leq b)\) can be approximated by
\(\Phi(\frac{b-\mu}{\sigma})\), where \(\Phi\) is the standard normal
cumulative distribution function.

If \(X\) is a normal random variable with mean \(\mu\) and variance
\(\sigma^2\), i.e., \(X \sim \cal{N} (\mu, \sigma^2)\), then
\begin{equation}
  X = \mu + \sigma Z \textrm{ where } Z \sim \cal{N}(0,1).
  \end{equation}

If \(Z \sim \cal{N}(0,1)\) and \(X\sim \cal{N}(\mu, \sigma^2)\), then
\begin{equation*}
  X = \mu + \sigma Z. 
\end{equation*}

The probability of getting a value in a particular interval is the area
under the corresponding part of the curve. Consider the density function
of the normal distribution. It can be plotted using the following
commands. The result is shown in Figure~\ref{fig-normal-density}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\CommentTok{\# Calculating the normal distribution\textquotesingle{}s density function values for each point in x}
\NormalTok{y }\OperatorTok{=}\NormalTok{ norm.pdf(x, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{plt.plot(x, y, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Normal Distribution\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Density\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-normal-density-output-1.pdf}}

}

\caption{\label{fig-normal-density}Normal Distribution Density Function}

\end{figure}%

The \emph{cumulative distribution function} (CDF) describes the
probability of ``hitting'' \(x\) or less in a given distribution. We
consider the CDF function of the normal distribution. It can be plotted
using the following commands. The result is shown in
Figure~\ref{fig-normal-cdf}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}

\CommentTok{\# Generating a sequence of numbers from {-}4 to 4 with 0.1 intervals}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}

\CommentTok{\# Calculating the cumulative distribution function value of the normal distribution for each point in x}
\NormalTok{y }\OperatorTok{=}\NormalTok{ norm.cdf(x, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)  }\CommentTok{\# mean=0, stddev=1}

\CommentTok{\# Plotting the results. The equivalent of \textquotesingle{}type="l"\textquotesingle{} in R (line plot) becomes the default plot type in matplotlib.}
\NormalTok{plt.plot(x, y, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Normal Distribution CDF\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Cumulative Probability\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-normal-cdf-output-1.pdf}}

}

\caption{\label{fig-normal-cdf}Normal Distribution Cumulative
Distribution Function}

\end{figure}%

\section{The Exponential
Distribution}\label{the-exponential-distribution}

The exponential distribution is a continuous probability distribution
that describes the time between events in a Poisson process, where
events occur continuously and independently at a constant average rate.
It is characterized by a single parameter, the rate parameter
\(\lambda\), which represents the average number of events per unit
time.

\subsection{Standardization of Random
Variables}\label{standardization-of-random-variables}

To compare statistical properties of random variables which use
different units, it is a common practice to transform these random
variables into standardized variables.

\begin{definition}[Standard
Units]\protect\hypertarget{def-standard-units}{}\label{def-standard-units}

If a random variable \(X\) has expectation \(E(X) = \mu\) and standard
deviation \(sd(X) = \sigma >0\), the random variable \[
X^{\ast} = (X-\mu)/\sigma
\] is called \(X\) in standard units. It has \(E(X^{\ast}) = 0\) and
\(sd(X^{\ast}) =1\).

\end{definition}

\section{Covariance and Correlation}\label{covariance-and-correlation}

\subsection{The Multivariate Normal
Distribution}\label{the-multivariate-normal-distribution}

The multivariate normal, multinormal, or Gaussian distribution serves as
a generalization of the one-dimensional normal distribution to higher
dimensions. We will consider \(k\)-dimensional random vectors
\(X = (X_1, X_2, \ldots, X_k)\). When drawing samples from this
distribution, it results in a set of values represented as
\(\{x_1, x_2, \ldots, x_k\}\). To fully define this distribution, it is
necessary to specify its mean \(\mu\) and covariance matrix \(\Sigma\).
These parameters are analogous to the mean, which represents the central
location, and the variance (squared standard deviation) of the
one-dimensional normal distribution introduced in
Equation~\ref{eq-normal-one}.

\begin{definition}[The Multivariate Normal
Distribution]\protect\hypertarget{def-multivariate-normal}{}\label{def-multivariate-normal}

The probability density function (PDF) of the multivariate normal
distribution is defined as: \[
f_X(x) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp\left(-\frac{1}{2} (x-\mu)^T\Sigma^{-1} (x-\mu)\right),
\] where: \(\mu\) is the \(k \times 1\) mean vector; \(\Sigma\) is the
\(k \times k\) covariance matrix. The covariance matrix \(\Sigma\) is
assumed to be positive definite, so that its determinant is strictly
positive.

\end{definition}

In the context of the multivariate normal distribution, the mean takes
the form of a coordinate within an \(k\)-dimensional space. This
coordinate represents the location where samples are most likely to be
generated, akin to the peak of the bell curve in a one-dimensional or
univariate normal distribution.

\begin{definition}[Covariance of two random
variables]\protect\hypertarget{def-covariance-2}{}\label{def-covariance-2}

For two random variables \(X\) and \(Y\), the covariance is defined as
the expected value (or mean) of the product of their deviations from
their individual expected values: \[
\operatorname{cov}(X, Y) = \operatorname{E}{\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]}
\]

For discrete random variables, covariance can be written as: \[
\operatorname{cov} (X,Y) = \frac{1}{n}\sum_{i=1}^n (x_i-E(X)) (y_i-E(Y)).
\]

\end{definition}

The covariance within the multivariate normal distribution denotes the
extent to which two variables vary together. The elements of the
covariance matrix, such as \(\Sigma_{ij}\), represent the covariances
between the variables \(x_i\) and \(x_j\). These covariances describe
how the different variables in the distribution are related to each
other in terms of their variability.

\begin{example}[The Bivariate Normal Distribution with Positive
Covariances]\protect\hypertarget{exm-bivariate-normal-cov-pos}{}\label{exm-bivariate-normal-cov-pos}

Figure~\ref{fig-bi9040} shows draws from a bivariate normal distribution
with \(\mu = \begin{pmatrix}0 \\ 0\end{pmatrix}\) and
\(\Sigma=\begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix}\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-bi9040-output-1.pdf}}

}

\caption{\label{fig-bi9040}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & 4 \\ 4 & 9\end{pmatrix}\)}

\end{figure}%

\end{example}

The covariance matrix of a bivariate normal distribution determines the
shape, orientation, and spread of the distribution in the
two-dimensional space.

The diagonal elements of the covariance matrix (\(\sigma_1^2\),
\(\sigma_2^2\)) are the variances of the individual variables. They
determine the spread of the distribution along each axis. A larger
variance corresponds to a greater spread along that axis.

The off-diagonal elements of the covariance matrix
(\(\sigma_{12}, \sigma_{21}\)) are the covariances between the
variables. They determine the orientation and shape of the distribution.
If the covariance is positive, the distribution is stretched along the
line \(y=x\), indicating that the variables tend to increase together.
If the covariance is negative, the distribution is stretched along the
line \(y=-x\), indicating that one variable tends to decrease as the
other increases. If the covariance is zero, the variables are
uncorrelated and the distribution is axis-aligned.

In Figure~\ref{fig-bi9040}, the variances are identical and the
variables are correlated (covariance is 4), so the distribution is
stretched along the line \(y=x\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-bi90403d-output-1.pdf}}

}

\caption{\label{fig-bi90403d}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & 4 \\ 4 & 9\end{pmatrix}\).}

\end{figure}%

\begin{example}[The Bivariate Normal Distribution with Mean Zero and
Zero
Covariances]\protect\hypertarget{exm-bivariate-normal-zero}{}\label{exm-bivariate-normal-zero}

The Bivariate Normal Distribution with Mean Zero and Zero Covariances
\(\sigma_{12} = \sigma_{21} = 0\).

\(\Sigma=\begin{pmatrix} 9 & 0 \\ 0 & 9\end{pmatrix}\)

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-bi9000-output-1.pdf}}

}

\caption{\label{fig-bi9000}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & 0 \\ 0 & 9\end{pmatrix}\)}

\end{figure}%

\end{example}

\begin{example}[The Bivariate Normal Distribution with Mean Zero and
Negative
Covariances]\protect\hypertarget{exm-bivariate-normal-zero-neg}{}\label{exm-bivariate-normal-zero-neg}

The Bivariate Normal Distribution with Mean Zero and Negative
Covariances \(\sigma_{12} = \sigma_{21} = -4\).

\(\Sigma=\begin{pmatrix} 9 & -4 \\ -4 & 9\end{pmatrix}\)

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_eda_files/figure-pdf/fig-bi9449-output-1.pdf}}

}

\caption{\label{fig-bi9449}Bivariate Normal. Mean zero and covariance
\(\Sigma=\begin{pmatrix} 9 & -4 \\ -4 & 9\end{pmatrix}\)}

\end{figure}%

\end{example}

\section{Covariance}\label{covariance}

In statistics, understanding the relationship between random variables
is crucial for making inferences and predictions. Two common measures of
such relationships are covariance and correlation. Covariance is a
measure of how much two random variables change together. If the
variables tend to show similar behavior (i.e., when one increases, the
other tends to increase), the covariance is positive. Conversely, if
they tend to move in opposite directions, the covariance is negative.

\begin{definition}[Covariance]\protect\hypertarget{def-covariance}{}\label{def-covariance}

Covariance is calculated as:

\[
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
\]

Here, \(E[X]\) and \(E[Y]\) are the expected values (means) of \(X\) and
\(Y\), respectively. Covariance has units that are the product of the
units of \(X\) and \(Y\).

\end{definition}

For a vector of random variables
\(\mathbf{Y} = \begin{pmatrix} Y^{(1)}, \ldots, Y^{(n)} \end{pmatrix}^T\),
the covariance matrix \(\Sigma\) encapsulates the covariances between
each pair of variables:

\[
\Sigma = \text{Cov}(\mathbf{Y}, \mathbf{Y}) =
\begin{pmatrix}
\text{Var}(Y^{(1)}) & \text{Cov}(Y^{(1)}, Y^{(2)}) & \ldots \\
\text{Cov}(Y^{(2)}, Y^{(1)}) & \text{Var}(Y^{(2)}) & \ldots \\
\vdots & \vdots & \ddots
\end{pmatrix}
\]

The diagonal elements represent the variances, while the off-diagonal
elements are the covariances.

\section{Correlation}\label{correlation}

\subsection{Definitions}\label{definitions}

\begin{definition}[(Pearson) Correlation
Coefficient]\protect\hypertarget{def-correlation-coefficient}{}\label{def-correlation-coefficient}

\index{correlation coefficient} \index{Pearson correlation coefficient}
The Pearson correlation coefficient, often denoted by \(\rho\) for the
population or \(r\) for a sample, is calculated by dividing the
covariance of two variables by the product of their standard deviations.

\begin{equation}\phantomsection\label{eq-pears-corr}{
\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y},
}\end{equation}

where \(\text{Cov}(X, Y)\) is the covariance between variables \(X\) and
\(Y\), and \(\sigma_X\) and \(\sigma_Y\) are the standard deviations of
\(X\) and \(Y\), respectively.

\end{definition}

Correlation, specifically the correlation coefficient, is a normalized
measure of the linear relationship between two variables. It provides a
value ranging from \(-1\) to \(1\), which is scale-free, making it
easier to interpret:

\begin{itemize}
\tightlist
\item
  \(-1\): Perfect negative correlation, indicating that as one variable
  increases, the other decreases.
\item
  \(0\): No correlation, indicating no linear relationship between the
  variables.
\item
  \(1\): Perfect positive correlation, indicating that both variables
  increase together.
\end{itemize}

The correlation matrix \(\Psi\) provides a way to quantify the linear
relationship between multiple variables, extending the notion of the
correlation coefficient beyond just pairs of variables. It is derived
from the covariance matrix \(\Sigma\) by normalizing each element with
respect to the variances of the relevant variables.

\begin{definition}[The Correlation Matrix
\(\Psi\)]\protect\hypertarget{def-correlation-matrix}{}\label{def-correlation-matrix}

Given a set of random variables \(X_1, X_2, \ldots, X_n\), the
covariance matrix \(\Sigma\) is:

\[
\Sigma = \begin{pmatrix}
\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1n} \\
\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{n1} & \sigma_{n2} & \cdots & \sigma_{nn}
\end{pmatrix},
\]

where \(\sigma_{ij} = \text{cov}(X_i, X_j)\) is the covariance between
the \(i^{\text{th}}\) and \(j^{\text{th}}\) variables. The correlation
matrix \(\Psi\) is then defined as:

\[
\Psi = \begin{pmatrix} \rho_{ij} \end{pmatrix} = \begin{pmatrix} \frac{\sigma_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}} \end{pmatrix},
\]

where:

\begin{itemize}
\tightlist
\item
  \(\rho_{ij}\) is the correlation coefficient between the
  \(i^{\text{th}}\) and \(j^{\text{th}}\) variables.
\item
  \(\sigma_{ii}\) is the variance of the \(i^{\text{th}}\) variable,
  i.e., \(\sigma_i^2\).
\item
  \(\sqrt{\sigma_{ii}}\) is the standard deviation of the
  \(i^{\text{th}}\) variable, denoted as \(\sigma_i\).
\end{itemize}

Thus, \(\Psi\) can also be expressed as:

\[
\Psi = \begin{pmatrix}
1 & \frac{\sigma_{12}}{\sigma_1 \sigma_2} & \cdots & \frac{\sigma_{1n}}{\sigma_1 \sigma_n} \\
\frac{\sigma_{21}}{\sigma_2 \sigma_1} & 1 & \cdots & \frac{\sigma_{2n}}{\sigma_2 \sigma_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\sigma_{n1}}{\sigma_n \sigma_1} & \frac{\sigma_{n2}}{\sigma_n \sigma_2} & \cdots & 1
\end{pmatrix}
\]

\end{definition}

The correlation matrix \(\Psi\) has the following properties:

\begin{itemize}
\tightlist
\item
  The matrix \(\Psi\) is symmetric, meaning \(\rho_{ij} = \rho_{ji}\).
\item
  The diagonal elements are all 1, as
  \(\rho_{ii} = \frac{\sigma_{ii}}{\sigma_i \sigma_i} = 1\).
\item
  Each off-diagonal element is constrained between -1 and 1, indicating
  the strength and direction of the linear relationship between pairs of
  variables.
\end{itemize}

\subsection{Computations}\label{computations}

\begin{example}[Computing a Correlation
Matrix]\protect\hypertarget{exm-covariance-correlation-matrix}{}\label{exm-covariance-correlation-matrix}

Suppose you have a dataset consisting of three variables: \(X\), \(Y\),
and \(Z\). You can compute the correlation matrix as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the covariance matrix \(\Sigma\), which contains covariances
  between all pairs of variables.
\item
  Extract the standard deviations for each variable from the diagonal
  elements of \(\Sigma\).
\item
  Use the standard deviations to compute the correlation matrix
  \(\Psi\).
\end{enumerate}

Suppose we have two sets of data points:

\begin{itemize}
\tightlist
\item
  \(X = [1, 2, 3]\)
\item
  \(Y = [4, 5, 6]\)
\end{itemize}

We want to compute the correlation matrix \(\Psi\) for these variables.
First, calculate the mean of each variable.

\[
\bar{X} = \frac{1 + 2 + 3}{3} = 2
\]

\[
\bar{Y} = \frac{4 + 5 + 6}{3} = 5
\]

Second, compute the covariance between \(X\) and \(Y\). The covariance
is calculated as:

\[
\text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})
\]

For our data:

\[
\text{Cov}(X, Y) = \frac{1}{3-1} \left[(1 - 2)(4 - 5) + (2 - 2)(5 - 5) + (3 - 2)(6 - 5)\right]
\]

\[
= \frac{1}{2} \left[1 + 0 + 1\right] = 1
\]

Third, calculate the variances of \(X\) and \(Y\). Variance is
calculated as:

\[
\text{Var}(X) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
\]

\[
= \frac{1}{2} \left[(1-2)^2 + (2-2)^2 + (3-2)^2\right] = \frac{1}{2} (1 + 0 + 1) = 1
\]

Similarly,

\[
\text{Var}(Y) = \frac{1}{2} \left[(4-5)^2 + (5-5)^2 + (6-5)^2\right] = \frac{1}{2} (1 + 0 + 1) = 1
\]

Then, compute the correlation coefficient. The correlation coefficient
\(\rho_{XY}\) is:

\[
\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \cdot \sqrt{\text{Var}(Y)}}
\]

\[
= \frac{1}{\sqrt{1} \cdot \sqrt{1}} = 1
\]

Finally, construct the correlation matrix. The correlation matrix
\(\Psi\) is given as:

\[
\Psi = \begin{pmatrix}
1 & \rho_{XY} \\
\rho_{XY} & 1
\end{pmatrix}
= \begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix}
\]

Thus, for these two variables, the correlation matrix indicates a
perfect positive linear relationship (correlation coefficient of 1)
between \(X\) and \(Y\).

\end{example}

\subsection{\texorpdfstring{The Outer-product and the \texttt{np.outer}
Function}{The Outer-product and the np.outer Function}}\label{the-outer-product-and-the-np.outer-function}

The function \texttt{np.outer} from the NumPy library computes the outer
product of two vectors. The outer product of two vectors results in a
matrix, where each element is the product of an element from the first
vector and an element from the second vector.

\begin{definition}[Outer
Product]\protect\hypertarget{def-outer-product}{}\label{def-outer-product}

For two vectors \(\mathbf{a}\) and \(\mathbf{b}\), the outer product is
defined in terms of their elements as:

\[
\text{outer}(\mathbf{a}, \mathbf{b}) = \begin{pmatrix}
a_1 \cdot b_1 & a_1 \cdot b_2 & \cdots & a_1 \cdot b_n \\
a_2 \cdot b_1 & a_2 \cdot b_2 & \cdots & a_2 \cdot b_n \\
\vdots & \vdots & \ddots & \vdots \\
a_m \cdot b_1 & a_m \cdot b_2 & \cdots & a_m \cdot b_n
\end{pmatrix},
\]

where \(\mathbf{a}\) is a vector of length \(m\) and \(\mathbf{b}\) is a
vector of length \(n\).

\end{definition}

\begin{example}[Computing the Outer
Product]\protect\hypertarget{exm-outer-product}{}\label{exm-outer-product}

We will consider two vectors, \(\mathbf{a}\) and \(\mathbf{b}\):

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{a }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])}
\NormalTok{b }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{outer\_product }\OperatorTok{=}\NormalTok{ np.outer(a, b)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Vector a:"}\NormalTok{, a)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Vector b:"}\NormalTok{, b)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Outer Product:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, outer\_product)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Vector a: [1 2 3]
Vector b: [4 5]
Outer Product:
 [[ 4  5]
 [ 8 10]
 [12 15]]
\end{verbatim}

For the vectors defined:

\[
\mathbf{a} = [1, 2, 3], \quad \mathbf{b} = [4, 5]
\]

The outer product will be:

\[
\begin{pmatrix}
1 \cdot 4 & 1 \cdot 5 \\
2 \cdot 4 & 2 \cdot 5 \\
3 \cdot 4 & 3 \cdot 5
\end{pmatrix}
=
\begin{pmatrix}
4 & 5 \\
8 & 10 \\
12 & 15
\end{pmatrix}.
\]

\end{example}

Thus, \texttt{np.outer} creates a matrix with dimensions \(m \times n\),
where \(m\) is the length of the first vector and \(n\) is the length of
the second vector. The function is particularly useful in various
mathematical and scientific computations where matrix representations of
vector relationships are needed.

\begin{example}[Computing the Covariance and the Correlation
Matrix]\protect\hypertarget{exm-covariance-correlation}{}\label{exm-covariance-correlation}

The following Python code computes the covariance and correlation
matrices using the NumPy library.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ calculate\_cov\_corr\_matrices(data, rowvar}\OperatorTok{=}\VariableTok{False}\NormalTok{)}\OperatorTok{{-}\textgreater{}}\NormalTok{(np.array, np.array):}
    \CommentTok{"""}
\CommentTok{    Calculate the covariance and correlation matrices of the input data.}

\CommentTok{    Args:}
\CommentTok{        data (np.array):}
\CommentTok{            Input data array.}
\CommentTok{        rowvar (bool):}
\CommentTok{            Whether the data is row{-}wise or column{-}wise.}
\CommentTok{            Default is False (column{-}wise).}

\CommentTok{    Returns:}
\CommentTok{        np.array: Covariance matrix.}
\CommentTok{        np.array: Correlation matrix.}

\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} data = np.array([[1, 2, 3],}
\CommentTok{                         [4, 5, 6],}
\CommentTok{                         [7, 8, 9]])}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} calculate\_cov\_corr\_matrices(data)}
\CommentTok{    """}   
\NormalTok{    cov\_matrix }\OperatorTok{=}\NormalTok{ np.cov(data, rowvar}\OperatorTok{=}\NormalTok{rowvar)}
\NormalTok{    std\_devs }\OperatorTok{=}\NormalTok{ np.sqrt(np.diag(cov\_matrix))}
    \CommentTok{\# check whether the standard deviations are zero}
    \CommentTok{\# and throw an error if they are}
    \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{any}\NormalTok{(std\_devs }\OperatorTok{==} \DecValTok{0}\NormalTok{):}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Correlation matrix cannot be computed,"}\OperatorTok{+}
                         \StringTok{" because one or more variables have zero variance."}\NormalTok{)}
\NormalTok{    corr\_matrix }\OperatorTok{=}\NormalTok{ cov\_matrix }\OperatorTok{/}\NormalTok{ np.outer(std\_devs, std\_devs)}
    \ControlFlowTok{return}\NormalTok{ cov\_matrix, corr\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{                 [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Input matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{A}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{Sigma, Psi }\OperatorTok{=}\NormalTok{ calculate\_cov\_corr\_matrices(A)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Covariance matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Sigma}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Correlation matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Psi}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Input matrix:
 [[0 1]
 [1 0]]
Covariance matrix:
 [[ 0.5 -0.5]
 [-0.5  0.5]]
Correlation matrix:
 [[ 1. -1.]
 [-1.  1.]]
\end{verbatim}

\end{example}

\subsection{Correlation and
Independence}\label{correlation-and-independence}

\begin{definition}[Statistical Independence (Independence of Random
Vectors)]\protect\hypertarget{def-statistical-independence}{}\label{def-statistical-independence}

Two random vectors are statistically independent if the joint
distribution of the vectors is equal to the product of their marginal
distributions.

\end{definition}

This means that knowing the realization of one vector gives you no
information about the realization of the other vector. This independence
is a probabilistic concept used in statistics and probability theory to
denote that two sets of random variables do not affect each other.
Independence implies that all pairwise covariances between the
components of the two vectors are zero, but zero covariance does not
imply independence unless certain conditions are met (e.g., normality).
Statistical independence is a stronger condition than zero covariance.
Statistical independence is not related to the linear independence of
vectors in linear algebra.

\begin{example}[Covariance of Independent
Variables]\protect\hypertarget{exm-cov-independent}{}\label{exm-cov-independent}

Consider a covariance matrix where variables are independent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{],}
\NormalTok{[}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{[}\DecValTok{4}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{[}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Input matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{A}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{Sigma, Psi }\OperatorTok{=}\NormalTok{ calculate\_cov\_corr\_matrices(A)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Covariance matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Sigma}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Correlation matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Psi}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Input matrix:
 [[ 1 -1]
 [ 2  0]
 [ 3  1]
 [ 4  0]
 [ 5 -1]]
Covariance matrix:
 [[2.5 0. ]
 [0.  0.7]]
Correlation matrix:
 [[1. 0.]
 [0. 1.]]
\end{verbatim}

Here, since the off-diagonal elements are 0, the variables are
uncorrelated. \(X\) increases linearly, while \(Y\) alternates in a
simple pattern with no trend that is linearly related to \(Y\).

\end{example}

\begin{example}[Strong
Correlation]\protect\hypertarget{exm-cov-strong}{}\label{exm-cov-strong}

For a covariance matrix with strong positive correlation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{10}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{],}
\NormalTok{[}\DecValTok{20}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{[}\DecValTok{30}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{[}\DecValTok{40}\NormalTok{,}\DecValTok{2}\NormalTok{],}
\NormalTok{[}\DecValTok{50}\NormalTok{,}\DecValTok{3}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Input matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{A}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{Sigma, Psi }\OperatorTok{=}\NormalTok{ calculate\_cov\_corr\_matrices(A)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Covariance matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Sigma}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Correlation matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Psi}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Input matrix:
 [[10 -1]
 [20  0]
 [30  1]
 [40  2]
 [50  3]]
Covariance matrix:
 [[250.   25. ]
 [ 25.    2.5]]
Correlation matrix:
 [[1. 1.]
 [1. 1.]]
\end{verbatim}

A value close to 1 suggests a strong positive relationship between the
variables.

\end{example}

\begin{example}[Strong Negative
Correlation]\protect\hypertarget{exm-cov-negative}{}\label{exm-cov-negative}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{[}\DecValTok{20}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{[}\DecValTok{30}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{],}
\NormalTok{[}\DecValTok{40}\NormalTok{,}\OperatorTok{{-}}\DecValTok{2}\NormalTok{],}
\NormalTok{[}\DecValTok{50}\NormalTok{,}\OperatorTok{{-}}\DecValTok{3}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Input matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{A}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{Sigma, Psi }\OperatorTok{=}\NormalTok{ calculate\_cov\_corr\_matrices(A)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Covariance matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Sigma}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Correlation matrix:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{Psi}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Input matrix:
 [[10  1]
 [20  0]
 [30 -1]
 [40 -2]
 [50 -3]]
Covariance matrix:
 [[250.  -25. ]
 [-25.    2.5]]
Correlation matrix:
 [[ 1. -1.]
 [-1.  1.]]
\end{verbatim}

This matrix indicates a perfect negative correlation where one variable
increases as the other decreases.

\end{example}

\subsection{Pearson's Correlation}\label{pearsons-correlation}

\begin{itemize}
\tightlist
\item
  Video: {[}Pearson's Correlation, Clearly Explained{]}
\end{itemize}

\subsection{Interpreting the Correlation: Correlation
Squared}\label{interpreting-the-correlation-correlation-squared}

Rummel (1976) describes how to interpret correlations as follows:

Seldom, indeed, will a correlation be zero or perfect. Usually, the
covariation between things will be something like \(.56\) or \(-.16\).
Clearly \(.56\) is positive, indicating positive covariation; \(-.16\)
is negative, indicating some negative covariation. Moreover, we can say
that the positive correlation is greater than the negative. But, we
require more than. If we have a correlation of \(.56\) between two
variables, for example, what precisely can we say other than the
correlation is positive and \(.56\)? The squared correlation describes
the proportion of variance in common between the two variables. If we
multiply this by 100 we then get the percent of variance in common
between two variables. That is:

\[
r^2_{XY} \times  100 = \text{percent of variance in common between} X \text{ and } Y.
\]

For example, we found that the correlation between a nation's power and
its defense budget was \(.66\). This correlation squared is \(.45\),
which means that across the fourteen nations constituting the sample
\(45\) percent of their variance on the two variables is in common (or
\(55\) percent is not in common). In thus squaring correlations and
transforming covariance to percentage terms we have an easy to
understand meaning of correlation. And we are then in a position to
evaluate a particular correlation. As a matter of routine it is the
squared correlations that should be interpreted. This is because the
correlation coefficient is misleading in suggesting the existence of
more covariation than exists, and this problem gets worse as the
correlation approaches zero.

\begin{example}[The relationship between study time and test
scores]\protect\hypertarget{exm-explain-cov}{}\label{exm-explain-cov}

Imagine we are examining the relationship between the number of hours
students study for a subject (Variable \(A\)) and their scores on a test
(Variable \(B\)). After analyzing the data, we calculate a correlation
of 0.8 between study time and test scores. When we square this
correlation coefficient (\(0.8^2\) = 0.64), we get 0.64 or 64\%. This
means that 64\% of the variability in test scores can be accounted for
by the variability in study hours. This indicates that a substantial
part of why students score differently on the test can be explained by
how much they studied. However, there remains 36\% of the variability in
test scores that needs to be explained by other factors, such as
individual abilities, the difficulty of the test, or other external
influences.

\end{example}

\subsection{Partial Correlation}\label{partial-correlation}

Often, a correlation between two variables \(X\) and \(Y\) can be found
only because both variables are correlated with a third variable \(Z\).
The correlation between \(X\) and \(Y\) is then a spurious correlation.
Therefore, it is often of interest to determine the correlation between
\(X\) and \(Y\) while partializing a variable \(Z\), i.e., the
correlation between \(X\) and \(Y\) that exists without the influence of
\(Z\). Such a correlation \(\rho_{(X,Y)/Z}\) is called the partial
correlation of \(X\) and \(Y\) while holding \(Z\) constant. It is given
by \begin{equation}
\rho_{(X,Y)/Z} = \frac{\rho_{XY} - \rho_{XZ}\rho_{YZ}}{\sqrt{(1-\rho_{XZ}^2)(1-\rho_{YZ}^2)}}, 
\end{equation} where \(\rho_{XY}\) is the correlation between \(X\) and
\(Y\), \(\rho_{XZ}\) is the correlation between \(X\) and \(Z\), and
\(\rho_{YZ}\) is the correlation between \(Y\) and \(Z\) (Hartung,
Elpert, and KlÃ¶sener 1995).

If the variables \(X\), \(Y\) and \(Z\) are jointly normally distributed
in the population of interest, one can estimate \(\rho_{(X,Y)/Z}\) based
on \(n\) realizations \(x_1, \ldots, x_n\), \(y_1, \ldots, y_n\) and
\(z_1, \ldots, z_n\) of the random variables \(X\), \(Y\) and \(Z\) by
replacing the simple correlations \(\rho_{XY}\), \(\rho_{XZ}\) and
\(\rho_{YZ}\) with the empirical correlations \(\hat{\rho}_{XY}\),
\(\hat{\rho}_{XZ}\) and \(\hat{\rho}_{YZ}\). The partial correlation
coefficient \(\hat{\rho}_{(X,Y)/Z}\) is then estimated using
\begin{equation}
r_{(X,Y)/Z} = \frac{r_{XY} - r_{XZ}r_{YZ}}{\sqrt{(1-r_{XZ}^2)(1-r_{YZ}^2)}}.
\end{equation} Based on this estimated value for the partial
correlation, a test at the \(\alpha\) level for partial uncorrelatedness
or independence of \(X\) and \(Y\) under \(Z\) can also be carried out.
The hypothesis

\begin{equation}
H_0: \rho_{(X,Y)/Z} = 0
\end{equation} is tested against the alternative \begin{equation}
H_1: \rho_{(X,Y)/Z} \neq 0
\end{equation} at the level \(\alpha\) is discarded if \[
\left| 
\frac{r_{(X,Y)/Z} \sqrt{n-3}}{\sqrt{1-r_{(X,Y)/Z}^2}} 
\right| > t_{n-3, 1-\alpha/2}
\] applies. Here \(t_{n-3, 1-\alpha/2}\) is the
(\(1-\alpha/2\))-quantile of the \(t\)-distribution with \(n-3\) degrees
of freedom.

\begin{example}[]\protect\hypertarget{exm-partial-corr1}{}\label{exm-partial-corr1}

For example, given economic data on the consumption \(X\), income \(Y\),
and wealth \(Z\) of various individuals, consider the relationship
between consumption and income. Failing to control for wealth when
computing a correlation coefficient between consumption and income would
give a misleading result, since income might be numerically related to
wealth which in turn might be numerically related to consumption; a
measured correlation between consumption and income might actually be
contaminated by these other correlations. The use of a partial
correlation avoids this problem (Wikipedia contributors 2024).

\end{example}

\begin{example}[Partial Correlation. Numerical
Example]\protect\hypertarget{exm-partial-correlation}{}\label{exm-partial-correlation}

Given the following data, calculate the partial correlation between
\(A\) and \(B\), controlling for \(C\). \[
A = \begin{pmatrix}
2\\
4\\
15\\
20
\end{pmatrix}, \quad B = \begin{pmatrix}
1\\
2\\
3\\
4
\end{pmatrix}, \quad C = \begin{pmatrix}
0\\
0\\
1\\
1
\end{pmatrix}
\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.stats }\ImportTok{import}\NormalTok{ partial\_correlation}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{: [}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{],}
    \StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{: [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{],}
    \StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{: [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{\})}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Correlation between A and B: }\SpecialCharTok{\{}\NormalTok{data[}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{]}\SpecialCharTok{.}\NormalTok{corr(data[}\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{pc }\OperatorTok{=}\NormalTok{ partial\_correlation(data, method}\OperatorTok{=}\StringTok{\textquotesingle{}pearson\textquotesingle{}}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Partial Correlation between A and B: }\SpecialCharTok{\{}\NormalTok{pc[}\StringTok{"estimate"}\NormalTok{][}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Correlation between A and B: 0.9695015519208121
Partial Correlation between A and B: 0.9191450300180576
\end{verbatim}

\end{example}

Instead of considering only one variable \(Z\), multiple variables
\(Z_i\) can be considered. The formal definiton of partial correlation
reads as follows:

\begin{definition}[Partial
Correlation]\protect\hypertarget{def-partial-correlation}{}\label{def-partial-correlation}

Formally, the partial correlation between \(X\) and \(Y\) given a set of
\(n\) controlling variables \(\mathbf{Z} = \{Z_1, Z_2, \ldots, Z_n\}\),
written \(\rho_{XY \cdot \mathbf{Z}}\), is the correlation between the
residuals \(e_X\) and \(e_Y\) resulting from the linear regression of
\(X\) with \(\mathbf{Z}\) and of \(Y\) with \(\mathbf{Z}\),
respectively. The first-order partial correlation (i.e., when \(n = 1\))
is the difference between a correlation and the product of the removable
correlations divided by the product of the coefficients of alienation of
the removable correlations (Wikipedia contributors 2024).

\end{definition}

Like the correlation coefficient, the partial correlation coefficient
takes on a value in the range from -1 to 1. The value -1 conveys a
perfect negative correlation controlling for some variables (that is, an
exact linear relationship in which higher values of one variable are
associated with lower values of the other); the value 1 conveys a
perfect positive linear relationship, and the value 0 conveys that there
is no linear relationship (Wikipedia contributors 2024).

\begin{lemma}[Matrix Representation of the Partial
Correlation]\protect\hypertarget{lem-matrix-part-corr}{}\label{lem-matrix-part-corr}

The partial correlation can also be written in terms of the joint
precision matrix (Wikipedia contributors 2024). Consider a set of random
variables, \(\mathbf{V} = \{X_1,\dots, X_n\}\) of cardinality \(n\). We
want the partial correlation between two variables \(X_i\) and \(X_j\)
given all others, i.e., \(\mathbf{V} \setminus \{X_i, X_j\}\). Suppose
the (joint/full) covariance matrix \(\Sigma = (\sigma_{ij})\) is
positive definite and therefore invertible. If the precision matrix is
defined as \(\Omega = (p_{ij}) = \Sigma^{-1}\), then \begin{equation}
\rho_{X_i X_j \cdot \mathbf{V} \setminus \{X_i,X_j\}} = - \frac{p_{ij}}{\sqrt{p_{ii}p_{jj}}}
\end{equation}

\end{lemma}

The semipartial correlation statistic is similar to the partial
correlation statistic; both compare variations of two variables after
certain factors are controlled for. However, to calculate the
semipartial correlation, one holds the third variable constant for
either X or Y but not both; whereas for the partial correlation, one
holds the third variable constant for both (Wikipedia contributors
2024).

\section{Hypothesis Testing and the
Null-Hypothesis}\label{hypothesis-testing-and-the-null-hypothesis}

\subsection{Alternative Hypotheses, Main
Ideas}\label{alternative-hypotheses-main-ideas}

\subsection{p-values: What they are and how to interpret
them}\label{p-values-what-they-are-and-how-to-interpret-them}

\subsubsection{How to calculate
p-values}\label{how-to-calculate-p-values}

\subsubsection{p-hacking: What it is and how to avoid
it}\label{p-hacking-what-it-is-and-how-to-avoid-it}

\section{Statistical Power}\label{statistical-power}

\begin{itemize}
\tightlist
\item
  Video:
  \href{https://youtu.be/Rsc5znwR5FA?si=Ca4e-EopumAtgl8Q}{Statistical
  Power, Clearly Explained}
\end{itemize}

\subsubsection{Power Analysis}\label{power-analysis}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/VX_M3tIyiYk?si=Vb6Fr1aJWQU5Ujjp}{Power
  Analysis, Clearly Explained!!!}
\end{itemize}

\section{The Central Limit Theorem}\label{the-central-limit-theorem}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/YAlJCEDH2uY?si=NRYvP7Y0Mow32jV2}{The
  Central Limit Theorem, Clearly Explained!!!}
\end{itemize}

\section{Maximum Likelihood}\label{maximum-likelihood}

Maximum likelihood estimation is a method used to estimate the
parameters of a statistical model. It is based on the principle of
choosing the parameter values that maximize the likelihood of the
observed data. The likelihood function represents the probability of
observing the data given the model parameters. By maximizing this
likelihood, we can find the parameter values that best explain the
observed data.

\begin{example}[Maximum Likelihood Estimation: Bernoulli
Experiment]\protect\hypertarget{exm-MaxLike1Bern}{}\label{exm-MaxLike1Bern}

\index{Maximum Likelihood Estimation} Bernoulli experiment for the event
\(A\), repeated \(n\) times, with the probability of success \(p\).
Result given as \(n\) tuple with entries \(A\) and \(\overline{A}\).
\(A\) appears \(k\) times. The probability of this event is given by
\begin{equation}
L(p) = p^k (1-p)^{n-k}
\end{equation} \label{eq:mle-bernoulli} Applying maximum likelihood
estimation, we find the maximum of the likelihood function \(L(p)\),
i.e., we are trying to find the value of \(p\) that maximizes the
probability of observing the data. This value will be denoted as
\(\hat{p}\).

Differentiating the likelihood function with respect to \(p\) and
setting the derivative to zero, we find the maximum likelihood estimate
\(\hat{p}\). We get \begin{align}
\frac{d}{dp} L(p) & = k p^{k-1} (1-p)^{n-k} - p^k (n-k) (1-p)^{n-k-1}\\
                  & = p^{k-1} (1-p)^{n-k-1} \left(k(1-p) - p(n-k)\right) = 0
\end{align}

Because \[
p \neq 0 \text{ and } (1-p) p \neq 0,
\] we can divide by \(p^{k-1} (1-p)^{n-k-1}\) and get \begin{equation}
k(1-p) - p(n-k) = 0.
\end{equation} Solving for \(p\) gives \begin{equation}
\hat{p} = \frac{k}{n}
\end{equation}

Therefore, the maximum likelihood estimate for the probability of
success in a Bernoulli experiment is the ratio of the number of
successes to the total number of trials.

\hfill \(\Box\)

\end{example}

\begin{example}[Maximum Likelihood Estimation: Normal
Distribution]\protect\hypertarget{exm-MaxLikeNormal}{}\label{exm-MaxLikeNormal}

Random variable \(X \sim \mathcal{N}(\mu, \sigma^2)\) with \(n\)
observations \(x_1, x_2, \ldots, x_n\). The likelihood function is given
by \begin{equation}
L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
\end{equation}

Taking the logarithm of the likelihood function, we get \begin{equation}
\log L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
\end{equation}

Partial derivative with respect to \(\mu\) is \begin{align}
\frac{\partial}{\partial \mu} \log L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) & = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0
\end{align} We obtain the maximum likelihood estimate for \(\mu\) as
\begin{equation}
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i
\end{equation}

The partial derivative with respect to \(\sigma^2\) is \begin{align}
\frac{\partial}{\partial \sigma^2} \log L(x_1, x_2, \ldots, x_n, \mu, \sigma^2) & = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (x_i - \mu)^2 = 0
\end{align} This can be simplified to \begin{align}
-n + \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 = 0\\
\Rightarrow n \sigma^2 = \sum_{i=1}^n (x_i - \mu)^2
\end{align} Using the maximum likelihood estimate for \(\mu\), we get
\begin{equation}
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2
\end{equation} \begin{equation}
= \frac{n-1}{n} \frac{\sum_{i=1}^n (x_i - \hat{\mu})^2{n-1}} = \frac{n-1}{n} s^2,
\end{equation} where \begin{equation}
s = \sqrt{\frac{\sum_{i=1}^n (x_i- \overline{x})}{n-1}}
\end{equation} is the sample standard deviation. \index{sample variance}
\index{sample standard deviation} We obtain the maximum likelihood
estimate for \(\sigma^2\) as \begin{equation}
\hat{\sigma}^2 = \frac{n-1}{n} s^2
\end{equation}

\end{example}

\begin{itemize}
\item
  Video: \href{https://youtu.be/XepXtl9YKwc?si=ADMYC10DscaxSTvk}{Maximum
  Likelihood, clearly explained!!!}
\item
  Video:
  \href{https://youtu.be/pYxNSUDSFH4?si=eEan9lAUp1NNGEjY}{Probability is
  not Likelihood. Find out why!!!}
\end{itemize}

\section{Maximum Likelihood Estimation: Multivariate Normal
Distribution}\label{maximum-likelihood-estimation-multivariate-normal-distribution}

\subsection{The Joint Probability Density Function of the Multivariate
Normal
Distribution}\label{the-joint-probability-density-function-of-the-multivariate-normal-distribution}

Consider the first \(n\) terms of an identically and independently
distributed (i.i..d.) sequence \({X^{(j)}}\) of \(k\)-dimensional
multivariate normal random vectors, i.e.,
\begin{equation}\phantomsection\label{eq-mvn}{
X^{(j)} \sim N(\mu, \Sigma), j=1,2,\ldots.
}\end{equation}

The joint probability density function of the \(j\)-th term of the
sequence is \[
f_X(x_j) = \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right),
\]

where: \(\mu\) is the \(k \times 1\) mean vector; \(\Sigma\) is the
\(k \times k\) covariance matrix. The covariance matrix \(\Sigma\) is
assumed to be positive definite, so that its determinant is strictly
positive. We use \(x_1, \ldots x_n\), i.e., the realizations of the
first \(n\) random vectors in the sequence, to estimate the two unknown
parameters \(\mu\) and \(\Sigma\).

\subsection{The Log-Likelihood
Function}\label{the-log-likelihood-function}

\begin{definition}[Likelihood
Function]\protect\hypertarget{def-likelihood}{}\label{def-likelihood}

The likelihood function is defined as the joint probability density
function of the observed data, viewed as a function of the unknown
parameters.

\end{definition}

Since the terms in the sequence Equation~\ref{eq-mvn} are independent,
their joint density is equal to the product of their marginal densities.
As a consequence, the likelihood function can be written as the product
of the individual densities:

\[
L(\mu, \Sigma) = \prod_{j=1}^n f_X(x_j) = \prod_{j=1}^n \frac{1}{\sqrt{(2\pi)^k \det(\Sigma)}} \exp\left(-\frac{1}{2} (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right)
\] \begin{equation}\phantomsection\label{eq-likelihood-mvn}{
= \frac{1}{(2\pi)^{nk/2} \det(\Sigma)^{n/2}} \exp\left(-\frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu)\right).
}\end{equation}

Taking the natural logarithm of the likelihood function, we obtain the
log-likelihood function:

\begin{example}[Log-Likelihood Function of the Multivariate Normal
Distribution]\protect\hypertarget{exm-log-likelihood}{}\label{exm-log-likelihood}

The log-likelihood function of the multivariate normal distribution is
given by \[
\ell(\mu, \Sigma) = -\frac{nk}{2} \ln(2\pi) - \frac{n}{2} \ln(\det(\Sigma)) - \frac{1}{2} \sum_{j=1}^n (x_j-\mu)^T\Sigma^{-1} (x_j-\mu).
\]

\end{example}

The likelihood function is well-defined only if \(\det(\Sigma)>0\).

\section{Cross-Validation}\label{cross-validation-1}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/fSytzGwwBVw?si=a8U5yCIEhwAw4AyU}{Machine
  Learning Fundamentals: Cross Validation}
\end{itemize}

\subsubsection{Bias and Variance}\label{bias-and-variance}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/EuBBz3bI-aA?si=7MVv_J1HbzMSQS4K}{Machine
  Learning Fundamentals: Bias and Variance}
\end{itemize}

\section{Mutual Information}\label{mutual-information}

\(R^2\) works only for numerical data. Mutual information can also be
used, when the dependent variable is boolean or categorical. Mutual
information provides a way to quantify the relationship of a mixture of
continuous and discrete variables to a target variable. Mutual
information explains how closely related two variables are. It is a
measure of the amount of information that one variable provides about
another variable.

\begin{definition}[Mutual
Information]\protect\hypertarget{def-mutual-information}{}\label{def-mutual-information}

\begin{equation}\phantomsection\label{eq-mutual-information}{
\text{MI} =
\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \left(\frac{p(x, y)}{p(x)p(y)}\right)
}\end{equation}

\end{definition}

The terms in the nominator and denominator in
Equation~\ref{eq-mutual-information} are the joint probability,
\(p(x, y)\), the marginal probability of \(X\), \(p(x)\) and the
marginal probability of \(Y\), \(p(y)\) respectively. Joint probabilites
are the probabilities of two events happening together. Marginal
probabilities are the probabilities of individual events happening. If
\(p(x,y) = p(x)p(y)\), then \(X\) and \(Y\) are independent. In this
case, the mutual information is one.

The mutual information of two variables \(X\) and \(Y\) is \begin{align}
\text{MI}(X, Y) = 
p(X,Y) \log \left(\frac{p(X,Y)}{p(X)p(Y)}\right) + 
p(X, \overline{Y}) \log \left(\frac{p(X, \overline{Y})}{p(X)p(\overline{Y})}\right) +  \\ 
p(\overline{X}, Y) \log \left(\frac{p(\overline{X}, Y)}{p(\overline{X})p(Y)}\right) + 
p(\overline{X}, \overline{Y}) \log \left(\frac{p(\overline{X}, \overline{Y})}{p(\overline{X})p(\overline{Y})}\right)
\end{align}

In general, when at least one of the two features has no variance (i.e.,
it is constant), the mutual information is zero, because something that
never changes cannot tell us about something that does. When two
features change, but change in exact the same way, then \(\text{MI}\) is
\(1/2\). When two factors change, but in exact the opposite way, then
\(\text{MI}\) is \(1/2\). When both features change, it does not matter
if they change in the exact same or exact opposite ways; both result in
the same \(\text{MI}\) value

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Mutual Information}]

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/eJIp_mgVLwE?si=KaeiRN0st1gqkj4c}{Mutual
  Information, Clearly Explained}
\end{itemize}

\end{tcolorbox}

\section{Principal Component Analysis
(PCA)}\label{principal-component-analysis-pca}

\begin{itemize}
\item
  Video:
  \href{https://youtu.be/FgakZw6K1QQ?si=lmXhc-bpOqb7RmDP}{Principal
  Component Analysis (PCA), Step-by-Step}
\item
  Vidoe: \href{https://youtu.be/oRvgq966yZg?si=TIUsxNItfyYOjTLt}{PCA -
  Practical Tips}
\item
  Video: \href{https://youtu.be/Lsue2gEM9D0?si=_fV_RzK8j1jwcb-e}{PCA in
  Python}
\end{itemize}

\section{t-SNE}\label{t-sne}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/NEaUSP4YerM?si=f8-6ewwv5TMD7gdL}{t-SNE,
  Clearly Explained}
\end{itemize}

\chapter{Regression}\label{regression}

\section{Supervised and Unsupervised
Learning}\label{supervised-and-unsupervised-learning}

Two important types: supervised and unsupervised learning. There is even
more, e.g., semi-supervised learning.

\subsection{Starting point}\label{starting-point}

\begin{itemize}
\tightlist
\item
  Outcome measurement \(Y\) (dependent variable, response, target).
\item
  Vector of \(p\) predictor measurements \(X\) (inputs, regressors,
  covariates, features, independent variables).
\item
  Training data \((x_1, y1), \ldots ,(x_N, y_N)\). These are
  observations (examples, instances) of these measurements.
\end{itemize}

In the \emph{regression} problem, \(Y\) is quantitative (e.g., price,
blood pressure). In the \emph{classification} problem, \(Y\) takes
values in a finite, unordered set (e.g., survived/died, digit 0-9,
cancer class of tissue sample).

\subsection{Philosophy}\label{philosophy}

It is important to understand the ideas behind the various techniques,
in order to know how and when to use them. One has to understand the
simpler methods first, in order to grasp the more sophisticated ones. It
is important to accurately assess the performance of a method, to know
how well or how badly it is working (simpler methods often perform as
well as fancier ones!) This is an exciting research area, having
important applications in science, industry and finance. Statistical
learning is a fundamental ingredient in the training of a modern data
scientist.

\subsection{Supervised Learning}\label{supervised-learning}

Objectives of supervised learning: On the basis of the training data we
would like to:

\begin{itemize}
\tightlist
\item
  Accurately predict unseen test cases.
\item
  Understand which inputs affect the outcome, and how.
\item
  Assess the quality of our predictions and inferences.
\end{itemize}

Note: Supervised means \(Y\) is known.

\begin{exercise}[]\protect\hypertarget{exr-starting-point}{}\label{exr-starting-point}

~

\begin{itemize}
\tightlist
\item
  Do children learn supervised?
\item
  When do you learn supervised?
\item
  Can learning be unsupervised?
\end{itemize}

\end{exercise}

\subsection{Unsupervised Learning}\label{unsupervised-learning}

No outcome variable, just a set of predictors (features) measured on a
set of samples. The objective is more fuzzy---find groups of samples
that behave similarly, find features that behave similarly, find linear
combinations of features with the most variation. It is difficult to
know how well your are doing. Unsupervised learning different from
supervised learning, but can be useful as a pre-processing step for
supervised learning. Clustering and principle component analysis are
important techniques.

Unsupervised: \(Y\) is unknown, there is no \(Y\), no trainer, no
teacher, but: distances between the inputs values (features). A distance
(or similarity) measure is necessary.

\paragraph{Statistical Learning}\label{statistical-learning}

We consider supervised learning first.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0201.png}

}

\caption{\label{fig-0201}Sales as a function of TV, radio and newspaper.
Taken from James et al. (2014)}

\end{figure}%

Sales figures from a marketing campaign, see Figure~\ref{fig-0201}.
Trend shown using regression. First seems to be stronger than the third.

Can we predict \(Y\) = Sales using these three? Perhaps we can do better
using a model \[
Y = Sales \approx  f(X_1 = TV,  X_2 = Radio, X_3= Newspaper)
\] modeling the joint relationsship.

Here Sales is a response or target that we wish to predict. We
generically refer to the response as \(Y\). TV is a feature, or input,
or predictor; we name it \(X_1\). Likewise name Radio as \(X_2\), and so
on. We can refer to the input vector collectively as \[
X =
\begin{pmatrix}
X_1\\
X_2\\
X_3
\end{pmatrix}
\]

Now we write our model as \[
Y = f(X) + \epsilon
\] where \(\epsilon\) captures measurement errors and other
discrepancies.

What is \(f\) good for? With a good \(f\) we can make predictions of
\(Y\) at new points \(X = x\). We can understand which components of
\(X = (X_1, X_2, \ldots X_p)\) are important in explaining \(Y\), and
which are irrelevant.

For example, Seniority and Years of Education have a big impact on
Income, but Marital Status typically does not. Depending on the
complexity of \(f\), we may be able to understand how each component
\(X_j\) of \(X\) affects \(Y\).

\section{Linear Regression}\label{linear-regression-1}

\subsection{The main ideas of fitting a line to data (The main ideas of
least squares and linear
regression.)}\label{the-main-ideas-of-fitting-a-line-to-data-the-main-ideas-of-least-squares-and-linear-regression.}

\begin{itemize}
\tightlist
\item
  Video: \href{https://www.youtube.com/embed/PaFPbb66DxQ}{The main ideas
  of fitting a line to data (The main ideas of least squares and linear
  regression.)}
\end{itemize}

\subsubsection{Linear Regression}\label{linear-regression-2}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/nk2CQITm_eo}{Linear Regression, Clearly
  Explained}
\end{itemize}

\subsubsection{Multiple Regression}\label{multiple-regression}

\begin{itemize}
\tightlist
\item
  Video: Multiple Regression, Clearly Explained
\end{itemize}

\subsubsection{A Gentle Introduction to Machine
Learning}\label{a-gentle-introduction-to-machine-learning}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/Gv9_4yMHFhI?si=BPtw5Rekl37bJ9V1}{A
  Gentle Introduction to Machine Learning}
\end{itemize}

\subsubsection{Regression Function}\label{regression-function}

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0202a.png}

}

\caption{\label{fig-0202a}Scatter plot of 2000 points (population). What
is a good function \(f\)? There are many function values at \(X=4\). A
function can return only one value. We can take the mean from these
values as a return value. Taken from James et al. (2014)}

\end{figure}%

Consider Figure~\ref{fig-0202a}. Is there an ideal \(f(X)\)? In
particular, what is a good value for \(f(X)\) at any selected value of
\(X\), say \(X = 4\)? There can be many \(Y\) values at \(X=4\). A good
value is \[
f(4) = E(Y |X = 4).
\]

\(E(Y |X = 4)\) means \textbf{expected value} (average) of \(Y\) given
\(X = 4\).

The ideal \(f(x) = E(Y |X = x)\) is called the \textbf{regression
function}. Read: The regression function gives the conditional
expectation of \(Y\) given \(X\).

The regression function \(f(x)\) is also defined for the vector \(X\);
e.g., \(f(x) = f(x_1, x_2, x_3) = E(Y | X_1 =x_1, X_2 =x_2, X_3 =x_3).\)

\subsection{Optimal Predictor}\label{optimal-predictor}

The regression function is the \textbf{ideal} or \textbf{optimal
predictor} of \(Y\) with regard to mean-squared prediction error: It
means that \(f(x) = E(Y | X = x)\) is the function that minimizes \[
E[(Y - g(X))^2|X = x]
\] over all functions \(g\) at all points \(X = x\).

\subsubsection{Residuals, Reducible and Irreducible
Error}\label{residuals-reducible-and-irreducible-error}

At each point \(X\) we make mistakes: \[
\epsilon = Y-f(x)
\] is the \textbf{residual}. Even if we knew \(f(x)\), we would still
make errors in prediction, since at each \(X=x\) there is typically a
distribution of possible \(Y\) values as is illustrated in
Figure~\ref{fig-0202a}.

For any estimate \(\hat{f}(x)\) of \(f(x)\), we have \[
E\left[ ( Y - \hat{f}(X))^2 | X = x\right] = \left[ f(x) - \hat{f}(x) \right]^2 + \text{var}(\epsilon),
\] and \(\left[ f(x) - \hat{f}(x) \right]^2\) is the \textbf{reducible}
error, because it depends on the model (changing the model \(f\) might
reduce this error), and \(\text{var}(\epsilon)\) is the
\textbf{irreducible} error.

\subsubsection{Local Regression
(Smoothing)}\label{local-regression-smoothing}

Typically we have few if any data points with \(X = 4\) exactly. So we
cannot compute \(E(Y |X = x)\)! Idea: Relax the definition and let \[
 \hat{f}(x)=  Ave(Y|X \in  \cal{N}(x)),
 \] where \(\cal{N} (x)\) is some neighborhood of \(x\), see
Figure~\ref{fig-0203a}.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0203a.png}

}

\caption{\label{fig-0203a}Relaxing the definition. There is no \(Y\)
value at \(X=4\). Taken from James et al. (2014)}

\end{figure}%

Nearest neighbor averaging can be pretty good for small \(p\), i.e.,
\(p \leq 4\) and large-ish \(N\). We will discuss smoother versions,
such as kernel and spline smoothing later in the course.

\subsection{Curse of Dimensionality and Parametric
Models}\label{curse-of-dimensionality-and-parametric-models}

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0204a.png}

}

\caption{\label{fig-0204a}A 10\% neighborhood in high dimensions need no
longer be local. Left: Values of two variables \(x_1\) and \(x_2\),
uniformly distributed. Form two 10\% neighborhoods: (a) the first is
just involving \(x_1\) ignoring \(x_2\). (b) is the neighborhood in two
dimension. Notice that the radius of the circle is much larger than the
lenght of the interval in one dimension. Right: radius plotted against
fraction of the volume. In 10 dim, you have to break out the interval
\([-1;+1]\) to get 10\% of the data. Taken from James et al. (2014)}

\end{figure}%

Local, e.g., nearest neighbor, methods can be lousy when \(p\) is large.
Reason: \textbf{the curse of dimensionality}, i.e., nearest neighbors
tend to be far away in high dimensions. We need to get a reasonable
fraction of the \(N\) values of \(y_i\) to average to bring the variance
down---e.g., 10\%. A 10\% neighborhood in high dimensions need no longer
be local, so we lose the spirit of estimating \(E(Y |X = x)\) by local
averaging, see Figure~\ref{fig-0204a}. If the curse of dimensionality
does not exist, nearest neighbor models would be perfect prediction
models.

We will use structured (parametric) models to deal with the curse of
dimensionality. The linear model is an important example of a parametric
model: \[
f_L(X) = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p.
\] A linear model is specified in terms of \(p + 1\) parameters \$
\beta\_1, \beta\_2, \ldots, \beta\_p\$. We estimate the parameters by
fitting the model to \emph{training data}. Although it is almost never
correct, a linear model often serves as a good and interpretable
approximation to the unknown true function \(f(X)\).

The linear model is avoiding the curse of dimensionality, because it is
not relying on any local properties. Linear models belong to the class
of \emph{model-based} approaches: they replace the problem of estimating
\(f\) with estimating a fixed set of coefficients \(\beta_i\), with
\(i=1,2, \ldots, p\).

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0301a.png}

}

\caption{\label{fig-0301a}A linear model \(\hat{f}_L\) gives a
reasonable fit. Taken from James et al. (2014)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0302a.png}

}

\caption{\label{fig-0302a}A quadratic model \(\hat{f}_Q\) fits slightly
better. Taken from James et al. (2014)}

\end{figure}%

A linear model \[
\hat{f}_L(X) = \hat{\beta}_0 + \hat{\beta}_1 X
\] gives a reasonable fit, see Figure~\ref{fig-0301a}. A quadratic model
\[
\hat{f}_Q(X) = \hat{\beta}_0 + \hat{\beta}_1 X + \hat{\beta}_2 X^2
\] gives a slightly improved fit, see Figure~\ref{fig-0302a}.

Figure~\ref{fig-0203} shows a simulated example. Red points are
simulated values for income from the model \[
income = f(education, seniority) + \epsilon
\] \(f\) is the blue surface.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0203.png}

}

\caption{\label{fig-0203}The true model. Red points are simulated values
for income from the model, \(f\) is the blue surface. Taken from James
et al. (2014)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0204.png}

}

\caption{\label{fig-0204}Linear regression fit to the simulated data
(red points). Taken from James et al. (2014)}

\end{figure}%

The linear regression model \[
\hat{f}(education, seniority) = \hat{\beta}_0 + \hat{\beta}_1 \times education +
 \hat{\beta}_2 \times seniority
\] captures the important information. But it does not capture
everything. More flexible regression model \[
\hat{f}_S (education, seniority)
\] fit to the simulated data. Here we use a technique called a
\textbf{thin-plate spline} to fit a flexible surface. Even more flexible
spline regression model \[
\hat{f}_S (education, seniority)
\] fit to the simulated data. Here the fitted model makes no errors on
the training data! Also known as \textbf{overfitting}.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0205.png}

}

\caption{\label{fig-0205}Thin-plate spline models
\(\hat{f}_S (education, seniority)\) fitted to the model from
Figure~\ref{fig-0203}. Taken from James et al. (2014)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0206.png}

}

\caption{\label{fig-0206}Thin-plate spline models
\(\hat{f}_S (education, seniority)\) fitted to the model from
Figure~\ref{fig-0203}. The model makes no errors on the training data
(overfitting). Taken from James et al. (2014)}

\end{figure}%

\subsubsection{Trade-offs}\label{trade-offs}

\begin{itemize}
\tightlist
\item
  Prediction accuracy versus interpretability: Linear models are easy to
  interpret; thin-plate splines are not.
\item
  Good fit versus over-fit or under-fit: How do we know when the fit is
  just right?
\item
  Parsimony (Occam's razor) versus black-box: We often prefer a simpler
  model involving fewer variables over a black-box predictor involving
  them all.
\end{itemize}

The trad-offs are visualized in Figure~\ref{fig-0207}.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0207.png}

}

\caption{\label{fig-0207}Interpretability versus flexibility.
Flexibility corresponds with the number of model parameters. Taken from
James et al. (2014)}

\end{figure}%

\subsection{Assessing Model Accuracy and Bias-Variance
Trade-off}\label{assessing-model-accuracy-and-bias-variance-trade-off}

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0303a.png}

}

\caption{\label{fig-0303a}Black curve is truth. Red curve on right is
\(MSETe\), grey curve is \(MSETr\). Orange, blue and green
curves/squares correspond to fits of different flexibility. The dotted
line represents the irreducible error, i.e., \(var(\epsilon)\). Taken
from James et al. (2014)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0210.png}

}

\caption{\label{fig-0210}Here, the truth is smoother. Black curve is
truth. Red curve on right is \(MSETe\), grey curve is \(MSETr\). Orange,
blue and green curves/squares correspond to fits of different
flexibility. The dotted line represents the irreducible error, i.e.,
\(var(\epsilon)\). Taken from James et al. (2014)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0211.png}

}

\caption{\label{fig-0211}Here the truth is wiggly and the noise is low,
so the more flexible fits do the best. Black curve is truth. Red curve
on right is \(MSETe\), grey curve is \(MSETr\). Orange, blue and green
curves/squares correspond to fits of different flexibility. The dotted
line represents the irreducible error, i.e., \(var(\epsilon)\). Taken
from James et al. (2014)}

\end{figure}%

Suppose we fit a model \(f(x)\) to some training data
\(Tr = \{x_i, y_i \}^N_1\), and we wish to see how well it performs. We
could compute the average squared prediction error over \(Tr\): \[
MSE_{Tr} = Ave_{i \in Tr}[y_i - \hat{f}(x_i)]^2.
\] This may be biased toward more overfit models. Instead we should, if
possible, compute it using fresh \textbf{test data}
\(Te== \{x_i, y_i \}^N_1\): \[
MSE_{Te} = Ave_{i \in Te}[y_i - \hat{f}(x_i)]^2.
\] The red curve, which illustrated the test error, can be estimated by
holding out some data to get the test-data set.

\subsubsection{Bias-Variance Trade-off}\label{bias-variance-trade-off}

Suppose we have fit a model \(f(x)\) to some training data \(Tr\), and
let \((x_0, y_0)\) be a test observation drawn from the population. If
the true model is \[
Y = f(X) + \epsilon  \qquad \text{ with } f(x) = E(Y|X=x),
\] then \begin{equation}\phantomsection\label{eq-biasvariance}{
E \left( y_0 - \hat{f}(x_0) \right)^2 = \text{var} (\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + \text{var}(\epsilon).
}\end{equation}

Here, \(\text{var}(\epsilon)\) is the irreducible error. The reducible
error consists of two components:

\begin{itemize}
\tightlist
\item
  \(\text{var} (\hat{f}(x_0))\) is the variance that comes from
  different training sets. Different training sets result in different
  functions \(\hat{f}\).
\item
  \(Bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0)\).
\end{itemize}

The expectation averages over the variability of \(y_0\) as well as the
variability in \(Tr\). Note that \[
Bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0).
\] Typically as the flexibility of \(\hat{f}\) increases, its variance
increases (because the fits differ from training set to trainig set),
and its bias decreases. So choosing the flexibility based on average
test error amounts to a bias-variance trade-off, see
Figure~\ref{fig-0212}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0212.png}

}

\caption{\label{fig-0212}Bias-variance trade-off for the three examples.
Taken from James et al. (2014)}

\end{figure}%

If we add the two components (reducible and irreducible error), we get
the MSE in Figure~\ref{fig-0212} as can be seen in
Equation~\ref{eq-biasvariance}.

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/EuBBz3bI-aA?si=7MVv_J1HbzMSQS4K}{Machine
  Learning Fundamentals: Bias and Variance}
\end{itemize}

\section{Multiple Regression}\label{multiple-regression-1}

\section{R-squared}\label{r-squared}

\subsection{R-Squared in Simple Linear
Regression}\label{r-squared-in-simple-linear-regression}

In simple linear regression, the relationship between the independent
variable \(X\) and the dependent variable \(Y\) is modeled using the
equation:

\[
Y = \beta_0 + \beta_1 X + \epsilon
\]

Here, \(\beta_0\) is the intercept, \(\beta_1\) is the slope or
regression coefficient, and \(\epsilon\) is the error term.

\begin{definition}[R-Squared
(\(R^2\))\index{R-squared}]\protect\hypertarget{def-r-squared}{}\label{def-r-squared}

\(R^2\) is a measure of how well the regression model explains the
variance in the dependent variable. It is calculated as the square of
the correlation coefficient (\(r\)) between the actual values \(Y\) and
the predicted values \(\hat{Y}\) from the regression model. It ranges
from 0 to 1, where:

\begin{itemize}
\tightlist
\item
  1 indicates that the regression predictions perfectly fit the data.
\item
  0 indicates that the model does not explain any of the variability in
  the target data around its mean.
\end{itemize}

\end{definition}

In simple linear regression, where there is one independent variable
\(X\) and one dependent variable \(Y\), the R-squared (\(R^2\)) is the
square of the Pearson correlation coefficient (\(r\)) between the
observed values of the dependent variable and the values predicted by
the regression model. That is, in simple linear regression, we have\\
\[
R^2 = r^2.
\]

This equivalence holds specifically for simple linear regression due to
the direct relationship between the linear fit and the correlation of
two variables. In multiple linear regression, while \(R^2\) still
represents the proportion of variance explained by the model, it is not
simply the square of a single correlation coefficient as it involves
multiple predictors.

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/2AQKmw14mHM}{R-squared, Clearly
  Explained}
\end{itemize}

\section{Assessing Confounding Effects in Multiple
Regression}\label{assessing-confounding-effects-in-multiple-regression}

Confounding is a bias introduced by the imbalanced distribution of
extraneous risk factors among comparison groups (Wang 2007).
\texttt{spotpython} provides tools for assessing confounding effects in
multiple regression models.

\begin{example}[Assessing Confounding Effects in Multiple Regression
with
\texttt{spotpython}]\protect\hypertarget{exm-confounding}{}\label{exm-confounding}

Consider the following data generation function \texttt{generate\_data}
and the \texttt{fit\_ols\_model} function to fit an ordinary least
squares (OLS) regression model.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ statsmodels.formula.api }\ImportTok{as}\NormalTok{ smf}

\KeywordTok{def}\NormalTok{ generate\_data(n\_samples}\OperatorTok{=}\DecValTok{100}\NormalTok{, b0}\OperatorTok{=}\DecValTok{0}\NormalTok{, b1}\OperatorTok{={-}}\DecValTok{1}\NormalTok{, b2}\OperatorTok{=}\DecValTok{0}\NormalTok{, b3}\OperatorTok{=}\DecValTok{10}\NormalTok{, b12}\OperatorTok{=}\DecValTok{0}\NormalTok{, b13}\OperatorTok{=}\DecValTok{0}\NormalTok{, b23}\OperatorTok{=}\DecValTok{0}\NormalTok{, b123}\OperatorTok{=}\DecValTok{0}\NormalTok{, noise\_std}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ pd.DataFrame:}
    \CommentTok{"""}
\CommentTok{    Generate data for the linear formula y \textasciitilde{} b0 + b1*x1 + b2*x2 + b3*x3 + b12*x1*x2 + b13*x1*x3 + b23*x2*x3 + b123*x1*x2*x3.}

\CommentTok{    Args:}
\CommentTok{        n\_samples (int): Number of samples to generate.}
\CommentTok{        b0 (float): Coefficient for the intercept.}
\CommentTok{        b1 (float): Coefficient for x1.}
\CommentTok{        b2 (float): Coefficient for x2.}
\CommentTok{        b3 (float): Coefficient for x3.}
\CommentTok{        b12 (float): Coefficient for the interaction term x1*x2.}
\CommentTok{        b13 (float): Coefficient for the interaction term x1*x3.}
\CommentTok{        b23 (float): Coefficient for the interaction term x2*x3.}
\CommentTok{        b123 (float): Coefficient for the interaction term x1*x2*x3.}
\CommentTok{        noise\_std (float): Standard deviation of the Gaussian noise added to y.}

\CommentTok{    Returns:}
\CommentTok{        pd.DataFrame: A DataFrame containing the generated data with columns [\textquotesingle{}x1\textquotesingle{}, \textquotesingle{}x2\textquotesingle{}, \textquotesingle{}x3\textquotesingle{}, \textquotesingle{}y\textquotesingle{}].}
\CommentTok{    """}
\NormalTok{    np.random.seed(}\DecValTok{42}\NormalTok{)  }\CommentTok{\# For reproducibility}
\NormalTok{    x1 }\OperatorTok{=}\NormalTok{ np.random.uniform(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, n\_samples)}
\NormalTok{    x2 }\OperatorTok{=}\NormalTok{ np.random.uniform(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, n\_samples)}
\NormalTok{    x3 }\OperatorTok{=}\NormalTok{ np.random.uniform(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, n\_samples)}
    
\NormalTok{    y }\OperatorTok{=}\NormalTok{ (b0 }\OperatorTok{+}\NormalTok{ b1}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+}\NormalTok{ b2}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{+}\NormalTok{ b3}\OperatorTok{*}\NormalTok{x3 }\OperatorTok{+}\NormalTok{ b12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{+}\NormalTok{ b13}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x3 }\OperatorTok{+}\NormalTok{ b23}\OperatorTok{*}\NormalTok{x2}\OperatorTok{*}\NormalTok{x3 }\OperatorTok{+}\NormalTok{ b123}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}\OperatorTok{*}\NormalTok{x3 }\OperatorTok{+}
\NormalTok{         np.random.normal(}\DecValTok{0}\NormalTok{, noise\_std, n\_samples))}
    
\NormalTok{    data }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{: y, }\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{: x1, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{: x2, }\StringTok{\textquotesingle{}x3\textquotesingle{}}\NormalTok{: x3\})}
    \ControlFlowTok{return}\NormalTok{ data}

\KeywordTok{def}\NormalTok{ fit\_ols\_model(formula, data) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{dict}\NormalTok{:}
    \CommentTok{"""}
\CommentTok{    Fit an OLS model using the given formula and data, and print the results.}

\CommentTok{    Args:}
\CommentTok{        formula (str): The formula for the OLS model.}
\CommentTok{        data (pd.DataFrame): The data frame containing the variables.}

\CommentTok{    Returns:}
\CommentTok{        dict: A dictionary containing the p{-}values, estimates, confidence intervals, and AIC value.}
\CommentTok{    """}
\NormalTok{    mod\_0 }\OperatorTok{=}\NormalTok{ smf.ols(formula}\OperatorTok{=}\NormalTok{formula, data}\OperatorTok{=}\NormalTok{data).fit()}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ mod\_0.pvalues.iloc[}\DecValTok{1}\NormalTok{]}
\NormalTok{    estimate }\OperatorTok{=}\NormalTok{ mod\_0.params.iloc[}\DecValTok{1}\NormalTok{]}
\NormalTok{    conf\_int }\OperatorTok{=}\NormalTok{ mod\_0.conf\_int().iloc[}\DecValTok{1}\NormalTok{]}
\NormalTok{    aic\_value }\OperatorTok{=}\NormalTok{ mod\_0.aic}

    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p{-}values: }\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"estimate: }\SpecialCharTok{\{}\NormalTok{estimate}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"conf\_int: }\SpecialCharTok{\{}\NormalTok{conf\_int}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"aic: }\SpecialCharTok{\{}\NormalTok{aic\_value}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

These functions can be used to generate data and fit an OLS model. Here
we use the model \[
y = f(x_1, x_2, x_3) + \epsilon =  x_1 + 10 x_3 + \epsilon.
\] We set up the basic model \(y_0 = f_0(x_1)\) and analyze how the
model fit changes when adding \(x_2\) and \(x_3\) to the model. If the
\(p\)-values are decreasing by adding a variable, this indicates that
the variable is relevant for the model. Similiarly, if the \(p\)-values
are increasing by removing a variable, this indicates that the variable
is not relevant for the model.

\phantomsection\label{data_generation}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ generate\_data(b0}\OperatorTok{=}\DecValTok{0}\NormalTok{, b1}\OperatorTok{=}\DecValTok{1}\NormalTok{, b2}\OperatorTok{=}\DecValTok{0}\NormalTok{, b3}\OperatorTok{=}\DecValTok{10}\NormalTok{, b12}\OperatorTok{=}\DecValTok{0}\NormalTok{, b13}\OperatorTok{=}\DecValTok{0}\NormalTok{, b23}\OperatorTok{=}\DecValTok{0}\NormalTok{, b123}\OperatorTok{=}\DecValTok{0}\NormalTok{, noise\_std}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{fit\_ols\_model(}\StringTok{"y \textasciitilde{} x1"}\NormalTok{, data)}
\NormalTok{fit\_ols\_model(}\StringTok{"y \textasciitilde{} x1 + x2"}\NormalTok{, data)}
\NormalTok{fit\_ols\_model(}\StringTok{"y \textasciitilde{} x1 + x3"}\NormalTok{, data)}
\NormalTok{fit\_ols\_model(}\StringTok{"y \textasciitilde{} x1 + x2 + x3"}\NormalTok{, data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
p-values: 0.34343741859526267
estimate: 1.025306391110114
conf_int: 0   -1.111963
1    3.162575
Name: x1, dtype: float64
aic: 517.6397392012537
p-values: 0.3637511850778461
estimate: 0.9810502049698089
conf_int: 0   -1.152698
1    3.114798
Name: x1, dtype: float64
aic: 518.1426513151566
p-values: 4.9467606744218404e-05
estimate: 1.4077923469421165
conf_int: 0    0.750106
1    2.065479
Name: x1, dtype: float64
aic: 282.73524524532
p-values: 4.849840959643538e-05
estimate: 1.4159292625696247
conf_int: 0    0.755494
1    2.076364
Name: x1, dtype: float64
aic: 284.34665447613634
\end{verbatim}

The function \texttt{fit\_all\_lm()} simplifies this procedure. It can
be used to fit all possible linear models with the given data and print
the results in a systematic way for various combinations of variables.

\phantomsection\label{fit_all_lm_example}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.stats }\ImportTok{import}\NormalTok{ fit\_all\_lm, plot\_coeff\_vs\_pvals, plot\_coeff\_vs\_pvals\_by\_included}
\NormalTok{res }\OperatorTok{=}\NormalTok{ fit\_all\_lm(}\StringTok{"y \textasciitilde{} x1"}\NormalTok{, [}\StringTok{"x2"}\NormalTok{, }\StringTok{"x3"}\NormalTok{], data)}
\BuiltInTok{print}\NormalTok{(res[}\StringTok{"estimate"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The basic model is: y ~ x1
The following features will be used for fitting the basic model: Index(['x3', 'x2', 'y', 'x1'], dtype='object')
p-values: 0.34343741859526267
estimate: 1.025306391110114
conf_int: 0   -1.111963
1    3.162575
Name: x1, dtype: float64
aic: 517.6397392012537
Combinations: [('x2',), ('x3',), ('x2', 'x3')]
  variables  estimate  conf_low  conf_high         p         aic    n
0     basic  1.025306 -1.111963   3.162575  0.343437  517.639739  100
1        x2  0.981050 -1.152698   3.114798  0.363751  518.142651  100
2        x3  1.407792  0.750106   2.065479  0.000049  282.735245  100
3    x2, x3  1.415929  0.755494   2.076364  0.000048  284.346654  100
\end{verbatim}

Interpreting the results, we can see that the \(p\)-values decrease when
adding \(x_3\) (as well as both \(x_2\) and \$x\_3) to the model,
indicating that \(x_3\) is relevant for the model. Adding only \(x_2\)
does not significantly improve the model fit.

In addition to the textural output, the function
\texttt{plot\_coeff\_vs\_pvals\_by\_included()} can be used to visualize
the coefficients and p-values of the fitted models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_coeff\_vs\_pvals\_by\_included(res)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{100_ddmo_regression_files/figure-pdf/fig-plot-coeff-vs-pvals-output-1.pdf}}

}

\caption{\label{fig-plot-coeff-vs-pvals}Coefficients vs.~p-values for
different models. The right plot indicates that x3 should be included in
the model, whereas the left plot shows that x2 is not relevant.}

\end{figure}%

Figure~\ref{fig-plot-coeff-vs-pvals} shows the coefficients and p-values
for different models. Because \(y\) depends on \(x1\) and \(x3\), the
p-value much smaller if \(x3\) is included in the model as can be seen
in the right plot in Figure~\ref{fig-plot-coeff-vs-pvals}. The left plot
shows that including \(x2\) in the model does not significantly improve
the model fit.

\end{example}

\section{Cross-Validation}\label{cross-validation-2}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/fSytzGwwBVw?si=a8U5yCIEhwAw4AyU}{Machine
  Learning Fundamentals: Cross Validation}
\end{itemize}

\chapter{Classification}\label{classification}

In classification we have a qualitative response variable.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0218a.png}

}

\caption{\label{fig-0218a}Classification. Taken from James et al.
(2014)}

\end{figure}%

Here the response variable \(Y\) is qualitative, e.g., email is one of
\(\cal{C} = (spam, ham)\), where ham is good email, digit class is one
of \(\cal{C} = \{ 0, 1, \ldots, 9 \}\). Our goals are to:

\begin{itemize}
\tightlist
\item
  Build a classifier \(C(X)\) that assigns a class label from
  \(\cal{C}\) to a future unlabeled observation \(X\).
\item
  Assess the uncertainty in each classification
\item
  Understand the roles of the different predictors among
  \(X = (X_1,X_2, \ldots, X_p)\).
\end{itemize}

Simulation example depicted in@fig-0218a. \(Y\) takes two values, zero
and one, and \(X\) has only one value. Big sample: each single vertical
bar indicates an occurrance of a zero (orange) or one (blue) as a
function of the \(X\)s. Black curve generated the data: it is the
probability of generating a one. For high values of \(X\), the
probability of ones is increasing. What is an ideal classifier \(C(X)\)?

Suppose the \(K\) elements in \(\cal{C}\) are numbered
\(1,2,\ldots, K\). Let \[
p_k(x) = Pr(Y = k|X = x), k = 1,2,\ldots,K.
\]

These are the \textbf{conditional class probabilities} at \(x\);
e.g.~see little barplot at \(x = 5\). Then the \textbf{Bayes optimal
classifier} at \(x\) is \[
C(x) = j \qquad \text{ if }  p_j(x) = \max \{p_1(x),p_2(x),\ldots, p_K(x)\}.
\] At \(x=5\) there is an 80\% probability of one, and an 20\%
probability of a zero. So, we classify this point to the class with the
highest probability, the majority class.

Nearest-neighbor averaging can be used as before. This is illustrated in
Fig.\textasciitilde{}\ref{fig:0219a}. Here, we consider 100 points only.
Nearest-neighbor averaging also breaks down as dimension grows. However,
the impact on \(\hat{C}(x)\) is less than on \(\hat{p}_k (x)\),
\(k = 1, \ldots, K\).

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/0219a.png}

}

\caption{\label{fig-0219a}Classification. Taken from James et al.
(2014)}

\end{figure}%

\section{Classification: Some
Details}\label{classification-some-details}

Average number of errors made to measure the performance. Typically we
measure the performance of \(\hat{C}(x)\) using the
\textbf{misclassification error rate}: \[
Err_{Te} = Ave_{i\in Te} I[y_i \neq \hat{C} (x_i) ].
\] The Bayes classifier (using the true \(p_k(x)\)) has smallest error
(in the population).

\section{k-Nearest Neighbor
Classification}\label{k-nearest-neighbor-classification}

Consider k-nearest neighbors in two dimensions. Orange and blue dots
label the true class memberships of the underlying points in the 2-dim
plane. Dotted line is the decision boundary, that is the contour with
equal probability for both classes.

Nearest-neighbor averaging in 2-dim. At any given point we want to
classify, we spread out a little neighborhood, say \(K=10\) points from
the neighborhood and calulated the percentage of blue and orange. We
assign the color with the highest probability to this point. If this is
done for every point in the plane, we obtain the solid black curve as
the esitmated decsion boundary.

We can use \(K=1\). This is the \textbf{nearest-neighbor classifier}.
The decision boundary is piecewise linear. Islands occur. Approximation
is rather noisy.

\(K=100\) leads to a smooth decision boundary. But gets uninteresting.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0213.png}

}

\caption{\label{fig-0213}K-nearest neighbors in two dimensions. Taken
from James et al. (2014)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0215.png}

}

\caption{\label{fig-0215}K-nearest neighbors in two dimensions. Taken
from James et al. (2014)}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0216.png}

}

\caption{\label{fig-0216}K-nearest neighbors in two dimensions. Taken
from James et al. (2014)}

\end{figure}%

\(K\) large means higher bias, so \(1/K\) is chosen, because we go from
low to high complexity on the \(x\)-error, see Figure~\ref{fig-0217}.
Horizontal dotted line is the base error.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/0217.png}

}

\caption{\label{fig-0217}K-nearest neighbors classification error. Taken
from James et al. (2014)}

\end{figure}%

\subsection{Minkowski Distance}\label{minkowski-distance}

The Minkowski distance of order \(p\) (where \(p\) is an integer)
between two points
\(X=(x_1,x_2,\ldots,x_n)\text{ and }Y=(y_1,y_2,\ldots,y_n) \in \mathbb{R}^n\)
is defined as: \[
D \left( X,Y \right) = \left( \sum_{i=1}^n |x_i-y_i|^p \right)^\frac{1}{p}.
\]

\begin{itemize}
\tightlist
\item
  Video:
  \href{https://youtu.be/HVXime0nQeI?si=wTGGkn_6vIshrTk0}{StatQuest:
  K-nearest neighbors, Clearly Explained}
\end{itemize}

\section{Decision and Classification
Trees}\label{decision-and-classification-trees}

\subsection{Decision Trees}\label{decision-trees}

\subsubsection{\texorpdfstring{\href{https://youtu.be/_L39rN6gz7Y?si=KtY-CsLGeAbIJN-f}{Decision
and Classification Trees, Clearly
Explained}}{Decision and Classification Trees, Clearly Explained}}\label{decision-and-classification-trees-clearly-explained}

\subsubsection{\texorpdfstring{\href{https://youtu.be/wpNl-JwwplA?si=R7qiQ4rVzsrW1GAI}{StatQuest:
Decision Trees, Part 2 - Feature Selection and Missing
Data}}{StatQuest: Decision Trees, Part 2 - Feature Selection and Missing Data}}\label{statquest-decision-trees-part-2---feature-selection-and-missing-data}

\subsection{Regression Trees}\label{regression-trees}

\subsubsection{\texorpdfstring{\href{https://youtu.be/g9c66TUylZ4?si=aXOCqkDl-fGAFRx2}{Regression
Trees, Clearly
Explained!!!}}{Regression Trees, Clearly Explained!!!}}\label{regression-trees-clearly-explained}

\subsubsection{\texorpdfstring{\href{https://youtu.be/D0efHEJsfHo?si=OKizIPtcrWDOSCRF}{How
to Prune Regression Trees, Clearly
Explained!!!}}{How to Prune Regression Trees, Clearly Explained!!!}}\label{how-to-prune-regression-trees-clearly-explained}

\section{The Confusion Matrix}\label{the-confusion-matrix}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/Kdsp6soqA7o?si=pOEUeyk1Crt9heg1}{Machine
  Learning Fundamentals: The Confusion Matrix}
\end{itemize}

\subsection{Sensitivity and
Specificity}\label{sensitivity-and-specificity}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/vP06aMoz4v8?si=9O6FfcKtOWSdx84t}{Machine
  Learning Fundamentals: Sensitivity and Specificity}
\end{itemize}

\section{Naive Bayes}\label{naive-bayes}

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/O2L2Uv9pdDA?si=CTRhu0XXwTZuxxwS}{Naive
  Bayes, Clearly Explained!!!}
\end{itemize}

\section{Gaussian Naive Bayes}\label{gaussian-naive-bayes}

\begin{itemize}
\tightlist
\item
  Video:
  \href{https://youtu.be/H3EjCKtlVog?si=cXWTWaQ1cw5wbFXr}{Gaussian Naive
  Bayes, Clearly Explained!!!}
\end{itemize}

\chapter{Clustering}\label{clustering}

\section{DBSCAN}\label{dbscan}

\begin{itemize}
\tightlist
\item
  Video:
  \href{https://youtu.be/RDZUdRSDOok?si=C7SzTAQC8BmD8AZy}{Clustering
  with DBSCAN, Clearly Explained!!!}
\end{itemize}

\section{k-Means Clustering}\label{k-means-clustering}

The \(k\)-means algorithm is an unsupervised learning algorithm that has
a loose relationship to the \(k\)-nearest neighbor classifier. The
\(k\)-means algorithm works as follows:

\begin{itemize}
\tightlist
\item
  Step 1: Randomly choose \(k\) centers. Assign points to cluster.
\item
  Step 2: Determine the distances of each data point to the centroids
  and re-assign each point to the closest cluster centroid based upon
  minimum distance
\item
  Step 3: Calculate cluster centroids again
\item
  Step 4: Repeat steps 2 and 3 until we reach global optima where no
  improvements are possible and no switching of data points from one
  cluster to other.
\end{itemize}

The basic principle of the \(k\)-means algorithm is illustrated in
Figure~\ref{fig-kmeans1}, Figure~\ref{fig-kmeans2},
Figure~\ref{fig-kmeans3}, and Figure~\ref{fig-kmeans4}.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/kmeans1.png}

}

\caption{\label{fig-kmeans1}k-means algorithm. Step 1. Randomly choose
\(k\) centers. Assign points to cluster. \(k\) initial \lq means\rq (in
this case \(k=3\)) are randomly generated within the data domain (shown
in color). Attribution: I, Weston.pace, CC BY-SA 3.0
\url{http://creativecommons.org/licenses/by-sa/3.0/}, via Wikimedia
Commons}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/kmeans2.png}

}

\caption{\label{fig-kmeans2}k-means algorithm. Step 2. \(k\) clusters
are created by associating every observation with the nearest mean. The
partitions here represent the Voronoi diagram generated by the means.
Attribution: I, Weston.pace, CC BY-SA 3.0
\url{http://creativecommons.org/licenses/by-sa/3.0/}, via Wikimedia
Commons}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/kmeans3.png}

}

\caption{\label{fig-kmeans3}k-means algorithm. Step 3. The centroid of
each of the \(k\) clusters becomes the new mean. Attribution: I,
Weston.pace, CC BY-SA 3.0
\url{http://creativecommons.org/licenses/by-sa/3.0/}, via Wikimedia
Commons}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/kmeans4.png}

}

\caption{\label{fig-kmeans4}k-means algorithm. Step 4. Steps 2 and 3 are
repeated until convergence has been reached. Attribution: I,
Weston.pace, CC BY-SA 3.0
\url{http://creativecommons.org/licenses/by-sa/3.0/}, via Wikimedia
Commons}

\end{figure}%

\begin{itemize}
\tightlist
\item
  Video: \href{https://youtu.be/4b5d3muPQmA?si=O9-s32Kw676wXCQF}{K-means
  clustering}
\end{itemize}

\section{DDMO-Additional Videos}\label{ddmo-additional-videos}

\begin{itemize}
\tightlist
\item
  \href{https://youtu.be/ARfXDSkQf1Y?si=E4TjFoloRjbQYbzQ}{Odds and
  Log(Odds), Clearly Explained!!!}
\item
  \href{https://youtu.be/589nCGeWG1w?si=YN9vO0HQlnll1wb6}{One-Hot,
  Label, Target and K-Fold Target Encoding, Clearly Explained!!!}
\item
  \href{https://youtu.be/p3T-_LMrvBc?si=3Jcjueue1otXzS1r}{Maximum
  Likelihood for the Exponential Distribution, Clearly Explained!!!}
\item
  \href{https://youtu.be/4jRBRDbJemM?si=hJXxjRV7_Ib2ckVe}{ROC and AUC,
  Clearly Explained!}
\item
  \href{https://youtu.be/YtebGVx-Fxw?si=xbMzfX2oqAsE6MGK}{Entropy (for
  data science) Clearly Explained!!!}
\item
  \href{https://youtu.be/q90UDEgYqeI?si=teRC5oYkHcXXgUSU}{Classification
  Trees in Python from Start to Finish}: Long live video!
\end{itemize}

\section{DDMO-Exercises}\label{ddmo-exercises}

\begin{exercise}[Smaller
Bins]\protect\hypertarget{exr-small-bins}{}\label{exr-small-bins}

What happens when we use smaller bins in a histogram?

\end{exercise}

\begin{exercise}[Density
Curve]\protect\hypertarget{exr-curve}{}\label{exr-curve}

Why plot a curve to approximate a histogram?

\end{exercise}

\begin{exercise}[TwoSDQuestion]\protect\hypertarget{exr-2SD}{}\label{exr-2SD}

How many samples are plus/minus two SD around the mean?

\end{exercise}

\begin{exercise}[OneSDQuestion]\protect\hypertarget{exr-2SD1}{}\label{exr-2SD1}

How many samples are plus/minus one SD around the mean?

\end{exercise}

\begin{exercise}[ThreeSDQuestion]\protect\hypertarget{exr-2SD2}{}\label{exr-2SD2}

How many samples are plus/minus three SD around the mean?

\end{exercise}

\begin{exercise}[DataRangeQuestion]\protect\hypertarget{exr-2SD3}{}\label{exr-2SD3}

You have a mean at 100 and a SD of 10. Where are 95\% of the data?

\end{exercise}

\begin{exercise}[PeakHeightQuestion]\protect\hypertarget{exr-2SD4}{}\label{exr-2SD4}

If the peak is very high, is the SD low or high?

\end{exercise}

\begin{exercise}[ProbabilityQuestion]\protect\hypertarget{exr-POP1}{}\label{exr-POP1}

If we have a certain curve and want to calculate the probability of
values equal to 20 if the mean is 20.

\end{exercise}

\begin{exercise}[MeanDifferenceQuestion]\protect\hypertarget{exr-CAL1}{}\label{exr-CAL1}

The difference between \(\mu\) and x-bar?

\end{exercise}

\begin{exercise}[EstimateMeanQuestion]\protect\hypertarget{exr-CAL2}{}\label{exr-CAL2}

How do you calculate the sample mean?

\end{exercise}

\begin{exercise}[SigmaSquaredQuestion]\protect\hypertarget{exr-CAL3}{}\label{exr-CAL3}

What is sigma squared?

\end{exercise}

\begin{exercise}[EstimatedSDQuestion]\protect\hypertarget{exr-CAL4}{}\label{exr-CAL4}

What is the formula for the estimated standard deviation?

\end{exercise}

\begin{exercise}[VarianceDifferenceQuestion]\protect\hypertarget{exr-CAL5}{}\label{exr-CAL5}

Difference between the variance and the estimated variance?

\end{exercise}

\begin{exercise}[ModelBenefitsQuestion]\protect\hypertarget{exr-MAT1}{}\label{exr-MAT1}

What are the benefits of using models?

\end{exercise}

\begin{exercise}[SampleDefinitionQuestion]\protect\hypertarget{exr-SAM1}{}\label{exr-SAM1}

What is a sample in statistics?

\end{exercise}

\begin{exercise}[RejectHypothesisQuestion]\protect\hypertarget{exr-Hyp1}{}\label{exr-Hyp1}

What does it mean to reject a hypothesis?

\end{exercise}

\begin{exercise}[NullHypothesisQuestion]\protect\hypertarget{exr-Hyp2}{}\label{exr-Hyp2}

What is a null hypothesis?

\end{exercise}

\begin{exercise}[BetterDrugQuestion]\protect\hypertarget{exr-Hyp3}{}\label{exr-Hyp3}

How can you show that you have found a better drug?

\end{exercise}

\begin{exercise}[PValueIntroductionQuestion]\protect\hypertarget{exr-PVal1}{}\label{exr-PVal1}

What is the reason for introducing the p-value?

\end{exercise}

\begin{exercise}[PValueRangeQuestion]\protect\hypertarget{exr-PVal2}{}\label{exr-PVal2}

Is there any range for p-values? Can it be negative?

\end{exercise}

\begin{exercise}[PValueRangeQuestion]\protect\hypertarget{exr-PVal3}{}\label{exr-PVal3}

Is there any range for p-values? Can it be negative?

\end{exercise}

\begin{exercise}[TypicalPValueQuestion]\protect\hypertarget{exr-PVal4}{}\label{exr-PVal4}

What are typical values of the p-value and what does it mean? 5\%?

\end{exercise}

\begin{exercise}[FalsePositiveQuestion]\protect\hypertarget{exr-PVal5}{}\label{exr-PVal5}

What is a false-positive?

\end{exercise}

\begin{exercise}[CalculatePValueQuestion]\protect\hypertarget{exr-Calc1}{}\label{exr-Calc1}

How to calculate p-value?

\end{exercise}

\begin{exercise}[SDCalculationQuestion]\protect\hypertarget{exr-Calc2}{}\label{exr-Calc2}

What is the SD if the mean is 155 and in the range from 142 - 169 there
are 95\% of the data?

\end{exercise}

\begin{exercise}[SidedPValueQuestion]\protect\hypertarget{exr-Calc3}{}\label{exr-Calc3}

When do we need the two-sided p-value and when the one-sided?

\end{exercise}

\begin{exercise}[CoinTestQuestion]\protect\hypertarget{exr-Calc4}{}\label{exr-Calc4}

Test a coin with Tail-Head-Head. What is the p-value?

\end{exercise}

\begin{exercise}[BorderPValueQuestion]\protect\hypertarget{exr-Calc5}{}\label{exr-Calc5}

If you get exactly the 0.05 border value, can you reject?

\end{exercise}

\begin{exercise}[OneSidedPValueCautionQuestion]\protect\hypertarget{exr-Calc6}{}\label{exr-Calc6}

Why should you be careful with a one-sided p-test?

\end{exercise}

\begin{exercise}[BinomialDistributionQuestion]\protect\hypertarget{exr-Calc7}{}\label{exr-Calc7}

What is the binomial distribution?

\end{exercise}

\begin{exercise}[PHackingWaysQuestion]\protect\hypertarget{exr-Hack1}{}\label{exr-Hack1}

Name two typical ways of p-hacking.

\end{exercise}

\begin{exercise}[AvoidPHackingQuestion]\protect\hypertarget{exr-Hack2}{}\label{exr-Hack2}

How can p-hacking be avoided?

\end{exercise}

\begin{exercise}[MultipleTestingProblemQuestion]\protect\hypertarget{exr-Hack3}{}\label{exr-Hack3}

What is the multiple testing problem?

\end{exercise}

\subsubsection{Covariance}\label{covariance-2}

\begin{exercise}[CovarianceDefinitionQuestion]\protect\hypertarget{exr-Cov1}{}\label{exr-Cov1}

What is covariance?

\end{exercise}

\begin{exercise}[CovarianceMeaningQuestion]\protect\hypertarget{exr-Cov2}{}\label{exr-Cov2}

What is the meaning of covariance?

\end{exercise}

\begin{exercise}[CovarianceVarianceRelationshipQuestion]\protect\hypertarget{exr-Cov3}{}\label{exr-Cov3}

What is the relationship between covariance and variance?

\end{exercise}

\begin{exercise}[HighCovarianceQuestion]\protect\hypertarget{exr-Cov4}{}\label{exr-Cov4}

If covariance is high, is there a strong relationship?

\end{exercise}

\begin{exercise}[ZeroCovarianceQuestion]\protect\hypertarget{exr-Cov5}{}\label{exr-Cov5}

What if the covariance is zero?

\end{exercise}

\begin{exercise}[NegativeCovarianceQuestion]\protect\hypertarget{exr-Cov6}{}\label{exr-Cov6}

Can covariance be negative?

\end{exercise}

\begin{exercise}[NegativeVarianceQuestion]\protect\hypertarget{exr-Cov7}{}\label{exr-Cov7}

Can variance be negative?

\end{exercise}

\begin{exercise}[CorrelationValueQuestion]\protect\hypertarget{exr-Corr1}{}\label{exr-Corr1}

What do you do if the correlation value is 10?

\end{exercise}

\begin{exercise}[CorrelationRangeQuestion]\protect\hypertarget{exr-Corr2}{}\label{exr-Corr2}

What is the possible range of correlation values?

\end{exercise}

\begin{exercise}[CorrelationFormulaQuestion]\protect\hypertarget{exr-Corr3}{}\label{exr-Corr3}

What is the formula for correlation?

\end{exercise}

\begin{exercise}[UnderstandingStatisticalPower]\protect\hypertarget{exr-StatPow1}{}\label{exr-StatPow1}

What is the definition of power in a statistical test?

\end{exercise}

\begin{exercise}[DistributionEffectOnPower]\protect\hypertarget{exr-StatPow2}{}\label{exr-StatPow2}

What is the implication for power analysis if the samples come from the
same distribution?

\end{exercise}

\begin{exercise}[IncreasingPower]\protect\hypertarget{exr-StatPow3}{}\label{exr-StatPow3}

How can you increase the power if the distributions are very similar?

\end{exercise}

\begin{exercise}[PreventingPHacking]\protect\hypertarget{exr-StatPow4}{}\label{exr-StatPow4}

What should be done to avoid p-hacking when the distributions are close
to each other?

\end{exercise}

\begin{exercise}[SampleSizeAndPower]\protect\hypertarget{exr-StatPow5}{}\label{exr-StatPow5}

If there is overlap and the sample size is small, will the power be high
or low?

\end{exercise}

\begin{exercise}[FactorsAffectingPower]\protect\hypertarget{exr-PowAn1}{}\label{exr-PowAn1}

Which are the two main factors that affect power?

\end{exercise}

\begin{exercise}[PurposeOfPowerAnalysis]\protect\hypertarget{exr-PowAn2}{}\label{exr-PowAn2}

What does power analysis tell us?

\end{exercise}

\begin{exercise}[ExperimentRisks]\protect\hypertarget{exr-PowAn3}{}\label{exr-PowAn3}

What are the two risks faced when performing an experiment?

\end{exercise}

\begin{exercise}[PerformingPowerAnalysis]\protect\hypertarget{exr-PowAn4}{}\label{exr-PowAn4}

How do you perform a power analysis?

\end{exercise}

\begin{exercise}[CentralLimitTheoremExplanation]\protect\hypertarget{exr-CenLi1}{}\label{exr-CenLi1}

What does the Central Limit Theorem state?

\end{exercise}

\begin{exercise}[MedianInBoxplot]\protect\hypertarget{exr-BoxPlo1}{}\label{exr-BoxPlo1}

What is represented by the middle line in a boxplot?

\end{exercise}

\begin{exercise}[BoxContentInBoxplot]\protect\hypertarget{exr-BoxPlo2}{}\label{exr-BoxPlo2}

What does the box in a boxplot represent?

\end{exercise}

\begin{exercise}[RSquaredDefinition]\protect\hypertarget{exr-RSqu1}{}\label{exr-RSqu1}

What is R-squared? Show the formula.

\end{exercise}

\begin{exercise}[NegativeRSquared]\protect\hypertarget{exr-RSqu2}{}\label{exr-RSqu2}

Can the R-squared value be negative?

\end{exercise}

\begin{exercise}[RSquaredCalculation]\protect\hypertarget{exr-RSqu3}{}\label{exr-RSqu3}

Perform a calculation involving R-squared.

\end{exercise}

\begin{exercise}[LeastSquaresMeaning]\protect\hypertarget{exr-FitLin1}{}\label{exr-FitLin1}

What is the meaning of the least squares method?

\end{exercise}

\begin{exercise}[RegressionVsClassification]\protect\hypertarget{exr-ML1}{}\label{exr-ML1}

What is the difference between regression and classification?

\end{exercise}

\begin{exercise}[LikelihoodConcept]\protect\hypertarget{exr-MaxLike1}{}\label{exr-MaxLike1}

What is the idea of likelihood?

\end{exercise}

\begin{exercise}[ProbabilityVsLikelihood]\protect\hypertarget{exr-Prob1}{}\label{exr-Prob1}

What is the difference between probability and likelihood?

\end{exercise}

\begin{exercise}[TrainVsTestData]\protect\hypertarget{exr-CroVal1}{}\label{exr-CroVal1}

What is the difference between training and testing data?

\end{exercise}

\begin{exercise}[SingleValidationIssue]\protect\hypertarget{exr-CroVal2}{}\label{exr-CroVal2}

What is the problem if you validate the model only once?

\end{exercise}

\begin{exercise}[FoldDefinition]\protect\hypertarget{exr-CroVal3}{}\label{exr-CroVal3}

What is a fold in cross-validation?

\end{exercise}

\begin{exercise}[LeaveOneOutValidation]\protect\hypertarget{exr-CroVal4}{}\label{exr-CroVal4}

What is leave-one-out cross-validation?

\end{exercise}

\begin{exercise}[DrawingConfusionMatrix]\protect\hypertarget{exr-ConMat1}{}\label{exr-ConMat1}

Draw the confusion matrix.

\end{exercise}

\begin{exercise}[SensitivitySpecificityCalculation1]\protect\hypertarget{exr-SenSpe1}{}\label{exr-SenSpe1}

Calculate the sensitivity and specificity for a given confusion matrix.

\end{exercise}

\begin{exercise}[SensitivitySpecificityCalculation2]\protect\hypertarget{exr-SenSpe2}{}\label{exr-SenSpe2}

Calculate the sensitivity and specificity for a given confusion matrix.

\end{exercise}

\begin{exercise}[BiasAndVariance]\protect\hypertarget{exr-MalLea1}{}\label{exr-MalLea1}

What are bias and variance?

\end{exercise}

\begin{exercise}[MutualInformationExample]\protect\hypertarget{exr-MutInf1}{}\label{exr-MutInf1}

Provide an example and calculate if mutual information is high or low.

\end{exercise}

\begin{exercise}[WhatIsPCA]\protect\hypertarget{exr-PCA1}{}\label{exr-PCA1}

What is PCA?

\end{exercise}

\begin{exercise}[ScreePlotExplanation]\protect\hypertarget{exr-PCA2}{}\label{exr-PCA2}

What is a scree plot?

\end{exercise}

\begin{exercise}[LeastSquaresInPCA]\protect\hypertarget{exr-PCA3}{}\label{exr-PCA3}

Does PCA use least squares?

\end{exercise}

\begin{exercise}[PCASteps]\protect\hypertarget{exr-PCA4}{}\label{exr-PCA4}

Which steps are performed by PCA?

\end{exercise}

\begin{exercise}[EigenvaluePC1]\protect\hypertarget{exr-PCA5}{}\label{exr-PCA5}

What is the eigenvalue of the first principal component?

\end{exercise}

\begin{exercise}[DifferencesBetweenPoints]\protect\hypertarget{exr-PCA6}{}\label{exr-PCA6}

Are the differences between red and yellow the same as the differences
between red and blue points?

\end{exercise}

\begin{exercise}[ScalingInPCA]\protect\hypertarget{exr-PCA7}{}\label{exr-PCA7}

How to scale data in PCA?

\end{exercise}

\begin{exercise}[DetermineNumberOfComponents]\protect\hypertarget{exr-PCA8}{}\label{exr-PCA8}

How to determine the number of principal components?

\end{exercise}

\begin{exercise}[LimitingNumberOfComponents]\protect\hypertarget{exr-PCA9}{}\label{exr-PCA9}

How is the number of principal components limited?

\end{exercise}

\begin{exercise}[WhyUseTSNE]\protect\hypertarget{exr-tSNE1}{}\label{exr-tSNE1}

Why use t-SNE?

\end{exercise}

\begin{exercise}[MainIdeaOfTSNE]\protect\hypertarget{exr-tSNE2}{}\label{exr-tSNE2}

What is the main idea of t-SNE?

\end{exercise}

\begin{exercise}[BasicConceptOfTSNE]\protect\hypertarget{exr-tSNE3}{}\label{exr-tSNE3}

What is the basic concept of t-SNE?

\end{exercise}

\begin{exercise}[TSNESteps]\protect\hypertarget{exr-tSNE4}{}\label{exr-tSNE4}

What are the steps in t-SNE?

\end{exercise}

\begin{exercise}[HowKMeansWorks]\protect\hypertarget{exr-KMeans1}{}\label{exr-KMeans1}

How does K-means clustering work?

\end{exercise}

\begin{exercise}[QualityOfClusters]\protect\hypertarget{exr-KMeans2}{}\label{exr-KMeans2}

How can the quality of the resulting clusters be calculated?

\end{exercise}

\begin{exercise}[IncreasingK]\protect\hypertarget{exr-KMeans3}{}\label{exr-KMeans3}

Why is it not a good idea to increase k too much?

\end{exercise}

\begin{exercise}[CorePointInDBSCAN]\protect\hypertarget{exr-DBSCAN1}{}\label{exr-DBSCAN1}

What is a core point in DBSCAN?

\end{exercise}

\begin{exercise}[AddingVsExtending]\protect\hypertarget{exr-DBSCAN2}{}\label{exr-DBSCAN2}

What is the difference between adding and extending in DBSCAN?

\end{exercise}

\begin{exercise}[OutliersInDBSCAN]\protect\hypertarget{exr-DBSCAN3}{}\label{exr-DBSCAN3}

What are outliers in DBSCAN?

\end{exercise}

\begin{exercise}[AdvantagesAndDisadvantagesOfK]\protect\hypertarget{exr-KNN1}{}\label{exr-KNN1}

What are the advantages and disadvantages of k = 1 and k = 100 in
K-nearest neighbors?

\end{exercise}

\begin{exercise}[NaiveBayesFormula]\protect\hypertarget{exr-NaiveBayes1}{}\label{exr-NaiveBayes1}

What is the formula for Naive Bayes?

\end{exercise}

\begin{exercise}[CalculateProbabilities]\protect\hypertarget{exr-NaiveBayes2}{}\label{exr-NaiveBayes2}

Calculate the probabilities for a given example using Naive Bayes.

\end{exercise}

\begin{exercise}[UnderflowProblem]\protect\hypertarget{exr-GaussianNB1}{}\label{exr-GaussianNB1}

Why is underflow a problem in Gaussian Naive Bayes?

\end{exercise}

\begin{exercise}[Tree
Usage]\protect\hypertarget{exr-Tree1}{}\label{exr-Tree1}

For what can we use trees?

\end{exercise}

\begin{exercise}[Tree
Usage]\protect\hypertarget{exr-DTree1}{}\label{exr-DTree1}

Based on a shown tree graph:

\begin{itemize}
\tightlist
\item
  How can you use this tree?
\item
  What is the root node?
\item
  What are branches and internal nodes?
\item
  What are the leafs?
\item
  Are the leafs pure or impure?
\item
  Which of the leafs is more impure?
\end{itemize}

\end{exercise}

\begin{exercise}[Tree Feature
Importance]\protect\hypertarget{exr-DTree2}{}\label{exr-DTree2}

Is the most or least important feature on top?

\end{exercise}

\begin{exercise}[Tree Feature
Imputation]\protect\hypertarget{exr-DTree3}{}\label{exr-DTree3}

How can you fill a gap/missing data?

\end{exercise}

\begin{refsolution}[Tree Feature Imputation]
\leavevmode

\begin{itemize}
\tightlist
\item
  Mean
\item
  Median
\item
  Comparing to column with high correlation
\end{itemize}

\label{sol-DTree3}

\end{refsolution}

\begin{exercise}[Regression Tree
Limitations]\protect\hypertarget{exr-RTree1}{}\label{exr-RTree1}

What are limitations?

\end{exercise}

\begin{exercise}[Regression Tree
Score]\protect\hypertarget{exr-RTree2}{}\label{exr-RTree2}

How is the tree score calculated?

\end{exercise}

\begin{exercise}[Regression Tree Alpha Value
Small]\protect\hypertarget{exr-RTree3}{}\label{exr-RTree3}

What can we say about the tree if the alpha value is small?

\end{exercise}

\begin{exercise}[Regression Tree Increase Alpha
Value]\protect\hypertarget{exr-RTree4}{}\label{exr-RTree4}

What happens if you increase alpha?

\end{exercise}

\begin{exercise}[Regression Tree
Pruning]\protect\hypertarget{exr-RTree5}{}\label{exr-RTree5}

What is the meaning of pruning?

\end{exercise}

\part{Machine Learning and AI}

\chapter{Machine Learning and Artificial
Intelligence}\label{machine-learning-and-artificial-intelligence}

\section{Jupyter Notebooks}\label{jupyter-notebooks}

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook version of this file can be found here:
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/mlai.ipynb}{malai.ipynb}
\end{itemize}

\section{Videos}\label{videos}

\subsection{June, 11th 2024}\label{june-11th-2024}

\begin{itemize}
\tightlist
\item
  \href{https://youtu.be/zxagGtF9MeU?si=4klFloENih3Pw7Ix}{Happy
  Halloween (Neural Networks Are Not Scary)}
\item
  \href{https://youtu.be/CqOfi41LfDw?si=tGfuObKzWonsNLZ1}{The Essential
  Main Ideas of Neural Networks}
\end{itemize}

\subsection{June, 18th 2024}\label{june-18th-2024}

\begin{itemize}
\item
  \href{https://youtu.be/wl1myxrtQHQ?si=jcGIAhXkBLUvSqeV}{The Chain
  Rule}
\item
  \href{https://youtu.be/sDv4f4s2SB8?si=V3XzPVbJNsbZSbNw}{Gradient
  Descent, Step-by-Step}
\item
  \href{https://youtu.be/IN2XmBhILt4?si=Ldx6rk6mPplQjZZv}{Neural
  Networks Pt. 2: Backpropagation Main Ideas}
\end{itemize}

\subsubsection{Gradient Descent}\label{gradient-descent-3}

\begin{exercise}[GradDescStepSize]\protect\hypertarget{exr-GradDesc1}{}\label{exr-GradDesc1}

How is the step size calculated?

\end{exercise}

\begin{exercise}[GradDescIntercept]\protect\hypertarget{exr-GradDesc2}{}\label{exr-GradDesc2}

How to calculate the new intercept?

\end{exercise}

\begin{exercise}[GradDescIntercept]\protect\hypertarget{exr-GradDesc3}{}\label{exr-GradDesc3}

When does the gradient descend stop?

\end{exercise}

\subsubsection{Backpropagation}\label{backpropagation}

\begin{exercise}[ChainRuleAndGradientDescent]\protect\hypertarget{exr-BacPro1}{}\label{exr-BacPro1}

What are the key components involved in backpropagation?

\end{exercise}

\begin{exercise}[BackpropagationNaming]\protect\hypertarget{exr-BacPro2}{}\label{exr-BacPro2}

Why is it called backpropagation?

\end{exercise}

\subsubsection{ReLU}\label{relu}

\begin{exercise}[Graph
ReLU]\protect\hypertarget{exr-Relu1}{}\label{exr-Relu1}

Draw the graph of a ReLU function.

\end{exercise}

\begin{itemize}
\item
  \href{https://youtu.be/iyn2zdALii8?si=yBsgPec1R1O55f9q}{Backpropagation
  Details Pt. 1: Optimizing 3 parameters simultaneously.}
\item
  \href{https://youtu.be/GKZoOHXGcLo?si=Ypv_EDEMMC--8Flj}{Backpropagation
  Details Pt. 2: Going bonkers with The Chain Rule}
\item
  \href{https://youtu.be/68BZ5f7P94E?si=3hPUkdicWLwFzOGZ}{Neural
  Networks Pt. 3: ReLU In Action!!!}
\item
  \href{https://youtu.be/83LYR-1IcjA?si=kePw0yRCj-A6MsOH}{Neural
  Networks Pt. 4: Multiple Inputs and Outputs}
\item
  \href{https://youtu.be/KpKog-L9veg?si=gqXLSbOxwJwYs0hu}{Neural
  Networks Part 5: ArgMax and SoftMax}
\item
  \href{https://youtu.be/L35fFDpwIM4?si=Q-oglIUJb8wVO9nd}{Tensors for
  Neural Networks, Clearly Explained!!!}
\item
  \href{https://youtu.be/ZTt9gsGcdDo?si=sKDLZ8nbj4vVi9aj}{Essential
  Matrix Algebra for Neural Networks, Clearly Explained!!!}
\item
  \href{https://youtu.be/FHdlXe1bSe4?si=Yh5gfWsnjDd2WqxN}{The StatQuest
  Introduction to PyTorch}
\end{itemize}

\subsubsection{PyTorch Links}\label{pytorch-links}

\begin{itemize}
\tightlist
\item
  \href{https://lightning.ai/lightning-ai/studios/statquest-introduction-to-coding-neural-networks-with-pytorch?view=public&section=all}{StatQuest:
  Introduction to Coding Neural Networks with PyTorch}
\item
  \href{https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html}{ML-AI
  Pytorch Introduction}
\end{itemize}

\subsection{June, 25th 2024}\label{june-25th-2024}

\subsection{CNNs}\label{cnns}

\subsubsection{\texorpdfstring{\href{https://youtu.be/HGwBXDKFk9I?si=3yBfpZQ0dXw7s6j9}{Neural
Networks Part 8: Image Classification with Convolutional Neural Networks
(CNNs)}}{Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)}}\label{neural-networks-part-8-image-classification-with-convolutional-neural-networks-cnns}

\begin{exercise}[CNNImageRecognition]\protect\hypertarget{exr-CNN1}{}\label{exr-CNN1}

Why are classical neural networks poor at image recognition?

\end{exercise}

\begin{exercise}[CNNFiltersInitialization]\protect\hypertarget{exr-CNN2}{}\label{exr-CNN2}

How are the filter values in CNNs initialized and optimized?

\end{exercise}

\begin{exercise}[CNNFilterInitialization]\protect\hypertarget{exr-CNN3}{}\label{exr-CNN3}

How are the filter values determined in Convolutional Neural Networks
(CNNs)?

\end{exercise}

\begin{exercise}[GenNNStockPrediction]\protect\hypertarget{exr-CNN4}{}\label{exr-CNN4}

What is a limitation of using classical neural networks for stock market
prediction?

\end{exercise}

\subsection{RNN}\label{rnn}

\subsubsection{\texorpdfstring{\href{https://youtu.be/AsNTP8Kwu80?si=-JiRYXyOpu-gnhnk}{Recurrent
Neural Networks (RNNs), Clearly
Explained!!!}}{Recurrent Neural Networks (RNNs), Clearly Explained!!!}}\label{recurrent-neural-networks-rnns-clearly-explained}

\begin{exercise}[RNNUnrolling]\protect\hypertarget{exr-RNN1}{}\label{exr-RNN1}

How does the unrolling process work in Recurrent Neural Networks (RNNs)?

\end{exercise}

\begin{exercise}[RNNReliability]\protect\hypertarget{exr-RNN2}{}\label{exr-RNN2}

Why do Recurrent Neural Networks (RNNs) sometimes fail to work reliably?

\end{exercise}

\subsection{LSTM}\label{lstm}

\subsubsection{\texorpdfstring{\href{https://youtu.be/YCzL96nL7j0?si=DphYdoYgx23Twgz6}{Long
Short-Term Memory (LSTM), Clearly
Explained}}{Long Short-Term Memory (LSTM), Clearly Explained}}\label{long-short-term-memory-lstm-clearly-explained}

\begin{exercise}[LSTMSigmoidTanh]\protect\hypertarget{exr-LSTM1}{}\label{exr-LSTM1}

What are the differences between the sigmoid and tanh activation
functions?

\end{exercise}

\begin{exercise}[LSTMSigmoidTanh]\protect\hypertarget{exr-LSTM11}{}\label{exr-LSTM11}

What is the ?

\end{exercise}

\begin{exercise}[LSTMGates]\protect\hypertarget{exr-LSTM2}{}\label{exr-LSTM2}

What are the gates in an LSTM network and their functions?

\end{exercise}

\begin{exercise}[LSTMLongTermInfo]\protect\hypertarget{exr-LSTM3}{}\label{exr-LSTM3}

In which gate is long-term information used in an LSTM network?

\end{exercise}

\begin{exercise}[LSTMUpdateGates]\protect\hypertarget{exr-LSTM4}{}\label{exr-LSTM4}

In which Gates is it updated in an LSTM?

\end{exercise}

\subsection{Pytorch/Lightning}\label{pytorchlightning}

\subsubsection{\texorpdfstring{\href{https://youtu.be/khMzi6xPbuM?si=6aqbmYIIaefKQnWX}{Introduction
to Coding Neural Networks with PyTorch and
Lightning}}{Introduction to Coding Neural Networks with PyTorch and Lightning}}\label{introduction-to-coding-neural-networks-with-pytorch-and-lightning}

\begin{exercise}[PyTorchRequiresGrad]\protect\hypertarget{exr-PyTorch1}{}\label{exr-PyTorch1}

What does \texttt{requires\_grad} mean in PyTorch?

\end{exercise}

\subsection{July, 2nd 2024}\label{july-2nd-2024}

\begin{itemize}
\tightlist
\item
  \href{https://youtu.be/viZrOnJclY0?si=B0gvlx4_ppegZAB-}{Word Embedding
  and Word2Vec, Clearly Explained!!!}
\item
  \href{https://youtu.be/L8HKweZIOmg?si=LzC6wjlC2yE9ZekP}{Sequence-to-Sequence
  (seq2seq) Encoder-Decoder Neural Networks, Clearly Explained!!!}
\item
  \href{https://youtu.be/PSs6nxngL6k?si=jajDsVYk4FQgCgNA}{Attention for
  Neural Networks, Clearly Explained!!!}
\end{itemize}

\subsubsection{Embeddings}\label{embeddings}

\begin{exercise}[NN
Strings]\protect\hypertarget{exr-Embedding1}{}\label{exr-Embedding1}

Can neural networks process strings?

\end{exercise}

\begin{exercise}[Embedding
Definition]\protect\hypertarget{exr-Embedding2}{}\label{exr-Embedding2}

What is the meaning of word embedding?

\end{exercise}

\begin{exercise}[Embedding
Dimensions]\protect\hypertarget{exr-Embedding3}{}\label{exr-Embedding3}

Why do we need high dimension in word embedding?

\end{exercise}

\subsubsection{Sequence to Sequence}\label{sequence-to-sequence}

\begin{exercise}[LSTM]\protect\hypertarget{exr-S2S1}{}\label{exr-S2S1}

Why are LSTMs used?

\end{exercise}

\begin{exercise}[Teacher
Forcing]\protect\hypertarget{exr-S2S2}{}\label{exr-S2S2}

Why is teacher forcing used?

\end{exercise}

\begin{exercise}[Attention]\protect\hypertarget{exr-S2S3}{}\label{exr-S2S3}

What is the idea of attention?

\end{exercise}

\subsection{Additional Lecture (July, 9th
2024)?}\label{additional-lecture-july-9th-2024}

\begin{itemize}
\tightlist
\item
  \href{https://youtu.be/zxQyTK8quyY?si=LGe6J13PJ4s0qKbr}{Transformer
  Neural Networks, ChatGPT's foundation, Clearly Explained!!!}
\item
  \href{https://youtu.be/bQ5BoolX9Ag?si=cojnYPck8CK6NK8p}{Decoder-Only
  Transformers, ChatGPTs specific Transformer, Clearly Explained!!!}
\item
  \href{https://youtu.be/KphmOJnLAdI?si=JwIK3MhmoHxnuI3G}{The matrix
  math behind transformer neural networks, one step at a time!!!}
\item
  \href{https://youtu.be/Qf06XDYXCXI?si=gIKMOQ0xjAxLo_7_}{Word Embedding
  in PyTorch + Lightning}
\end{itemize}

\subsubsection{Transformers}\label{transformers}

\begin{exercise}[ChatGPT]\protect\hypertarget{exr-Transformer1}{}\label{exr-Transformer1}

What kind of transformer does ChatGPT use?

\end{exercise}

\begin{exercise}[Translation]\protect\hypertarget{exr-Transformer2}{}\label{exr-Transformer2}

What kind of NN are used for translation?

\end{exercise}

\begin{exercise}[Difference Encoder-Decoder and Decoder
Only.]\protect\hypertarget{exr-Transformer3}{}\label{exr-Transformer3}

What is the encoder-decoder transformer and the decoder only
transformer?

\end{exercise}

\begin{exercise}[Weights]\protect\hypertarget{exr-Transformer4}{}\label{exr-Transformer4}

How are the weights initialized (a) and trained (b)?

\end{exercise}

\begin{exercise}[Order of
Words]\protect\hypertarget{exr-Transformer5}{}\label{exr-Transformer5}

How is the word order preserved?

\end{exercise}

\begin{exercise}[Relationship Between
Words]\protect\hypertarget{exr-Transformer6}{}\label{exr-Transformer6}

How is the relationship between words modeled?

\end{exercise}

\begin{exercise}[Masked Self
Attention]\protect\hypertarget{exr-Transformer7}{}\label{exr-Transformer7}

What is masked self-attention?

\end{exercise}

\begin{exercise}[Softmax]\protect\hypertarget{exr-Transformer8}{}\label{exr-Transformer8}

Why is Softmax used to calculate percentage of similarities?

\end{exercise}

\begin{exercise}[Softmax
Output]\protect\hypertarget{exr-Transformer9}{}\label{exr-Transformer9}

How is the percentage output of softmax in Transformers used?

\end{exercise}

\begin{exercise}[VÂ´s]\protect\hypertarget{exr-Transformer10}{}\label{exr-Transformer10}

What is done with the scaled VÂ´s that we get for each token so far
(example: ``is'',''what'')?

\end{exercise}

\begin{exercise}[Residual
Connections]\protect\hypertarget{exr-Transformer11}{}\label{exr-Transformer11}

What are residual connections?

\end{exercise}

\begin{exercise}[Generate Known Word in
Sequence]\protect\hypertarget{exr-Transformer12}{}\label{exr-Transformer12}

Why do we want to generate the word in the sequence that comes after
``what'' that we already know? (Example from video)

\end{exercise}

\begin{exercise}[Masked-Self-Attention Values and
Bypass]\protect\hypertarget{exr-Transformer13}{}\label{exr-Transformer13}

How do we use the two values (``masked-self-attention values + bypass'')
which we have for each input? (Example from video: (``What'', ''is'',
''StatQuest''))

\end{exercise}

\subsection{Additional Videos}\label{additional-videos}

\begin{itemize}
\tightlist
\item
  \href{https://youtu.be/M59JElEPgIg?si=KoZGFEZWVc-PclSU}{The SoftMax
  Derivative, Step-by-Step!!!}
\item
  \href{https://youtu.be/6ArSys5qHAU?si=TxyJi22ELyYl0m3L}{Neural
  Networks Part 6: Cross Entropy}
\item
  \href{https://youtu.be/xBEh66V9gZo?si=kUco4zKdH8CNW23k}{Neural
  Networks Part 7: Cross Entropy Derivatives and Backpropagation}
\end{itemize}

\subsection{All Videos in a Playlist}\label{all-videos-in-a-playlist}

\begin{itemize}
\tightlist
\item
  Full Playlist
  \href{https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1}{ML-AI}
\end{itemize}

\section{The StatQuest Introduction to
PyTorch}\label{the-statquest-introduction-to-pytorch}

The following code is taken from
\href{https://lightning.ai/lightning-ai/studios/statquest-introduction-to-coding-neural-networks-with-pytorch?view=public&section=all&tab=files&layout=column&path=cloudspaces\%2F01hf54c4fhjc8wwadsd037kjjm&y=3&x=0}{The
StatQuest Introduction to PyTorch}. Attribution goes to Josh Starmer,
the creator of StatQuest, see
\href{https://lightning.ai/josh-starmer}{Josh Starmer}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch }\CommentTok{\# torch provides basic functions, from setting a random seed (for reproducability) to creating tensors.}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn }\CommentTok{\# torch.nn allows us to create a neural network.}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F }\CommentTok{\# nn.functional give us access to the activation and loss functions.}
\ImportTok{from}\NormalTok{ torch.optim }\ImportTok{import}\NormalTok{ SGD }\CommentTok{\# optim contains many optimizers. Here, we\textquotesingle{}re using SGD, stochastic gradient descent.}

\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt }\CommentTok{\#\# matplotlib allows us to draw graphs.}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns }\CommentTok{\#\# seaborn makes it easier to draw nice{-}looking graphs.}

\OperatorTok{\%}\NormalTok{matplotlib inline}
\end{Highlighting}
\end{Shaded}

Building a neural network in PyTorch means creating a new class with two
methods: init() and forward(). The init() method defines and initializes
all of the parameters that we want to use, and the forward() method
tells PyTorch what should happen during a forward pass through the
neural network.

\subsection{Build a Simple Neural Network in
PyTorch}\label{build-a-simple-neural-network-in-pytorch}

\texttt{\_\_init\_\_()} is the class constructor function, and we use it
to initialize the weights and biases.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# create a neural network class by creating a class that inherits from nn.Module.}
\KeywordTok{class}\NormalTok{ BasicNN(nn.Module):}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{): }\CommentTok{\# \_\_init\_\_() is the class constructor function, and we use it to initialize the weights and biases.}
        
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{() }\CommentTok{\# initialize an instance of the parent class, nn.Model.}
        
        \CommentTok{\#\# Now create the weights and biases that we need for our neural network.}
        \CommentTok{\#\# Each weight or bias is an nn.Parameter, which gives us the option to optimize the parameter by setting}
        \CommentTok{\#\# requires\_grad, which is short for "requires gradient", to True. Since we don\textquotesingle{}t need to optimize any of these}
        \CommentTok{\#\# parameters now, we set requires\_grad=False.}
        \CommentTok{\#\#}
        \CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: Because our neural network is already fit to the data, we will input specific values}
        \CommentTok{\#\# for each weight and bias. In contrast, if we had not already fit the neural network to the data,}
        \CommentTok{\#\# we might start with a random initalization of the weights and biases.}
        \VariableTok{self}\NormalTok{.w00 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{1.7}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.b00 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\OperatorTok{{-}}\FloatTok{0.85}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.w01 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\OperatorTok{{-}}\FloatTok{40.8}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        
        \VariableTok{self}\NormalTok{.w10 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{12.6}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.b10 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{0.0}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.w11 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{2.7}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

        \VariableTok{self}\NormalTok{.final\_bias }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\OperatorTok{{-}}\FloatTok{16.}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        
        
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, }\BuiltInTok{input}\NormalTok{): }\CommentTok{\#\# forward() takes an input value and runs it though the neural network }
                              \CommentTok{\#\# illustrated at the top of this notebook. }
        
        \CommentTok{\#\# the next three lines implement the top of the neural network (using the top node in the hidden layer).}
\NormalTok{        input\_to\_top\_relu }\OperatorTok{=} \BuiltInTok{input} \OperatorTok{*} \VariableTok{self}\NormalTok{.w00 }\OperatorTok{+} \VariableTok{self}\NormalTok{.b00}
\NormalTok{        top\_relu\_output }\OperatorTok{=}\NormalTok{ F.relu(input\_to\_top\_relu)}
\NormalTok{        scaled\_top\_relu\_output }\OperatorTok{=}\NormalTok{ top\_relu\_output }\OperatorTok{*} \VariableTok{self}\NormalTok{.w01}
        
        \CommentTok{\#\# the next three lines implement the bottom of the neural network (using the bottom node in the hidden layer).}
\NormalTok{        input\_to\_bottom\_relu }\OperatorTok{=} \BuiltInTok{input} \OperatorTok{*} \VariableTok{self}\NormalTok{.w10 }\OperatorTok{+} \VariableTok{self}\NormalTok{.b10}
\NormalTok{        bottom\_relu\_output }\OperatorTok{=}\NormalTok{ F.relu(input\_to\_bottom\_relu)}
\NormalTok{        scaled\_bottom\_relu\_output }\OperatorTok{=}\NormalTok{ bottom\_relu\_output }\OperatorTok{*} \VariableTok{self}\NormalTok{.w11}
        
        \CommentTok{\#\# here, we combine both the top and bottom nodes from the hidden layer with the final bias.}
\NormalTok{        input\_to\_final\_relu }\OperatorTok{=}\NormalTok{ scaled\_top\_relu\_output }\OperatorTok{+}\NormalTok{ scaled\_bottom\_relu\_output }\OperatorTok{+} \VariableTok{self}\NormalTok{.final\_bias}
        
\NormalTok{        output }\OperatorTok{=}\NormalTok{ F.relu(input\_to\_final\_relu)}
    
        \ControlFlowTok{return}\NormalTok{ output }\CommentTok{\# output is the predicted effectiveness for a drug dose.}
\end{Highlighting}
\end{Shaded}

Once we have created the class that defines the neural network, we can
create an actual neural network and print out its parameters, just to
make sure things are what we expect.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# create the neural network. }
\NormalTok{model }\OperatorTok{=}\NormalTok{ BasicNN()}

\CommentTok{\#\# print out the name and value for each parameter}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
    \BuiltInTok{print}\NormalTok{(name, param.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
w00 tensor(1.7000)
b00 tensor(-0.8500)
w01 tensor(-40.8000)
w10 tensor(12.6000)
b10 tensor(0.)
w11 tensor(2.7000)
final_bias tensor(-16.)
\end{verbatim}

\subsection{Use the Neural Network and Graph the
Output}\label{use-the-neural-network-and-graph-the-output}

Now that we have a neural network, we can use it on a variety of doses
to determine which will be effective. Then we can make a graph of these
data, and this graph should match the green bent shape fit to the
training data that's shown at the top of this document. So, let's start
by making a sequence of input doses\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# now create the different doses we want to run through the neural network.}
\CommentTok{\#\# torch.linspace() creates the sequence of numbers between, and including, 0 and 1.}
\NormalTok{input\_doses }\OperatorTok{=}\NormalTok{ torch.linspace(start}\OperatorTok{=}\DecValTok{0}\NormalTok{, end}\OperatorTok{=}\DecValTok{1}\NormalTok{, steps}\OperatorTok{=}\DecValTok{11}\NormalTok{)}

\CommentTok{\# now print out the doses to make sure they are what we expect...}
\NormalTok{input\_doses}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,
        0.9000, 1.0000])
\end{verbatim}

Now that we have input\_doses, let's run them through the neural network
and graph the output\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# create the neural network. }
\NormalTok{model }\OperatorTok{=}\NormalTok{ BasicNN() }

\CommentTok{\#\# now run the different doses through the neural network.}
\NormalTok{output\_values }\OperatorTok{=}\NormalTok{ model(input\_doses)}

\CommentTok{\#\# Now draw a graph that shows the effectiveness for each dose.}
\CommentTok{\#\#}
\CommentTok{\#\# First, set the style for seaborn so that the graph looks cool.}
\NormalTok{sns.}\BuiltInTok{set}\NormalTok{(style}\OperatorTok{=}\StringTok{"whitegrid"}\NormalTok{)}

\CommentTok{\#\# create the graph (you might not see it at this point, but you will after we save it as a PDF).}
\NormalTok{sns.lineplot(x}\OperatorTok{=}\NormalTok{input\_doses, }
\NormalTok{     y}\OperatorTok{=}\NormalTok{output\_values, }
\NormalTok{     color}\OperatorTok{=}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }
\NormalTok{     linewidth}\OperatorTok{=}\FloatTok{2.5}\NormalTok{)}

\CommentTok{\#\# now label the y{-} and x{-}axes.}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Effectiveness\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Dose\textquotesingle{}}\NormalTok{)}

\CommentTok{\#\# optionally, save the graph as a PDF.}
\CommentTok{\# plt.savefig(\textquotesingle{}BasicNN.pdf\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 0, 'Dose')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{200_mlai_files/figure-pdf/cell-6-output-2.pdf}}

The graph shows that the neural network fits the training data. In other
words, so far, we don't have any bugs in our code.

\subsection{Optimize (Train) a Parameter in the Neural Network and Graph
the
Output}\label{optimize-train-a-parameter-in-the-neural-network-and-graph-the-output}

Now that we know how to create and use a simple neural network, and we
can graph the output relative to the input, let's see how to train a
neural network. The first thing we need to do is tell PyTorch which
parameter (or parameters) we want to train, and we do that by setting
requiresgrad=True. In this example, we'll train finalbias.

Now we create a neural network by creating a class that inherits from
nn.Module.

NOTE: This code is the same as before, except we changed the class name
to BasicNN\_train and we modified final\_bias in two ways:

\begin{verbatim}
1) we set the value of the tensor to 0, and
2) we set "requires_grad=True".
\end{verbatim}

Now let's graph the output of BasicNN\_train, which is currently not
optimized, and compare it to the graph we drew earlier of the optimized
neural network.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ BasicNN\_train(nn.Module):}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{): }\CommentTok{\# \_\_init\_\_ is the class constructor function, and we use it to initialize the weights and biases.}
        
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{() }\CommentTok{\# initialize an instance of the parent class, nn.Module.}
        
        \VariableTok{self}\NormalTok{.w00 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{1.7}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.b00 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\OperatorTok{{-}}\FloatTok{0.85}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.w01 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\OperatorTok{{-}}\FloatTok{40.8}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        
        \VariableTok{self}\NormalTok{.w10 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{12.6}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.b10 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{0.0}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.w11 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{2.7}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

        \CommentTok{\#\# we want to modify final\_bias to demonstrate how to optimize it with backpropagation.}
        \CommentTok{\#\# The optimal value for final\_bias is {-}16...}
\CommentTok{\#         self.final\_bias = nn.Parameter(torch.tensor({-}16.), requires\_grad=False)}
        \CommentTok{\#\# ...so we set it to 0 and tell Pytorch that it now needs to calculate the gradient for this parameter.}
        \VariableTok{self}\NormalTok{.final\_bias }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{0.}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{) }
        
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, }\BuiltInTok{input}\NormalTok{):}
        
\NormalTok{        input\_to\_top\_relu }\OperatorTok{=} \BuiltInTok{input} \OperatorTok{*} \VariableTok{self}\NormalTok{.w00 }\OperatorTok{+} \VariableTok{self}\NormalTok{.b00}
\NormalTok{        top\_relu\_output }\OperatorTok{=}\NormalTok{ F.relu(input\_to\_top\_relu)}
\NormalTok{        scaled\_top\_relu\_output }\OperatorTok{=}\NormalTok{ top\_relu\_output }\OperatorTok{*} \VariableTok{self}\NormalTok{.w01}
        
\NormalTok{        input\_to\_bottom\_relu }\OperatorTok{=} \BuiltInTok{input} \OperatorTok{*} \VariableTok{self}\NormalTok{.w10 }\OperatorTok{+} \VariableTok{self}\NormalTok{.b10}
\NormalTok{        bottom\_relu\_output }\OperatorTok{=}\NormalTok{ F.relu(input\_to\_bottom\_relu)}
\NormalTok{        scaled\_bottom\_relu\_output }\OperatorTok{=}\NormalTok{ bottom\_relu\_output }\OperatorTok{*} \VariableTok{self}\NormalTok{.w11}
    
\NormalTok{        input\_to\_final\_relu }\OperatorTok{=}\NormalTok{ scaled\_top\_relu\_output }\OperatorTok{+}\NormalTok{ scaled\_bottom\_relu\_output }\OperatorTok{+} \VariableTok{self}\NormalTok{.final\_bias}
        
\NormalTok{        output }\OperatorTok{=}\NormalTok{ F.relu(input\_to\_final\_relu)}
        
        \ControlFlowTok{return}\NormalTok{ output}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# create the neural network. }
\NormalTok{model }\OperatorTok{=}\NormalTok{ BasicNN\_train() }

\CommentTok{\#\# now run the different doses through the neural network.}
\NormalTok{output\_values }\OperatorTok{=}\NormalTok{ model(input\_doses)}

\CommentTok{\#\# Now draw a graph that shows the effectiveness for each dose.}
\CommentTok{\#\#}
\CommentTok{\#\# set the style for seaborn so that the graph looks cool.}
\NormalTok{sns.}\BuiltInTok{set}\NormalTok{(style}\OperatorTok{=}\StringTok{"whitegrid"}\NormalTok{)}

\CommentTok{\#\# create the graph (you might not see it at this point, but you will after we save it as a PDF).}
\NormalTok{sns.lineplot(x}\OperatorTok{=}\NormalTok{input\_doses, }
\NormalTok{             y}\OperatorTok{=}\NormalTok{output\_values.detach(), }\CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: because final\_bias has a gradident, we call detach() }
                                       \CommentTok{\#\# to return a new tensor that only has the value and not the gradient.}
\NormalTok{             color}\OperatorTok{=}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }
\NormalTok{             linewidth}\OperatorTok{=}\FloatTok{2.5}\NormalTok{)}

\CommentTok{\#\# now label the y{-} and x{-}axes.}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Effectiveness\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Dose\textquotesingle{}}\NormalTok{)}

\CommentTok{\#\# lastly, save the graph as a PDF.}
\CommentTok{\# plt.savefig(\textquotesingle{}BasicNN\_train.pdf\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 0, 'Dose')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{200_mlai_files/figure-pdf/cell-8-output-2.pdf}}

The graph shows that when the dose is 0.5, the output from the
unoptimized neural network is 17, which is wrong, since the output value
should be 1. So, now that we have a parameter we can optimize, let's
create some training data that we can use to optimize it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# create the training data for the neural network.}
\NormalTok{inputs }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{1.}\NormalTok{])}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{1.}\NormalTok{, }\FloatTok{0.}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

..and now let's use that training data to train (or optimize)
final\_bias.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# create the neural network we want to train.}
\NormalTok{model }\OperatorTok{=}\NormalTok{ BasicNN\_train()}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SGD(model.parameters(), lr}\OperatorTok{=}\FloatTok{0.1}\NormalTok{) }\CommentTok{\#\# here we\textquotesingle{}re creating an optimizer to train the neural network.}
                                            \CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: There are a bunch of different ways to optimize a neural network.}
                                            \CommentTok{\#\# In this example, we\textquotesingle{}ll use Stochastic Gradient Descent (SGD). However,}
                                            \CommentTok{\#\# another popular algortihm is Adam (which will be covered in a StatQuest).}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Final bias, before optimization: "} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(model.final\_bias.data) }\OperatorTok{+} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

\CommentTok{\#\# this is the optimization loop. Each time the optimizer sees all of the training data is called an "epoch".}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{100}\NormalTok{):}

    \CommentTok{\#\# we create and initialize total\_loss for each epoch so that we can evaluate how well model fits the}
    \CommentTok{\#\# training data. At first, when the model doesn\textquotesingle{}t fit the training data very well, total\_loss}
    \CommentTok{\#\# will be large. However, as gradient descent improves the fit, total\_loss will get smaller and smaller.}
    \CommentTok{\#\# If total\_loss gets really small, we can decide that the model fits the data well enough and stop}
    \CommentTok{\#\# optimizing the fit. Otherwise, we can just keep optimizing until we reach the maximum number of epochs. }
\NormalTok{    total\_loss }\OperatorTok{=} \DecValTok{0}

    \CommentTok{\#\# this internal loop is where the optimizer sees all of the training data and where we }
    \CommentTok{\#\# calculate the total\_loss for all of the training data.}
    \ControlFlowTok{for}\NormalTok{ iteration }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(inputs)):}

\NormalTok{        input\_i }\OperatorTok{=}\NormalTok{ inputs[iteration] }\CommentTok{\#\# extract a single input value (a single dose)...}
\NormalTok{        label\_i }\OperatorTok{=}\NormalTok{ labels[iteration] }\CommentTok{\#\# ...and its corresponding label (the effectiveness for the dose).}

\NormalTok{        output\_i }\OperatorTok{=}\NormalTok{ model(input\_i) }\CommentTok{\#\# calculate the neural network output for the input (the single dose).}

\NormalTok{        loss }\OperatorTok{=}\NormalTok{ (output\_i }\OperatorTok{{-}}\NormalTok{ label\_i)}\OperatorTok{**}\DecValTok{2} \CommentTok{\#\# calculate the loss for the single value.}
                                       \CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: Because output\_i = model(input\_i), "loss" has a connection to "model"}
                                       \CommentTok{\#\# and the derivative (calculated in the next step) is kept and accumulated}
                                       \CommentTok{\#\# in "model".}

\NormalTok{        loss.backward() }\CommentTok{\# backward() calculates the derivative for that single value and adds it to the previous one.}

\NormalTok{        total\_loss }\OperatorTok{+=} \BuiltInTok{float}\NormalTok{(loss) }\CommentTok{\# accumulate the total loss for this epoch.}


    \ControlFlowTok{if}\NormalTok{ (total\_loss }\OperatorTok{\textless{}} \FloatTok{0.0001}\NormalTok{):}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"Num steps: "} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(epoch))}
        \ControlFlowTok{break}

\NormalTok{    optimizer.step() }\CommentTok{\#\# take a step toward the optimal value.}
\NormalTok{    optimizer.zero\_grad() }\CommentTok{\#\# This zeroes out the gradient stored in "model". }
                          \CommentTok{\#\# Remember, by default, gradients are added to the previous step (the gradients are accumulated),}
                          \CommentTok{\#\# and we took advantage of this process to calculate the derivative one data point at a time.}
                          \CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: "optimizer" has access to "model" because of how it was created with the call }
                          \CommentTok{\#\# (made earlier): optimizer = SGD(model.parameters(), lr=0.1).}
                          \CommentTok{\#\# ALSO }\AlertTok{NOTE}\CommentTok{: Alternatively, we can zero out the gradient with model.zero\_grad().}
    \ControlFlowTok{if}\NormalTok{ epoch }\OperatorTok{\%} \DecValTok{10} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"Step: "} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(epoch) }\OperatorTok{+} \StringTok{" Final Bias: "} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(model.final\_bias.data) }\OperatorTok{+} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    \CommentTok{\#\# now go back to the start of the loop and go through another epoch.}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Total loss: "} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(total\_loss))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Final bias, after optimization: "} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(model.final\_bias.data))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Final bias, before optimization: tensor(0.)

Step: 0 Final Bias: tensor(-3.2020)

Step: 10 Final Bias: tensor(-14.6348)

Step: 20 Final Bias: tensor(-15.8623)

Step: 30 Final Bias: tensor(-15.9941)

Num steps: 34
Total loss: 6.58966600894928e-05
Final bias, after optimization: tensor(-16.0019)
\end{verbatim}

So, if everything worked correctly, the optimizer should have converged
on final\_bias = 16.0019 after 34 steps, or epochs. BAM!

Lastly, let's graph the output from the optimized neural network and see
if it's the same as what we started with. If so, then the optimization
worked.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# run the different doses through the neural network}
\NormalTok{output\_values }\OperatorTok{=}\NormalTok{ model(input\_doses)}

\CommentTok{\#\# set the style for seaborn so that the graph looks cool.}
\NormalTok{sns.}\BuiltInTok{set}\NormalTok{(style}\OperatorTok{=}\StringTok{"whitegrid"}\NormalTok{)}

\CommentTok{\#\# create the graph (you might not see it at this point, but you will after we save it as a PDF).}
\NormalTok{sns.lineplot(x}\OperatorTok{=}\NormalTok{input\_doses, }
\NormalTok{     y}\OperatorTok{=}\NormalTok{output\_values.detach(), }\CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: we call detach() because final\_bias has a gradient}
\NormalTok{     color}\OperatorTok{=}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }
\NormalTok{     linewidth}\OperatorTok{=}\FloatTok{2.5}\NormalTok{)}

\CommentTok{\#\# now label the y{-} and x{-}axes.}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Effectiveness\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Dose\textquotesingle{}}\NormalTok{)}

\CommentTok{\#\# lastly, save the graph as a PDF.}
\CommentTok{\# plt.savefig(\textquotesingle{}BascNN\_optimized.pdf\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 0, 'Dose')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{200_mlai_files/figure-pdf/cell-11-output-2.pdf}}

And we see that the optimized model results in the same graph that we
started with, so the optimization worked as expected.

\section{Build a Long Short-Term Memory unit by hand using PyTorch +
Lightning}\label{build-a-long-short-term-memory-unit-by-hand-using-pytorch-lightning}

The following code is based on
\href{https://www.youtube.com/watch?v=RHGiXPuo_pI&t=23s}{Long Short-Term
Memory with PyTorch + Lightning} and
\href{https://lightning.ai/lightning-ai/studios/statquest-long-short-term-memory-lstm-with-pytorch-lightning?view=public&section=all&tab=files&layout=column&path=cloudspaces\%2F01henpavmdtqndyk17xpzjdbj6&y=3&x=0}{StatQuest:
Long Short-Term Memory (LSTM) with PyTorch + Lightning!!!}. Attribution
goes to Josh Starmer, the creator of StatQuest, see
\href{https://lightning.ai/josh-starmer}{Josh Starmer}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch }\CommentTok{\# torch will allow us to create tensors.}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn }\CommentTok{\# torch.nn allows us to create a neural network.}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F }\CommentTok{\# nn.functional give us access to the activation and loss functions.}
\ImportTok{from}\NormalTok{ torch.optim }\ImportTok{import}\NormalTok{ Adam }\CommentTok{\# optim contains many optimizers. This time we\textquotesingle{}re using Adam}

\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L }\CommentTok{\# lightning has tons of cool tools that make neural networks easier}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ TensorDataset, DataLoader }\CommentTok{\# these are needed for the training data}
\end{Highlighting}
\end{Shaded}

A Long Short-Term Memory (LSTM) unit is a type of neural network, and
that means we need to create a new class. To make it easy to train the
LSTM, this class will inherit from LightningModule and we'll create the
following methods:

\begin{itemize}
\tightlist
\item
  \texttt{init()} to initialize the Weights and Biases and keep track of
  a few other house keeping things.
\item
  \texttt{lstm\_unit()} to do the LSTM math. For example, to calculate
  the percentage of the long-term memory to remember.
\item
  \texttt{forward()} to make a forward pass through the unrolled LSTM.
  In other words forward() calls \texttt{lstm\_unit()} for each data
  point.
\item
  \texttt{configure\_optimizers()} to configure the opimimizer. In the
  past, we have use SGD (Stochastic Gradient Descent), however, in this
  tutorial we'll change things up and use Adam, another popular
  algorithm for optimizing the Weights and Biases.
\item
  \texttt{training\_step()} to pass the training data to forward(),
  calculate the loss and to keep track of the loss values in a log file.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LSTMbyHand(L.LightningModule):}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
\NormalTok{        L.seed\_everything(seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

        \CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: nn.LSTM() uses random values from a uniform distribution to initialize the tensors}
        \CommentTok{\#\# Here we can do it 2 different ways 1) Normal Distribution and 2) Uniform Distribution}
        \CommentTok{\#\# We\textquotesingle{}ll start with the Normal distribution.}
\NormalTok{        mean }\OperatorTok{=}\NormalTok{ torch.tensor(}\FloatTok{0.0}\NormalTok{)}
\NormalTok{        std }\OperatorTok{=}\NormalTok{ torch.tensor(}\FloatTok{1.0}\NormalTok{)}

        \CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: In this case, I\textquotesingle{}m only using the normal distribution for the Weights.}
        \CommentTok{\#\# All Biases are initialized to 0.}
        \CommentTok{\#\#}
        \CommentTok{\#\# These are the Weights and Biases in the first stage, which determines what percentage}
        \CommentTok{\#\# of the long{-}term memory the LSTM unit will remember.}
        \VariableTok{self}\NormalTok{.wlr1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.wlr2 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.blr1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{0.}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

        \CommentTok{\#\# These are the Weights and Biases in the second stage, which determines the new}
        \CommentTok{\#\# potential long{-}term memory and what percentage will be remembered.}
        \VariableTok{self}\NormalTok{.wpr1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.wpr2 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bpr1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{0.}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

        \VariableTok{self}\NormalTok{.wp1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.wp2 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bp1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{0.}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

        \CommentTok{\#\# These are the Weights and Biases in the third stage, which determines the}
        \CommentTok{\#\# new short{-}term memory and what percentage will be sent to the output.}
        \VariableTok{self}\NormalTok{.wo1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.wo2 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.normal(mean}\OperatorTok{=}\NormalTok{mean, std}\OperatorTok{=}\NormalTok{std), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bo1 }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.tensor(}\FloatTok{0.}\NormalTok{), requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

        \CommentTok{\#\# We can also initialize all Weights and Biases using a uniform distribution. This is}
        \CommentTok{\#\# how nn.LSTM() does it.}
\CommentTok{\#         self.wlr1 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.wlr2 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.blr1 = nn.Parameter(torch.rand(1), requires\_grad=True)}

\CommentTok{\#         self.wpr1 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.wpr2 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.bpr1 = nn.Parameter(torch.rand(1), requires\_grad=True)}

\CommentTok{\#         self.wp1 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.wp2 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.bp1 = nn.Parameter(torch.rand(1), requires\_grad=True)}

\CommentTok{\#         self.wo1 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.wo2 = nn.Parameter(torch.rand(1), requires\_grad=True)}
\CommentTok{\#         self.bo1 = nn.Parameter(torch.rand(1), requires\_grad=True)}


    \KeywordTok{def}\NormalTok{ lstm\_unit(}\VariableTok{self}\NormalTok{, input\_value, long\_memory, short\_memory):}
        \CommentTok{\#\# lstm\_unit does the math for a single LSTM unit.}

        \CommentTok{\#\# NOTES:}
        \CommentTok{\#\# long term memory is also called "cell state"}
        \CommentTok{\#\# short term memory is also called "hidden state"}

        \CommentTok{\#\# 1) The first stage determines what percent of the current long{-}term memory}
        \CommentTok{\#\#    should be remembered}
\NormalTok{        long\_remember\_percent }\OperatorTok{=}\NormalTok{ torch.sigmoid((short\_memory }\OperatorTok{*} \VariableTok{self}\NormalTok{.wlr1) }\OperatorTok{+}
\NormalTok{                                              (input\_value }\OperatorTok{*} \VariableTok{self}\NormalTok{.wlr2) }\OperatorTok{+}
                                              \VariableTok{self}\NormalTok{.blr1)}

        \CommentTok{\#\# 2) The second stage creates a new, potential long{-}term memory and determines what}
        \CommentTok{\#\#    percentage of that to add to the current long{-}term memory}
\NormalTok{        potential\_remember\_percent }\OperatorTok{=}\NormalTok{ torch.sigmoid((short\_memory }\OperatorTok{*} \VariableTok{self}\NormalTok{.wpr1) }\OperatorTok{+}
\NormalTok{                                                   (input\_value }\OperatorTok{*} \VariableTok{self}\NormalTok{.wpr2) }\OperatorTok{+}
                                                   \VariableTok{self}\NormalTok{.bpr1)}
\NormalTok{        potential\_memory }\OperatorTok{=}\NormalTok{ torch.tanh((short\_memory }\OperatorTok{*} \VariableTok{self}\NormalTok{.wp1) }\OperatorTok{+}
\NormalTok{                                      (input\_value }\OperatorTok{*} \VariableTok{self}\NormalTok{.wp2) }\OperatorTok{+}
                                      \VariableTok{self}\NormalTok{.bp1)}

        \CommentTok{\#\# Once we have gone through the first two stages, we can update the long{-}term memory}
\NormalTok{        updated\_long\_memory }\OperatorTok{=}\NormalTok{ ((long\_memory }\OperatorTok{*}\NormalTok{ long\_remember\_percent) }\OperatorTok{+}
\NormalTok{                       (potential\_remember\_percent }\OperatorTok{*}\NormalTok{ potential\_memory))}

        \CommentTok{\#\# 3) The third stage creates a new, potential short{-}term memory and determines what}
        \CommentTok{\#\#    percentage of that should be remembered and used as output.}
\NormalTok{        output\_percent }\OperatorTok{=}\NormalTok{ torch.sigmoid((short\_memory }\OperatorTok{*} \VariableTok{self}\NormalTok{.wo1) }\OperatorTok{+}
\NormalTok{                                       (input\_value }\OperatorTok{*} \VariableTok{self}\NormalTok{.wo2) }\OperatorTok{+}
                                       \VariableTok{self}\NormalTok{.bo1)}
\NormalTok{        updated\_short\_memory }\OperatorTok{=}\NormalTok{ torch.tanh(updated\_long\_memory) }\OperatorTok{*}\NormalTok{ output\_percent}

        \CommentTok{\#\# Finally, we return the updated long and short{-}term memories}
        \ControlFlowTok{return}\NormalTok{([updated\_long\_memory, updated\_short\_memory])}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, }\BuiltInTok{input}\NormalTok{):}
        \CommentTok{\#\# forward() unrolls the LSTM for the training data by calling lstm\_unit() for each day of training data}
        \CommentTok{\#\# that we have. forward() also keeps track of the long and short{-}term memories after each day and returns}
        \CommentTok{\#\# the final short{-}term memory, which is the \textquotesingle{}output\textquotesingle{} of the LSTM.}

\NormalTok{        long\_memory }\OperatorTok{=} \DecValTok{0} \CommentTok{\# long term memory is also called "cell state" and indexed with c0, c1, ..., cN}
\NormalTok{        short\_memory }\OperatorTok{=} \DecValTok{0} \CommentTok{\# short term memory is also called "hidden state" and indexed with h0, h1, ..., cN}
\NormalTok{        day1 }\OperatorTok{=} \BuiltInTok{input}\NormalTok{[}\DecValTok{0}\NormalTok{]}
\NormalTok{        day2 }\OperatorTok{=} \BuiltInTok{input}\NormalTok{[}\DecValTok{1}\NormalTok{]}
\NormalTok{        day3 }\OperatorTok{=} \BuiltInTok{input}\NormalTok{[}\DecValTok{2}\NormalTok{]}
\NormalTok{        day4 }\OperatorTok{=} \BuiltInTok{input}\NormalTok{[}\DecValTok{3}\NormalTok{]}

        \CommentTok{\#\# Day 1}
\NormalTok{        long\_memory, short\_memory }\OperatorTok{=} \VariableTok{self}\NormalTok{.lstm\_unit(day1, long\_memory, short\_memory)}

        \CommentTok{\#\# Day 2}
\NormalTok{        long\_memory, short\_memory }\OperatorTok{=} \VariableTok{self}\NormalTok{.lstm\_unit(day2, long\_memory, short\_memory)}

        \CommentTok{\#\# Day 3}
\NormalTok{        long\_memory, short\_memory }\OperatorTok{=} \VariableTok{self}\NormalTok{.lstm\_unit(day3, long\_memory, short\_memory)}

        \CommentTok{\#\# Day 4}
\NormalTok{        long\_memory, short\_memory }\OperatorTok{=} \VariableTok{self}\NormalTok{.lstm\_unit(day4, long\_memory, short\_memory)}

        \CommentTok{\#\#\#\#\# Now return short\_memory, which is the \textquotesingle{}output\textquotesingle{} of the LSTM.}
        \ControlFlowTok{return}\NormalTok{ short\_memory}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{): }\CommentTok{\# this configures the optimizer we want to use for backpropagation.}
        \CommentTok{\# return Adam(self.parameters(), lr=0.1) \# }\AlertTok{NOTE}\CommentTok{: Setting the learning rate to 0.1 trains way faster than}
                                                 \CommentTok{\# using the default learning rate, lr=0.001, which requires a lot more}
                                                 \CommentTok{\# training. However, if we use the default value, we get}
                                                 \CommentTok{\# the exact same Weights and Biases that I used in}
                                                 \CommentTok{\# the LSTM Clearly Explained StatQuest video. So we\textquotesingle{}ll use the}
                                                 \CommentTok{\# default value.}
        \ControlFlowTok{return}\NormalTok{ Adam(}\VariableTok{self}\NormalTok{.parameters())}


    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx): }\CommentTok{\# take a step during gradient descent.}
\NormalTok{        input\_i, label\_i }\OperatorTok{=}\NormalTok{ batch }\CommentTok{\# collect input}
\NormalTok{        output\_i }\OperatorTok{=} \VariableTok{self}\NormalTok{.forward(input\_i[}\DecValTok{0}\NormalTok{]) }\CommentTok{\# run input through the neural network}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ (output\_i }\OperatorTok{{-}}\NormalTok{ label\_i)}\OperatorTok{**}\DecValTok{2} \CommentTok{\#\# loss = sum of squared residual}
        \CommentTok{\# Logging the loss and the predicted values so we can evaluate the training:}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"train\_loss"}\NormalTok{, loss)}
        \CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: Our dataset consists of two sequences of values representing Company A and Company B}
        \CommentTok{\#\# For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,}
        \CommentTok{\#\# the goal is to predict that the value on Day 5 = 1. We use label\_i, the value we want to}
        \CommentTok{\#\# predict, to keep track of which company we just made a prediction for and}
        \CommentTok{\#\# log that output value in a company specific file}
        \ControlFlowTok{if}\NormalTok{ (label\_i }\OperatorTok{==} \DecValTok{0}\NormalTok{):}
            \VariableTok{self}\NormalTok{.log(}\StringTok{"out\_0"}\NormalTok{, output\_i)}
        \ControlFlowTok{else}\NormalTok{:}
            \VariableTok{self}\NormalTok{.log(}\StringTok{"out\_1"}\NormalTok{, output\_i)}
        \ControlFlowTok{return}\NormalTok{ loss}
\end{Highlighting}
\end{Shaded}

Once we have created the class that defines an LSTM, we can use it to
create a model and print out the randomly initialized Weights and
Biases. Then, just for fun, we'll see what those random Weights and
Biases predict for Company A and Company B. If they are good
predictions, then we're done! However, the chances of getting good
predictions from random values is very small.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Create the model object, print out parameters and see how well}
\CommentTok{\#\# the untrained LSTM can make predictions...}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LSTMbyHand() }

\BuiltInTok{print}\NormalTok{(}\StringTok{"Before optimization, the parameters are..."}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
    \BuiltInTok{print}\NormalTok{(name, param.data)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Now let\textquotesingle{}s compare the observed and predicted values..."}\NormalTok{)}
\CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: To make predictions, we pass in the first 4 days worth of stock values }
\CommentTok{\#\# in an array for each company. In this case, the only difference between the}
\CommentTok{\#\# input values for Company A and B occurs on the first day. Company A has 0 and}
\CommentTok{\#\# Company B has 1.}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company A: Observed = 0, Predicted ="}\NormalTok{, }
\NormalTok{      model(torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company B: Observed = 1, Predicted ="}\NormalTok{, }
\NormalTok{      model(torch.tensor([}\FloatTok{1.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Before optimization, the parameters are...
wlr1 tensor(0.3367)
wlr2 tensor(0.1288)
blr1 tensor(0.)
wpr1 tensor(0.2345)
wpr2 tensor(0.2303)
bpr1 tensor(0.)
wp1 tensor(-1.1229)
wp2 tensor(-0.1863)
bp1 tensor(0.)
wo1 tensor(2.2082)
wo2 tensor(-0.6380)
bo1 tensor(0.)

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(-0.0377)
Company B: Observed = 1, Predicted = tensor(-0.0383)
\end{verbatim}

With the unoptimized paramters, the predicted value for Company A,
-0.0377, isn't terrible, since it is relatively close to the observed
value, 0. However, the predicted value for Company B, -0.0383, is
terrible, because it is relatively far from the observed value, 1. So,
that means we need to train the LSTM.

\subsection{Train the LSTM unit and use Lightning and TensorBoard to
evaluate: Part 1 - Getting
Started}\label{train-the-lstm-unit-and-use-lightning-and-tensorboard-to-evaluate-part-1---getting-started}

Since we are using Lightning training, training the LSTM we created by
hand is pretty easy. All we have to do is create the training data and
put it into a DataLoader\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# create the training data for the neural network.}
\NormalTok{inputs }\OperatorTok{=}\NormalTok{ torch.tensor([[}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{], [}\FloatTok{1.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{]])}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{1.}\NormalTok{])}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ TensorDataset(inputs, labels)}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset)}

\CommentTok{\# show the training data}
\ControlFlowTok{for}\NormalTok{ i, (input\_i, label\_i) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(dataloader):}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Training data: "}\NormalTok{, input\_i, label\_i)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Training data:  tensor([[0.0000, 0.5000, 0.2500, 1.0000]]) tensor([0.])
Training data:  tensor([[1.0000, 0.5000, 0.2500, 1.0000]]) tensor([1.])
\end{verbatim}

\ldots and then create a Lightning Trainer, L.Trainer, and fit it to the
training data. NOTE: We are starting with 2000 epochs. This may be
enough to successfully optimize all of the parameters, but it might not.
We'll find out after we compare the predictions to the observed values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(max\_epochs}\OperatorTok{=}\DecValTok{2000}\NormalTok{) }\CommentTok{\# with default learning rate, 0.001 (this tiny learning rate makes learning slow)}
\NormalTok{trainer.fit(model, train\_dataloaders}\OperatorTok{=}\NormalTok{dataloader)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Training: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

Now that we've trained the model with 2000 epochs, we can see how good
the predictions are\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Now let\textquotesingle{}s compare the observed and predicted values..."}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company A: Observed = 0, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company B: Observed = 1, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{1.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(0.4342)
Company B: Observed = 1, Predicted = tensor(0.6171)
\end{verbatim}

Unfortunately, these predictions are terrible. So it seems like we'll
have to do more training. However, it would be awesome if we could be
confident that more training will actually improve the predictions. If
not, we can spare ourselves a lot of time, and potentially money, and
just give up. So, before we dive into more training, let's look at the
loss values and predictions that we saved in log files with TensorBoard.
TensorBoard will graph everything that we logged during training, making
it super easy to see if things are headed in the right direction or not.

To get TensorBoard working:

\begin{itemize}
\tightlist
\item
  First, check to see if the TensorBoard plugin is installed. If it's
  not, install it with the following command: pip install tensorboard
\item
  Next, run the following command:
  \texttt{tensorboard\ -\/-logdir\ lightning\_logs}
\end{itemize}

NOTE: If your graphs look messed up and you see a bunch of different
lines, instead of just one red line per graph, then check where this
notebook is saved for a directory called \texttt{lightning\_logs}.
Delete \texttt{lightning\_logs} and the re-run everything in this
notebook. One source of problems with the graphs is that every time we
train a model, a new batch of log files is created and stored in
lightning\_logs and TensorBoard, by default, will plot all of them. You
can turn off unwanted log files in TensorBoard, and we'll do this later
on in this notebook, but for now, the easiest thing to do is to start
with a clean slate.

Anyway, if we look at the loss (trainloss), we see that it is going
down, which is good, but it still has further to go. When we look at the
predictions for Company A (out0), we see that they started out pretty
good, close to 0, but then got really bad early on in training, shooting
all the way up to 0.5, but are starting to get smaller. In contrast,
when we look at the predictions for Company B (out\_1), we see that they
started out really bad, close to 0, but have been getting better ever
since and look like they could continue to get better if we kept
training.

In summary, the graphs seem to suggest that if we continued training our
model, the predictions would improve. So let's add more epochs to the
training.

\subsection{Optimizing (Training) the Weights and Biases in the LSTM
that we made by hand: Part 2 - Adding More Epochs without Starting
Over}\label{optimizing-training-the-weights-and-biases-in-the-lstm-that-we-made-by-hand-part-2---adding-more-epochs-without-starting-over}

The good news is that because we're using Lightning, we can pick up
where we left off training without having to start over from scratch.
This is because when we train with Lightning, it creates checkpoint
files that keep track of the Weights and Biases as they change. As a
result, all we have to do to pick up where we left off is tell the
Trainer where the checkpoint files are located. This is awesome and will
save us a lot of time since we don't have to retrain the first 2000
epochs. So let's add an additional 1000 epochs to the training.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# First, find where the most recent checkpoint files are stored}
\NormalTok{path\_to\_checkpoint }\OperatorTok{=}\NormalTok{ trainer.checkpoint\_callback.best\_model\_path }\CommentTok{\#\# By default, "best" = "most recent"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The new trainer will start where the last left off, and the check point data is here: "} \OperatorTok{+} 
\NormalTok{      path\_to\_checkpoint }\OperatorTok{+} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

\CommentTok{\#\# Then create a new Lightning Trainer}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(max\_epochs}\OperatorTok{=}\DecValTok{3000}\NormalTok{) }\CommentTok{\# Before, max\_epochs=2000, so, by setting it to 3000, we\textquotesingle{}re adding 1000 more.}
\CommentTok{\#\# And then call fit() using the path to the most recent checkpoint files}
\CommentTok{\#\# so that we can pick up where we left off.}
\NormalTok{trainer.fit(model, train\_dataloaders}\OperatorTok{=}\NormalTok{dataloader, ckpt\_path}\OperatorTok{=}\NormalTok{path\_to\_checkpoint)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The new trainer will start where the last left off, and the check point data is here: /Users/bartz/workspace/Hyperparameter-Tuning-Cookbook/lightning_logs/version_1520/checkpoints/epoch=1999-step=4000.ckpt
\end{verbatim}

\begin{verbatim}
Training: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

Now that we have added 1000 epochs to the training, let's check the
predictions\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Now let\textquotesingle{}s compare the observed and predicted values..."}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company A: Observed = 0, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company B: Observed = 1, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{1.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(0.2708)
Company B: Observed = 1, Predicted = tensor(0.7534)
\end{verbatim}

The blue lines in each graph represents the values we logged during the
extra 1000 epochs. The loss is getting smaller and the predictions for
both companies are improving! Hooray!!! However, because it looks like
there is even more room for improvement, let's add 2000 more epochs to
the training.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# First, find where the most recent checkpoint files are stored}
\NormalTok{path\_to\_checkpoint }\OperatorTok{=}\NormalTok{ trainer.checkpoint\_callback.best\_model\_path }\CommentTok{\#\# By default, "best" = "most recent"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The new trainer will start where the last left off, and the check point data is here: "} \OperatorTok{+} 
\NormalTok{      path\_to\_checkpoint }\OperatorTok{+} \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

\CommentTok{\#\# Then create a new Lightning Trainer}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(max\_epochs}\OperatorTok{=}\DecValTok{5000}\NormalTok{) }\CommentTok{\# Before, max\_epochs=3000, so, by setting it to 5000, we\textquotesingle{}re adding 2000 more.}
\CommentTok{\#\# And then call fit() using the path to the most recent checkpoint files}
\CommentTok{\#\# so that we can pick up where we left off.}
\NormalTok{trainer.fit(model, train\_dataloaders}\OperatorTok{=}\NormalTok{dataloader, ckpt\_path}\OperatorTok{=}\NormalTok{path\_to\_checkpoint)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The new trainer will start where the last left off, and the check point data is here: /Users/bartz/workspace/Hyperparameter-Tuning-Cookbook/lightning_logs/version_1521/checkpoints/epoch=2999-step=6000.ckpt
\end{verbatim}

\begin{verbatim}
Training: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

Now that we have added 2000 more epochs to the training (for a total of
5000 epochs), let's check the predictions.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Now let\textquotesingle{}s compare the observed and predicted values..."}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company A: Observed = 0, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company B: Observed = 1, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{1.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor(0.0022)
Company B: Observed = 1, Predicted = tensor(0.9693)
\end{verbatim}

The prediction for Company A is super close to 0, which is exactly what
we want, and the prediction for Company B is close to 1, which is also
what we want.

The dark red lines show how things changed when we added an additional
2000 epochs to the training, for a total of 5000 epochs. Now we see that
the loss (train\_loss) and the predictions for each company appear to be
tapering off, suggesting that adding more epochs may not improve the
predictions much, so we're done!

Lastly, let's print out the final estimates for the Weights and Biases.
In theory, they should be the same (within rounding error) as what we
used in the StatQuest on Long Short-Term Memory and seen in the diagram
of the LSTM unit at the top of this Jupyter notebook.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"After optimization, the parameters are..."}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
    \BuiltInTok{print}\NormalTok{(name, param.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
After optimization, the parameters are...
wlr1 tensor(2.7043)
wlr2 tensor(1.6307)
blr1 tensor(1.6234)
wpr1 tensor(1.9983)
wpr2 tensor(1.6525)
bpr1 tensor(0.6204)
wp1 tensor(1.4122)
wp2 tensor(0.9393)
bp1 tensor(-0.3217)
wo1 tensor(4.3848)
wo2 tensor(-0.1943)
bo1 tensor(0.5935)
\end{verbatim}

\section{Using and optimzing the PyTorch LSTM,
nn.LSTM()}\label{using-and-optimzing-the-pytorch-lstm-nn.lstm}

Now that we know how to create an LSTM unit by hand, train it, and then
use it to make good predictions, let's learn how to take advantage of
PyTorch's \texttt{nn.LSTM()} function. For the most part, using
nn.LSTM() allows us to simplify the init() function and the forward()
function. The other big difference is that this time, we're not going to
try and recreate the parameter values we used in the StatQuest on Long
Short-Term Memory, and that means we can set the learning rate for the
Adam to 0.1. This will speed up training a lot. Everything else stays
the same.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Instead of coding an LSTM by hand, let\textquotesingle{}s see what we can do with PyTorch\textquotesingle{}s nn.LSTM()}
\KeywordTok{class}\NormalTok{ LightningLSTM(L.LightningModule):}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{): }\CommentTok{\# \_\_init\_\_() is the class constructor function, and we use it to initialize the Weights and Biases.}

        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{() }\CommentTok{\# initialize an instance of the parent class, LightningModule.}

\NormalTok{        L.seed\_everything(seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

        \CommentTok{\#\# input\_size = number of features (or variables) in the data. In our example}
        \CommentTok{\#\#              we only have a single feature (value)}
        \CommentTok{\#\# hidden\_size = this determines the dimension of the output}
        \CommentTok{\#\#               in other words, if we set hidden\_size=1, then we have 1 output node}
        \CommentTok{\#\#               if we set hidden\_size=50, then we hve 50 output nodes (that can then be 50 input}
        \CommentTok{\#\#               nodes to a subsequent fully connected neural network.}
        \VariableTok{self}\NormalTok{.lstm }\OperatorTok{=}\NormalTok{ nn.LSTM(input\_size}\OperatorTok{=}\DecValTok{1}\NormalTok{, hidden\_size}\OperatorTok{=}\DecValTok{1}\NormalTok{)}


    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, }\BuiltInTok{input}\NormalTok{):}
        \CommentTok{\#\# transpose the input vector}
\NormalTok{        input\_trans }\OperatorTok{=} \BuiltInTok{input}\NormalTok{.view(}\BuiltInTok{len}\NormalTok{(}\BuiltInTok{input}\NormalTok{), }\DecValTok{1}\NormalTok{)}

\NormalTok{        lstm\_out, temp }\OperatorTok{=} \VariableTok{self}\NormalTok{.lstm(input\_trans)}

        \CommentTok{\#\# lstm\_out has the short{-}term memories for all inputs. We make our prediction with the last one}
\NormalTok{        prediction }\OperatorTok{=}\NormalTok{ lstm\_out[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
        \ControlFlowTok{return}\NormalTok{ prediction}


    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{): }\CommentTok{\# this configures the optimizer we want to use for backpropagation.}
        \ControlFlowTok{return}\NormalTok{ Adam(}\VariableTok{self}\NormalTok{.parameters(), lr}\OperatorTok{=}\FloatTok{0.1}\NormalTok{) }\CommentTok{\#\# we\textquotesingle{}ll just go ahead and set the learning rate to 0.1}


    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx): }\CommentTok{\# take a step during gradient descent.}
\NormalTok{        input\_i, label\_i }\OperatorTok{=}\NormalTok{ batch }\CommentTok{\# collect input}
\NormalTok{        output\_i }\OperatorTok{=} \VariableTok{self}\NormalTok{.forward(input\_i[}\DecValTok{0}\NormalTok{]) }\CommentTok{\# run input through the neural network}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ (output\_i }\OperatorTok{{-}}\NormalTok{ label\_i)}\OperatorTok{**}\DecValTok{2} \CommentTok{\#\# loss = squared residual}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"train\_loss"}\NormalTok{, loss)}

        \ControlFlowTok{if}\NormalTok{ (label\_i }\OperatorTok{==} \DecValTok{0}\NormalTok{):}
            \VariableTok{self}\NormalTok{.log(}\StringTok{"out\_0"}\NormalTok{, output\_i)}
        \ControlFlowTok{else}\NormalTok{:}
            \VariableTok{self}\NormalTok{.log(}\StringTok{"out\_1"}\NormalTok{, output\_i)}

        \ControlFlowTok{return}\NormalTok{ loss}
\end{Highlighting}
\end{Shaded}

Now let's create the model and print out the initial Weights and Biases
and predictions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ LightningLSTM() }\CommentTok{\# First, make model from the class}

\CommentTok{\#\# print out the name and value for each parameter}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Before optimization, the parameters are..."}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
    \BuiltInTok{print}\NormalTok{(name, param.data)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Now let\textquotesingle{}s compare the observed and predicted values..."}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company A: Observed = 0, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company B: Observed = 1, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{1.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Before optimization, the parameters are...
lstm.weight_ih_l0 tensor([[ 0.7645],
        [ 0.8300],
        [-0.2343],
        [ 0.9186]])
lstm.weight_hh_l0 tensor([[-0.2191],
        [ 0.2018],
        [-0.4869],
        [ 0.5873]])
lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])
lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor([0.6675])
Company B: Observed = 1, Predicted = tensor([0.6665])
\end{verbatim}

As expected, the predictions are bad, so we will train the model.
However, because we've increased the learning rate to 0.1, we only need
to train for 300 epochs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# }\AlertTok{NOTE}\CommentTok{: Because we have set Adam\textquotesingle{}s learning rate to 0.1, we will train much, much faster.}
\CommentTok{\#\# Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train}
\CommentTok{\#\# the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,}
\CommentTok{\#\# we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)}
\CommentTok{\#\# because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(max\_epochs}\OperatorTok{=}\DecValTok{300}\NormalTok{, log\_every\_n\_steps}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{trainer.fit(model, train\_dataloaders}\OperatorTok{=}\NormalTok{dataloader)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"After optimization, the parameters are..."}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
    \BuiltInTok{print}\NormalTok{(name, param.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Training: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

\begin{verbatim}
After optimization, the parameters are...
lstm.weight_ih_l0 tensor([[3.5364],
        [1.3869],
        [1.5390],
        [1.2488]])
lstm.weight_hh_l0 tensor([[5.2070],
        [2.9577],
        [3.2652],
        [2.0678]])
lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])
lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])
\end{verbatim}

Now that training is done, let's print out the new predictions\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Now let\textquotesingle{}s compare the observed and predicted values..."}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company A: Observed = 0, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{0.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Company B: Observed = 1, Predicted ="}\NormalTok{, model(torch.tensor([}\FloatTok{1.}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{1.}\NormalTok{])).detach())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Now let's compare the observed and predicted values...
Company A: Observed = 0, Predicted = tensor([6.8527e-05])
Company B: Observed = 1, Predicted = tensor([0.9809])
\end{verbatim}

\ldots and, as we can see, after just 300 epochs, the LSTM is making
great predictions. The prediction for Company A is close to the observed
value 0 and the prediction for Company B is close to the observed value
1.

Lastly, let's go back to TensorBoard to see the latest graphs. NOTE: To
make it easier to see what we just did, deselect version0, version1 and
version2 and make sure version3 is checked on the left-hand side of the
page, under where it says Runs. This allows us to just look at the log
files from the most recent training, which only went for 300 epochs.

In all three graphs, the loss (trainloss) and the predictions for
Company A (out0) and Company B (out\_1) started to taper off after 500
steps, or just 250 epochs, suggesting that adding more epochs may not
improve the predictions much, so we're done!

\part{Introduction to Hyperparameter Tuning}

\chapter{Hyperparameter Tuning}\label{hyperparameter-tuning}

\section{Structure of the Hyperparameter Tuning
Chapters}\label{structure-of-the-hyperparameter-tuning-chapters}

The first part is structured as follows:

The concept of the hyperparameter tuning is described in
Section~\ref{sec-hyperparameter-tuning-goals}.

Hyperparameter tuning with sklearn in Python is described in
Chapter~\ref{sec-hpt-sklearn}.

Hyperparameter tuning with river in Python is described in
Chapter~\ref{sec-hpt-river}.

This part of the book is concluded with a description of the most recent
\texttt{PyTorch} hyperparameter tuning approach, which is the
integration of \texttt{spotpython} into the \texttt{PyTorch\ Lightning}
training workflow. Hyperparameter tuning with PyTorch Lightning in
Python is described in Chapter~\ref{sec-hpt-pytorch}. This is considered
as the most effective, efficient, and flexible way to integrate
\texttt{spotpython} into the \texttt{PyTorch} training workflow.

Figure~\ref{fig-spotGUI} shows the graphical user interface of
\texttt{spotpython} that is used in this book.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/spotGUI.png}}

}

\caption{\label{fig-spotGUI}spot GUI}

\end{figure}%

\section{Goals of Hyperparameter
Tuning}\label{sec-hyperparameter-tuning-goals}

The goal of hyperparameter tuning is to optimize the hyperparameters in
a way that improves the performance of the machine learning or deep
learning model. Hyperparameters are parameters that are not learned
during the training process, but are set before the training process
begins. Hyperparameter tuning is an important, but often difficult and
computationally intensive task. Changing the architecture of a neural
network or the learning rate of an optimizer can have a significant
impact on the performance.

Hyperparameter tuning is referred to as ``hyperparameter optimization''
(HPO) in the literature. However, since we do not consider the
optimization, but also the understanding of the hyperparameters, we use
the term ``hyperparameter tuning'' in this book. See also the discussion
in Chapter 2 of Bartz et al. (2022), which lays the groundwork and
presents an introduction to the process of tuning Machine Learning and
Deep Learning hyperparameters and the respective methodology. Since the
key elements such as the hyperparameter tuning process and measures of
tunability and performance are presented in Bartz et al. (2022), we
refer to this chapter for details.

The simplest, but also most computationally expensive, hyperparameter
tuning approach uses manual search (or trial-and-error (Meignan et al.
2015)). Commonly encountered is simple random search, i.e., random and
repeated selection of hyperparameters for evaluation, and lattice search
(``grid search''). In addition, methods that perform directed search and
other model-free algorithms, i.e., algorithms that do not explicitly
rely on a model, e.g., evolution strategies (Bartz-Beielstein et al.
2014) or pattern search (Lewis, Torczon, and Trosset 2000) play an
important role. Also, ``hyperband'', i.e., a multi-armed bandit strategy
that dynamically allocates resources to a set of random configurations
and uses successive bisections to stop configurations with poor
performance (Li et al. 2016), is very common in hyperparameter tuning.
The most sophisticated and efficient approaches are the Bayesian
optimization and surrogate model based optimization methods, which are
based on the optimization of cost functions determined by simulations or
experiments.

We consider a surrogate optimization based hyperparameter tuning
approach that uses the Python version of the SPOT (``Sequential
Parameter Optimization Toolbox'') (Bartz-Beielstein, Lasarczyk, and
Preuss 2005), which is suitable for situations where only limited
resources are available. This may be due to limited availability and
cost of hardware, or due to the fact that confidential data may only be
processed locally, e.g., due to legal requirements. Furthermore, in our
approach, the understanding of algorithms is seen as a key tool for
enabling transparency and explainability. This can be enabled, for
example, by quantifying the contribution of machine learning and deep
learning components (nodes, layers, split decisions, activation
functions, etc.). Understanding the importance of hyperparameters and
the interactions between multiple hyperparameters plays a major role in
the interpretability and explainability of machine learning models. SPOT
provides statistical tools for understanding hyperparameters and their
interactions. Last but not least, it should be noted that the SPOT
software code is available in the open source \texttt{spotpython}
package on github\footnote{\url{https://github.com/sequential-parameter-optimization}},
allowing replicability of the results. This tutorial describes the
Python variant of SPOT, which is called \texttt{spotpython}. The R
implementation is described in Bartz et al. (2022). SPOT is an
established open source software that has been maintained for more than
15 years (Bartz-Beielstein, Lasarczyk, and Preuss 2005) (Bartz et al.
2022).

\part{Hyperparameter Tuning with Sklearn}

\chapter{HPT: sklearn}\label{sec-hpt-sklearn}

\section{Introduction to sklearn}\label{sec-hpt-sklearn-intro}

\chapter{HPT: sklearn SVC on Moons Data}\label{sec-hpt-sklearn-svc}

This chapter is a tutorial for the Hyperparameter Tuning (HPT) of a
\texttt{sklearn} SVC model on the Moons dataset.

\section{Step 1: Setup}\label{sec-setup-17}

Before we consider the detailed experimental setup, we select the
parameters that affect run time, initial design size and the device that
is used.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-caution-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-caution-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Run time and initial design size should be increased for real
experiments}]

\begin{itemize}
\tightlist
\item
  MAX\_TIME is set to one minute for demonstration purposes. For real
  experiments, this should be increased to at least 1 hour.
\item
  INIT\_SIZE is set to 5 for demonstration purposes. For real
  experiments, this should be increased to at least 10.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{10}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"10"}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Step 2: Initialization of the Empty
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the Empty fun\_control Dictionary}}\label{step-2-initialization-of-the-empty-fun_control-dictionary}

\texttt{spotpython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotpython}. The \texttt{fun\_control}
dictionary is the central data structure that is used to control the
optimization process. It is initialized as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{MAX\_TIME,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_18_57_0
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TensorBoard}]

\begin{itemize}
\tightlist
\item
  Since the \texttt{spot\_tensorboard\_path} argument is not
  \texttt{None}, which is the default, \texttt{spotpython} will log the
  optimization process in the TensorBoard folder.
\item
  The \texttt{TENSORBOARD\_CLEAN} argument is set to \texttt{True} to
  archive the TensorBoard folder if it already exists. This is useful if
  you want to start a hyperparameter tuning process from scratch. If you
  want to continue a hyperparameter tuning process, set
  \texttt{TENSORBOARD\_CLEAN} to \texttt{False}. Then the TensorBoard
  folder will not be archived and the old and new TensorBoard files will
  shown in the TensorBoard dashboard.
\end{itemize}

\end{tcolorbox}

\section{Step 3: SKlearn Load Data
(Classification)}\label{sec-data-loading-14}

Randomly generate classification data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ make\_moons, make\_circles, make\_classification}
\NormalTok{n\_features }\OperatorTok{=} \DecValTok{2}
\NormalTok{n\_samples }\OperatorTok{=} \DecValTok{500}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\NormalTok{ds }\OperatorTok{=}\NormalTok{  make\_moons(n\_samples, noise}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{X, y }\OperatorTok{=}\NormalTok{ ds}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{    X, y, test\_size}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{train }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_train, y\_train.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{test }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_test, y\_test.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{train.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& x1 & x2 & y \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1.960101 & 0.383172 & 0.0 \\
1 & 2.354420 & -0.536942 & 1.0 \\
2 & 1.682186 & -0.332108 & 0.0 \\
3 & 1.856507 & 0.687220 & 1.0 \\
4 & 1.925524 & 0.427413 & 1.0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ matplotlib.colors }\ImportTok{import}\NormalTok{ ListedColormap}

\NormalTok{x\_min, x\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{y\_min, y\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{cm }\OperatorTok{=}\NormalTok{ plt.cm.RdBu}
\NormalTok{cm\_bright }\OperatorTok{=}\NormalTok{ ListedColormap([}\StringTok{"\#FF0000"}\NormalTok{, }\StringTok{"\#0000FF"}\NormalTok{])}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ plt.subplot(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Input data"}\NormalTok{)}
\CommentTok{\# Plot the training points}
\NormalTok{ax.scatter(X\_train[:, }\DecValTok{0}\NormalTok{], X\_train[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\NormalTok{y\_train, cmap}\OperatorTok{=}\NormalTok{cm\_bright, edgecolors}\OperatorTok{=}\StringTok{"k"}\NormalTok{)}
\CommentTok{\# Plot the testing points}
\NormalTok{ax.scatter(}
\NormalTok{    X\_test[:, }\DecValTok{0}\NormalTok{], X\_test[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\NormalTok{y\_test, cmap}\OperatorTok{=}\NormalTok{cm\_bright, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{"k"}
\NormalTok{)}
\NormalTok{ax.set\_xlim(x\_min, x\_max)}
\NormalTok{ax.set\_ylim(y\_min, y\_max)}
\NormalTok{ax.set\_xticks(())}
\NormalTok{ax.set\_yticks(())}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-6-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(train)}
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}\StringTok{"data"}\NormalTok{: }\VariableTok{None}\NormalTok{, }\CommentTok{\# dataset,}
               \StringTok{"train"}\NormalTok{: train,}
               \StringTok{"test"}\NormalTok{: test,}
               \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
               \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-401}

Data preprocesssing can be very simple, e.g., you can ignore it. Then
you would choose the \texttt{prep\_model} ``None'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

A default approach for numerical data is the \texttt{StandardScaler}
(mean 0, variance 1). This can be selected as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\NormalTok{prep\_model }\OperatorTok{=}\NormalTok{ StandardScaler}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

Even more complicated pre-processing steps are possible, e.g., the
follwing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_columns = []}
\NormalTok{one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore", sparse\_output=False)}
\NormalTok{prep\_model = ColumnTransformer(}
\NormalTok{         transformers=[}
\NormalTok{             ("categorical", one\_hot\_encoder, categorical\_columns),}
\NormalTok{         ],}
\NormalTok{         remainder=StandardScaler,}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{step-5-select-model-algorithm-and-core_model_hyper_dict}

The selection of the algorithm (ML model) that should be tuned is done
by specifying the its name from the \texttt{sklearn} implementation. For
example, the \texttt{SVC} support vector machine classifier is selected
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.sklearn\_hyper\_dict }\ImportTok{import}\NormalTok{ SklearnHyperDict}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{SVC,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{SklearnHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now \texttt{fun\_control} has the information from the JSON file. The
corresponding entries for the \texttt{core\_model} class are shown
below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control[}\StringTok{\textquotesingle{}core\_model\_hyper\_dict\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'C': {'type': 'float',
  'default': 1.0,
  'transform': 'None',
  'lower': 0.1,
  'upper': 10.0},
 'kernel': {'levels': ['linear', 'poly', 'rbf', 'sigmoid'],
  'type': 'factor',
  'default': 'rbf',
  'transform': 'None',
  'core_model_parameter_type': 'str',
  'lower': 0,
  'upper': 3},
 'degree': {'type': 'int',
  'default': 3,
  'transform': 'None',
  'lower': 3,
  'upper': 3},
 'gamma': {'levels': ['scale', 'auto'],
  'type': 'factor',
  'default': 'scale',
  'transform': 'None',
  'core_model_parameter_type': 'str',
  'lower': 0,
  'upper': 1},
 'coef0': {'type': 'float',
  'default': 0.0,
  'transform': 'None',
  'lower': 0.0,
  'upper': 0.0},
 'shrinking': {'levels': [0, 1],
  'type': 'factor',
  'default': 0,
  'transform': 'None',
  'core_model_parameter_type': 'bool',
  'lower': 0,
  'upper': 1},
 'probability': {'levels': [0, 1],
  'type': 'factor',
  'default': 0,
  'transform': 'None',
  'core_model_parameter_type': 'bool',
  'lower': 0,
  'upper': 1},
 'tol': {'type': 'float',
  'default': 0.001,
  'transform': 'None',
  'lower': 0.0001,
  'upper': 0.01},
 'cache_size': {'type': 'float',
  'default': 200,
  'transform': 'None',
  'lower': 100,
  'upper': 400},
 'break_ties': {'levels': [0, 1],
  'type': 'factor',
  'default': 0,
  'transform': 'None',
  'core_model_parameter_type': 'bool',
  'lower': 0,
  'upper': 1}}
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\texttt{sklearn\ Model} Selection}]

The following \texttt{sklearn} models are supported by default:

\begin{itemize}
\tightlist
\item
  RidgeCV
\item
  RandomForestClassifier
\item
  SVC
\item
  LogisticRegression
\item
  KNeighborsClassifier
\item
  GradientBoostingClassifier
\item
  GradientBoostingRegressor
\item
  ElasticNet
\end{itemize}

They can be imported as follows:

\phantomsection\label{import_sklearn_models}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ RidgeCV}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingRegressor}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ ElasticNet}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model}

\texttt{spotpython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modifying-hyperparameter-levels}.

\subsection{Modify hyperparameter of type numeric and integer
(boolean)}\label{modify-hyperparameter-of-type-numeric-and-integer-boolean}

Numeric and boolean values can be modified using the
\texttt{modify\_hyper\_parameter\_bounds} method.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\texttt{sklearn\ Model} Hyperparameters}]

The hyperparameters of the \texttt{sklearn} \texttt{SVC} model are
described in the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}{sklearn
documentation}.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  For example, to change the \texttt{tol} hyperparameter of the
  \texttt{SVC} model to the interval {[}1e-5, 1e-3{]}, the following
  code can be used:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"tol"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{1e{-}5}\NormalTok{, }\FloatTok{1e{-}3}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"probability"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{][}\StringTok{"tol"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'type': 'float',
 'default': 0.001,
 'transform': 'None',
 'lower': 1e-05,
 'upper': 0.001}
\end{verbatim}

\subsection{Modify hyperparameter of type
factor}\label{modify-hyperparameter-of-type-factor}

Factors can be modified with the
\texttt{modify\_hyper\_parameter\_levels} function. For example, to
exclude the \texttt{sigmoid} kernel from the tuning, the \texttt{kernel}
hyperparameter of the \texttt{SVC} model can be modified as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"kernel"}\NormalTok{, [}\StringTok{"poly"}\NormalTok{, }\StringTok{"rbf"}\NormalTok{])}
\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{][}\StringTok{"kernel"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'levels': ['poly', 'rbf'],
 'type': 'factor',
 'default': 'rbf',
 'transform': 'None',
 'core_model_parameter_type': 'str',
 'lower': 0,
 'upper': 1}
\end{verbatim}

\subsection{Optimizers}\label{sec-optimizers-401}

Optimizers are described in Section~\ref{sec-optimizer}.

\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function}

There are two metrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{metric\_river} is used for the river based evaluation via
  \texttt{eval\_oml\_iter\_progressive}.
\item
  \texttt{metric\_sklearn} is used for the sklearn based evaluation.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_absolute\_error, accuracy\_score, roc\_curve, roc\_auc\_score, log\_loss, mean\_squared\_error}
\NormalTok{fun\_control.update(\{}
               \StringTok{"metric\_sklearn"}\NormalTok{: log\_loss,}
               \StringTok{"weights"}\NormalTok{: }\FloatTok{1.0}\NormalTok{,}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{\texttt{metric\_sklearn}: Minimization and Maximization}]

\begin{itemize}
\tightlist
\item
  Because the \texttt{metric\_sklearn} is used for the sklearn based
  evaluation, it is important to know whether the metric should be
  minimized or maximized.
\item
  The \texttt{weights} parameter is used to indicate whether the metric
  should be minimized or maximized.
\item
  If \texttt{weights} is set to \texttt{-1.0}, the metric is maximized.
\item
  If \texttt{weights} is set to \texttt{1.0}, the metric is minimized,
  e.g., \texttt{weights\ =\ 1.0} for \texttt{mean\_absolute\_error}, or
  \texttt{weights\ =\ -1.0} for \texttt{roc\_auc\_score}.
\end{itemize}

\end{tcolorbox}

\subsection{Predict Classes or Class
Probabilities}\label{predict-classes-or-class-probabilities}

If the key \texttt{"predict\_proba"} is set to \texttt{True}, the class
probabilities are predicted. \texttt{False} is the default, i.e., the
classes are predicted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
               \StringTok{"predict\_proba"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function}

\subsection{The Objective
Function}\label{sec-the-objective-function-401}

The objective function is selected next. It implements an interface from
\texttt{sklearn}'s training, validation, and testing methods to
\texttt{spotpython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperSklearn().fun\_sklearn}
\end{Highlighting}
\end{Shaded}

The following code snippet shows how to get the default hyperparameters
as an array, so that they can be passed to the \texttt{Spot} function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer}

The class \texttt{Spot}
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/spot/spot.py}{{[}SOURCE{]}}
is the hyperparameter tuning workhorse. It is initialized with the
following parameters:

\begin{itemize}
\tightlist
\item
  \texttt{fun}: the objective function
\item
  \texttt{fun\_control}: the dictionary with the control parameters for
  the objective function
\item
  \texttt{design}: the experimental design
\item
  \texttt{design\_control}: the dictionary with the control parameters
  for the experimental design
\item
  \texttt{surrogate}: the surrogate model
\item
  \texttt{surrogate\_control}: the dictionary with the control
  parameters for the surrogate model
\item
  \texttt{optimizer}: the optimizer
\item
  \texttt{optimizer\_control}: the dictionary with the control
  parameters for the optimizer
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Total run time}]

The total run time may exceed the specified \texttt{max\_time}, because
the initial design (here: \texttt{init\_size} = INIT\_SIZE as specified
above) is always evaluated, even if this takes longer than
\texttt{max\_time}.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ design\_control\_init, surrogate\_control\_init}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init()}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"init\_size"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{INIT\_SIZE,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{,}
\NormalTok{                                           n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 6.436366676628063 [----------] 4.69% 
spotpython tuning: 6.436366676628063 [#---------] 7.91% 
spotpython tuning: 6.436366676628063 [#---------] 10.98% 
spotpython tuning: 6.436366676628063 [#---------] 14.56% 
spotpython tuning: 6.436366676628063 [##--------] 18.75% 
spotpython tuning: 6.436366676628063 [##--------] 22.84% 
spotpython tuning: 6.436366676628063 [###-------] 27.46% 
spotpython tuning: 6.436366676628063 [###-------] 31.98% 
spotpython tuning: 6.436366676628063 [####------] 36.80% 
spotpython tuning: 6.436366676628063 [####------] 42.49% 
spotpython tuning: 6.436366676628063 [#####-----] 48.80% 
spotpython tuning: 6.436366676628063 [#####-----] 54.23% 
spotpython tuning: 6.436366676628063 [######----] 60.35% 
spotpython tuning: 6.436366676628063 [#######---] 66.47% 
spotpython tuning: 6.436366676628063 [#######---] 72.94% 
spotpython tuning: 6.436366676628063 [########--] 81.52% 
spotpython tuning: 6.436366676628063 [#########-] 89.98% 
spotpython tuning: 6.436366676628063 [##########] 99.57% 
spotpython tuning: 6.436366676628063 [##########] 100.00% Done...

Experiment saved to 10_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x159c0d880>
\end{verbatim}

\subsection{TensorBoard}\label{sec-tensorboard-401}

Now we can start TensorBoard in the background with the following
command, where \texttt{./runs} is the default directory for the
TensorBoard log files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TENSORBOARD\_PATH}]

The TensorBoard path can be printed with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ get\_tensorboard\_path}
\NormalTok{get\_tensorboard\_path(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'runs/'
\end{verbatim}

\end{tcolorbox}

We can access the TensorBoard web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

The TensorBoard plot illustrates how \texttt{spotpython} can be used as
a microscope for the internal mechanisms of the surrogate-based
optimization process. Here, one important parameter, the learning rate
\(\theta\) of the Kriging surrogate
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/build/kriging.py}{{[}SOURCE{]}}
is plotted against the number of optimization steps.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures_static/13_tensorboard_01.png}

}

\caption{TensorBoard visualization of the spotpython optimization
process and the surrogate model.}

\end{figure}%

\section{Step 9: Results}\label{sec-results-tuning-17}

After the hyperparameter tuning run is finished, the results can be
saved and reloaded with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle, load\_pickle}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ get\_experiment\_name}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(PREFIX)}
\NormalTok{SAVE\_AND\_LOAD }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ SAVE\_AND\_LOAD }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
\NormalTok{    save\_pickle(spot\_tuner, experiment\_name)}
\NormalTok{    spot\_tuner }\OperatorTok{=}\NormalTok{ load\_pickle(experiment\_name)}
\end{Highlighting}
\end{Shaded}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_progress.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-22-output-1.pdf}}

Results can also be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name        | type   | default   |   lower |   upper | tuned                 | transform   |   importance | stars   |
|-------------|--------|-----------|---------|---------|-----------------------|-------------|--------------|---------|
| C           | float  | 1.0       |     0.1 |    10.0 | 1.3459476182876375    | None        |         0.01 |         |
| kernel      | factor | rbf       |     0.0 |     1.0 | rbf                   | None        |       100.00 | ***     |
| degree      | int    | 3         |     3.0 |     3.0 | 3.0                   | None        |         0.00 |         |
| gamma       | factor | scale     |     0.0 |     1.0 | scale                 | None        |         0.01 |         |
| coef0       | float  | 0.0       |     0.0 |     0.0 | 0.0                   | None        |         0.00 |         |
| shrinking   | factor | 0         |     0.0 |     1.0 | 1                     | None        |         0.01 |         |
| probability | factor | 0         |     0.0 |     0.0 | 0                     | None        |         0.00 |         |
| tol         | float  | 0.001     |   1e-05 |   0.001 | 2.988661226661179e-05 | None        |         0.01 |         |
| cache_size  | float  | 200.0     |   100.0 |   400.0 | 174.45504889441855    | None        |         0.01 |         |
| break_ties  | factor | 0         |     0.0 |     1.0 | 0                     | None        |         0.01 |         |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.0025}\NormalTok{, filename}\OperatorTok{=}\StringTok{"./figures/"} \OperatorTok{+}\NormalTok{ experiment\_name}\OperatorTok{+}\StringTok{"\_importance.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-24-output-1.pdf}}

\section{Get Default Hyperparameters}\label{get-default-hyperparameters}

The default hyperparameters, whihc will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control, default}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
SVC(cache_size=200.0, shrinking=False)
\end{verbatim}

\section{Get SPOT Results}\label{get-spot-results}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotpython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\subsection{Plot: Compare Predictions}\label{plot-compare-predictions}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.validation }\ImportTok{import}\NormalTok{ plot\_roc}
\NormalTok{plot\_roc(model\_list}\OperatorTok{=}\NormalTok{[model\_default, model\_spot], fun\_control}\OperatorTok{=}\NormalTok{ fun\_control, model\_names}\OperatorTok{=}\NormalTok{[}\StringTok{"Default"}\NormalTok{, }\StringTok{"Spot"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-27-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.validation }\ImportTok{import}\NormalTok{ plot\_confusion\_matrix}
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_default, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title }\OperatorTok{=} \StringTok{"Default"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-28-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_confusion\_matrix(model}\OperatorTok{=}\NormalTok{model\_spot, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-29-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{min}\NormalTok{(spot\_tuner.y), }\BuiltInTok{max}\NormalTok{(spot\_tuner.y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(np.float64(6.436366676628063), np.float64(10.813096016735146))
\end{verbatim}

\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
C:  0.0050035268857468946
kernel:  100.0
gamma:  0.0050035268857468946
shrinking:  0.0050035268857468946
tol:  0.0050035268857468946
cache_size:  0.0050035268857468946
break_ties:  0.0050035268857468946
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-4.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-5.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-6.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-7.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-8.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-9.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-10.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-11.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-12.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-13.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-14.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-15.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-16.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-17.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-18.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-19.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-20.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-21.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_classification_files/figure-pdf/cell-31-output-22.pdf}}

\subsection{Parallel Coordinates Plot}\label{parallel-coordinates-plot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\subsection{Plot all Combinations of
Hyperparameters}\label{plot-all-combinations-of-hyperparameters}

\begin{itemize}
\tightlist
\item
  Warning: this may take a while.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PLOT\_ALL }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ PLOT\_ALL:}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ spot\_tuner.k}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, n):}
\NormalTok{            spot\_tuner.plot\_contour(i}\OperatorTok{=}\NormalTok{i, j}\OperatorTok{=}\NormalTok{j, min\_z}\OperatorTok{=}\NormalTok{min\_z, max\_z }\OperatorTok{=}\NormalTok{ max\_z)}
\end{Highlighting}
\end{Shaded}

\chapter{\texorpdfstring{Step 2: Initialization of the Empty
\texttt{fun\_control}
Dictionary}{Step 2: Initialization of the Empty fun\_control Dictionary}}\label{step-2-initialization-of-the-empty-fun_control-dictionary-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAX\_TIME }\OperatorTok{=} \DecValTok{1}
\NormalTok{INIT\_SIZE }\OperatorTok{=} \DecValTok{20}
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"18"}
\end{Highlighting}
\end{Shaded}

\texttt{spotpython} supports the visualization of the hyperparameter
tuning process with TensorBoard. The following example shows how to use
TensorBoard with \texttt{spotpython}. The \texttt{fun\_control}
dictionary is the central data structure that is used to control the
optimization process. It is initialized as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{MAX\_TIME,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_21_43_0
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip: TensorBoard}]

\begin{itemize}
\tightlist
\item
  Since the \texttt{spot\_tensorboard\_path} argument is not
  \texttt{None}, which is the default, \texttt{spotpython} will log the
  optimization process in the TensorBoard folder.
\item
  The \texttt{TENSORBOARD\_CLEAN} argument is set to \texttt{True} to
  archive the TensorBoard folder if it already exists. This is useful if
  you want to start a hyperparameter tuning process from scratch. If you
  want to continue a hyperparameter tuning process, set
  \texttt{TENSORBOARD\_CLEAN} to \texttt{False}. Then the TensorBoard
  folder will not be archived and the old and new TensorBoard files will
  shown in the TensorBoard dashboard.
\end{itemize}

\end{tcolorbox}

\section{Step 3: SKlearn Load Data
(Classification)}\label{sec-data-loading-17}

Randomly generate classification data. Here, we use similar data as in
\href{https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_ridge_regression.html\#sphx-glr-auto-examples-miscellaneous-plot-kernel-ridge-regression-py}{Comparison
of kernel ridge regression and SVR}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{42}\NormalTok{)}

\NormalTok{X }\OperatorTok{=} \DecValTok{5} \OperatorTok{*}\NormalTok{ rng.rand(}\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(}\DecValTok{1}\OperatorTok{/}\NormalTok{X).ravel()}\OperatorTok{*}\NormalTok{np.cos(X).ravel()}

\CommentTok{\# Add noise to targets}
\NormalTok{y[::}\DecValTok{5}\NormalTok{] }\OperatorTok{+=} \DecValTok{3} \OperatorTok{*}\NormalTok{ (}\FloatTok{0.5} \OperatorTok{{-}}\NormalTok{ rng.rand(X.shape[}\DecValTok{0}\NormalTok{] }\OperatorTok{//} \DecValTok{5}\NormalTok{))}

\NormalTok{X\_plot }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{100000}\NormalTok{)[:, }\VariableTok{None}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}

\NormalTok{n\_features }\OperatorTok{=} \DecValTok{1}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{    X, y, test\_size}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{train }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_train, y\_train.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{test }\OperatorTok{=}\NormalTok{ pd.DataFrame(np.hstack((X\_test, y\_test.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))))}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, n\_features}\OperatorTok{+}\DecValTok{1}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\NormalTok{train.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& x1 & y \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1.872701 & 1.286910 \\
1 & 4.330881 & -0.085207 \\
2 & 3.659970 & -0.234389 \\
3 & 3.540363 & -0.256848 \\
4 & 0.780093 & 0.681389 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_samples }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(train)}
\CommentTok{\# add the dataset to the fun\_control}
\NormalTok{fun\_control.update(\{}\StringTok{"data"}\NormalTok{: }\VariableTok{None}\NormalTok{, }\CommentTok{\# dataset,}
               \StringTok{"train"}\NormalTok{: train,}
               \StringTok{"test"}\NormalTok{: test,}
               \StringTok{"n\_samples"}\NormalTok{: n\_samples,}
               \StringTok{"target\_column"}\NormalTok{: target\_column\})}
\end{Highlighting}
\end{Shaded}

\section{Step 4: Specification of the Preprocessing
Model}\label{sec-specification-of-preprocessing-model-17}

Data preprocesssing can be very simple, e.g., you can ignore it. Then
you would choose the \texttt{prep\_model} ``None'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model }\OperatorTok{=} \VariableTok{None}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

A default approach for numerical data is the \texttt{StandardScaler}
(mean 0, variance 1). This can be selected as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\NormalTok{prep\_model }\OperatorTok{=}\NormalTok{ StandardScaler}
\NormalTok{fun\_control.update(\{}\StringTok{"prep\_model"}\NormalTok{: prep\_model\})}
\end{Highlighting}
\end{Shaded}

Even more complicated pre-processing steps are possible, e.g., the
follwing pipeline:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{categorical\_columns = []}
\NormalTok{one\_hot\_encoder = OneHotEncoder(handle\_unknown="ignore", sparse\_output=False)}
\NormalTok{prep\_model = ColumnTransformer(}
\NormalTok{         transformers=[}
\NormalTok{             ("categorical", one\_hot\_encoder, categorical\_columns),}
\NormalTok{         ],}
\NormalTok{         remainder=StandardScaler,}
\NormalTok{     )}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Step 5: Select Model (\texttt{algorithm}) and
\texttt{core\_model\_hyper\_dict}}{Step 5: Select Model (algorithm) and core\_model\_hyper\_dict}}\label{step-5-select-model-algorithm-and-core_model_hyper_dict-1}

The selection of the algorithm (ML model) that should be tuned is done
by specifying the its name from the \texttt{sklearn} implementation. For
example, the \texttt{SVC} support vector machine classifier is selected
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.sklearn\_hyper\_dict }\ImportTok{import}\NormalTok{ SklearnHyperDict}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVR}
\NormalTok{add\_core\_model\_to\_fun\_control(core\_model}\OperatorTok{=}\NormalTok{SVR,}
\NormalTok{                              fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{SklearnHyperDict,}
\NormalTok{                              filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now \texttt{fun\_control} has the information from the JSON file. The
corresponding entries for the \texttt{core\_model} class are shown
below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control[}\StringTok{\textquotesingle{}core\_model\_hyper\_dict\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'C': {'type': 'float',
  'default': 1.0,
  'transform': 'None',
  'lower': 0.1,
  'upper': 10.0},
 'kernel': {'levels': ['linear', 'poly', 'rbf', 'sigmoid'],
  'type': 'factor',
  'default': 'rbf',
  'transform': 'None',
  'core_model_parameter_type': 'str',
  'lower': 0,
  'upper': 3},
 'degree': {'type': 'int',
  'default': 3,
  'transform': 'None',
  'lower': 3,
  'upper': 3},
 'gamma': {'levels': ['scale', 'auto'],
  'type': 'factor',
  'default': 'scale',
  'transform': 'None',
  'core_model_parameter_type': 'str',
  'lower': 0,
  'upper': 1},
 'coef0': {'type': 'float',
  'default': 0.0,
  'transform': 'None',
  'lower': 0.0,
  'upper': 0.0},
 'epsilon': {'type': 'float',
  'default': 0.1,
  'transform': 'None',
  'lower': 0.01,
  'upper': 1.0},
 'shrinking': {'levels': [0, 1],
  'type': 'factor',
  'default': 0,
  'transform': 'None',
  'core_model_parameter_type': 'bool',
  'lower': 0,
  'upper': 1},
 'tol': {'type': 'float',
  'default': 0.001,
  'transform': 'None',
  'lower': 0.0001,
  'upper': 0.01},
 'cache_size': {'type': 'float',
  'default': 200,
  'transform': 'None',
  'lower': 100,
  'upper': 400}}
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\texttt{sklearn\ Model} Selection}]

The following \texttt{sklearn} models are supported by default:

\begin{itemize}
\tightlist
\item
  RidgeCV
\item
  RandomForestClassifier
\item
  SVC
\item
  SVR
\item
  LogisticRegression
\item
  KNeighborsClassifier
\item
  GradientBoostingClassifier
\item
  GradientBoostingRegressor
\item
  ElasticNet
\end{itemize}

They can be imported as follows:

\phantomsection\label{import_sklearn_models}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ RidgeCV}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestClassifier}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVC}
\ImportTok{from}\NormalTok{ sklearn.svm }\ImportTok{import}\NormalTok{ SVR}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingClassifier}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ GradientBoostingRegressor}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ ElasticNet}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{\texorpdfstring{Step 6: Modify \texttt{hyper\_dict}
Hyperparameters for the Selected Algorithm aka
\texttt{core\_model}}{Step 6: Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model-1}

\texttt{spotpython} provides functions for modifying the
hyperparameters, their bounds and factors as well as for activating and
de-activating hyperparameters without re-compilation of the Python
source code. These functions were described in
Section~\ref{sec-modifying-hyperparameter-levels}.

\subsection{Modify hyperparameter of type numeric and integer
(boolean)}\label{modify-hyperparameter-of-type-numeric-and-integer-boolean-1}

Numeric and boolean values can be modified using the
\texttt{modify\_hyper\_parameter\_bounds} method.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\texttt{sklearn\ Model} Hyperparameters}]

The hyperparameters of the \texttt{sklearn} \texttt{SVC} model are
described in the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}{sklearn
documentation}.

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  For example, to change the \texttt{tol} hyperparameter of the
  \texttt{SVC} model to the interval {[}1e-5, 1e-3{]}, the following
  code can be used:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_bounds}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"tol"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{1e{-}5}\NormalTok{, }\FloatTok{1e{-}3}\NormalTok{])}
\NormalTok{modify\_hyper\_parameter\_bounds(fun\_control, }\StringTok{"epsilon"}\NormalTok{, bounds}\OperatorTok{=}\NormalTok{[}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.0}\NormalTok{])}
\CommentTok{\# modify\_hyper\_parameter\_bounds(fun\_control, "degree", bounds=[2, 5])}
\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{][}\StringTok{"tol"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'type': 'float',
 'default': 0.001,
 'transform': 'None',
 'lower': 1e-05,
 'upper': 0.001}
\end{verbatim}

\subsection{Modify hyperparameter of type
factor}\label{modify-hyperparameter-of-type-factor-1}

Factors can be modified with the
\texttt{modify\_hyper\_parameter\_levels} function. For example, to
exclude the \texttt{sigmoid} kernel from the tuning, the \texttt{kernel}
hyperparameter of the \texttt{SVR} model can be modified as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ modify\_hyper\_parameter\_levels}
\CommentTok{\# modify\_hyper\_parameter\_levels(fun\_control, "kernel", ["poly", "rbf"])}
\NormalTok{modify\_hyper\_parameter\_levels(fun\_control, }\StringTok{"kernel"}\NormalTok{, [}\StringTok{"rbf"}\NormalTok{])}
\NormalTok{fun\_control[}\StringTok{"core\_model\_hyper\_dict"}\NormalTok{][}\StringTok{"kernel"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'levels': ['rbf'],
 'type': 'factor',
 'default': 'rbf',
 'transform': 'None',
 'core_model_parameter_type': 'str',
 'lower': 0,
 'upper': 0}
\end{verbatim}

\subsection{Optimizers}\label{sec-optimizers-17}

Optimizers are described in Section~\ref{sec-optimizer}.

\section{Step 7: Selection of the Objective (Loss)
Function}\label{step-7-selection-of-the-objective-loss-function-1}

There are two metrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{metric\_river} is used for the river based evaluation via
  \texttt{eval\_oml\_iter\_progressive}.
\item
  \texttt{metric\_sklearn} is used for the sklearn based evaluation.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_absolute\_error, accuracy\_score, roc\_curve, roc\_auc\_score, log\_loss, mean\_squared\_error}
\NormalTok{fun\_control.update(\{}
               \StringTok{"metric\_sklearn"}\NormalTok{: mean\_squared\_error,}
               \StringTok{"weights"}\NormalTok{: }\FloatTok{1.0}\NormalTok{,}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{\texttt{metric\_sklearn}: Minimization and Maximization}]

\begin{itemize}
\tightlist
\item
  Because the \texttt{metric\_sklearn} is used for the sklearn based
  evaluation, it is important to know whether the metric should be
  minimized or maximized.
\item
  The \texttt{weights} parameter is used to indicate whether the metric
  should be minimized or maximized.
\item
  If \texttt{weights} is set to \texttt{-1.0}, the metric is maximized.
\item
  If \texttt{weights} is set to \texttt{1.0}, the metric is minimized,
  e.g., \texttt{weights\ =\ 1.0} for \texttt{mean\_absolute\_error}, or
  \texttt{weights\ =\ -1.0} for \texttt{roc\_auc\_score}.
\end{itemize}

\end{tcolorbox}

\subsection{Predict Classes or Class
Probabilities}\label{predict-classes-or-class-probabilities-1}

If the key \texttt{"predict\_proba"} is set to \texttt{True}, the class
probabilities are predicted. \texttt{False} is the default, i.e., the
classes are predicted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}
               \StringTok{"predict\_proba"}\NormalTok{: }\VariableTok{False}\NormalTok{,}
\NormalTok{               \})}
\end{Highlighting}
\end{Shaded}

\section{Step 8: Calling the SPOT
Function}\label{step-8-calling-the-spot-function-1}

\subsection{The Objective Function}\label{sec-the-objective-function-17}

The objective function is selected next. It implements an interface from
\texttt{sklearn}'s training, validation, and testing methods to
\texttt{spotpython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.hypersklearn }\ImportTok{import}\NormalTok{ HyperSklearn}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperSklearn().fun\_sklearn}
\end{Highlighting}
\end{Shaded}

The following code snippet shows how to get the default hyperparameters
as an array, so that they can be passed to the \texttt{Spot} function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-1}

The class \texttt{Spot}
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/spot/spot.py}{{[}SOURCE{]}}
is the hyperparameter tuning workhorse. It is initialized with the
following parameters:

\begin{itemize}
\tightlist
\item
  \texttt{fun}: the objective function
\item
  \texttt{fun\_control}: the dictionary with the control parameters for
  the objective function
\item
  \texttt{design}: the experimental design
\item
  \texttt{design\_control}: the dictionary with the control parameters
  for the experimental design
\item
  \texttt{surrogate}: the surrogate model
\item
  \texttt{surrogate\_control}: the dictionary with the control
  parameters for the surrogate model
\item
  \texttt{optimizer}: the optimizer
\item
  \texttt{optimizer\_control}: the dictionary with the control
  parameters for the optimizer
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Total run time}]

The total run time may exceed the specified \texttt{max\_time}, because
the initial design (here: \texttt{init\_size} = INIT\_SIZE as specified
above) is always evaluated, even if this takes longer than
\texttt{max\_time}.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ design\_control\_init, surrogate\_control\_init}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init()}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"init\_size"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{INIT\_SIZE,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{,}
\NormalTok{                                           n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\NormalTok{spot\_tuner.run(X\_start}\OperatorTok{=}\NormalTok{X\_start)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.11831336203046443 [----------] 3.21% 
spotpython tuning: 0.11831336203046443 [#---------] 5.07% 
spotpython tuning: 0.11831336203046443 [#---------] 7.93% 
spotpython tuning: 0.11312522934830822 [#---------] 11.17% 
spotpython tuning: 0.11028295967958186 [#---------] 14.74% 
spotpython tuning: 0.10549049842162794 [##--------] 18.38% 
spotpython tuning: 0.10456925631246643 [##--------] 22.67% 
spotpython tuning: 0.10394150275124696 [###-------] 27.38% 
spotpython tuning: 0.10394150275124696 [###-------] 31.97% 
spotpython tuning: 0.10331600680037849 [####------] 36.49% 
spotpython tuning: 0.1022579208739993 [####------] 42.00% 
spotpython tuning: 0.09869979682515821 [#####-----] 47.39% 
spotpython tuning: 0.08707238876821873 [#####-----] 52.91% 
spotpython tuning: 0.08423427702103133 [######----] 59.00% 
spotpython tuning: 0.08423427702103133 [######----] 63.86% 
spotpython tuning: 0.08423427702103133 [#######---] 68.69% 
spotpython tuning: 0.08423427702103133 [#######---] 73.86% 
spotpython tuning: 0.08423427702103133 [########--] 78.87% 
spotpython tuning: 0.08423427702103133 [########--] 83.52% 
spotpython tuning: 0.08423427702103133 [#########-] 88.87% 
spotpython tuning: 0.08423427702103133 [#########-] 94.17% 
spotpython tuning: 0.08423427702103133 [##########] 100.00% Done...

Experiment saved to 18_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x15d79dac0>
\end{verbatim}

\subsection{TensorBoard}\label{sec-tensorboard-17}

Now we can start TensorBoard in the background with the following
command, where \texttt{./runs} is the default directory for the
TensorBoard log files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ get\_tensorboard\_path}
\NormalTok{get\_tensorboard\_path(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'runs/'
\end{verbatim}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-21-output-1.pdf}}

Results can also be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name       | type   | default   |   lower |   upper | tuned             | transform   |   importance | stars   |
|------------|--------|-----------|---------|---------|-------------------|-------------|--------------|---------|
| C          | float  | 1.0       |     0.1 |    10.0 | 4.441505058124631 | None        |         1.97 | *       |
| kernel     | factor | rbf       |     0.0 |     0.0 | rbf               | None        |         0.00 |         |
| degree     | int    | 3         |     3.0 |     3.0 | 3.0               | None        |         0.00 |         |
| gamma      | factor | scale     |     0.0 |     1.0 | scale             | None        |         0.01 |         |
| coef0      | float  | 0.0       |     0.0 |     0.0 | 0.0               | None        |         0.00 |         |
| epsilon    | float  | 0.1       |     0.1 |     1.0 | 0.1               | None        |       100.00 | ***     |
| shrinking  | factor | 0         |     0.0 |     1.0 | 1                 | None        |         0.01 |         |
| tol        | float  | 0.001     |   1e-05 |   0.001 | 0.001             | None        |         0.02 |         |
| cache_size | float  | 200.0     |   100.0 |   400.0 | 342.2075507599466 | None        |         0.01 |         |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{0.0025}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-23-output-1.pdf}}

\section{Get Default
Hyperparameters}\label{get-default-hyperparameters-1}

The default hyperparameters, which will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control, default}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{model\_default}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
SVR(cache_size=200.0, shrinking=False)
\end{verbatim}

\section{Get SPOT Results}\label{get-spot-results-1}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotpython}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X\_tuned }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_tuned, fun\_control)}
\end{Highlighting}
\end{Shaded}

\subsection{Plot: Compare Predictions}\label{plot-compare-predictions-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_default.fit(X\_train, y\_train)}
\NormalTok{y\_default }\OperatorTok{=}\NormalTok{ model\_default.predict(X\_plot)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_spot.fit(X\_train, y\_train)}
\NormalTok{y\_spot }\OperatorTok{=}\NormalTok{ model\_spot.predict(X\_plot)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.scatter(X[:}\DecValTok{100}\NormalTok{], y[:}\DecValTok{100}\NormalTok{], c}\OperatorTok{=}\StringTok{"orange"}\NormalTok{, label}\OperatorTok{=}\StringTok{"data"}\NormalTok{, zorder}\OperatorTok{=}\DecValTok{1}\NormalTok{, edgecolors}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{plt.plot(}
\NormalTok{    X\_plot,}
\NormalTok{    y\_default,}
\NormalTok{    c}\OperatorTok{=}\StringTok{"red"}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\StringTok{"Default SVR"}\NormalTok{)}

\NormalTok{plt.plot(}
\NormalTok{    X\_plot, y\_spot, c}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, label}\OperatorTok{=}\StringTok{"SPOT SVR"}\NormalTok{)}

\NormalTok{plt.xlabel(}\StringTok{"data"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"target"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"SVR"}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.legend()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-28-output-1.pdf}}

\subsection{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
C:  1.9696824842198417
gamma:  0.013433711760202433
epsilon:  100.0
shrinking:  0.013433711760202433
tol:  0.02239945336346596
cache_size:  0.013433711760202433
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-4.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-5.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-6.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-7.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-8.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-9.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-10.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-11.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-12.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-13.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-14.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-15.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{401_spot_hpt_sklearn_regression_files/figure-pdf/cell-29-output-16.pdf}}

\subsection{Parallel Coordinates
Plot}\label{parallel-coordinates-plot-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\part{Hyperparameter Tuning with River}

\chapter{HPT: River}\label{sec-hpt-river}

\section{Introduction to River}\label{sec-hpt-river-intro}

\chapter{Simplifying Hyperparameter Tuning in Online Machine
Learning---The
spotRiverGUI}\label{simplifying-hyperparameter-tuning-in-online-machine-learningthe-spotrivergui}

\section{Introduction}\label{introduction-1}

Batch Machine Learning (BML) often encounters limitations when
processing substantial volumes of streaming data (Keller-McNulty 2004;
Gaber, Zaslavsky, and Krishnaswamy 2005; Aggarwal 2007). These
limitations become particularly evident in terms of available memory,
managing drift in data streams (Bifet and GavaldÃ  2007, 2009; Gama et
al. 2004; Bartz-Beielstein 2024c), and processing novel, unclassified
data (Bifet 2010), (Dredze, Oates, and Piatko 2010). As a solution,
Online Machine Learning (OML) serves as an effective alternative to BML,
adeptly addressing these constraints. OML's ability to sequentially
process data proves especially beneficial for handling data streams
(Bifet et al. 2010a; Masud et al. 2011; Gama, SebastiÃ£o, and Rodrigues
2013; Putatunda 2021; Bartz-Beielstein and Hans 2024).

The Online Machine Learning (OML) methods provided by software packages
such as \texttt{river} (Montiel et al. 2021) or \texttt{MOA} (Bifet et
al. 2010b) require the specification of many hyperparameters. To give an
example, Hoeffding trees (Hoeglinger and Pears 2007), which are very
popular in OML, offer a variety of ``splitters'' to generate subtrees.
There are also several methods to limit the tree size, ensuring time and
memory requirements remain manageable. Given the multitude of
parameters, manually searching for the optimal hyperparameter setting
can be a daunting and often futile task due to the complexity of
possible combinations. This article elucidates how automatic
hyperparameter optimization, or ``tuning'', can be achieved. Beyond
optimizing the OML process, Hyperparameter Tuning (HPT) executed with
the Sequential Parameter Optimization Toolbox (SPOT) enhances the
explainability and interpretability of OML procedures. This can result
in a more efficient, resource-conserving algorithm, contributing to the
concept of ``Green AI''.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}]

Note: This document refers to \texttt{spotRiverGUI} version 0.0.26 which
was released on Feb 18, 2024 on GitHub, see:
\url{https://github.com/sequential-parameter-optimization/spotGUI/tree/main}.
The GUI is under active development and new features will be added soon.

\end{tcolorbox}

This article describes the \texttt{spotRiverGUI}, which is a graphical
user interface for the \texttt{spotriver} package. The GUI allows the
user to select the task, the data set, the preprocessing model, the
metric, and the online machine learning model. The user can specify the
experiment duration, the initial design, and the evaluation options. The
GUI provides information about the data set and allows the user to save
and load experiments. It also starts and stops a tensorboard process to
observe the tuning online and provides an analysis of the hyperparameter
tuning process. The \texttt{spotRiverGUI} releases the user from the
burden of manually searching for the optimal hyperparameter setting.
After providing the data, users can compare different OML algorithms
from the powerful \texttt{river} package in a convenient way and tune
the selected algorithm very efficiently.

This article is structured as follows:

Section~\ref{sec-starting-gui} describes how to install the software. It
also explains how the \texttt{spotRiverGUI} can be started.
Section~\ref{sec-binary-classification} describes the binary
classification task and the options available in the
\texttt{spotRiverGUI}. Section~\ref{sec-regression} provides information
about the planned regression task. Section~\ref{sec-showing-data}
describes how the data can be visualized in the \texttt{spotRiverGUI}.
Section~\ref{sec-saving-loading} provides information about saving and
loading experiments. Section~\ref{sec-running-experiment} describes how
to start an experiment and how the associated tensorboard process can be
started and stopped. Section~\ref{sec-analysis} provides information
about the analysis of the results from the hyperparameter tuning
process. Section~\ref{sec-summary} concludes the article and provides an
outlook.

\section{Installation and Starting}\label{sec-starting-gui}

\subsection{Installation}\label{installation}

We strongly recommend using a virtual environment for the installation
of the \texttt{river}, \texttt{spotriver}, \texttt{build} and
\texttt{spotRiverGUI} packages.

Miniforge, which holds the minimal installers for Conda, is a good
starting point. Please follow the instructions on
\url{https://github.com/conda-forge/miniforge}. Using Conda, the
following commands can be used to create a virtual environment (Python
3.11 is recommended):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}\textgreater{} conda create {-}n myenv python=3.11}
\NormalTok{\textgreater{}\textgreater{} conda activate myenv}
\end{Highlighting}
\end{Shaded}

Now the \texttt{river} and \texttt{spotriver} packages can be installed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}\textgreater{} (myenv) pip install river spotriver build}
\end{Highlighting}
\end{Shaded}

Although the \texttt{spotGUI} package is available on PyPI, we recommend
an installation from the GitHub repository
\href{./https://github.com/sequential-parameter-optimization/spotGUI}{https://github.com/sequential-parameter-optimization/spotGUI},
because the \texttt{spotGUI} package is under active development and new
features will be added soon. The installation from the GitHub repository
is done by executing the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}\textgreater{} (myenv) git clone git@github.com:sequential{-}parameter{-}optimization/spotGUI.git}
\end{Highlighting}
\end{Shaded}

Building the \texttt{spotGUI} package is done by executing the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}\textgreater{} (myenv) cd spotGUI}
\NormalTok{\textgreater{}\textgreater{} (myenv) python {-}m build}
\end{Highlighting}
\end{Shaded}

Now the \texttt{spotRiverGUI} package can be installed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}\textgreater{} (myenv) pip install dist/spotGUI{-}0.0.26.tar.gz}
\end{Highlighting}
\end{Shaded}

\subsection{Starting the GUI}\label{starting-the-gui}

The GUI can be started by executing the \texttt{spotRiverGUI.py} file in
the \texttt{spotGUI/spotRiverGUI} directory. Change to the
\texttt{spotRiverGUI} directory and start the GUI:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}\textgreater{} (myenv) cd spotGUI/spotRiverGUI}
\NormalTok{\textgreater{}\textgreater{} (myenv) python spotRiverGUI.py}
\end{Highlighting}
\end{Shaded}

The GUI window will open, as shown in Figure~\ref{fig-spotRiverGUI-00}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/spotRiverGUI-00.png}}

}

\caption{\label{fig-spotRiverGUI-00}spotriver GUI}

\end{figure}%

After the GUI window has opened, the user can select the task.
Currently, \texttt{Binary\ Classification} is available. Further tasks
like \texttt{Regression} will be available soon.

Depending on the task, the user can select the data set, the
preprocessing model, the metric, and the online machine learning model.

\section{Binary Classification}\label{sec-binary-classification}

\subsection{Binary Classification
Options}\label{binary-classification-options}

If the \texttt{Binary\ Classification} task is selected, the user can
select pre-specified data sets from the \texttt{Data} drop-down menu.

\subsubsection{River Data Sets}\label{sec-river-datasets}

The following data sets from the \texttt{river} package are available
(the descriptions are taken from the \texttt{river} package):

\begin{itemize}
\tightlist
\item
  \texttt{Bananas}: An artificial dataset where instances belongs to
  several clusters with a banana shape.There are two attributes that
  correspond to the x and y axis, respectively. More:
  \url{https://riverml.xyz/dev/api/datasets/Bananas/}.
\item
  \texttt{CreditCard}: Credit card frauds. The datasets contains
  transactions made by credit cards in September 2013 by European
  cardholders. Feature `\texttt{Class}' is the response variable and it
  takes value 1 in case of fraud and 0 otherwise. More:
  \url{https://riverml.xyz/dev/api/datasets/CreditCard/}.
\item
  \texttt{Elec2}: Electricity prices in New South Wales. This is a
  binary classification task, where the goal is to predict if the price
  of electricity will go up or down. This data was collected from the
  Australian New South Wales Electricity Market. In this market, prices
  are not fixed and are affected by demand and supply of the market.
  They are set every five minutes. Electricity transfers to/from the
  neighboring state of Victoria were done to alleviate fluctuations.
  More: \url{https://riverml.xyz/dev/api/datasets/Elec2/}.
\item
  \texttt{Higgs}: The data has been produced using Monte Carlo
  simulations. The first 21 features (columns 2-22) are kinematic
  properties measured by the particle detectors in the accelerator. The
  last seven features are functions of the first 21 features; these are
  high-level features derived by physicists to help discriminate between
  the two classes. More:
  \url{https://riverml.xyz/dev/api/datasets/Higgs/}.
\item
  \texttt{HTTP}: HTTP dataset of the KDD 1999 cup. The goal is to
  predict whether or not an HTTP connection is anomalous or not. The
  dataset only contains 2,211 (0.4\%) positive labels. More:
  \url{https://riverml.xyz/dev/api/datasets/HTTP/}.
\item
  \texttt{Phishing}: Phishing websites. This dataset contains features
  from web pages that are classified as phishing or
  not.\url{https://riverml.xyz/dev/api/datasets/Phishing/}
\end{itemize}

\subsubsection{User Data Sets}\label{user-data-sets}

Besides the \texttt{river} data sets described in
Section~\ref{sec-river-datasets}, the user can also select a
user-defined data set. Currently, comma-separated values (CSV) files are
supported. Further formats will be supported soon. The user-defined CSV
data set must be a binary classification task with the target variable
in the last column. The first row must contain the column names. If the
file is copied to the subdirectory \texttt{userData}, the user can
select the data set from the \texttt{Data} drop-down menu.

As an example, we have provided a CSV-version of the \texttt{Phishing}
data set. The file is located in the \texttt{userData} subdirectory and
is called \texttt{PhishingData.csv}. It contains the columns
\texttt{empty\_server\_form\_handler}, \texttt{popup\_window},
\texttt{https}, \texttt{request\_from\_other\_domain},
\texttt{anchor\_from\_other\_domain}, \texttt{is\_popular},
\texttt{long\_url}, \texttt{age\_of\_domain}, \texttt{ip\_in\_url}, and
\texttt{is\_phishing}. The first few lines of the file are shown below
(modified due to formatting reasons):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{empty\_server\_form\_handler,...,is\_phishing}
\NormalTok{0.0,0.0,0.0,0.0,0.0,0.5,1.0,1,1,1}
\NormalTok{1.0,0.0,0.5,0.5,0.0,0.5,0.0,1,0,1}
\NormalTok{0.0,0.0,1.0,0.0,0.5,0.5,0.0,1,0,1}
\NormalTok{0.0,0.0,1.0,0.0,0.0,1.0,0.5,0,0,1}
\end{Highlighting}
\end{Shaded}

Based on the required format, we can see that \texttt{is\_phishing} is
the target column, because it is the last column of the data set.

\subsubsection{Stream Data Sets}\label{stream-data-sets}

Forthcoming versions of the GUI will support stream data sets, e.g, the
Friedman-Drift generator (Ikonomovska 2012) or the SEA-Drift generator
(Street and Kim 2001). The Friedman-Drift generator was also used in the
hyperparameter tuning study in Bartz-Beielstein (2024b).

\subsubsection{Data Set Options}\label{data-set-options}

Currently, the user can select the following parameters for the data
sets:

\begin{itemize}
\tightlist
\item
  \texttt{n\_total}: The total number of instances. Since some data sets
  are quite large, the user can select a subset of the data set by
  specifying the \texttt{n\_total} value.
\item
  \texttt{test\_size}: The size of the test set in percent
  (\texttt{0.0\ -\ 1.0}). The training set will be
  \texttt{1.0\ -\ test\_size}.
\end{itemize}

The target column should be the last column of the data set. Future
versions of the GUI will support the selection of the
\texttt{target\_column} from the GUI. Currently, the value from the
field \texttt{target\_column} has not effect.

To compare different data scaling methods, the user can select the
preprocessing model from the \texttt{Preprocessing} drop-down menu.
Currently, the following preprocessing models are available:

\begin{itemize}
\tightlist
\item
  \texttt{StandardScaler}: Standardize features by removing the mean and
  scaling to unit variance.
\item
  \texttt{MinMaxScaler}: Scale features to a range.
\item
  \texttt{None}: No scaling is performed.
\end{itemize}

The \texttt{spotRiverGUI} will not provide sophisticated data
preprocessing methods. We assume that the data was preprocessed before
it is copied into the \texttt{userData} subdirectory.

\subsection{Experiment Options}\label{experiment-options}

Currently, the user can select the following options for specifying the
experiment duration:

\begin{itemize}
\tightlist
\item
  \texttt{MAX\_TIME}: The maximum time in minutes for the experiment.
\item
  \texttt{FUN\_EVALS}: The number of function evaluations for the
  experiment. This is the number of OML-models that are built and
  evaluated.
\end{itemize}

If the \texttt{MAX\_TIME} is reached or \texttt{FUN\_EVALS} OML models
are evaluated, the experiment will be stopped.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Initial design is always evaluated}]

\begin{itemize}
\tightlist
\item
  The initial design will always be evaluated before one of the stopping
  criteria is reached.
\item
  If the initial design is very large or the model evaluations are very
  time-consuming, the runtime will be larger than the \texttt{MAX\_TIME}
  value.
\end{itemize}

\end{tcolorbox}

Based on the \texttt{INIT\_SIZE}, the number of hyperparameter
configurations for the initial design can be specified. The initial
design is evaluated before the first surrogate model is built. A
detailed description of the initial design and the surrogate model based
hyperparameter tuning can be found in Bartz-Beielstein (2024a) and in
Bartz-Beielstein and Zaefferer (2022). The \texttt{spotpython} package
is used for the hyperparameter tuning process. It implements a robust
surrogate model based optimization method (Forrester, SÃ³bester, and
Keane 2008).

The \texttt{PREFIX} parameter can be used to specify the experiment
name.

The \texttt{spotpython} hyperparameter tuning program allows the user to
specify several options for the hyperparameter tuning process. The
\texttt{spotRiverGUI} will support more options in future versions.
Currently, the user can specify whether the outcome from the experiment
is noisy or deterministic. The corresponding parameter is called
\texttt{NOISE}. The reader is referred to Bartz-Beielstein (2024b) and
to the chapter ``Handling Noise''
(\url{https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/013_num_spot_noisy.html})
for further information about the \texttt{NOISE} parameter.

\subsection{Evaluation Options}\label{evaluation-options}

The user can select one of the following evaluation metrics for binary
classification tasks from the \texttt{metric} drop-down menu:

\begin{itemize}
\tightlist
\item
  \texttt{accuracy\_score}
\item
  \texttt{cohen\_kappa\_score}
\item
  \texttt{f1\_score}
\item
  \texttt{hamming\_loss}
\item
  \texttt{hinge\_loss}
\item
  \texttt{jaccard\_score}
\item
  \texttt{matthews\_corrcoef}
\item
  \texttt{precision\_score}
\item
  \texttt{recall\_score}
\item
  \texttt{roc\_auc\_score}
\item
  \texttt{zero\_one\_loss}
\end{itemize}

These metrics are based on the \texttt{scikit-learn} module (Pedregosa
et al. 2011), which implements several loss, score, and utility
functions to measure classification performance, see
\url{https://scikit-learn.org/stable/modules/model_evaluation.html\#classification-metrics}.
\texttt{spotRiverGUI} supports metrics that are computed from the
\texttt{y\_pred} and the \texttt{y\_true} values. The \texttt{y\_pred}
values are the predicted target values, and the \texttt{y\_true} values
are the true target values. The \texttt{y\_pred} values are generated by
the online machine learning model, and the \texttt{y\_true} values are
the true target values from the data set.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Evaluation Metrics: Minimization and Maximization}]

\begin{itemize}
\tightlist
\item
  Some metrics are minimized, and some are maximized. The
  \texttt{spotRiverGUI} will support the user in selecting the correct
  metric based on the task. For example, the \texttt{accuracy\_score} is
  maximized, and the \texttt{hamming\_loss} is minimized. The user can
  select the metric and \texttt{spotRiverGUI} will automatically
  determine whether the metric is minimized or maximized.
\end{itemize}

\end{tcolorbox}

In addition to the evaluation metric results, \texttt{spotriver}
considers the time and memory consumption of the online machine learning
model. The \texttt{spotRiverGUI} will support the user in selecting the
time and memory consumption as additional evaluation metrics. By
modifying the weight vector, which is shown in the
\texttt{weights:\ y,\ time,\ mem} field, the user can specify the
importance of the evaluation metrics. For example, the weight vector
\texttt{1,0,0} specifies that only the \texttt{y} metric (e.g.,
accuracy) is considered. The weight vector \texttt{0,1,0} specifies that
only the time metric is considered. The weight vector \texttt{0,0,1}
specifies that only the memory metric is considered. The weight vector
\texttt{1,1,1} specifies that all metrics are considered. Any real
values (also negative ones) are allowed for the weights.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{The weight vector}]

\begin{itemize}
\tightlist
\item
  The specification of adequate weights is highly problem dependent.
\item
  There is no generic setting that fits to all problems.
\end{itemize}

\end{tcolorbox}

As described in Bartz-Beielstein (2024a), a prediction horizon is used
for the comparison of the online-machine learning algorithms. The
\texttt{horizon} can be specified in the \texttt{spotRiverGUI} by the
user and is highly problem dependent. The \texttt{spotRiverGUI} uses the
\texttt{eval\_oml\_horizon} method from the \texttt{spotriver} package,
which evaluates the online-machine learning model on a rolling horizon
basis.

In addition to the \texttt{horizon} value, the user can specify the
\texttt{oml\_grace\_period} value. During the
\texttt{oml\_grace\_period}, the OML-model is trained on the (small)
training data set. No predictions are made during this initial training
phase, but the memory and computation time are measured. Then, the
OML-model is evaluated on the test data set using a given (sklearn)
evaluation metric. The default value of the \texttt{oml\_grace\_period}
is \texttt{horizon}. For convenience, the value \texttt{horizon} is also
selected when the user specifies the \texttt{oml\_grace\_period} value
as \texttt{None}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{The oml\_grace\_period}]

\begin{itemize}
\tightlist
\item
  If the \texttt{oml\_grace\_period} is set to the size of the training
  data set, the OML-model is trained on the entire training data set and
  then evaluated on the test data set using a given (sklearn) evaluation
  metric.
\item
  This setting might be ``unfair'' in some cases, because the OML-model
  should learn online and not on the entire training data set.
\item
  Therefore, a small data set is recommended for the
  \texttt{oml\_grace\_period} setting and the prediction
  \texttt{horizon} is a recommended value for the
  \texttt{oml\_grace\_period} setting. The reader is referred to
  Bartz-Beielstein (2024a) for further information about the
  \texttt{oml\_grace\_period} setting.
\end{itemize}

\end{tcolorbox}

\subsection{Online Machine Learning Model
Options}\label{online-machine-learning-model-options}

The user can select one of the following online machine learning models
from the \texttt{coremodel} drop-down menu:

\begin{itemize}
\tightlist
\item
  \texttt{forest.AMFClassifier}: Aggregated Mondrian Forest classifier
  for online learning (Mourtada, Gaiffas, and Scornet 2019). This
  implementation is truly online, in the sense that a single pass is
  performed, and that predictions can be produced anytime. More:
  \url{https://riverml.xyz/dev/api/forest/AMFClassifier/}.
\item
  \texttt{tree.ExtremelyFastDecisionTreeClassifier}: Extremely Fast
  Decision Tree (EFDT) classifier (Manapragada, Webb, and Salehi 2018).
  Also referred to as the Hoeffding AnyTime Tree (HATT) classifier. In
  practice, despite the name, EFDTs are typically slower than a vanilla
  Hoeffding Tree to process data. More:
  \url{https://riverml.xyz/dev/api/tree/ExtremelyFastDecisionTreeClassifier/}.
\item
  \texttt{tree.HoeffdingTreeClassifier}: Hoeffding Tree or Very Fast
  Decision Tree classifier (Bifet et al. 2010a; Domingos and Hulten
  2000). More:
  \url{https://riverml.xyz/dev/api/tree/HoeffdingTreeClassifier/}.
\item
  \texttt{tree.HoeffdingAdaptiveTreeClassifier}: Hoeffding Adaptive Tree
  classifier (Bifet and GavaldÃ  2009). More:
  \url{https://riverml.xyz/dev/api/tree/HoeffdingAdaptiveTreeClassifier/}.
\item
  \texttt{linear\_model.LogisticRegression}: Logistic regression
  classifier. More:
  \href{https://riverml.xyz/dev/api/linear-model/LogisticRegression/}{hhttps://riverml.xyz/dev/api/linear-model/LogisticRegression/}.
\end{itemize}

The \texttt{spotRiverGUI} automatically determines the hyperparameters
for the selected online machine learning model and adapts the input
fields to the model hyperparameters. The user can modify the
hyperparameters in the GUI. Figure~\ref{fig-spotRiverGUI-01} shows the
\texttt{spotRiverGUI} when the \texttt{forest.AMFClassifier} is selected
and Figure~\ref{fig-spotRiverGUI-02} shows the \texttt{spotRiverGUI}
when the \texttt{tree.HoeffdingTreeClassifier} is selected.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/spotRiverGUI-01.png}}

}

\caption{\label{fig-spotRiverGUI-01}\texttt{spotRiverGUI} when
\texttt{forest.AMFClassifier} is selected}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/spotRiverGUI-02.png}}

}

\caption{\label{fig-spotRiverGUI-02}\texttt{spotRiverGUI} when
\texttt{tree.HoeffdingAdaptiveTreeClassifier} is selected}

\end{figure}%

Numerical and categorical hyperparameters are treated differently in the
\texttt{spotRiverGUI}:

\begin{itemize}
\tightlist
\item
  The user can modify the lower and upper bounds for the numerical
  hyperparameters.
\item
  There are no upper or lower bounds for categorical hyperparameters.
  Instead, hyperparameter values for the categorical hyperparameters are
  considered as sets of values, e.g., the set of
  \texttt{ExhaustiveSplitter}, \texttt{HistogramSplitter},
  \texttt{GaussianSplitter} is provided for the \texttt{splitter}
  hyperparameter of the \texttt{tree.HoeffdingAdaptiveTreeClassifier}
  model as can be seen in Figure~\ref{fig-spotRiverGUI-02}. The user can
  select the full set or any subset of the set of values for the
  categorical hyperparameters.
\end{itemize}

In addition to the lower and upper bounds (or the set of values for the
categorical hyperparameters), the \texttt{spotRiverGUI} provides
information about the \texttt{Default\ values} and the
\texttt{Transformation} function. If the \texttt{Transformation}
function is set to \texttt{None}, the values of the hyperparameters are
passed to the \texttt{spot} tuner as they are. If the
\texttt{Transformation} function is set to
\texttt{transform\_power\_2\_int}, the value \(x\) is transformed to
\(2^x\) before it is passed to the \texttt{spot} tuner.

Modifications of the \texttt{Default\ values} and
\texttt{Transformation} functions values in the \texttt{spotRiverGUI}
have no effect on the hyperparameter tuning process. This is
intensional. In future versions, the user will be able to add their own
hyperparameter dictionaries to the \texttt{spotRiverGUI}, which allows
the modification of \texttt{Default\ values} and \texttt{Transformation}
functions values. Furthermore, the \texttt{spotRiverGUI} will support
more online machine learning models in future versions.

\section{Regression}\label{sec-regression}

Regression tasks will be supported soon. The same workflow as for the
binary classification task will be used, i.e., the user can select the
data set, the preprocessing model, the metric, and the online machine
learning model.

\section{Showing the Data}\label{sec-showing-data}

The \texttt{spotRiverGUI} provides the \texttt{Show\ Data} button, which
opens a new window and shows information about the data set. The first
figure (Figure~\ref{fig-bananas-01}) shows histograms of the target
variables in the train and test data sets. The second figure
(Figure~\ref{fig-bananas-02}) shows scatter plots of the features in the
train data set. The third figure (Figure~\ref{fig-bananas-03}) shows the
corresponding scatter plots of the features in the test data set.

\begin{figure}

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{./figures_static/bananas-01.png}

}

\caption{\label{fig-bananas-01}Output from the \texttt{spotRiverGUI}
when \texttt{Bananas} data is selected for the \texttt{Show\ Data}
option}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{./figures_static/bananas-02.png}

}

\caption{\label{fig-bananas-02}Visualization of the train data. Output
from the \texttt{spotRiverGUI} when \texttt{Bananas} data is selected
for the \texttt{Show\ Data} option}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{./figures_static/bananas-03.png}

}

\caption{\label{fig-bananas-03}Visualization of the test data. Output
from the \texttt{spotRiverGUI} when \texttt{Bananas} data is selected
for the \texttt{Show\ Data} option}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Size of the Displayed Data Sets}]

\begin{itemize}
\tightlist
\item
  Some data sets are quite large and the display of the data sets might
  take some time.
\item
  Therefore, a random subset of 1000 instances of the data set is
  displayed if the data set is larger than 1000 instances.
\end{itemize}

\end{tcolorbox}

Showing the data is important, especially for the new / unknown data
sets as can be seen in Figure~\ref{fig-http-01},
Figure~\ref{fig-http-02}, and Figure~\ref{fig-http-03}: The target
variable is highly biased. The user can check whether the data set is
correctly formatted and whether the target variable is correctly
specified.

\begin{figure}

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{./figures_static/http-01.png}

}

\caption{\label{fig-http-01}Output from the \texttt{spotRiverGUI} when
\texttt{HTTP} data is selected for the \texttt{Show\ Data} option. The
target variable is biased.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{./figures_static/http-02.png}

}

\caption{\label{fig-http-02}Output from the \texttt{spotRiverGUI} when
\texttt{HTTP} data is selected for the \texttt{Show\ Data} option. A
subset of 1000 randomly chosen data points is shown. Only a few positive
events are in the data.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{./figures_static/http-03.png}

}

\caption{\label{fig-http-03}Output from the \texttt{spotRiverGUI} when
\texttt{HTTP} data is selected for the \texttt{Show\ Data} option. The
test data set shows the same structure as the train data set.}

\end{figure}%

In addition to the histograms and scatter plots, the
\texttt{spotRiverGUI} provides textual information about the data set in
the console window. e.g., for the \texttt{Bananas} data set, the
following information is shown:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Train data summary:}
\NormalTok{                 x1           x2            y}
\NormalTok{count  3710.000000  3710.000000  3710.000000}
\NormalTok{mean     {-}0.016243     0.002430     0.451482}
\NormalTok{std       0.995490     1.001150     0.497708}
\NormalTok{min      {-}3.089839    {-}2.385937     0.000000}
\NormalTok{25\%      {-}0.764512    {-}0.914144     0.000000}
\NormalTok{50\%      {-}0.027259    {-}0.033754     0.000000}
\NormalTok{75\%       0.745066     0.836618     1.000000}
\NormalTok{max       2.754447     2.517112     1.000000}

\NormalTok{Test data summary:}
\NormalTok{                 x1           x2            y}
\NormalTok{count  1590.000000  1590.000000  1590.000000}
\NormalTok{mean      0.037900    {-}0.005670     0.440881}
\NormalTok{std       1.009744     0.997603     0.496649}
\NormalTok{min      {-}2.980834    {-}2.199138     0.000000}
\NormalTok{25\%      {-}0.718710    {-}0.911151     0.000000}
\NormalTok{50\%       0.034858    {-}0.046502     0.000000}
\NormalTok{75\%       0.862049     0.806506     1.000000}
\NormalTok{max       2.813360     3.194302     1.000000}
\end{Highlighting}
\end{Shaded}

\section{Saving and Loading}\label{sec-saving-loading}

\subsection{Saving the Experiment}\label{saving-the-experiment}

If the experiment should not be started immediately, the user can save
the experiment by clicking on the \texttt{Save\ Experiment} button. The
\texttt{spotRiverGUI} will save the experiment as a pickle file. The
file name is generated based on the \texttt{PREFIX} parameter. The
pickle file contains a set of dictionaries, which are used to start the
experiment.

\texttt{spotRiverGUI} shows a summary of the selected hyperparameters in
the console window as can be seen in Table~\ref{tbl-hyperdict}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2526}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0842}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1895}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1263}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1053}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2421}}@{}}
\caption{The hyperparameter values for the
\texttt{tree.HoeffdingAdaptiveTreeClassifier}
model.}\label{tbl-hyperdict}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lower
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
upper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
transform
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lower
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
upper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
transform
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
grace\_period & int & 200 & 10 & 1000 & None \\
max\_depth & int & 20 & 2 & 20 & transform\_power\_2\_int \\
delta & float & 1e-07 & 1e-08 & 1e-06 & None \\
tau & float & 0.05 & 0.01 & 0.1 & None \\
leaf\_prediction & factor & nba & 0 & 2 & None \\
nb\_threshold & int & 0 & 0 & 10 & None \\
splitter & factor & GaussianSplitter & 0 & 2 & None \\
bootstrap\_sampling & factor & 0 & 0 & 1 & None \\
drift\_window\_threshold & int & 300 & 100 & 500 & None \\
drift\_detector & factor & ADWIN & 0 & 0 & None \\
switch\_significance & float & 0.05 & 0.01 & 0.1 & None \\
binary\_split & factor & 0 & 0 & 1 & None \\
max\_size & float & 100.0 & 100 & 1000 & None \\
memory\_estimate\_period & int & 1000000 & 100000 & 1e+06 & None \\
stop\_mem\_management & factor & 0 & 0 & 1 & None \\
remove\_poor\_attrs & factor & 0 & 0 & 1 & None \\
merit\_preprune & factor & 0 & 0 & 1 & None \\
\end{longtable}

\subsection{Loading an Experiment}\label{loading-an-experiment}

Future versions of the \texttt{spotRiverGUI} will support the loading of
experiments from the GUI. Currently, the user can load the experiment by
executing the command \texttt{load\_experiment}, see
\url{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/utils/file/\#spotpython.utils.file.load_experiment}.

\section{Running a New Experiment}\label{sec-running-experiment}

An experiment can be started by clicking on the \texttt{Run\ Experiment}
button. The GUI calls \texttt{run\_spot\_python\_experiment} from
\texttt{spotGUI.tuner.spotRun}. Output will be shown in the console
window from which the GUI was started.

\subsection{Starting and Stopping
Tensorboard}\label{starting-and-stopping-tensorboard}

Tensorboard (Abadi et al. 2016) is automatically started when an
experiment is started. The tensorboard process can be observed in a
browser by opening the \url{http://localhost:6006} page. Tensorboard
provides a visual representation of the hyperparameter tuning process.
Figure~\ref{fig-tensorboard-05} and Figure~\ref{fig-tensorboard-04} show
the tensorboard page when the \texttt{spotRiverGUI} is performing the
tuning process.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/tb-05.png}

}

\caption{\label{fig-tensorboard-05}Tensorboard visualization of the
hyperparameter tuning process}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/tb-04.png}

}

\caption{\label{fig-tensorboard-04}Tensorboard. Parallel coordinates
plot}

\end{figure}%

\texttt{spotpython.utils.tensorboard} provides the methods
\texttt{start\_tensorboard} and \texttt{stop\_tensorboard} to start and
stop tensorboard as a background process. After the experiment is
finished, the tensorboard process is stopped automatically.

\section{Performing the Analysis}\label{sec-analysis}

If the hyperparameter tuning process is finished, the user can analyze
the results by clicking on the \texttt{Analysis} button. The following
options are available:

\begin{itemize}
\tightlist
\item
  Progress plot
\item
  Compare tuned versus default hyperparameters
\item
  Importance of hyperparameters
\item
  Contour plot
\item
  Parallel coordinates plot
\end{itemize}

Figure~\ref{fig-prg-00} shows the progress plot of the hyperparameter
tuning process. Black dots denote results from the initial design. Red
dots illustrate the improvement found by the surrogate model based
optimization. For binary classification tasks, the
\texttt{roc\_auc\_score} can be used as the evaluation metric. The
confusion matrix is shown in Figure~\ref{fig-cm-00}. The default versus
tuned hyperparameters are shown in Figure~\ref{fig-default-tuned-00}.
The surrogate plot is shown in Figure~\ref{fig-surrogate-00},
Figure~\ref{fig-surrogate-01}, and Figure~\ref{fig-surrogate-02}.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/prg-00.png}

}

\caption{\label{fig-prg-00}Progress plot of the hyperparameter tuning
process}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{./figures_static/cm-00.png}

}

\caption{\label{fig-cm-00}Confusion matrix}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/default-tuned-00.png}}

}

\caption{\label{fig-default-tuned-00}Default versus tuned
hyperparameters}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/surrogate-00.png}

}

\caption{\label{fig-surrogate-00}Surrogate plot based on the Kriging
model. \texttt{x0} and \texttt{x1} plotted against each other.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/surrogate-01.png}

}

\caption{\label{fig-surrogate-01}Surrogate plot based on the Kriging
model. \texttt{x1} and \texttt{x2} plotted against each other.}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/surrogate-02.png}

}

\caption{\label{fig-surrogate-02}Surrogate plot based an the Kriging
model. \texttt{x0} and \texttt{x2} plotted against each other.}

\end{figure}%

Furthermore, the tuned hyperparameters are shown in the console window.
A typical output is shown below (modified due to formatting reasons):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{|name    |type   |default |low | up |tuned |transf |importance|stars|}
\NormalTok{|{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}|{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}|}
\NormalTok{|n\_estim |int    |    3.0 |2.0 |7.0 |  3.0 | pow\_2 |      0.04|     |}
\NormalTok{|step    |float  |    1.0 |0.1 |10.0|  5.12| None  |      0.21| .   |}
\NormalTok{|use\_agg |factor |    1.0 |0.0 |1.0 |  0.0 | None  |     10.17| *   |}
\NormalTok{|dirichl |float  |    0.5 |0.1 |0.75|  0.37| None  |     13.64| *   |}
\NormalTok{|split\_p |factor |    0.0 |0.0 |1.0 |  0.0 | None  |    100.00| *** |}
\end{Highlighting}
\end{Shaded}

In addition to the tuned parameters that are shown in the column
\texttt{tuned}, the columns \texttt{importance} and \texttt{stars} are
shown. Both columns show the most important hyperparameters based on
information from the surrogate model. The \texttt{stars} column shows
the importance of the hyperparameters in a graphical way. It is
important to note that the results are based on a demo of the
hyperparameter tuning process. The plots are not based on a real
hyperparameter tuning process. The reader is referred to
Bartz-Beielstein (2024b) for further information about the analysis of
the hyperparameter tuning process.

\section{Summary and Outlook}\label{sec-summary}

The \texttt{spotRiverGUI} provides a graphical user interface for the
\texttt{spotriver} package. It releases the user from the burden of
manually searching for the optimal hyperparameter setting. After copying
a data set into the \texttt{userData} folder and starting
\texttt{spotRiverGUI}, users can compare different OML algorithms from
the powerful \texttt{river} package in a convenient way. Users can
generate configurations on their local machines, which can be
transferred to a remote machine for execution. Results from the remote
machine can be copied back to the local machine for analysis.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Benefits of the spotRiverGUI:}]

\begin{itemize}
\tightlist
\item
  Very easy to use (only the data must be provided in the correct
  format).
\item
  Reproducible results.
\item
  State-of-the-art hyperparameter tuning methods.
\item
  Powerful analysis tools, e.g., Bayesian optimization (Forrester,
  SÃ³bester, and Keane 2008; Gramacy 2020).
\item
  Visual representation of the hyperparameter tuning process with
  tensorboard.
\item
  Most advanced online machine learning models from the \texttt{river}
  package.
\end{itemize}

\end{tcolorbox}

The \texttt{river} package (Montiel et al. 2021), which is very well
documented, can be downloaded from \url{https://riverml.xyz/latest/}.

The \texttt{spotRiverGUI} is under active development and new features
will be added soon. It can be downloaded from GitHub:
\url{https://github.com/sequential-parameter-optimization/spotGUI}.

Interactive Jupyter Notebooks and further material about OML are
provided in the GitHub repository
\url{https://github.com/sn-code-inside/online-machine-learning}. This
material is part of the supplementary material of the book ``Online
Machine Learning - A Practical Guide with Examples in Python'', see
\url{https://link.springer.com/book/9789819970063} and the forthcoming
book ``Online Machine Learning - Eine praxisorientierte EinfÃ¼hrung'',
see \url{https://link.springer.com/book/9783658425043}.

\section{Appendix}\label{sec-appendix}

\subsection{Adding new Tasks}\label{adding-new-tasks}

Currently, three tasks are supported in the \texttt{spotRiverGUI}:
\texttt{Binary\ Classification}, \texttt{Regression}, and
\texttt{Rules}. \texttt{Rules} was added in ver 0.6.0. Here, we document
how this task updated was implemented. Adding an additional task
requires modifications in the following files:

\begin{itemize}
\tightlist
\item
  \texttt{spotRun.py}:

  \begin{itemize}
  \tightlist
  \item
    The \texttt{river}class \texttt{rules} must be imported, i.e.,
    \texttt{from\ river\ import\ forest,\ tree,\ linear\_model,\ rules}.
  \item
    The method \texttt{get\_river\_rules\_core\_model\_names()} must be
    modified.
  \item
    The \texttt{get\_scenario\_dict()} method must be modified.
  \end{itemize}
\item
  \texttt{CTk.py}:

  \begin{itemize}
  \tightlist
  \item
    The \texttt{task\_frame} must be extended.
  \item
    The \texttt{change\_task\_event()} method must be modified.
  \end{itemize}
\end{itemize}

In addition, the hyperparameter dictionary in \texttt{spotriver} must be
updated. This is the only modification required in the
\texttt{spotriver}package.

\chapter{\texorpdfstring{\texttt{river} Hyperparameter Tuning: Hoeffding
Tree Regressor with Friedman Drift
Data}{river Hyperparameter Tuning: Hoeffding Tree Regressor with Friedman Drift Data}}\label{river-hyperparameter-tuning-hoeffding-tree-regressor-with-friedman-drift-data}

This chapter demonstrates hyperparameter tuning for \texttt{river}'s
\texttt{Hoeffding\ Tree\ Regressor\ (HTR)} with the Friedman drift data
set
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
The \texttt{Hoeffding\ Tree\ Regressor} is a regression tree that uses
the Hoeffding bound to limit the number of splits evaluated at each
node, i.e., it predicts a real value for each sample.

\section{The Friedman Drift Data
Set}\label{sec-the-friedman-drift-data-set-24}

We will use the Friedman synthetic dataset with concept drifts, which is
described in detail in Section~\ref{sec-a-05-friedman}. The following
parameters are used to generate and handle the data set:

\begin{itemize}
\tightlist
\item
  \texttt{position}: The positions of the concept drifts.
\item
  \texttt{n\_train}: The number of samples used for training.
\item
  \texttt{n\_test}: The number of samples used for testing.
\item
  \texttt{seed}: The seed for the random number generator.
\item
  \texttt{target\_column}: The name of the target column.
\item
  \texttt{drift\_type}: The type of the concept drift.
\end{itemize}

We will use \texttt{spotriver}'s \texttt{convert\_to\_df} function
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/utils/data_conversion.py}{{[}SOURCE{]}}
to convert the \texttt{river} data set to a \texttt{pandas} data frame.
Then we add column names x1 until x10 to the first 10 columns of the
dataframe and the column name y to the last column of the dataframe.

This data generation is independently repeated for the training and test
data sets, because the data sets are generated with concept drifts and
the usual train-test split would not work.

\phantomsection\label{data_set}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.datasets }\ImportTok{import}\NormalTok{ synth}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotriver.utils.data\_conversion }\ImportTok{import}\NormalTok{ convert\_to\_df}

\NormalTok{n\_train }\OperatorTok{=} \DecValTok{6\_000}
\NormalTok{n\_test }\OperatorTok{=} \DecValTok{4\_000}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ n\_train }\OperatorTok{+}\NormalTok{ n\_test}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{(n\_train}\OperatorTok{/}\DecValTok{4}\NormalTok{, n\_train}\OperatorTok{/}\DecValTok{2}\NormalTok{),}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}

\NormalTok{train }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, n\_total}\OperatorTok{=}\NormalTok{n\_train)}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{data_set_testing}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{(n\_test}\OperatorTok{/}\DecValTok{4}\NormalTok{, n\_test}\OperatorTok{/}\DecValTok{2}\NormalTok{),}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\NormalTok{test }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, n\_total}\OperatorTok{=}\NormalTok{n\_test)}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{The Data Set}]

Data sets that are available as \texttt{pandas} dataframes can easily be
passed to the \texttt{spot} hyperparameter tuner. \texttt{spotpython}
requires a \texttt{train} and a \texttt{test} data set, where the column
names must be identical.

\end{tcolorbox}

We combine the train and test data sets and save them to a csv file.

\phantomsection\label{save_data}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.concat([train, test])}
\NormalTok{df.to\_csv(}\StringTok{"./userData/friedman.csv"}\NormalTok{, index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The Friedman Drift data set described in this section is avaialble as a
\texttt{csv} data file and can be downloaded from github:
\href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userData/friedman.csv}{friedman.csv}.

\section{Setup}\label{sec-setup-24}

\subsection{General Experiment
Setup}\label{sec-general-experiment-setup-24}

To keep track of the different experiments, we use a \texttt{PREFIX} for
the experiment name. The \texttt{PREFIX} is used to create a unique
experiment name. The \texttt{PREFIX} is also used to create a unique
TensorBoard folder, which is used to store the TensorBoard log files.

\texttt{spotpython} allows the specification of two different types of
stopping criteria: first, the number of function evaluations
(\texttt{fun\_evals}), and second, the maximum run time in seconds
(\texttt{max\_time}). Here, we will set the number of function
evaluations to infinity and the maximum run time to one minute.

Furthermore, we set the initial design size (\texttt{init\_size}) to 10.
The initial design is used to train the surrogate model. The surrogate
model is used to predict the performance of the hyperparameter
configurations. The initial design is also used to train the first
model. Since the \texttt{init\_size} belongs to the experimental design,
it is set in the \texttt{design\_control} dictionary, see
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/utils/init/\#spotpython.utils.init.design_control_init}{{[}SOURCE{]}}.

\texttt{max\_time} is set to one minute for demonstration purposes and
\texttt{init\_size} is set to 10 for demonstration purposes. For real
experiments, these values should be increased. Note, the total run time
may exceed the specified \texttt{max\_time}, because the initial design
is always evaluated, even if this takes longer than \texttt{max\_time}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: General Experiment Setup}]

The following parameters are used to specify the general experiment
setup:

\phantomsection\label{sum_exp}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX }\OperatorTok{=} \StringTok{"024"}
\NormalTok{fun\_evals }\OperatorTok{=}\NormalTok{ inf}
\NormalTok{max\_time }\OperatorTok{=} \DecValTok{1}
\NormalTok{init\_size }\OperatorTok{=} \DecValTok{10}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{Data Setup}\label{data-setup}

We use the \texttt{StandardScaler}
\href{https://riverml.xyz/dev/api/preprocessing/StandardScaler/}{{[}SOURCE{]}}
from \texttt{river} as the data-preprocessing model. The
\texttt{StandardScaler} is used to standardize the data set, i.e., it
has zero mean and unit variance.

The names of the training and test data sets are \texttt{train} and
\texttt{test}, respectively. They are available as \texttt{pandas}
dataframes. Both must use the same column names. The column names were
set to \texttt{x1} to \texttt{x10} for the features and \texttt{y} for
the target column during the data set generation in
Section~\ref{sec-the-friedman-drift-data-set-24}. Therefore, the
\texttt{target\_column} is set to \texttt{y} (as above).

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Data Setup}]

The following parameters are used to specify the data setup:

\phantomsection\label{sum_data}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_model\_name }\OperatorTok{=} \StringTok{"StandardScaler"}
\NormalTok{test }\OperatorTok{=}\NormalTok{ test}
\NormalTok{train }\OperatorTok{=}\NormalTok{ train}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{Evaluation Setup}\label{evaluation-setup}

Here we use the \texttt{mean\_absolute\_error}
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html}{{[}SOURCE{]}}
as the evaluation metric. Internally, this metric is passed to the
objective (or loss) function \texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/fun/hyperriver.py}{{[}SOURCE{]}}
and further to the evaluation function \texttt{eval\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/evaluation/eval_bml.py}{{[}SOURCE{]}}.

\texttt{spotriver} also supports additional metrics. For example, the
\texttt{metric\_river} is used for the river based evaluation via
\texttt{eval\_oml\_iter\_progressive}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/evaluation/eval_oml.py}{{[}SOURCE{]}}.
The \texttt{metric\_river} is implemented to simulate the behaviour of
the ``original'' \texttt{river} metrics.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Evaluation Setup}]

The following parameter are used to select the evaluation metric:

\phantomsection\label{sum_eval}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{metric\_sklearn\_name }\OperatorTok{=} \StringTok{"mean\_absolute\_error"}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{River-Specific Setup}\label{sec-river-specific-setup-24}

In the online-machine-learning (OML) setup, the model is trained on a
fixed number of observations and then evaluated on a fixed number of
observations. The \texttt{horizon} defines the number of observations
that are used for the evaluation. Here, a horizon of 7*24 is used, which
corresponds to one week of data.

The \texttt{oml\_grace\_period} defines the number of observations that
are used for the initial training of the model. This value is relatively
small, since the online-machine-learning is trained on the incoming data
and the model is updated continuously. However, it needs a certain
number of observations to start the training process. Therefore, this
short training period aka \texttt{oml\_grace\_period} is set to the
horizon, i.e., the number of observations that are used for the
evaluation. In this case, we use a horizon of 7*24.

The \texttt{weights} provide a flexible way to define specific
requirements, e.g., if the memory is more important than the time, the
weight for the memory can be increased. \texttt{spotriver} stores
information about the model' s score (metric), memory, and time. The
hyperparamter tuner requires a single objective. Therefore, a weighted
sum of the metric, memory, and time is computed. The weights are defined
in the \texttt{weights} array. The \texttt{weights} provide a flexible
way to define specific requirements, e.g., if the memory is more
important than the time, the weight for the memory can be increased.

The \texttt{weight\_coeff} defines a multiplier for the results: results
are multiplied by (step/n\_steps)**weight\_coeff, where n\_steps is the
total number of iterations. Results from the beginning have a lower
weight than results from the end if weight\_coeff \textgreater{} 1. If
weight\_coeff == 0, all results have equal weight. Note, that the
\texttt{weight\_coeff} is only used internally for the tuner and does
not affect the results that are used for the evaluation or comparisons.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: River-Specific Setup}]

The following parameters are used:

\phantomsection\label{sum_river-setup}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{horizon }\OperatorTok{=} \DecValTok{7}\OperatorTok{*}\DecValTok{24}
\NormalTok{oml\_grace\_period }\OperatorTok{=} \DecValTok{7}\OperatorTok{*}\DecValTok{24}
\NormalTok{weights }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{])}
\NormalTok{weight\_coeff }\OperatorTok{=} \FloatTok{0.0}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{Model Setup}\label{model-setup}

By using \texttt{core\_model\_name\ =\ "tree.HoeffdingTreeRegressor"},
the \texttt{river} model class \texttt{HoeffdingTreeRegressor}
\href{https://riverml.xyz/dev/api/tree/HoeffdingTreeRegressor/}{{[}SOURCE{]}}
from the \texttt{tree} module is selected. For a given
\texttt{core\_model\_name}, the corresponding hyperparameters are
automatically loaded from the associated dictionary, which is stored as
a JSON file. The JSON file contains hyperparameter type information,
names, and bounds. For \texttt{river} models, the hyperparameters are
stored in the \texttt{RiverHyperDict}, see
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/data/river_hyper_dict.json}{{[}SOURCE{]}}

Alternatively, you can load a local hyper\_dict. Simply set
\texttt{river\_hyper\_dict.json} as the filename. If \texttt{filename}is
set to \texttt{None}, which is the default, the hyper\_dict
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/data/river_hyper_dict.json}{{[}SOURCE{]}}
is loaded from the \texttt{spotriver} package.

How hyperparameter levels can be modified is described in
Section~\ref{sec-modifying-hyperparameter-levels}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Model Setup}]

The following parameters are used for the model setup:

\phantomsection\label{sum_model-setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.fun.hyperriver }\ImportTok{import}\NormalTok{ HyperRiver}
\ImportTok{from}\NormalTok{ spotriver.hyperdict.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\NormalTok{core\_model\_name }\OperatorTok{=} \StringTok{"tree.HoeffdingTreeRegressor"}
\NormalTok{hyperdict }\OperatorTok{=}\NormalTok{ RiverHyperDict}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{Objective Function Setup}\label{objective-function-setup}

The loss function (metric) values are passed to the objective function
\texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/fun/hyperriver.py}{{[}SOURCE{]}},
which combines information about the loss, required memory and time as
described in Section~\ref{sec-river-specific-setup-24}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Objective Function Setup}]

The following parameters are used:

\phantomsection\label{sum_fun-setup}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperRiver().fun\_oml\_horizon}
\end{Highlighting}
\end{Shaded}

\subsection{Surrogate Model Setup}\label{surrogate-model-setup}

The default surrogate model is the \texttt{Kriging} model, see
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/build/kriging/}{{[}SOURCE{]}}.
We specify \texttt{noise} as \texttt{True} to include noise in the
model. An \texttt{anisotropic} kernel is used, which allows different
length scales for each dimension, by setting \texttt{n\_theta\ =\ 2}.
Furthermore, the interval for the \texttt{Lambda} value is set to
\texttt{{[}1e-3,\ 1e2{]}}.

These parameters are set in the \texttt{surrogate\_control} dictionary
and therefore passed to the \texttt{surrogate\_control\_init} function
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/utils/init/\#spotpython.utils.init.surrogate_control_init}{{[}SOURCE{]}}.

\phantomsection\label{surrogate_control_setup}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noise }\OperatorTok{=} \VariableTok{True}
\NormalTok{n\_theta }\OperatorTok{=} \DecValTok{2}
\NormalTok{min\_Lambda }\OperatorTok{=} \FloatTok{1e{-}3}
\NormalTok{max\_Lambda }\OperatorTok{=} \DecValTok{10}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\subsection{Summary: Setting up the
Experiment}\label{sec-summary-setting-up-the-experiment-24}

At this stage, all required information is available to set up the
dictionaries for the hyperparameter tuning. Altogether, the
\texttt{fun\_control}, \texttt{design\_control},
\texttt{surrogate\_control}, and \texttt{optimize\_control} dictionaries
are initialized as follows:

\phantomsection\label{summary_control}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init, optimizer\_control\_init}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperRiver().fun\_oml\_horizon}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"024"}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}

\NormalTok{    prep\_model\_name}\OperatorTok{=}\StringTok{"StandardScaler"}\NormalTok{,}
\NormalTok{    test}\OperatorTok{=}\NormalTok{test,}
\NormalTok{    train}\OperatorTok{=}\NormalTok{train,}
\NormalTok{    target\_column}\OperatorTok{=}\NormalTok{target\_column,}

\NormalTok{    metric\_sklearn\_name}\OperatorTok{=}\StringTok{"mean\_absolute\_error"}\NormalTok{,}
\NormalTok{    horizon}\OperatorTok{=}\DecValTok{7}\OperatorTok{*}\DecValTok{24}\NormalTok{,}
\NormalTok{    oml\_grace\_period}\OperatorTok{=}\DecValTok{7}\OperatorTok{*}\DecValTok{24}\NormalTok{,}
\NormalTok{    weight\_coeff}\OperatorTok{=}\FloatTok{0.0}\NormalTok{,}
\NormalTok{    weights}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{]),}

\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"tree.HoeffdingTreeRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{   )}


\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(}
\NormalTok{    method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{,}
\NormalTok{    n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    min\_Lambda}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{,}
\NormalTok{    max\_Lambda}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{)}

\NormalTok{optimizer\_control }\OperatorTok{=}\NormalTok{ optimizer\_control\_init()}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-2}

The class \texttt{Spot}
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/spot/spot.py}{{[}SOURCE{]}}
is the hyperparameter tuning workhorse. It is initialized with the
following parameters, which were specified above.

\begin{itemize}
\tightlist
\item
  \texttt{fun}: the objective function
\item
  \texttt{fun\_control}: the dictionary with the control parameters for
  the objective function
\item
  \texttt{design\_control}: the dictionary with the control parameters
  for the experimental design
\item
  \texttt{surrogate\_control}: the dictionary with the control
  parameters for the surrogate model
\item
  \texttt{optimizer\_control}: the dictionary with the control
  parameters for the optimizer
\end{itemize}

\texttt{spotpython} allows maximum flexibility in the definition of the
hyperparameter tuning setup. Alternative surrogate models, optimizers,
and experimental designs can be used. Thus, interfaces for the
\texttt{surrogate} model, experimental \texttt{design}, and
\texttt{optimizer} are provided. The default surrogate model is the
kriging model, the default optimizer is the differential evolution, and
default experimental design is the Latin hypercube design.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: \texttt{Spot} Setup}]

The following parameters are used for the \texttt{Spot} setup. These
were specified above:

\phantomsection\label{sum_spot-setup}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ fun}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control}
\NormalTok{optimizer\_control }\OperatorTok{=}\NormalTok{ optimizer\_control}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\phantomsection\label{spot_run}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{    design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control,}
\NormalTok{    optimizer\_control}\OperatorTok{=}\NormalTok{optimizer\_control,}
\NormalTok{)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 3.1966766927566135 [----------] 0.78% 
spotpython tuning: 2.236441439413514 [#---------] 5.25% 
spotpython tuning: 2.236441439413514 [#---------] 7.79% 
spotpython tuning: 2.236441439413514 [#---------] 11.45% 
spotpython tuning: 2.236441439413514 [##--------] 17.06% 
spotpython tuning: 2.236441439413514 [##--------] 22.52% 
spotpython tuning: 2.156054279779148 [####------] 37.33% 
spotpython tuning: 2.156054279779148 [#####-----] 52.38% 
spotpython tuning: 2.156054279779148 [######----] 62.73% 
spotpython tuning: 2.156054279779148 [#######---] 74.73% 
spotpython tuning: 2.156054279779148 [#########-] 89.91% 
spotpython tuning: 2.156054279779148 [##########] 100.00% Done...

Experiment saved to 024_res.pkl
\end{verbatim}

\section{\texorpdfstring{Using the
\texttt{spotgui}}{Using the spotgui}}\label{using-the-spotgui}

The \texttt{spotgui}
\href{https://github.com/sequential-parameter-optimization/spotGUI}{{[}github{]}}
provides a convenient way to interact with the hyperparameter tuning
process. To obtain the settings from
Section~\ref{sec-summary-setting-up-the-experiment-24}, the
\texttt{spotgui} can be started as shown in Figure~\ref{fig-spotgui}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/024_gui.png}

}

\caption{\label{fig-spotgui}spotgui}

\end{figure}%

\section{Results}\label{results-9}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/cell-16-output-1.pdf}}

Results can be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name                   | type   | default          |   lower |   upper | tuned               | transform              |   importance | stars   |
|------------------------|--------|------------------|---------|---------|---------------------|------------------------|--------------|---------|
| grace_period           | int    | 200              |    10.0 |  1000.0 | 41.0                | None                   |         0.59 | .       |
| max_depth              | int    | 20               |     2.0 |    20.0 | 11.0                | transform_power_2_int  |         1.25 | *       |
| delta                  | float  | 1e-07            |   1e-08 |   1e-06 | 1e-06               | None                   |         0.23 | .       |
| tau                    | float  | 0.05             |    0.01 |     0.1 | 0.02976642210440251 | None                   |         0.01 |         |
| leaf_prediction        | factor | mean             |     0.0 |     2.0 | model               | None                   |         0.03 |         |
| leaf_model             | factor | LinearRegression |     0.0 |     2.0 | LinearRegression    | None                   |         0.02 |         |
| model_selector_decay   | float  | 0.95             |     0.9 |    0.99 | 0.9224317358584254  | None                   |         0.14 | .       |
| splitter               | factor | EBSTSplitter     |     0.0 |     2.0 | TEBSTSplitter       | None                   |         1.88 | *       |
| min_samples_split      | int    | 5                |     2.0 |    10.0 | 8.0                 | None                   |         0.05 |         |
| binary_split           | factor | 0                |     0.0 |     1.0 | 1                   | None                   |         1.59 | *       |
| max_size               | float  | 500.0            |   100.0 |  1000.0 | 457.4467581457723   | None                   |       100.00 | ***     |
| memory_estimate_period | int    | 6                |     3.0 |     8.0 | 6.0                 | transform_power_10_int |        11.77 | *       |
| stop_mem_management    | factor | 0                |     0.0 |     1.0 | 1                   | None                   |         1.25 | *       |
| remove_poor_attrs      | factor | 0                |     0.0 |     1.0 | 1                   | None                   |         5.76 | *       |
| merit_preprune         | factor | 1                |     0.0 |     1.0 | 0                   | None                   |         3.60 | *       |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{10.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/cell-18-output-1.pdf}}

\section{Performance of the Model with Default
Hyperparameters}\label{performance-of-the-model-with-default-hyperparameters}

\subsection{Get Default Hyperparameters and Fit the
Model}\label{get-default-hyperparameters-and-fit-the-model}

The default hyperparameters, which will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\texttt{spotpython} tunes numpy arrays, i.e., the hyperparameters are
stored in a numpy array.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control, default}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Evaluate the Model with Default
Hyperparameters}\label{evaluate-the-model-with-default-hyperparameters}

The model with the default hyperparameters can be trained and evaluated.
The evaluation function \texttt{eval\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/evaluation/eval_bml.py}{{[}SOURCE{]}}
is the same function that was used for the hyperparameter tuning. During
the hyperparameter tuning, the evaluation function was called from the
objective (or loss) function \texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/fun/hyperriver.py}{{[}SOURCE{]}}.

\phantomsection\label{eval_default}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ eval\_oml\_horizon}

\NormalTok{df\_eval\_default, df\_true\_default }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_default,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

The three performance criteria, i.e., score (metric), runtime, and
memory consumption, can be visualized with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ plot\_bml\_oml\_horizon\_metrics, plot\_bml\_oml\_horizon\_predictions}
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_bml_oml_horizon_metrics_default-output-1.pdf}}

\subsection{Show Predictions of the Model with Default
Hyperparameters}\label{show-predictions-of-the-model-with-default-hyperparameters}

\begin{itemize}
\tightlist
\item
  Select a subset of the data set for the visualization of the
  predictions:

  \begin{itemize}
  \tightlist
  \item
    We use the mean, \(m\), of the data set as the center of the
    visualization.
  \item
    We use 100 data points, i.e., \(m \pm 50\) as the visualization
    window.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"test"}\NormalTok{].shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{a }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{{-}}\DecValTok{50}
\NormalTok{b }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{+}\DecValTok{50}
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_bml_oml_horizon_predictions_default-output-1.pdf}}

\section{Get SPOT Results}\label{get-spot-results-2}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotpython}.

\phantomsection\label{get_one_core_model_from_x}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{eval_om_horizon}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval\_spot, df\_true\_spot }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_spot,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{, }\StringTok{"spot"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default, df\_eval\_spot], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_bml_oml_horizon_metrics-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b], df\_true\_spot[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_bml_oml_horizon_predictions-output-1.pdf}}

\phantomsection\label{plot_actual_vs_predicted}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.validation }\ImportTok{import}\NormalTok{ plot\_actual\_vs\_predicted}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_default[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_default[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"Default"}\NormalTok{)}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_spot[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_spot[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_actual_vs_predicted-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_actual_vs_predicted-output-2.pdf}}

\section{Visualize Regression Trees}\label{visualize-regression-trees}

\phantomsection\label{model_default_learn_one}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_f }\OperatorTok{=}\NormalTok{ dataset.take(n\_samples)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"n\_samples: }\SpecialCharTok{\{}\NormalTok{n\_samples}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ x, y }\KeywordTok{in}\NormalTok{ dataset\_f:}
\NormalTok{    model\_default.learn\_one(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_samples: 10000
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-caution-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-caution-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Large Trees}]

\begin{itemize}
\tightlist
\item
  Since the trees are large, the visualization is suppressed by default.
\item
  To visualize the trees, uncomment the following line.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model\_default.draw()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_default.summary}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{model_default_summary}
\begin{verbatim}
{'n_nodes': 23,
 'n_branches': 11,
 'n_leaves': 12,
 'n_active_leaves': 12,
 'n_inactive_leaves': 0,
 'height': 7,
 'total_observed_weight': 14168.0}
\end{verbatim}

\subsection{Spot Model}\label{spot-model}

\phantomsection\label{model_spot_learn_one}
\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"n\_samples: }\SpecialCharTok{\{}\NormalTok{n\_samples}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dataset\_f }\OperatorTok{=}\NormalTok{ dataset.take(n\_samples)}
\ControlFlowTok{for}\NormalTok{ x, y }\KeywordTok{in}\NormalTok{ dataset\_f:}
\NormalTok{    model\_spot.learn\_one(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_samples: 10000
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-caution-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-caution-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution: Large Trees}]

\begin{itemize}
\tightlist
\item
  Since the trees are large, the visualization is suppressed by default.
\item
  To visualize the trees, uncomment the following line.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model\_spot.draw()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_spot.summary}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{model_spot_summary}
\begin{verbatim}
{'n_nodes': 63,
 'n_branches': 31,
 'n_leaves': 32,
 'n_active_leaves': 32,
 'n_inactive_leaves': 0,
 'height': 9,
 'total_observed_weight': 14168.0}
\end{verbatim}

\phantomsection\label{compare_two_tree_models}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ compare\_two\_tree\_models}
\BuiltInTok{print}\NormalTok{(compare\_two\_tree\_models(model\_default, model\_spot))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| Parameter             |   Default |   Spot |
|-----------------------|-----------|--------|
| n_nodes               |        23 |     63 |
| n_branches            |        11 |     31 |
| n_leaves              |        12 |     32 |
| n_active_leaves       |        12 |     32 |
| n_inactive_leaves     |         0 |      0 |
| height                |         7 |      9 |
| total_observed_weight |     14168 |  14168 |
\end{verbatim}

\section{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-2}

\phantomsection\label{plot_important_hyperparameter_contour}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
grace_period:  0.5866485715908147
max_depth:  1.2529710618062155
delta:  0.23005732522419112
tau:  0.006467181540908191
leaf_prediction:  0.030078399261997138
leaf_model:  0.024079133390437643
model_selector_decay:  0.135053212446612
splitter:  1.8796971591414964
min_samples_split:  0.048552784623947964
binary_split:  1.5914792644849094
max_size:  100.0
memory_estimate_period:  11.76685866276041
stop_mem_management:  1.2545961176578517
remove_poor_attrs:  5.762169993947819
merit_preprune:  3.6039672988763813
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_important_hyperparameter_contour-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_important_hyperparameter_contour-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{502_spot_hpt_river_friedman_htr_files/figure-pdf/plot_important_hyperparameter_contour-output-4.pdf}}

\section{Parallel Coordinates Plots}\label{parallel-coordinates-plots}

\phantomsection\label{parallel_plot}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{parallel_plot-1}
\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\phantomsection\label{parallel_plot-2}
\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\chapter{The Friedman Drift Data
Set}\label{sec-the-friedman-drift-data-set-25}

This chapter demonstrates hyperparameter tuning for \texttt{river}'s
\texttt{Mondrian\ Tree\ Regressor}
\href{https://riverml.xyz/latest/api/forest/AMFRegressor/}{{[}SOURCE{]}}
with the Friedman drift data set
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}}.
The \texttt{Mondrian\ Tree\ Regressor} is a regression tree, i.e., it
predicts a real value for each sample.

The data set was introduced in
Section~\ref{sec-the-friedman-drift-data-set-24}.

\phantomsection\label{data_set}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.datasets }\ImportTok{import}\NormalTok{ synth}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotriver.utils.data\_conversion }\ImportTok{import}\NormalTok{ convert\_to\_df}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table, print\_res\_table}

\NormalTok{n\_train }\OperatorTok{=} \DecValTok{6\_000}
\NormalTok{n\_test }\OperatorTok{=} \DecValTok{4\_000}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ n\_train }\OperatorTok{+}\NormalTok{ n\_test}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{(n\_train}\OperatorTok{/}\DecValTok{4}\NormalTok{, n\_train}\OperatorTok{/}\DecValTok{2}\NormalTok{),}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}

\NormalTok{train }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, n\_total}\OperatorTok{=}\NormalTok{n\_train)}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{(n\_test}\OperatorTok{/}\DecValTok{4}\NormalTok{, n\_test}\OperatorTok{/}\DecValTok{2}\NormalTok{),}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\NormalTok{test }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, n\_total}\OperatorTok{=}\NormalTok{n\_test)}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\end{Highlighting}
\end{Shaded}

\section{Setup}\label{setup}

We will use a general experiment, data, evaluation, river-specific,
objective-function, and surrogate setup similar to the setup from
Section~\ref{sec-setup-24}. Only the model setup differs from the setup
in Section~\ref{sec-setup-24}. Here we use the
\texttt{Mondrian\ Tree\ Regressor} from \texttt{river}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.hyperdict.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\NormalTok{core\_model\_name }\OperatorTok{=} \StringTok{"forest.AMFRegressor"}
\NormalTok{hyperdict }\OperatorTok{=}\NormalTok{ RiverHyperDict}
\NormalTok{hyperdict}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{model_setup}
\begin{verbatim}
spotriver.hyperdict.river_hyper_dict.RiverHyperDict
\end{verbatim}

\subsection{Select a User
Hyperdictionary}\label{select-a-user-hyperdictionary}

Alternatively, you can load a local hyper\_dict from the ``userModel''
folder. Here, we have selected a copy of the JSON
\texttt{MondrianHyperDict} hyperdictionary from
\href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userModel/mondrian_hyper_dict.json}{{[}SOURCE{]}}
and the \texttt{MondrianHyperDict} class from
\href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userModel/mondrian_hyper_dict.py}{{[}SOURCE{]}}.
The hyperparameters of the \texttt{Mondrian\ Tree\ Regressor} are
defined in the \texttt{MondrianHyperDict} class, i.e., there is an key
``AMFRegressor'' in the \texttt{hyperdict}
``\href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userModel/mondrian_hyper_dict.json}{mondrian\_hyper\_dict.json}''
file.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\NormalTok{sys.path.insert(}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}./userModel\textquotesingle{}}\NormalTok{)}
\ImportTok{import}\NormalTok{ mondrian\_hyper\_dict}
\NormalTok{hyperdict }\OperatorTok{=}\NormalTok{ mondrian\_hyper\_dict.MondrianHyperDict}
\NormalTok{hyperdict}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{hyperdict_from_user_model}
\begin{verbatim}
mondrian_hyper_dict.MondrianHyperDict
\end{verbatim}

\phantomsection\label{summary_control}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init, optimizer\_control\_init}
\ImportTok{from}\NormalTok{ spotriver.fun.hyperriver }\ImportTok{import}\NormalTok{ HyperRiver}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperRiver().fun\_oml\_horizon}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"503"}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}

\NormalTok{    prep\_model\_name}\OperatorTok{=}\StringTok{"StandardScaler"}\NormalTok{,}
\NormalTok{    test}\OperatorTok{=}\NormalTok{test,}
\NormalTok{    train}\OperatorTok{=}\NormalTok{train,}
\NormalTok{    target\_column}\OperatorTok{=}\NormalTok{target\_column,}

\NormalTok{    metric\_sklearn\_name}\OperatorTok{=}\StringTok{"mean\_absolute\_error"}\NormalTok{,}
\NormalTok{    horizon}\OperatorTok{=}\DecValTok{7}\OperatorTok{*}\DecValTok{24}\NormalTok{,}
\NormalTok{    oml\_grace\_period}\OperatorTok{=}\DecValTok{7}\OperatorTok{*}\DecValTok{24}\NormalTok{,}
\NormalTok{    weight\_coeff}\OperatorTok{=}\FloatTok{0.0}\NormalTok{,}
\NormalTok{    weights}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{]),}

\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"forest.AMFRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{hyperdict,}
\NormalTok{   )}


\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{    init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{)}

\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(}
\NormalTok{    method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{,}
\NormalTok{    n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    min\_Lambda}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{,}
\NormalTok{    max\_Lambda}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{)}

\NormalTok{optimizer\_control }\OperatorTok{=}\NormalTok{ optimizer\_control\_init()}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Modify \texttt{hyper\_dict} Hyperparameters for
the Selected Algorithm aka
\texttt{core\_model}}{Modify hyper\_dict Hyperparameters for the Selected Algorithm aka core\_model}}\label{modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model}

After the \texttt{core\_model} and the \texttt{hyperdict} are added to
the \texttt{fun\_control} dictionary, the hyperparameter tuning can be
started. However, in some settings, the user wants to modify the
hyperparameters. This can be done with the
\texttt{set\_int\_hyperparameter\_values},
\texttt{set\_float\_hyperparameter\_values},
\texttt{set\_boolean\_hyperparameter\_values}, and
\texttt{set\_factor\_hyperparameter\_values} functions, which can be
imported from \texttt{from\ spotpython.hyperparameters.values}
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/hyperparameters/values.py}{{[}SOURCE{]}}.

The following code shows how hyperparameter of type float and integer
can be modified. Additional examples can be found in
Section~\ref{sec-modifying-hyperparameter-levels}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |      10 | transform_power_2_int |
| step            | float  |         1 |     0.1 |      10 | None                  |
| use_aggregation | factor |         1 |     0   |       1 | None                  |
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_int\_hyperparameter\_values, set\_float\_hyperparameter\_values, set\_factor\_hyperparameter\_values}
\NormalTok{set\_int\_hyperparameter\_values(fun\_control, }\StringTok{"n\_estimators"}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\NormalTok{set\_float\_hyperparameter\_values(fun\_control, }\StringTok{"step"}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Setting hyperparameter n_estimators to value [2, 7].
Variable type is int.
Core type is None.
Calling modify_hyper_parameter_bounds().
Setting hyperparameter step to value [0.1, 15].
Variable type is float.
Core type is None.
Calling modify_hyper_parameter_bounds().
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |       7 | transform_power_2_int |
| step            | float  |         1 |     0.1 |      15 | None                  |
| use_aggregation | factor |         1 |     0   |       1 | None                  |
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Active and Inactive Hyperparameters}]

Hyperparameters can be excluded from the tuning procedure by selecting
identical values for the lower and upper bounds.

\end{tcolorbox}

\subsection{\texorpdfstring{Run the \texttt{Spot}
Optimizer}{Run the Spot Optimizer}}\label{run-the-spot-optimizer-3}

\phantomsection\label{spot_run}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{    fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{    design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control,}
\NormalTok{    optimizer\_control}\OperatorTok{=}\NormalTok{optimizer\_control,}
\NormalTok{)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 2.828857683897432 [####------] 40.15% 
spotpython tuning: 2.828857683897432 [########--] 80.19% 
spotpython tuning: 2.828857683897432 [##########] 100.00% Done...

Experiment saved to 503_res.pkl
\end{verbatim}

We can start TensorBoard in the background with the following command,
where ./runs is the default directory for the TensorBoard log files:

\texttt{tensorboard\ -\/-logdir="./runs"}We can access the TensorBoard
web server with the following URL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{http://localhost:6006/}
\end{Highlighting}
\end{Shaded}

\section{Results}\label{results-10}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-10-output-1.pdf}}

Results can be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name            | type   |   default |   lower |   upper |              tuned | transform             |   importance | stars   |
|-----------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|
| n_estimators    | int    |       3.0 |     2.0 |       7 |                5.0 | transform_power_2_int |       100.00 | ***     |
| step            | float  |       1.0 |     0.1 |      15 | 10.491348043415995 | None                  |         0.04 |         |
| use_aggregation | factor |       1.0 |     0.0 |       1 |                0.0 | None                  |         0.04 |         |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{10.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/cell-12-output-1.pdf}}

\section{Performance of the Model with Default
Hyperparameters}\label{performance-of-the-model-with-default-hyperparameters-1}

\subsection{Get Default Hyperparameters and Fit the
Model}\label{get-default-hyperparameters-and-fit-the-model-1}

The default hyperparameters, which will be used for a comparion with the
tuned hyperparameters, can be obtained with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\end{Highlighting}
\end{Shaded}

\texttt{spotpython} tunes numpy arrays, i.e., the hyperparameters are
stored in a numpy array.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{model\_default }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X\_start, fun\_control, default}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Evaluate the Model with Default
Hyperparameters}\label{evaluate-the-model-with-default-hyperparameters-1}

The model with the default hyperparameters can be trained and evaluated.
The evaluation function \texttt{eval\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/evaluation/eval_bml.py}{{[}SOURCE{]}}
is the same function that was used for the hyperparameter tuning. During
the hyperparameter tuning, the evaluation function was called from the
objective (or loss) function \texttt{fun\_oml\_horizon}
\href{https://github.com/sequential-parameter-optimization/spotriver/blob/main/src/spotriver/fun/hyperriver.py}{{[}SOURCE{]}}.

\phantomsection\label{eval_default}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ eval\_oml\_horizon}

\NormalTok{df\_eval\_default, df\_true\_default }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_default,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

The three performance criteria, i.e., score (metric), runtime, and
memory consumption, can be visualized with the following commands:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.evaluation.eval\_bml }\ImportTok{import}\NormalTok{ plot\_bml\_oml\_horizon\_metrics, plot\_bml\_oml\_horizon\_predictions}
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_bml_oml_horizon_metrics_default-output-1.pdf}}

\subsection{Show Predictions of the Model with Default
Hyperparameters}\label{show-predictions-of-the-model-with-default-hyperparameters-1}

\begin{itemize}
\tightlist
\item
  Select a subset of the data set for the visualization of the
  predictions:

  \begin{itemize}
  \tightlist
  \item
    We use the mean, \(m\), of the data set as the center of the
    visualization.
  \item
    We use 100 data points, i.e., \(m \pm 50\) as the visualization
    window.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"test"}\NormalTok{].shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{a }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{{-}}\DecValTok{50}
\NormalTok{b }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(m}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{+}\DecValTok{50}
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_bml_oml_horizon_predictions_default-output-1.pdf}}

\section{Get SPOT Results}\label{get-spot-results-3}

In a similar way, we can obtain the hyperparameters found by
\texttt{spotpython}.

\phantomsection\label{get_one_core_model_from_x}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_one\_core\_model\_from\_X}
\NormalTok{X }\OperatorTok{=}\NormalTok{ spot\_tuner.to\_all\_dim(spot\_tuner.min\_X.reshape(}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{model\_spot }\OperatorTok{=}\NormalTok{ get\_one\_core\_model\_from\_X(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{eval_om_horizon}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_eval\_spot, df\_true\_spot }\OperatorTok{=}\NormalTok{ eval\_oml\_horizon(}
\NormalTok{                    model}\OperatorTok{=}\NormalTok{model\_spot,}
\NormalTok{                    train}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"train"}\NormalTok{],}
\NormalTok{                    test}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test"}\NormalTok{],}
\NormalTok{                    target\_column}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"target\_column"}\NormalTok{],}
\NormalTok{                    horizon}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"horizon"}\NormalTok{],}
\NormalTok{                    oml\_grace\_period}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"oml\_grace\_period"}\NormalTok{],}
\NormalTok{                    metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{],}
\NormalTok{                )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_labels}\OperatorTok{=}\NormalTok{[}\StringTok{"default"}\NormalTok{, }\StringTok{"spot"}\NormalTok{]}
\NormalTok{plot\_bml\_oml\_horizon\_metrics(df\_eval }\OperatorTok{=}\NormalTok{ [df\_eval\_default, df\_eval\_spot], log\_y}\OperatorTok{=}\VariableTok{False}\NormalTok{, df\_labels}\OperatorTok{=}\NormalTok{df\_labels, metric}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"metric\_sklearn"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_bml_oml_horizon_metrics-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bml\_oml\_horizon\_predictions(df\_true }\OperatorTok{=}\NormalTok{ [df\_true\_default[a:b], df\_true\_spot[a:b]], target\_column}\OperatorTok{=}\NormalTok{target\_column,  df\_labels}\OperatorTok{=}\NormalTok{df\_labels)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_bml_oml_horizon_predictions-output-1.pdf}}

\phantomsection\label{plot_actual_vs_predicted}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.validation }\ImportTok{import}\NormalTok{ plot\_actual\_vs\_predicted}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_default[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_default[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"Default"}\NormalTok{)}
\NormalTok{plot\_actual\_vs\_predicted(y\_test}\OperatorTok{=}\NormalTok{df\_true\_spot[target\_column], y\_pred}\OperatorTok{=}\NormalTok{df\_true\_spot[}\StringTok{"Prediction"}\NormalTok{], title}\OperatorTok{=}\StringTok{"SPOT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_actual_vs_predicted-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_actual_vs_predicted-output-2.pdf}}

\section{Detailed Hyperparameter
Plots}\label{detailed-hyperparameter-plots-3}

\phantomsection\label{plot_important_hyperparameter_contour}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n_estimators:  100.0
step:  0.044166090488836636
use_aggregation:  0.044166090488836636
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_important_hyperparameter_contour-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_important_hyperparameter_contour-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{503_spot_hpt_river_friedman_amfr_files/figure-pdf/plot_important_hyperparameter_contour-output-4.pdf}}

\section{Parallel Coordinates Plots}\label{parallel-coordinates-plots-1}

\phantomsection\label{parallel_plot}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.parallel\_plot()}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{parallel_plot-1}
\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\phantomsection\label{parallel_plot-2}
\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\part{Hyperparameter Tuning with PyTorch Lightning}

\chapter{Basic Lightning Module}\label{basic-lightning-module}

\section{Introduction}\label{introduction-2}

This chapter implements a basic Pytorch Lightning module. It is based on
the Lightning documentation
\href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{LIGHTNINGMODULE}.

A \texttt{LightningModule} organizes your \texttt{PyTorch} code into six
sections:

\begin{itemize}
\tightlist
\item
  Initialization (\texttt{\_\_init\_\_} and \texttt{setup()}).
\item
  Train Loop (\texttt{training\_step()})
\item
  Validation Loop (\texttt{validation\_step()})
\item
  Test Loop (\texttt{test\_step()})
\item
  Prediction Loop (\texttt{predict\_step()})
\item
  Optimizers and LR Schedulers (\texttt{configure\_optimizers()})
\end{itemize}

The \texttt{Trainer} automates every required step in a clear and
reproducible way. It is the most important part of PyTorch Lightning. It
is responsible for training, testing, and validating the model. The
\texttt{Lightning} core structure looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{net }\OperatorTok{=}\NormalTok{ MyLightningModuleNet()}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ Trainer()}
\NormalTok{trainer.fit(net)}
\end{Highlighting}
\end{Shaded}

There are no \texttt{.cuda()} or \texttt{.to(device)} calls required.
Lightning does these for you.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# don\textquotesingle{}t do in Lightning}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.Tensor(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ x.cuda()}
\NormalTok{x }\OperatorTok{=}\NormalTok{ x.to(device)}

\CommentTok{\# do this instead}
\NormalTok{x }\OperatorTok{=}\NormalTok{ x  }\CommentTok{\# leave it alone!}

\CommentTok{\# or to init a new tensor}
\NormalTok{new\_x }\OperatorTok{=}\NormalTok{ torch.Tensor(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{new\_x }\OperatorTok{=}\NormalTok{ new\_x.to(x)}
\end{Highlighting}
\end{Shaded}

A LightningModule is a \texttt{torch.nn.Module} but with added
functionality. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{net }\OperatorTok{=}\NormalTok{ Net.load\_from\_checkpoint(PATH)}
\NormalTok{net.freeze()}
\NormalTok{out }\OperatorTok{=}\NormalTok{ net(x)}
\end{Highlighting}
\end{Shaded}

\section{Starter Example:
Transformer}\label{starter-example-transformer}

Here are the only required methods for setting up a transfomer model:

\phantomsection\label{transformer-setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}

\ImportTok{from}\NormalTok{ lightning.pytorch.demos }\ImportTok{import}\NormalTok{ Transformer}


\KeywordTok{class}\NormalTok{ LightningTransformer(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, vocab\_size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ Transformer(vocab\_size}\OperatorTok{=}\NormalTok{vocab\_size)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, inputs, target):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.model(inputs, target)}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{(inputs, target)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ torch.nn.functional.nll\_loss(output, target.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ torch.optim.SGD(}\VariableTok{self}\NormalTok{.model.parameters(), lr}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{LightningTransformer} class is a subclass of
\texttt{LightningModule}. It can be trainted as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ lightning.pytorch.demos }\ImportTok{import}\NormalTok{ WikiText2}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ WikiText2()}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LightningTransformer(vocab\_size}\OperatorTok{=}\NormalTok{dataset.vocab\_size)}

\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(fast\_dev\_run}\OperatorTok{=}\DecValTok{100}\NormalTok{)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model, train\_dataloaders}\OperatorTok{=}\NormalTok{dataloader)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{transformer-train}
\begin{verbatim}
Training: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

\section{Lightning Core Methods}\label{lightning-core-methods}

The LightningModule has many convenient methods, but the core ones you
need to know about are shown in Table~\ref{tbl-lm-core-methods}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\caption{The core methods of a
LightningModule}\label{tbl-lm-core-methods}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{\_\_init\_\_} and \texttt{setup} & Initializes the model. \\
\texttt{forward} & Performs a forward pass through the model. To run
data through your model only (separate from \texttt{training\_step}). \\
\texttt{training\_step} & Performs a complete training step. \\
\texttt{validation\_step} & Performs a complete validation step. \\
\texttt{test\_step} & Performs a complete test step. \\
\texttt{predict\_step} & Performs a complete prediction step. \\
\texttt{configure\_optimizers} & Configures the optimizers and
learning-rate schedulers. \\
\end{longtable}

We will take a closer look at thes methods.

\subsection{Training Step}\label{training-step}

\subsubsection{Basics}\label{basics}

To activate the training loop, override the \texttt{training\_step()}
method. \index{training\_step()} If you want to calculate epoch-level
metrics and log them, use \texttt{log()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LightningTransformer(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, vocab\_size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ Transformer(vocab\_size}\OperatorTok{=}\NormalTok{vocab\_size)}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{.model(inputs, target)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ torch.nn.functional.nll\_loss(output, target.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}

        \CommentTok{\# logs metrics for each training\_step,}
        \CommentTok{\# and the average across the epoch, to the progress bar and logger}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"train\_loss"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{True}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}
\end{Highlighting}
\end{Shaded}

The \texttt{log()} method automatically reduces the requested metrics
across a complete epoch and devices.

\subsubsection{Background}\label{background}

\begin{itemize}
\tightlist
\item
  Here is the pseudocode of what the \texttt{log()} method does under
  the hood: \index{log()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{outs }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ batch\_idx, batch }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(train\_dataloader):}
    \CommentTok{\# forward}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ training\_step(batch, batch\_idx)}
\NormalTok{    outs.append(loss.detach())}

    \CommentTok{\# clear gradients}
\NormalTok{    optimizer.zero\_grad()}
    \CommentTok{\# backward}
\NormalTok{    loss.backward()}
    \CommentTok{\# update parameters}
\NormalTok{    optimizer.step()}

\CommentTok{\# note: in reality, we do this incrementally, instead of keeping all outputs in memory}
\NormalTok{epoch\_metric }\OperatorTok{=}\NormalTok{ torch.mean(torch.stack(outs))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  In the case that you need to make use of all the outputs from each
  \texttt{training\_step()}, override the
  \texttt{on\_train\_epoch\_end()} method.
  \index{on\_train\_epoch\_end()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LightningTransformer(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, vocab\_size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ Transformer(vocab\_size}\OperatorTok{=}\NormalTok{vocab\_size)}
        \VariableTok{self}\NormalTok{.training\_step\_outputs }\OperatorTok{=}\NormalTok{ []}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{.model(inputs, target)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ torch.nn.functional.nll\_loss(output, target.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{        preds }\OperatorTok{=}\NormalTok{ ...}
        \VariableTok{self}\NormalTok{.training\_step\_outputs.append(preds)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ on\_train\_epoch\_end(}\VariableTok{self}\NormalTok{):}
\NormalTok{        all\_preds }\OperatorTok{=}\NormalTok{ torch.stack(}\VariableTok{self}\NormalTok{.training\_step\_outputs)}
        \CommentTok{\# do something with all preds}
\NormalTok{        ...}
        \VariableTok{self}\NormalTok{.training\_step\_outputs.clear()  }\CommentTok{\# free memory}
\end{Highlighting}
\end{Shaded}

\subsection{Validation Step}\label{validation-step}

\subsubsection{Basics}\label{basics-1}

To activate the validation loop while training, override the
\texttt{validation\_step()} method.

\index{validation\_step()}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LightningTransformer(L.LightningModule):}
    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{.model(inputs, target)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ F.cross\_entropy(y\_hat, y)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, loss)}
        \ControlFlowTok{return}\NormalTok{ loss}
\end{Highlighting}
\end{Shaded}

\subsubsection{Background}\label{background-1}

\begin{itemize}
\tightlist
\item
  You can also run just the validation loop on your validation
  dataloaders by overriding \texttt{validation\_step()} and calling
  \texttt{validate()}.
\end{itemize}

\index{validate()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ LightningTransformer(vocab\_size}\OperatorTok{=}\NormalTok{dataset.vocab\_size)}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer()}
\NormalTok{trainer.validate(model)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  In the case that you need to make use of all the outputs from each
  \texttt{validation\_step()}, override the
  \texttt{on\_validation\_epoch\_end()} method. Note that this method is
  called before \texttt{on\_train\_epoch\_end()}.
\end{itemize}

\index{on\_validation\_epoch\_end()}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LightningTransformer(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, vocab\_size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ Transformer(vocab\_size}\OperatorTok{=}\NormalTok{vocab\_size)}
        \VariableTok{self}\NormalTok{.validation\_step\_outputs }\OperatorTok{=}\NormalTok{ []}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{.model(inputs, target)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ torch.nn.functional.nll\_loss(output, target.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{        pred }\OperatorTok{=}\NormalTok{ ...}
        \VariableTok{self}\NormalTok{.validation\_step\_outputs.append(pred)}
        \ControlFlowTok{return}\NormalTok{ pred}

    \KeywordTok{def}\NormalTok{ on\_validation\_epoch\_end(}\VariableTok{self}\NormalTok{):}
\NormalTok{        all\_preds }\OperatorTok{=}\NormalTok{ torch.stack(}\VariableTok{self}\NormalTok{.validation\_step\_outputs)}
        \CommentTok{\# do something with all preds}
\NormalTok{        ...}
        \VariableTok{self}\NormalTok{.validation\_step\_outputs.clear()  }\CommentTok{\# free memory}
\end{Highlighting}
\end{Shaded}

\subsection{Test Step}\label{test-step}

The process for enabling a test loop is the same as the process for
enabling a validation loop. For this you need to override the
\texttt{test\_step()} method. The only difference is that the test loop
is only called when \texttt{test()} is used. \index{test\_step()}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{    inputs, target }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{    output }\OperatorTok{=} \VariableTok{self}\NormalTok{.model(inputs, target)}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ F.cross\_entropy(y\_hat, y)}
    \VariableTok{self}\NormalTok{.log(}\StringTok{"test\_loss"}\NormalTok{, loss)}
    \ControlFlowTok{return}\NormalTok{ loss}
\end{Highlighting}
\end{Shaded}

\subsection{Predict Step}\label{predict-step}

\subsubsection{Basics}\label{basics-2}

By default, the \texttt{predict\_step()} method runs the
\texttt{forward()} method. In order to customize this behaviour, simply
override the \texttt{predict\_step()} method.

\index{predict\_step()}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LightningTransformer(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, vocab\_size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ Transformer(vocab\_size}\OperatorTok{=}\NormalTok{vocab\_size)}

    \KeywordTok{def}\NormalTok{ predict\_step(}\VariableTok{self}\NormalTok{, batch):}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.model(inputs, target)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Background}\label{background-2}

\begin{itemize}
\tightlist
\item
  If you want to perform inference with the system, you can add a
  \texttt{forward} method to the LightningModule.
\item
  When using forward, you are responsible to call \texttt{eval()} and
  use the \texttt{no\_grad()} context manager.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LightningTransformer(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, vocab\_size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ Transformer(vocab\_size}\OperatorTok{=}\NormalTok{vocab\_size)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, batch):}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.model(inputs, target)}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        inputs, target }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        output }\OperatorTok{=} \VariableTok{self}\NormalTok{.model(inputs, target)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ torch.nn.functional.nll\_loss(output, target.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ torch.optim.SGD(}\VariableTok{self}\NormalTok{.model.parameters(), lr}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}


\NormalTok{model }\OperatorTok{=}\NormalTok{ LightningTransformer(vocab\_size}\OperatorTok{=}\NormalTok{dataset.vocab\_size)}

\NormalTok{model.}\BuiltInTok{eval}\NormalTok{()}
\ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
\NormalTok{    batch }\OperatorTok{=}\NormalTok{ dataloader.dataset[}\DecValTok{0}\NormalTok{]}
\NormalTok{    pred }\OperatorTok{=}\NormalTok{ model(batch)}
\end{Highlighting}
\end{Shaded}

\section{Lightning Extras}\label{lightning-extras}

This section covers some additional features of Lightning.

\subsection{Lightning: Save
Hyperparameters}\label{lightning-save-hyperparameters}

Often times we train many versions of a model. You might share that
model or come back to it a few months later at which point it is very
useful to know how that model was trained (i.e.: what learning rate,
neural network, etc.).

Lightning has a standardized way of saving the information for you in
checkpoints and YAML files. The goal here is to improve readability and
reproducibility.

Use \texttt{save\_hyperparameters()} within your
\texttt{LightningModule}'s \texttt{\_\_init\_\_} method.
\index{save\_hyperparameters()} It will enable Lightning to store all
the provided arguments under the \texttt{self.hparams} attribute. These
hyperparameters will also be stored within the model checkpoint, which
simplifies model re-instantiation after training.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ LitMNIST(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, layer\_1\_dim}\OperatorTok{=}\DecValTok{128}\NormalTok{, learning\_rate}\OperatorTok{=}\FloatTok{1e{-}2}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \CommentTok{\# call this to save (layer\_1\_dim=128, learning\_rate=1e{-}4) to the checkpoint}
        \VariableTok{self}\NormalTok{.save\_hyperparameters()}

        \CommentTok{\# equivalent}
        \VariableTok{self}\NormalTok{.save\_hyperparameters(}\StringTok{"layer\_1\_dim"}\NormalTok{, }\StringTok{"learning\_rate"}\NormalTok{)}

        \CommentTok{\# Now possible to access layer\_1\_dim from hparams}
        \VariableTok{self}\NormalTok{.hparams.layer\_1\_dim}
\end{Highlighting}
\end{Shaded}

\subsection{Lightning: Model Loading}\label{lightning-model-loading}

LightningModules that have hyperparameters automatically saved with
\texttt{save\_hyperparameters()} can conveniently be loaded and
instantiated directly from a checkpoint with
\texttt{load\_from\_checkpoint()}:

\index{load\_from\_checkpoint()}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to load specify the other args}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LitMNIST.load\_from\_checkpoint(PATH, loss\_fx}\OperatorTok{=}\NormalTok{torch.nn.SomeOtherLoss, generator\_network}\OperatorTok{=}\NormalTok{MyGenerator())}
\end{Highlighting}
\end{Shaded}

\section{Starter Example: Linear Neural
Network}\label{starter-example-linear-neural-network}

We will use the \texttt{LightningModule} to create a simple neural
network for regression. It will be implemented as the
\texttt{LightningBasic} class.

\subsection{Hidden Layers}\label{hidden-layers}

To specify the number of hidden layers, we will use the hyperparameter
\texttt{l1} and the function \texttt{get\_hidden\_sizes()}
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/hyperparameters/architecture/\#spotpython.hyperparameters.architecture.get_hidden_sizes}{{[}DOC{]}}
from the \texttt{spotpython} package.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.architecture }\ImportTok{import}\NormalTok{ get\_hidden\_sizes}
\NormalTok{\_L\_in }\OperatorTok{=} \DecValTok{10}
\NormalTok{l1 }\OperatorTok{=} \DecValTok{20}
\NormalTok{max\_n }\OperatorTok{=} \DecValTok{4}
\NormalTok{get\_hidden\_sizes(\_L\_in, l1, max\_n)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{get_hidden_sizes}
\begin{verbatim}
[20, 10, 10, 5]
\end{verbatim}

\subsection{Hyperparameters}\label{hyperparameters}

The argument \texttt{l1} will be treated as a hyperparameter, so it will
be tuned in the following steps. Besides \texttt{l1}, additonal
hyperparameters are \texttt{act\_fn} and \texttt{dropout\_prob}.

The arguments \texttt{\_L\_in}, \texttt{\_L\_out}, and
\texttt{\_torchmetric} are not hyperparameters, but are needed to create
the network. The first two are specified by the data and the latter by
user preferences (the desired evaluation metric).

\subsection{The LightningBasic Class}\label{the-lightningbasic-class}

\phantomsection\label{lightning_starter_example_regression}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F}
\ImportTok{import}\NormalTok{ torchmetrics.functional.regression}
\ImportTok{from}\NormalTok{ torch }\ImportTok{import}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.architecture }\ImportTok{import}\NormalTok{ get\_hidden\_sizes}

\KeywordTok{class}\NormalTok{ LightningBasic(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
    \VariableTok{self}\NormalTok{,}
\NormalTok{    l1: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{    act\_fn: nn.Module,}
\NormalTok{    dropout\_prob: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{    \_L\_in: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{    \_L\_out: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{    \_torchmetric: }\BuiltInTok{str}\NormalTok{,}
    \OperatorTok{*}\NormalTok{args,}
    \OperatorTok{**}\NormalTok{kwargs):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{=}\NormalTok{ \_L\_in}
        \VariableTok{self}\NormalTok{.\_L\_out }\OperatorTok{=}\NormalTok{ \_L\_out}
        \VariableTok{self}\NormalTok{.\_torchmetric }\OperatorTok{=}\NormalTok{ \_torchmetric}
        \VariableTok{self}\NormalTok{.metric }\OperatorTok{=} \BuiltInTok{getattr}\NormalTok{(torchmetrics.functional.regression, \_torchmetric)}
        \CommentTok{\# \_L\_in and \_L\_out are not hyperparameters, but are needed to create the network}
        \CommentTok{\# \_torchmetric is not a hyperparameter, but is needed to calculate the loss}
        \VariableTok{self}\NormalTok{.save\_hyperparameters(ignore}\OperatorTok{=}\NormalTok{[}\StringTok{"\_L\_in"}\NormalTok{, }\StringTok{"\_L\_out"}\NormalTok{, }\StringTok{"\_torchmetric"}\NormalTok{])}
        \CommentTok{\# set dummy input array for Tensorboard Graphs}
        \CommentTok{\# set log\_graph=True in Trainer to see the graph (in traintest.py)}
\NormalTok{        hidden\_sizes }\OperatorTok{=}\NormalTok{ get\_hidden\_sizes(\_L\_in}\OperatorTok{=}\VariableTok{self}\NormalTok{.\_L\_in, l1}\OperatorTok{=}\NormalTok{l1, max\_n}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
        \CommentTok{\# Create the network based on the specified hidden sizes}
\NormalTok{        layers }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        layer\_sizes }\OperatorTok{=}\NormalTok{ [}\VariableTok{self}\NormalTok{.\_L\_in] }\OperatorTok{+}\NormalTok{ hidden\_sizes}
\NormalTok{        layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_sizes[}\DecValTok{0}\NormalTok{]}
        \ControlFlowTok{for}\NormalTok{ layer\_size }\KeywordTok{in}\NormalTok{ layer\_sizes[}\DecValTok{1}\NormalTok{:]:}
\NormalTok{            layers }\OperatorTok{+=}\NormalTok{ [}
\NormalTok{                nn.Linear(layer\_size\_last, layer\_size),}
                \VariableTok{self}\NormalTok{.hparams.act\_fn,}
\NormalTok{                nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.dropout\_prob),}
\NormalTok{            ]}
\NormalTok{            layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_size}
\NormalTok{        layers }\OperatorTok{+=}\NormalTok{ [nn.Linear(layer\_sizes[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.\_L\_out)]}
        \CommentTok{\# nn.Sequential summarizes a list of modules into a single module,}
        \CommentTok{\# applying them in sequence}
        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{layers)}

    \KeywordTok{def}\NormalTok{ \_calculate\_loss(}\VariableTok{self}\NormalTok{, batch):}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        y\_hat }\OperatorTok{=} \VariableTok{self}\NormalTok{.layers(x)}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.metric(y\_hat, y)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.layers(x)}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"train\_loss"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{True}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \CommentTok{\# logs metrics for each training\_step,}
        \CommentTok{\# and the average across the epoch, to the progress bar and logger}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{True}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \CommentTok{\# logs metrics for each training\_step,}
        \CommentTok{\# and the average across the epoch, to the progress bar and logger}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"test\_loss"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{True}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ predict\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx, dataloader\_idx}\OperatorTok{=}\DecValTok{0}\NormalTok{):}
\NormalTok{        x, \_ }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        y\_hat }\OperatorTok{=} \VariableTok{self}\NormalTok{.layers(x)}
        \ControlFlowTok{return}\NormalTok{ y\_hat}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ torch.optim.Adam(}\VariableTok{self}\NormalTok{.layers.parameters(), lr}\OperatorTok{=}\FloatTok{0.02}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can instantiate the \texttt{LightningBasic} class as follows:

\phantomsection\label{lightning_starter_instantiate}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_base }\OperatorTok{=}\NormalTok{ LightningBasic(}
\NormalTok{    l1}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    act\_fn}\OperatorTok{=}\NormalTok{nn.ReLU(),}
\NormalTok{    dropout\_prob}\OperatorTok{=}\FloatTok{0.01}\NormalTok{,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    \_torchmetric}\OperatorTok{=}\StringTok{"mean\_squared\_error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It has the following structure:

\phantomsection\label{lightning_starter_print_model}
\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(model\_base)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightningBasic(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=20, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.01, inplace=False)
    (3): Linear(in_features=20, out_features=10, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.01, inplace=False)
    (6): Linear(in_features=10, out_features=10, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.01, inplace=False)
    (9): Linear(in_features=10, out_features=5, bias=True)
    (10): ReLU()
    (11): Dropout(p=0.01, inplace=False)
    (12): Linear(in_features=5, out_features=1, bias=True)
  )
)
\end{verbatim}

\phantomsection\label{lightning_starter_model_architecture_plot}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.xai }\ImportTok{import}\NormalTok{ viz\_net}
\NormalTok{viz\_net(net}\OperatorTok{=}\NormalTok{model\_base,}
\NormalTok{    device}\OperatorTok{=}\StringTok{"cpu"}\NormalTok{,}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"model\_architecture700"}\NormalTok{, }\BuiltInTok{format}\OperatorTok{=}\StringTok{"png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{./model_architecture700.png}}

}

\caption{Model architecture}

\end{figure}%

\subsection{The Data Set: Diabetes}\label{the-data-set-diabetes}

We will use the \texttt{Diabetes}
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/data/diabetes/}{{[}DOC{]}}
data set from the \texttt{spotpython} package, which is a PyTorch
Dataset for regression based on a data set from \texttt{scikit-learn}.
It consists of DataFrame entries, which were converted to PyTorch
tensors.

Ten baseline variables, age, sex, body mass index, average blood
pressure, and six blood serum measurements were obtained for each of n =
442 diabetes patients, as well as the response of interest, a
quantitative measure of disease progression one year after baseline.

The \texttt{Diabetes} data set has the following properties:

\begin{itemize}
\tightlist
\item
  Number of Instances: 442
\item
  Number of Attributes: First 10 columns are numeric predictive values.
\item
  Target: Column 11 is a quantitative measure of disease progression one
  year after baseline.
\item
  Attribute Information:

  \begin{itemize}
  \tightlist
  \item
    age age in years
  \item
    sex
  \item
    bmi body mass index
  \item
    bp average blood pressure
  \item
    s1 tc, total serum cholesterol
  \item
    s2 ldl, low-density lipoproteins
  \item
    s3 hdl, high-density lipoproteins
  \item
    s4 tch, total cholesterol / HDL
  \item
    s5 ltg, possibly log of serum triglycerides level
  \item
    s6 glu, blood sugar level
  \end{itemize}
\end{itemize}

\phantomsection\label{lightning_starter_diabetes_dataset}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(feature\_type}\OperatorTok{=}\NormalTok{torch.float32, target\_type}\OperatorTok{=}\NormalTok{torch.float32)}
\CommentTok{\# Set batch size for DataLoader to 2 for demonstration purposes}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{2}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Batch Size: 2
---------------
Inputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,
          0.0199, -0.0176],
        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,
         -0.0683, -0.0922]])
Targets: tensor([151.,  75.])
\end{verbatim}

\subsection{The DataLoaders}\label{the-dataloaders}

Before we can call the \texttt{Trainer} to fit, validate, and test the
model, we need to create the \texttt{DataLoaders} for each of these
steps. The \texttt{DataLoaders} are used to load the data into the model
in batches and need the \texttt{batch\_size}.

\index{DataLoaders} \index{batch\_size}

\phantomsection\label{lightning_starter_dataloaders}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}

\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{8}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{train1\_set, test\_set }\OperatorTok{=}\NormalTok{ torch.utils.data.random\_split(dataset, [}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.4}\NormalTok{])}
\NormalTok{train\_set, val\_set }\OperatorTok{=}\NormalTok{ torch.utils.data.random\_split(train1\_set, [}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.4}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Full Data Set: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Train Set: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_set)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Validation Set: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(val\_set)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test Set: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(test\_set)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{train\_loader }\OperatorTok{=}\NormalTok{ DataLoader(train\_set, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{, drop\_last}\OperatorTok{=}\VariableTok{True}\NormalTok{, pin\_memory}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{test\_loader }\OperatorTok{=}\NormalTok{ DataLoader(test\_set, batch\_size}\OperatorTok{=}\NormalTok{batch\_size)}
\NormalTok{val\_loader }\OperatorTok{=}\NormalTok{ DataLoader(val\_set, batch\_size}\OperatorTok{=}\NormalTok{batch\_size)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Full Data Set: 442
Train Set: 160
Validation Set: 106
Test Set: 176
\end{verbatim}

\subsection{The Trainer}\label{the-trainer}

Now we are ready to train the model. We will use the \texttt{Trainer}
class from the \texttt{lightning} package. For demonstration purposes,
we will train the model for 100 epochs only.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{epochs }\OperatorTok{=} \DecValTok{100}

\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(max\_epochs}\OperatorTok{=}\NormalTok{epochs, enable\_progress\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model\_base, train\_dataloaders}\OperatorTok{=}\NormalTok{train\_loader)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{lightning_starter_train}
\begin{verbatim}
Training: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainer.validate(model\_base, val\_loader)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{lightning_starter_test}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# automatically loads the best weights for you}
\NormalTok{out }\OperatorTok{=}\NormalTok{ trainer.test(model\_base, test\_loader, verbose}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{lightning_starter_test-1}
\begin{verbatim}
Testing: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

\phantomsection\label{lightning_starter_test-2}
\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â        Test metric        â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â      test_loss_epoch      â      2884.3681640625      â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat }\OperatorTok{=}\NormalTok{ trainer.predict(model\_base, test\_loader)}
\CommentTok{\# convert the list of tensors to a numpy array}
\NormalTok{yhat }\OperatorTok{=}\NormalTok{ torch.cat(yhat).numpy()}
\NormalTok{yhat.shape}
\end{Highlighting}
\end{Shaded}

\subsection{Using a DataModule}\label{using-a-datamodule}

Instead of creating the three \texttt{DataLoaders} manually, we can use
the \texttt{LightDataModule} class from the \texttt{spotpython} package.

\phantomsection\label{lightning_starter_datamodule}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\end{Highlighting}
\end{Shaded}

There is a minor difference in the sizes of the data sets due to the
random split as can be seen in the following code:

\phantomsection\label{lightning_starter_datamodule_print_sizes}
\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Full Data Set: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_train)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Validation set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_val)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_test)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Full Data Set: 442
Training set size: 160
Validation set size: 106
Test set size: 177
\end{verbatim}

The \texttt{DataModule} can be used to train the model as follows:

\phantomsection\label{lightning_starter_train_datamodule}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(max\_epochs}\OperatorTok{=}\NormalTok{epochs, enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model\_base, datamodule}\OperatorTok{=}\NormalTok{data\_module)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{lightning_starter_validate_datamodule}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainer.validate(model}\OperatorTok{=}\NormalTok{model\_base, datamodule}\OperatorTok{=}\NormalTok{data\_module, verbose}\OperatorTok{=}\VariableTok{True}\NormalTok{, ckpt\_path}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{lightning_starter_validate_datamodule-1}
\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â      Validate metric      â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â      val_loss_epoch       â      3337.6259765625      â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\phantomsection\label{lightning_starter_validate_datamodule-2}
\begin{verbatim}
[{'val_loss_epoch': 3337.6259765625}]
\end{verbatim}

\phantomsection\label{lightning_starter_test_datamodule}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainer.test(model}\OperatorTok{=}\NormalTok{model\_base, datamodule}\OperatorTok{=}\NormalTok{data\_module, verbose}\OperatorTok{=}\VariableTok{True}\NormalTok{, ckpt\_path}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{lightning_starter_test_datamodule-1}
\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â        Test metric        â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â      test_loss_epoch      â     2851.374267578125     â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\phantomsection\label{lightning_starter_test_datamodule-2}
\begin{verbatim}
[{'test_loss_epoch': 2851.374267578125}]
\end{verbatim}

\section{Using spotpython with Pytorch
Lightning}\label{using-spotpython-with-pytorch-lightning}

\phantomsection\label{lightning_starter_imports_spotpython}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table, print\_res\_table}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"700"}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    fun\_repeats}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set}\OperatorTok{=}\NormalTok{data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    noise}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    ocba\_delta }\OperatorTok{=} \DecValTok{1}\NormalTok{,  )}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{, repeats}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{print\_exp\_table(fun\_control)}

\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\NormalTok{spot\_tuner.plot\_progress()}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_36_12_0
Created spot_tensorboard_path: runs/spot_logs/700_maans08_2025-05-06_10-36-12 for SummaryWriter()
module_name: light
submodule_name: regression
model_name: NNLinearRegressor
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |     3   |   4     | transform_power_2_int |
| epochs         | int    | 4         |     3   |   7     | transform_power_2_int |
| batch_size     | int    | 4         |     4   |  11     | transform_power_2_int |
| act_fn         | factor | ReLU      |     0   |   5     | None                  |
| optimizer      | factor | SGD       |     0   |   2     | None                  |
| dropout_prob   | float  | 0.01      |     0   |   0.025 | None                  |
| lr_mult        | float  | 1.0       |     0.1 |  10     | None                  |
| patience       | int    | 2         |     2   |   3     | transform_power_2_int |
| batch_norm     | factor | 0         |     0   |   1     | None                  |
| initialization | factor | Default   |     0   |   4     | None                  |
Experiment saved to 700_exp.pkl
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23075.09765625, 'hp_metric': 23075.09765625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23175.75, 'hp_metric': 23175.75}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3110.947021484375, 'hp_metric': 3110.947021484375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3212.389404296875, 'hp_metric': 3212.389404296875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4957.79296875, 'hp_metric': 4957.79296875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4993.95263671875, 'hp_metric': 4993.95263671875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 24098.697265625, 'hp_metric': 24098.697265625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23989.583984375, 'hp_metric': 23989.583984375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22617.146484375, 'hp_metric': 22617.146484375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22949.05859375, 'hp_metric': 22949.05859375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4575.27001953125, 'hp_metric': 4575.27001953125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4273.986328125, 'hp_metric': 4273.986328125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 21430.103515625, 'hp_metric': 21430.103515625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 21671.5234375, 'hp_metric': 21671.5234375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3811.26171875, 'hp_metric': 3811.26171875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4142.75537109375, 'hp_metric': 4142.75537109375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20498.853515625, 'hp_metric': 20498.853515625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20785.01953125, 'hp_metric': 20785.01953125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 24013.759765625, 'hp_metric': 24013.759765625}
train_model result: {'val_loss': 23966.04296875, 'hp_metric': 23966.04296875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4272.42431640625, 'hp_metric': 4272.42431640625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23667.060546875, 'hp_metric': 23667.060546875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23240.486328125, 'hp_metric': 23240.486328125}
spotpython tuning: 3110.947021484375 [#---------] 7.75% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4224.5546875, 'hp_metric': 4224.5546875}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
spotpython tuning: 3110.947021484375 [#---------] 14.46% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4110.9140625, 'hp_metric': 4110.9140625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3379.241943359375, 'hp_metric': 3379.241943359375}
train_model result: {'val_loss': 3240.904541015625, 'hp_metric': 3240.904541015625}
spotpython tuning: 3110.947021484375 [##--------] 24.89% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3184.6337890625, 'hp_metric': 3184.6337890625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3268.97607421875, 'hp_metric': 3268.97607421875}
train_model result: {'val_loss': 3201.219970703125, 'hp_metric': 3201.219970703125}
spotpython tuning: 3110.947021484375 [###-------] 34.07% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2953.638916015625, 'hp_metric': 2953.638916015625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22122.26171875, 'hp_metric': 22122.26171875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22847.4296875, 'hp_metric': 22847.4296875}
spotpython tuning: 2953.638916015625 [#####-----] 52.45% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3112.65087890625, 'hp_metric': 3112.65087890625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5882.8837890625, 'hp_metric': 5882.8837890625}
train_model result: {'val_loss': 3231.22412109375, 'hp_metric': 3231.22412109375}
spotpython tuning: 2953.638916015625 [#######---] 70.45% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3024.216064453125, 'hp_metric': 3024.216064453125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5045.22265625, 'hp_metric': 5045.22265625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5491.28515625, 'hp_metric': 5491.28515625}
spotpython tuning: 2953.638916015625 [#########-] 87.58% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2919.887451171875, 'hp_metric': 2919.887451171875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3178.40576171875, 'hp_metric': 3178.40576171875}
train_model result: {'val_loss': 3674.564208984375, 'hp_metric': 3674.564208984375}
spotpython tuning: 2919.887451171875 [##########] 100.00% Done...

Experiment saved to 700_res.pkl
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{700_lightning_basic_files/figure-pdf/lightning_starter_full_spot-output-40.png}}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned              | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     4.0 | 3.0                | transform_power_2_int |         0.00 |         |
| epochs         | int    | 4         |     3.0 |     7.0 | 5.0                | transform_power_2_int |         0.00 |         |
| batch_size     | int    | 4         |     4.0 |    11.0 | 6.0                | transform_power_2_int |         0.00 |         |
| act_fn         | factor | ReLU      |     0.0 |     5.0 | ReLU               | None                  |         0.88 | .       |
| optimizer      | factor | SGD       |     0.0 |     2.0 | Adadelta           | None                  |         0.00 |         |
| dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.0184251494885258 | None                  |         0.00 |         |
| lr_mult        | float  | 1.0       |     0.1 |    10.0 | 3.1418668140600845 | None                  |         0.00 |         |
| patience       | int    | 2         |     2.0 |     3.0 | 3.0                | transform_power_2_int |         0.00 |         |
| batch_norm     | factor | 0         |     0.0 |     1.0 | 0                  | None                  |       100.00 | ***     |
| initialization | factor | Default   |     0.0 |     4.0 | kaiming_normal     | None                  |         0.22 | .       |
\end{verbatim}

\chapter{Details of the Lightning Module Integration in
spotpython}\label{details-of-the-lightning-module-integration-in-spotpython}

\section{Introduction}\label{introduction-3}

Based on the Diabetes Data set and the \texttt{NNLinearRegressor} model,
we will provide details on the integration of the Lightning module in
spotpython.

\begin{itemize}
\tightlist
\item
  Section~\ref{sec-hyperlight-fun}: The \texttt{Hyperlight} class
  provides the \texttt{fun} method, which takes \texttt{X} and
  \texttt{fun\_control} as arguments. It calls the \texttt{train\_model}
  method.
\item
  Section~\ref{sec-trainmodel}: The \texttt{train\_model} method trains
  the model and returns the loss.
\item
  Section~\ref{sec-trainer}: The \texttt{Trainer} class is used to train
  the model and validate it. It also uses the \texttt{LightDataModule}
  class to load the data.
\end{itemize}

\section{1.
spotpython.fun.hyperlight.HyperLight.fun()}\label{sec-hyperlight-fun}

The class \texttt{Hyperlight} provides the method \texttt{fun}, which
takes \texttt{X} (\texttt{np.ndarray}) and \texttt{fun\_control}
(\texttt{dict}) as arguments. It calls the

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"000"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,)}

\NormalTok{print\_exp\_table(fun\_control)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\CommentTok{\# set epochs to 2\^{}8:}
\CommentTok{\# X[0, 1] = 8}
\CommentTok{\# set patience to 2\^{}10:}
\CommentTok{\# X[0, 7] = 10}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"X: }\SpecialCharTok{\{}\NormalTok{X}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# combine X and X to a np.array with shape (2, n\_hyperparams)}
\CommentTok{\# so that two values are returned}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.vstack((X, X, X))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"X: }\SpecialCharTok{\{}\NormalTok{X}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper\_light }\OperatorTok{=}\NormalTok{ HyperLight(seed}\OperatorTok{=}\DecValTok{125}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{hyper\_light.fun(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Using the same seed:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper\_light }\OperatorTok{=}\NormalTok{ HyperLight(seed}\OperatorTok{=}\DecValTok{125}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{hyper\_light.fun(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Using a different seed:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper\_light }\OperatorTok{=}\NormalTok{ HyperLight(seed}\OperatorTok{=}\DecValTok{123}\NormalTok{, log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{hyper\_light.fun(X, fun\_control)}
\end{Highlighting}
\end{Shaded}

\section{2.
spotpython.light.trainmodel.train\_model()}\label{sec-trainmodel}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, generate\_one\_config\_from\_var\_dict, get\_var\_name}
\ImportTok{from}\NormalTok{ spotpython.light.trainmodel }\ImportTok{import}\NormalTok{ train\_model}
\ImportTok{import}\NormalTok{ pprint}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"000"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,)}

\NormalTok{print\_exp\_table(fun\_control)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\CommentTok{\# set epochs to 2\^{}8:}
\CommentTok{\# X[0, 1] = 8}
\CommentTok{\# set patience to 2\^{}10:}
\CommentTok{\# X[0, 7] = 10}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"X: }\SpecialCharTok{\{}\NormalTok{X}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# combine X and X to a np.array with shape (2, n\_hyperparams)}
\CommentTok{\# so that two values are returned}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.vstack((X, X))}
\NormalTok{var\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, get\_var\_name(fun\_control))}
\ControlFlowTok{for}\NormalTok{ config }\KeywordTok{in}\NormalTok{ generate\_one\_config\_from\_var\_dict(var\_dict, fun\_control):}
\NormalTok{    pprint.pprint(config)}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ train\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\section{3. Trainer: fit and validate}\label{sec-trainer}

\begin{itemize}
\tightlist
\item
  Generate the \texttt{config} dictionary:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, generate\_one\_config\_from\_var\_dict, get\_var\_name}
\ImportTok{from}\NormalTok{ spotpython.light.trainmodel }\ImportTok{import}\NormalTok{ train\_model}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"000"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,)}
\NormalTok{print\_exp\_table(fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\CommentTok{\# set epochs to 2\^{}8:}
\NormalTok{X[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\CommentTok{\# set patience to 2\^{}10:}
\NormalTok{X[}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"X: }\SpecialCharTok{\{}\NormalTok{X}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{var\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, get\_var\_name(fun\_control))}
\NormalTok{config }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(generate\_one\_config\_from\_var\_dict(var\_dict, fun\_control))[}\DecValTok{0}\NormalTok{]}
\NormalTok{config}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_L\_in }\OperatorTok{=} \DecValTok{10}
\NormalTok{\_L\_out }\OperatorTok{=} \DecValTok{1}
\NormalTok{\_L\_cond }\OperatorTok{=} \DecValTok{0}
\NormalTok{\_torchmetric }\OperatorTok{=} \StringTok{"mean\_squared\_error"}
\end{Highlighting}
\end{Shaded}

\subsection{Commented: Using the fun\_control
dictionary}\label{commented-using-the-fun_control-dictionary}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model = fun\_control["core\_model"](**config, \_L\_in=\_L\_in, \_L\_out=\_L\_out, \_L\_cond=\_L\_cond, \_torchmetric=\_torchmetric)}
\CommentTok{\# model}
\end{Highlighting}
\end{Shaded}

\subsection{Using the source code:}\label{using-the-source-code}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch }\ImportTok{import}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.optimizer }\ImportTok{import}\NormalTok{ optimizer\_handler}
\ImportTok{import}\NormalTok{ torchmetrics.functional.regression}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.architecture }\ImportTok{import}\NormalTok{ get\_hidden\_sizes}


\KeywordTok{class}\NormalTok{ NNLinearRegressor(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        l1: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        epochs: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        batch\_size: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        initialization: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        act\_fn: nn.Module,}
\NormalTok{        optimizer: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        dropout\_prob: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        lr\_mult: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        patience: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        batch\_norm: }\BuiltInTok{bool}\NormalTok{,}
\NormalTok{        \_L\_in: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_out: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_torchmetric: }\BuiltInTok{str}\NormalTok{,}
        \OperatorTok{*}\NormalTok{args,}
        \OperatorTok{**}\NormalTok{kwargs,}
\NormalTok{    ):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \CommentTok{\# Attribute \textquotesingle{}act\_fn\textquotesingle{} is an instance of \textasciigrave{}nn.Module\textasciigrave{} and is already saved during}
        \CommentTok{\# checkpointing. It is recommended to ignore them}
        \CommentTok{\# using \textasciigrave{}self.save\_hyperparameters(ignore=[\textquotesingle{}act\_fn\textquotesingle{}])\textasciigrave{}}
        \CommentTok{\# self.save\_hyperparameters(ignore=["act\_fn"])}
        \CommentTok{\#}
        \VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{=}\NormalTok{ \_L\_in}
        \VariableTok{self}\NormalTok{.\_L\_out }\OperatorTok{=}\NormalTok{ \_L\_out}
        \ControlFlowTok{if}\NormalTok{ \_torchmetric }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            \_torchmetric }\OperatorTok{=} \StringTok{"mean\_squared\_error"}
        \VariableTok{self}\NormalTok{.\_torchmetric }\OperatorTok{=}\NormalTok{ \_torchmetric}
        \VariableTok{self}\NormalTok{.metric }\OperatorTok{=} \BuiltInTok{getattr}\NormalTok{(torchmetrics.functional.regression, \_torchmetric)}
        \CommentTok{\# \_L\_in and \_L\_out are not hyperparameters, but are needed to create the network}
        \CommentTok{\# \_torchmetric is not a hyperparameter, but is needed to calculate the loss}
        \VariableTok{self}\NormalTok{.save\_hyperparameters(ignore}\OperatorTok{=}\NormalTok{[}\StringTok{"\_L\_in"}\NormalTok{, }\StringTok{"\_L\_out"}\NormalTok{, }\StringTok{"\_torchmetric"}\NormalTok{])}
        \CommentTok{\# set dummy input array for Tensorboard Graphs}
        \CommentTok{\# set log\_graph=True in Trainer to see the graph (in traintest.py)}
        \VariableTok{self}\NormalTok{.example\_input\_array }\OperatorTok{=}\NormalTok{ torch.zeros((batch\_size, }\VariableTok{self}\NormalTok{.\_L\_in))}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.hparams.l1 }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"l1 must be at least 4"}\NormalTok{)}
\NormalTok{        hidden\_sizes }\OperatorTok{=}\NormalTok{ get\_hidden\_sizes(\_L\_in}\OperatorTok{=}\VariableTok{self}\NormalTok{.\_L\_in, l1}\OperatorTok{=}\NormalTok{l1, n}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

        \ControlFlowTok{if}\NormalTok{ batch\_norm:}
            \CommentTok{\# Add batch normalization layers}
\NormalTok{            layers }\OperatorTok{=}\NormalTok{ []}
\NormalTok{            layer\_sizes }\OperatorTok{=}\NormalTok{ [}\VariableTok{self}\NormalTok{.\_L\_in] }\OperatorTok{+}\NormalTok{ hidden\_sizes}
            \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(layer\_sizes) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
\NormalTok{                current\_layer\_size }\OperatorTok{=}\NormalTok{ layer\_sizes[i]}
\NormalTok{                next\_layer\_size }\OperatorTok{=}\NormalTok{ layer\_sizes[i }\OperatorTok{+} \DecValTok{1}\NormalTok{]}
\NormalTok{                layers }\OperatorTok{+=}\NormalTok{ [}
\NormalTok{                    nn.Linear(current\_layer\_size, next\_layer\_size),}
\NormalTok{                    nn.BatchNorm1d(next\_layer\_size),}
                    \VariableTok{self}\NormalTok{.hparams.act\_fn,}
\NormalTok{                    nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.dropout\_prob),}
\NormalTok{                ]}
\NormalTok{            layers }\OperatorTok{+=}\NormalTok{ [nn.Linear(layer\_sizes[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.\_L\_out)]}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            layers }\OperatorTok{=}\NormalTok{ []}
\NormalTok{            layer\_sizes }\OperatorTok{=}\NormalTok{ [}\VariableTok{self}\NormalTok{.\_L\_in] }\OperatorTok{+}\NormalTok{ hidden\_sizes}
            \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(layer\_sizes) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
\NormalTok{                current\_layer\_size }\OperatorTok{=}\NormalTok{ layer\_sizes[i]}
\NormalTok{                next\_layer\_size }\OperatorTok{=}\NormalTok{ layer\_sizes[i }\OperatorTok{+} \DecValTok{1}\NormalTok{]}
\NormalTok{                layers }\OperatorTok{+=}\NormalTok{ [}
\NormalTok{                    nn.Linear(current\_layer\_size, next\_layer\_size),}
                    \VariableTok{self}\NormalTok{.hparams.act\_fn,}
\NormalTok{                    nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.dropout\_prob),}
\NormalTok{                ]}
\NormalTok{            layers }\OperatorTok{+=}\NormalTok{ [nn.Linear(layer\_sizes[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.\_L\_out)]}

        \CommentTok{\# Wrap the layers into a sequential container}
        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{layers)}

        \CommentTok{\# Initialization (Xavier, Kaiming, or Default)}
        \VariableTok{self}\NormalTok{.}\BuiltInTok{apply}\NormalTok{(}\VariableTok{self}\NormalTok{.\_init\_weights)}

    \KeywordTok{def}\NormalTok{ \_init\_weights(}\VariableTok{self}\NormalTok{, module):}
        \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(module, nn.Linear):}
            \ControlFlowTok{if} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"xavier\_uniform"}\NormalTok{:}
\NormalTok{                nn.init.xavier\_uniform\_(module.weight)}
            \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"xavier\_normal"}\NormalTok{:}
\NormalTok{                nn.init.xavier\_normal\_(module.weight)}
            \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"kaiming\_uniform"}\NormalTok{:}
\NormalTok{                nn.init.kaiming\_uniform\_(module.weight)}
            \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"kaiming\_normal"}\NormalTok{:}
\NormalTok{                nn.init.kaiming\_normal\_(module.weight)}
            \ControlFlowTok{else}\NormalTok{:  }\CommentTok{\# "Default"}
\NormalTok{                nn.init.uniform\_(module.weight)}
            \ControlFlowTok{if}\NormalTok{ module.bias }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{                nn.init.zeros\_(module.bias)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a forward pass through the model.}

\CommentTok{        Args:}
\CommentTok{            x (torch.Tensor): A tensor containing a batch of input data.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the output of the model.}

\CommentTok{        """}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.layers(x)}
        \ControlFlowTok{return}\NormalTok{ x}

    \KeywordTok{def}\NormalTok{ \_calculate\_loss(}\VariableTok{self}\NormalTok{, batch):}
        \CommentTok{"""}
\CommentTok{        Calculate the loss for the given batch.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            mode (str, optional): The mode of the model. Defaults to "train".}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        y\_hat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.metric(y\_hat, y)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single training step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"train\_loss"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{False}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single validation step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{False}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{False}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single test step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}
\CommentTok{        """}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{False}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, loss, on\_step}\OperatorTok{=}\VariableTok{False}\NormalTok{, on\_epoch}\OperatorTok{=}\VariableTok{True}\NormalTok{, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ predict\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single prediction step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the prediction for this batch.}
\CommentTok{        """}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        yhat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        yhat }\OperatorTok{=}\NormalTok{ yhat.view(}\BuiltInTok{len}\NormalTok{(yhat), }\DecValTok{1}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step x: }\SpecialCharTok{\{}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step y: }\SpecialCharTok{\{}\NormalTok{y}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step y\_hat: }\SpecialCharTok{\{}\NormalTok{yhat}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \CommentTok{\# pred\_loss = F.mse\_loss(y\_hat, y)}
        \CommentTok{\# pred loss not registered}
        \CommentTok{\# self.log("pred\_loss", pred\_loss, prog\_bar=prog\_bar)}
        \CommentTok{\# self.log("hp\_metric", pred\_loss, prog\_bar=prog\_bar)}
        \CommentTok{\# MisconfigurationException: You are trying to \textasciigrave{}self.log()\textasciigrave{}}
        \CommentTok{\# but the loop\textquotesingle{}s result collection is not registered yet.}
        \CommentTok{\# This is most likely because you are trying to log in a \textasciigrave{}predict\textasciigrave{} hook, but it doesn\textquotesingle{}t support logging.}
        \CommentTok{\# If you want to manually log, please consider using \textasciigrave{}self.log\_dict(\{\textquotesingle{}pred\_loss\textquotesingle{}: pred\_loss\})\textasciigrave{} instead.}
        \ControlFlowTok{return}\NormalTok{ (x, y, yhat)}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.optim.Optimizer:}
        \CommentTok{"""}
\CommentTok{        Configures the optimizer for the model.}

\CommentTok{        Notes:}
\CommentTok{            The default Lightning way is to define an optimizer as}
\CommentTok{            \textasciigrave{}optimizer = torch.optim.Adam(self.parameters(), lr=self.learning\_rate)\textasciigrave{}.}
\CommentTok{            spotpython uses an optimizer handler to create the optimizer, which}
\CommentTok{            adapts the learning rate according to the lr\_mult hyperparameter as}
\CommentTok{            well as other hyperparameters. See \textasciigrave{}spotpython.hyperparameters.optimizer.py\textasciigrave{} for details.}

\CommentTok{        Returns:}
\CommentTok{            torch.optim.Optimizer: The optimizer to use during training.}

\CommentTok{        """}
        \CommentTok{\# optimizer = torch.optim.Adam(self.parameters(), lr=self.learning\_rate)}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ optimizer\_handler(optimizer\_name}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.optimizer, params}\OperatorTok{=}\VariableTok{self}\NormalTok{.parameters(), lr\_mult}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.lr\_mult)}

\NormalTok{        num\_milestones }\OperatorTok{=} \DecValTok{3}  \CommentTok{\# Number of milestones to divide the epochs}
\NormalTok{        milestones }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(}\VariableTok{self}\NormalTok{.hparams.epochs }\OperatorTok{/}\NormalTok{ (num\_milestones }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ (i }\OperatorTok{+} \DecValTok{1}\NormalTok{)) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_milestones)]}
\NormalTok{        scheduler }\OperatorTok{=}\NormalTok{ optim.lr\_scheduler.MultiStepLR(optimizer, milestones}\OperatorTok{=}\NormalTok{milestones, gamma}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Decay factor}

\NormalTok{        lr\_scheduler\_config }\OperatorTok{=}\NormalTok{ \{}
            \StringTok{"scheduler"}\NormalTok{: scheduler,}
            \StringTok{"interval"}\NormalTok{: }\StringTok{"epoch"}\NormalTok{,}
            \StringTok{"frequency"}\NormalTok{: }\DecValTok{1}\NormalTok{,}
\NormalTok{        \}}

        \ControlFlowTok{return}\NormalTok{ \{}\StringTok{"optimizer"}\NormalTok{: optimizer, }\StringTok{"lr\_scheduler"}\NormalTok{: lr\_scheduler\_config\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ NNLinearRegressor(}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\NormalTok{\_L\_in, \_L\_out}\OperatorTok{=}\NormalTok{\_L\_out, \_L\_cond}\OperatorTok{=}\NormalTok{\_L\_cond, \_torchmetric}\OperatorTok{=}\NormalTok{\_torchmetric)}
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{dm }\OperatorTok{=}\NormalTok{ LightDataModule(}
\NormalTok{    dataset}\OperatorTok{=}\NormalTok{data\_set,}
\NormalTok{    batch\_size}\OperatorTok{=}\NormalTok{config[}\StringTok{"batch\_size"}\NormalTok{],}
\NormalTok{    test\_size}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_size"}\NormalTok{],}
\NormalTok{    test\_seed}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_seed"}\NormalTok{],}
\NormalTok{    scaler}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Using \texttt{callbacks} for early stopping:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ lightning.pytorch.callbacks.early\_stopping }\ImportTok{import}\NormalTok{ EarlyStopping}
\NormalTok{callbacks }\OperatorTok{=}\NormalTok{ [EarlyStopping(monitor}\OperatorTok{=}\StringTok{"val\_loss"}\NormalTok{, patience}\OperatorTok{=}\NormalTok{config[}\StringTok{"patience"}\NormalTok{], mode}\OperatorTok{=}\StringTok{"min"}\NormalTok{, strict}\OperatorTok{=}\VariableTok{False}\NormalTok{, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timestamp }\OperatorTok{=} \VariableTok{True}

\ImportTok{from}\NormalTok{ lightning.pytorch.callbacks }\ImportTok{import}\NormalTok{ ModelCheckpoint}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ timestamp:}
    \CommentTok{\# add ModelCheckpoint only if timestamp is False}
\NormalTok{    callbacks.append(ModelCheckpoint(dirpath}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id), save\_last}\OperatorTok{=}\VariableTok{True}\NormalTok{))  }\CommentTok{\# Save the last checkpoint}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ generate\_config\_id}
\ControlFlowTok{if}\NormalTok{ timestamp:}
    \CommentTok{\# config id is unique. Since the model is not loaded from a checkpoint,}
    \CommentTok{\# the config id is generated here with a timestamp.}
\NormalTok{    config\_id }\OperatorTok{=}\NormalTok{ generate\_config\_id(config, timestamp}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
    \CommentTok{\# config id is not time{-}dependent and therefore unique,}
    \CommentTok{\# so that the model can be loaded from a checkpoint,}
    \CommentTok{\# the config id is generated here without a timestamp.}
\NormalTok{    config\_id }\OperatorTok{=}\NormalTok{ generate\_config\_id(config, timestamp}\OperatorTok{=}\VariableTok{False}\NormalTok{) }\OperatorTok{+} \StringTok{"\_TRAIN"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pytorch\_lightning.loggers }\ImportTok{import}\NormalTok{ TensorBoardLogger}
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ os}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(}
    \CommentTok{\# Where to save models}
\NormalTok{    default\_root\_dir}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id),}
\NormalTok{    max\_epochs}\OperatorTok{=}\NormalTok{model.hparams.epochs,}
\NormalTok{    accelerator}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"accelerator"}\NormalTok{],}
\NormalTok{    devices}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"devices"}\NormalTok{],}
\NormalTok{    strategy}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"strategy"}\NormalTok{],}
\NormalTok{    num\_nodes}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_nodes"}\NormalTok{],}
\NormalTok{    precision}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"precision"}\NormalTok{],}
\NormalTok{    logger}\OperatorTok{=}\NormalTok{TensorBoardLogger(save\_dir}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"TENSORBOARD\_PATH"}\NormalTok{], version}\OperatorTok{=}\NormalTok{config\_id, default\_hp\_metric}\OperatorTok{=}\VariableTok{True}\NormalTok{, log\_graph}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"log\_graph"}\NormalTok{], name}\OperatorTok{=}\StringTok{""}\NormalTok{),}
\NormalTok{    callbacks}\OperatorTok{=}\NormalTok{callbacks,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    num\_sanity\_val\_steps}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_sanity\_val\_steps"}\NormalTok{],}
\NormalTok{    log\_every\_n\_steps}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"log\_every\_n\_steps"}\NormalTok{],}
\NormalTok{    gradient\_clip\_val}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{    gradient\_clip\_algorithm}\OperatorTok{=}\StringTok{"norm"}\NormalTok{,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm, ckpt\_path}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainer.validate(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm, verbose}\OperatorTok{=}\VariableTok{True}\NormalTok{, ckpt\_path}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{DataModule}\label{datamodule}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.csvdataset }\ImportTok{import}\NormalTok{ CSVDataset}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ CSVDataset(csv\_file}\OperatorTok{=}\StringTok{\textquotesingle{}data.csv\textquotesingle{}}\NormalTok{, target\_column}\OperatorTok{=}\StringTok{\textquotesingle{}prognosis\textquotesingle{}}\NormalTok{, feature\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{long}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_train)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Generate the \texttt{config} dictionary:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_default\_hyperparameters\_as\_array}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, generate\_one\_config\_from\_var\_dict, get\_var\_name}
\ImportTok{from}\NormalTok{ spotpython.light.trainmodel }\ImportTok{import}\NormalTok{ train\_model, generate\_config\_id\_with\_timestamp}
\ImportTok{from}\NormalTok{ pytorch\_lightning.loggers }\ImportTok{import}\NormalTok{ TensorBoardLogger}
\ImportTok{from}\NormalTok{ lightning.pytorch.callbacks.early\_stopping }\ImportTok{import}\NormalTok{ EarlyStopping}
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"000"}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,)}
\NormalTok{print\_exp\_table(fun\_control)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ get\_default\_hyperparameters\_as\_array(fun\_control)}
\CommentTok{\# set epochs to 2\^{}8:}
\NormalTok{X[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\CommentTok{\# set patience to 2\^{}10:}
\NormalTok{X[}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"X: }\SpecialCharTok{\{}\NormalTok{X}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{var\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, get\_var\_name(fun\_control))}
\NormalTok{config }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(generate\_one\_config\_from\_var\_dict(var\_dict, fun\_control))[}\DecValTok{0}\NormalTok{]}
\NormalTok{\_L\_in }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"\_L\_in"}\NormalTok{]}
\NormalTok{\_L\_out }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"\_L\_out"}\NormalTok{]}
\NormalTok{\_L\_cond }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"\_L\_cond"}\NormalTok{]}
\NormalTok{\_torchmetric }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"\_torchmetric"}\NormalTok{]}
\NormalTok{model }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\NormalTok{\_L\_in, \_L\_out}\OperatorTok{=}\NormalTok{\_L\_out, \_L\_cond}\OperatorTok{=}\NormalTok{\_L\_cond, \_torchmetric}\OperatorTok{=}\NormalTok{\_torchmetric)}
\NormalTok{dm }\OperatorTok{=}\NormalTok{ LightDataModule(}
\NormalTok{    dataset}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"data\_set"}\NormalTok{],}
\NormalTok{    batch\_size}\OperatorTok{=}\NormalTok{config[}\StringTok{"batch\_size"}\NormalTok{],}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_workers"}\NormalTok{],}
\NormalTok{    test\_size}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_size"}\NormalTok{],}
\NormalTok{    test\_seed}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_seed"}\NormalTok{],}
\NormalTok{    scaler}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"scaler"}\NormalTok{],}
\NormalTok{)}
\NormalTok{config\_id }\OperatorTok{=}\NormalTok{ generate\_config\_id\_with\_timestamp(config, timestamp}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{callbacks }\OperatorTok{=}\NormalTok{ [EarlyStopping(monitor}\OperatorTok{=}\StringTok{"val\_loss"}\NormalTok{, patience}\OperatorTok{=}\NormalTok{config[}\StringTok{"patience"}\NormalTok{], mode}\OperatorTok{=}\StringTok{"min"}\NormalTok{, strict}\OperatorTok{=}\VariableTok{False}\NormalTok{, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{)]}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(}
\NormalTok{    default\_root\_dir}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id),}
\NormalTok{    max\_epochs}\OperatorTok{=}\NormalTok{model.hparams.epochs,}
\NormalTok{    accelerator}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"accelerator"}\NormalTok{],}
\NormalTok{    devices}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"devices"}\NormalTok{],}
\NormalTok{    strategy}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"strategy"}\NormalTok{],}
\NormalTok{    num\_nodes}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_nodes"}\NormalTok{],}
\NormalTok{    precision}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"precision"}\NormalTok{],}
\NormalTok{    logger}\OperatorTok{=}\NormalTok{TensorBoardLogger(save\_dir}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"TENSORBOARD\_PATH"}\NormalTok{], version}\OperatorTok{=}\NormalTok{config\_id, default\_hp\_metric}\OperatorTok{=}\VariableTok{True}\NormalTok{, log\_graph}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"log\_graph"}\NormalTok{], name}\OperatorTok{=}\StringTok{""}\NormalTok{),}
\NormalTok{    callbacks}\OperatorTok{=}\NormalTok{callbacks,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    num\_sanity\_val\_steps}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_sanity\_val\_steps"}\NormalTok{],}
\NormalTok{    log\_every\_n\_steps}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"log\_every\_n\_steps"}\NormalTok{],}
\NormalTok{    gradient\_clip\_val}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{    gradient\_clip\_algorithm}\OperatorTok{=}\StringTok{"norm"}\NormalTok{,}
\NormalTok{)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm, ckpt\_path}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ assign\_values, generate\_one\_config\_from\_var\_dict, get\_var\_name}
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.utils.scaler }\ImportTok{import}\NormalTok{ TorchStandardScaler}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"000"}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{3.0e+00}\NormalTok{, }\FloatTok{5.0}\NormalTok{, }\FloatTok{4.0e+00}\NormalTok{, }\FloatTok{2.0e+00}\NormalTok{, }\FloatTok{1.1e+01}\NormalTok{, }\FloatTok{1.0e{-}02}\NormalTok{, }\FloatTok{1.0e+00}\NormalTok{, }\FloatTok{1.0e+01}\NormalTok{, }\FloatTok{0.0e+00}\NormalTok{,}
  \FloatTok{0.0e+00}\NormalTok{]])}
\NormalTok{var\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, get\_var\_name(fun\_control))}
\NormalTok{config }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(generate\_one\_config\_from\_var\_dict(var\_dict, fun\_control))[}\DecValTok{0}\NormalTok{]}
\NormalTok{\_torchmetric }\OperatorTok{=} \StringTok{"mean\_squared\_error"}
\NormalTok{model }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{, \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{, \_L\_cond}\OperatorTok{=}\VariableTok{None}\NormalTok{, \_torchmetric}\OperatorTok{=}\NormalTok{\_torchmetric)}
\NormalTok{dm }\OperatorTok{=}\NormalTok{ LightDataModule(}
\NormalTok{    dataset}\OperatorTok{=}\NormalTok{data\_set,}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{16}\NormalTok{,}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.6}\NormalTok{,}
\NormalTok{    scaler}\OperatorTok{=}\NormalTok{TorchStandardScaler())}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(}
\NormalTok{    max\_epochs}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm, ckpt\_path}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\NormalTok{trainer.validate(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm, ckpt\_path}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\chapter{User Specified Basic Lightning Module With
spotpython}\label{user-specified-basic-lightning-module-with-spotpython}

\section{Introduction}\label{introduction-4}

This chapter implements a user-defined DataModule and a user-defined
neural network. Remember, that a \texttt{LightningModule} organizes your
\texttt{PyTorch} code into six sections:

\begin{itemize}
\tightlist
\item
  Initialization (\texttt{\_\_init\_\_} and \texttt{setup()}).
\item
  Train Loop (\texttt{training\_step()})
\item
  Validation Loop (\texttt{validation\_step()})
\item
  Test Loop (\texttt{test\_step()})
\item
  Prediction Loop (\texttt{predict\_step()})
\item
  Optimizers and LR Schedulers (\texttt{configure\_optimizers()})
\end{itemize}

The \texttt{Trainer} automates every required step in a clear and
reproducible way. It is the most important part of PyTorch Lightning. It
is responsible for training, testing, and validating the model. The
\texttt{Lightning} core structure looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read\_pickle(}\StringTok{"./userData/Turbo\_Charger\_Data.pkl"}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"M"}\NormalTok{, }\StringTok{"R"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Features des DataFrames: }\SpecialCharTok{\{}\NormalTok{df}\SpecialCharTok{.}\NormalTok{columns}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(df.shape)}
\end{Highlighting}
\end{Shaded}

\subsection{Dataset}\label{dataset}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ LabelEncoder}
\ImportTok{from}\NormalTok{ lightning }\ImportTok{import}\NormalTok{ LightningDataModule}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ Dataset, DataLoader, random\_split}

\KeywordTok{class}\NormalTok{ UserDataset(Dataset):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, data, y\_varname}\OperatorTok{=}\StringTok{"N"}\NormalTok{, x\_varnames}\OperatorTok{=}\VariableTok{None}\NormalTok{, dtype}\OperatorTok{=}\NormalTok{torch.float32):}
        \CommentTok{"""}
\CommentTok{        Args:}
\CommentTok{            data (pd.DataFrame):}
\CommentTok{                The user data. for example,}
\CommentTok{                generated by the \textasciigrave{}preprocess\_data\textasciigrave{} function.}
\CommentTok{            y\_varname (str):}
\CommentTok{                The name of the target variable.}
\CommentTok{                Default is "N".}
\CommentTok{            x\_varnames (list):}
\CommentTok{                The names of the input variables.}
\CommentTok{                Default is \textasciigrave{}None\textasciigrave{}, which means all columns}
\CommentTok{                except the target variable are used.}
\CommentTok{            dtype (torch.dtype):}
\CommentTok{                The data type for the tensors.}
\CommentTok{                Default is \textasciigrave{}torch.float32\textasciigrave{}.}

\CommentTok{        Examples:}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} dataset = UserDataset(data)}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} x, y = dataset[0]}
\CommentTok{        """}
        \VariableTok{self}\NormalTok{.data }\OperatorTok{=}\NormalTok{ data.reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{if}\NormalTok{ x\_varnames }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
            \VariableTok{self}\NormalTok{.x\_varnames }\OperatorTok{=}\NormalTok{ x\_varnames}
        \ControlFlowTok{else}\NormalTok{:}
            \VariableTok{self}\NormalTok{.x\_varnames }\OperatorTok{=}\NormalTok{ [col }\ControlFlowTok{for}\NormalTok{ col }\KeywordTok{in} \VariableTok{self}\NormalTok{.data.columns }\ControlFlowTok{if}\NormalTok{ col }\OperatorTok{!=}\NormalTok{ y\_varname]}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"X variables: }\SpecialCharTok{\{}\VariableTok{self}\SpecialCharTok{.}\NormalTok{x\_varnames}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Y variable: }\SpecialCharTok{\{}\NormalTok{y\_varname}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \VariableTok{self}\NormalTok{.y\_varname }\OperatorTok{=}\NormalTok{ y\_varname}
        \VariableTok{self}\NormalTok{.dtype }\OperatorTok{=}\NormalTok{ dtype}
        \VariableTok{self}\NormalTok{.encoders }\OperatorTok{=}\NormalTok{ \{\}}

        \ControlFlowTok{for}\NormalTok{ var }\KeywordTok{in} \VariableTok{self}\NormalTok{.x\_varnames:}
            \ControlFlowTok{if} \VariableTok{self}\NormalTok{.data[var].dtype }\OperatorTok{==} \StringTok{"object"} \KeywordTok{or} \BuiltInTok{isinstance}\NormalTok{(}\VariableTok{self}\NormalTok{.data[var][}\DecValTok{0}\NormalTok{], }\BuiltInTok{str}\NormalTok{):}
\NormalTok{                le }\OperatorTok{=}\NormalTok{ LabelEncoder()}
                \VariableTok{self}\NormalTok{.data[var] }\OperatorTok{=}\NormalTok{ le.fit\_transform(}\VariableTok{self}\NormalTok{.data[var])}
                \VariableTok{self}\NormalTok{.encoders[var] }\OperatorTok{=}\NormalTok{ le}

        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.data[}\VariableTok{self}\NormalTok{.y\_varname].dtype }\OperatorTok{==} \StringTok{"object"} \KeywordTok{or} \BuiltInTok{isinstance}\NormalTok{(}\VariableTok{self}\NormalTok{.data[}\VariableTok{self}\NormalTok{.y\_varname][}\DecValTok{0}\NormalTok{], }\BuiltInTok{str}\NormalTok{):}
\NormalTok{            le }\OperatorTok{=}\NormalTok{ LabelEncoder()}
            \VariableTok{self}\NormalTok{.data[}\VariableTok{self}\NormalTok{.y\_varname] }\OperatorTok{=}\NormalTok{ le.fit\_transform(}\VariableTok{self}\NormalTok{.data[}\VariableTok{self}\NormalTok{.y\_varname])}
            \VariableTok{self}\NormalTok{.encoders[}\VariableTok{self}\NormalTok{.y\_varname] }\OperatorTok{=}\NormalTok{ le}

        \CommentTok{\# Convert entire dataset to tensors}
        \VariableTok{self}\NormalTok{.features }\OperatorTok{=}\NormalTok{ torch.tensor(}\VariableTok{self}\NormalTok{.data[}\VariableTok{self}\NormalTok{.x\_varnames].values, dtype}\OperatorTok{=}\VariableTok{self}\NormalTok{.dtype)}
        \VariableTok{self}\NormalTok{.targets }\OperatorTok{=}\NormalTok{ torch.tensor(}\VariableTok{self}\NormalTok{.data[}\VariableTok{self}\NormalTok{.y\_varname].values, dtype}\OperatorTok{=}\VariableTok{self}\NormalTok{.dtype)}

    \KeywordTok{def} \FunctionTok{\_\_len\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data)}

    \KeywordTok{def} \FunctionTok{\_\_getitem\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, idx):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.features[idx], }\VariableTok{self}\NormalTok{.targets[idx]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ UserDataset(df)}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ dataset[}\DecValTok{0}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(x)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\subsection{DataModule}\label{datamodule-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader, random\_split, TensorDataset}
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Optional}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ floor}


\KeywordTok{class}\NormalTok{ LightDataModule(L.LightningDataModule):}
    \CommentTok{"""}
\CommentTok{    A LightningDataModule for handling data.}

\CommentTok{    Args:}
\CommentTok{        batch\_size (int):}
\CommentTok{            The batch size. Required.}
\CommentTok{        dataset (torch.utils.data.Dataset, optional):}
\CommentTok{            The dataset from the torch.utils.data Dataset class.}
\CommentTok{            It must implement three functions: \_\_init\_\_, \_\_len\_\_, and \_\_getitem\_\_.}
\CommentTok{        test\_size (float, optional):}
\CommentTok{            The test size. If test\_size is float, then train\_size is 1 {-} test\_size.}
\CommentTok{            If test\_size is int, then train\_size is len(data\_full) {-} test\_size.}
\CommentTok{        test\_seed (int):}
\CommentTok{            The test seed. Defaults to 42.}
\CommentTok{        num\_workers (int):}
\CommentTok{            The number of workers. Defaults to 0.}
\CommentTok{        verbosity (int):}
\CommentTok{            The verbosity level. Defaults to 0.}

\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} from spotpython.data.lightdatamodule import LightDataModule}
\CommentTok{            from spotpython.data.csvdataset import CSVDataset}
\CommentTok{            from spotpython.utils.scaler import TorchStandardScaler}
\CommentTok{            import torch}
\CommentTok{            \# data.csv is simple csv file with 11 samples}
\CommentTok{            dataset = CSVDataset(csv\_file=\textquotesingle{}data.csv\textquotesingle{}, target\_column=\textquotesingle{}prognosis\textquotesingle{}, feature\_type=torch.long)}
\CommentTok{            scaler = TorchStandardScaler()}
\CommentTok{            data\_module = LightDataModule(dataset=dataset, batch\_size=5, test\_size=0.5, scaler=scaler)}
\CommentTok{            data\_module.setup()}
\CommentTok{            print(f"Training set size: \{len(data\_module.data\_train)\}")}
\CommentTok{            print(f"Validation set size: \{len(data\_module.data\_val)\}")}
\CommentTok{            print(f"Test set size: \{len(data\_module.data\_test)\}")}
\CommentTok{            full\_train\_size: 0.5}
\CommentTok{            val\_size: 0.25}
\CommentTok{            train\_size: 0.25}
\CommentTok{            test\_size: 0.5}
\CommentTok{            Training set size: 3}
\CommentTok{            Validation set size: 3}
\CommentTok{            Test set size: 6}

\CommentTok{    References:}
\CommentTok{        See https://lightning.ai/docs/pytorch/stable/data/datamodule.html}

\CommentTok{    """}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        batch\_size: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        dataset: Optional[}\BuiltInTok{object}\NormalTok{] }\OperatorTok{=} \VariableTok{None}\NormalTok{,}
\NormalTok{        test\_size: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \VariableTok{None}\NormalTok{,}
\NormalTok{        test\_seed: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{42}\NormalTok{,}
\NormalTok{        num\_workers: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{0}\NormalTok{,}
\NormalTok{        verbosity: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{0}\NormalTok{,}
\NormalTok{    ):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.batch\_size }\OperatorTok{=}\NormalTok{ batch\_size}
        \VariableTok{self}\NormalTok{.data\_full }\OperatorTok{=}\NormalTok{ dataset}
        \VariableTok{self}\NormalTok{.test\_size }\OperatorTok{=}\NormalTok{ test\_size}
        \VariableTok{self}\NormalTok{.test\_seed }\OperatorTok{=}\NormalTok{ test\_seed}
        \VariableTok{self}\NormalTok{.num\_workers }\OperatorTok{=}\NormalTok{ num\_workers}
        \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{=}\NormalTok{ verbosity}

    \KeywordTok{def}\NormalTok{ prepare\_data(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
        \CommentTok{"""Prepares the data for use."""}
        \CommentTok{\# download}
        \ControlFlowTok{pass}

    \KeywordTok{def}\NormalTok{ \_setup\_full\_data\_provided(}\VariableTok{self}\NormalTok{, stage) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
\NormalTok{        full\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_full)}
\NormalTok{        test\_size }\OperatorTok{=} \VariableTok{self}\NormalTok{.test\_size}

        \CommentTok{\# consider the case when test\_size is a float}
        \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(}\VariableTok{self}\NormalTok{.test\_size, }\BuiltInTok{float}\NormalTok{):}
\NormalTok{            full\_train\_size }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{{-}} \VariableTok{self}\NormalTok{.test\_size}
\NormalTok{            val\_size }\OperatorTok{=}\NormalTok{ full\_train\_size }\OperatorTok{*} \VariableTok{self}\NormalTok{.test\_size}
\NormalTok{            train\_size }\OperatorTok{=}\NormalTok{ full\_train\_size }\OperatorTok{{-}}\NormalTok{ val\_size}
        \ControlFlowTok{else}\NormalTok{:}
            \CommentTok{\# test\_size is an int, training size calculation directly based on it}
\NormalTok{            full\_train\_size }\OperatorTok{=}\NormalTok{ full\_size }\OperatorTok{{-}} \VariableTok{self}\NormalTok{.test\_size}
\NormalTok{            val\_size }\OperatorTok{=}\NormalTok{ floor(full\_train\_size }\OperatorTok{*} \VariableTok{self}\NormalTok{.test\_size }\OperatorTok{/}\NormalTok{ full\_size)}
\NormalTok{            train\_size }\OperatorTok{=}\NormalTok{ full\_size }\OperatorTok{{-}}\NormalTok{ val\_size }\OperatorTok{{-}}\NormalTok{ test\_size}

        \CommentTok{\# Assign train/val datasets for use in dataloaders}
        \ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"fit"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            generator\_fit }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(}\VariableTok{self}\NormalTok{.test\_seed)}
            \VariableTok{self}\NormalTok{.data\_train, }\VariableTok{self}\NormalTok{.data\_val, \_ }\OperatorTok{=}\NormalTok{ random\_split(}\VariableTok{self}\NormalTok{.data\_full, [train\_size, val\_size, test\_size], generator}\OperatorTok{=}\NormalTok{generator\_fit)}
            \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"train\_size: }\SpecialCharTok{\{}\NormalTok{train\_size}\SpecialCharTok{\}}\SpecialStringTok{, val\_size: }\SpecialCharTok{\{}\NormalTok{val\_size}\SpecialCharTok{\}}\SpecialStringTok{, test\_sie: }\SpecialCharTok{\{}\NormalTok{test\_size}\SpecialCharTok{\}}\SpecialStringTok{ for splitting train \& val data."}\NormalTok{)}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"train samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_train)}\SpecialCharTok{\}}\SpecialStringTok{, val samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_val)}\SpecialCharTok{\}}\SpecialStringTok{ generated for train \& val data."}\NormalTok{)}

        \CommentTok{\# Assign test dataset for use in dataloader(s)}
        \ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"test"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            generator\_test }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(}\VariableTok{self}\NormalTok{.test\_seed)}
            \VariableTok{self}\NormalTok{.data\_test, \_, \_ }\OperatorTok{=}\NormalTok{ random\_split(}\VariableTok{self}\NormalTok{.data\_full, [test\_size, train\_size, val\_size], generator}\OperatorTok{=}\NormalTok{generator\_test)}
            \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"train\_size: }\SpecialCharTok{\{}\NormalTok{train\_size}\SpecialCharTok{\}}\SpecialStringTok{, val\_size: }\SpecialCharTok{\{}\NormalTok{val\_size}\SpecialCharTok{\}}\SpecialStringTok{, test\_sie: }\SpecialCharTok{\{}\NormalTok{test\_size}\SpecialCharTok{\}}\SpecialStringTok{ for splitting test data."}\NormalTok{)}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"test samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_test)}\SpecialCharTok{\}}\SpecialStringTok{ generated for test data."}\NormalTok{)}

        \CommentTok{\# Assign pred dataset for use in dataloader(s)}
        \ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"predict"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            generator\_predict }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(}\VariableTok{self}\NormalTok{.test\_seed)}
            \VariableTok{self}\NormalTok{.data\_predict, \_, \_ }\OperatorTok{=}\NormalTok{ random\_split(}\VariableTok{self}\NormalTok{.data\_full, [test\_size, train\_size, val\_size], generator}\OperatorTok{=}\NormalTok{generator\_predict)}
            \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"train\_size: }\SpecialCharTok{\{}\NormalTok{train\_size}\SpecialCharTok{\}}\SpecialStringTok{, val\_size: }\SpecialCharTok{\{}\NormalTok{val\_size}\SpecialCharTok{\}}\SpecialStringTok{, test\_size (= predict\_size): }\SpecialCharTok{\{}\NormalTok{test\_size}\SpecialCharTok{\}}\SpecialStringTok{ for splitting predict data."}\NormalTok{)}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"predict samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_predict)}\SpecialCharTok{\}}\SpecialStringTok{ generated for train \& val data."}\NormalTok{)}
    

    \KeywordTok{def}\NormalTok{ setup(}\VariableTok{self}\NormalTok{, stage: Optional[}\BuiltInTok{str}\NormalTok{] }\OperatorTok{=} \VariableTok{None}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
        \CommentTok{"""}
\CommentTok{        Splits the data for use in training, validation, and testing.}
\CommentTok{        Uses torch.utils.data.random\_split() to split the data.}
\CommentTok{        Splitting is based on the test\_size and test\_seed.}
\CommentTok{        The test\_size can be a float or an int.}
\CommentTok{        If a spotpython scaler object is defined, the data will be scaled.}

\CommentTok{        Args:}
\CommentTok{            stage (Optional[str]):}
\CommentTok{                The current stage. Can be "fit" (for training and validation), "test" (testing),}
\CommentTok{                or None (for all three stages). Defaults to None.}

\CommentTok{        Examples:}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} from spotpython.data.lightdatamodule import LightDataModule}
\CommentTok{                from spotpython.data.csvdataset import CSVDataset}
\CommentTok{                import torch}
\CommentTok{                dataset = CSVDataset(csv\_file=\textquotesingle{}data.csv\textquotesingle{}, target\_column=\textquotesingle{}prognosis\textquotesingle{}, feature\_type=torch.long)}
\CommentTok{                data\_module = LightDataModule(dataset=dataset, batch\_size=5, test\_size=0.5)}
\CommentTok{                data\_module.setup()}
\CommentTok{                print(f"Training set size: \{len(data\_module.data\_train)\}")}
\CommentTok{                Training set size: 3}

\CommentTok{        """}
        \VariableTok{self}\NormalTok{.\_setup\_full\_data\_provided(stage)}


    \KeywordTok{def}\NormalTok{ train\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
        \CommentTok{"""}
\CommentTok{        Returns the training dataloader, i.e., a pytorch DataLoader instance}
\CommentTok{        using the training dataset.}

\CommentTok{        Returns:}
\CommentTok{            DataLoader: The training dataloader.}

\CommentTok{        Examples:}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} from spotpython.data.lightdatamodule import LightDataModule}
\CommentTok{                from spotpython.data.csvdataset import CSVDataset}
\CommentTok{                import torch}
\CommentTok{                dataset = CSVDataset(csv\_file=\textquotesingle{}data.csv\textquotesingle{}, target\_column=\textquotesingle{}prognosis\textquotesingle{}, feature\_type=torch.long)}
\CommentTok{                data\_module = LightDataModule(dataset=dataset, batch\_size=5, test\_size=0.5)}
\CommentTok{                data\_module.setup()}
\CommentTok{                print(f"Training set size: \{len(data\_module.data\_train)\}")}
\CommentTok{                Training set size: 3}

\CommentTok{        """}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule.train\_dataloader(). data\_train size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_train)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_train, batch\_size}\OperatorTok{=}\VariableTok{self}\NormalTok{.batch\_size, num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}

    \KeywordTok{def}\NormalTok{ val\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
        \CommentTok{"""}
\CommentTok{        Returns the validation dataloader, i.e., a pytorch DataLoader instance}
\CommentTok{        using the validation dataset.}

\CommentTok{        Returns:}
\CommentTok{            DataLoader: The validation dataloader.}

\CommentTok{        Examples:}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} from spotpython.data.lightdatamodule import LightDataModule}
\CommentTok{                from spotpython.data.csvdataset import CSVDataset}
\CommentTok{                import torch}
\CommentTok{                dataset = CSVDataset(csv\_file=\textquotesingle{}data.csv\textquotesingle{}, target\_column=\textquotesingle{}prognosis\textquotesingle{}, feature\_type=torch.long)}
\CommentTok{                data\_module = LightDataModule(dataset=dataset, batch\_size=5, test\_size=0.5)}
\CommentTok{                data\_module.setup()}
\CommentTok{                print(f"Training set size: \{len(data\_module.data\_val)\}")}
\CommentTok{                Training set size: 3}
\CommentTok{        """}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule.val\_dataloader(). Val. set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_val)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_val, batch\_size}\OperatorTok{=}\VariableTok{self}\NormalTok{.batch\_size, num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}

    \KeywordTok{def}\NormalTok{ test\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
        \CommentTok{"""}
\CommentTok{        Returns the test dataloader, i.e., a pytorch DataLoader instance}
\CommentTok{        using the test dataset.}

\CommentTok{        Returns:}
\CommentTok{            DataLoader: The test dataloader.}

\CommentTok{        Examples:}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} from spotpython.data.lightdatamodule import LightDataModule}
\CommentTok{                from spotpython.data.csvdataset import CSVDataset}
\CommentTok{                import torch}
\CommentTok{                dataset = CSVDataset(csv\_file=\textquotesingle{}data.csv\textquotesingle{}, target\_column=\textquotesingle{}prognosis\textquotesingle{}, feature\_type=torch.long)}
\CommentTok{                data\_module = LightDataModule(dataset=dataset, batch\_size=5, test\_size=0.5)}
\CommentTok{                data\_module.setup()}
\CommentTok{                print(f"Test set size: \{len(data\_module.data\_test)\}")}
\CommentTok{                Test set size: 6}

\CommentTok{        """}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule.test\_dataloader(). Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_test)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_test, batch\_size}\OperatorTok{=}\VariableTok{self}\NormalTok{.batch\_size, num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}

    \KeywordTok{def}\NormalTok{ predict\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
        \CommentTok{"""}
\CommentTok{        Returns the predict dataloader, i.e., a pytorch DataLoader instance}
\CommentTok{        using the predict dataset.}

\CommentTok{        Returns:}
\CommentTok{            DataLoader: The predict dataloader.}

\CommentTok{        Examples:}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} from spotpython.data.lightdatamodule import LightDataModule}
\CommentTok{                from spotpython.data.csvdataset import CSVDataset}
\CommentTok{                import torch}
\CommentTok{                dataset = CSVDataset(csv\_file=\textquotesingle{}data.csv\textquotesingle{}, target\_column=\textquotesingle{}prognosis\textquotesingle{}, feature\_type=torch.long)}
\CommentTok{                data\_module = LightDataModule(dataset=dataset, batch\_size=5, test\_size=0.5)}
\CommentTok{                data\_module.setup()}
\CommentTok{                print(f"Predict set size: \{len(data\_module.data\_predict)\}")}
\CommentTok{                Predict set size: 6}

\CommentTok{        """}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule.predict\_dataloader(). Predict set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_predict)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_predict, batch\_size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_predict), num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(batch\_size}\OperatorTok{=}\DecValTok{2}\NormalTok{, dataset}\OperatorTok{=}\NormalTok{dataset, test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{)}
\NormalTok{data\_module.setup()}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ data\_module.train\_dataloader():}
    \BuiltInTok{print}\NormalTok{(batch)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Number of input features: }\SpecialCharTok{\{}\NormalTok{batch[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\section{The Neural Network:
MyRegressor}\label{the-neural-network-myregressor}

\phantomsection\label{myregressor}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch }\ImportTok{import}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.optimizer }\ImportTok{import}\NormalTok{ optimizer\_handler}
\ImportTok{import}\NormalTok{ torchmetrics.functional.regression}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ ceil}

\KeywordTok{class}\NormalTok{ MyRegressor(L.LightningModule):}
    \CommentTok{"""}
\CommentTok{    A LightningModule class for a regression neural network model.}

\CommentTok{    Attributes:}
\CommentTok{        l1 (int):}
\CommentTok{            The number of neurons in the first hidden layer.}
\CommentTok{        epochs (int):}
\CommentTok{            The number of epochs to train the model for.}
\CommentTok{        batch\_size (int):}
\CommentTok{            The batch size to use during training.}
\CommentTok{        initialization (str):}
\CommentTok{            The initialization method to use for the weights.}
\CommentTok{        act\_fn (nn.Module):}
\CommentTok{            The activation function to use in the hidden layers.}
\CommentTok{        optimizer (str):}
\CommentTok{            The optimizer to use during training.}
\CommentTok{        dropout\_prob (float):}
\CommentTok{            The probability of dropping out a neuron during training.}
\CommentTok{        lr\_mult (float):}
\CommentTok{            The learning rate multiplier for the optimizer.}
\CommentTok{        patience (int):}
\CommentTok{            The number of epochs to wait before early stopping.}
\CommentTok{        \_L\_in (int):}
\CommentTok{            The number of input features.}
\CommentTok{        \_L\_out (int):}
\CommentTok{            The number of output classes.}
\CommentTok{        \_torchmetric (str):}
\CommentTok{            The metric to use for the loss function. If \textasciigrave{}None\textasciigrave{},}
\CommentTok{            then "mean\_squared\_error" is used.}
\CommentTok{        layers (nn.Sequential):}
\CommentTok{            The neural network model.}

\CommentTok{    """}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        l1: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        epochs: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        batch\_size: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        initialization: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        act\_fn: nn.Module,}
\NormalTok{        optimizer: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        dropout\_prob: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        lr\_mult: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        patience: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_in: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_out: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_torchmetric: }\BuiltInTok{str}\NormalTok{,}
        \OperatorTok{*}\NormalTok{args,}
        \OperatorTok{**}\NormalTok{kwargs,}
\NormalTok{    ):}
        \CommentTok{"""}
\CommentTok{        Initializes the MyRegressor object.}

\CommentTok{        Args:}
\CommentTok{            l1 (int):}
\CommentTok{                The number of neurons in the first hidden layer.}
\CommentTok{            epochs (int):}
\CommentTok{                The number of epochs to train the model for.}
\CommentTok{            batch\_size (int):}
\CommentTok{                The batch size to use during training.}
\CommentTok{            initialization (str):}
\CommentTok{                The initialization method to use for the weights.}
\CommentTok{            act\_fn (nn.Module):}
\CommentTok{                The activation function to use in the hidden layers.}
\CommentTok{            optimizer (str):}
\CommentTok{                The optimizer to use during training.}
\CommentTok{            dropout\_prob (float):}
\CommentTok{                The probability of dropping out a neuron during training.}
\CommentTok{            lr\_mult (float):}
\CommentTok{                The learning rate multiplier for the optimizer.}
\CommentTok{            patience (int):}
\CommentTok{                The number of epochs to wait before early stopping.}
\CommentTok{            \_L\_in (int):}
\CommentTok{                The number of input features. Not a hyperparameter, but needed to create the network.}
\CommentTok{            \_L\_out (int):}
\CommentTok{                The number of output classes. Not a hyperparameter, but needed to create the network.}
\CommentTok{            \_torchmetric (str):}
\CommentTok{                The metric to use for the loss function. If \textasciigrave{}None\textasciigrave{},}
\CommentTok{                then "mean\_squared\_error" is used.}

\CommentTok{        Returns:}
\CommentTok{            (NoneType): None}

\CommentTok{        Raises:}
\CommentTok{            ValueError: If l1 is less than 4.}

\CommentTok{        """}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{=}\NormalTok{ \_L\_in}
        \VariableTok{self}\NormalTok{.\_L\_out }\OperatorTok{=}\NormalTok{ \_L\_out}
        \ControlFlowTok{if}\NormalTok{ \_torchmetric }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            \_torchmetric }\OperatorTok{=} \StringTok{"mean\_squared\_error"}
        \VariableTok{self}\NormalTok{.\_torchmetric }\OperatorTok{=}\NormalTok{ \_torchmetric}
        \VariableTok{self}\NormalTok{.metric }\OperatorTok{=} \BuiltInTok{getattr}\NormalTok{(torchmetrics.functional.regression, \_torchmetric)}
        \CommentTok{\# \_L\_in and \_L\_out are not hyperparameters, but are needed to create the network}
        \CommentTok{\# \_torchmetric is not a hyperparameter, but is needed to calculate the loss}
        \VariableTok{self}\NormalTok{.save\_hyperparameters(ignore}\OperatorTok{=}\NormalTok{[}\StringTok{"\_L\_in"}\NormalTok{, }\StringTok{"\_L\_out"}\NormalTok{, }\StringTok{"\_torchmetric"}\NormalTok{])}
        \CommentTok{\# set dummy input array for Tensorboard Graphs}
        \CommentTok{\# set log\_graph=True in Trainer to see the graph (in traintest.py)}
        \VariableTok{self}\NormalTok{.example\_input\_array }\OperatorTok{=}\NormalTok{ torch.zeros((batch\_size, }\VariableTok{self}\NormalTok{.\_L\_in))}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.hparams.l1 }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"l1 must be at least 4"}\NormalTok{)}
\NormalTok{        hidden\_sizes }\OperatorTok{=}\NormalTok{ [l1 }\OperatorTok{*} \DecValTok{2}\NormalTok{, l1, ceil(l1}\OperatorTok{/}\DecValTok{2}\NormalTok{)]}
        \CommentTok{\# Create the network based on the specified hidden sizes}
\NormalTok{        layers }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        layer\_sizes }\OperatorTok{=}\NormalTok{ [}\VariableTok{self}\NormalTok{.\_L\_in] }\OperatorTok{+}\NormalTok{ hidden\_sizes}
\NormalTok{        layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_sizes[}\DecValTok{0}\NormalTok{]}
        \ControlFlowTok{for}\NormalTok{ layer\_size }\KeywordTok{in}\NormalTok{ layer\_sizes[}\DecValTok{1}\NormalTok{:]:}
\NormalTok{            layers }\OperatorTok{+=}\NormalTok{ [}
\NormalTok{                nn.Linear(layer\_size\_last, layer\_size),}
                \VariableTok{self}\NormalTok{.hparams.act\_fn,}
\NormalTok{                nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.dropout\_prob),}
\NormalTok{            ]}
\NormalTok{            layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_size}
\NormalTok{        layers }\OperatorTok{+=}\NormalTok{ [nn.Linear(layer\_sizes[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.\_L\_out)]}
        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{layers)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a forward pass through the model.}

\CommentTok{        Args:}
\CommentTok{            x (torch.Tensor): A tensor containing a batch of input data.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the output of the model.}

\CommentTok{        """}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.layers(x)}
        \ControlFlowTok{return}\NormalTok{ x}

    \KeywordTok{def}\NormalTok{ \_calculate\_loss(}\VariableTok{self}\NormalTok{, batch):}
        \CommentTok{"""}
\CommentTok{        Calculate the loss for the given batch.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        y\_hat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.metric(y\_hat, y)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single training step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single validation step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single test step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}
\CommentTok{        """}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ predict\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single prediction step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            A tuple containing the input data, the true labels, and the predicted values.}
\CommentTok{        """}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        yhat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        yhat }\OperatorTok{=}\NormalTok{ yhat.view(}\BuiltInTok{len}\NormalTok{(yhat), }\DecValTok{1}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step x: }\SpecialCharTok{\{}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step y: }\SpecialCharTok{\{}\NormalTok{y}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step y\_hat: }\SpecialCharTok{\{}\NormalTok{yhat}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ (x, y, yhat)}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.optim.Optimizer:}
        \CommentTok{"""}
\CommentTok{        Configures the optimizer for the model.}
\CommentTok{        Simple examples use the following code here:}
\CommentTok{        \textasciigrave{}optimizer = torch.optim.Adam(self.parameters(), lr=self.learning\_rate)\textasciigrave{}}

\CommentTok{        Notes:}
\CommentTok{            The default Lightning way is to define an optimizer as}
\CommentTok{            \textasciigrave{}optimizer = torch.optim.Adam(self.parameters(), lr=self.learning\_rate)\textasciigrave{}.}
\CommentTok{            spotpython uses an optimizer handler to create the optimizer, which}
\CommentTok{            adapts the learning rate according to the lr\_mult hyperparameter as}
\CommentTok{            well as other hyperparameters. See \textasciigrave{}spotpython.hyperparameters.optimizer.py\textasciigrave{} for details.}

\CommentTok{        Returns:}
\CommentTok{            torch.optim.Optimizer: The optimizer to use during training.}

\CommentTok{        """}
        \CommentTok{\# optimizer = torch.optim.Adam(self.parameters(), lr=self.learning\_rate)}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ optimizer\_handler(}
\NormalTok{            optimizer\_name}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.optimizer, params}\OperatorTok{=}\VariableTok{self}\NormalTok{.parameters(), lr\_mult}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.lr\_mult}
\NormalTok{        )}
        \ControlFlowTok{return}\NormalTok{ optimizer}
\end{Highlighting}
\end{Shaded}

\section{Calling the Neural Network With
spotpython}\label{calling-the-neural-network-with-spotpython}

\phantomsection\label{prefix_setup}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"702\_lightning\_user\_datamodule"}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{spotpython_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\NormalTok{sys.path.insert(}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}./userModel\textquotesingle{}}\NormalTok{)}
\ImportTok{import}\NormalTok{ my\_regressor}
\ImportTok{import}\NormalTok{ my\_hyper\_dict}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    fun\_repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{    accelerator}\OperatorTok{=}\StringTok{"cpu"}\NormalTok{,}
\NormalTok{    data\_module}\OperatorTok{=}\NormalTok{data\_module,}
\NormalTok{    \_L\_in}\OperatorTok{=}\NormalTok{dataset[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{].shape[}\DecValTok{0}\NormalTok{],}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    ocba\_delta}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    \_torchmetric}\OperatorTok{=}\StringTok{"mean\_squared\_error"}\NormalTok{,}
\NormalTok{    log\_level}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    verbosity}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              core\_model}\OperatorTok{=}\NormalTok{my\_regressor.MyRegressor,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{my\_hyper\_dict.MyHyperDict)}

\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"act\_fn"}\NormalTok{, [ }\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Swish"}\NormalTok{, }\StringTok{"LeakyReLU"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\CommentTok{\# set\_hyperparameter(fun\_control, "initialization", ["Default"])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}

\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control, surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{spot_tuner_run}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_experiment}
\ControlFlowTok{if}\NormalTok{ os.path.exists(}\StringTok{"spot\_"} \OperatorTok{+}\NormalTok{ PREFIX }\OperatorTok{+} \StringTok{"\_experiment.pickle"}\NormalTok{):}
\NormalTok{    (spot\_tuner, fun\_control, design\_control,}
\NormalTok{        surrogate\_control, optimizer\_control) }\OperatorTok{=}\NormalTok{ load\_experiment(PREFIX}\OperatorTok{=}\NormalTok{PREFIX)}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\section{Looking at the Results}\label{looking-at-the-results}

\subsection{Tuning Progress}\label{tuning-progress}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\phantomsection\label{plot_progress_1}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\subsection{Tuned Hyperparameters and Their
Importance}\label{tuned-hyperparameters-and-their-importance}

Results can be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-702}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

\chapter{HPT PyTorch Lightning:
Data}\label{sec-hpt-pytorch-lightning-data-30}

In this tutorial, we will show how \texttt{spotpython} can be integrated
into the \texttt{PyTorch} Lightning training workflow.

This chapter describes the data preparation and processing in
\texttt{spotpython}. The Diabetes data set is used as an example. This
is a PyTorch Dataset for regression. A toy data set from scikit-learn.
Ten baseline variables, age, sex, body mass index, average blood
pressure, and six blood serum measurements were obtained for each of n =
442 diabetes patients, as well as the response of interest, a
quantitative measure of disease progression one year after baseline.

\section{Setup}\label{sec-setup-30}

\begin{itemize}
\tightlist
\item
  Before we consider the detailed experimental setup, we select the
  parameters that affect run time, initial design size, etc.
\item
  The parameter \texttt{WORKERS} specifies the number of workers.
\item
  The prefix \texttt{PREFIX} is used for the experiment name and the
  name of the log file.
\item
  The parameter \texttt{DEVICE} specifies the device to use for
  training.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ spotpython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\NormalTok{WORKERS }\OperatorTok{=} \DecValTok{0}
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"030"}
\NormalTok{DEVICE }\OperatorTok{=}\NormalTok{ getDevice()}
\NormalTok{DEVICES }\OperatorTok{=} \DecValTok{1}
\NormalTok{TEST\_SIZE }\OperatorTok{=} \FloatTok{0.4}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: Device selection}]

\begin{itemize}
\tightlist
\item
  Although there are no .cuda() or .to(device) calls required, because
  Lightning does these for you, see
  \href{https://lightning.ai/docs/pytorch/stable/common/lightning_module.html}{LIGHTNINGMODULE},
  we would like to know which device is used. Threrefore, we imitate the
  LightningModule behaviour which selects the highest device.
\item
  The method \texttt{spotpython.utils.device.getDevice()} returns the
  device that is used by Lightning.
\end{itemize}

\end{tcolorbox}

\section{\texorpdfstring{Initialization of the \texttt{fun\_control}
Dictionary}{Initialization of the fun\_control Dictionary}}\label{initialization-of-the-fun_control-dictionary}

\texttt{spotpython} uses a Python dictionary for storing the information
required for the hyperparameter tuning process.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    \_torchmetric}\OperatorTok{=}\StringTok{"mean\_squared\_error"}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    device}\OperatorTok{=}\NormalTok{DEVICE,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{WORKERS,}
\NormalTok{    show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    test\_size}\OperatorTok{=}\NormalTok{TEST\_SIZE,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\section{Loading the Diabetes Data
Set}\label{loading-the-diabetes-data-set}

Here, we load the Diabetes data set from \texttt{spotpython}'s
\texttt{data} module.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(dataset))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
442
\end{verbatim}

\subsection{Data Set and Data Loader}\label{data-set-and-data-loader}

As shown below, a DataLoader from \texttt{torch.utils.data} can be used
to check the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,
          0.0199, -0.0176],
        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,
         -0.0683, -0.0922],
        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,
          0.0029, -0.0259],
        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,
          0.0227, -0.0094],
        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,
         -0.0320, -0.0466]])
Targets: tensor([151.,  75., 141., 206., 135.])
\end{verbatim}

\subsection{Preparing Training, Validation, and Test
Data}\label{preparing-training-validation-and-test-data}

The following code shows how to split the data into training,
validation, and test sets. Then a Lightning Trainer is used to train
(\texttt{fit}) the model, validate it, and test it.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.light.regression.netlightregression }\ImportTok{import}\NormalTok{ NetLightRegression}
\ImportTok{from}\NormalTok{ torch }\ImportTok{import}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{BATCH\_SIZE }\OperatorTok{=} \DecValTok{8}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{train1\_set, test\_set }\OperatorTok{=}\NormalTok{ torch.utils.data.random\_split(dataset, [}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.4}\NormalTok{])}
\NormalTok{train\_set, val\_set }\OperatorTok{=}\NormalTok{ torch.utils.data.random\_split(train1\_set, [}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.4}\NormalTok{])}
\NormalTok{train\_loader }\OperatorTok{=}\NormalTok{ DataLoader(train\_set, batch\_size}\OperatorTok{=}\NormalTok{BATCH\_SIZE, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{, drop\_last}\OperatorTok{=}\VariableTok{True}\NormalTok{, pin\_memory}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{test\_loader }\OperatorTok{=}\NormalTok{ DataLoader(test\_set, batch\_size}\OperatorTok{=}\NormalTok{BATCH\_SIZE)}
\NormalTok{val\_loader }\OperatorTok{=}\NormalTok{ DataLoader(val\_set, batch\_size}\OperatorTok{=}\NormalTok{BATCH\_SIZE)}
\NormalTok{batch\_x, batch\_y }\OperatorTok{=} \BuiltInTok{next}\NormalTok{(}\BuiltInTok{iter}\NormalTok{(train\_loader))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"batch\_x.shape: }\SpecialCharTok{\{}\NormalTok{batch\_x}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"batch\_y.shape: }\SpecialCharTok{\{}\NormalTok{batch\_y}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{net\_light\_base }\OperatorTok{=}\NormalTok{ NetLightRegression(l1}\OperatorTok{=}\DecValTok{128}\NormalTok{,}
\NormalTok{                                    epochs}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{                                    batch\_size}\OperatorTok{=}\NormalTok{BATCH\_SIZE,}
\NormalTok{                                    initialization}\OperatorTok{=}\StringTok{\textquotesingle{}Default\textquotesingle{}}\NormalTok{,}
\NormalTok{                                    act\_fn}\OperatorTok{=}\NormalTok{nn.ReLU(),}
\NormalTok{                                    optimizer}\OperatorTok{=}\StringTok{\textquotesingle{}Adam\textquotesingle{}}\NormalTok{,}
\NormalTok{                                    dropout\_prob}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{                                    lr\_mult}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{                                    patience}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                                    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{                                    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                    \_torchmetric}\OperatorTok{=}\StringTok{"mean\_squared\_error"}\NormalTok{)}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(max\_epochs}\OperatorTok{=}\DecValTok{10}\NormalTok{,  enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{trainer.fit(net\_light\_base, train\_loader)}
\NormalTok{trainer.validate(net\_light\_base, val\_loader)}
\NormalTok{trainer.test(net\_light\_base, test\_loader)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
batch_x.shape: torch.Size([8, 10])
batch_y.shape: torch.Size([8])
\end{verbatim}

\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â      Validate metric      â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â         hp_metric         â      32421.490234375      â
â         val_loss          â      32421.490234375      â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â        Test metric        â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â         hp_metric         â       25781.0234375       â
â         val_loss          â       25781.0234375       â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\begin{verbatim}
[{'val_loss': 25781.0234375, 'hp_metric': 25781.0234375}]
\end{verbatim}

\subsection{Dataset for spotpython}\label{dataset-for-spotpython}

\texttt{spotpython} handles the data set, which is added to the
\texttt{fun\_control} dictionary with the key \texttt{data\_set} as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_control\_key\_value}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{set\_control\_key\_value(control\_dict}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                        key}\OperatorTok{=}\StringTok{"data\_set"}\NormalTok{,}
\NormalTok{                        value}\OperatorTok{=}\NormalTok{dataset,}
\NormalTok{                        replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(dataset))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
442
\end{verbatim}

If the data set is in the \texttt{fun\_control} dictionary, it is used
to create a \texttt{LightDataModule} object. This object is used to
create the data loaders for the training, validation, and test sets.
Therefore, the following information must be provided in the
\texttt{fun\_control} dictionary:

\begin{itemize}
\tightlist
\item
  \texttt{data\_set}: the data set
\item
  \texttt{batch\_size}: the batch size
\item
  \texttt{num\_workers}: the number of workers
\item
  \texttt{test\_size}: the size of the test set
\item
  \texttt{test\_seed}: the seed for the test set
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    data\_set}\OperatorTok{=}\NormalTok{dataset,}
\NormalTok{    device}\OperatorTok{=}\StringTok{"cpu"}\NormalTok{,}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    num\_workers}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{,}
\NormalTok{    test\_seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,    }
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\NormalTok{dm }\OperatorTok{=}\NormalTok{ LightDataModule(}
\NormalTok{    dataset}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"data\_set"}\NormalTok{],}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{8}\NormalTok{,}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_workers"}\NormalTok{],}
\NormalTok{    test\_size}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_size"}\NormalTok{],}
\NormalTok{    test\_seed}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_seed"}\NormalTok{],}
\NormalTok{)}
\NormalTok{dm.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"train\_model(): Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(dm.data\_test)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"train\_model(): Train set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(dm.data\_train)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model(): Test set size: 177
train_model(): Train set size: 160
\end{verbatim}

\section{The LightDataModule}\label{the-lightdatamodule}

The steps described above are handled by the \texttt{LightDataModule}
class. This class is used to create the data loaders for the training,
validation, and test sets. The \texttt{LightDataModule} class is part of
the \texttt{spotpython} package. The \texttt{LightDataModule} class
provides the following methods:

\begin{itemize}
\tightlist
\item
  \texttt{prepare\_data()}: This method is used to prepare the data set.
\item
  \texttt{setup()}: This method is used to create the data loaders for
  the training, validation, and test sets.
\item
  \texttt{train\_dataloader()}: This method is used to return the data
  loader for the training set.
\item
  \texttt{val\_dataloader()}: This method is used to return the data
  loader for the validation set.
\item
  \texttt{test\_dataloader()}: This method is used to return the data
  loader for the test set.
\item
  \texttt{predict\_dataloader()}: This method is used to return the data
  loader for the prediction set.
\end{itemize}

\subsection{\texorpdfstring{The \texttt{prepare\_data()}
Method}{The prepare\_data() Method}}\label{the-prepare_data-method}

The \texttt{prepare\_data()} method is used to prepare the data set.
This method is called only once and on a single process. It can be used
to download the data set. In our case, the data set is already
available, so this method uses a simple \texttt{pass} statement.

\subsection{\texorpdfstring{The \texttt{setup()}
Method}{The setup() Method}}\label{the-setup-method}

Splits the data for use in training, validation, and testing. It uses
\texttt{torch.utils.data.random\_split()} to split the data. Splitting
is based on the \texttt{test\_size} and \texttt{test\_seed}. The
\texttt{test\_size} can be a float or an int.

\subsubsection{Determine the Sizes of the Data
Sets}\label{determine-the-sizes-of-the-data-sets}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ random\_split}
\NormalTok{data\_full }\OperatorTok{=}\NormalTok{ dataset}
\NormalTok{test\_size }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"test\_size"}\NormalTok{]}
\NormalTok{test\_seed}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_seed"}\NormalTok{]}
\CommentTok{\# if test\_size is float, then train\_size is 1 {-} test\_size}
\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(test\_size, }\BuiltInTok{float}\NormalTok{):}
\NormalTok{    full\_train\_size }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ test\_size, }\DecValTok{2}\NormalTok{)}
\NormalTok{    val\_size }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(full\_train\_size }\OperatorTok{*}\NormalTok{ test\_size, }\DecValTok{2}\NormalTok{)}
\NormalTok{    train\_size }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(full\_train\_size }\OperatorTok{{-}}\NormalTok{ val\_size, }\DecValTok{2}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
    \CommentTok{\# if test\_size is int, then train\_size is len(data\_full) {-} test\_size}
\NormalTok{    full\_train\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(data\_full) }\OperatorTok{{-}}\NormalTok{ test\_size}
\NormalTok{    val\_size }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(full\_train\_size }\OperatorTok{*}\NormalTok{ test\_size }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(data\_full))}
\NormalTok{    train\_size }\OperatorTok{=}\NormalTok{ full\_train\_size }\OperatorTok{{-}}\NormalTok{ val\_size}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): full\_train\_size: }\SpecialCharTok{\{}\NormalTok{full\_train\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): val\_size: }\SpecialCharTok{\{}\NormalTok{val\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): train\_size: }\SpecialCharTok{\{}\NormalTok{train\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): test\_size: }\SpecialCharTok{\{}\NormalTok{test\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule setup(): full_train_size: 0.6
LightDataModule setup(): val_size: 0.24
LightDataModule setup(): train_size: 0.36
LightDataModule setup(): test_size: 0.4
\end{verbatim}

\texttt{stage} is used to define the data set to be returned. The
\texttt{stage} can be \texttt{None}, \texttt{fit}, \texttt{test}, or
\texttt{predict}. If \texttt{stage} is \texttt{None}, the method returns
the training (\texttt{fit}), testing (\texttt{test}) and prediction
(\texttt{predict}) data sets.

\subsubsection{Stage ``fit''}\label{sec-stage-fit-30}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stage }\OperatorTok{=} \StringTok{"fit"}
\ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"fit"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{    generator\_fit }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(test\_seed)}
\NormalTok{    data\_train, data\_val, \_ }\OperatorTok{=}\NormalTok{ random\_split(data\_full, [train\_size, val\_size, test\_size], generator}\OperatorTok{=}\NormalTok{generator\_fit)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): Train set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_train)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): Validation set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_val)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule setup(): Train set size: 160
LightDataModule setup(): Validation set size: 106
\end{verbatim}

\subsubsection{Stage ``test''}\label{sec-stage-test-30}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stage }\OperatorTok{=} \StringTok{"test"}
\ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"test"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{    generator\_test }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(test\_seed)}
\NormalTok{    data\_test, \_ }\OperatorTok{=}\NormalTok{ random\_split(data\_full, [test\_size, full\_train\_size], generator}\OperatorTok{=}\NormalTok{generator\_test)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_test)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(data\_test, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule setup(): Test set size: 177
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,
         -0.0283,  0.0445],
        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,
         -0.0225,  0.0072],
        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,
         -0.0741, -0.0591],
        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,
         -0.0514, -0.0591],
        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,
         -0.0181, -0.0135]])
Targets: tensor([158.,  49., 142.,  96.,  59.])
\end{verbatim}

\subsubsection{Stage ``predict''}\label{sec-stage-predict-30}

Prediction and testing use the same data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stage }\OperatorTok{=} \StringTok{"predict"}
\ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"predict"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{    generator\_predict }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(test\_seed)}
\NormalTok{    data\_predict, \_ }\OperatorTok{=}\NormalTok{ random\_split(}
\NormalTok{        data\_full, [test\_size, full\_train\_size], generator}\OperatorTok{=}\NormalTok{generator\_predict}
\NormalTok{    )}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): Predict set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_predict)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(data\_predict, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule setup(): Predict set size: 177
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,
         -0.0283,  0.0445],
        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,
         -0.0225,  0.0072],
        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,
         -0.0741, -0.0591],
        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,
         -0.0514, -0.0591],
        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,
         -0.0181, -0.0135]])
Targets: tensor([158.,  49., 142.,  96.,  59.])
\end{verbatim}

\subsection{\texorpdfstring{The \texttt{train\_dataloader()}
Method}{The train\_dataloader() Method}}\label{the-train_dataloader-method}

Returns the training dataloader, i.e., a Pytorch DataLoader instance
using the training dataset. It simply returns a DataLoader with the
\texttt{data\_train} set that was created in the \texttt{setup()} method
as described in Section~\ref{sec-stage-fit-30}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ train\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_train, batch\_size}\OperatorTok{=}\VariableTok{self}\NormalTok{.batch\_size, num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}
\end{Highlighting}
\end{Shaded}

The \texttt{train\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_train)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.train\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Training set size: 160
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0708,  0.0507, -0.0310,  0.0219, -0.0373, -0.0470,  0.0339, -0.0395,
         -0.0150, -0.0011],
        [ 0.0344,  0.0507, -0.0299,  0.0047,  0.0934,  0.0870,  0.0339, -0.0026,
          0.0241, -0.0384],
        [ 0.0090, -0.0446,  0.0552, -0.0057,  0.0576,  0.0447, -0.0029,  0.0232,
          0.0557,  0.1066],
        [ 0.0417, -0.0446, -0.0644,  0.0356,  0.0122, -0.0580,  0.1812, -0.0764,
         -0.0006, -0.0508],
        [ 0.0381,  0.0507,  0.0164,  0.0219,  0.0397,  0.0450, -0.0434,  0.0712,
          0.0498,  0.0155]])
Targets: tensor([ 66.,  69., 173., 170., 212.])
\end{verbatim}

\subsection{\texorpdfstring{The \texttt{val\_dataloader()}
Method}{The val\_dataloader() Method}}\label{the-val_dataloader-method}

Returns the validation dataloader, i.e., a Pytorch DataLoader instance
using the validation dataset. It simply returns a DataLoader with the
\texttt{data\_val} set that was created in the \texttt{setup()} method
as desccribed in Section~\ref{sec-stage-fit-30}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ val\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_val, batch\_size}\OperatorTok{=}\VariableTok{self}\NormalTok{.batch\_size, num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}
\end{Highlighting}
\end{Shaded}

The \texttt{val\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Validation set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_val)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.val\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Validation set size: 106
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0163, -0.0446,  0.0736, -0.0412, -0.0043, -0.0135, -0.0139, -0.0011,
          0.0429,  0.0445],
        [ 0.0453, -0.0446,  0.0714,  0.0012, -0.0098, -0.0010,  0.0155, -0.0395,
         -0.0412, -0.0715],
        [ 0.0308,  0.0507,  0.0326,  0.0494, -0.0401, -0.0436, -0.0692,  0.0343,
          0.0630,  0.0031],
        [ 0.0235,  0.0507, -0.0396, -0.0057, -0.0484, -0.0333,  0.0118, -0.0395,
         -0.1016, -0.0674],
        [-0.0091,  0.0507,  0.0013, -0.0022,  0.0796,  0.0701,  0.0339, -0.0026,
          0.0267,  0.0818]])
Targets: tensor([275., 141., 208.,  78., 142.])
\end{verbatim}

\subsection{\texorpdfstring{The \texttt{test\_dataloader()}
Method}{The test\_dataloader() Method}}\label{the-test_dataloader-method}

Returns the test dataloader, i.e., a Pytorch DataLoader instance using
the test dataset. It simply returns a DataLoader with the
\texttt{data\_test} set that was created in the \texttt{setup()} method
as described in Section~\ref{sec-stage-test-30}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ test\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_test, batch\_size}\OperatorTok{=}\VariableTok{self}\NormalTok{.batch\_size, num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}
\end{Highlighting}
\end{Shaded}

The \texttt{test\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_test)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.test\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Test set size: 177
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,
         -0.0283,  0.0445],
        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,
         -0.0225,  0.0072],
        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,
         -0.0741, -0.0591],
        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,
         -0.0514, -0.0591],
        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,
         -0.0181, -0.0135]])
Targets: tensor([158.,  49., 142.,  96.,  59.])
\end{verbatim}

\subsection{\texorpdfstring{The \texttt{predict\_dataloader()}
Method}{The predict\_dataloader() Method}}\label{the-predict_dataloader-method}

Returns the prediction dataloader, i.e., a Pytorch DataLoader instance
using the prediction dataset. It simply returns a DataLoader with the
\texttt{data\_predict} set that was created in the \texttt{setup()}
method as described in Section~\ref{sec-stage-predict-30}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}]

The \texttt{batch\_size} is set to the length of the
\texttt{data\_predict} set.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ predict\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(}\VariableTok{self}\NormalTok{.data\_predict, batch\_size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.data\_predict), num\_workers}\OperatorTok{=}\VariableTok{self}\NormalTok{.num\_workers)}
\end{Highlighting}
\end{Shaded}

The \texttt{predict\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_predict)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.predict\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Test set size: 177
Batch Size: 177
Inputs Shape: torch.Size([177, 10])
Targets Shape: torch.Size([177])
---------------
Inputs: tensor([[ 0.0562, -0.0446, -0.0579,  ..., -0.0214, -0.0283,  0.0445],
        [ 0.0018, -0.0446, -0.0709,  ..., -0.0395, -0.0225,  0.0072],
        [-0.0527, -0.0446,  0.0542,  ..., -0.0395, -0.0741, -0.0591],
        ...,
        [ 0.0090, -0.0446, -0.0321,  ..., -0.0764, -0.0119, -0.0384],
        [-0.0273, -0.0446, -0.0666,  ..., -0.0395, -0.0358, -0.0094],
        [ 0.0817,  0.0507,  0.0067,  ...,  0.0919,  0.0547,  0.0072]])
Targets: tensor([158.,  49., 142.,  96.,  59.,  74., 137., 136.,  39.,  66., 310., 198.,
        235., 116.,  55., 177.,  59., 246.,  53., 135.,  88., 198., 186., 217.,
         51., 118., 153., 180.,  51., 229.,  84.,  72., 237., 142., 185.,  91.,
         88., 148., 179., 144.,  25.,  89.,  42.,  60., 124., 170., 215., 263.,
        178., 245., 202.,  97., 321.,  71., 123., 220., 132., 243.,  61., 102.,
        187.,  70., 242., 134.,  63.,  72.,  88., 219., 127., 146., 122., 143.,
        220., 293.,  59., 317.,  60., 140.,  65., 277.,  90.,  96., 109., 190.,
         90.,  52., 160., 233., 230., 175.,  68., 272., 144.,  70.,  68., 163.,
         71.,  93., 263., 118., 220.,  90., 232., 120., 163.,  88.,  85.,  52.,
        181., 232., 212., 332.,  81., 214., 145., 268., 115.,  93.,  64., 156.,
        128., 200., 281., 103., 220.,  66.,  48., 246.,  42., 150., 125., 109.,
        129.,  97., 265.,  97., 173., 216., 237., 121.,  42., 151.,  31.,  68.,
        137., 221., 283., 124., 243., 150.,  69., 306., 182., 252., 132., 258.,
        121., 110., 292., 101., 275., 141., 208.,  78., 142., 185., 167., 258.,
        144.,  89., 225., 140., 303., 236.,  87.,  77., 131.])
\end{verbatim}

\section{\texorpdfstring{Using the \texttt{LightDataModule} in the
\texttt{train\_model()}
Method}{Using the LightDataModule in the train\_model() Method}}\label{using-the-lightdatamodule-in-the-train_model-method}

First, a \texttt{LightDataModule} object is created and the
\texttt{setup()} method is called.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dm }\OperatorTok{=}\NormalTok{ LightDataModule(}
\NormalTok{    dataset}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"data\_set"}\NormalTok{],}
\NormalTok{    batch\_size}\OperatorTok{=}\NormalTok{config[}\StringTok{"batch\_size"}\NormalTok{],}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_workers"}\NormalTok{],}
\NormalTok{    test\_size}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_size"}\NormalTok{],}
\NormalTok{    test\_seed}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_seed"}\NormalTok{],}
\NormalTok{)}
\NormalTok{dm.setup()}
\end{Highlighting}
\end{Shaded}

Then, the \texttt{Trainer} is initialized.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Init trainer}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(}
\NormalTok{    default\_root\_dir}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id),}
\NormalTok{    max\_epochs}\OperatorTok{=}\NormalTok{model.hparams.epochs,}
\NormalTok{    accelerator}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"accelerator"}\NormalTok{],}
\NormalTok{    devices}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"devices"}\NormalTok{],}
\NormalTok{    logger}\OperatorTok{=}\NormalTok{TensorBoardLogger(}
\NormalTok{        save\_dir}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"TENSORBOARD\_PATH"}\NormalTok{],}
\NormalTok{        version}\OperatorTok{=}\NormalTok{config\_id,}
\NormalTok{        default\_hp\_metric}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        log\_graph}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"log\_graph"}\NormalTok{],}
\NormalTok{    ),}
\NormalTok{    callbacks}\OperatorTok{=}\NormalTok{[}
\NormalTok{        EarlyStopping(monitor}\OperatorTok{=}\StringTok{"val\_loss"}\NormalTok{, patience}\OperatorTok{=}\NormalTok{config[}\StringTok{"patience"}\NormalTok{], mode}\OperatorTok{=}\StringTok{"min"}\NormalTok{, strict}\OperatorTok{=}\VariableTok{False}\NormalTok{, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{    ],}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\NormalTok{enable\_progress\_bar,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, the \texttt{fit()} method is called to train the model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pass the datamodule as arg to trainer.fit to override model hooks :)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm)}
\end{Highlighting}
\end{Shaded}

Finally, the \texttt{validate()} method is called to validate the model.
The \texttt{validate()} method returns the validation loss.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test best model on validation and test set}
\CommentTok{\# result = trainer.validate(model=model, datamodule=dm, ckpt\_path="last")}
\NormalTok{result }\OperatorTok{=}\NormalTok{ trainer.validate(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm)}
\CommentTok{\# unlist the result (from a list of one dict)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ result[}\DecValTok{0}\NormalTok{]}
\ControlFlowTok{return}\NormalTok{ result[}\StringTok{"val\_loss"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\section{Further Information}\label{further-information}

\subsection{Preprocessing}\label{sec-preprocessing-30}

Preprocessing is handled by \texttt{Lightning} and \texttt{PyTorch}. It
is described in the
\href{https://lightning.ai/docs/pytorch/stable/data/datamodule.html}{LIGHTNINGDATAMODULE}
documentation. Here you can find information about the
\texttt{transforms} methods.

\chapter{\texorpdfstring{Hyperparameter Tuning with \texttt{spotpython}
and \texttt{PyTorch} Lightning for the Diabetes Data
Set}{Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set}}\label{sec-hpt-pytorch}

In this section, we will show how \texttt{spotpython} can be integrated
into the \texttt{PyTorch} Lightning training workflow for a regression
task. It demonstrates how easy it is to use \texttt{spotpython} to tune
hyperparameters for a \texttt{PyTorch} Lightning model.

\section{The Basic Setting}\label{sec-basic-setup-601}

\phantomsection\label{imports}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After importing the necessary libraries, the \texttt{fun\_control}
dictionary is set up via the \texttt{fun\_control\_init} function. The
\texttt{fun\_control} dictionary contains

\begin{itemize}
\tightlist
\item
  \texttt{PREFIX}: a unique identifier for the experiment
\item
  \texttt{fun\_evals}: the number of function evaluations
\item
  \texttt{max\_time}: the maximum run time in minutes
\item
  \texttt{data\_set}: the data set. Here we use the \texttt{Diabetes}
  data set that is provided by \texttt{spotpython}.
\item
  \texttt{core\_model\_name}: the class name of the neural network
  model. This neural network model is provided by \texttt{spotpython}.
\item
  \texttt{hyperdict}: the hyperparameter dictionary. This dictionary is
  used to define the hyperparameters of the neural network model. It is
  also provided by \texttt{spotpython}.
\item
  \texttt{\_L\_in}: the number of input features. Since the
  \texttt{Diabetes} data set has 10 features, \texttt{\_L\_in} is set to
  10.
\item
  \texttt{\_L\_out}: the number of output features. Since we want to
  predict a single value, \texttt{\_L\_out} is set to 1.
\end{itemize}

The \texttt{HyperLight} class is used to define the objective function
\texttt{fun}. It connects the \texttt{PyTorch} and the
\texttt{spotpython} methods and is provided by \texttt{spotpython}.

\phantomsection\label{spotpython_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table, print\_res\_table}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"601"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNLinearRegressor
\end{verbatim}

The method \texttt{set\_hyperparameter} allows the user to modify
default hyperparameter settings. Here we modify some hyperparameters to
keep the model small and to decrease the tuning time.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |     3   |   4     | transform_power_2_int |
| epochs         | int    | 4         |     3   |   7     | transform_power_2_int |
| batch_size     | int    | 4         |     4   |  11     | transform_power_2_int |
| act_fn         | factor | ReLU      |     0   |   5     | None                  |
| optimizer      | factor | SGD       |     0   |   2     | None                  |
| dropout_prob   | float  | 0.01      |     0   |   0.025 | None                  |
| lr_mult        | float  | 1.0       |     0.1 |  10     | None                  |
| patience       | int    | 2         |     2   |   3     | transform_power_2_int |
| batch_norm     | factor | 0         |     0   |   1     | None                  |
| initialization | factor | Default   |     0   |   4     | None                  |
\end{verbatim}

Finally, a \texttt{Spot} object is created. Calling the method
\texttt{run()} starts the hyperparameter tuning process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{S.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': 23075.09765625, 'hp_metric': 23075.09765625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3466.6259765625, 'hp_metric': 3466.6259765625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4775.9482421875, 'hp_metric': 4775.9482421875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23974.962890625, 'hp_metric': 23974.962890625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22921.740234375, 'hp_metric': 22921.740234375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4060.3369140625, 'hp_metric': 4060.3369140625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20739.09375, 'hp_metric': 20739.09375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4064.956298828125, 'hp_metric': 4064.956298828125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20911.953125, 'hp_metric': 20911.953125}
train_model result: {'val_loss': 23951.484375, 'hp_metric': 23951.484375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3396.2470703125, 'hp_metric': 3396.2470703125}
spotpython tuning: 3396.2470703125 [----------] 2.04% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
train_model result: {'val_loss': nan, 'hp_metric': nan}
spotpython tuning: 3396.2470703125 [----------] 3.61% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3414.830078125, 'hp_metric': 3414.830078125}
spotpython tuning: 3396.2470703125 [#---------] 8.30% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23692.912109375, 'hp_metric': 23692.912109375}
spotpython tuning: 3396.2470703125 [#---------] 10.57% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 8255.0693359375, 'hp_metric': 8255.0693359375}
spotpython tuning: 3396.2470703125 [#---------] 14.29% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 17554.509765625, 'hp_metric': 17554.509765625}
spotpython tuning: 3396.2470703125 [##--------] 16.72% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 8407.7724609375, 'hp_metric': 8407.7724609375}
spotpython tuning: 3396.2470703125 [##--------] 20.89% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2980.89794921875, 'hp_metric': 2980.89794921875}
spotpython tuning: 2980.89794921875 [###-------] 26.16% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3568.241943359375, 'hp_metric': 3568.241943359375}
spotpython tuning: 2980.89794921875 [###-------] 30.97% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3090.523193359375, 'hp_metric': 3090.523193359375}
spotpython tuning: 2980.89794921875 [####------] 36.01% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 12307.81640625, 'hp_metric': 12307.81640625}
spotpython tuning: 2980.89794921875 [####------] 40.59% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4920.1708984375, 'hp_metric': 4920.1708984375}
spotpython tuning: 2980.89794921875 [#####-----] 46.21% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 8808.490234375, 'hp_metric': 8808.490234375}
spotpython tuning: 2980.89794921875 [######----] 59.55% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22241.390625, 'hp_metric': 22241.390625}
spotpython tuning: 2980.89794921875 [#######---] 65.37% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3245.012939453125, 'hp_metric': 3245.012939453125}
spotpython tuning: 2980.89794921875 [#######---] 72.45% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3300.112548828125, 'hp_metric': 3300.112548828125}
spotpython tuning: 2980.89794921875 [########--] 80.75% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3785.927978515625, 'hp_metric': 3785.927978515625}
spotpython tuning: 2980.89794921875 [#########-] 89.37% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 1.284459928928387e+18, 'hp_metric': 1.284459928928387e+18}
spotpython tuning: 2980.89794921875 [#########-] 93.64% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5260.1796875, 'hp_metric': 5260.1796875}
spotpython tuning: 2980.89794921875 [##########] 96.84% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23540.33984375, 'hp_metric': 23540.33984375}
spotpython tuning: 2980.89794921875 [##########] 100.00% Done...

Experiment saved to 601_res.pkl
\end{verbatim}

\phantomsection\label{run}
\begin{verbatim}
<spotpython.spot.spot.Spot at 0x109e34320>
\end{verbatim}

\section{Looking at the Results}\label{looking-at-the-results-1}

\subsection{Tuning Progress}\label{tuning-progress-1}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_diabetes_files/figure-pdf/plot_progress_1-output-1.pdf}}

\subsection{Tuned Hyperparameters and Their
Importance}\label{tuned-hyperparameters-and-their-importance-1}

Results can be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_res\_table(S)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned             | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|-------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     4.0 | 4.0               | transform_power_2_int |        19.52 | *       |
| epochs         | int    | 4         |     3.0 |     7.0 | 4.0               | transform_power_2_int |         0.00 |         |
| batch_size     | int    | 4         |     4.0 |    11.0 | 6.0               | transform_power_2_int |         0.00 |         |
| act_fn         | factor | ReLU      |     0.0 |     5.0 | ReLU              | None                  |        22.14 | *       |
| optimizer      | factor | SGD       |     0.0 |     2.0 | Adamax            | None                  |       100.00 | ***     |
| dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.025             | None                  |         0.01 |         |
| lr_mult        | float  | 1.0       |     0.1 |    10.0 | 4.935071237769679 | None                  |         0.00 |         |
| patience       | int    | 2         |     2.0 |     3.0 | 3.0               | transform_power_2_int |         0.05 |         |
| batch_norm     | factor | 0         |     0.0 |     1.0 | 0                 | None                  |         0.00 |         |
| initialization | factor | Default   |     0.0 |     4.0 | xavier_normal     | None                  |         0.00 |         |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.plot\_importance(threshold}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_diabetes_files/figure-pdf/cell-8-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
l1:  19.52316051504337
epochs:  0.001
batch_size:  0.001
act_fn:  22.138353692001353
optimizer:  100.0
dropout_prob:  0.0058589372149437545
lr_mult:  0.001
patience:  0.04986384824415975
batch_norm:  0.001
initialization:  0.001
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_diabetes_files/figure-pdf/cell-9-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_diabetes_files/figure-pdf/cell-9-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_diabetes_files/figure-pdf/cell-9-output-4.pdf}}

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-601}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(S)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 64,
 'dropout_prob': 0.025,
 'epochs': 16,
 'initialization': 'xavier_normal',
 'l1': 16,
 'lr_mult': 4.935071237769679,
 'optimizer': 'Adamax',
 'patience': 8}
\end{verbatim}

\subsection{Test on the full data set}\label{test-on-the-full-data-set}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set the value of the key "TENSORBOARD\_CLEAN" to True in the fun\_control dictionary and use the update() method to update the fun\_control dictionary}
\NormalTok{fun\_control.update(\{}\StringTok{"TENSORBOARD\_CLEAN"}\NormalTok{: }\VariableTok{True}\NormalTok{\})}
\NormalTok{fun\_control.update(\{}\StringTok{"tensorboard\_log"}\NormalTok{: }\VariableTok{True}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.light.testmodel }\ImportTok{import}\NormalTok{ test\_model}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ get\_feature\_names}

\NormalTok{test\_model(config, fun\_control)}
\NormalTok{get\_feature\_names(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â        Test metric        â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â         hp_metric         â     3503.43017578125      â
â         val_loss          â     3503.43017578125      â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\begin{verbatim}
test_model result: {'val_loss': 3503.43017578125, 'hp_metric': 3503.43017578125}
\end{verbatim}

\begin{verbatim}
['age',
 'sex',
 'bmi',
 'bp',
 's1_tc',
 's2_ldl',
 's3_hdl',
 's4_tch',
 's5_ltg',
 's6_glu']
\end{verbatim}

\section{Cross Validation With
Lightning}\label{cross-validation-with-lightning}

\begin{itemize}
\tightlist
\item
  The \texttt{KFold} class from \texttt{sklearn.model\_selection} is
  used to generate the folds for cross-validation.
\item
  These mechanism is used to generate the folds for the final evaluation
  of the model.
\item
  The \texttt{CrossValidationDataModule} class
  \href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/data/lightcrossvalidationdatamodule.py}{{[}SOURCE{]}}
  is used to generate the folds for the hyperparameter tuning process.
\item
  It is called from the \texttt{cv\_model} function
  \href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/light/cvmodel.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{config}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'l1': 16,
 'epochs': 16,
 'batch_size': 64,
 'act_fn': ReLU(),
 'optimizer': 'Adamax',
 'dropout_prob': 0.025,
 'lr_mult': 4.935071237769679,
 'patience': 8,
 'batch_norm': False,
 'initialization': 'xavier_normal'}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.light.cvmodel }\ImportTok{import}\NormalTok{ cv\_model}
\NormalTok{fun\_control.update(\{}\StringTok{"k\_folds"}\NormalTok{: }\DecValTok{2}\NormalTok{\})}
\NormalTok{fun\_control.update(\{}\StringTok{"test\_size"}\NormalTok{: }\FloatTok{0.6}\NormalTok{\})}
\NormalTok{cv\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
k: 0
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3465.541259765625, 'hp_metric': 3465.541259765625}
k: 1
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3386.83544921875, 'hp_metric': 3386.83544921875}
\end{verbatim}

\begin{verbatim}
3426.1883544921875
\end{verbatim}

\section{Extending the Basic Setup}\label{extending-the-basic-setup}

This basic setup can be adapted to user-specific needs in many ways. For
example, the user can specify a custom data set, a custom model, or a
custom loss function. The following sections provide more details on how
to customize the hyperparameter tuning process. Before we proceed, we
will provide an overview of the basic settings of the hyperparameter
tuning process and explain the parameters used so far.

\subsection{General Experiment
Setup}\label{sec-general-experiment-setup-601}

To keep track of the different experiments, we use a \texttt{PREFIX} for
the experiment name. The \texttt{PREFIX} is used to create a unique
experiment name. The \texttt{PREFIX} is also used to create a unique
TensorBoard folder, which is used to store the TensorBoard log files.

\texttt{spotpython} allows the specification of two different types of
stopping criteria: first, the number of function evaluations
(\texttt{fun\_evals}), and second, the maximum run time in seconds
(\texttt{max\_time}). Here, we will set the number of function
evaluations to infinity and the maximum run time to one minute.

\texttt{max\_time} is set to one minute for demonstration purposes. For
real experiments, this value should be increased. Note, the total run
time may exceed the specified \texttt{max\_time}, because the initial
design is always evaluated, even if this takes longer than
\texttt{max\_time}.

\subsection{Data Setup}\label{sec-data-601}

Here, we have provided the \texttt{Diabetes} data set class, which is a
subclass of \texttt{torch.utils.data.Dataset}. Data preprocessing is
handled by \texttt{Lightning} and \texttt{PyTorch}. It is described in
the
\href{https://lightning.ai/docs/pytorch/stable/data/datamodule.html}{LIGHTNINGDATAMODULE}
documentation.

The data splitting, i.e., the generation of training, validation, and
testing data, is handled by \texttt{Lightning}.

\subsection{\texorpdfstring{Objective Function
\texttt{fun}}{Objective Function fun}}\label{sec-the-objective-function-601}

The objective function \texttt{fun} from the class \texttt{HyperLight}
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/fun/hyperlight.py}{{[}SOURCE{]}}
is selected next. It implements an interface from \texttt{PyTorch}'s
training, validation, and testing methods to \texttt{spotpython}.

\subsection{Core-Model Setup}\label{core-model-setup}

By using
\texttt{core\_model\_name\ =\ "light.regression.NNLinearRegressor"}, the
\texttt{spotpython} model class \texttt{NetLightRegression}
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/light/regression/netlightregression/}{{[}SOURCE{]}}
from the \texttt{light.regression} module is selected.

\subsection{Hyperdict Setup}\label{hyperdict-setup}

For a given \texttt{core\_model\_name}, the corresponding
hyperparameters are automatically loaded from the associated dictionary,
which is stored as a JSON file. The JSON file contains hyperparameter
type information, names, and bounds. For \texttt{spotpython} models, the
hyperparameters are stored in the \texttt{LightHyperDict}, see
\href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/hyperdict/light_hyper_dict.json}{{[}SOURCE{]}}
Alternatively, you can load a local hyper\_dict. The \texttt{hyperdict}
uses the default hyperparameter settings. These can be modified as
described in Section~\ref{sec-modifying-hyperparameter-levels}.

\subsection{Other Settings}\label{sec-other-settings-601}

There are several additional parameters that can be specified, e.g.,
since we did not specify a loss function, \texttt{mean\_squared\_error}
is used, which is the default loss function. These will be explained in
more detail in the following sections.

\section{Tensorboard}\label{sec-tensorboard-601-diabetes}

The textual output shown in the console (or code cell) can be visualized
with Tensorboard, if the argument \texttt{tensorboard\_log} to
\texttt{fun\_control\_init()} is set to \texttt{True}. The Tensorboard
log files are stored in the \texttt{runs} folder. To start Tensorboard,
run the following command in the terminal:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="runs/"}
\end{Highlighting}
\end{Shaded}

Further information can be found in the
\href{https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.tensorboard.html}{PyTorch
Lightning documentation} for Tensorboard.

\section{Loading the Saved Experiment and Getting the Hyperparameters of
the Tuned
Model}\label{loading-the-saved-experiment-and-getting-the-hyperparameters-of-the-tuned-model}

To get the tuned hyperparameters as a dictionary, the
\texttt{get\_tuned\_architecture} function can be used.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_result}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ load\_result(PREFIX}\OperatorTok{=}\NormalTok{PREFIX)}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner)}
\NormalTok{config}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loaded experiment from 601_res.pkl
\end{verbatim}

\begin{verbatim}
{'l1': 16,
 'epochs': 16,
 'batch_size': 64,
 'act_fn': ReLU(),
 'optimizer': 'Adamax',
 'dropout_prob': 0.025,
 'lr_mult': 4.935071237769679,
 'patience': 8,
 'batch_norm': False,
 'initialization': 'xavier_normal'}
\end{verbatim}

\section{\texorpdfstring{Using the
\texttt{spotgui}}{Using the spotgui}}\label{using-the-spotgui-1}

The \texttt{spotgui}
\href{https://github.com/sequential-parameter-optimization/spotGUI}{{[}github{]}}
provides a convenient way to interact with the hyperparameter tuning
process. To obtain the settings from Section~\ref{sec-basic-setup-601},
the \texttt{spotgui} can be started as shown in
Figure~\ref{fig-spotgui}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/024_gui.png}

}

\caption{\label{fig-spotgui}spotgui}

\end{figure}%

\section{Summary}\label{summary}

This section presented an introduction to the basic setup of
hyperparameter tuning with \texttt{spotpython} and \texttt{PyTorch}
Lightning.

\chapter{Hyperparameter Tuning with PyTorch Lightning and User Data
Sets}\label{sec-light-user-data-601}

In this section, we will show how user specfied data can be used for the
\texttt{PyTorch} Lightning hyperparameter tuning workflow with
\texttt{spotpython}.

\section{Loading a User Specified Data
Set}\label{loading-a-user-specified-data-set}

Using a user-specified data set is straightforward.

The user simply needs to provide a data set and loads is as a
\texttt{spotpython} \texttt{CVSDataset()} class by specifying the path,
filename, and target column.

Consider the following example, where the user has a data set stored in
the \texttt{userData} directory. The data set is stored in a file named
\texttt{data.csv}. The target column is named \texttt{target}. To show
the data, it is loaded as a \texttt{pandas} data frame and the first 5
rows are displayed. This step is not necessary for the hyperparameter
tuning process, but it is useful for understanding the data.

\phantomsection\label{user_data_load}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the csv data set as a pandas dataframe and dislay the first 5 rows}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"./userData/data.csv"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(data.head())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        age       sex       bmi        bp        s1        s2        s3  \
0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   
1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   
2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   
3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   
4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   

         s4        s5        s6  target  
0 -0.002592  0.019907 -0.017646   151.0  
1 -0.039493 -0.068332 -0.092204    75.0  
2 -0.002592  0.002861 -0.025930   141.0  
3  0.034309  0.022688 -0.009362   206.0  
4 -0.002592 -0.031988 -0.046641   135.0  
\end{verbatim}

Next, the data set is loaded as a \texttt{spotpython}
\texttt{CSVDataset()} class. This step is necessary for the
hyperparameter tuning process.

\phantomsection\label{user_data_load_csvdataset}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.csvdataset }\ImportTok{import}\NormalTok{ CSVDataset}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ CSVDataset(directory}\OperatorTok{=}\StringTok{"./userData/"}\NormalTok{,}
\NormalTok{                     filename}\OperatorTok{=}\StringTok{"data.csv"}\NormalTok{,}
\NormalTok{                     target\_column}\OperatorTok{=}\StringTok{"target"}\NormalTok{,}
\NormalTok{                     feature\_type}\OperatorTok{=}\NormalTok{torch.float32,}
\NormalTok{                     target\_type}\OperatorTok{=}\NormalTok{torch.float32,}
\NormalTok{                     rmNA}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(data\_set))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
442
\end{verbatim}

The following step is not necessary for the hyperparameter tuning
process, but it is useful for understanding the data. The data set is
loaded as a \texttt{DataLoader} from \texttt{torch.utils.data} to check
the data.

\phantomsection\label{user_data_dataloader}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(data\_set, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0381,  0.0507,  0.0617,  0.0219, -0.0442, -0.0348, -0.0434, -0.0026,
          0.0199, -0.0176],
        [-0.0019, -0.0446, -0.0515, -0.0263, -0.0084, -0.0192,  0.0744, -0.0395,
         -0.0683, -0.0922],
        [ 0.0853,  0.0507,  0.0445, -0.0057, -0.0456, -0.0342, -0.0324, -0.0026,
          0.0029, -0.0259],
        [-0.0891, -0.0446, -0.0116, -0.0367,  0.0122,  0.0250, -0.0360,  0.0343,
          0.0227, -0.0094],
        [ 0.0054, -0.0446, -0.0364,  0.0219,  0.0039,  0.0156,  0.0081, -0.0026,
         -0.0320, -0.0466]])
Targets: tensor([151.,  75., 141., 206., 135.])
\end{verbatim}

Similar to the setting from Section~\ref{sec-basic-setup-601}, the
hyperparameter tuning setup is defined. Instead of using the
\texttt{Diabetes} data set, the user data set is used. The
\texttt{data\_set} parameter is set to the user data set. The
\texttt{fun\_control} dictionary is set up via the
\texttt{fun\_control\_init} function.

Note, that we have modified the \texttt{fun\_evals} parameter to 12 and
the \texttt{init\_size} to 7 to reduce the computational time for this
example.

\phantomsection\label{user_data_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"601"}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{12}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{7}\NormalTok{)}

\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"initialization"}\NormalTok{, [}\StringTok{"Default"}\NormalTok{])}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}

\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNLinearRegressor
\end{verbatim}

\phantomsection\label{user_data_run}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\NormalTok{print\_res\_table(spot\_tuner)}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model(): trainer.fit failed with exception: SparseAdam does not support dense gradients, please consider Adam instead
train_model result: {'val_loss': 23320.16015625, 'hp_metric': 23320.16015625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4052.16845703125, 'hp_metric': 4052.16845703125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3584.89013671875, 'hp_metric': 3584.89013671875}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 18160.900390625, 'hp_metric': 18160.900390625}
\end{verbatim}

\begin{verbatim}
train_model(): trainer.fit failed with exception: SparseAdam does not support dense gradients, please consider Adam instead
train_model result: {'val_loss': 180457504.0, 'hp_metric': 180457504.0}
spotpython tuning: 3584.89013671875 [####------] 41.67% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 26572.333984375, 'hp_metric': 26572.333984375}
spotpython tuning: 3584.89013671875 [#####-----] 50.00% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4510.11474609375, 'hp_metric': 4510.11474609375}
spotpython tuning: 3584.89013671875 [######----] 58.33% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2946.150390625, 'hp_metric': 2946.150390625}
spotpython tuning: 2946.150390625 [#######---] 66.67% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4247.826171875, 'hp_metric': 4247.826171875}
spotpython tuning: 2946.150390625 [########--] 75.00% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3915.138916015625, 'hp_metric': 3915.138916015625}
spotpython tuning: 2946.150390625 [########--] 83.33% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4119.43701171875, 'hp_metric': 4119.43701171875}
spotpython tuning: 2946.150390625 [#########-] 91.67% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3251.498779296875, 'hp_metric': 3251.498779296875}
spotpython tuning: 2946.150390625 [##########] 100.00% Done...

Experiment saved to 601_res.pkl
| name           | type   | default   |   lower |   upper | tuned              | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     8.0 | 5.0                | transform_power_2_int |         0.00 |         |
| epochs         | int    | 4         |     4.0 |     9.0 | 5.0                | transform_power_2_int |        82.48 | **      |
| batch_size     | int    | 4         |     1.0 |     4.0 | 3.0                | transform_power_2_int |       100.00 | ***     |
| act_fn         | factor | ReLU      |     0.0 |     5.0 | ELU                | None                  |         0.00 |         |
| optimizer      | factor | SGD       |     0.0 |    11.0 | AdamW              | None                  |         0.00 |         |
| dropout_prob   | float  | 0.01      |     0.0 |    0.25 | 0.1316215890425084 | None                  |         0.00 |         |
| lr_mult        | float  | 1.0       |     0.1 |    10.0 | 6.381928597834528  | None                  |         0.00 |         |
| patience       | int    | 2         |     2.0 |     6.0 | 4.0                | transform_power_2_int |         0.00 |         |
| batch_norm     | factor | 0         |     0.0 |     1.0 | 1                  | None                  |         0.03 |         |
| initialization | factor | Default   |     0.0 |     0.0 | Default            | None                  |         0.00 |         |
l1:  0.0014130464100230577
epochs:  82.4803841925923
batch_size:  100.0
act_fn:  0.0014130464100230577
optimizer:  0.0014130464100230577
dropout_prob:  0.0014130464100230577
lr_mult:  0.0014130464100230577
patience:  0.0014130464100230577
batch_norm:  0.025860457382000553
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_user_data_files/figure-pdf/user_data_run-output-16.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_user_data_files/figure-pdf/user_data_run-output-17.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_user_data_files/figure-pdf/user_data_run-output-18.pdf}}

\section{Summary}\label{summary-1}

This section showed how to use user-specified data sets for the
hyperparameter tuning process with \texttt{spotpython}. The user needs
to provide the data set and load it as a \texttt{spotpython}
\texttt{CSVDataset()} class.

\chapter{Hyperparameter Tuning with PyTorch Lightning and User
Models}\label{sec-light-user-model-601}

In this section, we will show how a user defined model can be used for
the \texttt{PyTorch} Lightning hyperparameter tuning workflow with
\texttt{spotpython}.

\section{Using a User Specified
Model}\label{using-a-user-specified-model}

As templates, we provide the following three files that allow the user
to specify a model in the \texttt{/userModel} directory:

\begin{itemize}
\tightlist
\item
  \texttt{my\_regressor.py}, see Section~\ref{sec-my-regressor}
\item
  \texttt{my\_hyperdict.json}, see Section~\ref{sec-my-hyper-dict-json}
\item
  \texttt{my\_hyperdict.py}, see Section~\ref{sec-my-hyper-dict}.
\end{itemize}

The \texttt{my\_regressor.py} file contains the model class, which is a
subclass of \texttt{nn.Module}. The \texttt{my\_hyperdict.json} file
contains the hyperparameter settings as a dictionary, which are loaded
via the \texttt{my\_hyperdict.py} file.

Note, that we have to add the path to the \texttt{userModel} directory
to the \texttt{sys.path} list as shown below.

\phantomsection\label{user_model_imports_sys}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\NormalTok{sys.path.insert(}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}./userModel\textquotesingle{}}\NormalTok{)}
\ImportTok{import}\NormalTok{ my\_regressor}
\ImportTok{import}\NormalTok{ my\_hyper\_dict}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}

\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"601{-}user{-}model"}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ Diabetes(),}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              core\_model}\OperatorTok{=}\NormalTok{my\_regressor.MyRegressor,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{my\_hyper\_dict.MyHyperDict)}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{7}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}

\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{user_model_run}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\NormalTok{print\_res\_table(spot\_tuner)}
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model(): trainer.fit failed with exception: SparseAdam does not support dense gradients, please consider Adam instead
train_model result: {'val_loss': 23988.361328125, 'hp_metric': 23988.361328125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3127.356689453125, 'hp_metric': 3127.356689453125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4815.912109375, 'hp_metric': 4815.912109375}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2994.751953125, 'hp_metric': 2994.751953125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2893.5380859375, 'hp_metric': 2893.5380859375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2925.6025390625, 'hp_metric': 2925.6025390625}
spotpython tuning: 2893.5380859375 [----------] 1.88% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4444.89453125, 'hp_metric': 4444.89453125}
spotpython tuning: 2893.5380859375 [###-------] 29.31% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4985.4736328125, 'hp_metric': 4985.4736328125}
spotpython tuning: 2893.5380859375 [###-------] 31.61% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4903.39306640625, 'hp_metric': 4903.39306640625}
spotpython tuning: 2893.5380859375 [###-------] 34.03% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4839.94140625, 'hp_metric': 4839.94140625}
spotpython tuning: 2893.5380859375 [####------] 36.22% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3076.719970703125, 'hp_metric': 3076.719970703125}
spotpython tuning: 2893.5380859375 [####------] 43.53% 
\end{verbatim}

\begin{verbatim}
train_model(): trainer.fit failed with exception: SparseAdam does not support dense gradients, please consider Adam instead
train_model result: {'val_loss': 24015.21484375, 'hp_metric': 24015.21484375}
spotpython tuning: 2893.5380859375 [#####-----] 45.09% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3004.9287109375, 'hp_metric': 3004.9287109375}
spotpython tuning: 2893.5380859375 [#####-----] 47.57% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3433.583740234375, 'hp_metric': 3433.583740234375}
spotpython tuning: 2893.5380859375 [#####-----] 54.37% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 6609.55078125, 'hp_metric': 6609.55078125}
spotpython tuning: 2893.5380859375 [######----] 59.91% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
train_model result: {'val_loss': 3098.64794921875, 'hp_metric': 3098.64794921875}
spotpython tuning: 2893.5380859375 [#########-] 90.30% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3154.20654296875, 'hp_metric': 3154.20654296875}
spotpython tuning: 2893.5380859375 [#########-] 94.05% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3332.117431640625, 'hp_metric': 3332.117431640625}
spotpython tuning: 2893.5380859375 [##########] 100.00% Done...

Experiment saved to 601-user-model_res.pkl
| name           | type   | default   |   lower |   upper | tuned               | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     8.0 | 5.0                 | transform_power_2_int |         0.00 |         |
| epochs         | int    | 4         |     4.0 |     9.0 | 4.0                 | transform_power_2_int |       100.00 | ***     |
| batch_size     | int    | 4         |     1.0 |     4.0 | 4.0                 | transform_power_2_int |         0.00 |         |
| act_fn         | factor | ReLU      |     0.0 |     5.0 | ReLU                | None                  |         0.00 |         |
| optimizer      | factor | SGD       |     0.0 |    11.0 | AdamW               | None                  |         0.00 |         |
| dropout_prob   | float  | 0.01      |     0.0 |    0.25 | 0.14960635316721133 | None                  |         0.00 |         |
| lr_mult        | float  | 1.0       |     0.1 |    10.0 | 9.293583024571447   | None                  |         0.01 |         |
| patience       | int    | 2         |     2.0 |     6.0 | 3.0                 | transform_power_2_int |         0.00 |         |
| initialization | factor | Default   |     0.0 |     4.0 | xavier_uniform      | None                  |         0.00 |         |
l1:  0.001
epochs:  100.0
batch_size:  0.001
act_fn:  0.0028739134099338083
optimizer:  0.001
dropout_prob:  0.001
lr_mult:  0.008842064340296895
patience:  0.001
initialization:  0.001
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_user_model_files/figure-pdf/user_model_run-output-22.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_user_model_files/figure-pdf/user_model_run-output-23.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_user_model_files/figure-pdf/user_model_run-output-24.pdf}}

\section{Details}\label{details}

\subsection{Model Setup}\label{model-setup-1}

By using \texttt{core\_model\_name\ =\ "my\_regressor.MyRegressor"}, the
user specified model class \texttt{MyRegressor}
\href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userModel/my_regressor.py}{{[}SOURCE{]}}
is selected. For this given \texttt{core\_model\_name}, the local
hyper\_dict is loaded using the \texttt{my\_hyper\_dict.py} file as
shown below.

\subsection{\texorpdfstring{The \texttt{my\_hyper\_dict.py}
File}{The my\_hyper\_dict.py File}}\label{sec-my-hyper-dict}

The \texttt{my\_hyper\_dict.py} file must be placed in the
\texttt{/userModel} directory. It provides a convenience function to
load the hyperparameters from user specified the
\texttt{my\_hyper\_dict.json} file, see Section~\ref{sec-my-hyper-dict}.
The user does not need to modify this file, if the JSON file is stored
as \texttt{my\_hyper\_dict.json}. Alternative filenames can be specified
via the \texttt{filename} argument (which is default set to
\texttt{"my\_hyper\_dict.json"}).

\subsection{\texorpdfstring{The \texttt{my\_hyper\_dict.json}
File}{The my\_hyper\_dict.json File}}\label{sec-my-hyper-dict-json}

The \texttt{my\_hyper\_dict.json} file contains the hyperparameter
settings as a dictionary, which are loaded via the
\texttt{my\_hyper\_dict.py} file. The example below shows the content of
the \texttt{my\_hyper\_dict.json} file.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\{}
    \DataTypeTok{"MyRegressor"}\FunctionTok{:} \FunctionTok{\{}
        \DataTypeTok{"l1"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"int"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \DecValTok{3}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"transform\_power\_2\_int"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{3}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{8}
        \FunctionTok{\},}
        \DataTypeTok{"epochs"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"int"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \DecValTok{4}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"transform\_power\_2\_int"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{4}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{9}
        \FunctionTok{\},}
        \DataTypeTok{"batch\_size"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"int"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \DecValTok{4}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"transform\_power\_2\_int"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{1}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{4}
        \FunctionTok{\},}
        \DataTypeTok{"act\_fn"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"levels"}\FunctionTok{:} \OtherTok{[}
                \StringTok{"Sigmoid"}\OtherTok{,}
                \StringTok{"Tanh"}\OtherTok{,}
                \StringTok{"ReLU"}\OtherTok{,}
                \StringTok{"LeakyReLU"}\OtherTok{,}
                \StringTok{"ELU"}\OtherTok{,}
                \StringTok{"Swish"}
            \OtherTok{]}\FunctionTok{,}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"factor"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \StringTok{"ReLU"}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"None"}\FunctionTok{,}
            \DataTypeTok{"class\_name"}\FunctionTok{:} \StringTok{"spotpython.torch.activation"}\FunctionTok{,}
            \DataTypeTok{"core\_model\_parameter\_type"}\FunctionTok{:} \StringTok{"instance()"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{0}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{5}
        \FunctionTok{\},}
        \DataTypeTok{"optimizer"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"levels"}\FunctionTok{:} \OtherTok{[}
                \StringTok{"Adadelta"}\OtherTok{,}
                \StringTok{"Adagrad"}\OtherTok{,}
                \StringTok{"Adam"}\OtherTok{,}
                \StringTok{"AdamW"}\OtherTok{,}
                \StringTok{"SparseAdam"}\OtherTok{,}
                \StringTok{"Adamax"}\OtherTok{,}
                \StringTok{"ASGD"}\OtherTok{,}
                \StringTok{"NAdam"}\OtherTok{,}
                \StringTok{"RAdam"}\OtherTok{,}
                \StringTok{"RMSprop"}\OtherTok{,}
                \StringTok{"Rprop"}\OtherTok{,}
                \StringTok{"SGD"}
            \OtherTok{]}\FunctionTok{,}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"factor"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \StringTok{"SGD"}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"None"}\FunctionTok{,}
            \DataTypeTok{"class\_name"}\FunctionTok{:} \StringTok{"torch.optim"}\FunctionTok{,}
            \DataTypeTok{"core\_model\_parameter\_type"}\FunctionTok{:} \StringTok{"str"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{0}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{11}
        \FunctionTok{\},}
        \DataTypeTok{"dropout\_prob"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"float"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \DecValTok{0}\ErrorTok{.01}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"None"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{0}\ErrorTok{.}\DecValTok{0}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{0}\ErrorTok{.}\DecValTok{25}
        \FunctionTok{\},}
        \DataTypeTok{"lr\_mult"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"float"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \FloatTok{1.0}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"None"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{0}\ErrorTok{.}\DecValTok{1}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \FloatTok{10.0}
        \FunctionTok{\},}
        \DataTypeTok{"patience"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"int"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \DecValTok{2}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"transform\_power\_2\_int"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{2}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{6}
        \FunctionTok{\},}
        \DataTypeTok{"initialization"}\FunctionTok{:} \FunctionTok{\{}
            \DataTypeTok{"levels"}\FunctionTok{:} \OtherTok{[}
                \StringTok{"Default"}\OtherTok{,}
                \StringTok{"Kaiming"}\OtherTok{,}
                \StringTok{"Xavier"}
            \OtherTok{]}\FunctionTok{,}
            \DataTypeTok{"type"}\FunctionTok{:} \StringTok{"factor"}\FunctionTok{,}
            \DataTypeTok{"default"}\FunctionTok{:} \StringTok{"Default"}\FunctionTok{,}
            \DataTypeTok{"transform"}\FunctionTok{:} \StringTok{"None"}\FunctionTok{,}
            \DataTypeTok{"core\_model\_parameter\_type"}\FunctionTok{:} \StringTok{"str"}\FunctionTok{,}
            \DataTypeTok{"lower"}\FunctionTok{:} \DecValTok{0}\FunctionTok{,}
            \DataTypeTok{"upper"}\FunctionTok{:} \DecValTok{2}
        \FunctionTok{\}}
    \FunctionTok{\}}
\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{The \texttt{my\_regressor.py}
File}{The my\_regressor.py File}}\label{sec-my-regressor}

The \texttt{my\_regressor.py} file contains
\href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/userModel/my_regressor.py}{{[}SOURCE{]}}~the
model class, which is a subclass of \texttt{nn.Module}. It must
implement the following methods:

\begin{itemize}
\tightlist
\item
  \texttt{\_\_init\_\_(self,\ **kwargs)}: The constructor of the model
  class. The hyperparameters are passed as keyword arguments.
\item
  \texttt{forward(self,\ x:\ torch.Tensor)\ -\textgreater{}\ torch.Tensor}:
  The forward pass of the model. The input \texttt{x} is passed through
  the model and the output is returned.
\item
  \texttt{training\_step(self,\ batch,\ batch\_idx)\ -\textgreater{}\ torch.Tensor}:
  The training step of the model. It takes a batch of data and the batch
  index as input and returns the loss.
\item
  \texttt{validation\_step(self,\ batch,\ batch\_idx)\ -\textgreater{}\ torch.Tensor}:
  The validation step of the model. It takes a batch of data and the
  batch index as input and returns the loss.
\item
  \texttt{test\_step(self,\ batch,\ batch\_idx)\ -\textgreater{}\ torch.Tensor}:
  The test step of the model. It takes a batch of data and the batch
  index as input and returns the loss.
\item
  \texttt{predict(self,\ x:\ torch.Tensor)\ -\textgreater{}\ torch.Tensor}:
  The prediction method of the model. It takes an input \texttt{x} and
  returns the prediction.
\item
  \texttt{configure\_optimizers(self)\ -\textgreater{}\ torch.optim.Optimizer}:
  The method to configure the optimizer of the model. It returns the
  optimizer.
\end{itemize}

The file \texttt{my\_regressor.py} must be placed in the
\texttt{/userModel} directory. The user can modify the model class to
implement a custom model architecture.

We will take a closer look at the methods defined in the
\texttt{my\_regressor.py} file in the next subsections.

\subsubsection{\texorpdfstring{The \texttt{\_\_init\_\_}
Method}{The \_\_init\_\_ Method}}\label{the-__init__-method}

\texttt{\_\_init\_\_()} initializes the \texttt{MyRegressor} object. It
takes the following arguments:

\begin{itemize}
\tightlist
\item
  \texttt{l1} (int): The number of neurons in the first hidden layer.
\item
  \texttt{epochs} (int): The number of epochs to train the model for.
\item
  \texttt{batch\_size} (int): The batch size to use during training.
\item
  \texttt{initialization} (str): The initialization method to use for
  the weights.
\item
  \texttt{act\_fn} (nn.Module): The activation function to use in the
  hidden layers.
\item
  \texttt{optimizer} (str): The optimizer to use during training.
\item
  \texttt{dropout\_prob} (float): The probability of dropping out a
  neuron during training.
\item
  \texttt{lr\_mult} (float): The learning rate multiplier for the
  optimizer.
\item
  \texttt{patience} (int): The number of epochs to wait before early
  stopping.
\item
  \texttt{\_L\_in} (int): The number of input features. Not a
  hyperparameter, but needed to create the network.
\item
  \texttt{\_L\_out} (int): The number of output classes. Not a
  hyperparameter, but needed to create the network.
\item
  \texttt{\_torchmetric} (str): The metric to use for the loss function.
  If \texttt{None}, then ``mean\_squared\_error'' is used.
\end{itemize}

It is implemented as follows:

\phantomsection\label{user_model_init}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ MyRegressor(L.LightningModule):}
        \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        l1: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        epochs: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        batch\_size: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        initialization: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        act\_fn: nn.Module,}
\NormalTok{        optimizer: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        dropout\_prob: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        lr\_mult: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        patience: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_in: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_out: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_torchmetric: }\BuiltInTok{str}\NormalTok{,}
        \OperatorTok{*}\NormalTok{args,}
        \OperatorTok{**}\NormalTok{kwargs,}
\NormalTok{    ):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{=}\NormalTok{ \_L\_in}
        \VariableTok{self}\NormalTok{.\_L\_out }\OperatorTok{=}\NormalTok{ \_L\_out}
        \ControlFlowTok{if}\NormalTok{ \_torchmetric }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            \_torchmetric }\OperatorTok{=} \StringTok{"mean\_squared\_error"}
        \VariableTok{self}\NormalTok{.\_torchmetric }\OperatorTok{=}\NormalTok{ \_torchmetric}
        \VariableTok{self}\NormalTok{.metric }\OperatorTok{=} \BuiltInTok{getattr}\NormalTok{(torchmetrics.functional.regression, \_torchmetric)}
        \CommentTok{\# \_L\_in and \_L\_out are not hyperparameters, but are needed to create the network}
        \CommentTok{\# \_torchmetric is not a hyperparameter, but is needed to calculate the loss}
        \VariableTok{self}\NormalTok{.save\_hyperparameters(ignore}\OperatorTok{=}\NormalTok{[}\StringTok{"\_L\_in"}\NormalTok{, }\StringTok{"\_L\_out"}\NormalTok{, }\StringTok{"\_torchmetric"}\NormalTok{])}
        \CommentTok{\# set dummy input array for Tensorboard Graphs}
        \CommentTok{\# set log\_graph=True in Trainer to see the graph (in traintest.py)}
        \VariableTok{self}\NormalTok{.example\_input\_array }\OperatorTok{=}\NormalTok{ torch.zeros((batch\_size, }\VariableTok{self}\NormalTok{.\_L\_in))}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.hparams.l1 }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"l1 must be at least 4"}\NormalTok{)}
\NormalTok{        hidden\_sizes }\OperatorTok{=}\NormalTok{ [l1 }\OperatorTok{*} \DecValTok{2}\NormalTok{, l1, ceil(l1}\OperatorTok{/}\DecValTok{2}\NormalTok{)]}
        \CommentTok{\# Create the network based on the specified hidden sizes}
\NormalTok{        layers }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        layer\_sizes }\OperatorTok{=}\NormalTok{ [}\VariableTok{self}\NormalTok{.\_L\_in] }\OperatorTok{+}\NormalTok{ hidden\_sizes}
\NormalTok{        layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_sizes[}\DecValTok{0}\NormalTok{]}
        \ControlFlowTok{for}\NormalTok{ layer\_size }\KeywordTok{in}\NormalTok{ layer\_sizes[}\DecValTok{1}\NormalTok{:]:}
\NormalTok{            layers }\OperatorTok{+=}\NormalTok{ [}
\NormalTok{                nn.Linear(layer\_size\_last, layer\_size),}
                \VariableTok{self}\NormalTok{.hparams.act\_fn,}
\NormalTok{                nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.dropout\_prob),}
\NormalTok{            ]}
\NormalTok{            layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_size}
\NormalTok{        layers }\OperatorTok{+=}\NormalTok{ [nn.Linear(layer\_sizes[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.\_L\_out)]}
        \CommentTok{\# nn.Sequential summarizes a list of modules into a single module,}
        \CommentTok{\# applying them in sequence}
        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{layers)}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{The
\texttt{hidden\_sizes}}{The hidden\_sizes}}\label{the-hidden_sizes}

\texttt{\_\_init\_\_()} uses the \texttt{hidden\_sizes} list to define
the sizes of the hidden layers in the network. The hidden sizes are
calculated based on the \texttt{l1} hyperparameter. The hidden sizes can
be computed in different ways, depending on the problem and the desired
network architecture. We recommend using a separate function to
calculate the hidden sizes based on the hyperparameters.

\subsubsection{\texorpdfstring{The \texttt{forward}
Method}{The forward Method}}\label{the-forward-method}

The \texttt{forward()} method defines the forward pass of the model. It
takes an input tensor \texttt{x} and passes it through the network
layers to produce an output tensor. It is implemented as follows:

\phantomsection\label{user_model_forward}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
    \ControlFlowTok{return} \VariableTok{self}\NormalTok{.layers(x)}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{The \texttt{\_calculate\_loss}
Method}{The \_calculate\_loss Method}}\label{the-_calculate_loss-method}

The \texttt{\_calculate\_loss()} method calculates the loss based on the
predicted output and the target values. It uses the specified metric to
calculate the loss. It takes the following arguments:

\begin{itemize}
\tightlist
\item
  \texttt{batch\ (tuple)}: A tuple containing a batch of input data and
  labels.
\end{itemize}

It is implemented as follows:

\phantomsection\label{user_model_calculate_loss}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ \_calculate\_loss(}\VariableTok{self}\NormalTok{, batch):}
\NormalTok{    x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{    y\_hat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{    loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.metric(y\_hat, y)}
    \ControlFlowTok{return}\NormalTok{ loss}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{The \texttt{training\_step}
Method}{The training\_step Method}}\label{the-training_step-method}

The \texttt{training\_step()} method defines the training step of the
model. It takes a batch of data and returns the loss. It is implemented
as follows:

\phantomsection\label{user_model_training_step}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{    val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
    \ControlFlowTok{return}\NormalTok{ val\_loss}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{The \texttt{validation\_step}
Method}{The validation\_step Method}}\label{the-validation_step-method}

The \texttt{validation\_step()} method defines the validation step of
the model. It takes a batch of data and returns the loss. It is
implemented as follows:

\phantomsection\label{user_model_validation_step}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{    val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
    \ControlFlowTok{return}\NormalTok{ val\_loss}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{The \texttt{test\_step}
Method}{The test\_step Method}}\label{the-test_step-method}

The \texttt{test\_step()} method defines the test step of the model. It
takes a batch of data and returns the loss. It is implemented as
follows:

\phantomsection\label{user_model_test_step}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{    val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
    \ControlFlowTok{return}\NormalTok{ val\_loss}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{The \texttt{predict}
Method}{The predict Method}}\label{the-predict-method}

The \texttt{predict()} method defines the prediction method of the
model. It takes an input tensor \texttt{x} and returns a tuple with the
input tensor \texttt{x}, the target tensor \texttt{y}, and the predicted
tensor \texttt{y\_hat}.

It is implemented as follows:

\phantomsection\label{user_model_predict}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ predict(}\VariableTok{self}\NormalTok{, x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{    x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{    yhat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{    yhat }\OperatorTok{=}\NormalTok{ yhat.view(}\BuiltInTok{len}\NormalTok{(yhat), }\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ (x, y, yhat)}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{The \texttt{configure\_optimizers}
Method}{The configure\_optimizers Method}}\label{the-configure_optimizers-method}

The \texttt{configure\_optimizers()} method defines the optimizer to use
during training. It uses the \texttt{optimizer\_handler} from
\texttt{spotpython.hyperparameter.optimizer} to create the optimizer
based on the specified optimizer name, parameters, and learning rate
multiplier. It is implemented as follows:

\phantomsection\label{user_model_configure_optimizers}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.optim.Optimizer:}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ optimizer\_handler(}
\NormalTok{        optimizer\_name}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.optimizer, params}\OperatorTok{=}\VariableTok{self}\NormalTok{.parameters(), lr\_mult}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.lr\_mult}
\NormalTok{    )}
    \ControlFlowTok{return}\NormalTok{ optimizer}
\end{Highlighting}
\end{Shaded}

Note, the default Lightning way is to define an optimizer as
\texttt{optimizer\ =\ torch.optim.Adam(self.parameters(),\ lr=self.learning\_rate)}.
\texttt{spotpython} uses an optimizer handler to create the optimizer,
which adapts the learning rate according to the \texttt{lr\_mult}
hyperparameter as well as other hyperparameters. See
\texttt{spotpython.hyperparameters.optimizer.py}
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/hyperparameters/optimizer.py}{{[}SOURCE{]}}
for details.

\section{Connection with the
LightDataModule}\label{connection-with-the-lightdatamodule}

The steps described in Section~\ref{sec-my-regressor} are connected to
the \texttt{LightDataModule} class
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/data/lightdatamodule/}{{[}DOC{]}}.
This class is used to create the data loaders for the training,
validation, and test sets. The \texttt{LightDataModule} class is part of
the \texttt{spotpython} package and class provides the following methods
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/data/lightdatamodule.py}{{[}SOURCE{]}}:

\begin{itemize}
\tightlist
\item
  \texttt{prepare\_data()}: This method is used to prepare the data set.
\item
  \texttt{setup()}: This method is used to create the data loaders for
  the training, validation, and test sets.
\item
  \texttt{train\_dataloader()}: This method is used to return the data
  loader for the training set.
\item
  \texttt{val\_dataloader()}: This method is used to return the data
  loader for the validation set.
\item
  \texttt{test\_dataloader()}: This method is used to return the data
  loader for the test set.
\item
  \texttt{predict\_dataloader()}: This method is used to return the data
  loader for the prediction set.
\end{itemize}

\subsection{\texorpdfstring{The \texttt{prepare\_data()}
Method}{The prepare\_data() Method}}\label{the-prepare_data-method-1}

The \texttt{prepare\_data()} method is used to prepare the data set.
This method is called only once and on a single process. It can be used
to download the data set. In our case, the data set is already
available, so this method uses a simple \texttt{pass} statement.

\subsection{\texorpdfstring{The \texttt{setup()}
Method}{The setup() Method}}\label{the-setup-method-1}

The \texttt{stage} is used to define the data set to be returned. It can
be \texttt{None}, \texttt{fit}, \texttt{test}, or \texttt{predict}. If
\texttt{stage} is \texttt{None}, the method returns the training
(\texttt{fit}), testing (\texttt{test}), and prediction
(\texttt{predict}) data sets.

The \texttt{setup} methods splits the data based on the \texttt{stage}
setting for use in training, validation, and testing. It uses
\texttt{torch.utils.data.random\_split()} to split the data.

Splitting is based on the \texttt{test\_size} and \texttt{test\_seed}.
The \texttt{test\_size} can be a float or an int.

First, the data set sizes are determined as described in
Section~\ref{sec-determine-sizes-601}. Then, the data sets are split
based on the \texttt{stage} setting. \texttt{spotpython}'s
\texttt{LightDataModule} class uses the following sizes:

\begin{itemize}
\tightlist
\item
  \texttt{full\_train\_size}: The size of the full training data set.
  This data set is splitted into the final training data set and a
  validation data set.
\item
  \texttt{val\_size}: The size of the validation data set. The
  validation data set is used to validate the model during training.
\item
  \texttt{train\_size}: The size of the training data set. The training
  data set is used to train the model.
\item
  \texttt{test\_size}: The size of the test data set. The test data set
  is used to evaluate the model after training. It is not used during
  training (``hyperparameter tuning''). Only after everything is
  finished, the model is evaluated on the test data set.
\end{itemize}

\subsubsection{Determine the Sizes of the Data
Sets}\label{sec-determine-sizes-601}

\phantomsection\label{user_data_module_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ random\_split}
\NormalTok{data\_full }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{test\_size }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"test\_size"}\NormalTok{]}
\NormalTok{test\_seed}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_seed"}\NormalTok{]}
\CommentTok{\# if test\_size is float, then train\_size is 1 {-} test\_size}
\ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(test\_size, }\BuiltInTok{float}\NormalTok{):}
\NormalTok{    full\_train\_size }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ test\_size, }\DecValTok{2}\NormalTok{)}
\NormalTok{    val\_size }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(full\_train\_size }\OperatorTok{*}\NormalTok{ test\_size, }\DecValTok{2}\NormalTok{)}
\NormalTok{    train\_size }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(full\_train\_size }\OperatorTok{{-}}\NormalTok{ val\_size, }\DecValTok{2}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
    \CommentTok{\# if test\_size is int, then train\_size is len(data\_full) {-} test\_size}
\NormalTok{    full\_train\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(data\_full) }\OperatorTok{{-}}\NormalTok{ test\_size}
\NormalTok{    val\_size }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(full\_train\_size }\OperatorTok{*}\NormalTok{ test\_size }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(data\_full))}
\NormalTok{    train\_size }\OperatorTok{=}\NormalTok{ full\_train\_size }\OperatorTok{{-}}\NormalTok{ val\_size}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): full\_train\_size: }\SpecialCharTok{\{}\NormalTok{full\_train\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): val\_size: }\SpecialCharTok{\{}\NormalTok{val\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): train\_size: }\SpecialCharTok{\{}\NormalTok{train\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): test\_size: }\SpecialCharTok{\{}\NormalTok{test\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LightDataModule setup(): full_train_size: 0.6
LightDataModule setup(): val_size: 0.24
LightDataModule setup(): train_size: 0.36
LightDataModule setup(): test_size: 0.4
\end{verbatim}

\subsubsection{The ``setup'' Method: Stage
``fit''}\label{sec-stage-fit-601}

Here, \texttt{train\_size} and \texttt{val\_size} are used to split the
data into training and validation sets.

\phantomsection\label{user_data_module_setup_fit}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stage }\OperatorTok{=} \StringTok{"fit"}
\NormalTok{scaler }\OperatorTok{=} \VariableTok{None}
\CommentTok{\# Assign train/val datasets for use in dataloaders}
\ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"fit"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"train\_size: }\SpecialCharTok{\{}\NormalTok{train\_size}\SpecialCharTok{\}}\SpecialStringTok{, val\_size: }\SpecialCharTok{\{}\NormalTok{val\_size}\SpecialCharTok{\}}\SpecialStringTok{ used for train \& val data."}\NormalTok{)}
\NormalTok{    generator\_fit }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(test\_seed)}
\NormalTok{    data\_train, data\_val, \_ }\OperatorTok{=}\NormalTok{ random\_split(}
\NormalTok{        data\_full, [train\_size, val\_size, test\_size], generator}\OperatorTok{=}\NormalTok{generator\_fit}
\NormalTok{    )}
    \ControlFlowTok{if}\NormalTok{ scaler }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
        \CommentTok{\# Fit the scaler on training data and transform both train and val data}
\NormalTok{        scaler\_train\_data }\OperatorTok{=}\NormalTok{ torch.stack([data\_train[i][}\DecValTok{0}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(data\_train))]).squeeze(}\DecValTok{1}\NormalTok{)}
        \CommentTok{\# train\_val\_data = data\_train[:,0]}
        \BuiltInTok{print}\NormalTok{(scaler\_train\_data.shape)}
\NormalTok{        scaler.fit(scaler\_train\_data)}
\NormalTok{        data\_train }\OperatorTok{=}\NormalTok{ [(scaler.transform(data), target) }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_train]}
\NormalTok{        data\_tensors\_train }\OperatorTok{=}\NormalTok{ [data.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_train]}
\NormalTok{        target\_tensors\_train }\OperatorTok{=}\NormalTok{ [target.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_train]}
\NormalTok{        data\_train }\OperatorTok{=}\NormalTok{ TensorDataset(}
\NormalTok{            torch.stack(data\_tensors\_train).squeeze(}\DecValTok{1}\NormalTok{), torch.stack(target\_tensors\_train)}
\NormalTok{        )}
        \CommentTok{\# print(data\_train)}
\NormalTok{        data\_val }\OperatorTok{=}\NormalTok{ [(scaler.transform(data), target) }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_val]}
\NormalTok{        data\_tensors\_val }\OperatorTok{=}\NormalTok{ [data.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_val]}
\NormalTok{        target\_tensors\_val }\OperatorTok{=}\NormalTok{ [target.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_val]}
\NormalTok{        data\_val }\OperatorTok{=}\NormalTok{ TensorDataset(torch.stack(data\_tensors\_val).squeeze(}\DecValTok{1}\NormalTok{), torch.stack(target\_tensors\_val))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_size: 0.36, val_size: 0.24 used for train & val data.
\end{verbatim}

The \texttt{data\_train} and \texttt{data\_val} data sets are further
used to create the training and validation data loaders as described in
Section~\ref{sec-train-dataloader-601} and
Section~\ref{sec-val-dataloader-601}, respectively.

\subsubsection{The ``setup'' Method: Stage
``test''}\label{sec-stage-test-601}

Here, the test data set, which is based on the \texttt{test\_size}, is
created.

\phantomsection\label{user_data_module_setup_test}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stage }\OperatorTok{=} \StringTok{"test"}
\CommentTok{\# Assign test dataset for use in dataloader(s)}
\ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"test"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"test\_size: }\SpecialCharTok{\{}\NormalTok{test\_size}\SpecialCharTok{\}}\SpecialStringTok{ used for test dataset."}\NormalTok{)}
    \CommentTok{\# get test data set as test\_abs percent of the full dataset}
\NormalTok{    generator\_test }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(test\_seed)}
\NormalTok{    data\_test, \_ }\OperatorTok{=}\NormalTok{ random\_split(data\_full, [test\_size, full\_train\_size], generator}\OperatorTok{=}\NormalTok{generator\_test)}
    \ControlFlowTok{if}\NormalTok{ scaler }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        data\_test }\OperatorTok{=}\NormalTok{ [(scaler.transform(data), target) }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_test]}
\NormalTok{        data\_tensors\_test }\OperatorTok{=}\NormalTok{ [data.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_test]}
\NormalTok{        target\_tensors\_test }\OperatorTok{=}\NormalTok{ [target.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_test]}
\NormalTok{        data\_test }\OperatorTok{=}\NormalTok{ TensorDataset(}
\NormalTok{            torch.stack(data\_tensors\_test).squeeze(}\DecValTok{1}\NormalTok{), torch.stack(target\_tensors\_test)}
\NormalTok{        )}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_test)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(data\_test, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
test_size: 0.4 used for test dataset.
LightDataModule setup(): Test set size: 177
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0490, -0.0446, -0.0418,  0.1045,  0.0356, -0.0257,  0.1775, -0.0764,
         -0.0129,  0.0155],
        [-0.0273,  0.0507, -0.0159, -0.0298,  0.0039, -0.0007,  0.0413, -0.0395,
         -0.0236,  0.0113],
        [ 0.0708,  0.0507, -0.0170,  0.0219,  0.0438,  0.0563,  0.0376, -0.0026,
         -0.0702, -0.0176],
        [-0.0382,  0.0507,  0.0714, -0.0573,  0.1539,  0.1559,  0.0008,  0.0719,
          0.0503,  0.0693],
        [ 0.0453, -0.0446,  0.0391,  0.0460,  0.0067, -0.0242,  0.0081, -0.0126,
          0.0643,  0.0569]])
Targets: tensor([103.,  53.,  80., 220., 246.])
\end{verbatim}

\subsubsection{The ``setup'' Method: Stage
``predict''}\label{sec-stage-predict-601}

Prediction and testing use the same data set. The prediction data set is
created based on the \texttt{test\_size}.

\phantomsection\label{user_data_module_setup_predict}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stage }\OperatorTok{=} \StringTok{"predict"}
\ControlFlowTok{if}\NormalTok{ stage }\OperatorTok{==} \StringTok{"predict"} \KeywordTok{or}\NormalTok{ stage }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"test\_size: }\SpecialCharTok{\{}\NormalTok{test\_size}\SpecialCharTok{\}}\SpecialStringTok{ used for predict dataset."}\NormalTok{)}
    \CommentTok{\# get test data set as test\_abs percent of the full dataset}
\NormalTok{    generator\_predict }\OperatorTok{=}\NormalTok{ torch.Generator().manual\_seed(test\_seed)}
\NormalTok{    data\_predict, \_ }\OperatorTok{=}\NormalTok{ random\_split(}
\NormalTok{        data\_full, [test\_size, full\_train\_size], generator}\OperatorTok{=}\NormalTok{generator\_predict}
\NormalTok{    )}
    \ControlFlowTok{if}\NormalTok{ scaler }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        data\_predict }\OperatorTok{=}\NormalTok{ [(scaler.transform(data), target) }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_predict]}
\NormalTok{        data\_tensors\_predict }\OperatorTok{=}\NormalTok{ [data.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_predict]}
\NormalTok{        target\_tensors\_predict }\OperatorTok{=}\NormalTok{ [target.clone().detach() }\ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_predict]}
\NormalTok{        data\_predict }\OperatorTok{=}\NormalTok{ TensorDataset(}
\NormalTok{            torch.stack(data\_tensors\_predict).squeeze(}\DecValTok{1}\NormalTok{), torch.stack(target\_tensors\_predict)}
\NormalTok{        )}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"LightDataModule setup(): Predict set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_predict)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# Set batch size for DataLoader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{5}
\CommentTok{\# Create DataLoader}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}
\NormalTok{dataloader }\OperatorTok{=}\NormalTok{ DataLoader(data\_predict, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
test_size: 0.4 used for predict dataset.
LightDataModule setup(): Predict set size: 177
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0490, -0.0446, -0.0418,  0.1045,  0.0356, -0.0257,  0.1775, -0.0764,
         -0.0129,  0.0155],
        [-0.0273,  0.0507, -0.0159, -0.0298,  0.0039, -0.0007,  0.0413, -0.0395,
         -0.0236,  0.0113],
        [ 0.0708,  0.0507, -0.0170,  0.0219,  0.0438,  0.0563,  0.0376, -0.0026,
         -0.0702, -0.0176],
        [-0.0382,  0.0507,  0.0714, -0.0573,  0.1539,  0.1559,  0.0008,  0.0719,
          0.0503,  0.0693],
        [ 0.0453, -0.0446,  0.0391,  0.0460,  0.0067, -0.0242,  0.0081, -0.0126,
          0.0643,  0.0569]])
Targets: tensor([103.,  53.,  80., 220., 246.])
\end{verbatim}

\subsection{\texorpdfstring{The \texttt{train\_dataloader()}
Method}{The train\_dataloader() Method}}\label{sec-train-dataloader-601}

The method `\texttt{train\_dataloader} returns the training dataloader,
i.e., a Pytorch DataLoader instance using the training dataset. It
simply returns a DataLoader with the \texttt{data\_train} set that was
created in the \texttt{setup()} method as described in
Section~\ref{sec-stage-fit-601}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ train\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(data\_train, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, num\_workers}\OperatorTok{=}\NormalTok{num\_workers)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the \texttt{train\_dataloader()} Method}]

The \texttt{train\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_train)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.train\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Training set size: 160
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0272, -0.0446,  0.0498, -0.0550, -0.0029,  0.0406, -0.0581,  0.0528,
         -0.0530, -0.0052],
        [-0.0818, -0.0446, -0.0817, -0.0401,  0.0026, -0.0185,  0.0707, -0.0395,
         -0.0109, -0.0922],
        [ 0.0453,  0.0507,  0.0606,  0.0311,  0.0287, -0.0473, -0.0544,  0.0712,
          0.1336,  0.1356],
        [ 0.0671,  0.0507, -0.0148,  0.0586, -0.0594, -0.0345, -0.0618,  0.0129,
         -0.0051,  0.0486],
        [-0.0164, -0.0446,  0.0542,  0.0701, -0.0332, -0.0279,  0.0081, -0.0395,
         -0.0271, -0.0094]])
Targets: tensor([144.,  51., 245., 124., 293.])
\end{verbatim}

\end{tcolorbox}

\subsection{\texorpdfstring{The \texttt{val\_dataloader()}
Method}{The val\_dataloader() Method}}\label{sec-val-dataloader-601}

Returns the validation dataloader, i.e., a Pytorch DataLoader instance
using the validation dataset. It simply returns a DataLoader with the
\texttt{data\_val} set that was created in the \texttt{setup()} method
as desccribed in Section~\ref{sec-stage-fit-601}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ val\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(data\_val, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, num\_workers}\OperatorTok{=}\NormalTok{num\_workers)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the \texttt{val\_dataloader()} Method}]

The \texttt{val\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Validation set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_val)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.val\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Validation set size: 106
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0163, -0.0446,  0.0736, -0.0412, -0.0043, -0.0135, -0.0139, -0.0011,
          0.0429,  0.0445],
        [ 0.0453, -0.0446,  0.0714,  0.0012, -0.0098, -0.0010,  0.0155, -0.0395,
         -0.0412, -0.0715],
        [ 0.0308,  0.0507,  0.0326,  0.0494, -0.0401, -0.0436, -0.0692,  0.0343,
          0.0630,  0.0031],
        [ 0.0235,  0.0507, -0.0396, -0.0057, -0.0484, -0.0333,  0.0118, -0.0395,
         -0.1016, -0.0674],
        [-0.0091,  0.0507,  0.0013, -0.0022,  0.0796,  0.0701,  0.0339, -0.0026,
          0.0267,  0.0818]])
Targets: tensor([275., 141., 208.,  78., 142.])
\end{verbatim}

\end{tcolorbox}

\subsection{\texorpdfstring{The \texttt{test\_dataloader()}
Method}{The test\_dataloader() Method}}\label{the-test_dataloader-method-1}

Returns the test dataloader, i.e., a Pytorch DataLoader instance using
the test dataset. It simply returns a DataLoader with the
\texttt{data\_test} set that was created in the \texttt{setup()} method
as described in Section~\ref{sec-stage-test-30}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ test\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(data\_test, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, num\_workers}\OperatorTok{=}\NormalTok{num\_workers)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the \texttt{test\_dataloader()} Method}]

The \texttt{test\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_test)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.test\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Test set size: 177
Batch Size: 5
Inputs Shape: torch.Size([5, 10])
Targets Shape: torch.Size([5])
---------------
Inputs: tensor([[ 0.0562, -0.0446, -0.0579, -0.0080,  0.0521,  0.0491,  0.0560, -0.0214,
         -0.0283,  0.0445],
        [ 0.0018, -0.0446, -0.0709, -0.0229, -0.0016, -0.0010,  0.0266, -0.0395,
         -0.0225,  0.0072],
        [-0.0527, -0.0446,  0.0542, -0.0263, -0.0552, -0.0339, -0.0139, -0.0395,
         -0.0741, -0.0591],
        [ 0.0054, -0.0446, -0.0482, -0.0126,  0.0012, -0.0066,  0.0634, -0.0395,
         -0.0514, -0.0591],
        [-0.0527, -0.0446, -0.0094, -0.0057,  0.0397,  0.0447,  0.0266, -0.0026,
         -0.0181, -0.0135]])
Targets: tensor([158.,  49., 142.,  96.,  59.])
\end{verbatim}

\end{tcolorbox}

\subsection{\texorpdfstring{The \texttt{predict\_dataloader()}
Method}{The predict\_dataloader() Method}}\label{the-predict_dataloader-method-1}

Returns the prediction dataloader, i.e., a Pytorch DataLoader instance
using the prediction dataset. It simply returns a DataLoader with the
\texttt{data\_predict} set that was created in the \texttt{setup()}
method as described in Section~\ref{sec-stage-predict-30}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}]

The \texttt{batch\_size} is set to the length of the
\texttt{data\_predict} set.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ predict\_dataloader(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ DataLoader:}
    \ControlFlowTok{return}\NormalTok{ DataLoader(data\_predict, batch\_size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(data\_predict), num\_workers}\OperatorTok{=}\NormalTok{num\_workers)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the \texttt{predict\_dataloader()} Method}]

The \texttt{predict\_dataloader()} method can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.lightdatamodule }\ImportTok{import}\NormalTok{ LightDataModule}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ Diabetes(target\_type}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{)}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ LightDataModule(dataset}\OperatorTok{=}\NormalTok{dataset, batch\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{, test\_size}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{data\_module.setup()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test set size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(data\_module.data\_predict)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{dl }\OperatorTok{=}\NormalTok{ data\_module.predict\_dataloader()}
\CommentTok{\# Iterate over the data in the DataLoader}
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dl:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ batch}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch Size: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{size(}\DecValTok{0}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs Shape: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets Shape: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Inputs: }\SpecialCharTok{\{}\NormalTok{inputs}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets: }\SpecialCharTok{\{}\NormalTok{targets}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Test set size: 177
Batch Size: 177
Inputs Shape: torch.Size([177, 10])
Targets Shape: torch.Size([177])
---------------
Inputs: tensor([[ 0.0562, -0.0446, -0.0579,  ..., -0.0214, -0.0283,  0.0445],
        [ 0.0018, -0.0446, -0.0709,  ..., -0.0395, -0.0225,  0.0072],
        [-0.0527, -0.0446,  0.0542,  ..., -0.0395, -0.0741, -0.0591],
        ...,
        [ 0.0090, -0.0446, -0.0321,  ..., -0.0764, -0.0119, -0.0384],
        [-0.0273, -0.0446, -0.0666,  ..., -0.0395, -0.0358, -0.0094],
        [ 0.0817,  0.0507,  0.0067,  ...,  0.0919,  0.0547,  0.0072]])
Targets: tensor([158.,  49., 142.,  96.,  59.,  74., 137., 136.,  39.,  66., 310., 198.,
        235., 116.,  55., 177.,  59., 246.,  53., 135.,  88., 198., 186., 217.,
         51., 118., 153., 180.,  51., 229.,  84.,  72., 237., 142., 185.,  91.,
         88., 148., 179., 144.,  25.,  89.,  42.,  60., 124., 170., 215., 263.,
        178., 245., 202.,  97., 321.,  71., 123., 220., 132., 243.,  61., 102.,
        187.,  70., 242., 134.,  63.,  72.,  88., 219., 127., 146., 122., 143.,
        220., 293.,  59., 317.,  60., 140.,  65., 277.,  90.,  96., 109., 190.,
         90.,  52., 160., 233., 230., 175.,  68., 272., 144.,  70.,  68., 163.,
         71.,  93., 263., 118., 220.,  90., 232., 120., 163.,  88.,  85.,  52.,
        181., 232., 212., 332.,  81., 214., 145., 268., 115.,  93.,  64., 156.,
        128., 200., 281., 103., 220.,  66.,  48., 246.,  42., 150., 125., 109.,
        129.,  97., 265.,  97., 173., 216., 237., 121.,  42., 151.,  31.,  68.,
        137., 221., 283., 124., 243., 150.,  69., 306., 182., 252., 132., 258.,
        121., 110., 292., 101., 275., 141., 208.,  78., 142., 185., 167., 258.,
        144.,  89., 225., 140., 303., 236.,  87.,  77., 131.])
\end{verbatim}

\end{tcolorbox}

\section{\texorpdfstring{Using the \texttt{LightDataModule} in the
\texttt{train\_model()}
Method}{Using the LightDataModule in the train\_model() Method}}\label{using-the-lightdatamodule-in-the-train_model-method-1}

The methods discussed so far are used in \texttt{spotpython}'s
\texttt{train\_model()} method
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/light/trainmodel/}{{[}DOC{]}}
to train the model. It is implemented as follows
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/light/trainmodel.py}{{[}SOURCE{]}}.

First, a \texttt{LightDataModule} object is created and the
\texttt{setup()} method is called.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dm }\OperatorTok{=}\NormalTok{ LightDataModule(}
\NormalTok{    dataset}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"data\_set"}\NormalTok{],}
\NormalTok{    batch\_size}\OperatorTok{=}\NormalTok{config[}\StringTok{"batch\_size"}\NormalTok{],}
\NormalTok{    num\_workers}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"num\_workers"}\NormalTok{],}
\NormalTok{    test\_size}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_size"}\NormalTok{],}
\NormalTok{    test\_seed}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"test\_seed"}\NormalTok{],}
\NormalTok{)}
\NormalTok{dm.setup()}
\end{Highlighting}
\end{Shaded}

Then, the \texttt{Trainer} is initialized.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Init trainer}
\NormalTok{trainer }\OperatorTok{=}\NormalTok{ L.Trainer(}
\NormalTok{    default\_root\_dir}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id),}
\NormalTok{    max\_epochs}\OperatorTok{=}\NormalTok{model.hparams.epochs,}
\NormalTok{    accelerator}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"accelerator"}\NormalTok{],}
\NormalTok{    devices}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"devices"}\NormalTok{],}
\NormalTok{    logger}\OperatorTok{=}\NormalTok{TensorBoardLogger(}
\NormalTok{        save\_dir}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"TENSORBOARD\_PATH"}\NormalTok{],}
\NormalTok{        version}\OperatorTok{=}\NormalTok{config\_id,}
\NormalTok{        default\_hp\_metric}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        log\_graph}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"log\_graph"}\NormalTok{],}
\NormalTok{    ),}
\NormalTok{    callbacks}\OperatorTok{=}\NormalTok{[}
\NormalTok{        EarlyStopping(monitor}\OperatorTok{=}\StringTok{"val\_loss"}\NormalTok{, patience}\OperatorTok{=}\NormalTok{config[}\StringTok{"patience"}\NormalTok{], mode}\OperatorTok{=}\StringTok{"min"}\NormalTok{, strict}\OperatorTok{=}\VariableTok{False}\NormalTok{, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{    ],}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\NormalTok{enable\_progress\_bar,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, the \texttt{fit()} method is called to train the model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pass the datamodule as arg to trainer.fit to override model hooks :)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm)}
\end{Highlighting}
\end{Shaded}

Finally, the \texttt{validate()} method is called to validate the model.
The \texttt{validate()} method returns the validation loss.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test best model on validation and test set}
\NormalTok{result }\OperatorTok{=}\NormalTok{ trainer.validate(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm)}
\CommentTok{\# unlist the result (from a list of one dict)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ result[}\DecValTok{0}\NormalTok{]}
\ControlFlowTok{return}\NormalTok{ result[}\StringTok{"val\_loss"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{The Last Connection: The \texttt{HyperLight}
Class}{The Last Connection: The HyperLight Class}}\label{the-last-connection-the-hyperlight-class}

The method \texttt{train\_model()} is part of the \texttt{HyperLight}
class
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/light/trainmodel/}{{[}DOC{]}}.
It is called from \texttt{spotpython} as an objective function to train
the model and return the validation loss.

The \texttt{HyperLight} class is implemented as follows
\href{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/fun/hyperlight.py}{{[}SOURCE{]}}.

\phantomsection\label{user_hyperlight}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ HyperLight:}
    \KeywordTok{def}\NormalTok{ fun(}\VariableTok{self}\NormalTok{, X: np.ndarray, fun\_control: }\BuiltInTok{dict} \OperatorTok{=} \VariableTok{None}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
\NormalTok{        z\_res }\OperatorTok{=}\NormalTok{ np.array([], dtype}\OperatorTok{=}\BuiltInTok{float}\NormalTok{)}
        \VariableTok{self}\NormalTok{.check\_X\_shape(X}\OperatorTok{=}\NormalTok{X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\NormalTok{        var\_dict }\OperatorTok{=}\NormalTok{ assign\_values(X, get\_var\_name(fun\_control))}
        \ControlFlowTok{for}\NormalTok{ config }\KeywordTok{in}\NormalTok{ generate\_one\_config\_from\_var\_dict(var\_dict, fun\_control):}
\NormalTok{            df\_eval }\OperatorTok{=}\NormalTok{ train\_model(config, fun\_control)}
\NormalTok{            z\_val }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"weights"}\NormalTok{] }\OperatorTok{*}\NormalTok{ df\_eval}
\NormalTok{            z\_res }\OperatorTok{=}\NormalTok{ np.append(z\_res, z\_val)}
        \ControlFlowTok{return}\NormalTok{ z\_res}
\end{Highlighting}
\end{Shaded}

\section{Further Information}\label{further-information-1}

\subsection{Preprocessing}\label{sec-preprocessing-601}

Preprocessing is handled by \texttt{Lightning} and \texttt{PyTorch}. It
is described in the
\href{https://lightning.ai/docs/pytorch/stable/data/datamodule.html}{LIGHTNINGDATAMODULE}
documentation. Here you can find information about the
\texttt{transforms} methods.

\chapter{Hyperparameter Tuning with PyTorch Lightning:
ResNets}\label{sec-light-resnets-601}

Neural ODEs are related to Residual Neural Networks (ResNets). We
consider ResNets in Section~\ref{sec-resnets}.

\section{Residual Neural Networks}\label{sec-resnets}

He et al. (2015) introduced Residual Neural Networks (ResNets).

\subsection{Residual Connections}\label{residual-connections-1}

Residual connections are a key component of ResNets. They are used to
stabilize the training of very deep networks. The idea is to learn a
residual mapping instead of the full mapping. The residual mapping is
defined as:

\begin{definition}[Residual
Connection]\protect\hypertarget{def-residual-connection}{}\label{def-residual-connection}

Let \(F\) denote a non-linear mapping (usually a sequence of NN modules
likes convolutions, activation functions, and normalizations).

Instead of modeling \[
x_{l+1}=F(x_{l}),
\] residual connections model
\begin{equation}\phantomsection\label{eq-residual-connection}{
x_{l+1}=x_{l}+F(x_{l}).
}\end{equation}

This is illustrated in Figure~\ref{fig-block}.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{./figures_static/block.png}

}

\caption{\label{fig-block}Residual Connection. Figure credit He et al.
(2015)}

\end{figure}%

Applying backpropagation to the residual mapping results in the
following gradient calculation:

\begin{equation}\phantomsection\label{eq-residual-grad}{
\frac{\partial x_{l+1}}{\partial x_{l}} = \mathbf{I} + \frac{\partial F(x_{l})}{\partial x_{l}},
}\end{equation}

where \(\mathbf{I}\) is the identity matrix. The identity matrix is
added to the gradient, which helps to stabilize the training of very
deep networks. The identity matrix ensures that the gradient is not too
small, which can happen if the gradient of \(F\) is close to zero. This
is especially important for very deep networks, where the gradient can
vanish quickly.

\end{definition}

The bias towards the identity matrix guarantees a stable gradient
propagation being less effected by \(F\) itself.

There have been many variants of ResNet proposed, which mostly concern
the function \(F\), or operations applied on the sum.
Figure~\ref{fig-resnet-block} shows two different ResNet blocks:

\begin{itemize}
\tightlist
\item
  the original ResNet block, which applies a non-linear activation
  function, usually ReLU, after the skip connection. and
\item
  the pre-activation ResNet block, which applies the non-linearity at
  the beginning of \(F\).
\end{itemize}

\begin{figure}

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{index_files/mediabag/figures_static/resnet_block.pdf}

}

\caption{\label{fig-resnet-block}ResNet Block. Left: original Residual
block in He et al. (2015). Right: pre-activation block. BN describes
batch-normalization. Figure credit He et al. (2016)}

\end{figure}%

For very deep network the pre-activation ResNet has shown to perform
better as the gradient flow is guaranteed to have the identity matrix as
shown in Equation~\ref{eq-residual-grad}, and is not harmed by any
non-linear activation applied to it.

\subsection{Implementation of the Original ResNet
Block}\label{implementation-of-the-original-resnet-block}

One special case we have to handle is when we want to reduce the image
dimensions in terms of width and height. The basic ResNet block requires
\(F(x_{l})\) to be of the same shape as \(x_{l}\). Thus, we need to
change the dimensionality of \(x_{l}\) as well before adding to
\(F(x_{l})\). The original implementation used an identity mapping with
stride 2 and padded additional feature dimensions with 0. However, the
more common implementation is to use a 1x1 convolution with stride 2 as
it allows us to change the feature dimensionality while being efficient
in parameter and computation cost. The code for the ResNet block is
relatively simple, and shown below:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}

\KeywordTok{class}\NormalTok{ ResNetBlock(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, c\_in, act\_fn, subsample}\OperatorTok{=}\VariableTok{False}\NormalTok{, c\_out}\OperatorTok{={-}}\DecValTok{1}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Inputs:}
\CommentTok{            c\_in {-} Number of input features}
\CommentTok{            act\_fn {-} Activation class constructor (e.g. nn.ReLU)}
\CommentTok{            subsample {-} If True, we need to apply a transformation inside the block to change the feature dimensionality}
\CommentTok{            c\_out {-} Number of output features. Note that this is only relevant if subsample is True, as otherwise, c\_out = c\_in}
\CommentTok{        """}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ subsample:}
\NormalTok{            c\_out }\OperatorTok{=}\NormalTok{ c\_in}

        \CommentTok{\# Network representing F}
        \VariableTok{self}\NormalTok{.net }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.Linear(c\_in, c\_out, bias}\OperatorTok{=}\VariableTok{False}\NormalTok{),  }\CommentTok{\# Linear layer for feature transformation}
\NormalTok{            nn.BatchNorm1d(c\_out),               }\CommentTok{\# Batch normalization for stable learning}
\NormalTok{            act\_fn(),                            }\CommentTok{\# Activation function}
\NormalTok{            nn.Linear(c\_out, c\_out, bias}\OperatorTok{=}\VariableTok{False}\NormalTok{), }\CommentTok{\# Second linear layer}
\NormalTok{            nn.BatchNorm1d(c\_out)                }\CommentTok{\# Batch normalization}
\NormalTok{        )}
        
        \CommentTok{\# If subsampling, adjust the input feature dimensionality using a linear layer}
        \VariableTok{self}\NormalTok{.downsample }\OperatorTok{=}\NormalTok{ nn.Linear(c\_in, c\_out) }\ControlFlowTok{if}\NormalTok{ subsample }\ControlFlowTok{else} \VariableTok{None}
        \VariableTok{self}\NormalTok{.act\_fn }\OperatorTok{=}\NormalTok{ act\_fn()}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        z }\OperatorTok{=} \VariableTok{self}\NormalTok{.net(x)  }\CommentTok{\# Apply the main network}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.downsample }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            x }\OperatorTok{=} \VariableTok{self}\NormalTok{.downsample(x)  }\CommentTok{\# Adjust dimensionality if necessary}
\NormalTok{        out }\OperatorTok{=}\NormalTok{ z }\OperatorTok{+}\NormalTok{ x  }\CommentTok{\# Residual connection}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.act\_fn(out)  }\CommentTok{\# Apply activation function}
        \ControlFlowTok{return}\NormalTok{ out}

\KeywordTok{class}\NormalTok{ ResNetRegression(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_dim, output\_dim, block, num\_blocks}\OperatorTok{=}\DecValTok{1}\NormalTok{, hidden\_dim}\OperatorTok{=}\DecValTok{64}\NormalTok{, act\_fn}\OperatorTok{=}\NormalTok{nn.ReLU):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.input\_layer }\OperatorTok{=}\NormalTok{ nn.Linear(input\_dim, hidden\_dim)  }\CommentTok{\# Input layer transformation}
        \VariableTok{self}\NormalTok{.blocks }\OperatorTok{=}\NormalTok{ nn.ModuleList([block(hidden\_dim, act\_fn) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_blocks)])  }\CommentTok{\# List of ResNet blocks}
        \VariableTok{self}\NormalTok{.output\_layer }\OperatorTok{=}\NormalTok{ nn.Linear(hidden\_dim, output\_dim)  }\CommentTok{\# Output layer for regression}
        
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.input\_layer(x)  }\CommentTok{\# Apply input layer}
        \ControlFlowTok{for}\NormalTok{ block }\KeywordTok{in} \VariableTok{self}\NormalTok{.blocks:}
\NormalTok{            x }\OperatorTok{=}\NormalTok{ block(x)  }\CommentTok{\# Apply each block}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.output\_layer(x)  }\CommentTok{\# Get final output}
        \ControlFlowTok{return}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input\_dim }\OperatorTok{=} \DecValTok{10}
\NormalTok{output\_dim }\OperatorTok{=} \DecValTok{1}
\NormalTok{hidden\_dim }\OperatorTok{=} \DecValTok{64}
\NormalTok{model }\OperatorTok{=}\NormalTok{ ResNetRegression(input\_dim, output\_dim, ResNetBlock, num\_blocks}\OperatorTok{=}\DecValTok{2}\NormalTok{, hidden\_dim}\OperatorTok{=}\NormalTok{hidden\_dim, act\_fn}\OperatorTok{=}\NormalTok{nn.ReLU)}
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ResNetRegression(
  (input_layer): Linear(in_features=10, out_features=64, bias=True)
  (blocks): ModuleList(
    (0-1): 2 x ResNetBlock(
      (net): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=False)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=64, bias=False)
        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act_fn): ReLU()
    )
  )
  (output_layer): Linear(in_features=64, out_features=1, bias=True)
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a sample input tensor with a batch size of 2}
\ImportTok{from}\NormalTok{ torchviz }\ImportTok{import}\NormalTok{ make\_dot}
\NormalTok{sample\_input }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{2}\NormalTok{, input\_dim)}

\CommentTok{\# Generate the visualization}
\NormalTok{output }\OperatorTok{=}\NormalTok{ model(sample\_input)}
\NormalTok{dot }\OperatorTok{=}\NormalTok{ make\_dot(output, params}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(model.named\_parameters()))}

\CommentTok{\# Save and render the visualization}
\NormalTok{dot.}\BuiltInTok{format} \OperatorTok{=} \StringTok{\textquotesingle{}png\textquotesingle{}}
\NormalTok{dot.render(}\StringTok{\textquotesingle{}./figures\_static/resnet\_regression\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'figures_static/resnet_regression.png'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/resnet_regression.png}

}

\caption{ResNet Regression}

\end{figure}%

\subsection{Implementation of the Pre-Activation ResNet
Block}\label{implementation-of-the-pre-activation-resnet-block}

The second block we implement is the pre-activation ResNet block. For
this, we have to change the order of layer in \texttt{self.net}, and do
not apply an activation function on the output. Additionally, the
downsampling operation has to apply a non-linearity as well as the
input, \(x_l\), has not been processed by a non-linearity yet. Hence,
the block looks as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}

\KeywordTok{class}\NormalTok{ PreActResNetBlock(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, c\_in, act\_fn, subsample}\OperatorTok{=}\VariableTok{False}\NormalTok{, c\_out}\OperatorTok{={-}}\DecValTok{1}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ subsample:}
\NormalTok{            c\_out }\OperatorTok{=}\NormalTok{ c\_in}
        \VariableTok{self}\NormalTok{.net }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.LayerNorm(c\_in),  }\CommentTok{\# Replacing BatchNorm1d with LayerNorm}
\NormalTok{            act\_fn(),}
\NormalTok{            nn.Linear(c\_in, c\_out, bias}\OperatorTok{=}\VariableTok{False}\NormalTok{),}
\NormalTok{            nn.LayerNorm(c\_out),}
\NormalTok{            act\_fn(),}
\NormalTok{            nn.Linear(c\_out, c\_out, bias}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{        )}
        \VariableTok{self}\NormalTok{.downsample }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.LayerNorm(c\_in),}
\NormalTok{            act\_fn(),}
\NormalTok{            nn.Linear(c\_in, c\_out, bias}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{        ) }\ControlFlowTok{if}\NormalTok{ subsample }\ControlFlowTok{else} \VariableTok{None}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        z }\OperatorTok{=} \VariableTok{self}\NormalTok{.net(x)}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.downsample }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            x }\OperatorTok{=} \VariableTok{self}\NormalTok{.downsample(x)}
\NormalTok{        out }\OperatorTok{=}\NormalTok{ z }\OperatorTok{+}\NormalTok{ x}
        \ControlFlowTok{return}\NormalTok{ out}

\KeywordTok{class}\NormalTok{ PreActResNetRegression(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_dim, output\_dim, block, num\_blocks}\OperatorTok{=}\DecValTok{1}\NormalTok{, hidden\_dim}\OperatorTok{=}\DecValTok{64}\NormalTok{, act\_fn}\OperatorTok{=}\NormalTok{nn.ReLU):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.input\_layer }\OperatorTok{=}\NormalTok{ nn.Linear(input\_dim, hidden\_dim)}
        \VariableTok{self}\NormalTok{.blocks }\OperatorTok{=}\NormalTok{ nn.ModuleList([block(hidden\_dim, act\_fn) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_blocks)])}
        \VariableTok{self}\NormalTok{.output\_layer }\OperatorTok{=}\NormalTok{ nn.Linear(hidden\_dim, output\_dim)}
        
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.input\_layer(x)}
        \ControlFlowTok{for}\NormalTok{ block }\KeywordTok{in} \VariableTok{self}\NormalTok{.blocks:}
\NormalTok{            x }\OperatorTok{=}\NormalTok{ block(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.output\_layer(x)}
        \ControlFlowTok{return}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input\_dim }\OperatorTok{=} \DecValTok{10}
\NormalTok{output\_dim }\OperatorTok{=} \DecValTok{1}
\NormalTok{hidden\_dim }\OperatorTok{=} \DecValTok{64}
\NormalTok{model }\OperatorTok{=}\NormalTok{ PreActResNetRegression(input\_dim, output\_dim, PreActResNetBlock, num\_blocks}\OperatorTok{=}\DecValTok{2}\NormalTok{, hidden\_dim}\OperatorTok{=}\NormalTok{hidden\_dim, act\_fn}\OperatorTok{=}\NormalTok{nn.ReLU)}
\NormalTok{model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
PreActResNetRegression(
  (input_layer): Linear(in_features=10, out_features=64, bias=True)
  (blocks): ModuleList(
    (0-1): 2 x PreActResNetBlock(
      (net): Sequential(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=False)
        (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (4): ReLU()
        (5): Linear(in_features=64, out_features=64, bias=False)
      )
    )
  )
  (output_layer): Linear(in_features=64, out_features=1, bias=True)
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torchviz }\ImportTok{import}\NormalTok{ make\_dot}
\CommentTok{\# Create a sample input tensor}
\NormalTok{sample\_input }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1}\NormalTok{, input\_dim)}

\CommentTok{\# Generate the visualization}
\NormalTok{output }\OperatorTok{=}\NormalTok{ model(sample\_input)}
\NormalTok{dot }\OperatorTok{=}\NormalTok{ make\_dot(output, params}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(model.named\_parameters()))}

\CommentTok{\# Save and render the visualization}
\NormalTok{dot.}\BuiltInTok{format} \OperatorTok{=} \StringTok{\textquotesingle{}png\textquotesingle{}}
\NormalTok{dot.render(}\StringTok{\textquotesingle{}./figures\_static/preact\_resnet\_regression\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'figures_static/preact_resnet_regression.png'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{./figures_static/preact_resnet_regression.png}

}

\caption{Pre-Activation ResNet Regression}

\end{figure}%

\subsection{The Overall ResNet
Architecture}\label{the-overall-resnet-architecture}

The overall ResNet architecture for regression consists of stacking
multiple ResNet blocks, of which some are downsampling the input. When
discussing ResNet blocks within the entire network, they are usually
grouped by output shape. If we describe the ResNet as having
\texttt{{[}3,3,3{]}} blocks, it means there are three groups of ResNet
blocks, each containing three blocks, with downsampling occurring in the
first block of the second and third groups. The final layer produces
continuous outputs suitable for regression tasks.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{index_files/mediabag/figures_static/resnet_notation.pdf}

}

\caption{ResNet Notation. Figure credit Lippe (2022)}

\end{figure}%

The \texttt{output\_dim} parameter is used to determine the number of
outputs for regression. This is set to 1 for a single regression target
by default, but can be adjusted for multiple targets. Note, a final
layer without a softmax or similar classification layer has to be added
for regression tasks. A similar notation is used by many other
implementations such as in the
\href{https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html\#resnet18}{torchvision
library} from PyTorch.

\begin{example}[Example ResNet
Model]\protect\hypertarget{exm-example-resnet}{}\label{exm-example-resnet}

~

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ make\_regression}
\ImportTok{from}\NormalTok{ types }\ImportTok{import}\NormalTok{ SimpleNamespace}

\KeywordTok{def}\NormalTok{ get\_resnet\_blocks\_by\_name():}
    \ControlFlowTok{return}\NormalTok{ \{}\StringTok{"ResNetBlock"}\NormalTok{: ResNetBlock\}}

\KeywordTok{def}\NormalTok{ get\_act\_fn\_by\_name():}
    \ControlFlowTok{return}\NormalTok{ \{}\StringTok{"relu"}\NormalTok{: nn.ReLU\}}

\CommentTok{\# Define a simple ResNetBlock for fully connected layers}
\KeywordTok{class}\NormalTok{ ResNetBlock(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, c\_in, act\_fn, subsample}\OperatorTok{=}\VariableTok{False}\NormalTok{, c\_out}\OperatorTok{={-}}\DecValTok{1}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ subsample:}
\NormalTok{            c\_out }\OperatorTok{=}\NormalTok{ c\_in}

        \VariableTok{self}\NormalTok{.net }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.Linear(c\_in, c\_out, bias}\OperatorTok{=}\VariableTok{False}\NormalTok{),}
\NormalTok{            nn.BatchNorm1d(c\_out),}
\NormalTok{            act\_fn(),}
\NormalTok{            nn.Linear(c\_out, c\_out, bias}\OperatorTok{=}\VariableTok{False}\NormalTok{),}
\NormalTok{            nn.BatchNorm1d(c\_out)}
\NormalTok{        )}
        
        \VariableTok{self}\NormalTok{.downsample }\OperatorTok{=}\NormalTok{ nn.Linear(c\_in, c\_out) }\ControlFlowTok{if}\NormalTok{ subsample }\ControlFlowTok{else} \VariableTok{None}
        \VariableTok{self}\NormalTok{.act\_fn }\OperatorTok{=}\NormalTok{ act\_fn()}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        z }\OperatorTok{=} \VariableTok{self}\NormalTok{.net(x)}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.downsample }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            x }\OperatorTok{=} \VariableTok{self}\NormalTok{.downsample(x)}
\NormalTok{        out }\OperatorTok{=}\NormalTok{ z }\OperatorTok{+}\NormalTok{ x}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.act\_fn(out)}
        \ControlFlowTok{return}\NormalTok{ out}

\CommentTok{\# Generate a simple random dataset for regression}
\NormalTok{num\_samples }\OperatorTok{=} \DecValTok{100}
\NormalTok{num\_features }\OperatorTok{=} \DecValTok{20}  \CommentTok{\# Number of features, typical in a regression dataset}
\NormalTok{X, y }\OperatorTok{=}\NormalTok{ make\_regression(n\_samples}\OperatorTok{=}\NormalTok{num\_samples, n\_features}\OperatorTok{=}\NormalTok{num\_features, noise}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}

\CommentTok{\# Convert to PyTorch tensors}
\NormalTok{X\_tensor }\OperatorTok{=}\NormalTok{ torch.tensor(X, dtype}\OperatorTok{=}\NormalTok{torch.float32)}
\NormalTok{y\_tensor }\OperatorTok{=}\NormalTok{ torch.tensor(y, dtype}\OperatorTok{=}\NormalTok{torch.float32).unsqueeze(}\DecValTok{1}\NormalTok{)  }\CommentTok{\# Add a dimension for compatibility}

\CommentTok{\# Define the ResNet model for regression}
\KeywordTok{class}\NormalTok{ ResNet(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_dim, output\_dim, num\_blocks}\OperatorTok{=}\NormalTok{[}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{], c\_hidden}\OperatorTok{=}\NormalTok{[}\DecValTok{64}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{64}\NormalTok{], act\_fn\_name}\OperatorTok{=}\StringTok{"relu"}\NormalTok{, block\_name}\OperatorTok{=}\StringTok{"ResNetBlock"}\NormalTok{, }\OperatorTok{**}\NormalTok{kwargs):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
\NormalTok{        resnet\_blocks\_by\_name }\OperatorTok{=}\NormalTok{ get\_resnet\_blocks\_by\_name()}
\NormalTok{        act\_fn\_by\_name }\OperatorTok{=}\NormalTok{ get\_act\_fn\_by\_name()}
        \ControlFlowTok{assert}\NormalTok{ block\_name }\KeywordTok{in}\NormalTok{ resnet\_blocks\_by\_name}
        \VariableTok{self}\NormalTok{.hparams }\OperatorTok{=}\NormalTok{ SimpleNamespace(output\_dim}\OperatorTok{=}\NormalTok{output\_dim, }
\NormalTok{                                       c\_hidden}\OperatorTok{=}\NormalTok{c\_hidden, }
\NormalTok{                                       num\_blocks}\OperatorTok{=}\NormalTok{num\_blocks, }
\NormalTok{                                       act\_fn\_name}\OperatorTok{=}\NormalTok{act\_fn\_name,}
\NormalTok{                                       act\_fn}\OperatorTok{=}\NormalTok{act\_fn\_by\_name[act\_fn\_name],}
\NormalTok{                                       block\_class}\OperatorTok{=}\NormalTok{resnet\_blocks\_by\_name[block\_name])}
        \VariableTok{self}\NormalTok{.\_create\_network(input\_dim)}
        \VariableTok{self}\NormalTok{.\_init\_params()}

    \KeywordTok{def}\NormalTok{ \_create\_network(}\VariableTok{self}\NormalTok{, input\_dim):}
\NormalTok{        c\_hidden }\OperatorTok{=} \VariableTok{self}\NormalTok{.hparams.c\_hidden}
        \VariableTok{self}\NormalTok{.input\_net }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.Linear(input\_dim, c\_hidden[}\DecValTok{0}\NormalTok{]),}
            \VariableTok{self}\NormalTok{.hparams.act\_fn()}
\NormalTok{        )}

\NormalTok{        blocks }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ block\_idx, block\_count }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(}\VariableTok{self}\NormalTok{.hparams.num\_blocks):}
            \ControlFlowTok{for}\NormalTok{ bc }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(block\_count):}
\NormalTok{                subsample }\OperatorTok{=}\NormalTok{ (bc }\OperatorTok{==} \DecValTok{0} \KeywordTok{and}\NormalTok{ block\_idx }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\NormalTok{                blocks.append(}
                    \VariableTok{self}\NormalTok{.hparams.block\_class(c\_in}\OperatorTok{=}\NormalTok{c\_hidden[block\_idx }\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ subsample }\ControlFlowTok{else}\NormalTok{ block\_idx}\OperatorTok{{-}}\DecValTok{1}\NormalTok{],}
\NormalTok{                                             act\_fn}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.act\_fn,}
\NormalTok{                                             subsample}\OperatorTok{=}\NormalTok{subsample,}
\NormalTok{                                             c\_out}\OperatorTok{=}\NormalTok{c\_hidden[block\_idx])}
\NormalTok{                )}
        \VariableTok{self}\NormalTok{.blocks }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{blocks)}

        \VariableTok{self}\NormalTok{.output\_net }\OperatorTok{=}\NormalTok{ nn.Linear(c\_hidden[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.hparams.output\_dim)}

    \KeywordTok{def}\NormalTok{ \_init\_params(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ m }\KeywordTok{in} \VariableTok{self}\NormalTok{.modules():}
            \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(m, nn.Linear):}
\NormalTok{                nn.init.kaiming\_normal\_(m.weight, mode}\OperatorTok{=}\StringTok{\textquotesingle{}fan\_out\textquotesingle{}}\NormalTok{, nonlinearity}\OperatorTok{=}\StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{)}
            \ControlFlowTok{elif} \BuiltInTok{isinstance}\NormalTok{(m, nn.BatchNorm1d):}
\NormalTok{                nn.init.constant\_(m.weight, }\DecValTok{1}\NormalTok{)}
\NormalTok{                nn.init.constant\_(m.bias, }\DecValTok{0}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.input\_net(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.blocks(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.output\_net(x)}
        \ControlFlowTok{return}\NormalTok{ x}

\CommentTok{\# Instantiate the model}
\NormalTok{model }\OperatorTok{=}\NormalTok{ ResNet(input\_dim}\OperatorTok{=}\NormalTok{num\_features, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Define a loss function and optimizer}
\NormalTok{criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(model.parameters(), lr}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{)}

\CommentTok{\# Example training loop}
\NormalTok{num\_epochs }\OperatorTok{=} \DecValTok{10}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
\NormalTok{    model.train()}
\NormalTok{    optimizer.zero\_grad()}
    
    \CommentTok{\# Forward pass}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ model(X\_tensor)}
    
    \CommentTok{\# Compute loss}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ criterion(output, y\_tensor)}
    
    \CommentTok{\# Backward pass and optimization}
\NormalTok{    loss.backward()}
\NormalTok{    optimizer.step()}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}Epoch }\SpecialCharTok{\{}\NormalTok{epoch}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{/}\SpecialCharTok{\{}\NormalTok{num\_epochs}\SpecialCharTok{\}}\SpecialStringTok{, Loss: }\SpecialCharTok{\{}\NormalTok{loss}\SpecialCharTok{.}\NormalTok{item()}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Epoch 1/10, Loss: 23358.44921875
Epoch 2/10, Loss: 20116.880859375
Epoch 3/10, Loss: 18029.15234375
Epoch 4/10, Loss: 16321.3203125
Epoch 5/10, Loss: 14818.8271484375
Epoch 6/10, Loss: 13486.21875
Epoch 7/10, Loss: 12254.884765625
Epoch 8/10, Loss: 11098.2001953125
Epoch 9/10, Loss: 10024.029296875
Epoch 10/10, Loss: 8975.7685546875
\end{verbatim}

\end{example}

\chapter{Neural ODEs}\label{sec-light-neural-ode-601}

Neural ODEs are related to Residual Neural Networks (ResNets). We
consider ResNets in Section~\ref{sec-resnets}.

\section{Neural Ordinary Differential
Equations}\label{neural-ordinary-differential-equations}

Neural Ordinary Differential Equations (Neural ODEs) are a class of
models that are based on ordinary differential equations (ODEs). They
are a generalization of ResNets, where the depth of the network is
treated as a continuous parameter. Neural ODEs have been introduced by
R. T. Q. Chen et al. (2018). We will consider dynamical systems first.

\begin{definition}[]\protect\hypertarget{def-dynamical-system}{}\label{def-dynamical-system}

A dynamical system is a triple \[
(\mathcal{S}, \mathcal{T}, \Phi)
\] where

\begin{itemize}
\tightlist
\item
  \(\mathcal{S}\) is the \emph{state space}
\item
  \(\mathcal{T}\) is the \emph{parameter space}, and
\item
  \(\Phi: (\mathcal{T} \times \mathcal{S}) \longrightarrow \mathcal{S}\)
  is the evolution.
\end{itemize}

\end{definition}

Definition~\ref{def-dynamical-system} is a very general definition that
includes all sort of dynamical systems. We deal with ODEs where \(\Phi\)
plays the role of the \emph{general solution}: indeed a 1-parameter
family of transformations of the state space.
\(\mathcal{T}=\mathbb{R}_{+}\) is the time, and usually,
\(\mathcal{S}=\mathbb{R}^{n}\) is the state space. The evolution takes a
point in space (initial value), a point in time, and returns the a point
in space. A general solution to an ODE is a function
\(y: I \times \mathbb{R}^{n} â¶ \mathbb{R}^{n}\): a 1-parameter (usually
time is the parameter) family of transformations of the state space. A
1-parameter family of transformations is often called a \emph{flow}.

First-order Ordinary Differential Equations (ODEs) can be defined as
follows:

\begin{definition}[First-Order Ordinary Differential Equation
(ODE)]\protect\hypertarget{def-ode}{}\label{def-ode}

\[
\mathbf{\dot{y}}(t) = f(t, \mathbf{y}(t)),\quad \mathbf{y}(t_0) = y_0,\quad f: \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n
\]

\end{definition}

The solution of the ODE is the function \(\mathbf{y}(t)\) that satisfies
the ODE and the initial condition, which can be stated as an initial
value problems (IVP), i.e.~predict \(\mathbf{y}(t_1)\) given
\(\mathbf{y}(t_0)\).

\begin{definition}[Initial Value Problem
(IVP)]\protect\hypertarget{def-ivp}{}\label{def-ivp}

\begin{equation}\phantomsection\label{eq-ivp}{
\mathbf{y}(t_1) = \mathbf{y}(t_0) + \int_{t_0}^{t_1} f(\mathbf{y}(t), t)
\mathrm{d}t = \textrm{ODESolve}(\mathbf{y}(t_0), f, t_0, t_1)
}\end{equation}

\end{definition}

The existence and uniqueness of solutions to an IVP is ensured by the
Picard-LindelÃ¶f theorem, provided the RHS of the ODE is \emph{Lipschitz
continuous}. Lipschitz continuity is a property that pops up quite often
in ODE-related results in ML.

\begin{definition}[Lipschitz
Continuity]\protect\hypertarget{def-lipschitz}{}\label{def-lipschitz}

A function \(f: X \subset \mathbb{R}^{n} â¶ \mathbb{R}^{n}\) is called
\emph{Lipschitz continuous} (with constant \(\lambda\)) if

\[
|| f(x_{1}) - f(x_{2}) || \leq \lambda ||x_{1} - x_{2}|| \quad \forall x_{1},x_{2} \in X.
\]

\end{definition}

Note that Lipschitz continuity is a stronger condition than just
continuity.

Numerical solvers can be used to perform the forward pass and solve the
IVP. If we use, for example, Euler's method, we have the following
update rule:

\begin{equation}\phantomsection\label{eq-euler-update}{
\mathbf{y}(t+h) = \mathbf{y}(t) + hf(\mathbf{y}(t), t)
}\end{equation}

where \(h\) is the step size. The update rule is applied iteratively to
solve the IVP. The solution is a discrete approximation of the
continuous function \(\mathbf{y}(t)\).

Equation~\ref{eq-euler-update} looks almost identical to a ResNet block
(see Equation~\ref{eq-residual-connection}). This was one of the main
motivations for Neural ODEs (R. T. Q. Chen et al. 2018).

ResNets update hidden states by employing residual connections:

\[
\mathbf{y}_{l+1} = \mathbf{y}_l + f(\mathbf{y}_l, \theta_l)
\]

where \(f\) is a neural network with parameters \(\theta_l\), and
\(\mathbf{y}_l\) and \(\mathbf{y}_{l+1}\) are the hidden states at
subsequent layers, \(l \in \{0,
\ldots, L\}\).

These updates can be seen as Euler discretizations of continuous
transformations.

\begin{align}
\mathbf{\dot{y}} &= f(\mathbf{y}, t, \theta)
\\
&\Bigg\downarrow \ \textrm{Euler Discretization}
\\
\mathbf{y}_{n+1} &= \mathbf{y}_n + h f(\mathbf{y}_n, t_n, \theta)
\end{align}

What happens in a residual network (with step sizes \(h\)) if we
consider the continuous limit of each discrete layer in the network?
What happens as we add more layers and take smaller steps? The answer
seems rather astounding: instead of having a discrete number of layers
between the input and output domains, we allow the evolution of the
hidden states to become continuous.

\begin{figure}

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{./figures_static/resnet_0_viz.png}

}

\caption{\label{fig-neural-ode}A residual network defines a discrete
sequence of finite transformations. Circles represent evaluation
locations. Figure credit R. T. Q. Chen et al. (2018).}

\end{figure}%

\begin{figure}

\centering{

\includegraphics[width=0.5\linewidth,height=\textheight,keepaspectratio]{./figures_static/odenet_0_viz.png}

}

\caption{\label{fig-resnet-ode}An ODE network defines a vector field,
which continuously transforms the state. Circles represent evaluation
locations. Figure credit R. T. Q. Chen et al. (2018).}

\end{figure}%

The main technical difficulty in training continuous-depth networks is
performing backpropagation through the ODE solver. Differentiating
through the operations of the forward pass is straightforward, but
incurs a high memory cost and introduces additional numerical error.

Pontryagin (1987) treated the ODE solver as a black box, and computed
gradients using the adjoint sensitivity method. This approach computes
gradients by solving a second, augmented ODE backwards in time, and is
applicable to all ODE solvers. It scales linearly with problem size, has
low memory cost, and explicitly controls numerical error.

Consider optimizing a scalar-valued loss function \(L()\), whose input
is the result of an ODE solver:
\begin{equation}\phantomsection\label{eq-ode-loss}{
L(y(t_1) = L \left(y(t_0) + \int_{t_0}^{t_1} f(y(t), t, \theta) dt \right) = L \left( \textrm{ODESolve}( y(t_0), f, t_0, t_1, \theta) \right)
}\end{equation}

Equation~\ref{eq-ode-loss} is related to \{Equation~\ref{eq-ivp}\}. To
optimize \(L\), we require gradients with respect to \(\theta\).

Similar to standard neural networks, we start with determining how the
gradient of the loss depends on the hidden state \(y(t)\) at each
instant. This quantity is called the adjoint
\(a(t) = \frac{\partial{L}}{\partial y(t)}\). It satisfies the following
IVP:

\[
 \dot{\mathbf{a}}(t) = -\mathbf{a}(t)^{\top} \frac{\partial f(\mathbf{x}(t), t,
\theta)}{\partial \mathbf{x}}, \quad \mathbf{a}(t_1) = \frac{\partial L}{\partial \mathbf{x}(t_1)}.
\]

Its dynamics are given by another ODE, which can be thought of as the
instantaneous analog of the chain rule: \[
\frac{d a(t)}{d t} = - a(t)^{T} \frac{\partial f(y(t), t, \theta)}{\partial y}.
\]

Thus, starting from the initial (remember we are running backwards)
value \(\mathbf{a}(t_1) = \frac{\partial L}{\partial \mathbf{x}(t_1)}\),
we can compute
\(\mathbf{a}(t_0) = \frac{\partial L}{\partial \mathbf{x}(t_0)}\) by
another call to an ODE solver.

Finally, computing the gradients with respect to the parameters
\(\theta\) requires evaluating a third integral, which depends on both
\(\mathbf{x}(t)\) and \(\mathbf{a}(t)\):

\[
\frac{\mathrm{d}L}{\mathrm{d}\theta} = -\int_{t_1}^{t_0} \mathbf{a}(t)^{\top}\frac{\partial f}{\partial \theta} \mathrm{d}t,
\]

So this method trades off computation for memory---in fact the memory
requirement for this gradient calculation is only \(\mathcal{O}(1)\)
with respect to the number of layers. The corresponding algorithm is
described in R. T. Q. Chen et al. (2018), see also
Figure~\ref{fig-adjoint}.

\begin{figure}

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{./figures_static/AdjointFig_w_L.png}

}

\caption{\label{fig-adjoint}Reverse-mode differentiation of an ODE
solution. The adjoint sensitivity method solves an augmented ODE
backwards in time. The augmented system contains both the original state
and the sensitivity of the loss with respect to the state. If the loss
depends directly on the state at multiple observation times, the adjoint
state must be updated in the direction of the partial derivative of the
loss with respect to each observation. Figure credit R. T. Q. Chen et
al. (2018).}

\end{figure}%

\href{https://vaipatel.com/deriving-the-adjoint-equation-for-neural-odes-using-lagrange-multipliers/\#:~:text=Luckily\%2C\%20a\%20very\%20well\%2Dknown,to\%20store\%20intermediate\%20function\%20evaluations.}{Here}
you can find a very good explanation of the following result based on
Lagrange multipliers.

\section{Regression Example}\label{regression-example}

To illustrate this concept, we will consider a simple regression
example. This example is based on the Neural-ODEs tutorial from
\href{https://github.com/manncodes/neural-ODEs/blob/main/chapters/Chapter\%203/Neural_Ordinary_Differential_Equations.ipynb}{Neural
Ordinary Differential Equations}, which is provided by R. T. Q. Chen et
al. (2018). We will use the ODE solvers from
\href{https://github.com/rtqichen/torchdiffeq}{Torchdiffeq}.

Neural ODEs, or ODE-Nets, build complex models by chaining together
simple building blocks, similar to residual networks. Here, our base
layer will define the dynamics of an ODE, which will be interconnected
using an ODE solver to form the complete neural network model.

\subsection{Specifying the Dynamics
Layer}\label{specifying-the-dynamics-layer}

The dynamics of an ODE can be captured by the equation:

\[
\dot y(t) = f(y(t), t,  \theta), \qquad y(0) = y_0,
\] where the initial value \(y_0 \in \mathbb{R}^n\). The \(\theta\)
parameters were added to the dynamics, so the dynamics function has the
dimensions
\(f : \mathbb{R}^{n} \times \mathbb{R} \times \mathbb{R}^{|\theta|} \to \mathbb{R}^n\),
where \(|\theta|\) is the number of parameters we've added to \(f\).

We need the dynamics function to take in the current state \(y(t)\) of
the ODE, the current time \(t\), and some parameters \(\theta\), and
output \(\frac{\partial y(t)}{\partial t}\), which has the same shape as
\(y(t)\). They are passed as input to a multi-layer perceptron (MLP).
Multiple evaluations of this dynamics layer can be combined using any
suitable ODE solver, such as the adaptive-step Dormand-Price solver
implemented in the \texttt{torchdiffeq} library's \texttt{odeint}
function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ torchdiffeq}
\end{Highlighting}
\end{Shaded}

Let's start by defining an MLP class to serve as the building block of
our models.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ MLP(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, layer\_sizes):}
        \BuiltInTok{super}\NormalTok{(MLP, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
\NormalTok{        layers }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(layer\_sizes) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
\NormalTok{            layers.append(nn.Linear(layer\_sizes[i], layer\_sizes[i}\OperatorTok{+}\DecValTok{1}\NormalTok{]))}
            \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textless{}} \BuiltInTok{len}\NormalTok{(layer\_sizes) }\OperatorTok{{-}} \DecValTok{2}\NormalTok{:}
\NormalTok{                layers.append(nn.Tanh())}
        \VariableTok{self}\NormalTok{.network }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{layers)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.network(x)}
\end{Highlighting}
\end{Shaded}

Next, we'll define a ResNet class that uses the MLP as its inner
component.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ ResNet(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, layer\_sizes, depth):}
        \BuiltInTok{super}\NormalTok{(ResNet, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.mlp }\OperatorTok{=}\NormalTok{ MLP(layer\_sizes)}
        \VariableTok{self}\NormalTok{.depth }\OperatorTok{=}\NormalTok{ depth}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
        \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\VariableTok{self}\NormalTok{.depth):}
\NormalTok{            x }\OperatorTok{=} \VariableTok{self}\NormalTok{.mlp(x) }\OperatorTok{+}\NormalTok{ x}
        \ControlFlowTok{return}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{ODEFunc} defines how the system evolves over time using the
  MLP to approximate derivatives \(\dot{y}(t)\).
\item
  \texttt{ODEBlock} specifies the network structure. It uses
  \texttt{torchdiffeq.odeint} to integrate these dynamics over time.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ ODEFunc(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, layer\_sizes):}
        \BuiltInTok{super}\NormalTok{(ODEFunc, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.mlp }\OperatorTok{=}\NormalTok{ MLP(layer\_sizes)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, t, y):}
\NormalTok{        t\_expanded }\OperatorTok{=}\NormalTok{ t.expand\_as(y)}
\NormalTok{        state\_and\_time }\OperatorTok{=}\NormalTok{ torch.cat([y, t\_expanded], dim}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.mlp(state\_and\_time)}

\KeywordTok{class}\NormalTok{ ODEBlock(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, odefunc):}
        \BuiltInTok{super}\NormalTok{(ODEBlock, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.odefunc }\OperatorTok{=}\NormalTok{ odefunc}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        t }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{])}
\NormalTok{        out }\OperatorTok{=}\NormalTok{ torchdiffeq.odeint(}\VariableTok{self}\NormalTok{.odefunc, x, t, atol}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{, rtol}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ out[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Generate a toy 1D dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inputs }\OperatorTok{=}\NormalTok{ torch.linspace(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\DecValTok{10}\NormalTok{).reshape(}\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{targets }\OperatorTok{=}\NormalTok{ inputs}\OperatorTok{**}\DecValTok{3} \OperatorTok{+}  \FloatTok{0.1} \OperatorTok{*}\NormalTok{ inputs}
\end{Highlighting}
\end{Shaded}

We specify the hyperparameters for the ResNet and ODE-Net.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{layer\_sizes }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{param\_scale }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{step\_size }\OperatorTok{=} \FloatTok{0.01}
\NormalTok{train\_iters }\OperatorTok{=} \DecValTok{1000}
\NormalTok{resnet\_depth }\OperatorTok{=} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

Initialize and train the ResNet.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resnet }\OperatorTok{=}\NormalTok{ ResNet(layer\_sizes, resnet\_depth)}
\NormalTok{criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.SGD(resnet.parameters(), lr}\OperatorTok{=}\NormalTok{step\_size)}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(train\_iters):}
\NormalTok{    optimizer.zero\_grad()}
\NormalTok{    outputs }\OperatorTok{=}\NormalTok{ resnet(inputs)}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ criterion(outputs, targets)}
\NormalTok{    loss.backward()}
\NormalTok{    optimizer.step()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We need to change the input dimension to 2, to allow time{-}dependent dynamics.}
\NormalTok{odenet\_layer\_sizes }\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{1}\NormalTok{]}

\CommentTok{\# Initialize and train ODE{-}Net.}
\NormalTok{odefunc }\OperatorTok{=}\NormalTok{ ODEFunc(odenet\_layer\_sizes)}
\NormalTok{odenet }\OperatorTok{=}\NormalTok{ ODEBlock(odefunc)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.SGD(odenet.parameters(), lr}\OperatorTok{=}\NormalTok{step\_size)}

\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(train\_iters):}
\NormalTok{    optimizer.zero\_grad()}
\NormalTok{    outputs }\OperatorTok{=}\NormalTok{ odenet(inputs)}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ criterion(outputs, targets)}
\NormalTok{    loss.backward()}
\NormalTok{    optimizer.step()}
\end{Highlighting}
\end{Shaded}

Finally, plot the predictions of both models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fine\_inputs }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{3.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, }\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{fine\_inputs\_tensor }\OperatorTok{=}\NormalTok{ torch.from\_numpy(fine\_inputs).}\BuiltInTok{float}\NormalTok{()}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{), dpi}\OperatorTok{=}\DecValTok{150}\NormalTok{)}
\NormalTok{plt.scatter(inputs, targets, color}\OperatorTok{=}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Targets\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(fine\_inputs, resnet(fine\_inputs\_tensor).detach().numpy(), color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}ResNet predictions\textquotesingle{}}\NormalTok{)}

\NormalTok{plt.plot(fine\_inputs, odenet(fine\_inputs\_tensor).detach().numpy(), color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}ODE Net predictions\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Input\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Output\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_neural_ode_files/figure-pdf/cell-10-output-1.pdf}}

\section{Further Reading}\label{further-reading}

Neural ODEs have received a lot of attention in the past few years, ever
since their introduction in Neurips 2018. Some of many many work in this
field include:

\begin{itemize}
\tightlist
\item
  Neural Stochastic Differential Equations (Neural SDEs),
\item
  Neural Controlled Differential Equations (Neural CDEs),
\item
  Graph ODEs,
\item
  Hamiltonial Neural Networks, and
\item
  Lagrangian Neural Networks.
\end{itemize}

\href{https://zymrael.github.io/}{Michael Poli} maintains the excellent
\href{https://github.com/Zymrael/awesome-neural-ode}{Awesome Neural
ODE}, a collection of resources regarding the interplay between neural
differential equations, dynamical systems, deep learning, control,
numerical methods and scientific machine learning.

\href{https://github.com/DiffEqML/torchdyn}{Torchdyn} is an excellent
library for Neural Differential Equations.

\href{https://implicit-layers-tutorial.org/}{Implicit Layers} is a list
of tutorials on implicit functions and automatic differentiation, Neural
ODEs, and Deep Equilibrium Models.

\href{https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html}{Understanding
Neural ODE's} is an excellent blogpost on ODEs and Neural ODEs.

\href{https://kidger.site/}{Patrick Kidger}'s doctoral dissertation is
an excellent textbook on Neural Differential Equations, see Kidger
(2022).

\chapter{Neural ODE Example}\label{neural-ode-example}

\section{Implementation of a Neural
ODE}\label{implementation-of-a-neural-ode}

The following example is based on the ``UvA Deep Learning Tutorials''
(Lippe 2022).

\begin{example}[Example: Neural
ODE]\protect\hypertarget{exm-uva-neural-ode}{}\label{exm-uva-neural-ode}

~

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{matplotlib inline}
\ImportTok{import}\NormalTok{ time}
\ImportTok{import}\NormalTok{ logging}
\ImportTok{import}\NormalTok{ statistics}
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Optional, List}

\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib.image }\ImportTok{as}\NormalTok{ mpimg}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}
\ImportTok{from}\NormalTok{ matplotlib.animation }\ImportTok{import}\NormalTok{ FuncAnimation}
\ImportTok{from}\NormalTok{ IPython.display }\ImportTok{import}\NormalTok{ HTML}

\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.utils.data }\ImportTok{as}\NormalTok{ data}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ Dataset}

\ControlFlowTok{try}\NormalTok{:}
    \ImportTok{import}\NormalTok{ torchdiffeq}
\ControlFlowTok{except} \PreprocessorTok{ModuleNotFoundError}\NormalTok{:}
    \OperatorTok{!}\NormalTok{pip install }\OperatorTok{{-}{-}}\NormalTok{quiet torchdiffeq}
    \ImportTok{import}\NormalTok{ torchdiffeq}

\ControlFlowTok{try}\NormalTok{:}
    \ImportTok{import}\NormalTok{ rich}
\ControlFlowTok{except} \PreprocessorTok{ModuleNotFoundError}\NormalTok{:}
    \OperatorTok{!}\NormalTok{pip install }\OperatorTok{{-}{-}}\NormalTok{quiet rich}
    \ImportTok{import}\NormalTok{ rich}

\ControlFlowTok{try}\NormalTok{:}
    \CommentTok{\# import pytorch\_lightning as pl}
    \ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ pl}
\ControlFlowTok{except} \PreprocessorTok{ModuleNotFoundError}\NormalTok{:}
    \OperatorTok{!}\NormalTok{pip install }\OperatorTok{{-}{-}}\NormalTok{quiet pytorch}\OperatorTok{{-}}\NormalTok{lightning}\OperatorTok{\textgreater{}=}\FloatTok{1.4}
    \CommentTok{\# import pytorch\_lightning as pl}
    \ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ pl}
\ImportTok{from}\NormalTok{ torchmetrics.classification }\ImportTok{import}\NormalTok{ Accuracy}

\NormalTok{pl.seed\_everything(}\DecValTok{42}\NormalTok{)}

\NormalTok{torch.backends.cudnn.deterministic }\OperatorTok{=} \VariableTok{True}
\NormalTok{torch.backends.cudnn.benchmark }\OperatorTok{=} \VariableTok{False}

\NormalTok{device }\OperatorTok{=}\NormalTok{ torch.device(}\StringTok{"cuda"} \ControlFlowTok{if}\NormalTok{ torch.cuda.is\_available() }\ControlFlowTok{else} \StringTok{"cpu"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Device: }\SpecialCharTok{\{}\NormalTok{device}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\ImportTok{from}\NormalTok{ torchmetrics.functional }\ImportTok{import}\NormalTok{ accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Device: cpu
\end{verbatim}

First, we define the core of our Neural ODE model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ \_ODEFunc(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, module, autonomous}\OperatorTok{=}\VariableTok{True}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.module }\OperatorTok{=}\NormalTok{ module}
        \VariableTok{self}\NormalTok{.autonomous }\OperatorTok{=}\NormalTok{ autonomous}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, t, x):}
        \ControlFlowTok{if} \KeywordTok{not} \VariableTok{self}\NormalTok{.autonomous:}
\NormalTok{            x }\OperatorTok{=}\NormalTok{ torch.cat([torch.ones\_like(x[:, [}\DecValTok{0}\NormalTok{]]) }\OperatorTok{*}\NormalTok{ t, x], }\DecValTok{1}\NormalTok{)}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.module(x)}


\KeywordTok{class}\NormalTok{ ODEBlock(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, odefunc: nn.Module, solver: }\BuiltInTok{str} \OperatorTok{=} \StringTok{\textquotesingle{}dopri5\textquotesingle{}}\NormalTok{,}
\NormalTok{                 rtol: }\BuiltInTok{float} \OperatorTok{=} \FloatTok{1e{-}4}\NormalTok{, atol: }\BuiltInTok{float} \OperatorTok{=} \FloatTok{1e{-}4}\NormalTok{, adjoint: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{True}\NormalTok{,}
\NormalTok{                 autonomous: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{True}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.odefunc }\OperatorTok{=}\NormalTok{ \_ODEFunc(odefunc, autonomous}\OperatorTok{=}\NormalTok{autonomous)}
        \VariableTok{self}\NormalTok{.rtol }\OperatorTok{=}\NormalTok{ rtol}
        \VariableTok{self}\NormalTok{.atol }\OperatorTok{=}\NormalTok{ atol}
        \VariableTok{self}\NormalTok{.solver }\OperatorTok{=}\NormalTok{ solver}
        \VariableTok{self}\NormalTok{.use\_adjoint }\OperatorTok{=}\NormalTok{ adjoint}
        \VariableTok{self}\NormalTok{.integration\_time }\OperatorTok{=}\NormalTok{ torch.tensor([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], dtype}\OperatorTok{=}\NormalTok{torch.float32)  }
    
    \AttributeTok{@property}
    \KeywordTok{def}\NormalTok{ ode\_method(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ torchdiffeq.odeint\_adjoint }\ControlFlowTok{if} \VariableTok{self}\NormalTok{.use\_adjoint }\ControlFlowTok{else}\NormalTok{ torchdiffeq.odeint}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x: torch.Tensor, adjoint: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{True}\NormalTok{, integration\_time}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
\NormalTok{        integration\_time }\OperatorTok{=} \VariableTok{self}\NormalTok{.integration\_time }\ControlFlowTok{if}\NormalTok{ integration\_time }\KeywordTok{is} \VariableTok{None} \ControlFlowTok{else}\NormalTok{ integration\_time}
\NormalTok{        integration\_time }\OperatorTok{=}\NormalTok{ integration\_time.to(x.device)}
\NormalTok{        ode\_method }\OperatorTok{=}\NormalTok{  torchdiffeq.odeint\_adjoint }\ControlFlowTok{if}\NormalTok{ adjoint }\ControlFlowTok{else}\NormalTok{ torchdiffeq.odeint}
\NormalTok{        out }\OperatorTok{=}\NormalTok{ ode\_method(}
            \VariableTok{self}\NormalTok{.odefunc, x, integration\_time, rtol}\OperatorTok{=}\VariableTok{self}\NormalTok{.rtol,}
\NormalTok{            atol}\OperatorTok{=}\VariableTok{self}\NormalTok{.atol, method}\OperatorTok{=}\VariableTok{self}\NormalTok{.solver)}
        \ControlFlowTok{return}\NormalTok{ out}
\end{Highlighting}
\end{Shaded}

Next, we will wrap everything together in a LightningModule.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ Learner(pl.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, model:nn.Module, t\_span:torch.Tensor, learning\_rate:}\BuiltInTok{float}\OperatorTok{=}\FloatTok{5e{-}3}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.model }\OperatorTok{=}\NormalTok{ model}
        \VariableTok{self}\NormalTok{.t\_span }\OperatorTok{=}\NormalTok{ t\_span}
        \VariableTok{self}\NormalTok{.learning\_rate }\OperatorTok{=}\NormalTok{ learning\_rate}
        \CommentTok{\# self.accuracy = Accuracy(num\_classes=2)}
        \VariableTok{self}\NormalTok{.accuracy }\OperatorTok{=}\NormalTok{ accuracy}
    
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.model(x)}

    \KeywordTok{def}\NormalTok{ inference(}\VariableTok{self}\NormalTok{, x, time\_span):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.model(x, adjoint}\OperatorTok{=}\VariableTok{False}\NormalTok{, integration\_time}\OperatorTok{=}\NormalTok{time\_span)}

    \KeywordTok{def}\NormalTok{ inference\_no\_projection(}\VariableTok{self}\NormalTok{, x, time\_span):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.model.forward\_no\_projection(x, adjoint}\OperatorTok{=}\VariableTok{False}\NormalTok{, integration\_time}\OperatorTok{=}\NormalTok{time\_span)}
   
    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch      }
\NormalTok{        y\_pred }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ y\_pred[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]  }\CommentTok{\# select last point of solution trajectory}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ nn.CrossEntropyLoss()(y\_pred, y)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{\textquotesingle{}train\_loss\textquotesingle{}}\NormalTok{, loss, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch      }
\NormalTok{        y\_pred }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ y\_pred[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]  }\CommentTok{\# select last point of solution trajectory}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ nn.CrossEntropyLoss()(y\_pred, y)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{\textquotesingle{}val\_loss\textquotesingle{}}\NormalTok{, loss, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{        acc }\OperatorTok{=} \VariableTok{self}\NormalTok{.accuracy(y\_pred.softmax(dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{), y, num\_classes}\OperatorTok{=}\DecValTok{2}\NormalTok{, task}\OperatorTok{=}\StringTok{"MULTICLASS"}\NormalTok{)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{\textquotesingle{}val\_accuracy\textquotesingle{}}\NormalTok{, acc, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch      }
\NormalTok{        y\_pred }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ y\_pred[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]  }\CommentTok{\# select last point of solution trajectory}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ nn.CrossEntropyLoss()(y\_pred, y)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{\textquotesingle{}test\_loss\textquotesingle{}}\NormalTok{, loss, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{        acc }\OperatorTok{=} \VariableTok{self}\NormalTok{.accuracy(y\_pred.softmax(dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{), y, num\_classes}\OperatorTok{=}\DecValTok{2}\NormalTok{, task}\OperatorTok{=}\StringTok{"MULTICLASS"}\NormalTok{)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{\textquotesingle{}test\_accuracy\textquotesingle{}}\NormalTok{, acc, prog\_bar}\OperatorTok{=}\VariableTok{True}\NormalTok{, logger}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{):}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ torch.optim.Adam(}\VariableTok{self}\NormalTok{.model.parameters(), lr}\OperatorTok{=}\VariableTok{self}\NormalTok{.learning\_rate)}
        \ControlFlowTok{return}\NormalTok{ optimizer}
\end{Highlighting}
\end{Shaded}

We will be working will Half Moons Dataset, a non-linearly separable,
binary classification dataset. The code is based on the excellent
TorchDyn tutorials (https://github.com/DiffEqML/torchdyn), as well as
the original TorchDiffEq examples
(https://github.com/rtqichen/torchdiffeq).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ MoonsDataset(Dataset):}
    \CommentTok{"""Half Moons Classification Dataset}
\CommentTok{    }
\CommentTok{    Adapted from https://github.com/DiffEqML/torchdyn}
\CommentTok{    """}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, num\_samples}\OperatorTok{=}\DecValTok{100}\NormalTok{, noise\_std}\OperatorTok{=}\FloatTok{1e{-}4}\NormalTok{):}
        \VariableTok{self}\NormalTok{.num\_samples }\OperatorTok{=}\NormalTok{ num\_samples}
        \VariableTok{self}\NormalTok{.noise\_std }\OperatorTok{=}\NormalTok{ noise\_std}
        \VariableTok{self}\NormalTok{.X, }\VariableTok{self}\NormalTok{.y }\OperatorTok{=} \VariableTok{self}\NormalTok{.generate\_moons(num\_samples, noise\_std)}

    \AttributeTok{@staticmethod}
    \KeywordTok{def}\NormalTok{ generate\_moons(num\_samples}\OperatorTok{=}\DecValTok{100}\NormalTok{, noise\_std}\OperatorTok{=}\FloatTok{1e{-}4}\NormalTok{):}
        \CommentTok{"""Creates a *moons* dataset of \textasciigrave{}num\_samples\textasciigrave{} data points.}
\CommentTok{        :param num\_samples: number of data points in the generated dataset}
\CommentTok{        :type num\_samples: int}
\CommentTok{        :param noise\_std: standard deviation of noise magnitude added to each data point}
\CommentTok{        :type noise\_std: float}
\CommentTok{        """}
\NormalTok{        num\_samples\_out }\OperatorTok{=}\NormalTok{ num\_samples }\OperatorTok{//} \DecValTok{2}
\NormalTok{        num\_samples\_in }\OperatorTok{=}\NormalTok{ num\_samples }\OperatorTok{{-}}\NormalTok{ num\_samples\_out}
\NormalTok{        theta\_out }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, np.pi, num\_samples\_out)}
\NormalTok{        theta\_in }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, np.pi, num\_samples\_in)}
\NormalTok{        outer\_circ\_x }\OperatorTok{=}\NormalTok{ np.cos(theta\_out)}
\NormalTok{        outer\_circ\_y }\OperatorTok{=}\NormalTok{ np.sin(theta\_out)}
\NormalTok{        inner\_circ\_x }\OperatorTok{=} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ np.cos(theta\_in)}
\NormalTok{        inner\_circ\_y }\OperatorTok{=} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ np.sin(theta\_in) }\OperatorTok{{-}} \FloatTok{0.5}

\NormalTok{        X }\OperatorTok{=}\NormalTok{ np.vstack([np.append(outer\_circ\_x, inner\_circ\_x),}
\NormalTok{                       np.append(outer\_circ\_y, inner\_circ\_y)]).T}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ np.hstack([np.zeros(num\_samples\_out), np.ones(num\_samples\_in)])}

        \ControlFlowTok{if}\NormalTok{ noise\_std }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            X }\OperatorTok{+=}\NormalTok{ noise\_std }\OperatorTok{*}\NormalTok{ np.random.rand(num\_samples, }\DecValTok{2}\NormalTok{)}

\NormalTok{        X }\OperatorTok{=}\NormalTok{ torch.Tensor(X)}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ torch.LongTensor(y)}
        \ControlFlowTok{return}\NormalTok{ X, y}

    \KeywordTok{def} \FunctionTok{\_\_len\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.num\_samples}

    \KeywordTok{def} \FunctionTok{\_\_getitem\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, idx):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.X[idx], }\VariableTok{self}\NormalTok{.y[idx]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_binary\_classification\_dataset(X, y, title}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
\NormalTok{    CLASS\_COLORS }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}coral\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}darkviolet\textquotesingle{}}\NormalTok{]}
\NormalTok{    fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\NormalTok{    ax.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\NormalTok{[CLASS\_COLORS[yi.}\BuiltInTok{int}\NormalTok{()] }\ControlFlowTok{for}\NormalTok{ yi }\KeywordTok{in}\NormalTok{ y], alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{)}
\NormalTok{    ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ title }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        ax.set\_title(title)}

    \ControlFlowTok{return}\NormalTok{ fig, ax}
\end{Highlighting}
\end{Shaded}

Let's create a sample dataset and visualize it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_dataset }\OperatorTok{=}\NormalTok{ MoonsDataset(num\_samples}\OperatorTok{=}\DecValTok{400}\NormalTok{, noise\_std}\OperatorTok{=}\FloatTok{1e{-}1}\NormalTok{)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plot\_binary\_classification\_dataset(sample\_dataset.X, sample\_dataset.y, title}\OperatorTok{=}\StringTok{\textquotesingle{}Half Moons Dataset\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_neural_ode_example_files/figure-pdf/half-moons-dataset-output-1.pdf}}

Let's now create the train, validation, and test sets, with their
corresponding data loaders. We will create a single big dataset and
randomly split it in train, val, and test sets.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ split\_dataset(dataset\_size:}\BuiltInTok{int}\NormalTok{, split\_percentages:List[}\BuiltInTok{float}\NormalTok{]) }\OperatorTok{{-}\textgreater{}}\NormalTok{ List[}\BuiltInTok{int}\NormalTok{]:}
\NormalTok{    split\_sizes }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(pi }\OperatorTok{*}\NormalTok{ dataset\_size) }\ControlFlowTok{for}\NormalTok{ pi }\KeywordTok{in}\NormalTok{ split\_percentages]}
\NormalTok{    split\_sizes[}\DecValTok{0}\NormalTok{] }\OperatorTok{+=}\NormalTok{ dataset\_size }\OperatorTok{{-}} \BuiltInTok{sum}\NormalTok{(split\_sizes)}
    \ControlFlowTok{return}\NormalTok{ split\_sizes}


\KeywordTok{class}\NormalTok{ ToyDataModule(pl.LightningDataModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, dataset\_size:}\BuiltInTok{int}\NormalTok{, split\_percentages:Optional[}\BuiltInTok{float}\NormalTok{]}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.dataset\_size }\OperatorTok{=}\NormalTok{ dataset\_size}
        \ControlFlowTok{if}\NormalTok{ split\_percentages }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            split\_percentages }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{]}
        \VariableTok{self}\NormalTok{.split\_sizes }\OperatorTok{=}\NormalTok{ split\_dataset(}\VariableTok{self}\NormalTok{.dataset\_size, split\_percentages)}

    \KeywordTok{def}\NormalTok{ prepare\_data(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{pass}

    \KeywordTok{def}\NormalTok{ setup(}\VariableTok{self}\NormalTok{, stage: Optional[}\BuiltInTok{str}\NormalTok{] }\OperatorTok{=} \VariableTok{None}\NormalTok{):}
        \ControlFlowTok{pass}

    \KeywordTok{def}\NormalTok{ train\_dataloader(}\VariableTok{self}\NormalTok{):}
\NormalTok{        train\_loader }\OperatorTok{=}\NormalTok{ torch.utils.data.DataLoader(}\VariableTok{self}\NormalTok{.train\_set, batch\_size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.train\_set), shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ train\_loader}

    \KeywordTok{def}\NormalTok{ val\_dataloader(}\VariableTok{self}\NormalTok{):}
\NormalTok{        val\_loader }\OperatorTok{=}\NormalTok{ torch.utils.data.DataLoader(}\VariableTok{self}\NormalTok{.val\_set, batch\_size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.val\_set), shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ val\_loader}

    \KeywordTok{def}\NormalTok{ test\_dataloader(}\VariableTok{self}\NormalTok{):}
\NormalTok{        test\_loader }\OperatorTok{=}\NormalTok{ torch.utils.data.DataLoader(}\VariableTok{self}\NormalTok{.test\_set, batch\_size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.test\_set), shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ test\_loader}


\KeywordTok{class}\NormalTok{ HalfMoonsDataModule(ToyDataModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, dataset\_size:}\BuiltInTok{int}\NormalTok{, split\_percentages:Optional[}\BuiltInTok{float}\NormalTok{]}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{(dataset\_size, split\_percentages}\OperatorTok{=}\NormalTok{split\_percentages)}

    \KeywordTok{def}\NormalTok{ setup(}\VariableTok{self}\NormalTok{, stage: Optional[}\BuiltInTok{str}\NormalTok{] }\OperatorTok{=} \VariableTok{None}\NormalTok{):}
\NormalTok{        dataset }\OperatorTok{=}\NormalTok{ MoonsDataset(num\_samples}\OperatorTok{=}\VariableTok{self}\NormalTok{.dataset\_size, noise\_std}\OperatorTok{=}\FloatTok{1e{-}1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.train\_set, }\VariableTok{self}\NormalTok{.val\_set, }\VariableTok{self}\NormalTok{.test\_set }\OperatorTok{=}\NormalTok{ torch.utils.data.random\_split(dataset, }\VariableTok{self}\NormalTok{.split\_sizes)}
\end{Highlighting}
\end{Shaded}

We define a Neural ODE and train it. We will use a simple 2-layer MLP
with a \emph{tanh} activation and 64 hidden dimensions. We will train
the model using the adjoint method for backpropagation.

A quick note on the architectural choices for our model. The
\textbf{Picard-LindelÃ¶f theorem} (Coddington and Levinson, 1955) states
that the solution to an initial value problem \textbf{exists and is
unique} if the differential equation is \emph{uniformly Lipschitz
continuous} in \(\mathbf{z}\) and \emph{continuous} in \(t\). It turns
out that this theorem holds for our model if the neural network has
finite weights and uses Lipschitz nonlinearities, such as tanh or relu.
However, not all tools are our deep learning arsenal is c.~For example,
as shown in \href{https://arxiv.org/abs/2006.04710}{\textbf{The
Lipschitz Constant of Self-Attention}} by Hyunjik Kim et al., standard
self-attention is \textbf{\emph{not}} Lipschitz. The authors propose
alternative forms of self-attention that are Lipschitz.

\phantomsection\label{neural-ode-training}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ lightning.pytorch }\ImportTok{import}\NormalTok{ Trainer}
\ImportTok{from}\NormalTok{ lightning.pytorch.callbacks }\ImportTok{import}\NormalTok{ ModelCheckpoint, RichProgressBar}


\NormalTok{adjoint }\OperatorTok{=} \VariableTok{True}
\NormalTok{data\_module }\OperatorTok{=}\NormalTok{ HalfMoonsDataModule(}\DecValTok{1000}\NormalTok{)}
\NormalTok{t\_span }\OperatorTok{=}\NormalTok{ torch.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{f }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{    nn.Linear(}\DecValTok{2}\NormalTok{, }\DecValTok{64}\NormalTok{),}
\NormalTok{    nn.Tanh(),}
\NormalTok{    nn.Linear(}\DecValTok{64}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\NormalTok{model }\OperatorTok{=}\NormalTok{ ODEBlock(f, adjoint}\OperatorTok{=}\NormalTok{adjoint)}
\NormalTok{learner }\OperatorTok{=}\NormalTok{ Learner(model, t\_span)}

\NormalTok{trainer }\OperatorTok{=}\NormalTok{ Trainer(}
\NormalTok{    max\_epochs}\OperatorTok{=}\DecValTok{200}\NormalTok{,}
\NormalTok{    accelerator}\OperatorTok{=}\StringTok{"gpu"} \ControlFlowTok{if}\NormalTok{ torch.cuda.is\_available() }\ControlFlowTok{else} \StringTok{"cpu"}\NormalTok{,}
\NormalTok{    devices}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    callbacks}\OperatorTok{=}\NormalTok{[}
\NormalTok{        ModelCheckpoint(mode}\OperatorTok{=}\StringTok{"max"}\NormalTok{, monitor}\OperatorTok{=}\StringTok{"val\_accuracy"}\NormalTok{),}
\NormalTok{        RichProgressBar(),}
\NormalTok{    ],}
\NormalTok{    log\_every\_n\_steps}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{)}
\NormalTok{trainer.fit(learner, datamodule}\OperatorTok{=}\NormalTok{data\_module)}
\NormalTok{val\_result }\OperatorTok{=}\NormalTok{ trainer.validate(learner, datamodule}\OperatorTok{=}\NormalTok{data\_module, verbose}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{test\_result }\OperatorTok{=}\NormalTok{ trainer.test(learner, datamodule}\OperatorTok{=}\NormalTok{data\_module, verbose}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{neural-ode-training-1}
\begin{verbatim}
âââââ³ââââââââ³âââââââââââ³âââââââââ³ââââââââ
â   â Name  â Type     â Params â Mode  â
â¡ââââââââââââââââââââââââââââââââââââââââ©
â 0 â model â ODEBlock â    322 â train â
âââââ´ââââââââ´âââââââââââ´âââââââââ´ââââââââ
\end{verbatim}

\phantomsection\label{neural-ode-training-2}
\begin{verbatim}
Trainable params: 322                                                                                              
Non-trainable params: 0                                                                                            
Total params: 322                                                                                                  
Total estimated model params size (MB): 0                                                                          
Modules in train mode: 6                                                                                           
Modules in eval mode: 0                                                                                            
\end{verbatim}

\phantomsection\label{neural-ode-training-3}
\begin{verbatim}
Output()
\end{verbatim}

\phantomsection\label{neural-ode-training-4}
\begin{verbatim}
/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connect
or.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value
of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.
\end{verbatim}

\phantomsection\label{neural-ode-training-5}
\begin{verbatim}
/Users/bartz/miniforge3/envs/spot312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connect
or.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the 
value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.
\end{verbatim}

\phantomsection\label{neural-ode-training-6}
\begin{verbatim}
\end{verbatim}

\phantomsection\label{neural-ode-training-7}
\begin{verbatim}
Output()
\end{verbatim}

\phantomsection\label{neural-ode-training-8}
\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â      Validate metric      â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â       val_accuracy        â            1.0            â
â         val_loss          â   0.002127678133547306    â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\phantomsection\label{neural-ode-training-9}
\begin{verbatim}
\end{verbatim}

\phantomsection\label{neural-ode-training-10}
\begin{verbatim}
Output()
\end{verbatim}

\phantomsection\label{neural-ode-training-11}
\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â        Test metric        â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â       test_accuracy       â            1.0            â
â         test_loss         â   0.0018281706143170595   â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\phantomsection\label{neural-ode-training-12}
\begin{verbatim}
\end{verbatim}

It seems that in less that 200 epochs we have achieved perfect
validation accuracy. Let's now use the trained model to run inference
and visualize the trajectories using a dense time span of 100 timesteps.

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.no\_grad}\NormalTok{()}
\KeywordTok{def}\NormalTok{ run\_inference(learner, data\_loader, time\_span):}
\NormalTok{    learner.to(device)}
\NormalTok{    trajectories }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    classes }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    time\_span }\OperatorTok{=}\NormalTok{ torch.from\_numpy(time\_span).to(device)}
    \ControlFlowTok{for}\NormalTok{ data, target }\KeywordTok{in}\NormalTok{ data\_loader:}
\NormalTok{        data }\OperatorTok{=}\NormalTok{ data.to(device)}
\NormalTok{        traj }\OperatorTok{=}\NormalTok{ learner.inference(data, time\_span).cpu().numpy()}
\NormalTok{        trajectories.append(traj)}
\NormalTok{        classes.extend(target.numpy())}
\NormalTok{    trajectories }\OperatorTok{=}\NormalTok{ np.concatenate(trajectories, }\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ trajectories, classes}

\NormalTok{time\_span }\OperatorTok{=}\NormalTok{ np.linspace(}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{trajectories, classes }\OperatorTok{=}\NormalTok{ run\_inference(learner, data\_module.train\_dataloader(), time\_span)}

\NormalTok{colors }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}coral\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}darkviolet\textquotesingle{}}\NormalTok{]}
\NormalTok{class\_colors }\OperatorTok{=}\NormalTok{ [colors[ci] }\ControlFlowTok{for}\NormalTok{ ci }\KeywordTok{in}\NormalTok{ classes]}
\end{Highlighting}
\end{Shaded}

We will now define a few functions to visualize the learned
trajectories, the state-space, and the learned vector field.

Before we visualize the trajectories, let's plot the (training) data
once again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plot\_binary\_classification\_dataset(}\OperatorTok{*}\NormalTok{data\_module.train\_set[:], title}\OperatorTok{=}\StringTok{\textquotesingle{}Half Moons Dataset\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_neural_ode_example_files/figure-pdf/plot_half-moons-output-1.pdf}}

Below we visualize the evolution for each of the 2 inputs dimensions as
a function of time (depth):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_trajectories(time\_span, trajectories, class\_colors)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_neural_ode_example_files/figure-pdf/plot-trajectories-output-1.pdf}}

And the same evolution combined in a single plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_trajectories\_3d(time\_span, trajectories, class\_colors)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_neural_ode_example_files/figure-pdf/plot-trajectories-3d-output-1.pdf}}

The 3D plot can be somewhat complicated to decipher. Thus, we also plot
an animated version of the evolution. Each timestep of the animation is
a slice on the temporal axis of the figure above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anim }\OperatorTok{=}\NormalTok{ plot\_trajectories\_animation(time\_span, trajectories, colors, classes, lim}\OperatorTok{=}\FloatTok{8.0}\NormalTok{)}
\NormalTok{HTML(anim.to\_html5\_video())}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{animate-trajectories}
\begin{verbatim}
<IPython.core.display.HTML object>
\end{verbatim}

Finally, we can visualize the state-space diagram and the learned vector
field:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\NormalTok{plot\_state\_space(trajectories, class\_colors, ax}\OperatorTok{=}\NormalTok{ax[}\DecValTok{0}\NormalTok{])}
\NormalTok{plot\_static\_vector\_field(model, trajectories, ax}\OperatorTok{=}\NormalTok{ax[}\DecValTok{1}\NormalTok{], device}\OperatorTok{=}\NormalTok{device)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{601_neural_ode_example_files/figure-pdf/visualize-state-space-vector-field-output-1.pdf}}

\end{example}

\chapter{Physics Informed Neural Networks}\label{sec-pinn-601}

\section{PINNs}\label{pinns}

\phantomsection\label{import_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{import}\NormalTok{ torch.utils.data }\ImportTok{as}\NormalTok{ thdat}
\ImportTok{import}\NormalTok{ functools}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\NormalTok{sns.set\_theme()}
\NormalTok{torch.manual\_seed(}\DecValTok{42}\NormalTok{)}

\NormalTok{DEVICE }\OperatorTok{=}\NormalTok{ torch.device(}\StringTok{"cuda"} \ControlFlowTok{if}\NormalTok{ torch.cuda.is\_available() }\ControlFlowTok{else} \StringTok{"cpu"}\NormalTok{)}

\CommentTok{\# boundaries for the frequency range}
\NormalTok{a }\OperatorTok{=} \DecValTok{0}
\NormalTok{b }\OperatorTok{=} \DecValTok{500}
\end{Highlighting}
\end{Shaded}

\section{Generation and Visualization of the Training Data and the
Ground Truth
(Function)}\label{generation-and-visualization-of-the-training-data-and-the-ground-truth-function}

\begin{itemize}
\tightlist
\item
  Definition of the (unknown) differential equation:
\end{itemize}

\phantomsection\label{ode_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ ode(frequency, loc, sigma, R):}
    \CommentTok{"""Computes the amplitude. Defining equation, used}
\CommentTok{    to generate data and train models.}
\CommentTok{    The equation itself is not known to the model.}

\CommentTok{    Args:}
\CommentTok{        frequency: (N,) array{-}like}
\CommentTok{        loc: float}
\CommentTok{        sigma: float}
\CommentTok{        R: float}
\CommentTok{    }
\CommentTok{    Returns:}
\CommentTok{        (N,) array{-}like}
\CommentTok{    }
\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} ode(0, 25, 100, 0.005)}
\CommentTok{        100.0}
\CommentTok{    """}
\NormalTok{    A }\OperatorTok{=}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{R }\OperatorTok{*}\NormalTok{ (frequency }\OperatorTok{{-}}\NormalTok{ loc)}\OperatorTok{**}\DecValTok{2}\OperatorTok{/}\NormalTok{sigma}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ A}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Setting the parameters for the ode
\end{itemize}

\phantomsection\label{parameters_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{10}\NormalTok{)}
\NormalTok{loc }\OperatorTok{=} \DecValTok{250}
\NormalTok{sigma }\OperatorTok{=} \DecValTok{100}
\NormalTok{R }\OperatorTok{=} \FloatTok{0.5}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Generating the data
\end{itemize}

\phantomsection\label{amp_data_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frequencies }\OperatorTok{=}\NormalTok{ np.linspace(a, b, }\DecValTok{1000}\NormalTok{)}
\NormalTok{eq }\OperatorTok{=}\NormalTok{ functools.partial(ode, loc}\OperatorTok{=}\NormalTok{loc, sigma}\OperatorTok{=}\NormalTok{sigma, R}\OperatorTok{=}\NormalTok{R)}
\NormalTok{amplitudes }\OperatorTok{=}\NormalTok{ eq(frequencies)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Now we have the ground truth for the full frequency range and can take
  a look at the first 10 values:
\end{itemize}

\phantomsection\label{first_10_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{: frequencies[:}\DecValTok{10}\NormalTok{], }\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{: amplitudes[:}\DecValTok{10}\NormalTok{]\})}
\BuiltInTok{print}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Frequency  Amplitude
0   0.000000   0.043937
1   0.500501   0.044490
2   1.001001   0.045048
3   1.501502   0.045612
4   2.002002   0.046183
5   2.502503   0.046759
6   3.003003   0.047341
7   3.503504   0.047929
8   4.004004   0.048524
9   4.504505   0.049124
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We generate the training data as a subset of the full frequency range
  and add some noise:
\end{itemize}

\phantomsection\label{training_data_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t }\OperatorTok{=}\NormalTok{ np.linspace(a, }\DecValTok{2}\OperatorTok{*}\NormalTok{b}\OperatorTok{/}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{A }\OperatorTok{=}\NormalTok{ eq(t) }\OperatorTok{+}  \FloatTok{0.2} \OperatorTok{*}\NormalTok{ np.random.randn(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Plot of the training data and the ground truth:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(frequencies, amplitudes)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend([}\StringTok{\textquotesingle{}Equation (ground truth)\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Training data\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\centering{

\begin{verbatim}
Text(0.5, 0, 'Frequency')
\end{verbatim}

}

\subcaption{\label{fig-plot_data_601_pinns-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-plot_data_601_pinns-output-2.pdf}}

}

\subcaption{\label{fig-plot_data_601_pinns-2}}

}

\caption{\label{fig-plot_data_601_pinns}}

\end{figure}%

\section{Gradient With Autograd}\label{gradient-with-autograd}

\phantomsection\label{autograd_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ grad(outputs, inputs):}
    \CommentTok{"""Computes the partial derivative of }
\CommentTok{    an output with respect to an input.}

\CommentTok{    Args:}
\CommentTok{        outputs: (N, 1) tensor}
\CommentTok{        inputs: (N, D) tensor}

\CommentTok{    Returns:}
\CommentTok{        (N, D) tensor}
\CommentTok{    }
\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} x = torch.tensor([1.0, 2.0, 3.0], requires\_grad=True)}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} y = x**2}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} grad(y, x)}
\CommentTok{        tensor([2., 4., 6.])}
\CommentTok{    """}
    \ControlFlowTok{return}\NormalTok{ torch.autograd.grad(}
\NormalTok{        outputs, inputs, grad\_outputs}\OperatorTok{=}\NormalTok{torch.ones\_like(outputs), create\_graph}\OperatorTok{=}\VariableTok{True}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Autograd example:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}
\NormalTok{grad(y, x)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{autograd_example_601_pinns}
\begin{verbatim}
(tensor([2., 4., 6.], grad_fn=<MulBackward0>),)
\end{verbatim}

\section{Network}\label{network}

\phantomsection\label{numpy2torch_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ numpy2torch(x):}
    \CommentTok{"""Converts a numpy array to a pytorch tensor.}

\CommentTok{    Args:}
\CommentTok{        x: (N, D) array{-}like}

\CommentTok{    Returns:}
\CommentTok{        (N, D) tensor}

\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} numpy2torch(np.array([1,2,3]))}
\CommentTok{        tensor([1., 2., 3.])}
\CommentTok{    """}
\NormalTok{    n\_samples }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(x)}
    \ControlFlowTok{return}\NormalTok{ torch.from\_numpy(x).to(torch.}\BuiltInTok{float}\NormalTok{).to(DEVICE).reshape(n\_samples, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{net_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ Net(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        input\_dim,}
\NormalTok{        output\_dim,}
\NormalTok{        n\_units}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{        epochs}\OperatorTok{=}\DecValTok{1000}\NormalTok{,}
\NormalTok{        loss}\OperatorTok{=}\NormalTok{nn.MSELoss(),}
\NormalTok{        lr}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{,}
\NormalTok{        loss2}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{        loss2\_weight}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{    ) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}

        \VariableTok{self}\NormalTok{.epochs }\OperatorTok{=}\NormalTok{ epochs}
        \VariableTok{self}\NormalTok{.loss }\OperatorTok{=}\NormalTok{ loss}
        \VariableTok{self}\NormalTok{.loss2 }\OperatorTok{=}\NormalTok{ loss2}
        \VariableTok{self}\NormalTok{.loss2\_weight }\OperatorTok{=}\NormalTok{ loss2\_weight}
        \VariableTok{self}\NormalTok{.lr }\OperatorTok{=}\NormalTok{ lr}
        \VariableTok{self}\NormalTok{.n\_units }\OperatorTok{=}\NormalTok{ n\_units}

        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.Linear(input\_dim, }\VariableTok{self}\NormalTok{.n\_units),}
\NormalTok{            nn.ReLU(),}
\NormalTok{            nn.Linear(}\VariableTok{self}\NormalTok{.n\_units, }\VariableTok{self}\NormalTok{.n\_units),}
\NormalTok{            nn.ReLU(),}
\NormalTok{            nn.Linear(}\VariableTok{self}\NormalTok{.n\_units, }\VariableTok{self}\NormalTok{.n\_units),}
\NormalTok{            nn.ReLU(),}
\NormalTok{            nn.Linear(}\VariableTok{self}\NormalTok{.n\_units, }\VariableTok{self}\NormalTok{.n\_units),}
\NormalTok{            nn.ReLU(),}
\NormalTok{        )}
        \VariableTok{self}\NormalTok{.out }\OperatorTok{=}\NormalTok{ nn.Linear(}\VariableTok{self}\NormalTok{.n\_units, output\_dim)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        h }\OperatorTok{=} \VariableTok{self}\NormalTok{.layers(x)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.out(h)}
        \ControlFlowTok{return}\NormalTok{ out}

    \KeywordTok{def}\NormalTok{ fit(}\VariableTok{self}\NormalTok{, X, y):}
\NormalTok{        Xt }\OperatorTok{=}\NormalTok{ numpy2torch(X)}
\NormalTok{        yt }\OperatorTok{=}\NormalTok{ numpy2torch(y)}

\NormalTok{        optimiser }\OperatorTok{=}\NormalTok{ optim.Adam(}\VariableTok{self}\NormalTok{.parameters(), lr}\OperatorTok{=}\VariableTok{self}\NormalTok{.lr)}
        \VariableTok{self}\NormalTok{.train()}
\NormalTok{        losses }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ ep }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\VariableTok{self}\NormalTok{.epochs):}
\NormalTok{            optimiser.zero\_grad()}
\NormalTok{            outputs }\OperatorTok{=} \VariableTok{self}\NormalTok{.forward(Xt)}
\NormalTok{            loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.loss(yt, outputs)}
            \ControlFlowTok{if} \VariableTok{self}\NormalTok{.loss2:}
\NormalTok{                loss }\OperatorTok{+=} \VariableTok{self}\NormalTok{.loss2\_weight }\OperatorTok{+} \VariableTok{self}\NormalTok{.loss2\_weight }\OperatorTok{*} \VariableTok{self}\NormalTok{.loss2(}\VariableTok{self}\NormalTok{)}
\NormalTok{            loss.backward()}
\NormalTok{            optimiser.step()}
\NormalTok{            losses.append(loss.item())}
            \ControlFlowTok{if}\NormalTok{ ep }\OperatorTok{\%} \BuiltInTok{int}\NormalTok{(}\VariableTok{self}\NormalTok{.epochs }\OperatorTok{/} \DecValTok{10}\NormalTok{) }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{ep}\SpecialCharTok{\}}\SpecialStringTok{/}\SpecialCharTok{\{}\VariableTok{self}\SpecialCharTok{.}\NormalTok{epochs}\SpecialCharTok{\}}\SpecialStringTok{, loss: }\SpecialCharTok{\{}\NormalTok{losses[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ losses}

    \KeywordTok{def}\NormalTok{ predict(}\VariableTok{self}\NormalTok{, X):}
        \VariableTok{self}\NormalTok{.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.forward(numpy2torch(X))}
        \ControlFlowTok{return}\NormalTok{ out.detach().cpu().numpy()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Extended network for parameter estimation of parameter \texttt{r}:
\end{itemize}

\phantomsection\label{pinnparam_601_pinns_ext}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ PINNParam(Net):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        input\_dim,}
\NormalTok{        output\_dim,}
\NormalTok{        n\_units}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{        epochs}\OperatorTok{=}\DecValTok{1000}\NormalTok{,}
\NormalTok{        loss}\OperatorTok{=}\NormalTok{nn.MSELoss(),}
\NormalTok{        lr}\OperatorTok{=}\FloatTok{0.001}\NormalTok{,}
\NormalTok{        loss2}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{        loss2\_weight}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{    ) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{(}
\NormalTok{            input\_dim, output\_dim, n\_units, epochs, loss, lr, loss2, loss2\_weight}
\NormalTok{        )}

        \VariableTok{self}\NormalTok{.r }\OperatorTok{=}\NormalTok{ nn.Parameter(data}\OperatorTok{=}\NormalTok{torch.tensor([}\FloatTok{1.}\NormalTok{]))}
        \VariableTok{self}\NormalTok{.sigma }\OperatorTok{=}\NormalTok{ nn.Parameter(data}\OperatorTok{=}\NormalTok{torch.tensor([}\FloatTok{100.}\NormalTok{]))}
        \VariableTok{self}\NormalTok{.loc }\OperatorTok{=}\NormalTok{ nn.Parameter(data}\OperatorTok{=}\NormalTok{torch.tensor([}\FloatTok{100.}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\section{Basic Neutral Network}\label{basic-neutral-network}

\begin{itemize}
\tightlist
\item
  Network without regularization:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{net }\OperatorTok{=}\NormalTok{ Net(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, loss2}\OperatorTok{=}\VariableTok{None}\NormalTok{, epochs}\OperatorTok{=}\DecValTok{2000}\NormalTok{, lr}\OperatorTok{=}\FloatTok{1e{-}5}\NormalTok{).to(DEVICE)}

\NormalTok{losses }\OperatorTok{=}\NormalTok{ net.fit(t, A)}

\NormalTok{plt.plot(losses)}
\NormalTok{plt.yscale(}\StringTok{\textquotesingle{}log\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Epoch 0/2000, loss: 6.59
Epoch 200/2000, loss: 0.06
Epoch 400/2000, loss: 0.05
Epoch 600/2000, loss: 0.05
Epoch 800/2000, loss: 0.05
Epoch 1000/2000, loss: 0.05
Epoch 1200/2000, loss: 0.05
Epoch 1400/2000, loss: 0.05
Epoch 1600/2000, loss: 0.05
Epoch 1800/2000, loss: 0.05
\end{verbatim}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-net_601_pinns_wo_reg-output-2.pdf}}

}

\caption{\label{fig-net_601_pinns_wo_reg}}

\end{figure}%

\begin{itemize}
\tightlist
\item
  Adding L2 regularization:
\end{itemize}

\phantomsection\label{l2_reg_601_pinns_reg}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ l2\_reg(model: torch.nn.Module):}
    \CommentTok{"""L2 regularization for the model parameters.}

\CommentTok{    Args:}
\CommentTok{        model: torch.nn.Module}

\CommentTok{    Returns:}
\CommentTok{        torch.Tensor}

\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} l2\_reg(Net(1,1))}
\CommentTok{        tensor(0.0001, grad\_fn=\textless{}SumBackward0\textgreater{})}
\CommentTok{    """}
    \ControlFlowTok{return}\NormalTok{ torch.}\BuiltInTok{sum}\NormalTok{(}\BuiltInTok{sum}\NormalTok{([p.}\BuiltInTok{pow}\NormalTok{(}\FloatTok{2.}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ model.parameters()]))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{netreg }\OperatorTok{=}\NormalTok{ Net(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, loss2}\OperatorTok{=}\NormalTok{l2\_reg, epochs}\OperatorTok{=}\DecValTok{20000}\NormalTok{, lr}\OperatorTok{=}\FloatTok{1e{-}5}\NormalTok{, loss2\_weight}\OperatorTok{=}\FloatTok{.1}\NormalTok{).to(DEVICE)}
\NormalTok{losses }\OperatorTok{=}\NormalTok{ netreg.fit(t, A)}
\NormalTok{plt.plot(losses)}
\NormalTok{plt.yscale(}\StringTok{\textquotesingle{}log\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Epoch 0/20000, loss: 662.07
Epoch 2000/20000, loss: 612.81
Epoch 4000/20000, loss: 571.31
Epoch 6000/20000, loss: 533.74
Epoch 8000/20000, loss: 499.32
Epoch 10000/20000, loss: 467.44
Epoch 12000/20000, loss: 437.53
Epoch 14000/20000, loss: 409.15
Epoch 16000/20000, loss: 382.10
Epoch 18000/20000, loss: 356.29
\end{verbatim}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-net_601_pinns_reg-output-2.pdf}}

}

\caption{\label{fig-net_601_pinns_reg}}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predsreg }\OperatorTok{=}\NormalTok{ netreg.predict(frequencies)}
\NormalTok{preds }\OperatorTok{=}\NormalTok{ net.predict(frequencies)}
\NormalTok{plt.plot(frequencies, amplitudes, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, predsreg, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}

\NormalTok{plt.legend(labels}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}Equation\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Training data\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Network\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}L2 Regularization Network\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\centering{

\begin{verbatim}
Text(0.5, 0, 'Frequency')
\end{verbatim}

}

\subcaption{\label{fig-plot_results_601_pinns-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-plot_results_601_pinns-output-2.pdf}}

}

\subcaption{\label{fig-plot_results_601_pinns-2}}

}

\caption{\label{fig-plot_results_601_pinns}}

\end{figure}%

\section{PINNs}\label{pinns-1}

\begin{itemize}
\tightlist
\item
  Calculate the physics-informed loss (similar to the L2
  regularization):
\end{itemize}

\phantomsection\label{physics_loss_601_pinns}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ physics\_loss(model: torch.nn.Module):}
    \CommentTok{"""Computes the physics{-}informed loss for the model.}

\CommentTok{    Args:}
\CommentTok{        model: torch.nn.Module}

\CommentTok{    Returns:}
\CommentTok{        torch.Tensor}

\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} physics\_loss(Net(1,1))}
\CommentTok{        tensor(0.0001, grad\_fn=\textless{}MeanBackward0\textgreater{})}
\CommentTok{    """}
\NormalTok{    ts }\OperatorTok{=}\NormalTok{ torch.linspace(a, b, steps}\OperatorTok{=}\DecValTok{1000}\NormalTok{).view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{).requires\_grad\_(}\VariableTok{True}\NormalTok{).to(DEVICE)}
\NormalTok{    amplitudes }\OperatorTok{=}\NormalTok{ model(ts)}
\NormalTok{    dT }\OperatorTok{=}\NormalTok{ grad(amplitudes, ts)[}\DecValTok{0}\NormalTok{]}
\NormalTok{    ode }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{2}\OperatorTok{*}\NormalTok{R}\OperatorTok{*}\NormalTok{(ts}\OperatorTok{{-}}\NormalTok{loc)}\OperatorTok{/}\NormalTok{ sigma}\OperatorTok{**}\DecValTok{2} \OperatorTok{*}\NormalTok{ amplitudes }\OperatorTok{{-}}\NormalTok{ dT}
    \ControlFlowTok{return}\NormalTok{ torch.mean(ode}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Train the network with the physics-informed loss and plot the training
  error:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{net\_pinn }\OperatorTok{=}\NormalTok{ Net(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, loss2}\OperatorTok{=}\NormalTok{physics\_loss, epochs}\OperatorTok{=}\DecValTok{2000}\NormalTok{, loss2\_weight}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{1e{-}5}\NormalTok{).to(DEVICE)}
\NormalTok{losses }\OperatorTok{=}\NormalTok{ net\_pinn.fit(t, A)}
\NormalTok{plt.plot(losses)}
\NormalTok{plt.yscale(}\StringTok{\textquotesingle{}log\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Epoch 0/2000, loss: 12.23
Epoch 200/2000, loss: 1.05
Epoch 400/2000, loss: 1.05
Epoch 600/2000, loss: 1.05
Epoch 800/2000, loss: 1.05
Epoch 1000/2000, loss: 1.05
Epoch 1200/2000, loss: 1.05
Epoch 1400/2000, loss: 1.05
Epoch 1600/2000, loss: 1.05
Epoch 1800/2000, loss: 1.05
\end{verbatim}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-net_601_pinns_pinn_plot_loss-output-2.pdf}}

}

\caption{\label{fig-net_601_pinns_pinn_plot_loss}}

\end{figure}%

\begin{itemize}
\tightlist
\item
  Predict the amplitude and plot the results:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preds\_pinn }\OperatorTok{=}\NormalTok{ net\_pinn.predict(frequencies)}
\NormalTok{plt.plot(frequencies, amplitudes, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, predsreg, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds\_pinn, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.legend(labels}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}Equation\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Training data\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}NN\textquotesingle{}}\NormalTok{, }\StringTok{"R2"}\NormalTok{, }\StringTok{\textquotesingle{}PINN\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\centering{

\begin{verbatim}
Text(0.5, 0, 'Frequency')
\end{verbatim}

}

\subcaption{\label{fig-net_601_pinns_pinn_plot_results-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-net_601_pinns_pinn_plot_results-output-2.pdf}}

}

\subcaption{\label{fig-net_601_pinns_pinn_plot_results-2}}

}

\caption{\label{fig-net_601_pinns_pinn_plot_results}}

\end{figure}%

\subsection{PINNs: Parameter
Estimation}\label{pinns-parameter-estimation}

\phantomsection\label{pinn_param_601_pinns_param_est}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ physics\_loss\_estimation(model: torch.nn.Module):}
\NormalTok{    ts }\OperatorTok{=}\NormalTok{ torch.linspace(a, b, steps}\OperatorTok{=}\DecValTok{1000}\NormalTok{,).view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{).requires\_grad\_(}\VariableTok{True}\NormalTok{).to(DEVICE)}
\NormalTok{    amplitudes }\OperatorTok{=}\NormalTok{ model(ts)}
\NormalTok{    dT }\OperatorTok{=}\NormalTok{ grad(amplitudes, ts)[}\DecValTok{0}\NormalTok{]}
\NormalTok{    ode }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{2}\OperatorTok{*}\NormalTok{model.r}\OperatorTok{*}\NormalTok{(ts}\OperatorTok{{-}}\NormalTok{model.loc)}\OperatorTok{/}\NormalTok{ (model.sigma)}\OperatorTok{**}\DecValTok{2} \OperatorTok{*}\NormalTok{ amplitudes }\OperatorTok{{-}}\NormalTok{ dT}
    \ControlFlowTok{return}\NormalTok{ torch.mean(ode}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pinn\_param }\OperatorTok{=}\NormalTok{ PINNParam(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, loss2}\OperatorTok{=}\NormalTok{physics\_loss\_estimation, loss2\_weight}\OperatorTok{=}\DecValTok{1}\NormalTok{, epochs}\OperatorTok{=}\DecValTok{4000}\NormalTok{, lr}\OperatorTok{=} \FloatTok{5e{-}6}\NormalTok{).to(DEVICE)}
\NormalTok{losses }\OperatorTok{=}\NormalTok{ pinn\_param.fit(t, A)}
\NormalTok{plt.plot(losses)}
\NormalTok{plt.yscale(}\StringTok{\textquotesingle{}log\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Epoch 0/4000, loss: 15.06
Epoch 400/4000, loss: 1.06
Epoch 800/4000, loss: 1.06
Epoch 1200/4000, loss: 1.06
Epoch 1600/4000, loss: 1.05
Epoch 2000/4000, loss: 1.05
Epoch 2400/4000, loss: 1.05
Epoch 2800/4000, loss: 1.05
Epoch 3200/4000, loss: 1.05
Epoch 3600/4000, loss: 1.05
\end{verbatim}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-pinn_param_601_pinns_param_est-output-2.pdf}}

}

\caption{\label{fig-pinn_param_601_pinns_param_est}}

\end{figure}%

\phantomsection\label{print-pinn_param_601_pinns_param_est_plot_results}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preds\_disc }\OperatorTok{=}\NormalTok{ pinn\_param.predict(frequencies)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Estimated r: }\SpecialCharTok{\{}\NormalTok{pinn\_param}\SpecialCharTok{.}\NormalTok{r}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Estimated sigma: }\SpecialCharTok{\{}\NormalTok{pinn\_param}\SpecialCharTok{.}\NormalTok{sigma}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Estimated loc: }\SpecialCharTok{\{}\NormalTok{pinn\_param}\SpecialCharTok{.}\NormalTok{loc}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Estimated r: Parameter containing:
tensor([0.9893], requires_grad=True)
Estimated sigma: Parameter containing:
tensor([100.0067], requires_grad=True)
Estimated loc: Parameter containing:
tensor([100.0065], requires_grad=True)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(frequencies, amplitudes, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds\_disc, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.legend(labels}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}Equation\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Training data\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}estimation PINN\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\centering{

\begin{verbatim}
Text(0.5, 0, 'Frequency')
\end{verbatim}

}

\subcaption{\label{fig-pinn_param_601_pinns_param_est_plot_results-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-pinn_param_601_pinns_param_est_plot_results-output-2.pdf}}

}

\subcaption{\label{fig-pinn_param_601_pinns_param_est_plot_results-2}}

}

\caption{\label{fig-pinn_param_601_pinns_param_est_plot_results}}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(frequencies, amplitudes, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, predsreg, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds\_pinn, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds\_disc, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.legend(labels}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}Equation\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Training data\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}NN\textquotesingle{}}\NormalTok{, }\StringTok{"R2"}\NormalTok{, }\StringTok{\textquotesingle{}PINN\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}paramPINN\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\centering{

\begin{verbatim}
Text(0.5, 0, 'Frequency')
\end{verbatim}

}

\subcaption{\label{fig-pinn_param_601_pinns_param_est_amplitude-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-pinn_param_601_pinns_param_est_amplitude-output-2.pdf}}

}

\subcaption{\label{fig-pinn_param_601_pinns_param_est_amplitude-2}}

}

\caption{\label{fig-pinn_param_601_pinns_param_est_amplitude}}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(frequencies, amplitudes, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, predsreg, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds\_disc, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.legend(labels}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}Equation\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Training data\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}NN\textquotesingle{}}\NormalTok{, }\StringTok{"R2"}\NormalTok{, }\StringTok{\textquotesingle{}paramPINN\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\centering{

\begin{verbatim}
Text(0.5, 0, 'Frequency')
\end{verbatim}

}

\subcaption{\label{fig-pinn_param_601_pinns_param_est_plot_results_all-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-pinn_param_601_pinns_param_est_plot_results_all-output-2.pdf}}

}

\subcaption{\label{fig-pinn_param_601_pinns_param_est_plot_results_all-2}}

}

\caption{\label{fig-pinn_param_601_pinns_param_est_plot_results_all}}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(frequencies, amplitudes, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, predsreg, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.plot(frequencies, preds\_disc, alpha}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{plt.legend(labels}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}Grundwahrheit\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Trainingsdaten\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}NN\textquotesingle{}}\NormalTok{, }\StringTok{"NN+R2"}\NormalTok{, }\StringTok{\textquotesingle{}PINN\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequenz\textquotesingle{}}\NormalTok{)}
\CommentTok{\# save the plot as a pdf}
\NormalTok{plt.savefig(}\StringTok{\textquotesingle{}pinns.pdf\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.savefig(}\StringTok{\textquotesingle{}pinns.png\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{601_pinn_files/figure-pdf/fig-pinn_param_601_pinns_param_ground-output-1.pdf}}

}

\caption{\label{fig-pinn_param_601_pinns_param_ground}}

\end{figure}%

\section{Summary}\label{summary-2}

\begin{itemize}
\tightlist
\item
  Results strongly depend on the parametrization(s)
\item
  PINN parameter estimation not robust
\item
  Hyperparameter tuning is crucial
\item
  Use SPOT before further analysis is done
\end{itemize}

\chapter{Hyperparameter Tuning with PyTorch Lightning: Physics Informed
Neural Networks}\label{sec-light-pinn-601}

\section{PINNs}\label{pinns-2}

In this section, we will show how to set up PINN hyperparameter tuner
from scratch based on the \texttt{spotpython} programs from
Chapter~\ref{sec-light-user-model-601}.

\subsection{The Ground Truth Model}\label{the-ground-truth-model}

Definition of the (unknown) differential equation:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{import}\NormalTok{ torch.utils.data }\ImportTok{as}\NormalTok{ thdat}
\ImportTok{import}\NormalTok{ functools}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\CommentTok{\# boundaries for the frequency range}
\NormalTok{a }\OperatorTok{=} \DecValTok{0}
\NormalTok{b }\OperatorTok{=} \DecValTok{500}

\KeywordTok{def}\NormalTok{ ode(frequency, loc, sigma, R):}
    \CommentTok{"""Computes the amplitude. Defining equation, used}
\CommentTok{    to generate data and train models.}
\CommentTok{    The equation itself is not known to the model.}

\CommentTok{    Args:}
\CommentTok{        frequency: (N,) array{-}like}
\CommentTok{        loc: float}
\CommentTok{        sigma: float}
\CommentTok{        R: float}
\CommentTok{    }
\CommentTok{    Returns:}
\CommentTok{        (N,) array{-}like}
\CommentTok{    }
\CommentTok{    Examples:}
\CommentTok{        \textgreater{}\textgreater{}\textgreater{} ode(0, 25, 100, 0.005)}
\CommentTok{        100.0}
\CommentTok{    """}
\NormalTok{    A }\OperatorTok{=}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{R }\OperatorTok{*}\NormalTok{ (frequency }\OperatorTok{{-}}\NormalTok{ loc)}\OperatorTok{**}\DecValTok{2}\OperatorTok{/}\NormalTok{sigma}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ A}
\end{Highlighting}
\end{Shaded}

Setting the parameters for the ode

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{10}\NormalTok{)}
\NormalTok{loc }\OperatorTok{=} \DecValTok{250}
\NormalTok{sigma }\OperatorTok{=} \DecValTok{100}
\NormalTok{R }\OperatorTok{=} \FloatTok{0.5}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Generating the data
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frequencies }\OperatorTok{=}\NormalTok{ np.linspace(a, b, }\DecValTok{1000}\NormalTok{)}
\NormalTok{eq }\OperatorTok{=}\NormalTok{ functools.partial(ode, loc}\OperatorTok{=}\NormalTok{loc, sigma}\OperatorTok{=}\NormalTok{sigma, R}\OperatorTok{=}\NormalTok{R)}
\NormalTok{amplitudes }\OperatorTok{=}\NormalTok{ eq(frequencies)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Now we have the ground truth for the full frequency range and can take
  a look at the first 10 values:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{: frequencies[:}\DecValTok{10}\NormalTok{], }\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{: amplitudes[:}\DecValTok{10}\NormalTok{]\})}
\BuiltInTok{print}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Frequency  Amplitude
0   0.000000   0.043937
1   0.500501   0.044490
2   1.001001   0.045048
3   1.501502   0.045612
4   2.002002   0.046183
5   2.502503   0.046759
6   3.003003   0.047341
7   3.503504   0.047929
8   4.004004   0.048524
9   4.504505   0.049124
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We generate the training data as a subset of the full frequency range
  and add some noise:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make training data}
\NormalTok{t }\OperatorTok{=}\NormalTok{ np.linspace(a, }\DecValTok{2}\OperatorTok{*}\NormalTok{b}\OperatorTok{/}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{A }\OperatorTok{=}\NormalTok{ eq(t) }\OperatorTok{+}  \FloatTok{0.2} \OperatorTok{*}\NormalTok{ np.random.randn(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Plot of the training data and the ground truth:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(frequencies, amplitudes)}
\NormalTok{plt.plot(t, A, }\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend([}\StringTok{\textquotesingle{}Equation (ground truth)\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Training data\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Amplitude\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Frequency\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 0, 'Frequency')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{601_spot_hpt_light_pinn_files/figure-pdf/cell-8-output-2.pdf}}

\subsection{Required Files}\label{required-files}

We use the files from the \texttt{/userModel} directory as templates.
They are renamed as follows:

\begin{itemize}
\tightlist
\item
  \texttt{my\_regressor.py} \(\Rightarrow\) \texttt{pinn\_regressor.py},
  see Section~\ref{sec-pinn-regressor}
\item
  \texttt{my\_hyperdict.json} \(\Rightarrow\)
  \texttt{pinn\_hyperdict.py}, see
  Section~\ref{sec-pinn-hyper-dict-json}
\item
  \texttt{my\_hyperdict.py} \(\Rightarrow\) \texttt{pinn\_hyperdict.py},
  see Section~\ref{sec-pinn-hyperdict}.
\end{itemize}

\subsection{\texorpdfstring{The New \texttt{pinn\_hyperdict.py}
File}{The New pinn\_hyperdict.py File}}\label{sec-pinn-hyperdict}

Modifying the \texttt{pin\_hyperdict.py} file is very easy. We simply
have to change the classname \texttt{MyHyperDict} to
\texttt{PINNHyperDict} and the \texttt{filename} from
\texttt{"my\_hyper\_dict.json"} to \texttt{"pinn\_hyper\_dict.json"}.
The file is shown below.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ json}
\ImportTok{from}\NormalTok{ spotpython.data }\ImportTok{import}\NormalTok{ base}
\ImportTok{import}\NormalTok{ pathlib}

\KeywordTok{class}\NormalTok{ PINNHyperDict(base.FileConfig):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        filename: }\BuiltInTok{str} \OperatorTok{=} \StringTok{"pinn\_hyper\_dict.json"}\NormalTok{,}
\NormalTok{        directory: }\VariableTok{None} \OperatorTok{=} \VariableTok{None}\NormalTok{,}
\NormalTok{    ) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{(filename}\OperatorTok{=}\NormalTok{filename, directory}\OperatorTok{=}\NormalTok{directory)}
        \VariableTok{self}\NormalTok{.filename }\OperatorTok{=}\NormalTok{ filename}
        \VariableTok{self}\NormalTok{.directory }\OperatorTok{=}\NormalTok{ directory}
        \VariableTok{self}\NormalTok{.hyper\_dict }\OperatorTok{=} \VariableTok{self}\NormalTok{.load()}

    \AttributeTok{@property}
    \KeywordTok{def}\NormalTok{ path(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.directory:}
            \ControlFlowTok{return}\NormalTok{ pathlib.Path(}\VariableTok{self}\NormalTok{.directory).joinpath(}\VariableTok{self}\NormalTok{.filename)}
        \ControlFlowTok{return}\NormalTok{ pathlib.Path(}\VariableTok{\_\_file\_\_}\NormalTok{).parent.joinpath(}\VariableTok{self}\NormalTok{.filename)}

    \KeywordTok{def}\NormalTok{ load(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{dict}\NormalTok{:}
        \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\VariableTok{self}\NormalTok{.path, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{            d }\OperatorTok{=}\NormalTok{ json.load(f)}
        \ControlFlowTok{return}\NormalTok{ d}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{The New \texttt{pinn\_regressor.py}
File}{The New pinn\_regressor.py File}}\label{sec-pinn-regressor}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}]

The document is not complete. The code below is a template and needs to
be modified to work with the PINN model.

\end{tcolorbox}

\phantomsection\label{pinn_regressor}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch }\ImportTok{import}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.optimizer }\ImportTok{import}\NormalTok{ optimizer\_handler}
\ImportTok{import}\NormalTok{ torchmetrics.functional.regression}

\KeywordTok{class}\NormalTok{ PINNRegressor(L.LightningModule):}
    \CommentTok{"""}
\CommentTok{    A LightningModule class for a regression neural network model.}

\CommentTok{    Attributes:}
\CommentTok{        l1 (int):}
\CommentTok{            The number of neurons in the first hidden layer.}
\CommentTok{        epochs (int):}
\CommentTok{            The number of epochs to train the model for.}
\CommentTok{        batch\_size (int):}
\CommentTok{            The batch size to use during training.}
\CommentTok{        initialization (str):}
\CommentTok{            The initialization method to use for the weights.}
\CommentTok{        act\_fn (nn.Module):}
\CommentTok{            The activation function to use in the hidden layers.}
\CommentTok{        optimizer (str):}
\CommentTok{            The optimizer to use during training.}
\CommentTok{        dropout\_prob (float):}
\CommentTok{            The probability of dropping out a neuron during training.}
\CommentTok{        lr\_mult (float):}
\CommentTok{            The learning rate multiplier for the optimizer.}
\CommentTok{        patience (int):}
\CommentTok{            The number of epochs to wait before early stopping.}
\CommentTok{        \_L\_in (int):}
\CommentTok{            The number of input features.}
\CommentTok{        \_L\_out (int):}
\CommentTok{            The number of output classes.}
\CommentTok{        \_torchmetric (str):}
\CommentTok{            The metric to use for the loss function. If \textasciigrave{}None\textasciigrave{},}
\CommentTok{            then "mean\_squared\_error" is used.}
\CommentTok{        layers (nn.Sequential):}
\CommentTok{            The neural network model.}

\CommentTok{    """}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        l1: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        epochs: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        batch\_size: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        initialization: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        act\_fn: nn.Module,}
\NormalTok{        optimizer: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        dropout\_prob: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        lr\_mult: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        patience: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_in: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_out: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_torchmetric: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{    ):}
        \CommentTok{"""}
\CommentTok{        Initializes the MyRegressor object.}

\CommentTok{        Args:}
\CommentTok{            l1 (int):}
\CommentTok{                The number of neurons in the first hidden layer.}
\CommentTok{            epochs (int):}
\CommentTok{                The number of epochs to train the model for.}
\CommentTok{            batch\_size (int):}
\CommentTok{                The batch size to use during training.}
\CommentTok{            initialization (str):}
\CommentTok{                The initialization method to use for the weights.}
\CommentTok{            act\_fn (nn.Module):}
\CommentTok{                The activation function to use in the hidden layers.}
\CommentTok{            optimizer (str):}
\CommentTok{                The optimizer to use during training.}
\CommentTok{            dropout\_prob (float):}
\CommentTok{                The probability of dropping out a neuron during training.}
\CommentTok{            lr\_mult (float):}
\CommentTok{                The learning rate multiplier for the optimizer.}
\CommentTok{            patience (int):}
\CommentTok{                The number of epochs to wait before early stopping.}
\CommentTok{            \_L\_in (int):}
\CommentTok{                The number of input features. Not a hyperparameter, but needed to create the network.}
\CommentTok{            \_L\_out (int):}
\CommentTok{                The number of output classes. Not a hyperparameter, but needed to create the network.}
\CommentTok{            \_torchmetric (str):}
\CommentTok{                The metric to use for the loss function. If \textasciigrave{}None\textasciigrave{},}
\CommentTok{                then "mean\_squared\_error" is used.}

\CommentTok{        Returns:}
\CommentTok{            (NoneType): None}

\CommentTok{        Raises:}
\CommentTok{            ValueError: If l1 is less than 4.}

\CommentTok{        """}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \CommentTok{\# Attribute \textquotesingle{}act\_fn\textquotesingle{} is an instance of \textasciigrave{}nn.Module\textasciigrave{} and is already saved during}
        \CommentTok{\# checkpointing. It is recommended to ignore them}
        \CommentTok{\# using \textasciigrave{}self.save\_hyperparameters(ignore=[\textquotesingle{}act\_fn\textquotesingle{}])\textasciigrave{}}
        \CommentTok{\# self.save\_hyperparameters(ignore=["act\_fn"])}
        \CommentTok{\#}
        \VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{=}\NormalTok{ \_L\_in}
        \VariableTok{self}\NormalTok{.\_L\_out }\OperatorTok{=}\NormalTok{ \_L\_out}
        \ControlFlowTok{if}\NormalTok{ \_torchmetric }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            \_torchmetric }\OperatorTok{=} \StringTok{"mean\_squared\_error"}
        \VariableTok{self}\NormalTok{.\_torchmetric }\OperatorTok{=}\NormalTok{ \_torchmetric}
        \VariableTok{self}\NormalTok{.metric }\OperatorTok{=} \BuiltInTok{getattr}\NormalTok{(torchmetrics.functional.regression, \_torchmetric)}
        \CommentTok{\# \_L\_in and \_L\_out are not hyperparameters, but are needed to create the network}
        \CommentTok{\# \_torchmetric is not a hyperparameter, but is needed to calculate the loss}
        \VariableTok{self}\NormalTok{.save\_hyperparameters(ignore}\OperatorTok{=}\NormalTok{[}\StringTok{"\_L\_in"}\NormalTok{, }\StringTok{"\_L\_out"}\NormalTok{, }\StringTok{"\_torchmetric"}\NormalTok{])}
        \CommentTok{\# set dummy input array for Tensorboard Graphs}
        \CommentTok{\# set log\_graph=True in Trainer to see the graph (in traintest.py)}
        \VariableTok{self}\NormalTok{.example\_input\_array }\OperatorTok{=}\NormalTok{ torch.zeros((batch\_size, }\VariableTok{self}\NormalTok{.\_L\_in))}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.hparams.l1 }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"l1 must be at least 4"}\NormalTok{)}
\NormalTok{        hidden\_sizes }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_get\_hidden\_sizes()}
        \CommentTok{\# Create the network based on the specified hidden sizes}
\NormalTok{        layers }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        layer\_sizes }\OperatorTok{=}\NormalTok{ [}\VariableTok{self}\NormalTok{.\_L\_in] }\OperatorTok{+}\NormalTok{ hidden\_sizes}
\NormalTok{        layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_sizes[}\DecValTok{0}\NormalTok{]}
        \ControlFlowTok{for}\NormalTok{ layer\_size }\KeywordTok{in}\NormalTok{ layer\_sizes[}\DecValTok{1}\NormalTok{:]:}
\NormalTok{            layers }\OperatorTok{+=}\NormalTok{ [}
\NormalTok{                nn.Linear(layer\_size\_last, layer\_size),}
                \VariableTok{self}\NormalTok{.hparams.act\_fn,}
\NormalTok{                nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.dropout\_prob),}
\NormalTok{            ]}
\NormalTok{            layer\_size\_last }\OperatorTok{=}\NormalTok{ layer\_size}
\NormalTok{        layers }\OperatorTok{+=}\NormalTok{ [nn.Linear(layer\_sizes[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.\_L\_out)]}
        \CommentTok{\# nn.Sequential summarizes a list of modules into a single module, applying them in sequence}
        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{layers)}

    \KeywordTok{def}\NormalTok{ \_generate\_div2\_list(}\VariableTok{self}\NormalTok{, n, n\_min) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{list}\NormalTok{:}
        \CommentTok{"""}
\CommentTok{        Generate a list of numbers from n to n\_min (inclusive) by dividing n by 2}
\CommentTok{        until the result is less than n\_min.}
\CommentTok{        This function starts with n and keeps dividing it by 2 until n\_min is reached.}
\CommentTok{        The number of times each value is added to the list is determined by n // current.}
\CommentTok{        No more than 4 repeats of the same value (\textasciigrave{}max\_repeats\textasciigrave{} below) are added to the list.}

\CommentTok{        Args:}
\CommentTok{            n (int): The number to start with.}
\CommentTok{            n\_min (int): The minimum number to stop at.}

\CommentTok{        Returns:}
\CommentTok{            list: A list of numbers from n to n\_min (inclusive).}

\CommentTok{        Examples:}
\CommentTok{            \_generate\_div2\_list(10, 1)}
\CommentTok{            [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
\CommentTok{            \_ generate\_div2\_list(10, 2)}
\CommentTok{            [10, 5, 5, 2, 2, 2, 2, 2]}
\CommentTok{        """}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        current }\OperatorTok{=}\NormalTok{ n}
\NormalTok{        repeats }\OperatorTok{=} \DecValTok{1}
\NormalTok{        max\_repeats }\OperatorTok{=} \DecValTok{4}
        \ControlFlowTok{while}\NormalTok{ current }\OperatorTok{\textgreater{}=}\NormalTok{ n\_min:}
\NormalTok{            result.extend([current] }\OperatorTok{*} \BuiltInTok{min}\NormalTok{(repeats, max\_repeats))}
\NormalTok{            current }\OperatorTok{=}\NormalTok{ current }\OperatorTok{//} \DecValTok{2}
\NormalTok{            repeats }\OperatorTok{=}\NormalTok{ repeats }\OperatorTok{+} \DecValTok{1}
        \ControlFlowTok{return}\NormalTok{ result}

    \KeywordTok{def}\NormalTok{ \_get\_hidden\_sizes(}\VariableTok{self}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Generate the hidden layer sizes for the network.}

\CommentTok{        Returns:}
\CommentTok{            list: A list of hidden layer sizes.}

\CommentTok{        """}
\NormalTok{        n\_low }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{//} \DecValTok{4}
\NormalTok{        n\_high }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\VariableTok{self}\NormalTok{.hparams.l1, }\DecValTok{2} \OperatorTok{*}\NormalTok{ n\_low)}
\NormalTok{        hidden\_sizes }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_generate\_div2\_list(n\_high, n\_low)}
        \ControlFlowTok{return}\NormalTok{ hidden\_sizes}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a forward pass through the model.}

\CommentTok{        Args:}
\CommentTok{            x (torch.Tensor): A tensor containing a batch of input data.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the output of the model.}

\CommentTok{        """}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.layers(x)}
        \ControlFlowTok{return}\NormalTok{ x}

    \KeywordTok{def}\NormalTok{ \_calculate\_loss(}\VariableTok{self}\NormalTok{, batch):}
        \CommentTok{"""}
\CommentTok{        Calculate the loss for the given batch.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        y\_hat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.metric(y\_hat, y)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single training step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \CommentTok{\# self.log("train\_loss", val\_loss, on\_step=True, on\_epoch=True, prog\_bar=True)}
        \CommentTok{\# self.log("train\_mae\_loss", mae\_loss, on\_step=True, on\_epoch=True, prog\_bar=True)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single validation step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}

\CommentTok{        """}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \CommentTok{\# self.log("val\_loss", val\_loss, on\_step=False, on\_epoch=True, prog\_bar=prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single test step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            torch.Tensor: A tensor containing the loss for this batch.}
\CommentTok{        """}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ predict\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
        \CommentTok{"""}
\CommentTok{        Performs a single prediction step.}

\CommentTok{        Args:}
\CommentTok{            batch (tuple): A tuple containing a batch of input data and labels.}
\CommentTok{            batch\_idx (int): The index of the current batch.}
\CommentTok{            prog\_bar (bool, optional): Whether to display the progress bar. Defaults to False.}

\CommentTok{        Returns:}
\CommentTok{            A tuple containing the input data, the true labels, and the predicted values.}
\CommentTok{        """}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        yhat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        yhat }\OperatorTok{=}\NormalTok{ yhat.view(}\BuiltInTok{len}\NormalTok{(yhat), }\DecValTok{1}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step x: }\SpecialCharTok{\{}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step y: }\SpecialCharTok{\{}\NormalTok{y}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predict step y\_hat: }\SpecialCharTok{\{}\NormalTok{yhat}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        \CommentTok{\# pred\_loss = F.mse\_loss(y\_hat, y)}
        \CommentTok{\# pred loss not registered}
        \CommentTok{\# self.log("pred\_loss", pred\_loss, prog\_bar=prog\_bar)}
        \CommentTok{\# self.log("hp\_metric", pred\_loss, prog\_bar=prog\_bar)}
        \CommentTok{\# MisconfigurationException: You are trying to \textasciigrave{}self.log()\textasciigrave{}}
        \CommentTok{\# but the loop\textquotesingle{}s result collection is not registered yet.}
        \CommentTok{\# This is most likely because you are trying to log in a \textasciigrave{}predict\textasciigrave{} hook, but it doesn\textquotesingle{}t support logging.}
        \CommentTok{\# If you want to manually log, please consider using \textasciigrave{}self.log\_dict(\{\textquotesingle{}pred\_loss\textquotesingle{}: pred\_loss\})\textasciigrave{} instead.}
        \ControlFlowTok{return}\NormalTok{ (x, y, yhat)}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.optim.Optimizer:}
        \CommentTok{"""}
\CommentTok{        Configures the optimizer for the model.}

\CommentTok{        Notes:}
\CommentTok{            The default Lightning way is to define an optimizer as}
\CommentTok{            \textasciigrave{}optimizer = torch.optim.Adam(self.parameters(), lr=self.learning\_rate)\textasciigrave{}.}
\CommentTok{            spotpython uses an optimizer handler to create the optimizer, which}
\CommentTok{            adapts the learning rate according to the lr\_mult hyperparameter as}
\CommentTok{            well as other hyperparameters. See \textasciigrave{}spotpython.hyperparameters.optimizer.py\textasciigrave{} for details.}

\CommentTok{        Returns:}
\CommentTok{            torch.optim.Optimizer: The optimizer to use during training.}

\CommentTok{        """}
        \CommentTok{\# optimizer = torch.optim.Adam(self.parameters(), lr=self.learning\_rate)}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ optimizer\_handler(}
\NormalTok{            optimizer\_name}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.optimizer, params}\OperatorTok{=}\VariableTok{self}\NormalTok{.parameters(), lr\_mult}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.lr\_mult}
\NormalTok{        )}
        \ControlFlowTok{return}\NormalTok{ optimizer}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{The New \texttt{pinn\_hyperdict.json}
File}{The New pinn\_hyperdict.json File}}\label{sec-pinn-hyper-dict-json}

\chapter{Explainable AI with SpotPython and
Pytorch}\label{explainable-ai-with-spotpython-and-pytorch}

\phantomsection\label{configure_spot}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"602\_12\_1"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}


\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{10}\NormalTok{,}\DecValTok{12}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{9}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{7}\NormalTok{)}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNLinearRegressor
Experiment saved to 602_12_1_exp.pkl
\end{verbatim}

\section{Running the Hyperparameter Tuning or Loading the Existing
Model}\label{running-the-hyperparameter-tuning-or-loading-the-existing-model}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': nan, 'hp_metric': nan}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3785.54541015625, 'hp_metric': 3785.54541015625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3005.0224609375, 'hp_metric': 3005.0224609375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4960.9326171875, 'hp_metric': 4960.9326171875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3578.1474609375, 'hp_metric': 3578.1474609375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5508.482421875, 'hp_metric': 5508.482421875}
train_model result: {'val_loss': 4394.2734375, 'hp_metric': 4394.2734375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3010.92236328125, 'hp_metric': 3010.92236328125}
spotpython tuning: 3005.0224609375 [----------] 2.19% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3692.31640625, 'hp_metric': 3692.31640625}
spotpython tuning: 3005.0224609375 [#---------] 6.68% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3565.788818359375, 'hp_metric': 3565.788818359375}
spotpython tuning: 3005.0224609375 [#---------] 8.31% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3632.890869140625, 'hp_metric': 3632.890869140625}
spotpython tuning: 3005.0224609375 [##--------] 16.30% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3270.799560546875, 'hp_metric': 3270.799560546875}
spotpython tuning: 3005.0224609375 [##--------] 19.85% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4029.51416015625, 'hp_metric': 4029.51416015625}
spotpython tuning: 3005.0224609375 [##--------] 23.20% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3130.77783203125, 'hp_metric': 3130.77783203125}
spotpython tuning: 3005.0224609375 [###-------] 26.61% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4266.46484375, 'hp_metric': 4266.46484375}
spotpython tuning: 3005.0224609375 [###-------] 29.28% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5275.7763671875, 'hp_metric': 5275.7763671875}
spotpython tuning: 3005.0224609375 [####------] 38.44% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2935.796875, 'hp_metric': 2935.796875}
spotpython tuning: 2935.796875 [####------] 41.79% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3109.743408203125, 'hp_metric': 3109.743408203125}
spotpython tuning: 2935.796875 [####------] 44.97% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3526.829833984375, 'hp_metric': 3526.829833984375}
spotpython tuning: 2935.796875 [######----] 64.09% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3361.64794921875, 'hp_metric': 3361.64794921875}
spotpython tuning: 2935.796875 [#######---] 68.89% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5165.126953125, 'hp_metric': 5165.126953125}
spotpython tuning: 2935.796875 [#######---] 72.93% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2924.69921875, 'hp_metric': 2924.69921875}
spotpython tuning: 2924.69921875 [########--] 77.69% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4374.9560546875, 'hp_metric': 4374.9560546875}
spotpython tuning: 2924.69921875 [########--] 81.47% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3835.705078125, 'hp_metric': 3835.705078125}
spotpython tuning: 2924.69921875 [#########-] 86.21% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3773.844970703125, 'hp_metric': 3773.844970703125}
spotpython tuning: 2924.69921875 [#########-] 91.20% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 2939.6162109375, 'hp_metric': 2939.6162109375}
spotpython tuning: 2924.69921875 [##########] 96.55% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3410.932861328125, 'hp_metric': 3410.932861328125}
spotpython tuning: 2924.69921875 [##########] 100.00% Done...

Experiment saved to 602_12_1_res.pkl
\end{verbatim}

\phantomsection\label{run_experiment}
\begin{verbatim}
<spotpython.spot.spot.Spot at 0x151bba540>
\end{verbatim}

\section{Results from the Hyperparameter Tuning
Experiment}\label{results-from-the-hyperparameter-tuning-experiment}

\begin{itemize}
\tightlist
\item
  After the hyperparameter tuning is finished, the following information
  is available:

  \begin{itemize}
  \tightlist
  \item
    the \texttt{S} object and the associated
  \item
    \texttt{fun\_control} dictionary
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.print\_results(print\_screen}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2924.69921875
l1: 4.0
epochs: 12.0
batch_size: 11.0
act_fn: 2.0
optimizer: 1.0
dropout_prob: 0.010496789685251895
lr_mult: 3.789080686036231
patience: 4.0
batch_norm: 0.0
initialization: 1.0
\end{verbatim}

\phantomsection\label{print_results}
\begin{verbatim}
[['l1', np.float64(4.0)],
 ['epochs', np.float64(12.0)],
 ['batch_size', np.float64(11.0)],
 ['act_fn', np.float64(2.0)],
 ['optimizer', np.float64(1.0)],
 ['dropout_prob', np.float64(0.010496789685251895)],
 ['lr_mult', np.float64(3.789080686036231)],
 ['patience', np.float64(4.0)],
 ['batch_norm', np.float64(0.0)],
 ['initialization', np.float64(1.0)]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/plot_progress_xai-output-1.pdf}}

\subsection{Getting the Best Model, i.e, the Tuned
Architecture}\label{getting-the-best-model-i.e-the-tuned-architecture}

\begin{itemize}
\tightlist
\item
  The method \texttt{get\_tuned\_architecture}
  \href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/hyperparameters/values/\#spotpython.hyperparameters.values.get_tuned_architecture}{{[}DOC{]}}
  returns the best model architecture found during the hyperparameter
  tuning.
\item
  It returns the transformed values, i.e.,
  \texttt{batch\_size\ =\ 2\^{}x} if the hyperparameter
  \texttt{batch\_size} was transformed with the
  \texttt{transform\_power\_2\_int} function.
\end{itemize}

\phantomsection\label{get_tuned_architecture}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\ImportTok{import}\NormalTok{ pprint}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(S)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': 0.010496789685251895,
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': 3.789080686036231,
 'optimizer': 'Adam',
 'patience': 16}
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Note: \texttt{get\_tuned\_architecture} has the option
  \texttt{force\_minX} which does not have any effect in this case.
\end{itemize}

\phantomsection\label{get_tuned_architecture_force_minx}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(S, force\_minX}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'act_fn': ReLU(),
 'batch_norm': False,
 'batch_size': 2048,
 'dropout_prob': 0.010496789685251895,
 'epochs': 4096,
 'initialization': 'kaiming_uniform',
 'l1': 16,
 'lr_mult': 3.789080686036231,
 'optimizer': 'Adam',
 'patience': 16}
\end{verbatim}

\section{Training the Tuned Architecture on the Test
Data}\label{training-the-tuned-architecture-on-the-test-data}

\begin{itemize}
\tightlist
\item
  Since we are interested in the explainability of the model, we will
  train the tuned architecture on the test data.
\item
  \texttt{spotpythons}'s \texttt{test\_model} function
  \href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/light/testmodel/}{{[}DOC{]}}
  is used to train the model on the test data.
\item
  Note: Until now, we do not use any information about the NN's weights
  and biases. Only the architecture, which is available as the
  \texttt{config}, is used.
\item
  \texttt{spotpython} used the TensorBoard logger to save the training
  process in the \texttt{./runs} directory. Therefore, we have to enable
  the TensorBoard logger in the \texttt{fun\_control} dictionary. To get
  a clean start, we remove an existing \texttt{runs} folder.
\end{itemize}

\phantomsection\label{test_model}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.light.testmodel }\ImportTok{import}\NormalTok{ test\_model}
\ImportTok{from}\NormalTok{ spotpython.light.loadmodel }\ImportTok{import}\NormalTok{ load\_light\_from\_checkpoint}
\NormalTok{fun\_control.update(\{}\StringTok{"tensorboard\_log"}\NormalTok{: }\VariableTok{True}\NormalTok{\})}
\NormalTok{test\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{test_model-1}
\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â        Test metric        â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â         hp_metric         â     4129.66845703125      â
â         val_loss          â     4129.66845703125      â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\begin{verbatim}
test_model result: {'val_loss': 4129.66845703125, 'hp_metric': 4129.66845703125}
\end{verbatim}

\phantomsection\label{test_model-2}
\begin{verbatim}
(4129.66845703125, 4129.66845703125)
\end{verbatim}

\phantomsection\label{load_model_from_chkpt}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ load\_light\_from\_checkpoint(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 2048, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.010496789685251895, 'lr_mult': 3.789080686036231, 'patience': 16, 'batch_norm': False, 'initialization': 'kaiming_uniform'}
Loading model with 16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TEST from runs/saved_models/16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TEST/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.010496789685251895, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.010496789685251895, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.010496789685251895, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): ReLU()
    (11): Dropout(p=0.010496789685251895, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): ReLU()
    (14): Dropout(p=0.010496789685251895, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): ReLU()
    (17): Dropout(p=0.010496789685251895, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): ReLU()
    (20): Dropout(p=0.010496789685251895, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
\end{verbatim}

\subsubsection{Details of the Training Process on the Test
Data}\label{details-of-the-training-process-on-the-test-data}

\begin{itemize}
\tightlist
\item
  The \texttt{test\_model} method initializes the model with the tuned
  architecture as follows:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"core\_model"}\NormalTok{](}\OperatorTok{**}\NormalTok{config, \_L\_in}\OperatorTok{=}\NormalTok{\_L\_in, \_L\_out}\OperatorTok{=}\NormalTok{\_L\_out, \_torchmetric}\OperatorTok{=}\NormalTok{\_torchmetric)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Then, the Lightning Trainer is initialized with the
  \texttt{fun\_control} dictionary and the model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    trainer }\OperatorTok{=}\NormalTok{ L.Trainer(}
\NormalTok{    default\_root\_dir}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id),}
\NormalTok{    max\_epochs}\OperatorTok{=}\NormalTok{model.hparams.epochs,}
\NormalTok{    accelerator}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"accelerator"}\NormalTok{],}
\NormalTok{    devices}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"devices"}\NormalTok{],}
\NormalTok{    logger}\OperatorTok{=}\NormalTok{TensorBoardLogger(}
\NormalTok{        save\_dir}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"TENSORBOARD\_PATH"}\NormalTok{],}
\NormalTok{        version}\OperatorTok{=}\NormalTok{config\_id,}
\NormalTok{        default\_hp\_metric}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        log\_graph}\OperatorTok{=}\NormalTok{fun\_control[}\StringTok{"log\_graph"}\NormalTok{],}
\NormalTok{    ),}
\NormalTok{    callbacks}\OperatorTok{=}\NormalTok{[}
\NormalTok{        EarlyStopping(monitor}\OperatorTok{=}\StringTok{"val\_loss"}\NormalTok{, patience}\OperatorTok{=}\NormalTok{config[}\StringTok{"patience"}\NormalTok{], mode}\OperatorTok{=}\StringTok{"min"}\NormalTok{, strict}\OperatorTok{=}\VariableTok{False}\NormalTok{, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{),}
\NormalTok{        ModelCheckpoint(}
\NormalTok{            dirpath}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id), save\_last}\OperatorTok{=}\VariableTok{True}
\NormalTok{        ), }
\NormalTok{    ],}
\NormalTok{    enable\_progress\_bar}\OperatorTok{=}\NormalTok{enable\_progress\_bar,}
\NormalTok{)}
\NormalTok{trainer.fit(model}\OperatorTok{=}\NormalTok{model, datamodule}\OperatorTok{=}\NormalTok{dm)    }
\NormalTok{test\_result }\OperatorTok{=}\NormalTok{ trainer.test(datamodule}\OperatorTok{=}\NormalTok{dm, ckpt\_path}\OperatorTok{=}\StringTok{"last"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  As shown in the code above, the last checkpoint ist saved.
\item
  \texttt{spotpython}'s method \texttt{load\_light\_from\_checkpoint} is
  used to load the last checkpoint and to get the model's weights and
  biases. It requires the \texttt{fun\_control} dictionary and the
  \texttt{config\_id} as input to find the correct checkpoint.
\item
  Now, the model is trained and the weights and biases are available.
\end{itemize}

\section{Visualizing the Neural Network
Architecture}\label{visualizing-the-neural-network-architecture}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get the device}
\ImportTok{from}\NormalTok{ spotpython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\NormalTok{device }\OperatorTok{=}\NormalTok{ getDevice()}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{viz_net_spotpython}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.xai }\ImportTok{import}\NormalTok{ viz\_net}
\NormalTok{viz\_net(model, device}\OperatorTok{=}\NormalTok{device)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{./model_architecture.png}}

}

\caption{architecture}

\end{figure}%

\section{XAI Methods}\label{xai-methods}

\begin{itemize}
\tightlist
\item
  \texttt{spotpython} provides methods to explain the model's
  predictions. The following neural network elements can be analyzed:
\end{itemize}

\subsection{Weights}\label{weights-1}

\begin{itemize}
\tightlist
\item
  Weights are the parameters of the neural network that are learned from
  the data during training. They connect neurons between layers and
  determine the strength and direction of the signal sent from one
  neuron to another. The network adjusts the weights during training to
  minimize the error between the predicted output and the actual output.
\item
  Interpretation of the weights: A high weight value indicates a strong
  influence of the input neuron on the output. Positive weights suggest
  a positive correlation, whereas negative weights suggest an inverse
  relationship between neurons.
\end{itemize}

\subsection{Activations}\label{activations}

\begin{itemize}
\tightlist
\item
  Activations are the outputs produced by neurons after applying an
  activation function to the weighted sum of inputs. The activation
  function (e.g., ReLU, sigmoid, tanh) adds non-linearity to the model,
  allowing it to learn more complex relationships.
\item
  Interpretation of the activations: The value of activations indicates
  the intensity of the signal passed to the next layer. Certain
  activation patterns can highlight which features or parts of the data
  the network is focusing on.
\end{itemize}

\subsection{Gradients}\label{gradients}

\begin{itemize}
\tightlist
\item
  Gradients are the partial derivatives of the loss function with
  respect to different parameters (weights) of the network. During
  backpropagation, gradients are used to update the weights in the
  direction that reduces the loss by methods like gradient descent.
\item
  Interpretation of the gradients: The magnitude of the gradient
  indicates how much a parameter should change to reduce the error. A
  large gradient implies a steeper slope and a bigger update, while a
  small gradient suggests that the parameter is near an optimal point.
  If gradients are too small (vanishing gradient problem), the network
  may learn slowly or stop learning. If they are too large (exploding
  gradient problem), the updates may be unstable.
\item
  \texttt{sptpython} provides the method \texttt{get\_gradients} to get
  the gradients of the model.
\end{itemize}

\phantomsection\label{import_xai}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.xai }\ImportTok{import}\NormalTok{ (get\_activations, get\_gradients, get\_weights, visualize\_weights, visualize\_gradients, visualize\_mean\_activations, visualize\_gradient\_distributions, visualize\_weights\_distributions, visualize\_activations\_distributions)}
\NormalTok{batch\_size }\OperatorTok{=}\NormalTok{ config[}\StringTok{"batch\_size"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\subsection{Getting the Weights}\label{getting-the-weights}

\phantomsection\label{get_weights}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.xai }\ImportTok{import}\NormalTok{ sort\_layers}
\NormalTok{weights, \_ }\OperatorTok{=}\NormalTok{ get\_weights(model)}
\CommentTok{\# sort\_layers(weights)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{visualize_weights}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_weights(model, absolute}\OperatorTok{=}\VariableTok{True}\NormalTok{, cmap}\OperatorTok{=}\StringTok{"GreenYellowRed"}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
3200 values in Layer Layer 0. Geometry: (320, 10)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-2.pdf}}

\begin{verbatim}
51200 values in Layer Layer 3. Geometry: (160, 320)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-4.pdf}}

\begin{verbatim}
51200 values in Layer Layer 6. Geometry: (320, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-6.pdf}}

\begin{verbatim}
51200 values in Layer Layer 9. Geometry: (160, 320)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-8.pdf}}

\begin{verbatim}
25600 values in Layer Layer 12. Geometry: (160, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-10.pdf}}

\begin{verbatim}
12800 values in Layer Layer 15. Geometry: (80, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-12.pdf}}

\begin{verbatim}
6400 values in Layer Layer 18. Geometry: (80, 80)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-14.pdf}}

\begin{verbatim}
80 values in Layer Layer 21. Geometry: (1, 80)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights-output-16.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_weights\_distributions(model, color}\OperatorTok{=}\SpecialStringTok{f"C}\SpecialCharTok{\{}\DecValTok{0}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{, columns}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n:8
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_weights_distributions-output-2.pdf}}

\subsection{Getting the Activations}\label{getting-the-activations}

\phantomsection\label{get_activations}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.xai }\ImportTok{import}\NormalTok{ get\_activations}
\NormalTok{activations, mean\_activations, layer\_sizes }\OperatorTok{=}\NormalTok{ get\_activations(net}\OperatorTok{=}\NormalTok{model, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, device}\OperatorTok{=}\NormalTok{device)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train & val data.
train samples: 160, val samples: 106 generated for train & val data.
LightDataModule.train_dataloader(). data_train size: 160
\end{verbatim}

\phantomsection\label{visualize_mean_activations}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_mean\_activations(mean\_activations, layer\_sizes}\OperatorTok{=}\NormalTok{layer\_sizes, absolute}\OperatorTok{=}\VariableTok{True}\NormalTok{, cmap}\OperatorTok{=}\StringTok{"GreenYellowRed"}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
320 values in Layer 0. Geometry: (1, 320)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_mean_activations-output-2.pdf}}

\begin{verbatim}
160 values in Layer 3. Geometry: (1, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_mean_activations-output-4.pdf}}

\begin{verbatim}
320 values in Layer 6. Geometry: (1, 320)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_mean_activations-output-6.pdf}}

\begin{verbatim}
160 values in Layer 9. Geometry: (1, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_mean_activations-output-8.pdf}}

\begin{verbatim}
160 values in Layer 12. Geometry: (1, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_mean_activations-output-10.pdf}}

\begin{verbatim}
80 values in Layer 15. Geometry: (1, 80)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_mean_activations-output-12.pdf}}

\begin{verbatim}
80 values in Layer 18. Geometry: (1, 80)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_mean_activations-output-14.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_activations\_distributions(activations}\OperatorTok{=}\NormalTok{activations,}
\NormalTok{                                    net}\OperatorTok{=}\NormalTok{model, color}\OperatorTok{=}\StringTok{"C0"}\NormalTok{, columns}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_activations_distributions-output-1.pdf}}

\subsection{Getting the Gradients}\label{getting-the-gradients}

\phantomsection\label{get_gradients}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gradients, \_ }\OperatorTok{=}\NormalTok{ get\_gradients(net}\OperatorTok{=}\NormalTok{model, fun\_control}\OperatorTok{=}\NormalTok{fun\_control, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, device}\OperatorTok{=}\NormalTok{device)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train & val data.
train samples: 160, val samples: 106 generated for train & val data.
LightDataModule.train_dataloader(). data_train size: 160
\end{verbatim}

\phantomsection\label{visualize_gradients}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_gradients(model, fun\_control, batch\_size, absolute}\OperatorTok{=}\VariableTok{True}\NormalTok{, cmap}\OperatorTok{=}\StringTok{"GreenYellowRed"}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{), device}\OperatorTok{=}\NormalTok{device)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train & val data.
train samples: 160, val samples: 106 generated for train & val data.
LightDataModule.train_dataloader(). data_train size: 160
3200 values in Layer layers.0.weight. Geometry: (320, 10)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-2.pdf}}

\begin{verbatim}
51200 values in Layer layers.3.weight. Geometry: (160, 320)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-4.pdf}}

\begin{verbatim}
51200 values in Layer layers.6.weight. Geometry: (320, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-6.pdf}}

\begin{verbatim}
51200 values in Layer layers.9.weight. Geometry: (160, 320)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-8.pdf}}

\begin{verbatim}
25600 values in Layer layers.12.weight. Geometry: (160, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-10.pdf}}

\begin{verbatim}
12800 values in Layer layers.15.weight. Geometry: (80, 160)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-12.pdf}}

\begin{verbatim}
6400 values in Layer layers.18.weight. Geometry: (80, 80)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-14.pdf}}

\begin{verbatim}
80 values in Layer layers.21.weight. Geometry: (1, 80)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradients-output-16.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visualize\_gradient\_distributions(model, fun\_control, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, color}\OperatorTok{=}\SpecialStringTok{f"C}\SpecialCharTok{\{}\DecValTok{0}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{, device}\OperatorTok{=}\NormalTok{device, columns}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting train & val data.
train samples: 160, val samples: 106 generated for train & val data.
LightDataModule.train_dataloader(). data_train size: 160
n:8
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/visualize_gradient_distributions-output-2.pdf}}

\section{Feature Attributions}\label{feature-attributions}

\subsection{Integrated Gradients}\label{integrated-gradients}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.xai }\ImportTok{import}\NormalTok{ get\_attributions, plot\_attributions}
\NormalTok{df\_att }\OperatorTok{=}\NormalTok{ get\_attributions(S, fun\_control, attr\_method}\OperatorTok{=}\StringTok{"IntegratedGradients"}\NormalTok{, n\_rel}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{plot\_attributions(df\_att, attr\_method}\OperatorTok{=}\StringTok{"IntegratedGradients"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': 3916.88623046875, 'hp_metric': 3916.88623046875}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 2048, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.010496789685251895, 'lr_mult': 3.789080686036231, 'patience': 16, 'batch_norm': False, 'initialization': 'kaiming_uniform'}
Loading model with 16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN from runs/saved_models/16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.010496789685251895, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.010496789685251895, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.010496789685251895, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): ReLU()
    (11): Dropout(p=0.010496789685251895, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): ReLU()
    (14): Dropout(p=0.010496789685251895, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): ReLU()
    (17): Dropout(p=0.010496789685251895, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): ReLU()
    (20): Dropout(p=0.010496789685251895, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting test data.
test samples: 177 generated for test data.
LightDataModule.test_dataloader(). Test set size: 177
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/get_attributions_xai-output-2.pdf}}

\subsection{Deep Lift}\label{deep-lift}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_lift }\OperatorTok{=}\NormalTok{ get\_attributions(S, fun\_control, attr\_method}\OperatorTok{=}\StringTok{"DeepLift"}\NormalTok{,n\_rel}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(df\_lift)}
\NormalTok{plot\_attributions(df\_lift,  attr\_method}\OperatorTok{=}\StringTok{"DeepLift"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': 3195.6181640625, 'hp_metric': 3195.6181640625}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 2048, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.010496789685251895, 'lr_mult': 3.789080686036231, 'patience': 16, 'batch_norm': False, 'initialization': 'kaiming_uniform'}
Loading model with 16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN from runs/saved_models/16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.010496789685251895, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.010496789685251895, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.010496789685251895, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): ReLU()
    (11): Dropout(p=0.010496789685251895, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): ReLU()
    (14): Dropout(p=0.010496789685251895, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): ReLU()
    (17): Dropout(p=0.010496789685251895, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): ReLU()
    (20): Dropout(p=0.010496789685251895, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting test data.
test samples: 177 generated for test data.
LightDataModule.test_dataloader(). Test set size: 177
   Feature Index Feature  DeepLiftAttribution
0              1     sex           242.692978
1              3      bp           225.995712
2              0     age           210.781250
3              6  s3_hdl           208.978897
4              8  s5_ltg           207.611267
5              2     bmi           195.785995
6              9  s6_glu           166.454773
7              5  s2_ldl           111.387947
8              4   s1_tc            97.000870
9              7  s4_tch            86.606956
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/get_attributions_deep_lift-output-2.pdf}}

\subsection{Feature Ablation}\label{feature-ablation}

\phantomsection\label{get_attributions_feature_ablation}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_fl }\OperatorTok{=}\NormalTok{ get\_attributions(S, fun\_control, attr\_method}\OperatorTok{=}\StringTok{"FeatureAblation"}\NormalTok{,n\_rel}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': 2994.0615234375, 'hp_metric': 2994.0615234375}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 2048, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.010496789685251895, 'lr_mult': 3.789080686036231, 'patience': 16, 'batch_norm': False, 'initialization': 'kaiming_uniform'}
Loading model with 16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN from runs/saved_models/16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.010496789685251895, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.010496789685251895, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.010496789685251895, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): ReLU()
    (11): Dropout(p=0.010496789685251895, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): ReLU()
    (14): Dropout(p=0.010496789685251895, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): ReLU()
    (17): Dropout(p=0.010496789685251895, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): ReLU()
    (20): Dropout(p=0.010496789685251895, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_size: 0.36, val_size: 0.24, test_sie: 0.4 for splitting test data.
test samples: 177 generated for test data.
LightDataModule.test_dataloader(). Test set size: 177
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(df\_fl)}
\NormalTok{plot\_attributions(df\_fl, attr\_method}\OperatorTok{=}\StringTok{"FeatureAblation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Feature Index Feature  FeatureAblationAttribution
0              3      bp                  136.539505
1              0     age                  122.376190
2              1     sex                  120.620827
3              8  s5_ltg                  115.179298
4              6  s3_hdl                  112.293907
5              2     bmi                  109.317413
6              9  s6_glu                   84.425591
7              5  s2_ldl                   38.699623
8              4   s1_tc                   20.153212
9              7  s4_tch                    4.975080
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/plot_attributions_feature_ablation-output-2.pdf}}

\section{Conductance}\label{conductance}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.plot.xai }\ImportTok{import}\NormalTok{ plot\_conductance\_last\_layer, get\_weights\_conductance\_last\_layer}
\NormalTok{weights\_last, layer\_conductance\_last }\OperatorTok{=}\NormalTok{ get\_weights\_conductance\_last\_layer(S, fun\_control)}
\NormalTok{plot\_conductance\_last\_layer(weights\_last, layer\_conductance\_last, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': 3778.288818359375, 'hp_metric': 3778.288818359375}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 2048, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.010496789685251895, 'lr_mult': 3.789080686036231, 'patience': 16, 'batch_norm': False, 'initialization': 'kaiming_uniform'}
Loading model with 16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN from runs/saved_models/16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.010496789685251895, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.010496789685251895, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.010496789685251895, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): ReLU()
    (11): Dropout(p=0.010496789685251895, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): ReLU()
    (14): Dropout(p=0.010496789685251895, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): ReLU()
    (17): Dropout(p=0.010496789685251895, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): ReLU()
    (20): Dropout(p=0.010496789685251895, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
train_model result: {'val_loss': 3133.411865234375, 'hp_metric': 3133.411865234375}
config: {'l1': 16, 'epochs': 4096, 'batch_size': 2048, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.010496789685251895, 'lr_mult': 3.789080686036231, 'patience': 16, 'batch_norm': False, 'initialization': 'kaiming_uniform'}
Loading model with 16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN from runs/saved_models/16_4096_2048_ReLU_Adam_0.0105_3.7891_16_False_kaiming_uniform_TRAIN/last.ckpt
Model: NNLinearRegressor(
  (layers): Sequential(
    (0): Linear(in_features=10, out_features=320, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.010496789685251895, inplace=False)
    (3): Linear(in_features=320, out_features=160, bias=True)
    (4): ReLU()
    (5): Dropout(p=0.010496789685251895, inplace=False)
    (6): Linear(in_features=160, out_features=320, bias=True)
    (7): ReLU()
    (8): Dropout(p=0.010496789685251895, inplace=False)
    (9): Linear(in_features=320, out_features=160, bias=True)
    (10): ReLU()
    (11): Dropout(p=0.010496789685251895, inplace=False)
    (12): Linear(in_features=160, out_features=160, bias=True)
    (13): ReLU()
    (14): Dropout(p=0.010496789685251895, inplace=False)
    (15): Linear(in_features=160, out_features=80, bias=True)
    (16): ReLU()
    (17): Dropout(p=0.010496789685251895, inplace=False)
    (18): Linear(in_features=80, out_features=80, bias=True)
    (19): ReLU()
    (20): Dropout(p=0.010496789685251895, inplace=False)
    (21): Linear(in_features=80, out_features=1, bias=True)
  )
)
Conductance analysis for layer:  Linear(in_features=80, out_features=1, bias=True)
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{602_spot_lightning_xai_files/figure-pdf/get_conductance-output-2.pdf}}

\chapter{HPT PyTorch Lightning Transformer:
Introduction}\label{hpt-pytorch-lightning-transformer-introduction}

In this chapter, we will introduce transformer. The transformer
architecture is a neural network architecture that is based on the
attention mechanism (Vaswani et al. 2017). It is particularly well
suited for sequence-to-sequence tasks, such as machine translation, text
summarization, and more. The transformer architecture has been a
breakthrough in the field of natural language processing (NLP) and has
been the basis for many state-of-the-art models in the field.

We start with a description of the transformer basics in
Section~\ref{sec-transformer-basics}.
Section~\ref{sec-details-implementation} provides a detailed description
of the implementation of the transformer architecture. Finally, an
example of a transformer implemented in PyTorch Lightning in presented
in Section~\ref{sec-transformer-in-lightning}.

\section{Transformer Basics}\label{sec-transformer-basics}

\subsection{Embedding}\label{embedding}

Word embedding is a technique where words or phrases (so-called tokens)
from the vocabulary are mapped to vectors of real numbers. These vectors
capture the semantic properties of the words. Words that are similar in
meaning are mapped to vectors that are close to each other in the vector
space, and words that are dissimilar are mapped to vectors that are far
apart. Word embeddings are needed for transformers for several reasons:

\begin{itemize}
\tightlist
\item
  Dimensionality reduction: Word embeddings reduce the dimensionality of
  the data. Instead of dealing with high-dimensional sparse vectors
  (like one-hot encoded vectors), we deal with dense vectors of much
  lower dimensionality.
\item
  Capturing semantic similarities: Word embeddings capture semantic
  similarities between words. This is crucial for tasks like text
  classification, sentiment analysis, etc., where the meaning of the
  words is important.
\item
  Handling unknown words: If a word is not present in the training data
  but appears in the test data, one-hot encoding cannot handle it. But
  word embeddings can handle such situations by mapping the unknown word
  to a vector that is similar to known words.
\item
  Input to neural networks: Transformers, like other neural networks,
  work with numerical data. Word embeddings provide a way to convert
  text data into numerical form that can be fed into these networks.
\end{itemize}

In the context of transformers, word embeddings are used as the initial
input representation. The transformer then learns more complex
representations by considering the context in which each token appears.

\subsubsection{Neural Network for
Embeddings}\label{neural-network-for-embeddings}

Idea for word embeddings: use a relatively simple NN that has one input
for every token (word, symbol) in the vocabulary. The output of the NN
is a vector of a fixed size, which is the word embedding. The network
that is used in this chapter is visualized in
Figure~\ref{fig-transformer-attention}. For simplicity, a 2-dimensional
output vector is used in this visualization. The weights of the NN are
randomly initialized, and are learned during training.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/transformer.png}}

}

\caption{\label{fig-transformer-attention}Transformer. Computation of
the self attention. In this example, we consider two inputs, i.e., (1,0)
and (0,1). For each input, there are two values, which results in a
\(2 \times 2\) matrix. In general, when there are \(T\) inputs, a
\(T \times T\) matrix will be generated. Figure credits:
\href{https://youtu.be/bQ5BoolX9Ag?si=vO5N_cHUULcl-dBn}{Starmer, Josh:
Decoder-Only Transformers, ChatGPTs specific Transformer, Clearly
Explained}.}

\end{figure}%

All tokens are embedded in this way. For each token there are two
numerical values, the embedding vector. The same network is used for
embedding all tokens. If a longer input is added, it can be embedded
with the same net.

\subsubsection{Positional Encoding for the
Embeddings}\label{positional-encoding-for-the-embeddings}

Positional encoding is added to the input embeddings to give the model
some information about the relative or absolute position of the tokens
in the sequence. The positional encodings have the same dimension as the
embeddings so that the two can be summed.

If a token occurs several times, it is embedded several times and
receives different embedding vectors, as the position is taken into
account by the positional encoding.

\subsection{Attention}\label{attention-1}

Attention describes how similar is each token to itself and to all other
tokens in the input, e.g., in a sentence. The attention mechanism can be
implemented as a set of layers in neural networks. There are a lot of
different possible definitions of ``attention'' in the literature, but
the one we will use here is the following: \emph{the attention mechanism
describes a weighted average of (sequence) elements with the weights
dynamically computed based on an input query and elements' keys} (Lippe
2022).

The goal is to take an average over the features of multiple elements.
However, instead of weighting each element equally, we want to weight
them depending on their actual values. In other words, we want to
dynamically decide on which inputs we want to ``attend'' more than
others.

Calculation of the self-attention:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Queries: Calculate two new values from the (two) values of the
  embedding vector using an NN, which are referred to as query values.
\item
  Keys: Calculate two new values, called key values, from the (two)
  values of the embedding vector using an NN.
\item
  Dot product: Calculate the dot product of the query values and the key
  values. This is a measure of the similarity of the query and key
  values.
\item
  Softmax: Apply the softmax function to the outputs from the dot
  product. This is a measure of the attention that a token pays to other
  tokens.
\item
  Values: Calculate two new values from the (two) values of the
  embedding vector using an NN, which are referred to as value values.
\item
  The values are multiplied (weighted) by the values of the softmax
  function.
\item
  The weighted values are summed. Now we have the self attention value
  for the token.
\end{enumerate}

\subsection{Self-Attention}\label{sec-self-attention}

Most attention mechanisms differ in terms of what queries they use, how
the key and value vectors are defined, and what score function is used.
The attention applied inside the Transformer architecture is called
``self-attention''. In self-attention, each sequence element provides a
key, value, and query. For each element, we perform an attention layer
where based on its query, we check the similarity of the all sequence
elements' keys, and returned a different, averaged value vector for each
element.

\subsection{Masked Self-Attention}\label{masked-self-attention-1}

Masked self-attention is a variant of the self-attention method
described in Section~\ref{sec-self-attention}. It asks the question: How
similar is each token to itself and to all preceding tokens in the input
(sentence)? Masked self-attention is an autoregressive mechanism, which
means that the attention mechanism is only allowed to look at the tokens
that have already been processed. Calculation of the mask self-attention
is identical to the self-attention, but the attention is only calculated
for the tokens that have already been processed. If the masked
self-attention method is applied to the first token, the masked
self-attention value is exactly the value of the first token, as it only
takes itself into account. For the other tokens, the masked
self-attention value is a weighted sum of the values of the previous
tokens. The weighting is determined by the similarity of the query
values and the key values (dot product and softmax).

\subsection{Generation of Outputs}\label{generation-of-outputs}

To calculate the output, we use a residual connector that adds the
output of the neural network and the output of the masked self-attention
method. We thus obtain the residual connection values. The residual
connector is used to facilitate training.

To generate the next token, we use another neural network that
calculates the output from the (two) residual connection values. The
input layer of the neural network has the size of the residual
connection values, the output layer has the number of tokens in the
vocabulary as a dimension.

If we now enter the residual connection value of the first token, we
receive the token (or the probabilities using Softmax) that is to come
next as the output of the neural network. This makes sense even if we
already know the second token (as with the first token): We can use it
to calculate the error of the neural network and train the network. In
addition, the decoder-transformer uses the masked self-attention method
to calculate the output, i.e.~the encoding and generation of new tokens
is done with exactly the same elements of the network.

Note: ChatGPT does not use a new neural network, but the same network
that was already used to calculate the embedding. The network is
therefore used for embedding, masked self-attention and calculating the
output. In the last calculation, the network is inverted, i.e.~it is run
in the opposite direction to obtain the tokens and not the embeddings as
in the original run.

\subsection{End-Of-Sequence-Token}\label{end-of-sequence-token}

The end-of-sequence token is used to signal the end of the input and
also to start generating new tokens after the input. The EOS token
recognizes all other tokens, as it comes after all tokens. When
generating tokens, it is important to consider the relationships between
the input tokens and the generation of new tokens.

\section{Details of the
Implementation}\label{sec-details-implementation}

We will now go into a bit more detail by first looking at the specific
implementation of the attention mechanism which is in the Transformer
case the (scaled) dot product attention. The variables shown in
Table~\ref{tbl-transformer-variables} are used in the Transformer
architecture.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}@{}}
\caption{Variables used in the Transformer
architecture.}\label{tbl-transformer-variables}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(Q\) & \texttt{query} & The query vectors. \\
\(K\) & \texttt{key} & The key vectors. \\
\(V\) & \texttt{value} & The value vectors. \\
\(d_{\text{model}}\) & \texttt{d\_model} & The dimensionality of the
input and output features of the Transformer. \\
\(d_k\) & \texttt{d\_k} & The hidden dimensionality of the key and query
vectors. \\
\(d_v\) & \texttt{d\_v} & The hidden dimensionality of the value
vectors. \\
\(h\) & \texttt{num\_heads} & The number of heads in the Multi-Head
Attention layer. \\
\(B\) & \texttt{batch\_size} & The batch size. \\
\(T\) & \texttt{seq\_length} & The sequence length. \\
\(X\) & \texttt{x} & The input features (input elements in the
sequence). \\
\(W^{Q}\) & \texttt{qkv\_proj} & The weight matrix to transform the
input to the query vectors. \\
\(W^{K}\) & \texttt{qkv\_proj} & The weight matrix to transform the
input to the key vectors. \\
\(W^{V}\) & \texttt{qkv\_proj} & The weight matrix to transform the
input to the value vectors. \\
\(W^{O}\) & \texttt{o\_proj} & The weight matrix to transform the
concatenated output of the Multi-Head Attention layer to the final
output. \\
\(N\) & \texttt{num\_layers} & The number of layers in the
Transformer. \\
\(PE_{(pos,i)}\) & \texttt{positional\_encoding} & The positional
encoding for position \(pos\) and hidden dimensionality \(i\). \\
\end{longtable}

Summarizing the ideas from Section~\ref{sec-transformer-basics}, an
attention mechanism has usually four parts we need to specify (Lippe
2022):

\begin{itemize}
\tightlist
\item
  \emph{Query}: The query is a feature vector that describes what we are
  looking for in the sequence, i.e., what would we maybe want to pay
  attention to.
\item
  \emph{Keys}: For each input element, we have a key which is again a
  feature vector. This feature vector roughly describes what the element
  is ``offering'', or when it might be important. The keys should be
  designed such that we can identify the elements we want to pay
  attention to based on the query.
\item
  \emph{Score function}: To rate which elements we want to pay attention
  to, we need to specify a score function \(f_{attn}\). The score
  function takes the query and a key as input, and output the
  score/attention weight of the query-key pair. It is usually
  implemented by simple similarity metrics like a dot product, or a
  small MLP.
\item
  \emph{Values}: For each input element, we also have a value vector.
  This feature vector is the one we want to average over.
\end{itemize}

The weights of the average are calculated by a softmax over all score
function outputs. Hence, we assign those value vectors a higher weight
whose corresponding key is most similar to the query. If we try to
describe it with pseudo-math, we can write:

\[
\alpha_i = \frac{\exp\left(f_{attn}\left(\text{key}_i, \text{query}\right)\right)}{\sum_j \exp\left(f_{attn}\left(\text{key}_j, \text{query}\right)\right)}, \hspace{5mm} \text{out} = \sum_i \alpha_i \cdot \text{value}_i
\]

Visually, we can show the attention over a sequence of words as follows:

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{./figures_static/attention_example.png}}

}

\caption{Attention over a sequence of words. For every word, we have one
key and one value vector. The query is compared to all keys with a score
function (in this case the dot product) to determine the weights. The
softmax is not visualized for simplicity. Finally, the value vectors of
all words are averaged using the attention weights. Figure taken from
Lippe (2022)}

\end{figure}%

\subsection{Dot Product Attention}\label{dot-product-attention}

Our goal is to have an attention mechanism with which any element in a
sequence can attend to any other while still being efficient to compute.
The dot product attention takes as input a set of queries
\(Q\in\mathbb{R}^{T\times d_k}\), keys \(K\in\mathbb{R}^{T\times d_k}\)
and values \(V\in\mathbb{R}^{T\times d_v}\) where \(T\) is the sequence
length, and \(d_k\) and \(d_v\) are the hidden dimensionality for
queries/keys and values respectively. For simplicity, we neglect the
batch dimension for now. The attention value from element \(i\) to \(j\)
is based on its similarity of the query \(Q_i\) and key \(K_j\), using
the dot product as the similarity metric (in
Figure~\ref{fig-transformer-attention}, we considered \(Q_2\) and
\(K_1\) as well as \(Q_2\) and \(K_2\)). The dot product attention is
calculated as follows:

\begin{equation}\phantomsection\label{eq-dot-product-attention}{
\text{Attention}(Q,K,V)=\text{softmax}\left(QK^T\right) V 
}\end{equation}

The matrix multiplication \(QK^T\) performs the dot product for every
possible pair of queries and keys, resulting in a matrix of the shape
\(T\times T\). Each row represents the attention logits for a specific
element \(i\) to all other elements in the sequence. On these, we apply
a softmax and multiply with the value vector to obtain a weighted mean
(the weights being determined by the attention).

\subsection{Scaled Dot Product
Attention}\label{scaled-dot-product-attention}

An additional aspect is the scaling of the dot product using a scaling
factor of \(1/\sqrt{d_k}\). This scaling factor is crucial to maintain
an appropriate variance of attention values after initialization. We
initialize our layers with the intention of having equal variance
throughout the model, and hence, \(Q\) and \(K\) might also have a
variance close to \(1\). However, performing a dot product over two
vectors with a variance \(\sigma^2\) results in a scalar having
\(d_k\)-times higher variance:

\[
q_i \sim \mathcal{N}(0,\sigma^2), k_i \sim \mathcal{N}(0,\sigma^2) \to \text{Var}\left(\sum_{i=1}^{d_k} q_i\cdot k_i\right) = \sigma^4\cdot d_k
\]

If we do not scale down the variance back to \(\sim\sigma^2\), the
softmax over the logits will already saturate to \(1\) for one random
element and \(0\) for all others. The gradients through the softmax will
be close to zero so that we can't learn the parameters appropriately.
Note that the extra factor of \(\sigma^2\), i.e., having \(\sigma^4\)
instead of \(\sigma^2\), is usually not an issue, since we keep the
original variance \(\sigma^2\) close to \(1\) anyways.
Equation~\ref{eq-dot-product-attention} can be modified as follows to
calculate the dot product attention:

\[
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V.
\]

Another perspective on this scaled dot product attention mechanism
offers the computation graph which is visualized in
Figure~\ref{fig-scaled-dot-product-attn}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/scaled_dot_product_attn.png}}

}

\caption{\label{fig-scaled-dot-product-attn}Scaled dot product
attention. Figure credit Vaswani et al. (2017)}

\end{figure}%

The block \texttt{Mask\ (opt.)} in the diagram above represents the
optional masking of specific entries in the attention matrix. This is
for instance used if we stack multiple sequences with different lengths
into a batch. To still benefit from parallelization in PyTorch, we pad
the sentences to the same length and mask out the padding tokens during
the calculation of the attention values. This is usually done by setting
the respective attention logits to a very low value.

After we have discussed the details of the scaled dot product attention
block, we can write a function below which computes the output features
given the triple of queries, keys, and values:

\section{Example: Transformer in
Lightning}\label{sec-transformer-in-lightning}

The following code is based on
\url{https://github.com/phlippe/uvadlc_notebooks/tree/master} (Author:
Phillip Lippe)

First, we import the necessary libraries and download the pretrained
models.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ random}
\ImportTok{import}\NormalTok{ math}
\ImportTok{import}\NormalTok{ json}
\ImportTok{from}\NormalTok{ functools }\ImportTok{import}\NormalTok{ partial}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ matplotlib.colors }\ImportTok{import}\NormalTok{ to\_rgb}
\ImportTok{import}\NormalTok{ matplotlib}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\CommentTok{\#\# tqdm for loading bars}
\ImportTok{from}\NormalTok{ tqdm.notebook }\ImportTok{import}\NormalTok{ tqdm}

\CommentTok{\#\# PyTorch}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F}
\ImportTok{import}\NormalTok{ torch.utils.data }\ImportTok{as}\NormalTok{ data}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}

\CommentTok{\# PyTorch Lightning}
\ImportTok{import}\NormalTok{ pytorch\_lightning }\ImportTok{as}\NormalTok{ pl}
\ImportTok{from}\NormalTok{ pytorch\_lightning.callbacks }\ImportTok{import}\NormalTok{ LearningRateMonitor, ModelCheckpoint}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Path to the folder where the pretrained models are saved}
\NormalTok{CHECKPOINT\_PATH }\OperatorTok{=} \StringTok{"../saved\_models/tutorial6"}

\CommentTok{\# Ensure that all operations are deterministic on GPU (if used) for reproducibility}
\NormalTok{torch.backends.cudnn.deterministic }\OperatorTok{=} \VariableTok{True}
\NormalTok{torch.backends.cudnn.benchmark }\OperatorTok{=} \VariableTok{False}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\NormalTok{device }\OperatorTok{=}\NormalTok{ getDevice()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Device:"}\NormalTok{, device)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Device: mps
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Setting the seed}
\NormalTok{pl.seed\_everything(}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
42
\end{verbatim}

Two pre-trained models are downloaded below. Make sure to have adjusted
your \texttt{CHECKPOINT\_PATH} before running this code if not already
done.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ urllib.request}
\ImportTok{from}\NormalTok{ urllib.error }\ImportTok{import}\NormalTok{ HTTPError}
\CommentTok{\# Github URL where saved models are stored for this tutorial}
\NormalTok{base\_url }\OperatorTok{=} \StringTok{"https://raw.githubusercontent.com/phlippe/saved\_models/main/tutorial6/"}
\CommentTok{\# Files to download}
\NormalTok{pretrained\_files }\OperatorTok{=}\NormalTok{ [}\StringTok{"ReverseTask.ckpt"}\NormalTok{, }\StringTok{"SetAnomalyTask.ckpt"}\NormalTok{]}

\CommentTok{\# Create checkpoint path if it doesn\textquotesingle{}t exist yet}
\NormalTok{os.makedirs(CHECKPOINT\_PATH, exist\_ok}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Downloading the Pretrained
Models}\label{downloading-the-pretrained-models}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# For each file, check whether it already exists. If not, try downloading it.}
\ControlFlowTok{for}\NormalTok{ file\_name }\KeywordTok{in}\NormalTok{ pretrained\_files:}
\NormalTok{    file\_path }\OperatorTok{=}\NormalTok{ os.path.join(CHECKPOINT\_PATH, file\_name)}
    \ControlFlowTok{if} \StringTok{"/"} \KeywordTok{in}\NormalTok{ file\_name:}
\NormalTok{        os.makedirs(file\_path.rsplit(}\StringTok{"/"}\NormalTok{,}\DecValTok{1}\NormalTok{)[}\DecValTok{0}\NormalTok{], exist\_ok}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ os.path.isfile(file\_path):}
\NormalTok{        file\_url }\OperatorTok{=}\NormalTok{ base\_url }\OperatorTok{+}\NormalTok{ file\_name}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Downloading }\SpecialCharTok{\{}\NormalTok{file\_url}\SpecialCharTok{\}}\SpecialStringTok{..."}\NormalTok{)}
        \ControlFlowTok{try}\NormalTok{:}
\NormalTok{            urllib.request.urlretrieve(file\_url, file\_path)}
        \ControlFlowTok{except}\NormalTok{ HTTPError }\ImportTok{as}\NormalTok{ e:}
            \BuiltInTok{print}\NormalTok{(}\StringTok{"Error:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, e)}
\end{Highlighting}
\end{Shaded}

\subsection{The Transformer
Architecture}\label{the-transformer-architecture}

We will implement the Transformer architecture by hand. As the
architecture is so popular, there already exists a Pytorch module
\texttt{nn.Transformer}
(\href{https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html}{documentation})
and a
\href{https://pytorch.org/tutorials/beginner/transformer_tutorial.html}{tutorial}
on how to use it for next token prediction. However, we will implement
it here ourselves, to get through to the smallest details.

\subsection{Attention Mechanism}\label{attention-mechanism}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ scaled\_dot\_product(q, k, v, mask}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Compute scaled dot product attention.}
\CommentTok{    Args:}
\CommentTok{        q: Queries}
\CommentTok{        k: Keys}
\CommentTok{        v: Values}
\CommentTok{        mask: Mask to apply to the attention logits}

\CommentTok{    Returns:}
\CommentTok{        Tuple of (Values, Attention weights)}

\CommentTok{    Examples:}
\CommentTok{    \textgreater{}\textgreater{}\textgreater{} seq\_len, d\_k = 1, 2}
\CommentTok{        pl.seed\_everything(42)}
\CommentTok{        q = torch.randn(seq\_len, d\_k)}
\CommentTok{        k = torch.randn(seq\_len, d\_k)}
\CommentTok{        v = torch.randn(seq\_len, d\_k)}
\CommentTok{        values, attention = scaled\_dot\_product(q, k, v)}
\CommentTok{        print("Q}\CharTok{\textbackslash{}n}\CommentTok{", q)}
\CommentTok{        print("K}\CharTok{\textbackslash{}n}\CommentTok{", k)}
\CommentTok{        print("V}\CharTok{\textbackslash{}n}\CommentTok{", v)}
\CommentTok{        print("Values}\CharTok{\textbackslash{}n}\CommentTok{", values)}
\CommentTok{        print("Attention}\CharTok{\textbackslash{}n}\CommentTok{", attention)}
\CommentTok{    """}
\NormalTok{    d\_k }\OperatorTok{=}\NormalTok{ q.size()[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    attn\_logits }\OperatorTok{=}\NormalTok{ torch.matmul(q, k.transpose(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{    attn\_logits }\OperatorTok{=}\NormalTok{ attn\_logits }\OperatorTok{/}\NormalTok{ math.sqrt(d\_k)}
    \ControlFlowTok{if}\NormalTok{ mask }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        attn\_logits }\OperatorTok{=}\NormalTok{ attn\_logits.masked\_fill(mask }\OperatorTok{==} \DecValTok{0}\NormalTok{, }\OperatorTok{{-}}\FloatTok{9e15}\NormalTok{)}
\NormalTok{    attention }\OperatorTok{=}\NormalTok{ F.softmax(attn\_logits, dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)}
\NormalTok{    values }\OperatorTok{=}\NormalTok{ torch.matmul(attention, v)}
    \ControlFlowTok{return}\NormalTok{ values, attention}
\end{Highlighting}
\end{Shaded}

Note that our code above supports any additional dimensionality in front
of the sequence length so that we can also use it for batches. However,
for a better understanding, let's generate a few random queries, keys,
and value vectors, and calculate the attention outputs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq\_len, d\_k }\OperatorTok{=} \DecValTok{1}\NormalTok{, }\DecValTok{2}
\NormalTok{pl.seed\_everything(}\DecValTok{42}\NormalTok{)}
\NormalTok{q }\OperatorTok{=}\NormalTok{ torch.randn(seq\_len, d\_k)}
\NormalTok{k }\OperatorTok{=}\NormalTok{ torch.randn(seq\_len, d\_k)}
\NormalTok{v }\OperatorTok{=}\NormalTok{ torch.randn(seq\_len, d\_k)}
\NormalTok{values, attention }\OperatorTok{=}\NormalTok{ scaled\_dot\_product(q, k, v)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Q}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, q)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"K}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, k)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"V}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, v)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Values}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, values)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Attention}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, attention)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Q
 tensor([[0.3367, 0.1288]])
K
 tensor([[0.2345, 0.2303]])
V
 tensor([[-1.1229, -0.1863]])
Values
 tensor([[-1.1229, -0.1863]])
Attention
 tensor([[1.]])
\end{verbatim}

\subsection{Multi-Head Attention}\label{multi-head-attention}

The scaled dot product attention allows a network to attend over a
sequence. However, often there are multiple different aspects a sequence
element wants to attend to, and a single weighted average is not a good
option for it. This is why we extend the attention mechanisms to
multiple heads, i.e.~multiple different query-key-value triplets on the
same features. Specifically, given a query, key, and value matrix, we
transform those into \(h\) sub-queries, sub-keys, and sub-values, which
we pass through the scaled dot product attention independently.
Afterward, we concatenate the heads and combine them with a final weight
matrix. Mathematically, we can express this operation as:

\[
\begin{split}
    \text{Multihead}(Q,K,V) & = \text{Concat}(\text{head}_1,...,\text{head}_h)W^{O}\\
    \text{where } \text{head}_i & = \text{Attention}(QW_i^Q,KW_i^K, VW_i^V)
\end{split}
\]

We refer to this as Multi-Head Attention layer with the learnable
parameters \(W_{1...h}^{Q}\in\mathbb{R}^{D\times d_k}\),
\(W_{1...h}^{K}\in\mathbb{R}^{D\times d_k}\),
\(W_{1...h}^{V}\in\mathbb{R}^{D\times d_v}\), and
\(W^{O}\in\mathbb{R}^{h\cdot d_v\times d_{out}}\) (\(D\) being the input
dimensionality). Expressed in a computational graph, we can visualize it
as in Figure~\ref{fig-multihead-attention}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/multihead_attention.png}}

}

\caption{\label{fig-multihead-attention}Multi-Head Attention. Figure
taken from Vaswani et al. (2017)}

\end{figure}%

How are we applying a Multi-Head Attention layer in a neural network,
where we do not have an arbitrary query, key, and value vector as input?
Looking at the computation graph in
Figure~\ref{fig-multihead-attention}, a simple but effective
implementation is to set the current feature map in a NN,
\(X\in\mathbb{R}^{B\times T\times d_{\text{model}}}\), as \(Q\), \(K\)
and \(V\) (\(B\) being the batch size, \(T\) the sequence length,
\(d_{\text{model}}\) the hidden dimensionality of \(X\)). The
consecutive weight matrices \(W^{Q}\), \(W^{K}\), and \(W^{V}\) can
transform \(X\) to the corresponding feature vectors that represent the
queries, keys, and values of the input. Using this approach, we can
implement the Multi-Head Attention module below.

As a consequence, if the embedding dimension is 4, then 1, 2 or 4 heads
can be used, but not 3. If 4 heads are used, then the dimension of the
query, key and value vectors is 1. If 2 heads are used, then the
dimension of the query, key and value vectors is \(D=2\). If 1 head is
used, then the dimension of the query, key and value vectors is \(D=4\).
The number of heads is a hyperparameter that can be adjusted. The number
of heads is usually 8 or 16.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Helper function to support different mask shapes.}
\CommentTok{\# Output shape supports (batch\_size, number of heads, seq length, seq length)}
\CommentTok{\# If 2D: broadcasted over batch size and number of heads}
\CommentTok{\# If 3D: broadcasted over number of heads}
\CommentTok{\# If 4D: leave as is}
\KeywordTok{def}\NormalTok{ expand\_mask(mask):}
    \ControlFlowTok{assert}\NormalTok{ mask.ndim }\OperatorTok{\textgreater{}=} \DecValTok{2}\NormalTok{, }\StringTok{"Mask must be \textgreater{}= 2{-}dim. with seq\_length x seq\_length"}
    \ControlFlowTok{if}\NormalTok{ mask.ndim }\OperatorTok{==} \DecValTok{3}\NormalTok{:}
\NormalTok{        mask }\OperatorTok{=}\NormalTok{ mask.unsqueeze(}\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{while}\NormalTok{ mask.ndim }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
\NormalTok{        mask }\OperatorTok{=}\NormalTok{ mask.unsqueeze(}\DecValTok{0}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ mask}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ MultiheadAttention(nn.Module):}
    
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_dim, embed\_dim, num\_heads):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \ControlFlowTok{assert}\NormalTok{ embed\_dim }\OperatorTok{\%}\NormalTok{ num\_heads }\OperatorTok{==} \DecValTok{0}\NormalTok{, }\StringTok{"Embedding dim. must be 0 modulo number of heads."}
        
        \VariableTok{self}\NormalTok{.embed\_dim }\OperatorTok{=}\NormalTok{ embed\_dim}
        \VariableTok{self}\NormalTok{.num\_heads }\OperatorTok{=}\NormalTok{ num\_heads}
        \VariableTok{self}\NormalTok{.head\_dim }\OperatorTok{=}\NormalTok{ embed\_dim }\OperatorTok{//}\NormalTok{ num\_heads}
        
        \CommentTok{\# Stack all weight matrices 1...h together for efficiency}
        \CommentTok{\# Note that in many implementations you see "bias=False" which is optional}
        \VariableTok{self}\NormalTok{.qkv\_proj }\OperatorTok{=}\NormalTok{ nn.Linear(input\_dim, }\DecValTok{3}\OperatorTok{*}\NormalTok{embed\_dim)}
        \VariableTok{self}\NormalTok{.o\_proj }\OperatorTok{=}\NormalTok{ nn.Linear(embed\_dim, embed\_dim)}
        
        \VariableTok{self}\NormalTok{.\_reset\_parameters()}

    \KeywordTok{def}\NormalTok{ \_reset\_parameters(}\VariableTok{self}\NormalTok{):}
        \CommentTok{\# Original Transformer initialization, see PyTorch documentation}
\NormalTok{        nn.init.xavier\_uniform\_(}\VariableTok{self}\NormalTok{.qkv\_proj.weight)}
        \VariableTok{self}\NormalTok{.qkv\_proj.bias.data.fill\_(}\DecValTok{0}\NormalTok{)}
\NormalTok{        nn.init.xavier\_uniform\_(}\VariableTok{self}\NormalTok{.o\_proj.weight)}
        \VariableTok{self}\NormalTok{.o\_proj.bias.data.fill\_(}\DecValTok{0}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x, mask}\OperatorTok{=}\VariableTok{None}\NormalTok{, return\_attention}\OperatorTok{=}\VariableTok{False}\NormalTok{):}
\NormalTok{        batch\_size, seq\_length, \_ }\OperatorTok{=}\NormalTok{ x.size()}
        \ControlFlowTok{if}\NormalTok{ mask }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            mask }\OperatorTok{=}\NormalTok{ expand\_mask(mask)}
\NormalTok{        qkv }\OperatorTok{=} \VariableTok{self}\NormalTok{.qkv\_proj(x)}
        
        \CommentTok{\# Separate Q, K, V from linear output}
\NormalTok{        qkv }\OperatorTok{=}\NormalTok{ qkv.reshape(batch\_size, seq\_length, }\VariableTok{self}\NormalTok{.num\_heads, }\DecValTok{3}\OperatorTok{*}\VariableTok{self}\NormalTok{.head\_dim)}
\NormalTok{        qkv }\OperatorTok{=}\NormalTok{ qkv.permute(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{) }\CommentTok{\# [Batch, Head, SeqLen, Dims]}
\NormalTok{        q, k, v }\OperatorTok{=}\NormalTok{ qkv.chunk(}\DecValTok{3}\NormalTok{, dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)}
        
        \CommentTok{\# Determine value outputs}
\NormalTok{        values, attention }\OperatorTok{=}\NormalTok{ scaled\_dot\_product(q, k, v, mask}\OperatorTok{=}\NormalTok{mask)}
\NormalTok{        values }\OperatorTok{=}\NormalTok{ values.permute(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{) }\CommentTok{\# [Batch, SeqLen, Head, Dims]}
\NormalTok{        values }\OperatorTok{=}\NormalTok{ values.reshape(batch\_size, seq\_length, }\VariableTok{self}\NormalTok{.embed\_dim)}
\NormalTok{        o }\OperatorTok{=} \VariableTok{self}\NormalTok{.o\_proj(values)}
        
        \ControlFlowTok{if}\NormalTok{ return\_attention:}
            \ControlFlowTok{return}\NormalTok{ o, attention}
        \ControlFlowTok{else}\NormalTok{:}
            \ControlFlowTok{return}\NormalTok{ o}
\end{Highlighting}
\end{Shaded}

\subsection{Permutation Equivariance}\label{permutation-equivariance}

One crucial characteristic of the multi-head attention is that it is
permutation-equivariant with respect to its inputs. This means that if
we switch two input elements in the sequence,
e.g.~\(X_1\leftrightarrow X_2\) (neglecting the batch dimension for
now), the output is exactly the same besides the elements 1 and 2
switched. Hence, the multi-head attention is actually looking at the
input not as a sequence, but as a set of elements. This property makes
the multi-head attention block and the Transformer architecture so
powerful and widely applicable! But what if the order of the input is
actually important for solving the task, like language modeling? The
answer is to encode the position in the input features, which we will
take a closer look in Section~\ref{sec-positional-encoding}.

\subsection{Transformer Encoder}\label{transformer-encoder}

Next, we will look at how to apply the multi-head attention block inside
the Transformer architecture. Originally, the Transformer model was
designed for machine translation. Hence, it got an encoder-decoder
structure where the encoder takes as input the sentence in the original
language and generates an attention-based representation. On the other
hand, the decoder attends over the encoded information and generates the
translated sentence in an autoregressive manner, as in a standard RNN.
While this structure is extremely useful for Sequence-to-Sequence tasks
with the necessity of autoregressive decoding, we will focus here on the
encoder part. Many advances in NLP have been made using pure
encoder-based Transformer models (if interested, models include the
BERT-family (Devlin et al. 2018), the Vision Transformer (Dosovitskiy et
al. 2020), and more). We will also mainly focus on the encoder part. If
you have understood the encoder architecture, the decoder is a very
small step to implement as well. The full Transformer architecture looks
as shown in Figure~\ref{fig-transformer-architecture}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/transformer_architecture.png}}

}

\caption{\label{fig-transformer-architecture}Transformer architecture.
Figure credit: Vaswani et al. (2017)}

\end{figure}%

The encoder consists of \(N\) identical blocks that are applied in
sequence. Taking as input \(x\), it is first passed through a Multi-Head
Attention block as we have implemented above. The output is added to the
original input using a residual connection, and we apply a consecutive
Layer Normalization on the sum. Overall, it calculates \[
\text{LayerNorm}(x+\text{Multihead}(x,x,x))
\] (\(x\) being \(Q\), \(K\) and \(V\) input to the attention layer).
The residual connection is crucial in the Transformer architecture for
two reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Similar to ResNets, Transformers are designed to be very deep. Some
  models contain more than 24 blocks in the encoder. Hence, the residual
  connections are crucial for enabling a smooth gradient flow through
  the model.
\item
  Without the residual connection, the information about the original
  sequence is lost. Remember that the Multi-Head Attention layer ignores
  the position of elements in a sequence, and can only learn it based on
  the input features. Removing the residual connections would mean that
  this information is lost after the first attention layer (after
  initialization), and with a randomly initialized query and key vector,
  the output vectors for position \(i\) has no relation to its original
  input. All outputs of the attention are likely to represent
  similar/same information, and there is no chance for the model to
  distinguish which information came from which input element. An
  alternative option to residual connection would be to fix at least one
  head to focus on its original input, but this is very inefficient and
  does not have the benefit of the improved gradient flow.
\end{enumerate}

\subsection{Layer Normalization and Feed-Forward
Network}\label{layer-normalization-and-feed-forward-network}

The Layer Normalization also plays an important role in the Transformer
architecture as it enables faster training and provides small
regularization. Additionally, it ensures that the features are in a
similar magnitude among the elements in the sequence.

We are not using Batch Normalization because it depends on the batch
size which is often small with Transformers (they require a lot of GPU
memory), and BatchNorm has shown to perform particularly bad in language
as the features of words tend to have a much higher variance (there are
many, very rare words which need to be considered for a good
distribution estimate).

Additionally to the Multi-Head Attention, a small fully connected
feed-forward network is added to the model, which is applied to each
position separately and identically. Specifically, the model uses a
Linear\(\to\)ReLU\(\to\)Linear MLP. The full transformation including
the residual connection can be expressed as:

\[
\begin{split}
    \text{FFN}(x) & = \max(0, xW_1+b_1)W_2 + b_2\\
    x & = \text{LayerNorm}(x + \text{FFN}(x))
\end{split}
\]

This MLP adds extra complexity to the model and allows transformations
on each sequence element separately. You can imagine as this allows the
model to ``post-process'' the new information added by the previous
Multi-Head Attention, and prepare it for the next attention block.
Usually, the inner dimensionality of the MLP is 2-8\(\times\) larger
than \(d_{\text{model}}\), i.e.~the dimensionality of the original input
\(x\). The general advantage of a wider layer instead of a narrow,
multi-layer MLP is the faster, parallelizable execution.

Finally, after looking at all parts of the encoder architecture, we can
start implementing it below. We first start by implementing a single
encoder block. Additionally to the layers described above, we will add
dropout layers in the MLP and on the output of the MLP and Multi-Head
Attention for regularization.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ EncoderBlock(nn.Module):}
    
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_dim, num\_heads, dim\_feedforward, dropout}\OperatorTok{=}\FloatTok{0.0}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Inputs:}
\CommentTok{            input\_dim {-} Dimensionality of the input}
\CommentTok{            num\_heads {-} Number of heads to use in the attention block}
\CommentTok{            dim\_feedforward {-} Dimensionality of the hidden layer in the MLP}
\CommentTok{            dropout {-} Dropout probability to use in the dropout layers}
\CommentTok{        """}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        
        \CommentTok{\# Attention layer}
        \VariableTok{self}\NormalTok{.self\_attn }\OperatorTok{=}\NormalTok{ MultiheadAttention(input\_dim, input\_dim, num\_heads)}
        
        \CommentTok{\# Two{-}layer MLP}
        \VariableTok{self}\NormalTok{.linear\_net }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.Linear(input\_dim, dim\_feedforward),}
\NormalTok{            nn.Dropout(dropout),}
\NormalTok{            nn.ReLU(inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{            nn.Linear(dim\_feedforward, input\_dim)}
\NormalTok{        )}
        
        \CommentTok{\# Layers to apply in between the main layers}
        \VariableTok{self}\NormalTok{.norm1 }\OperatorTok{=}\NormalTok{ nn.LayerNorm(input\_dim)}
        \VariableTok{self}\NormalTok{.norm2 }\OperatorTok{=}\NormalTok{ nn.LayerNorm(input\_dim)}
        \VariableTok{self}\NormalTok{.dropout }\OperatorTok{=}\NormalTok{ nn.Dropout(dropout)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x, mask}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
        \CommentTok{\# Attention part}
\NormalTok{        attn\_out }\OperatorTok{=} \VariableTok{self}\NormalTok{.self\_attn(x, mask}\OperatorTok{=}\NormalTok{mask)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+} \VariableTok{self}\NormalTok{.dropout(attn\_out)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.norm1(x)}
        
        \CommentTok{\# MLP part}
\NormalTok{        linear\_out }\OperatorTok{=} \VariableTok{self}\NormalTok{.linear\_net(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+} \VariableTok{self}\NormalTok{.dropout(linear\_out)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.norm2(x)}
        
        \ControlFlowTok{return}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}

Based on this block, we can implement a module for the full Transformer
encoder. Additionally to a forward function that iterates through the
sequence of encoder blocks, we also provide a function called
\texttt{get\_attention\_maps}. The idea of this function is to return
the attention probabilities for all Multi-Head Attention blocks in the
encoder. This helps us in understanding, and in a sense, explaining the
model. However, the attention probabilities should be interpreted with a
grain of salt as it does not necessarily reflect the true interpretation
of the model (there is a series of papers about this, including Jain and
Wallace (2019) and Wiegreffe and Pinter (2019)).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ TransformerEncoder(nn.Module):}
    
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, num\_layers, }\OperatorTok{**}\NormalTok{block\_args):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.ModuleList(}
\NormalTok{            [EncoderBlock(}\OperatorTok{**}\NormalTok{block\_args) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_layers)])}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x, mask}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \VariableTok{self}\NormalTok{.layers:}
\NormalTok{            x }\OperatorTok{=}\NormalTok{ l(x, mask}\OperatorTok{=}\NormalTok{mask)}
        \ControlFlowTok{return}\NormalTok{ x}

    \KeywordTok{def}\NormalTok{ get\_attention\_maps(}\VariableTok{self}\NormalTok{, x, mask}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
\NormalTok{        attention\_maps }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \VariableTok{self}\NormalTok{.layers:}
\NormalTok{            \_, attn\_map }\OperatorTok{=}\NormalTok{ l.self\_attn(x, mask}\OperatorTok{=}\NormalTok{mask, return\_attention}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{            attention\_maps.append(attn\_map)}
\NormalTok{            x }\OperatorTok{=}\NormalTok{ l(x)}
        \ControlFlowTok{return}\NormalTok{ attention\_maps}
\end{Highlighting}
\end{Shaded}

\subsection{Positional Encoding}\label{sec-positional-encoding}

We have discussed before that the Multi-Head Attention block is
permutation-equivariant, and cannot distinguish whether an input comes
before another one in the sequence or not. In tasks like language
understanding, however, the position is important for interpreting the
input words. The position information can therefore be added via the
input features. We could learn a embedding for every possible position,
but this would not generalize to a dynamical input sequence length.
Hence, the better option is to use feature patterns that the network can
identify from the features and potentially generalize to larger
sequences. The specific pattern chosen by Vaswani et al. (2017) are sine
and cosine functions of different frequencies, as follows:

\[
PE_{(pos,i)} = \begin{cases}
    \sin\left(\frac{pos}{10000^{i/d_{\text{model}}}}\right) & \text{if}\hspace{3mm} i \text{ mod } 2=0\\
    \cos\left(\frac{pos}{10000^{(i-1)/d_{\text{model}}}}\right) & \text{otherwise}\\
\end{cases}
\]

\(PE_{(pos,i)}\) represents the position encoding at position \(pos\) in
the sequence, and hidden dimensionality \(i\). These values,
concatenated for all hidden dimensions, are added to the original input
features (in the Transformer visualization above, see ``Positional
encoding''), and constitute the position information. We distinguish
between even (\(i \text{ mod } 2=0\)) and uneven
(\(i \text{ mod } 2=1\)) hidden dimensionalities where we apply a
sine/cosine respectively. The intuition behind this encoding is that you
can represent \(PE_{(pos+k,:)}\) as a linear function of
\(PE_{(pos,:)}\), which might allow the model to easily attend to
relative positions. The wavelengths in different dimensions range from
\(2\pi\) to \(10000\cdot 2\pi\).

The positional encoding is implemented below. The code is taken from the
PyTorch tutorial
\url{https://pytorch.org/tutorials/beginner/transformer_tutorial.html\#define-the-model}
about Transformers on NLP and adjusted for our purposes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ PositionalEncoding(nn.Module):}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, d\_model, max\_len}\OperatorTok{=}\DecValTok{5000}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Inputs}
\CommentTok{            d\_model {-} Hidden dimensionality of the input.}
\CommentTok{            max\_len {-} Maximum length of a sequence to expect.}
\CommentTok{        """}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}

        \CommentTok{\# Create matrix of [SeqLen, HiddenDim] representing }
        \CommentTok{\# the positional encoding for max\_len inputs}
\NormalTok{        pe }\OperatorTok{=}\NormalTok{ torch.zeros(max\_len, d\_model)}
\NormalTok{        position }\OperatorTok{=}\NormalTok{ torch.arange(}\DecValTok{0}\NormalTok{, max\_len, dtype}\OperatorTok{=}\NormalTok{torch.}\BuiltInTok{float}\NormalTok{).unsqueeze(}\DecValTok{1}\NormalTok{)}
\NormalTok{        div\_term }\OperatorTok{=}\NormalTok{ torch.exp(torch.arange(}\DecValTok{0}\NormalTok{, d\_model, }\DecValTok{2}\NormalTok{).}\BuiltInTok{float}\NormalTok{() }\OperatorTok{*}\NormalTok{ (}\OperatorTok{{-}}\NormalTok{math.log(}\FloatTok{10000.0}\NormalTok{) }\OperatorTok{/}\NormalTok{ d\_model))}
\NormalTok{        pe[:, }\DecValTok{0}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ torch.sin(position }\OperatorTok{*}\NormalTok{ div\_term)}
\NormalTok{        pe[:, }\DecValTok{1}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ torch.cos(position }\OperatorTok{*}\NormalTok{ div\_term)}
\NormalTok{        pe }\OperatorTok{=}\NormalTok{ pe.unsqueeze(}\DecValTok{0}\NormalTok{)}
        
        \CommentTok{\# register\_buffer =\textgreater{} Tensor which is not a parameter,}
        \CommentTok{\# but should be part of the modules state.}
        \CommentTok{\# Used for tensors that need to be on the same device as the module.}
        \CommentTok{\# persistent=False tells PyTorch to not add the buffer to the }
        \CommentTok{\# state dict (e.g. when we save the model) }
        \VariableTok{self}\NormalTok{.register\_buffer(}\StringTok{\textquotesingle{}pe\textquotesingle{}}\NormalTok{, pe, persistent}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+} \VariableTok{self}\NormalTok{.pe[:, :x.size(}\DecValTok{1}\NormalTok{)]}
        \ControlFlowTok{return}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}

To understand the positional encoding, we can visualize it below. We
will generate an image of the positional encoding over hidden
dimensionality and position in a sequence. Each pixel, therefore,
represents the change of the input feature we perform to encode the
specific position. Let's do it below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matplotlib.rcParams[}\StringTok{\textquotesingle{}lines.linewidth\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \FloatTok{2.0}
\NormalTok{plt.set\_cmap(}\StringTok{\textquotesingle{}cividis\textquotesingle{}}\NormalTok{)}
\NormalTok{encod\_block }\OperatorTok{=}\NormalTok{ PositionalEncoding(d\_model}\OperatorTok{=}\DecValTok{48}\NormalTok{, max\_len}\OperatorTok{=}\DecValTok{96}\NormalTok{)}
\NormalTok{pe }\OperatorTok{=}\NormalTok{ encod\_block.pe.squeeze().T.cpu().numpy()}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(nrows}\OperatorTok{=}\DecValTok{1}\NormalTok{, ncols}\OperatorTok{=}\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{pos }\OperatorTok{=}\NormalTok{ ax.imshow(pe, cmap}\OperatorTok{=}\StringTok{"RdGy"}\NormalTok{, extent}\OperatorTok{=}\NormalTok{(}\DecValTok{1}\NormalTok{,pe.shape[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{,pe.shape[}\DecValTok{0}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{fig.colorbar(pos, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Position in sequence"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Hidden dimension"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Positional encoding over hidden dimensions"}\NormalTok{)}
\NormalTok{ax.set\_xticks([}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{[i}\OperatorTok{*}\DecValTok{10} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\OperatorTok{+}\NormalTok{pe.shape[}\DecValTok{1}\NormalTok{]}\OperatorTok{//}\DecValTok{10}\NormalTok{)])}
\NormalTok{ax.set\_yticks([}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{[i}\OperatorTok{*}\DecValTok{10} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\OperatorTok{+}\NormalTok{pe.shape[}\DecValTok{0}\NormalTok{]}\OperatorTok{//}\DecValTok{10}\NormalTok{)])}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Figure size 1650x1050 with 0 Axes>
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{603_spot_lightning_transformer_introduction_files/figure-pdf/cell-15-output-2.pdf}}

You can clearly see the sine and cosine waves with different wavelengths
that encode the position in the hidden dimensions. Specifically, we can
look at the sine/cosine wave for each hidden dimension separately, to
get a better intuition of the pattern. Below we visualize the positional
encoding for the hidden dimensions \(1\), \(2\), \(3\) and \(4\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.set\_theme()}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ [a }\ControlFlowTok{for}\NormalTok{ a\_list }\KeywordTok{in}\NormalTok{ ax }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ a\_list]}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(ax)):}
\NormalTok{    ax[i].plot(np.arange(}\DecValTok{1}\NormalTok{,}\DecValTok{17}\NormalTok{), pe[i,:}\DecValTok{16}\NormalTok{], color}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}C}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{,}
\NormalTok{                markersize}\OperatorTok{=}\DecValTok{6}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{"black"}\NormalTok{)}
\NormalTok{    ax[i].set\_title(}\SpecialStringTok{f"Encoding in hidden dimension }\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    ax[i].set\_xlabel(}\StringTok{"Position in sequence"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{    ax[i].set\_ylabel(}\StringTok{"Positional encoding"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{    ax[i].set\_xticks(np.arange(}\DecValTok{1}\NormalTok{,}\DecValTok{17}\NormalTok{))}
\NormalTok{    ax[i].tick\_params(axis}\OperatorTok{=}\StringTok{\textquotesingle{}both\textquotesingle{}}\NormalTok{, which}\OperatorTok{=}\StringTok{\textquotesingle{}major\textquotesingle{}}\NormalTok{, labelsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{    ax[i].tick\_params(axis}\OperatorTok{=}\StringTok{\textquotesingle{}both\textquotesingle{}}\NormalTok{, which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, labelsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{    ax[i].set\_ylim(}\OperatorTok{{-}}\FloatTok{1.2}\NormalTok{, }\FloatTok{1.2}\NormalTok{)}
\NormalTok{fig.subplots\_adjust(hspace}\OperatorTok{=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{sns.reset\_orig()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{603_spot_lightning_transformer_introduction_files/figure-pdf/cell-16-output-1.pdf}}

As we can see, the patterns between the hidden dimension \(1\) and \(2\)
only differ in the starting angle. The wavelength is \(2\pi\), hence the
repetition after position \(6\). The hidden dimensions \(2\) and \(3\)
have about twice the wavelength.

\subsection{Learning Rate Warm-up}\label{learning-rate-warm-up}

One commonly used technique for training a Transformer is learning rate
warm-up. This means that we gradually increase the learning rate from 0
on to our originally specified learning rate in the first few
iterations. Thus, we slowly start learning instead of taking very large
steps from the beginning. In fact, training a deep Transformer without
learning rate warm-up can make the model diverge and achieve a much
worse performance on training and testing. Take for instance the
following plot by \href{https://arxiv.org/pdf/1908.03265.pdf}{Liu et
al.~(2019)} comparing Adam-vanilla (i.e.~Adam without warm-up) vs Adam
with a warm-up:

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{./figures_static/warmup_loss_plot.png}}

}

\caption{Warm-up comparison. Figure taken from Liu et al. (2019)}

\end{figure}%

Clearly, the warm-up is a crucial hyperparameter in the Transformer
architecture. Why is it so important? There are currently two common
explanations. Firstly, Adam uses the bias correction factors which
however can lead to a higher variance in the adaptive learning rate
during the first iterations. Improved optimizers like
\href{https://arxiv.org/abs/1908.03265}{RAdam} have been shown to
overcome this issue, not requiring warm-up for training Transformers.
Secondly, the iteratively applied Layer Normalization across layers can
lead to very high gradients during the first iterations, which can be
solved by using
\href{https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf}{Pre-Layer
Normalization} (similar to Pre-Activation ResNet), or replacing Layer
Normalization by other techniques
(\href{https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf}{Adaptive
Normalization}, \href{https://arxiv.org/abs/2003.07845}{Power
Normalization}).

Nevertheless, many applications and papers still use the original
Transformer architecture with Adam, because warm-up is a simple, yet
effective way of solving the gradient problem in the first iterations.
There are many different schedulers we could use. For instance, the
original Transformer paper used an exponential decay scheduler with a
warm-up. However, the currently most popular scheduler is the cosine
warm-up scheduler, which combines warm-up with a cosine-shaped learning
rate decay. We can implement it below, and visualize the learning rate
factor over epochs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ CosineWarmupScheduler(optim.lr\_scheduler.\_LRScheduler):}
    
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, optimizer, warmup, max\_iters):}
        \VariableTok{self}\NormalTok{.warmup }\OperatorTok{=}\NormalTok{ warmup}
        \VariableTok{self}\NormalTok{.max\_num\_iters }\OperatorTok{=}\NormalTok{ max\_iters}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{(optimizer)}
        
    \KeywordTok{def}\NormalTok{ get\_lr(}\VariableTok{self}\NormalTok{):}
\NormalTok{        lr\_factor }\OperatorTok{=} \VariableTok{self}\NormalTok{.get\_lr\_factor(epoch}\OperatorTok{=}\VariableTok{self}\NormalTok{.last\_epoch)}
        \ControlFlowTok{return}\NormalTok{ [base\_lr }\OperatorTok{*}\NormalTok{ lr\_factor }\ControlFlowTok{for}\NormalTok{ base\_lr }\KeywordTok{in} \VariableTok{self}\NormalTok{.base\_lrs]}
    
    \KeywordTok{def}\NormalTok{ get\_lr\_factor(}\VariableTok{self}\NormalTok{, epoch):}
\NormalTok{        lr\_factor }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ np.cos(np.pi }\OperatorTok{*}\NormalTok{ epoch }\OperatorTok{/} \VariableTok{self}\NormalTok{.max\_num\_iters))}
        \ControlFlowTok{if}\NormalTok{ epoch }\OperatorTok{\textless{}=} \VariableTok{self}\NormalTok{.warmup:}
\NormalTok{            lr\_factor }\OperatorTok{*=}\NormalTok{ epoch }\OperatorTok{*} \FloatTok{1.0} \OperatorTok{/} \VariableTok{self}\NormalTok{.warmup}
        \ControlFlowTok{return}\NormalTok{ lr\_factor}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Needed for initializing the lr scheduler}
\NormalTok{p }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.empty(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.Adam([p], lr}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{)}
\NormalTok{lr\_scheduler }\OperatorTok{=}\NormalTok{ CosineWarmupScheduler(optimizer}\OperatorTok{=}\NormalTok{optimizer, warmup}\OperatorTok{=}\DecValTok{100}\NormalTok{, max\_iters}\OperatorTok{=}\DecValTok{2000}\NormalTok{)}

\CommentTok{\# Plotting}
\NormalTok{epochs }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\DecValTok{2000}\NormalTok{))}
\NormalTok{sns.}\BuiltInTok{set}\NormalTok{()}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{plt.plot(epochs, [lr\_scheduler.get\_lr\_factor(e) }\ControlFlowTok{for}\NormalTok{ e }\KeywordTok{in}\NormalTok{ epochs])}
\NormalTok{plt.ylabel(}\StringTok{"Learning rate factor"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Iterations (in batches)"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Cosine Warm{-}up Learning Rate Scheduler"}\NormalTok{)}
\NormalTok{plt.show()}
\NormalTok{sns.reset\_orig()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{603_spot_lightning_transformer_introduction_files/figure-pdf/cell-18-output-1.pdf}}

In the first 100 iterations, we increase the learning rate factor from 0
to 1, whereas for all later iterations, we decay it using the cosine
wave. Pre-implementations of this scheduler can be found in the popular
NLP Transformer library
\href{https://huggingface.co/transformers/main_classes/optimizer_schedules.html?highlight=cosine\#transformers.get_cosine_schedule_with_warmup}{huggingface}.

\subsection{PyTorch Lightning Module}\label{pytorch-lightning-module}

Finally, we can embed the Transformer architecture into a PyTorch
lightning module. PyTorch Lightning simplifies our training and test
code, as well as structures the code nicely in separate functions. We
will implement a template for a classifier based on the Transformer
encoder. Thereby, we have a prediction output per sequence element. If
we would need a classifier over the whole sequence, the common approach
is to add an additional \texttt{{[}CLS{]}} token to the sequence
(\texttt{CLS} stands for classification, i.e., the first token of every
sequence is always a special classification token, \texttt{CLS}).
However, here we focus on tasks where we have an output per element.

Additionally to the Transformer architecture, we add a small input
network (maps input dimensions to model dimensions), the positional
encoding, and an output network (transforms output encodings to
predictions). We also add the learning rate scheduler, which takes a
step each iteration instead of once per epoch. This is needed for the
warmup and the smooth cosine decay. The training, validation, and test
step is left empty for now and will be filled for our task-specific
models.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ TransformerPredictor(pl.LightningModule):}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_dim, model\_dim, num\_classes, num\_heads, num\_layers, lr, warmup, max\_iters, dropout}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, input\_dropout}\OperatorTok{=}\FloatTok{0.0}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Inputs:}
\CommentTok{            input\_dim {-} Hidden dimensionality of the input}
\CommentTok{            model\_dim {-} Hidden dimensionality to use inside the Transformer}
\CommentTok{            num\_classes {-} Number of classes to predict per sequence element}
\CommentTok{            num\_heads {-} Number of heads to use in the Multi{-}Head Attention blocks}
\CommentTok{            num\_layers {-} Number of encoder blocks to use.}
\CommentTok{            lr {-} Learning rate in the optimizer}
\CommentTok{            warmup {-} Number of warmup steps. Usually between 50 and 500}
\CommentTok{            max\_iters {-} Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler}
\CommentTok{            dropout {-} Dropout to apply inside the model}
\CommentTok{            input\_dropout {-} Dropout to apply on the input features}
\CommentTok{        """}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.save\_hyperparameters()}
        \VariableTok{self}\NormalTok{.\_create\_model()}

    \KeywordTok{def}\NormalTok{ \_create\_model(}\VariableTok{self}\NormalTok{):}
        \CommentTok{\# Input dim {-}\textgreater{} Model dim}
        \VariableTok{self}\NormalTok{.input\_net }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.input\_dropout),}
\NormalTok{            nn.Linear(}\VariableTok{self}\NormalTok{.hparams.input\_dim, }\VariableTok{self}\NormalTok{.hparams.model\_dim)}
\NormalTok{        )}
        \CommentTok{\# Positional encoding for sequences}
        \VariableTok{self}\NormalTok{.positional\_encoding }\OperatorTok{=}\NormalTok{ PositionalEncoding(d\_model}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.model\_dim)}
        \CommentTok{\# Transformer}
        \VariableTok{self}\NormalTok{.transformer }\OperatorTok{=}\NormalTok{ TransformerEncoder(num\_layers}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.num\_layers,}
\NormalTok{                                              input\_dim}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.model\_dim,}
\NormalTok{                                              dim\_feedforward}\OperatorTok{=}\DecValTok{2}\OperatorTok{*}\VariableTok{self}\NormalTok{.hparams.model\_dim,}
\NormalTok{                                              num\_heads}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.num\_heads,}
\NormalTok{                                              dropout}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.dropout)}
        \CommentTok{\# Output classifier per sequence lement}
        \VariableTok{self}\NormalTok{.output\_net }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{            nn.Linear(}\VariableTok{self}\NormalTok{.hparams.model\_dim, }\VariableTok{self}\NormalTok{.hparams.model\_dim),}
\NormalTok{            nn.LayerNorm(}\VariableTok{self}\NormalTok{.hparams.model\_dim),}
\NormalTok{            nn.ReLU(inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{),}
\NormalTok{            nn.Dropout(}\VariableTok{self}\NormalTok{.hparams.dropout),}
\NormalTok{            nn.Linear(}\VariableTok{self}\NormalTok{.hparams.model\_dim, }\VariableTok{self}\NormalTok{.hparams.num\_classes)}
\NormalTok{        ) }

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x, mask}\OperatorTok{=}\VariableTok{None}\NormalTok{, add\_positional\_encoding}\OperatorTok{=}\VariableTok{True}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Inputs:}
\CommentTok{            x {-} Input features of shape [Batch, SeqLen, input\_dim]}
\CommentTok{            mask {-} Mask to apply on the attention outputs (optional)}
\CommentTok{            add\_positional\_encoding {-} If True, we add the positional encoding to the input.}
\CommentTok{                                      Might not be desired for some tasks.}
\CommentTok{        """}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.input\_net(x)}
        \ControlFlowTok{if}\NormalTok{ add\_positional\_encoding:}
\NormalTok{            x }\OperatorTok{=} \VariableTok{self}\NormalTok{.positional\_encoding(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.transformer(x, mask}\OperatorTok{=}\NormalTok{mask)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.output\_net(x)}
        \ControlFlowTok{return}\NormalTok{ x}

    \AttributeTok{@torch.no\_grad}\NormalTok{()}
    \KeywordTok{def}\NormalTok{ get\_attention\_maps(}\VariableTok{self}\NormalTok{, x, mask}\OperatorTok{=}\VariableTok{None}\NormalTok{, add\_positional\_encoding}\OperatorTok{=}\VariableTok{True}\NormalTok{):}
        \CommentTok{"""}
\CommentTok{        Function for extracting the attention matrices of the whole Transformer for a single batch.}
\CommentTok{        Input arguments same as the forward pass.}
\CommentTok{        """}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.input\_net(x)}
        \ControlFlowTok{if}\NormalTok{ add\_positional\_encoding:}
\NormalTok{            x }\OperatorTok{=} \VariableTok{self}\NormalTok{.positional\_encoding(x)}
\NormalTok{        attention\_maps }\OperatorTok{=} \VariableTok{self}\NormalTok{.transformer.get\_attention\_maps(x, mask}\OperatorTok{=}\NormalTok{mask)}
        \ControlFlowTok{return}\NormalTok{ attention\_maps}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{):}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(}\VariableTok{self}\NormalTok{.parameters(), lr}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.lr)}
        
        \CommentTok{\# Apply lr scheduler per step}
\NormalTok{        lr\_scheduler }\OperatorTok{=}\NormalTok{ CosineWarmupScheduler(optimizer, }
\NormalTok{                                             warmup}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.warmup, }
\NormalTok{                                             max\_iters}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.max\_iters)}
        \ControlFlowTok{return}\NormalTok{ [optimizer], [\{}\StringTok{\textquotesingle{}scheduler\textquotesingle{}}\NormalTok{: lr\_scheduler, }\StringTok{\textquotesingle{}interval\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}step\textquotesingle{}}\NormalTok{\}]}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
        \ControlFlowTok{raise} \PreprocessorTok{NotImplementedError}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
        \ControlFlowTok{raise} \PreprocessorTok{NotImplementedError}    

    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
        \ControlFlowTok{raise} \PreprocessorTok{NotImplementedError}   
\end{Highlighting}
\end{Shaded}

\section{Experiment: Sequence to
Sequence}\label{experiment-sequence-to-sequence}

After having finished the implementation of the Transformer
architecture, we can start experimenting and apply it to various tasks.
We will focus on parallel Sequence-to-Sequence.

A Sequence-to-Sequence task represents a task where the input \emph{and}
the output is a sequence, not necessarily of the same length. Popular
tasks in this domain include machine translation and summarization. For
this, we usually have a Transformer encoder for interpreting the input
sequence, and a decoder for generating the output in an autoregressive
manner. Here, however, we will go back to a much simpler example task
and use only the encoder. Given a sequence of \(N\) numbers between
\(0\) and \(M\), the task is to reverse the input sequence. In Numpy
notation, if our input is \(x\), the output should be \(x\){[}::-1{]}.
Although this task sounds very simple, RNNs can have issues with such
because the task requires long-term dependencies. Transformers are built
to support such, and hence, we expect it to perform very well.

\subsection{Dataset and Data Loaders}\label{dataset-and-data-loaders}

First, let's create a dataset class below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ ReverseDataset(data.Dataset):}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, num\_categories, seq\_len, size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.num\_categories }\OperatorTok{=}\NormalTok{ num\_categories}
        \VariableTok{self}\NormalTok{.seq\_len }\OperatorTok{=}\NormalTok{ seq\_len}
        \VariableTok{self}\NormalTok{.size }\OperatorTok{=}\NormalTok{ size}
        
        \VariableTok{self}\NormalTok{.data }\OperatorTok{=}\NormalTok{ torch.randint(}\VariableTok{self}\NormalTok{.num\_categories, size}\OperatorTok{=}\NormalTok{(}\VariableTok{self}\NormalTok{.size, }\VariableTok{self}\NormalTok{.seq\_len))}
  
    \KeywordTok{def} \FunctionTok{\_\_len\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.size}

    \KeywordTok{def} \FunctionTok{\_\_getitem\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, idx):}
\NormalTok{        inp\_data }\OperatorTok{=} \VariableTok{self}\NormalTok{.data[idx]}
\NormalTok{        labels }\OperatorTok{=}\NormalTok{ torch.flip(inp\_data, dims}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,))}
        \ControlFlowTok{return}\NormalTok{ inp\_data, labels}
\end{Highlighting}
\end{Shaded}

We create an arbitrary number of random sequences of numbers between 0
and \texttt{num\_categories-1}. The label is simply the tensor flipped
over the sequence dimension. We can create the corresponding data
loaders below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ partial(ReverseDataset, }\DecValTok{10}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{train\_loader }\OperatorTok{=}\NormalTok{ data.DataLoader(dataset(}\DecValTok{50000}\NormalTok{),}
\NormalTok{                                batch\_size}\OperatorTok{=}\DecValTok{128}\NormalTok{,}
\NormalTok{                                shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                                drop\_last}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                                pin\_memory}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{val\_loader   }\OperatorTok{=}\NormalTok{ data.DataLoader(dataset(}\DecValTok{1000}\NormalTok{), batch\_size}\OperatorTok{=}\DecValTok{128}\NormalTok{)}
\NormalTok{test\_loader  }\OperatorTok{=}\NormalTok{ data.DataLoader(dataset(}\DecValTok{10000}\NormalTok{), batch\_size}\OperatorTok{=}\DecValTok{128}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inp\_data, labels }\OperatorTok{=}\NormalTok{ train\_loader.dataset[}\DecValTok{0}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Input data:"}\NormalTok{, inp\_data)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Labels:    "}\NormalTok{, labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Input data: tensor([0, 4, 1, 2, 5, 5, 7, 6, 9, 6, 3, 1, 9, 3, 1, 9])
Labels:     tensor([9, 1, 3, 9, 1, 3, 6, 9, 6, 7, 5, 5, 2, 1, 4, 0])
\end{verbatim}

During training, we pass the input sequence through the Transformer
encoder and predict the output for each input token. We use the standard
Cross-Entropy loss to perform this. Every number is represented as a
one-hot vector. Remember that representing the categories as single
scalars decreases the expressiveness of the model extremely as \(0\) and
\(1\) are not closer related than \(0\) and \(9\) in our example. An
alternative to a one-hot vector is using a learned embedding vector as
it is provided by the PyTorch module \texttt{nn.Embedding}. However,
using a one-hot vector with an additional linear layer as in our case
has the same effect as an embedding layer (\texttt{self.input\_net} maps
one-hot vector to a dense vector, where each row of the weight matrix
represents the embedding for a specific category).

\subsection{The Reverse Predictor
Class}\label{the-reverse-predictor-class}

To implement the training dynamic, we create a new class inheriting from
\texttt{TransformerPredictor} and overwriting the training, validation
and test step functions, which were left empty in the base class. We
also add a \texttt{\_calculate\_loss} function to calculate the loss and
accuracy for a batch.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ ReversePredictor(TransformerPredictor):}
    
    \KeywordTok{def}\NormalTok{ \_calculate\_loss(}\VariableTok{self}\NormalTok{, batch, mode}\OperatorTok{=}\StringTok{"train"}\NormalTok{):}
        \CommentTok{\# Fetch data and transform categories to one{-}hot vectors}
\NormalTok{        inp\_data, labels }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        inp\_data }\OperatorTok{=}\NormalTok{ F.one\_hot(inp\_data, num\_classes}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.num\_classes).}\BuiltInTok{float}\NormalTok{()}
        
        \CommentTok{\# Perform prediction and calculate loss and accuracy}
\NormalTok{        preds }\OperatorTok{=} \VariableTok{self}\NormalTok{.forward(inp\_data, add\_positional\_encoding}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ F.cross\_entropy(preds.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,preds.size(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)), labels.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{        acc }\OperatorTok{=}\NormalTok{ (preds.argmax(dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{) }\OperatorTok{==}\NormalTok{ labels).}\BuiltInTok{float}\NormalTok{().mean()}
        
        \CommentTok{\# Logging}
        \VariableTok{self}\NormalTok{.log(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{mode}\SpecialCharTok{\}}\SpecialStringTok{\_loss"}\NormalTok{, loss)}
        \VariableTok{self}\NormalTok{.log(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{mode}\SpecialCharTok{\}}\SpecialStringTok{\_acc"}\NormalTok{, acc)}
        \ControlFlowTok{return}\NormalTok{ loss, acc}
        
    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        loss, \_ }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch, mode}\OperatorTok{=}\StringTok{"train"}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ loss}
    
    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        \_ }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch, mode}\OperatorTok{=}\StringTok{"val"}\NormalTok{)}
    
    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch, batch\_idx):}
\NormalTok{        \_ }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch, mode}\OperatorTok{=}\StringTok{"test"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, we can create a training function. We create a
\texttt{pl.Trainer} object, running for \(N\) epochs, logging in
TensorBoard, and saving our best model based on the validation.
Afterward, we test our models on the test set.

\subsection{Gradient Clipping}\label{gradient-clipping}

An additional parameter we pass to the trainer here is
\texttt{gradient\_clip\_val}. This clips the norm of the gradients for
all parameters before taking an optimizer step and prevents the model
from diverging if we obtain very high gradients at, for instance, sharp
loss surfaces (see many good blog posts on gradient clipping, like
\href{https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping}{DeepAI
glossary}). For Transformers, gradient clipping can help to further
stabilize the training during the first few iterations, and also
afterward. In plain PyTorch, you can apply gradient clipping via
\texttt{torch.nn.utils.clip\_grad\_norm\_(...)} (see
\href{https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\#torch.nn.utils.clip_grad_norm_}{documentation}).
The clip value is usually between 0.5 and 10, depending on how harsh you
want to clip large gradients.

\subsection{Implementation of the Lightning
Trainer}\label{implementation-of-the-lightning-trainer}

The Lightning trainer can be implemented as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ train\_reverse(}\OperatorTok{**}\NormalTok{kwargs):}
    \CommentTok{\# Create a PyTorch Lightning trainer with the generation callback}
\NormalTok{    root\_dir }\OperatorTok{=}\NormalTok{ os.path.join(CHECKPOINT\_PATH, }\StringTok{"ReverseTask"}\NormalTok{)}
\NormalTok{    os.makedirs(root\_dir, exist\_ok}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    trainer }\OperatorTok{=}\NormalTok{ pl.Trainer(default\_root\_dir}\OperatorTok{=}\NormalTok{root\_dir, }
\NormalTok{                         callbacks}\OperatorTok{=}\NormalTok{[ModelCheckpoint(save\_weights\_only}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                                    mode}\OperatorTok{=}\StringTok{"max"}\NormalTok{, monitor}\OperatorTok{=}\StringTok{"val\_acc"}\NormalTok{)],}
\NormalTok{                         accelerator}\OperatorTok{=}\StringTok{"gpu"} \ControlFlowTok{if} \BuiltInTok{str}\NormalTok{(device).startswith(}\StringTok{"cuda"}\NormalTok{) }\ControlFlowTok{else} \StringTok{"cpu"}\NormalTok{,}
\NormalTok{                         devices}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                         max\_epochs}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{                         gradient\_clip\_val}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{    trainer.logger.\_default\_hp\_metric }\OperatorTok{=} \VariableTok{None} \CommentTok{\# Optional logging argument that we don\textquotesingle{}t need}
    
    \CommentTok{\# Check whether pretrained model exists. If yes, load it and skip training}
\NormalTok{    pretrained\_filename }\OperatorTok{=}\NormalTok{ os.path.join(CHECKPOINT\_PATH, }\StringTok{"ReverseTask.ckpt"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ os.path.isfile(pretrained\_filename):}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"Found pretrained model, loading..."}\NormalTok{)}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ ReversePredictor.load\_from\_checkpoint(pretrained\_filename)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ ReversePredictor(max\_iters}\OperatorTok{=}\NormalTok{trainer.max\_epochs}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(train\_loader), }\OperatorTok{**}\NormalTok{kwargs)}
\NormalTok{        trainer.fit(model, train\_loader, val\_loader)}
        
    \CommentTok{\# Test best model on validation and test set}
\NormalTok{    val\_result }\OperatorTok{=}\NormalTok{ trainer.test(model, val\_loader, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{    test\_result }\OperatorTok{=}\NormalTok{ trainer.test(model, test\_loader, verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ \{}\StringTok{"test\_acc"}\NormalTok{: test\_result[}\DecValTok{0}\NormalTok{][}\StringTok{"test\_acc"}\NormalTok{], }\StringTok{"val\_acc"}\NormalTok{: val\_result[}\DecValTok{0}\NormalTok{][}\StringTok{"test\_acc"}\NormalTok{]\}}
    
\NormalTok{    model }\OperatorTok{=}\NormalTok{ model.to(device)}
    \ControlFlowTok{return}\NormalTok{ model, result}
\end{Highlighting}
\end{Shaded}

\subsection{Training the Model}\label{training-the-model}

Finally, we can train the model. In this setup, we will use a single
encoder block and a single head in the Multi-Head Attention. This is
chosen because of the simplicity of the task, and in this case, the
attention can actually be interpreted as an ``explanation'' of the
predictions (compared to the other papers above dealing with deep
Transformers).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reverse\_model, reverse\_result }\OperatorTok{=}\NormalTok{ train\_reverse(input\_dim}\OperatorTok{=}\NormalTok{train\_loader.dataset.num\_categories,}
\NormalTok{                                              model\_dim}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{                                              num\_heads}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                              num\_classes}\OperatorTok{=}\NormalTok{train\_loader.dataset.num\_categories,}
\NormalTok{                                              num\_layers}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                                              dropout}\OperatorTok{=}\FloatTok{0.0}\NormalTok{,}
\NormalTok{                                              lr}\OperatorTok{=}\FloatTok{5e{-}4}\NormalTok{,}
\NormalTok{                                              warmup}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Found pretrained model, loading...
\end{verbatim}

\begin{verbatim}
Testing: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

\begin{verbatim}
Testing: |          | 0/? [00:00<?, ?it/s]
\end{verbatim}

The warning of PyTorch Lightning regarding the number of workers can be
ignored for now. As the data set is so simple and the
\texttt{\_\_getitem\_\_} finishes a neglectable time, we don't need
subprocesses to provide us the data (in fact, more workers can slow down
the training as we have communication overhead among processes/threads).
First, let's print the results:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Val accuracy:  }\SpecialCharTok{\{}\NormalTok{(}\FloatTok{100.0} \OperatorTok{*}\NormalTok{ reverse\_result[}\StringTok{\textquotesingle{}val\_acc\textquotesingle{}}\NormalTok{])}\SpecialCharTok{:4.2f\}}\SpecialStringTok{\%"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test accuracy: }\SpecialCharTok{\{}\NormalTok{(}\FloatTok{100.0} \OperatorTok{*}\NormalTok{ reverse\_result[}\StringTok{\textquotesingle{}test\_acc\textquotesingle{}}\NormalTok{])}\SpecialCharTok{:4.2f\}}\SpecialStringTok{\%"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Val accuracy:  100.00%
Test accuracy: 100.00%
\end{verbatim}

As we would have expected, the Transformer can correctly solve the task.

\section{Visualizing Attention Maps}\label{visualizing-attention-maps}

How does the attention in the Multi-Head Attention block looks like for
an arbitrary input? Let's try to visualize it below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_input, labels }\OperatorTok{=} \BuiltInTok{next}\NormalTok{(}\BuiltInTok{iter}\NormalTok{(val\_loader))}
\NormalTok{inp\_data }\OperatorTok{=}\NormalTok{ F.one\_hot(data\_input, num\_classes}\OperatorTok{=}\NormalTok{reverse\_model.hparams.num\_classes).}\BuiltInTok{float}\NormalTok{()}
\NormalTok{inp\_data }\OperatorTok{=}\NormalTok{ inp\_data.to(device)}
\NormalTok{attention\_maps }\OperatorTok{=}\NormalTok{ reverse\_model.get\_attention\_maps(inp\_data)}
\end{Highlighting}
\end{Shaded}

The object \texttt{attention\_maps} is a list of length \(N\) where
\(N\) is the number of layers. Each element is a tensor of shape
{[}Batch, Heads, SeqLen, SeqLen{]}, which we can verify below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attention\_maps[}\DecValTok{0}\NormalTok{].shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
torch.Size([128, 1, 16, 16])
\end{verbatim}

Next, we will write a plotting function that takes as input the
sequences, attention maps, and an index indicating for which batch
element we want to visualize the attention map. We will create a plot
where over rows, we have different layers, while over columns, we show
the different heads. Remember that the softmax has been applied for each
row separately.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_attention\_maps(input\_data, attn\_maps, idx}\OperatorTok{=}\DecValTok{0}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ input\_data }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        input\_data }\OperatorTok{=}\NormalTok{ input\_data[idx].detach().cpu().numpy()}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        input\_data }\OperatorTok{=}\NormalTok{ np.arange(attn\_maps[}\DecValTok{0}\NormalTok{][idx].shape[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{    attn\_maps }\OperatorTok{=}\NormalTok{ [m[idx].detach().cpu().numpy() }\ControlFlowTok{for}\NormalTok{ m }\KeywordTok{in}\NormalTok{ attn\_maps]}
    
\NormalTok{    num\_heads }\OperatorTok{=}\NormalTok{ attn\_maps[}\DecValTok{0}\NormalTok{].shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    num\_layers }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(attn\_maps)}
\NormalTok{    seq\_len }\OperatorTok{=}\NormalTok{ input\_data.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    fig\_size }\OperatorTok{=} \DecValTok{4} \ControlFlowTok{if}\NormalTok{ num\_heads }\OperatorTok{==} \DecValTok{1} \ControlFlowTok{else} \DecValTok{3}
\NormalTok{    fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(num\_layers, num\_heads, figsize}\OperatorTok{=}\NormalTok{(num\_heads}\OperatorTok{*}\NormalTok{fig\_size, num\_layers}\OperatorTok{*}\NormalTok{fig\_size))}
    \ControlFlowTok{if}\NormalTok{ num\_layers }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
\NormalTok{        ax }\OperatorTok{=}\NormalTok{ [ax]}
    \ControlFlowTok{if}\NormalTok{ num\_heads }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
\NormalTok{        ax }\OperatorTok{=}\NormalTok{ [[a] }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ ax]}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_layers):}
        \ControlFlowTok{for}\NormalTok{ column }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_heads):}
\NormalTok{            ax[row][column].imshow(attn\_maps[row][column], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{            ax[row][column].set\_xticks(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(seq\_len)))}
\NormalTok{            ax[row][column].set\_xticklabels(input\_data.tolist())}
\NormalTok{            ax[row][column].set\_yticks(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(seq\_len)))}
\NormalTok{            ax[row][column].set\_yticklabels(input\_data.tolist())}
\NormalTok{            ax[row][column].set\_title(}\SpecialStringTok{f"Layer }\SpecialCharTok{\{}\NormalTok{row}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{, Head }\SpecialCharTok{\{}\NormalTok{column}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    fig.subplots\_adjust(hspace}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    cax }\OperatorTok{=}\NormalTok{ fig.add\_axes([}\FloatTok{0.95}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.7}\NormalTok{])}
\NormalTok{    cbar }\OperatorTok{=}\NormalTok{ fig.colorbar(ax[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{].imshow(attn\_maps[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{0}\NormalTok{), cax}\OperatorTok{=}\NormalTok{cax)}
\NormalTok{    cbar.set\_label(}\StringTok{\textquotesingle{}Attention\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.show()}
\end{Highlighting}
\end{Shaded}

Finally, we can plot the attention map of our trained Transformer on the
reverse task:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_attention\_maps(data\_input, attention\_maps, idx}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{603_spot_lightning_transformer_introduction_files/figure-pdf/cell-30-output-1.pdf}}

The model has learned to attend to the token that is on the flipped
index of itself. Hence, it actually does what we intended it to do. We
see that it however also pays some attention to values close to the
flipped index. This is because the model doesn't need the perfect, hard
attention to solve this problem, but is fine with this approximate,
noisy attention map. The close-by indices are caused by the similarity
of the positional encoding, which we also intended with the positional
encoding.

\section{Conclusion}\label{conclusion-1}

In this chapter, we took a closer look at the Multi-Head Attention layer
which uses a scaled dot product between queries and keys to find
correlations and similarities between input elements. The Transformer
architecture is based on the Multi-Head Attention layer and applies
multiple of them in a ResNet-like block. The Transformer is a very
important, recent architecture that can be applied to many tasks and
datasets. Although it is best known for its success in NLP, there is so
much more to it. We have seen its application on sequence-to-sequence
tasks. Its property of being permutation-equivariant if we do not
provide any positional encodings, allows it to generalize to many
settings. Hence, it is important to know the architecture, but also its
possible issues such as the gradient problem during the first iterations
solved by learning rate warm-up. If you are interested in continuing
with the study of the Transformer architecture, please have a look at
the blog posts listed in the ``Further Reading'' section below.

\section{Additional Considerations}\label{additional-considerations}

\subsection{Complexity and Path
Length}\label{complexity-and-path-length}

We can compare the self-attention operation with our other common layer
competitors for sequence data: convolutions and recurrent neural
networks. In Figure~\ref{fig-comparison-conv-rnn} you can find a table
by Vaswani et al. (2017) on the complexity per layer, the number of
sequential operations, and maximum path length. The complexity is
measured by the upper bound of the number of operations to perform,
while the maximum path length represents the maximum number of steps a
forward or backward signal has to traverse to reach any other position.
The lower this length, the better gradient signals can backpropagate for
long-range dependencies. Let's take a look at the table in
Figure~\ref{fig-comparison-conv-rnn}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{./figures_static/comparison_conv_rnn.png}}

}

\caption{\label{fig-comparison-conv-rnn}Comparison of complexity and
path length of different sequence layers. Table taken from Lippe (2022)}

\end{figure}%

\(n\) is the sequence length, \(d\) is the representation dimension and
\(k\) is the kernel size of convolutions. In contrast to recurrent
networks, the self-attention layer can parallelize all its operations
making it much faster to execute for smaller sequence lengths. However,
when the sequence length exceeds the hidden dimensionality,
self-attention becomes more expensive than RNNs. One way of reducing the
computational cost for long sequences is by restricting the
self-attention to a neighborhood of inputs to attend over, denoted by
\(r\). Nevertheless, there has been recently a lot of work on more
efficient Transformer architectures that still allow long dependencies,
of which you can find an overview in the paper by Tay et al. (2020) if
interested.

\section{Further Reading}\label{further-reading-1}

There are of course many more tutorials out there about attention and
Transformers. Below, we list a few that are worth exploring if you are
interested in the topic and might want yet another perspective on the
topic after this one:

\begin{itemize}
\tightlist
\item
  \href{https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html}{Transformer:
  A Novel Neural Network Architecture for Language Understanding (Jakob
  Uszkoreit, 2017)} - The original Google blog post about the
  Transformer paper, focusing on the application in machine translation.
\item
  \href{http://jalammar.github.io/illustrated-transformer/}{The
  Illustrated Transformer (Jay Alammar, 2018)} - A very popular and
  great blog post intuitively explaining the Transformer architecture
  with many nice visualizations. The focus is on NLP.
\item
  \href{https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html}{Attention?
  Attention! (Lilian Weng, 2018)} - A nice blog post summarizing
  attention mechanisms in many domains including vision.
\item
  \href{https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a}{Illustrated:
  Self-Attention (Raimi Karim, 2019)} - A nice visualization of the
  steps of self-attention. Recommended going through if the explanation
  below is too abstract for you.
\item
  \href{https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html}{The
  Transformer family (Lilian Weng, 2020)} - A very detailed blog post
  reviewing more variants of Transformers besides the original one.
\end{itemize}

\chapter{Hyperparameter Tuning of a Transformer Network with PyTorch
Lightning}\label{sec-hyperparameter-tuning-with-pytorch-lightning-603}

\section{Basic Setup}\label{sec-basic-setup-603}

This section provides an overview of the hyperparameter tuning process
using \texttt{spotpython} and \texttt{PyTorch} Lightning. It uses the
\texttt{Diabetes} data set (see
Section~\ref{sec-a-05-diabetes-data-set}) for a regression task.

In this section, we will show how \texttt{spotpython} can be integrated
into the \texttt{PyTorch} Lightning training workflow for a regression
task. It demonstrates how easy it is to use \texttt{spotpython} to tune
hyperparameters for a \texttt{PyTorch} Lightning model.

After importing the necessary libraries, the \texttt{fun\_control}
dictionary is set up via the \texttt{fun\_control\_init} function. The
\texttt{fun\_control} dictionary contains

\begin{itemize}
\tightlist
\item
  \texttt{PREFIX}: a unique identifier for the experiment
\item
  \texttt{fun\_evals}: the number of function evaluations
\item
  \texttt{max\_time}: the maximum run time in minutes
\item
  \texttt{data\_set}: the data set. Here we use the \texttt{Diabetes}
  data set that is provided by \texttt{spotpython}.
\item
  \texttt{core\_model\_name}: the class name of the neural network
  model. This neural network model is provided by \texttt{spotpython}.
\item
  \texttt{hyperdict}: the hyperparameter dictionary. This dictionary is
  used to define the hyperparameters of the neural network model. It is
  also provided by \texttt{spotpython}.
\item
  \texttt{\_L\_in}: the number of input features. Since the
  \texttt{Diabetes} data set has 10 features, \texttt{\_L\_in} is set to
  10.
\item
  \texttt{\_L\_out}: the number of output features. Since we want to
  predict a single value, \texttt{\_L\_out} is set to 1.
\end{itemize}

The method \texttt{set\_hyperparameter} allows the user to modify
default hyperparameter settings. Here we set the \texttt{initialization}
method to \texttt{{[}"Default"{]}}. No other initializations are used in
this experiment. The \texttt{HyperLight} class is used to define the
objective function \texttt{fun}. It connects the \texttt{PyTorch} and
the \texttt{spotpython} methods and is provided by \texttt{spotpython}.
Finally, a \texttt{Spot} object is created.

\phantomsection\label{setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}
\ImportTok{from}\NormalTok{ spotpython.utils.scaler }\ImportTok{import}\NormalTok{ TorchStandardScaler}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\StringTok{"603"}\NormalTok{,}
\NormalTok{    TENSORBOARD\_CLEAN}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ Diabetes(),}
\NormalTok{    scaler}\OperatorTok{=}\NormalTok{TorchStandardScaler(),}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNTransformerRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [}
                \StringTok{"Adadelta"}\NormalTok{,}
                \StringTok{"Adagrad"}\NormalTok{,}
                \StringTok{"Adam"}\NormalTok{,}
                \StringTok{"AdamW"}\NormalTok{,}
                \StringTok{"Adamax"}\NormalTok{,}
\NormalTok{            ])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"nhead"}\NormalTok{, [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dim\_feedforward\_mult"}\NormalTok{, [}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(}
\NormalTok{    method}\OperatorTok{=}\StringTok{"regression"}\NormalTok{,}
\NormalTok{    min\_Lambda}\OperatorTok{=}\FloatTok{1e{-}3}\NormalTok{,}
\NormalTok{    max\_Lambda}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}

\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control, surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_05_06_10_57_42_0
Created spot_tensorboard_path: runs/spot_logs/603_maans08_2025-05-06_10-57-42 for SummaryWriter()
module_name: light
submodule_name: regression
model_name: NNTransformerRegressor
\end{verbatim}

We can take a look at the design table to see the initial design.

\phantomsection\label{design_table}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name                 | type   | default        |   lower |   upper | transform             |
|----------------------|--------|----------------|---------|---------|-----------------------|
| d_model_mult         | int    | 4              |    1    |     5   | transform_power_2_int |
| nhead                | int    | 3              |    1    |     2   | transform_power_2_int |
| num_encoder_layers   | int    | 1              |    1    |     4   | transform_power_2_int |
| dim_feedforward_mult | int    | 1              |    1    |     1   | transform_power_2_int |
| epochs               | int    | 7              |    5    |     7   | transform_power_2_int |
| batch_size           | int    | 5              |    5    |     8   | transform_power_2_int |
| optimizer            | factor | Adam           |    0    |     4   | None                  |
| dropout              | float  | 0.1            |    0.01 |     0.1 | None                  |
| lr_mult              | float  | 0.1            |    0.01 |     0.3 | None                  |
| patience             | int    | 5              |    4    |     7   | transform_power_2_int |
| initialization       | factor | xavier_uniform |    0    |     3   | None                  |
\end{verbatim}

Calling the method \texttt{run()} starts the hyperparameter tuning
process on the local machine.

\phantomsection\label{run}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
d_model: 8, dim_feedforward: 16
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23955.66015625, 'hp_metric': 23955.66015625}
d_model: 128, dim_feedforward: 256
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20858.498046875, 'hp_metric': 20858.498046875}
d_model: 32, dim_feedforward: 64
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23231.283203125, 'hp_metric': 23231.283203125}
d_model: 16, dim_feedforward: 32
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23903.546875, 'hp_metric': 23903.546875}
d_model: 8, dim_feedforward: 16
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23954.796875, 'hp_metric': 23954.796875}
d_model: 128, dim_feedforward: 256
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20722.6171875, 'hp_metric': 20722.6171875}
spotpython tuning: 20722.6171875 [###-------] 27.61% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 19918.572265625, 'hp_metric': 19918.572265625}
spotpython tuning: 19918.572265625 [####------] 44.06% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 19282.2734375, 'hp_metric': 19282.2734375}
spotpython tuning: 19282.2734375 [######----] 60.65% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5284.65283203125, 'hp_metric': 5284.65283203125}
spotpython tuning: 5284.65283203125 [#######---] 67.08% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
train_model result: {'val_loss': 5471.34375, 'hp_metric': 5471.34375}
spotpython tuning: 5284.65283203125 [#######---] 72.98% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
train_model result: {'val_loss': 5555.70068359375, 'hp_metric': 5555.70068359375}
spotpython tuning: 5284.65283203125 [########--] 79.26% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
train_model result: {'val_loss': 5511.9443359375, 'hp_metric': 5511.9443359375}
spotpython tuning: 5284.65283203125 [#########-] 91.65% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
train_model result: {'val_loss': 5484.96533203125, 'hp_metric': 5484.96533203125}
spotpython tuning: 5284.65283203125 [##########] 98.40% 
\end{verbatim}

\begin{verbatim}
d_model: 128, dim_feedforward: 256
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 19271.712890625, 'hp_metric': 19271.712890625}
spotpython tuning: 5284.65283203125 [##########] 100.00% Done...

Experiment saved to 603_res.pkl
\end{verbatim}

Note that we have enabled Tensorboard-Logging, so we can visualize the
results with Tensorboard. Execute the following command in the terminal
to start Tensorboard.

\phantomsection\label{tensorboard}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard }\OperatorTok{{-}{-}}\NormalTok{logdir}\OperatorTok{=}\StringTok{"runs/"}
\end{Highlighting}
\end{Shaded}

\section{Looking at the Results}\label{looking-at-the-results-2}

\subsection{Tuning Progress}\label{tuning-progress-2}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{603_spot_lightning_transformer_hpt_files/figure-pdf/plot_progress-output-1.pdf}}

\subsection{Tuned Hyperparameters and Their
Importance}\label{tuned-hyperparameters-and-their-importance-2}

Results can be printed in tabular form.

\phantomsection\label{gen_design_table_results}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name                 | type   | default        |   lower |   upper | tuned                | transform             |   importance | stars   |
|----------------------|--------|----------------|---------|---------|----------------------|-----------------------|--------------|---------|
| d_model_mult         | int    | 4              |     1.0 |     5.0 | 5.0                  | transform_power_2_int |         0.08 |         |
| nhead                | int    | 3              |     1.0 |     2.0 | 2.0                  | transform_power_2_int |         0.08 |         |
| num_encoder_layers   | int    | 1              |     1.0 |     4.0 | 1.0                  | transform_power_2_int |         0.08 |         |
| dim_feedforward_mult | int    | 1              |     1.0 |     1.0 | 1.0                  | transform_power_2_int |         0.00 |         |
| epochs               | int    | 7              |     5.0 |     7.0 | 7.0                  | transform_power_2_int |         0.08 |         |
| batch_size           | int    | 5              |     5.0 |     8.0 | 5.0                  | transform_power_2_int |         0.23 | .       |
| optimizer            | factor | Adam           |     0.0 |     4.0 | Adadelta             | None                  |        67.00 | **      |
| dropout              | float  | 0.1            |    0.01 |     0.1 | 0.010474648409984919 | None                  |         0.08 |         |
| lr_mult              | float  | 0.1            |    0.01 |     0.3 | 0.3                  | None                  |       100.00 | ***     |
| patience             | int    | 5              |     4.0 |     7.0 | 4.0                  | transform_power_2_int |         0.08 |         |
| initialization       | factor | xavier_uniform |     0.0 |     3.0 | xavier_normal        | None                  |         0.43 | .       |
\end{verbatim}

\section{Hyperparameter
Considerations}\label{hyperparameter-considerations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{d\_model} (or \texttt{d\_embedding}):

  \begin{itemize}
  \tightlist
  \item
    This is the dimension of the embedding space or the number of
    expected features in the input.
  \item
    All input features are projected into this dimensional space before
    entering the transformer encoder.
  \item
    This dimension must be divisible by \texttt{nhead} since each head
    in the multi-head attention mechanism will process a subset of
    \texttt{d\_model/nhead} features.
  \end{itemize}
\item
  \texttt{nhead}:

  \begin{itemize}
  \tightlist
  \item
    This is the number of attention heads in the multi-head attention
    mechanism.
  \item
    It allows the transformer to jointly attend to information from
    different representation subspaces.
  \item
    It's important that \texttt{d\_model\ \%\ nhead\ ==\ 0} to ensure
    the dimensions are evenly split among the heads.
  \end{itemize}
\item
  \texttt{num\_encoder\_layers}:

  \begin{itemize}
  \tightlist
  \item
    This specifies the number of transformer encoder layers stacked
    together.
  \item
    Each layer contains a multi-head attention mechanism followed by
    position-wise feedforward layers.
  \end{itemize}
\item
  \texttt{dim\_feedforward}:

  \begin{itemize}
  \tightlist
  \item
    This is the dimension of the feedforward network model within the
    transformer encoder layer.
  \item
    Typically, this dimension is larger than \texttt{d\_model} (e.g.,
    2048 for a Transformer model with \texttt{d\_model=512}).
  \end{itemize}
\end{enumerate}

\subsection{Important: Constraints and
Interconnections:}\label{important-constraints-and-interconnections}

\begin{itemize}
\tightlist
\item
  \texttt{d\_model} and \texttt{nhead}:

  \begin{itemize}
  \tightlist
  \item
    As mentioned, \texttt{d\_model} must be divisible by \texttt{nhead}.
    This is critical because each attention head operates simultaneously
    on a part of the embedding, so \texttt{d\_model/nhead} should be an
    integer.
  \end{itemize}
\item
  \texttt{num\_encoder\_layers} and \texttt{dim\_feedforward}**:

  \begin{itemize}
  \tightlist
  \item
    These parameters are more flexible and can be chosen independently
    of \texttt{d\_model} and \texttt{nhead}.
  \item
    However, the choice of \texttt{dim\_feedforward} does influence the
    computational cost and model capacity, as larger dimensions allow
    learning more complex representations.
  \end{itemize}
\item
  One hyperparameter does not strictly need to be a multiple of others
  except for ensuring \texttt{d\_model\ \%\ nhead\ ==\ 0}.
\end{itemize}

\subsection{Practical Considerations:}\label{practical-considerations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Setting \texttt{d\_model}:

  \begin{itemize}
  \tightlist
  \item
    Common choices for \texttt{d\_model} are powers of 2 (e.g., 256,
    512, 1024).
  \item
    Ensure that it matches the size of the input data after the linear
    projection layer.
  \end{itemize}
\item
  Setting \texttt{nhead}:

  \begin{itemize}
  \tightlist
  \item
    Typically, values are 1, 2, 4, 8, etc., depending on the
    \texttt{d\_model} value.
  \item
    Each head works on a subset of features, so
    \texttt{d\_model\ /\ nhead} should be large enough to be meaningful.
  \end{itemize}
\item
  Setting \texttt{num\_encoder\_layers}:

  \begin{itemize}
  \tightlist
  \item
    Practical values range from 1 to 12 or more depending on the depth
    desired.
  \item
    Deeper models can capture more complex patterns but are also more
    computationally intensive.
  \end{itemize}
\item
  Setting \texttt{dim\_feedforward}:

  \begin{itemize}
  \tightlist
  \item
    Often set to a multiple of \texttt{d\_model}, such as 2048 when
    \texttt{d\_model} is 512.
  \item
    Ensures sufficient capacity in the intermediate layers for complex
    feature transformations.
  \end{itemize}
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: \texttt{d\_model} Calculation}]

Since \texttt{d\_model\ \%\ nhead\ ==\ 0} is a critical constraint to
ensure that the multi-head attention mechanism can operate effectively,
\texttt{spotpython} computes the value of \texttt{d\_model} based on the
\texttt{nhead} value provided by the user. This ensures that the
hyperparameter configuration is valid. So, the final value of
\texttt{d\_model} is a multiple of \texttt{nhead}. \texttt{spotpython}
uses the hyperparameter \texttt{d\_model\_mult} to determine the
multiple of \texttt{nhead} to use for \texttt{d\_model}, i.e.,
\texttt{d\_model\ =\ nhead\ *\ d\_model\_mult}.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note: \texttt{dim\_feedforward} Calculation}]

Since this dimension is typically larger than \texttt{d\_model} (e.g.,
2048 for a Transformer model with \texttt{d\_model=512}),
\texttt{spotpython} uses the hyperparameter
\texttt{dim\_feedforward\_mult} to determine the multiple of
\texttt{d\_model} to use for \texttt{dim\_feedforward}, i.e.,
\texttt{dim\_feedforward\ =\ d\_model\ *\ dim\_feedforward\_mult}.

\end{tcolorbox}

\section{Summary}\label{summary-3}

This section presented an introduction to the basic setup of
hyperparameter tuning of a transformer with \texttt{spotpython} and
\texttt{PyTorch} Lightning.

\chapter{Saving and Loading}\label{saving-and-loading}

This tutorial shows how to save and load objects in \texttt{spotpython}.
It is split into the following parts:

\begin{itemize}
\tightlist
\item
  Section~\ref{sec-spotpython-saving-and-loading} shows how to save and
  load objects in \texttt{spotpython}, if \texttt{spotpython} is used as
  an optimizer.
\item
  Section~\ref{sec-spotpython-as-a-hyperparameter-tuner-604} shows how
  to save and load hyperparameter tuning experiments.
\item
  Section~\ref{sec-saving-and-loading-pytorch-lightning-models-604}
  shows how to save and load \texttt{PyTorch\ Lightning} models.
\item
  Section~\ref{sec-converting-a-lightning-model-to-a-plain-torch-model-604}
  shows how to convert a \texttt{PyTorch\ Lightning} model to a plain
  \texttt{PyTorch} model.
\end{itemize}

\section{spotpython: Saving and Loading Optimization
Experiments}\label{sec-spotpython-saving-and-loading}

In this section, we will show how results from \texttt{spotpython} can
be saved and reloaded. Here, \texttt{spotpython} can be used as an
optimizer. If \texttt{spotpython} is used as an optimizer, no dictionary
of hyperparameters has be specified. The \texttt{fun\_control}
dictionary is sufficient.

\phantomsection\label{code-optimization-experiment-604}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{import}\NormalTok{ pprint}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_experiment}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (}
\NormalTok{    fun\_control\_init,}
\NormalTok{    design\_control\_init,}
\NormalTok{    surrogate\_control\_init,}
\NormalTok{    optimizer\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{            PREFIX}\OperatorTok{=}\StringTok{"branin"}\NormalTok{,            }
\NormalTok{            lower }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]),}
\NormalTok{            upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{]),}
\NormalTok{            fun\_evals}\OperatorTok{=}\DecValTok{8}\NormalTok{,}
\NormalTok{            fun\_repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{            noise}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{            tolerance\_x}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{            ocba\_delta}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{            var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{],}
\NormalTok{            infill\_criterion}\OperatorTok{=}\StringTok{"ei"}\NormalTok{,}
\NormalTok{            n\_points}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{            log\_level}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{            show\_models}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{            show\_progress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(}
\NormalTok{            init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{            repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(}
\NormalTok{            model\_fun\_evals}\OperatorTok{=}\DecValTok{10000}\NormalTok{,}
\NormalTok{            min\_theta}\OperatorTok{={-}}\DecValTok{3}\NormalTok{,}
\NormalTok{            max\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{            n\_theta}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{            theta\_init\_zero}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{            n\_p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            optim\_p}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{            var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"num"}\NormalTok{],}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{124}\NormalTok{)}
\NormalTok{optimizer\_control }\OperatorTok{=}\NormalTok{ optimizer\_control\_init(}
\NormalTok{            max\_iter}\OperatorTok{=}\DecValTok{1000}\NormalTok{,}
\NormalTok{            seed}\OperatorTok{=}\DecValTok{125}\NormalTok{)}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{            fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{            design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{            surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control,}
\NormalTok{            optimizer\_control}\OperatorTok{=}\NormalTok{optimizer\_control)}
\NormalTok{spot\_tuner.run()}
\NormalTok{PREFIX }\OperatorTok{=}\NormalTok{ fun\_control[}\StringTok{"PREFIX"}\NormalTok{]}
\NormalTok{filename }\OperatorTok{=}\NormalTok{ get\_experiment\_filename(PREFIX)}
\NormalTok{spot\_tuner.save\_experiment(filename}\OperatorTok{=}\NormalTok{filename)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"filename: }\SpecialCharTok{\{}\NormalTok{filename}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{code-reload-optimization-experiment-604}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(spot\_tuner\_1, fun\_control\_1, design\_control\_1,}
\NormalTok{    surrogate\_control\_1, optimizer\_control\_1) }\OperatorTok{=}\NormalTok{ load\_experiment(filename)}
\end{Highlighting}
\end{Shaded}

The progress of the original experiment is shown in
Figure~\ref{fig-plot-progress-604a} and the reloaded experiment in
Figure~\ref{fig-plot-progress-604b}.

\begin{figure}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\caption{\label{fig-plot-progress-604a}}

\end{figure}%

\begin{figure}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner\_1.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\caption{\label{fig-plot-progress-604b}}

\end{figure}%

The results from the original experiment are shown in
Table~\ref{tbl-results-604a} and the reloaded experiment in
Table~\ref{tbl-results-604b}.

\begin{table}

\caption{\label{tbl-results-604a}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.print\_results()}
\end{Highlighting}
\end{Shaded}

}

\end{table}%

\begin{table}

\caption{\label{tbl-results-604b}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner\_1.print\_results()}
\end{Highlighting}
\end{Shaded}

}

\end{table}%

\subsection{Getting the Tuned
Hyperparameters}\label{getting-the-tuned-hyperparameters}

The tuned hyperparameters can be obtained as a dictionary with the
following code. Since \texttt{spotpython} is used as an optimizer, the
numerical levels of the hyperparameters are identical to the optimized
values of the underlying optimization problem, here: the Branin
function.

\phantomsection\label{code-get-tuned-optimization-604}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_hyperparameters}
\NormalTok{get\_tuned\_hyperparameters(spot\_tuner}\OperatorTok{=}\NormalTok{spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Saving and Loading Optimization Experiments}]

\begin{itemize}
\tightlist
\item
  If \texttt{spotpython} is used as an optimizer (without an
  hyperparameter dictionary), experiments can be saved and reloaded with
  the \texttt{save\_experiment} and \texttt{load\_experiment} functions.
\item
  The tuned hyperparameters can be obtained with the
  \texttt{get\_tuned\_hyperparameters} function.
\end{itemize}

\end{tcolorbox}

\section{spotpython as a Hyperparameter
Tuner}\label{sec-spotpython-as-a-hyperparameter-tuner-604}

If \texttt{spotpython} is used as a hyperparameter tuner, in addition to
the \texttt{fun\_control} dictionary a \texttt{core\_model} dictionary
has to be specified. Furthermore, a data set has to be selected and
added to the \texttt{fun\_control} dictionary. Here, we will use the
\texttt{Diabetes} data set.

\subsection{The Diabetes Data Set}\label{the-diabetes-data-set}

The hyperparameter tuning of a \texttt{PyTorch\ Lightning} network on
the \texttt{Diabetes} data set is used as an example. The
\texttt{Diabetes} data set is a PyTorch Dataset for regression, which
originates from the \texttt{scikit-learn} package, see
\url{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html\#sklearn.datasets.load_diabetes}.

Ten baseline variables, age, sex, body mass index, average blood
pressure, and six blood serum measurements were obtained for each of n =
442 diabetes patients, as well as the response of interest, a
quantitative measure of disease progression one year after baseline. The
\texttt{Diabetes} data set is has the following properties:

\begin{itemize}
\tightlist
\item
  Samples total: 442
\item
  Dimensionality: 10
\item
  Features: real, \(-.2 < x < .2\)
\item
  Targets: integer \(25 - 346\)
\end{itemize}

\phantomsection\label{code-diabetes-data-set-604}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{code-hyperparameter-tuning-604}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"604"}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}

\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

In contrast to the default setttin, where \texttt{sava\_experiment} is
set to \texttt{False}, here the \texttt{fun\_control} dictionary is
initialized \texttt{save\_experiment=True}. Alternatively, an existing
\texttt{fun\_control} dictionary can be updated with
\texttt{\{"save\_experiment":\ True\}} as shown in the following code.

\phantomsection\label{save_experiment}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control.update(\{}\StringTok{"save\_experiment"}\NormalTok{: }\VariableTok{True}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

If \texttt{save\_experiment} is set to \texttt{True}, the results of the
hyperparameter tuning experiment are stored in a pickle file with the
name \texttt{PREFIX} after the tuning is finished in the current
directory.

Alternatively, the spot object and the corresponding dictionaries can be
saved with the \texttt{save\_experiment} method, which is part of the
\texttt{spot} object. Therefore, the \texttt{spot} object has to be
created as shown in the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{spot\_tuner.save\_experiment(path}\OperatorTok{=}\StringTok{"userExperiment"}\NormalTok{, overwrite}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, we have added a \texttt{path} argument to specify the directory
where the experiment is saved. The resulting pickle file can be copied
to another directory or computer and reloaded with the
\texttt{load\_experiment} function. It can also be used for performing
the tuning run. Here, we will execute the tuning run on the local
machine, which can be done with the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

After the tuning run is finished, a pickle file with the name
\texttt{spot\_604\_experiment.pickle} is stored in the local directory.
This is a result of setting the \texttt{save\_experiment} argument to
\texttt{True} in the \texttt{fun\_control} dictionary. We can load the
experiment with the following code. Here, we have specified the
\texttt{PREFIX} as an argument to the \texttt{load\_experiment}
function. Alternatively, the filename (\texttt{filename}) can be used as
an argument.

\phantomsection\label{code-reload-hyper-experiment-37}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_experiment}
\NormalTok{(spot\_tuner\_1, fun\_control\_1, design\_control\_1,}
\NormalTok{    surrogate\_control\_1, optimizer\_control\_1) }\OperatorTok{=}\NormalTok{ load\_experiment(PREFIX}\OperatorTok{=}\NormalTok{PREFIX)}
\end{Highlighting}
\end{Shaded}

For comparison, the tuned hyperparameters of the original experiment are
shown first:

\phantomsection\label{code-get-tuned-hyperparameters-fun-ctrl604a}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_tuned\_hyperparameters(spot\_tuner, fun\_control)}
\end{Highlighting}
\end{Shaded}

Second, the tuned hyperparameters of the reloaded experiment are shown:

\phantomsection\label{code-get-tuned-hyperparameters-fun-ctrl604b}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_tuned\_hyperparameters(spot\_tuner\_1, fun\_control\_1)}
\end{Highlighting}
\end{Shaded}

Note: The numerical levels of the hyperparameters are used as keys in
the dictionary. If the \texttt{fun\_control} dictionary is used, the
names of the hyperparameters are used as keys in the dictionary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_tuned\_hyperparameters(spot\_tuner\_1, fun\_control\_1)}
\end{Highlighting}
\end{Shaded}

Plot the progress of the original experiment are identical to the
reloaded experiment.

\begin{figure}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

}

\caption{\label{fig-plot-progress-604aa}}

\end{figure}%

\begin{figure}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner\_1.plot\_progress()}
\end{Highlighting}
\end{Shaded}

}

\caption{\label{fig-plot-progress-604bb}}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Summary: Saving and Loading Hyperparameter-Tuning Experiments}]

\begin{itemize}
\tightlist
\item
  If \texttt{spotpython} is used as an hyperparameter tuner (with an
  hyperparameter dictionary), experiments can be saved and reloaded with
  the \texttt{save\_experiment} and \texttt{load\_experiment} functions.
\item
  The tuned hyperparameters can be obtained with the
  \texttt{get\_tuned\_hyperparameters} function.
\end{itemize}

\end{tcolorbox}

\section{Saving and Loading PyTorch Lightning
Models}\label{sec-saving-and-loading-pytorch-lightning-models-604}

Section~\ref{sec-spotpython-saving-and-loading} and
Section~\ref{sec-spotpython-as-a-hyperparameter-tuner-604} explained how
to save and load optimization and hyperparameter tuning experiments and
how to get the tuned hyperparameters as a dictionary. This section shows
how to save and load \texttt{PyTorch\ Lightning} models.

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-604}

In contrast to the function \texttt{get\_tuned\_hyperparameters}, the
function \texttt{get\_tuned\_architecture} returns the tuned
architecture of the model as a dictionary. Here, the transformations are
already applied to the numerical levels of the hyperparameters and the
encoding (and types) are the original types of the hyperparameters used
by the model. Important: The \texttt{config} dictionary from
\texttt{get\_tuned\_architecture} can be passed to the model without any
modifications.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

After getting the tuned architecture, the model can be created and
tested with the following code.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.light.testmodel }\ImportTok{import}\NormalTok{ test\_model}
\NormalTok{test\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\subsection{Load a Model from
Checkpoint}\label{load-a-model-from-checkpoint}

The method \texttt{load\_light\_from\_checkpoint} loads a model from a
checkpoint file. Important: The model has to be trained before the
checkpoint is loaded. As shown here, loading a model with trained
weights is possible, but requires two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The model weights have to be learned using \texttt{test\_model}. The
  \texttt{test\_model} method writes a checkpoint file.
\item
  The model has to be loaded from the checkpoint file.
\end{enumerate}

\subsubsection{\texorpdfstring{Details About the
\texttt{load\_light\_from\_checkpoint}
Method}{Details About the load\_light\_from\_checkpoint Method}}\label{details-about-the-load_light_from_checkpoint-method}

\begin{itemize}
\tightlist
\item
  The \texttt{test\_model} method saves the last checkpoint to a file
  using the following code:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ModelCheckpoint(}
\NormalTok{    dirpath}\OperatorTok{=}\NormalTok{os.path.join(fun\_control[}\StringTok{"CHECKPOINT\_PATH"}\NormalTok{], config\_id), save\_last}\OperatorTok{=}\VariableTok{True}
\NormalTok{), }
\end{Highlighting}
\end{Shaded}

The filename of the last checkpoint has a specific structure:

\begin{itemize}
\tightlist
\item
  A \texttt{config\_id} is generated from the \texttt{config}
  dictionary. It does not use a timestamp. This differs from the config
  id generated in cvmodel.py and trainmodel.py, which provide time
  information for the TensorBoard logging.
\item
  Furthermore, the postfix \texttt{\_TEST} is added to the
  \texttt{config\_id} to indicate that the model is tested.
\item
  For example:
  \texttt{runs/saved\_models/16\_16\_64\_LeakyReLU\_Adadelta\_0.0014\_8.5895\_8\_False\_kaiming\_uniform\_TEST/last.ckpt}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.light.loadmodel }\ImportTok{import}\NormalTok{ load\_light\_from\_checkpoint}
\NormalTok{model\_loaded }\OperatorTok{=}\NormalTok{ load\_light\_from\_checkpoint(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{vars}\NormalTok{(model\_loaded)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\NormalTok{torch.save(model\_loaded, }\StringTok{"model.pt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mymodel }\OperatorTok{=}\NormalTok{ torch.load(}\StringTok{"model.pt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# show all attributes of the model}
\BuiltInTok{vars}\NormalTok{(mymodel)}
\end{Highlighting}
\end{Shaded}

\section{Converting a Lightning Model to a Plain Torch
Model}\label{sec-converting-a-lightning-model-to-a-plain-torch-model-604}

\subsection{\texorpdfstring{The Function
\texttt{get\_removed\_attributes\_and\_base\_net}}{The Function get\_removed\_attributes\_and\_base\_net}}\label{the-function-get_removed_attributes_and_base_net}

\texttt{spotpython} provides a function to covert a
\texttt{PyTorch\ Lightning} model to a plain \texttt{PyTorch} model. The
function \texttt{get\_removed\_attributes\_and\_base\_net} returns a
tuple with the removed attributes and the base net. The base net is a
plain \texttt{PyTorch} model. The removed attributes are the attributes
of the \texttt{PyTorch\ Lightning} model that are not part of the base
net.

This conversion can be reverted.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ spotpython.utils.device }\ImportTok{import}\NormalTok{ getDevice}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ random\_split}
\ImportTok{from}\NormalTok{ spotpython.utils.classes }\ImportTok{import}\NormalTok{ get\_removed\_attributes\_and\_base\_net}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.optimizer }\ImportTok{import}\NormalTok{ optimizer\_handler}
\NormalTok{removed\_attributes, torch\_net }\OperatorTok{=}\NormalTok{ get\_removed\_attributes\_and\_base\_net(net}\OperatorTok{=}\NormalTok{mymodel)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(removed\_attributes)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(torch\_net)}
\end{Highlighting}
\end{Shaded}

\subsection{An Example how to use the Plain Torch
Net}\label{an-example-how-to-use-the-plain-torch-net}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader, TensorDataset}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_diabetes}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Load the Diabetes dataset from sklearn}
\NormalTok{diabetes }\OperatorTok{=}\NormalTok{ load\_diabetes()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ diabetes.data}
\NormalTok{y }\OperatorTok{=}\NormalTok{ diabetes.target}

\CommentTok{\# Split the dataset into training and testing sets}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Scale the features}
\NormalTok{scaler }\OperatorTok{=}\NormalTok{ StandardScaler()}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ scaler.fit\_transform(X\_train)}
\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ scaler.transform(X\_test)}

\CommentTok{\# Convert the data to PyTorch tensors}
\NormalTok{X\_train\_tensor }\OperatorTok{=}\NormalTok{ torch.tensor(X\_train, dtype}\OperatorTok{=}\NormalTok{torch.float32)}
\NormalTok{y\_train\_tensor }\OperatorTok{=}\NormalTok{ torch.tensor(y\_train, dtype}\OperatorTok{=}\NormalTok{torch.float32)}
\NormalTok{X\_test\_tensor }\OperatorTok{=}\NormalTok{ torch.tensor(X\_test, dtype}\OperatorTok{=}\NormalTok{torch.float32)}
\NormalTok{y\_test\_tensor }\OperatorTok{=}\NormalTok{ torch.tensor(y\_test, dtype}\OperatorTok{=}\NormalTok{torch.float32)}

\CommentTok{\# Create a PyTorch dataset}
\NormalTok{train\_dataset }\OperatorTok{=}\NormalTok{ TensorDataset(X\_train\_tensor, y\_train\_tensor)}
\NormalTok{test\_dataset }\OperatorTok{=}\NormalTok{ TensorDataset(X\_test\_tensor, y\_test\_tensor)}

\CommentTok{\# Create a PyTorch dataloader}
\NormalTok{batch\_size }\OperatorTok{=} \DecValTok{32}
\NormalTok{train\_dataloader }\OperatorTok{=}\NormalTok{ DataLoader(train\_dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{test\_dataloader }\OperatorTok{=}\NormalTok{ DataLoader(test\_dataset, batch\_size}\OperatorTok{=}\NormalTok{batch\_size)}

\NormalTok{torch\_net.to(getDevice(}\StringTok{"cpu"}\NormalTok{))}

\CommentTok{\# train the net}
\NormalTok{criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(torch\_net.parameters(), lr}\OperatorTok{=}\FloatTok{0.01}\NormalTok{)}
\NormalTok{n\_epochs }\OperatorTok{=} \DecValTok{100}
\NormalTok{losses }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_epochs):}
    \ControlFlowTok{for}\NormalTok{ inputs, targets }\KeywordTok{in}\NormalTok{ train\_dataloader:}
\NormalTok{        targets }\OperatorTok{=}\NormalTok{ targets.view(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{        optimizer.zero\_grad()}
\NormalTok{        outputs }\OperatorTok{=}\NormalTok{ torch\_net(inputs)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ criterion(outputs, targets)}
\NormalTok{        losses.append(loss.item())}
\NormalTok{        loss.backward()}
\NormalTok{        optimizer.step()}
\CommentTok{\# visualize the network training}
\NormalTok{plt.plot(losses)}
\NormalTok{plt.xlabel(}\StringTok{"Epoch"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Loss"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\chapter{\texorpdfstring{Hyperparameter Tuning with \texttt{spotpython}
and \texttt{PyTorch} Lightning for the Diabetes Data Set Using a ResNet
Model}{Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set Using a ResNet Model}}\label{hyperparameter-tuning-with-spotpython-and-pytorch-lightning-for-the-diabetes-data-set-using-a-resnet-model}

In this section, we will show how \texttt{spotpython} can be integrated
into the \texttt{PyTorch} Lightning training workflow for a regression
task. It demonstrates how easy it is to use \texttt{spotpython} to tune
hyperparameters for a \texttt{PyTorch} Lightning model.

After importing the necessary libraries, the \texttt{fun\_control}
dictionary is set up via the \texttt{fun\_control\_init} function. The
\texttt{fun\_control} dictionary contains

\begin{itemize}
\tightlist
\item
  \texttt{PREFIX}: a unique identifier for the experiment
\item
  \texttt{fun\_evals}: the number of function evaluations
\item
  \texttt{max\_time}: the maximum run time in minutes
\item
  \texttt{data\_set}: the data set. Here we use the \texttt{Diabetes}
  data set that is provided by \texttt{spotpython}.
\item
  \texttt{core\_model\_name}: the class name of the neural network
  model. This neural network model is provided by \texttt{spotpython}.
\item
  \texttt{hyperdict}: the hyperparameter dictionary. This dictionary is
  used to define the hyperparameters of the neural network model. It is
  also provided by \texttt{spotpython}.
\item
  \texttt{\_L\_in}: the number of input features. Since the
  \texttt{Diabetes} data set has 10 features, \texttt{\_L\_in} is set to
  10.
\item
  \texttt{\_L\_out}: the number of output features. Since we want to
  predict a single value, \texttt{\_L\_out} is set to 1.
\end{itemize}

The \texttt{HyperLight} class is used to define the objective function
\texttt{fun}. It connects the \texttt{PyTorch} and the
\texttt{spotpython} methods and is provided by \texttt{spotpython}.

\phantomsection\label{spotpython_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"605"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNResNetRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNResNetRegressor
\end{verbatim}

The method \texttt{set\_hyperparameter} allows the user to modify
default hyperparameter settings. Here we modify some hyperparameters to
keep the model small and to decrease the tuning time.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"lr\_mult"}\NormalTok{, [}\FloatTok{0.1}\NormalTok{, }\FloatTok{20.0}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |     3   |   4     | transform_power_2_int |
| epochs         | int    | 4         |     3   |   7     | transform_power_2_int |
| batch_size     | int    | 4         |     4   |  11     | transform_power_2_int |
| act_fn         | factor | ReLU      |     0   |   5     | None                  |
| optimizer      | factor | SGD       |     0   |   2     | None                  |
| dropout_prob   | float  | 0.01      |     0   |   0.025 | None                  |
| lr_mult        | float  | 1.0       |     0.1 |  20     | None                  |
| patience       | int    | 2         |     2   |   3     | transform_power_2_int |
| initialization | factor | Default   |     0   |   4     | None                  |
\end{verbatim}

Finally, a \texttt{Spot} object is created. Calling the method
\texttt{run()} starts the hyperparameter tuning process.

\phantomsection\label{run}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': 24041.166015625, 'hp_metric': 24041.166015625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22516.333984375, 'hp_metric': 22516.333984375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23957.216796875, 'hp_metric': 23957.216796875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23526.126953125, 'hp_metric': 23526.126953125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23435.740234375, 'hp_metric': 23435.740234375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5741.01416015625, 'hp_metric': 5741.01416015625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23971.83984375, 'hp_metric': 23971.83984375}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 24112.75, 'hp_metric': 24112.75}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20110.208984375, 'hp_metric': 20110.208984375}
train_model result: {'val_loss': 23546.658203125, 'hp_metric': 23546.658203125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4534.15283203125, 'hp_metric': 4534.15283203125}
spotpython tuning: 4534.15283203125 [#---------] 5.99% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3945.150390625, 'hp_metric': 3945.150390625}
spotpython tuning: 3945.150390625 [#---------] 14.42% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23405.11328125, 'hp_metric': 23405.11328125}
spotpython tuning: 3945.150390625 [###-------] 29.74% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23806.390625, 'hp_metric': 23806.390625}
spotpython tuning: 3945.150390625 [###-------] 33.37% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 19730.400390625, 'hp_metric': 19730.400390625}
spotpython tuning: 3945.150390625 [####------] 44.44% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4454.765625, 'hp_metric': 4454.765625}
spotpython tuning: 3945.150390625 [#####-----] 50.78% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4234.16943359375, 'hp_metric': 4234.16943359375}
spotpython tuning: 3945.150390625 [######----] 56.58% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5304.9833984375, 'hp_metric': 5304.9833984375}
spotpython tuning: 3945.150390625 [######----] 62.68% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22116.203125, 'hp_metric': 22116.203125}
spotpython tuning: 3945.150390625 [#######---] 73.75% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4638.31298828125, 'hp_metric': 4638.31298828125}
spotpython tuning: 3945.150390625 [########--] 80.31% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23442.00390625, 'hp_metric': 23442.00390625}
spotpython tuning: 3945.150390625 [##########] 100.00% Done...

Experiment saved to 605_res.pkl
\end{verbatim}

\section{Looking at the Results}\label{looking-at-the-results-3}

\subsection{Tuning Progress}\label{tuning-progress-3}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{605_spot_hpt_light_diabetes_resnet_files/figure-pdf/cell-6-output-1.pdf}}

\subsection{Tuned Hyperparameters and Their
Importance}\label{tuned-hyperparameters-and-their-importance-3}

Results can be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned                | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|----------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     4.0 | 3.0                  | transform_power_2_int |        54.95 | **      |
| epochs         | int    | 4         |     3.0 |     7.0 | 7.0                  | transform_power_2_int |         0.79 | .       |
| batch_size     | int    | 4         |     4.0 |    11.0 | 11.0                 | transform_power_2_int |         0.37 | .       |
| act_fn         | factor | ReLU      |     0.0 |     5.0 | ReLU                 | None                  |         0.01 |         |
| optimizer      | factor | SGD       |     0.0 |     2.0 | Adadelta             | None                  |        70.96 | **      |
| dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.018944610786155315 | None                  |         0.10 |         |
| lr_mult        | float  | 1.0       |     0.1 |    20.0 | 17.787675379409905   | None                  |         8.45 | *       |
| patience       | int    | 2         |     2.0 |     3.0 | 3.0                  | transform_power_2_int |         4.41 | *       |
| initialization | factor | Default   |     0.0 |     4.0 | kaiming_uniform      | None                  |       100.00 | ***     |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{605_spot_hpt_light_diabetes_resnet_files/figure-pdf/cell-8-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
l1:  54.948989972832315
epochs:  0.7910336032578257
batch_size:  0.37023065044288384
act_fn:  0.007559924566997962
optimizer:  70.95638647025385
dropout_prob:  0.09584198169452227
lr_mult:  8.44754813284851
patience:  4.406014002087552
initialization:  100.0
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{605_spot_hpt_light_diabetes_resnet_files/figure-pdf/cell-9-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{605_spot_hpt_light_diabetes_resnet_files/figure-pdf/cell-9-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{605_spot_hpt_light_diabetes_resnet_files/figure-pdf/cell-9-output-4.pdf}}

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-31}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'act_fn': ReLU(),
 'batch_size': 2048,
 'dropout_prob': 0.018944610786155315,
 'epochs': 128,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': 17.787675379409905,
 'optimizer': 'Adadelta',
 'patience': 8}
\end{verbatim}

\subsection{Test on the full data
set}\label{test-on-the-full-data-set-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set the value of the key "TENSORBOARD\_CLEAN" to True in the fun\_control dictionary and use the update() method to update the fun\_control dictionary}
\ImportTok{import}\NormalTok{ os}
\CommentTok{\# if the directory "./runs" exists, delete it}
\ControlFlowTok{if}\NormalTok{ os.path.exists(}\StringTok{"./runs"}\NormalTok{):}
\NormalTok{    os.system(}\StringTok{"rm {-}r ./runs"}\NormalTok{)}
\NormalTok{fun\_control.update(\{}\StringTok{"tensorboard\_log"}\NormalTok{: }\VariableTok{True}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.light.testmodel }\ImportTok{import}\NormalTok{ test\_model}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ get\_feature\_names}

\NormalTok{test\_model(config, fun\_control)}
\NormalTok{get\_feature\_names(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
âââââââââââââââââââââââââââââ³ââââââââââââââââââââââââââââ
â        Test metric        â       DataLoader 0        â
â¡ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©
â         hp_metric         â     2644.842529296875     â
â         val_loss          â     2644.842529296875     â
âââââââââââââââââââââââââââââ´ââââââââââââââââââââââââââââ
\end{verbatim}

\begin{verbatim}
test_model result: {'val_loss': 2644.842529296875, 'hp_metric': 2644.842529296875}
\end{verbatim}

\begin{verbatim}
['age',
 'sex',
 'bmi',
 'bp',
 's1_tc',
 's2_ldl',
 's3_hdl',
 's4_tch',
 's5_ltg',
 's6_glu']
\end{verbatim}

\subsection{Cross Validation With
Lightning}\label{cross-validation-with-lightning-1}

\begin{itemize}
\tightlist
\item
  The \texttt{KFold} class from \texttt{sklearn.model\_selection} is
  used to generate the folds for cross-validation.
\item
  These mechanism is used to generate the folds for the final evaluation
  of the model.
\item
  The \texttt{CrossValidationDataModule} class
  \href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/data/lightcrossvalidationdatamodule.py}{{[}SOURCE{]}}
  is used to generate the folds for the hyperparameter tuning process.
\item
  It is called from the \texttt{cv\_model} function
  \href{https://github.com/sequential-parameter-optimization/spotpython/blob/main/src/spotpython/light/cvmodel.py}{{[}SOURCE{]}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{config}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'l1': 8,
 'epochs': 128,
 'batch_size': 2048,
 'act_fn': ReLU(),
 'optimizer': 'Adadelta',
 'dropout_prob': 0.018944610786155315,
 'lr_mult': 17.787675379409905,
 'patience': 8,
 'initialization': 'kaiming_uniform'}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.light.cvmodel }\ImportTok{import}\NormalTok{ cv\_model}
\NormalTok{fun\_control.update(\{}\StringTok{"k\_folds"}\NormalTok{: }\DecValTok{2}\NormalTok{\})}
\NormalTok{fun\_control.update(\{}\StringTok{"test\_size"}\NormalTok{: }\FloatTok{0.6}\NormalTok{\})}
\NormalTok{cv\_model(config, fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
k: 0
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 6829.279296875, 'hp_metric': 6829.279296875}
k: 1
train_model result: {'val_loss': 18799.26171875, 'hp_metric': 18799.26171875}
\end{verbatim}

\begin{verbatim}
12814.2705078125
\end{verbatim}

\section{Summary}\label{summary-4}

This section presented an introduction to the basic setup of
hyperparameter tuning with \texttt{spotpython} and \texttt{PyTorch}
Lightning using a ResNet model for the Diabetes data set.

\chapter{\texorpdfstring{Hyperparameter Tuning with \texttt{spotpython}
and \texttt{PyTorch} Lightning for the Diabetes Data Set Using a User
Specified ResNet
Model}{Hyperparameter Tuning with spotpython and PyTorch Lightning for the Diabetes Data Set Using a User Specified ResNet Model}}\label{hyperparameter-tuning-with-spotpython-and-pytorch-lightning-for-the-diabetes-data-set-using-a-user-specified-resnet-model}

After importing the necessary libraries, the \texttt{fun\_control}
dictionary is set up via the \texttt{fun\_control\_init} function. The
\texttt{fun\_control} dictionary contains

\begin{itemize}
\tightlist
\item
  \texttt{PREFIX}: a unique identifier for the experiment
\item
  \texttt{fun\_evals}: the number of function evaluations
\item
  \texttt{max\_time}: the maximum run time in minutes
\item
  \texttt{data\_set}: the data set. Here we use the \texttt{Diabetes}
  data set that is provided by \texttt{spotpython}.
\item
  \texttt{core\_model\_name}: the class name of the neural network
  model. This neural network model is provided by \texttt{spotpython}.
\item
  \texttt{hyperdict}: the hyperparameter dictionary. This dictionary is
  used to define the hyperparameters of the neural network model. It is
  also provided by \texttt{spotpython}.
\item
  \texttt{\_L\_in}: the number of input features. Since the
  \texttt{Diabetes} data set has 10 features, \texttt{\_L\_in} is set to
  10.
\item
  \texttt{\_L\_out}: the number of output features. Since we want to
  predict a single value, \texttt{\_L\_out} is set to 1.
\end{itemize}

The \texttt{HyperLight} class is used to define the objective function
\texttt{fun}. It connects the \texttt{PyTorch} and the
\texttt{spotpython} methods and is provided by \texttt{spotpython}.

To access the user specified ResNet model, the path to the user model
must be added to the Python path:

\phantomsection\label{user-user-path_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\NormalTok{sys.path.insert(}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}./userModel\textquotesingle{}}\NormalTok{)}
\ImportTok{import}\NormalTok{ my\_resnet}
\ImportTok{import}\NormalTok{ my\_hyper\_dict}
\end{Highlighting}
\end{Shaded}

In the following code, we do not specify the ResNet model in the
\texttt{fun\_control} dictionary. It will be added in a second step as
the user specified model.

\phantomsection\label{user-user-spotpython_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"606{-}user{-}resnet"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}
\end{Highlighting}
\end{Shaded}

In a second step, we can add the user specified ResNet model to the
\texttt{fun\_control} dictionary:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ add\_core\_model\_to\_fun\_control}
\NormalTok{add\_core\_model\_to\_fun\_control(fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                              core\_model}\OperatorTok{=}\NormalTok{my\_resnet.MyResNet,}
\NormalTok{                              hyper\_dict}\OperatorTok{=}\NormalTok{my\_hyper\_dict.MyHyperDict)}
\end{Highlighting}
\end{Shaded}

The method \texttt{set\_hyperparameter} allows the user to modify
default hyperparameter settings. Here we modify some hyperparameters to
keep the model small and to decrease the tuning time.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"lr\_mult"}\NormalTok{, [}\FloatTok{0.1}\NormalTok{, }\FloatTok{20.0}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |     3   |   4     | transform_power_2_int |
| epochs         | int    | 4         |     3   |   7     | transform_power_2_int |
| batch_size     | int    | 4         |     4   |  11     | transform_power_2_int |
| act_fn         | factor | ReLU      |     0   |   5     | None                  |
| optimizer      | factor | SGD       |     0   |   2     | None                  |
| dropout_prob   | float  | 0.01      |     0   |   0.025 | None                  |
| lr_mult        | float  | 1.0       |     0.1 |  20     | None                  |
| patience       | int    | 2         |     2   |   3     | transform_power_2_int |
| initialization | factor | Default   |     0   |   4     | None                  |
\end{verbatim}

Finally, a \texttt{Spot} object is created. Calling the method
\texttt{run()} starts the hyperparameter tuning process.

\phantomsection\label{user-user-run}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Milestones: [16, 32, 48]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 24041.166015625, 'hp_metric': 24041.166015625}
Milestones: [2, 4, 6]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22516.333984375, 'hp_metric': 22516.333984375}
Milestones: [16, 32, 48]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23957.216796875, 'hp_metric': 23957.216796875}
Milestones: [2, 4, 6]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23526.126953125, 'hp_metric': 23526.126953125}
Milestones: [32, 64, 96]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23435.740234375, 'hp_metric': 23435.740234375}
Milestones: [32, 64, 96]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5741.01416015625, 'hp_metric': 5741.01416015625}
Milestones: [4, 8, 12]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23971.83984375, 'hp_metric': 23971.83984375}
Milestones: [4, 8, 12]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 24112.75, 'hp_metric': 24112.75}
Milestones: [8, 16, 24]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 20110.208984375, 'hp_metric': 20110.208984375}
Milestones: [8, 16, 24]
train_model result: {'val_loss': 23546.658203125, 'hp_metric': 23546.658203125}
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
train_model result: {'val_loss': 4534.15283203125, 'hp_metric': 4534.15283203125}
spotpython tuning: 4534.15283203125 [#---------] 6.04% 
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
train_model result: {'val_loss': 3945.150390625, 'hp_metric': 3945.150390625}
spotpython tuning: 3945.150390625 [#---------] 14.48% 
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
train_model result: {'val_loss': 23405.11328125, 'hp_metric': 23405.11328125}
spotpython tuning: 3945.150390625 [###-------] 30.33% 
\end{verbatim}

\begin{verbatim}
Milestones: [4, 8, 12]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23806.390625, 'hp_metric': 23806.390625}
spotpython tuning: 3945.150390625 [###-------] 34.00% 
\end{verbatim}

\begin{verbatim}
Milestones: [8, 16, 24]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 19730.400390625, 'hp_metric': 19730.400390625}
spotpython tuning: 3945.150390625 [#####-----] 45.13% 
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
train_model result: {'val_loss': 4454.765625, 'hp_metric': 4454.765625}
spotpython tuning: 3945.150390625 [#####-----] 51.73% 
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
train_model result: {'val_loss': 4234.16943359375, 'hp_metric': 4234.16943359375}
spotpython tuning: 3945.150390625 [######----] 57.76% 
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
train_model result: {'val_loss': 5304.9833984375, 'hp_metric': 5304.9833984375}
spotpython tuning: 3945.150390625 [######----] 64.10% 
\end{verbatim}

\begin{verbatim}
Milestones: [8, 16, 24]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22116.203125, 'hp_metric': 22116.203125}
spotpython tuning: 3945.150390625 [########--] 75.37% 
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
train_model result: {'val_loss': 4638.31298828125, 'hp_metric': 4638.31298828125}
spotpython tuning: 3945.150390625 [########--] 81.95% 
\end{verbatim}

\begin{verbatim}
Milestones: [32, 64, 96]
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23442.00390625, 'hp_metric': 23442.00390625}
spotpython tuning: 3945.150390625 [##########] 100.00% Done...

Experiment saved to 606-user-resnet_res.pkl
\end{verbatim}

\section{Looking at the Results}\label{looking-at-the-results-4}

\subsection{Tuning Progress}\label{tuning-progress-4}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{606_spot_hpt_light_diabetes_user_resnet_files/figure-pdf/cell-8-output-1.pdf}}

\subsection{Tuned Hyperparameters and Their
Importance}\label{tuned-hyperparameters-and-their-importance-4}

Results can be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned                | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|----------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     4.0 | 3.0                  | transform_power_2_int |        54.95 | **      |
| epochs         | int    | 4         |     3.0 |     7.0 | 7.0                  | transform_power_2_int |         0.79 | .       |
| batch_size     | int    | 4         |     4.0 |    11.0 | 11.0                 | transform_power_2_int |         0.37 | .       |
| act_fn         | factor | ReLU      |     0.0 |     5.0 | ReLU                 | None                  |         0.01 |         |
| optimizer      | factor | SGD       |     0.0 |     2.0 | Adadelta             | None                  |        70.96 | **      |
| dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.018944610786155315 | None                  |         0.10 |         |
| lr_mult        | float  | 1.0       |     0.1 |    20.0 | 17.787675379409905   | None                  |         8.45 | *       |
| patience       | int    | 2         |     2.0 |     3.0 | 3.0                  | transform_power_2_int |         4.41 | *       |
| initialization | factor | Default   |     0.0 |     4.0 | kaiming_uniform      | None                  |       100.00 | ***     |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{606_spot_hpt_light_diabetes_user_resnet_files/figure-pdf/cell-10-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
l1:  54.948989972832315
epochs:  0.7910336032578257
batch_size:  0.37023065044288384
act_fn:  0.007559924566997962
optimizer:  70.95638647025385
dropout_prob:  0.09584198169452227
lr_mult:  8.44754813284851
patience:  4.406014002087552
initialization:  100.0
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{606_spot_hpt_light_diabetes_user_resnet_files/figure-pdf/cell-11-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{606_spot_hpt_light_diabetes_user_resnet_files/figure-pdf/cell-11-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{606_spot_hpt_light_diabetes_user_resnet_files/figure-pdf/cell-11-output-4.pdf}}

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-605}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'act_fn': ReLU(),
 'batch_size': 2048,
 'dropout_prob': 0.018944610786155315,
 'epochs': 128,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': 17.787675379409905,
 'optimizer': 'Adadelta',
 'patience': 8}
\end{verbatim}

\section{Details of the User-Specified ResNet
Model}\label{details-of-the-user-specified-resnet-model}

The specification of a user model requires three files:

\begin{itemize}
\tightlist
\item
  \texttt{my\_resnet.py}: the Python file containing the user specified
  ResNet model
\item
  \texttt{my\_hyperdict.py}: the Python file for loading the
  hyperparameter dictionary \texttt{my\_hyperdict.json} for the user
  specified ResNet model
\item
  \texttt{my\_hyperdict.json}: the JSON file containing the
  hyperparameter dictionary for the user specified ResNet model
\end{itemize}

\subsection{\texorpdfstring{\texttt{my\_resnet.py}}{my\_resnet.py}}\label{my_resnet.py}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ lightning }\ImportTok{as}\NormalTok{ L}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch }\ImportTok{import}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.optimizer }\ImportTok{import}\NormalTok{ optimizer\_handler}
\ImportTok{import}\NormalTok{ torchmetrics.functional.regression}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}

\KeywordTok{class}\NormalTok{ ResidualBlock(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_dim, output\_dim, act\_fn, dropout\_prob):}
        \BuiltInTok{super}\NormalTok{(ResidualBlock, }\VariableTok{self}\NormalTok{).}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.fc1 }\OperatorTok{=}\NormalTok{ nn.Linear(input\_dim, output\_dim)}
        \VariableTok{self}\NormalTok{.bn1 }\OperatorTok{=}\NormalTok{ nn.BatchNorm1d(output\_dim)}
        \VariableTok{self}\NormalTok{.ln1 }\OperatorTok{=}\NormalTok{ nn.LayerNorm(output\_dim)  }
        \VariableTok{self}\NormalTok{.fc2 }\OperatorTok{=}\NormalTok{ nn.Linear(output\_dim, output\_dim)}
        \VariableTok{self}\NormalTok{.bn2 }\OperatorTok{=}\NormalTok{ nn.BatchNorm1d(output\_dim)}
        \VariableTok{self}\NormalTok{.ln2 }\OperatorTok{=}\NormalTok{ nn.LayerNorm(output\_dim)}
        \VariableTok{self}\NormalTok{.act\_fn }\OperatorTok{=}\NormalTok{ act\_fn}
        \VariableTok{self}\NormalTok{.dropout }\OperatorTok{=}\NormalTok{ nn.Dropout(dropout\_prob)}
        \VariableTok{self}\NormalTok{.shortcut }\OperatorTok{=}\NormalTok{ nn.Sequential()}

        \ControlFlowTok{if}\NormalTok{ input\_dim }\OperatorTok{!=}\NormalTok{ output\_dim:}
            \VariableTok{self}\NormalTok{.shortcut }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{                nn.Linear(input\_dim, output\_dim),}
\NormalTok{                nn.BatchNorm1d(output\_dim)}
\NormalTok{            )}
    
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        identity }\OperatorTok{=} \VariableTok{self}\NormalTok{.shortcut(x)}
        
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.fc1(x)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn1(out)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.ln1(out)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.act\_fn(out)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.dropout(out)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.fc2(out)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn2(out)}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.ln2(out)}
\NormalTok{        out }\OperatorTok{+=}\NormalTok{ identity  }\CommentTok{\# Residual connection}
\NormalTok{        out }\OperatorTok{=} \VariableTok{self}\NormalTok{.act\_fn(out)}
        \ControlFlowTok{return}\NormalTok{ out}

\KeywordTok{class}\NormalTok{ MyResNet(L.LightningModule):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        l1: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        epochs: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        batch\_size: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        initialization: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        act\_fn: nn.Module,}
\NormalTok{        optimizer: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{        dropout\_prob: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        lr\_mult: }\BuiltInTok{float}\NormalTok{,}
\NormalTok{        patience: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_in: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_L\_out: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{        \_torchmetric: }\BuiltInTok{str}\NormalTok{,}
\NormalTok{    ):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{=}\NormalTok{ \_L\_in}
        \VariableTok{self}\NormalTok{.\_L\_out }\OperatorTok{=}\NormalTok{ \_L\_out}
        \ControlFlowTok{if}\NormalTok{ \_torchmetric }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{            \_torchmetric }\OperatorTok{=} \StringTok{"mean\_squared\_error"}
        \VariableTok{self}\NormalTok{.\_torchmetric }\OperatorTok{=}\NormalTok{ \_torchmetric}
        \VariableTok{self}\NormalTok{.metric }\OperatorTok{=} \BuiltInTok{getattr}\NormalTok{(torchmetrics.functional.regression, \_torchmetric)}
        \VariableTok{self}\NormalTok{.save\_hyperparameters(ignore}\OperatorTok{=}\NormalTok{[}\StringTok{"\_L\_in"}\NormalTok{, }\StringTok{"\_L\_out"}\NormalTok{, }\StringTok{"\_torchmetric"}\NormalTok{])}
        \VariableTok{self}\NormalTok{.example\_input\_array }\OperatorTok{=}\NormalTok{ torch.zeros((batch\_size, }\VariableTok{self}\NormalTok{.\_L\_in))}
        
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.hparams.l1 }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"l1 must be at least 4"}\NormalTok{)}
        
        \CommentTok{\# Get hidden sizes}
\NormalTok{        hidden\_sizes }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_get\_hidden\_sizes()}
\NormalTok{        layer\_sizes }\OperatorTok{=}\NormalTok{ [}\VariableTok{self}\NormalTok{.\_L\_in] }\OperatorTok{+}\NormalTok{ hidden\_sizes}

        \CommentTok{\# Construct the layers with Residual Blocks and Linear Layer at the end}
\NormalTok{        layers }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(layer\_sizes) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
\NormalTok{            layers.append(}
\NormalTok{                ResidualBlock(}
\NormalTok{                    layer\_sizes[i], }
\NormalTok{                    layer\_sizes[i }\OperatorTok{+} \DecValTok{1}\NormalTok{], }
                    \VariableTok{self}\NormalTok{.hparams.act\_fn, }
                    \VariableTok{self}\NormalTok{.hparams.dropout\_prob}
\NormalTok{                )}
\NormalTok{            )}
\NormalTok{        layers.append(nn.Linear(layer\_sizes[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], }\VariableTok{self}\NormalTok{.\_L\_out))}
        
        \VariableTok{self}\NormalTok{.layers }\OperatorTok{=}\NormalTok{ nn.Sequential(}\OperatorTok{*}\NormalTok{layers)}

        \CommentTok{\# Initialization (Xavier, Kaiming, or Default)}
        \VariableTok{self}\NormalTok{.}\BuiltInTok{apply}\NormalTok{(}\VariableTok{self}\NormalTok{.\_init\_weights)}

    \KeywordTok{def}\NormalTok{ \_init\_weights(}\VariableTok{self}\NormalTok{, module):        }
        \ControlFlowTok{if} \BuiltInTok{isinstance}\NormalTok{(module, nn.Linear):}
            \ControlFlowTok{if} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"xavier\_uniform"}\NormalTok{:}
\NormalTok{                nn.init.xavier\_uniform\_(module.weight)}
            \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"xavier\_normal"}\NormalTok{:}
\NormalTok{                nn.init.xavier\_normal\_(module.weight)}
            \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"kaiming\_uniform"}\NormalTok{:}
\NormalTok{                nn.init.kaiming\_uniform\_(module.weight)}
            \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.hparams.initialization }\OperatorTok{==} \StringTok{"kaiming\_normal"}\NormalTok{:}
\NormalTok{                nn.init.kaiming\_normal\_(module.weight)}
            \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# "Default"}
\NormalTok{                nn.init.uniform\_(module.weight)}
            \ControlFlowTok{if}\NormalTok{ module.bias }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{                nn.init.zeros\_(module.bias)}
    
    \KeywordTok{def}\NormalTok{ \_generate\_div2\_list(}\VariableTok{self}\NormalTok{, n, n\_min) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{list}\NormalTok{:}
\NormalTok{        result }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        current }\OperatorTok{=}\NormalTok{ n}
\NormalTok{        repeats }\OperatorTok{=} \DecValTok{1}
\NormalTok{        max\_repeats }\OperatorTok{=} \DecValTok{4}
        \ControlFlowTok{while}\NormalTok{ current }\OperatorTok{\textgreater{}=}\NormalTok{ n\_min:}
\NormalTok{            result.extend([current] }\OperatorTok{*} \BuiltInTok{min}\NormalTok{(repeats, max\_repeats))}
\NormalTok{            current }\OperatorTok{=}\NormalTok{ current }\OperatorTok{//} \DecValTok{2}
\NormalTok{            repeats }\OperatorTok{=}\NormalTok{ repeats }\OperatorTok{+} \DecValTok{1}
        \ControlFlowTok{return}\NormalTok{ result}

    \KeywordTok{def}\NormalTok{ \_get\_hidden\_sizes(}\VariableTok{self}\NormalTok{):}
\NormalTok{        n\_low }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{2}\NormalTok{, }\BuiltInTok{int}\NormalTok{(}\VariableTok{self}\NormalTok{.\_L\_in }\OperatorTok{/} \DecValTok{4}\NormalTok{))  }\CommentTok{\# Ensure minimum reasonable size}
\NormalTok{        n\_high }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\VariableTok{self}\NormalTok{.hparams.l1, }\DecValTok{2} \OperatorTok{*}\NormalTok{ n\_low)}
\NormalTok{        hidden\_sizes }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_generate\_div2\_list(n\_high, n\_low)}
        \ControlFlowTok{return}\NormalTok{ hidden\_sizes}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.layers(x)}
        \ControlFlowTok{return}\NormalTok{ x}

    \KeywordTok{def}\NormalTok{ \_calculate\_loss(}\VariableTok{self}\NormalTok{, batch):}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        y\_hat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.metric(y\_hat, y)}
        \ControlFlowTok{return}\NormalTok{ loss}

    \KeywordTok{def}\NormalTok{ training\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ validation\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ test\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{        val\_loss }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_calculate\_loss(batch)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"val\_loss"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \VariableTok{self}\NormalTok{.log(}\StringTok{"hp\_metric"}\NormalTok{, val\_loss, prog\_bar}\OperatorTok{=}\NormalTok{prog\_bar)}
        \ControlFlowTok{return}\NormalTok{ val\_loss}

    \KeywordTok{def}\NormalTok{ predict\_step(}\VariableTok{self}\NormalTok{, batch: }\BuiltInTok{tuple}\NormalTok{, batch\_idx: }\BuiltInTok{int}\NormalTok{, prog\_bar: }\BuiltInTok{bool} \OperatorTok{=} \VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ batch}
\NormalTok{        yhat }\OperatorTok{=} \VariableTok{self}\NormalTok{(x)}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ y.view(}\BuiltInTok{len}\NormalTok{(y), }\DecValTok{1}\NormalTok{)}
\NormalTok{        yhat }\OperatorTok{=}\NormalTok{ yhat.view(}\BuiltInTok{len}\NormalTok{(yhat), }\DecValTok{1}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ (x, y, yhat)}

    \KeywordTok{def}\NormalTok{ configure\_optimizers(}\VariableTok{self}\NormalTok{):}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ optimizer\_handler(}
\NormalTok{            optimizer\_name}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.optimizer,}
\NormalTok{            params}\OperatorTok{=}\VariableTok{self}\NormalTok{.parameters(),}
\NormalTok{            lr\_mult}\OperatorTok{=}\VariableTok{self}\NormalTok{.hparams.lr\_mult}
\NormalTok{        )}

        \CommentTok{\# Dynamic creation of milestones based on the number of epochs.}
\NormalTok{        num\_milestones }\OperatorTok{=} \DecValTok{3}  \CommentTok{\# Number of milestones to divide the epochs}
\NormalTok{        milestones }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(}\VariableTok{self}\NormalTok{.hparams.epochs }\OperatorTok{/}\NormalTok{ (num\_milestones }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ (i }\OperatorTok{+} \DecValTok{1}\NormalTok{)) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_milestones)]}

        \CommentTok{\# Print milestones for debug purposes}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Milestones: }\SpecialCharTok{\{}\NormalTok{milestones}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

        \CommentTok{\# Create MultiStepLR scheduler with dynamic milestones and learning rate multiplier.}
\NormalTok{        scheduler }\OperatorTok{=}\NormalTok{ optim.lr\_scheduler.MultiStepLR(}
\NormalTok{            optimizer, }
\NormalTok{            milestones}\OperatorTok{=}\NormalTok{milestones, }
\NormalTok{            gamma}\OperatorTok{=}\FloatTok{0.1}  \CommentTok{\# Decay factor}
\NormalTok{        )}

        \CommentTok{\# Learning rate scheduler configuration}
\NormalTok{        lr\_scheduler\_config }\OperatorTok{=}\NormalTok{ \{}
            \StringTok{"scheduler"}\NormalTok{: scheduler,}
            \StringTok{"interval"}\NormalTok{: }\StringTok{"epoch"}\NormalTok{,  }\CommentTok{\# Adjust learning rate per epoch}
            \StringTok{"frequency"}\NormalTok{: }\DecValTok{1}\NormalTok{,      }\CommentTok{\# Apply the scheduler at every epoch}
\NormalTok{        \}}
        
        \ControlFlowTok{return}\NormalTok{ \{}\StringTok{"optimizer"}\NormalTok{: optimizer, }\StringTok{"lr\_scheduler"}\NormalTok{: lr\_scheduler\_config\}}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{my\_hyperdict.py}}{my\_hyperdict.py}}\label{my_hyperdict.py}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ json}
\ImportTok{from}\NormalTok{ spotpython.data }\ImportTok{import}\NormalTok{ base}
\ImportTok{import}\NormalTok{ pathlib}


\KeywordTok{class}\NormalTok{ MyHyperDict(base.FileConfig):}
    \CommentTok{"""User specified hyperparameter dictionary.}

\CommentTok{    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.}

\CommentTok{    Attributes:}
\CommentTok{        filename (str):}
\CommentTok{            The name of the file where the hyperparameters are stored.}
\CommentTok{    """}

    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}
        \VariableTok{self}\NormalTok{,}
\NormalTok{        filename: }\BuiltInTok{str} \OperatorTok{=} \StringTok{"my\_hyper\_dict.json"}\NormalTok{,}
\NormalTok{        directory: }\VariableTok{None} \OperatorTok{=} \VariableTok{None}\NormalTok{,}
\NormalTok{    ) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{(filename}\OperatorTok{=}\NormalTok{filename, directory}\OperatorTok{=}\NormalTok{directory)}
        \VariableTok{self}\NormalTok{.filename }\OperatorTok{=}\NormalTok{ filename}
        \VariableTok{self}\NormalTok{.directory }\OperatorTok{=}\NormalTok{ directory}
        \VariableTok{self}\NormalTok{.hyper\_dict }\OperatorTok{=} \VariableTok{self}\NormalTok{.load()}

    \AttributeTok{@property}
    \KeywordTok{def}\NormalTok{ path(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.directory:}
            \ControlFlowTok{return}\NormalTok{ pathlib.Path(}\VariableTok{self}\NormalTok{.directory).joinpath(}\VariableTok{self}\NormalTok{.filename)}
        \ControlFlowTok{return}\NormalTok{ pathlib.Path(}\VariableTok{\_\_file\_\_}\NormalTok{).parent.joinpath(}\VariableTok{self}\NormalTok{.filename)}

    \KeywordTok{def}\NormalTok{ load(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{dict}\NormalTok{:}
        \CommentTok{"""Load the hyperparameters from the file.}

\CommentTok{        Returns:}
\CommentTok{            dict: A dictionary containing the hyperparameters.}

\CommentTok{        Examples:}
\CommentTok{            \# Assume the user specified file \textasciigrave{}my\_hyper\_dict.json\textasciigrave{} is in the \textasciigrave{}./hyperdict/\textasciigrave{} directory.}
\CommentTok{            \textgreater{}\textgreater{}\textgreater{} user\_lhd = MyHyperDict(filename=\textquotesingle{}my\_hyper\_dict.json\textquotesingle{}, directory=\textquotesingle{}./hyperdict/\textquotesingle{})}
\CommentTok{        """}
        \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\VariableTok{self}\NormalTok{.path, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{            d }\OperatorTok{=}\NormalTok{ json.load(f)}
        \ControlFlowTok{return}\NormalTok{ d}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{my\_hyperdict.json}}{my\_hyperdict.json}}\label{my_hyperdict.json}

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{"MyResNet"}\NormalTok{: \{}
        \StringTok{"l1"}\NormalTok{: \{}
            \StringTok{"type"}\NormalTok{: }\StringTok{"int"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"transform\_power\_2\_int"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\DecValTok{3}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\DecValTok{10}
\NormalTok{        \},}
        \StringTok{"epochs"}\NormalTok{: \{}
            \StringTok{"type"}\NormalTok{: }\StringTok{"int"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\DecValTok{4}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"transform\_power\_2\_int"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\DecValTok{4}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\DecValTok{9}
\NormalTok{        \},}
        \StringTok{"batch\_size"}\NormalTok{: \{}
            \StringTok{"type"}\NormalTok{: }\StringTok{"int"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\DecValTok{4}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"transform\_power\_2\_int"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\DecValTok{1}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\DecValTok{6}
\NormalTok{        \},}
        \StringTok{"act\_fn"}\NormalTok{: \{}
            \StringTok{"levels"}\NormalTok{: [}
                \StringTok{"Sigmoid"}\NormalTok{,}
                \StringTok{"Tanh"}\NormalTok{,}
                \StringTok{"ReLU"}\NormalTok{,}
                \StringTok{"LeakyReLU"}\NormalTok{,}
                \StringTok{"ELU"}\NormalTok{,}
                \StringTok{"Swish"}
\NormalTok{            ],}
            \StringTok{"type"}\NormalTok{: }\StringTok{"factor"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\StringTok{"ReLU"}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"None"}\NormalTok{,}
            \StringTok{"class\_name"}\NormalTok{: }\StringTok{"spotpython.torch.activation"}\NormalTok{,}
            \StringTok{"core\_model\_parameter\_type"}\NormalTok{: }\StringTok{"instance()"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\DecValTok{0}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\DecValTok{5}
\NormalTok{        \},}
        \StringTok{"optimizer"}\NormalTok{: \{}
            \StringTok{"levels"}\NormalTok{: [}
                \StringTok{"Adadelta"}\NormalTok{,}
                \StringTok{"Adagrad"}\NormalTok{,}
                \StringTok{"Adam"}\NormalTok{,}
                \StringTok{"AdamW"}\NormalTok{,}
                \StringTok{"SparseAdam"}\NormalTok{,}
                \StringTok{"Adamax"}\NormalTok{,}
                \StringTok{"ASGD"}\NormalTok{,}
                \StringTok{"NAdam"}\NormalTok{,}
                \StringTok{"RAdam"}\NormalTok{,}
                \StringTok{"RMSprop"}\NormalTok{,}
                \StringTok{"Rprop"}\NormalTok{,}
                \StringTok{"SGD"}
\NormalTok{            ],}
            \StringTok{"type"}\NormalTok{: }\StringTok{"factor"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\StringTok{"SGD"}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"None"}\NormalTok{,}
            \StringTok{"class\_name"}\NormalTok{: }\StringTok{"torch.optim"}\NormalTok{,}
            \StringTok{"core\_model\_parameter\_type"}\NormalTok{: }\StringTok{"str"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\DecValTok{0}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\DecValTok{11}
\NormalTok{        \},}
        \StringTok{"dropout\_prob"}\NormalTok{: \{}
            \StringTok{"type"}\NormalTok{: }\StringTok{"float"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\FloatTok{0.01}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"None"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\FloatTok{0.0}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\FloatTok{0.25}
\NormalTok{        \},}
        \StringTok{"lr\_mult"}\NormalTok{: \{}
            \StringTok{"type"}\NormalTok{: }\StringTok{"float"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\FloatTok{1.0}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"None"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\FloatTok{0.1}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\FloatTok{10.0}
\NormalTok{        \},}
        \StringTok{"patience"}\NormalTok{: \{}
            \StringTok{"type"}\NormalTok{: }\StringTok{"int"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\DecValTok{2}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"transform\_power\_2\_int"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\DecValTok{2}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\DecValTok{6}
\NormalTok{        \},}
        \StringTok{"initialization"}\NormalTok{: \{}
            \StringTok{"levels"}\NormalTok{: [}
                \StringTok{"Default"}\NormalTok{,}
                \StringTok{"kaiming\_uniform"}\NormalTok{,}
                \StringTok{"kaiming\_normal"}\NormalTok{,}
                \StringTok{"xavier\_uniform"}\NormalTok{,}
                \StringTok{"xavier\_normal"}
\NormalTok{            ],}
            \StringTok{"type"}\NormalTok{: }\StringTok{"factor"}\NormalTok{,}
            \StringTok{"default"}\NormalTok{: }\StringTok{"Default"}\NormalTok{,}
            \StringTok{"transform"}\NormalTok{: }\StringTok{"None"}\NormalTok{,}
            \StringTok{"core\_model\_parameter\_type"}\NormalTok{: }\StringTok{"str"}\NormalTok{,}
            \StringTok{"lower"}\NormalTok{: }\DecValTok{0}\NormalTok{,}
            \StringTok{"upper"}\NormalTok{: }\DecValTok{4}
\NormalTok{        \}}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

\section{Summary}\label{summary-5}

This section presented an introduction to the basic setup of
hyperparameter tuning with \texttt{spotpython} and \texttt{PyTorch}
Lightning using a ResNet model for the Diabetes data set.

\chapter{\texorpdfstring{Hyperparameter Tuning with \texttt{spotpython}
and \texttt{PyTorch} Lightning Using a CondNet
Model}{Hyperparameter Tuning with spotpython and PyTorch Lightning Using a CondNet Model}}\label{hyperparameter-tuning-with-spotpython-and-pytorch-lightning-using-a-condnet-model}

\begin{itemize}
\tightlist
\item
  We use the \texttt{Diabetes} dataset to illustrate the hyperparameter
  tuning process of a \texttt{CondNet} model using the
  \texttt{spotpython} package.
\item
  The CondNet model is a conditional neural network that can be used to
  model conditional distributions
  \href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/light/regression/nn_condnet_regressor/}{{[}LINK{]}}.
\end{itemize}

\phantomsection\label{cond_net_setup}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"CondNet\_01"}

\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{input\_dim }\OperatorTok{=} \DecValTok{10}
\NormalTok{output\_dim }\OperatorTok{=} \DecValTok{1}
\NormalTok{cond\_dim }\OperatorTok{=} \DecValTok{2}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNCondNetRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\NormalTok{input\_dim }\OperatorTok{{-}}\NormalTok{ cond\_dim,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    \_L\_cond}\OperatorTok{=}\NormalTok{cond\_dim,)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}


\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"lr\_mult"}\NormalTok{, [}\FloatTok{0.1}\NormalTok{, }\FloatTok{20.0}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNCondNetRegressor
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |     3   |   4     | transform_power_2_int |
| epochs         | int    | 4         |     3   |   7     | transform_power_2_int |
| batch_size     | int    | 4         |     4   |   5     | transform_power_2_int |
| act_fn         | factor | ReLU      |     0   |   5     | None                  |
| optimizer      | factor | SGD       |     0   |   2     | None                  |
| dropout_prob   | float  | 0.01      |     0   |   0.025 | None                  |
| lr_mult        | float  | 1.0       |     0.1 |  20     | None                  |
| patience       | int    | 2         |     2   |   3     | transform_power_2_int |
| batch_norm     | factor | 0         |     0   |   1     | None                  |
| initialization | factor | Default   |     0   |   4     | None                  |
\end{verbatim}

\phantomsection\label{cond_net_run}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{res }\OperatorTok{=}\NormalTok{ spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train_model result: {'val_loss': 24159.23828125, 'hp_metric': 24159.23828125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23455.20703125, 'hp_metric': 23455.20703125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 25569.056640625, 'hp_metric': 25569.056640625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 24069.580078125, 'hp_metric': 24069.580078125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22380.91015625, 'hp_metric': 22380.91015625}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23910.111328125, 'hp_metric': 23910.111328125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 23796.3828125, 'hp_metric': 23796.3828125}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5523.42138671875, 'hp_metric': 5523.42138671875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 22435.35546875, 'hp_metric': 22435.35546875}
train_model result: {'val_loss': 22645.388671875, 'hp_metric': 22645.388671875}
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3096.057373046875, 'hp_metric': 3096.057373046875}
spotpython tuning: 3096.057373046875 [#---------] 5.67% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4201.18310546875, 'hp_metric': 4201.18310546875}
spotpython tuning: 3096.057373046875 [#---------] 11.04% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3649.884765625, 'hp_metric': 3649.884765625}
spotpython tuning: 3096.057373046875 [##--------] 19.59% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5814.76220703125, 'hp_metric': 5814.76220703125}
spotpython tuning: 3096.057373046875 [##--------] 23.63% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4117.13134765625, 'hp_metric': 4117.13134765625}
spotpython tuning: 3096.057373046875 [###-------] 32.10% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4142.01416015625, 'hp_metric': 4142.01416015625}
spotpython tuning: 3096.057373046875 [####------] 37.76% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 5454.61279296875, 'hp_metric': 5454.61279296875}
spotpython tuning: 3096.057373046875 [####------] 43.90% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 3486.806640625, 'hp_metric': 3486.806640625}
spotpython tuning: 3096.057373046875 [#####-----] 50.58% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4156.4521484375, 'hp_metric': 4156.4521484375}
spotpython tuning: 3096.057373046875 [######----] 56.61% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 16856.775390625, 'hp_metric': 16856.775390625}
spotpython tuning: 3096.057373046875 [#########-] 87.92% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 4796.46337890625, 'hp_metric': 4796.46337890625}
spotpython tuning: 3096.057373046875 [##########] 95.81% 
\end{verbatim}

\begin{verbatim}
train_model result: {'val_loss': 7036.24609375, 'hp_metric': 7036.24609375}
spotpython tuning: 3096.057373046875 [##########] 100.00% Done...

Experiment saved to CondNet_01_res.pkl
\end{verbatim}

\section{Looking at the Results}\label{looking-at-the-results-5}

\subsection{Tuning Progress}\label{tuning-progress-5}

After the hyperparameter tuning run is finished, the progress of the
hyperparameter tuning can be visualized with \texttt{spotpython}'s
method \texttt{plot\_progress}. The black points represent the
performace values (score or metric) of hyperparameter configurations
from the initial design, whereas the red points represents the
hyperparameter configurations found by the surrogate model based
optimization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{608_spot_hpt_light_condnet_files/figure-pdf/cell-5-output-1.pdf}}

\subsection{Tuned Hyperparameters and Their
Importance}\label{tuned-hyperparameters-and-their-importance-5}

Results can be printed in tabular form.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | tuned                 | transform             |   importance | stars   |
|----------------|--------|-----------|---------|---------|-----------------------|-----------------------|--------------|---------|
| l1             | int    | 3         |     3.0 |     4.0 | 3.0                   | transform_power_2_int |         0.00 |         |
| epochs         | int    | 4         |     3.0 |     7.0 | 7.0                   | transform_power_2_int |         0.00 |         |
| batch_size     | int    | 4         |     4.0 |     5.0 | 4.0                   | transform_power_2_int |         0.00 |         |
| act_fn         | factor | ReLU      |     0.0 |     5.0 | Swish                 | None                  |         0.00 |         |
| optimizer      | factor | SGD       |     0.0 |     2.0 | Adadelta              | None                  |         0.00 |         |
| dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.0012653320827643374 | None                  |         0.08 |         |
| lr_mult        | float  | 1.0       |     0.1 |    20.0 | 4.855819388527004     | None                  |         0.03 |         |
| patience       | int    | 2         |     2.0 |     3.0 | 2.0                   | transform_power_2_int |         0.00 |         |
| batch_norm     | factor | 0         |     0.0 |     1.0 | 1                     | None                  |         0.00 |         |
| initialization | factor | Default   |     0.0 |     4.0 | kaiming_uniform       | None                  |       100.00 | ***     |
\end{verbatim}

A histogram can be used to visualize the most important hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_importance(threshold}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{608_spot_hpt_light_condnet_files/figure-pdf/cell-7-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
l1:  0.004029153451543868
epochs:  0.004029153451543868
batch_size:  0.004029153451543868
act_fn:  0.004029153451543868
optimizer:  0.004029153451543868
dropout_prob:  0.08077498325537841
lr_mult:  0.03414707537824906
patience:  0.004029153451543868
batch_norm:  0.004029153451543868
initialization:  100.0
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{608_spot_hpt_light_condnet_files/figure-pdf/cell-8-output-2.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{608_spot_hpt_light_condnet_files/figure-pdf/cell-8-output-3.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{608_spot_hpt_light_condnet_files/figure-pdf/cell-8-output-4.pdf}}

\subsection{Get the Tuned Architecture}\label{sec-get-spot-results-608}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pprint}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_architecture}
\NormalTok{config }\OperatorTok{=}\NormalTok{ get\_tuned\_architecture(spot\_tuner)}
\NormalTok{pprint.pprint(config)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'act_fn': Swish(),
 'batch_norm': True,
 'batch_size': 16,
 'dropout_prob': 0.0012653320827643374,
 'epochs': 128,
 'initialization': 'kaiming_uniform',
 'l1': 8,
 'lr_mult': 4.855819388527004,
 'optimizer': 'Adadelta',
 'patience': 4}
\end{verbatim}

\part{Multi Objective Optimization}

\chapter{Introduction to Desirability
Functions}\label{introduction-to-desirability-functions}

The desirability function approach is a widely adopted method in
industry for optimizing multiple response processes ({``{NIST/SEMATECH
e-Handbook of Statistical Methods}''} 2021). It operates on the
principle that the overall ``quality'' of a product or process with
multiple quality characteristics is deemed unacceptable if any
characteristic falls outside the ``desired'' limits. This approach
identifies operating conditions that yield the most ``desirable''
response values, effectively balancing multiple objectives to achieve
optimal outcomes.

Often, different scales are used for various objectives. When combining
these objectives into a single new one, the challenge arises of how to
compare the scales with each other. The fundamental idea of the
desirability index is to transform the deviations of the objective value
from its target value into comparable desirabilities, i.e., onto a
common scale. For this, a target value as well as a lower and/or upper
specification limit must be known for each objective involved. A result
outside the specification limits is assigned a desirability of 0, while
a result at the target value is assigned a desirability of 1. Linear or
nonlinear transformation, such as a power transformation, can be chosen
as the transformation between the specification limits. The desirability
index according to Derringer and Suich (1980) is then the geometric mean
of the desirabilities of the various objectives (Weihe et al. 1999).

The \texttt{desirability} package (Kuhn 2016), which is written in the
statistical programming language \texttt{R}, contains \texttt{S3}
classes for multivariate optimization using the desirability function
approach of Harington (1965) with functional forms described by
Derringer and Suich (1980). It is available on CRAN, see
\url{https://cran.r-project.org/package=desirability}.

Hyperparameter Tuning (or Hyperparameter Optimization) is crucial for
configuring machine learning algorithms, as hyperparameters
significantly impact performance (Bartz et al. 2022; Bischl et al. 2023)
To avoid manual, time-consuming, and irreproducible trial-and-error
processes, these tuning methods can be used. They include simple
techniques like grid and random search, as well as advanced approaches
such as evolution strategies, surrogate optimization, Hyperband, and
racing. The tuning process has to consider several objectives, such as
maximizing the model's performance while minimizing the training time or
model complexity. The desirability function approach is a suitable
method for multi-objective optimization, as it allows for the
simultaneous optimization of multiple objectives by combining them into
a single desirability score.

This paper is structured as follows: After presenting the desirability
function approach in Section~\ref{sec-desirability}, we introduce the
\texttt{Python} package \texttt{spotdesirability}, which is a
\texttt{Python} implementation of the \texttt{R} package
\texttt{desirability}. The introduction is based on several ``hands-on''
examples. Section~\ref{sec-related-work} provides an overview of related
work in the field of multi-objective optimization and hyperparameter
tuning. Section~\ref{sec-example-chemical-reaction} presents an example
of a chemical reaction with two objectives: conversion and activity. The
example is based on a response surface experiment described by Myers,
Montgomery, and Anderson-Cook (2016) and also used by Kuhn (2016). It
allows a direct comparison of the results obtained with the \texttt{R}
package \texttt{desirability} and the \texttt{Python} package
\texttt{spotdesirability}. Section~\ref{sec-maximizing-desirability}
describes how to maximize the desirability function using the
Nelder-Mead algorithm from the \texttt{scipy.optimize.minimize}
function. This approach is common in RSM (Box and Wilson 1951; Myers,
Montgomery, and Anderson-Cook 2016). The optimization process is
illustrated using the chemical reaction example from
Section~\ref{sec-example-chemical-reaction}. This example is based on
the example presented in Kuhn (2016), so that, similar to the comparison
in Section~\ref{sec-example-chemical-reaction}, a comparison of the
results obtained with the \texttt{R} and \texttt{Python} packages is
possible. Section~\ref{sec-surrogate} presents an example of surrogate
model-based optimization (Gramacy 2020; Forrester, SÃ³bester, and Keane
2008) using the \texttt{spotdesirability} package. Results from the RSM
optimization can be compared with the results from surrogate model-based
optimization. The surrogate model is based on the \texttt{spotpython}
package (Bartz-Beielstein 2023b).
Section~\ref{sec-hyperparameter-tuning} presents an example of
hyperparameter tuning of a neural network implemented in
\texttt{PyTorch} using the \texttt{spotdesirability} package. The goal
of this example is to demonstrate how to use the desirability function
approach for hyperparameter tuning in a machine learning context. The
article concludes with a summary and outlook in
Section~\ref{sec-conclusion}.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Citation}]

\begin{itemize}
\tightlist
\item
  If this document has been useful to you and you wish to cite it in a
  scientific publication, please refer to the following paper, which can
  be found on arXiv: \url{https://arxiv.org/abs/2307.10262}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{@article\{bart25a,}
\NormalTok{    adsurl = \{https://ui.adsabs.harvard.edu/abs/2025arXiv250323595B\},}
\NormalTok{    archiveprefix = \{arXiv\},}
\NormalTok{    author = \{\{Bartz{-}Beielstein\}, Thomas\},}
\NormalTok{    doi = \{10.48550/arXiv.2503.23595\},}
\NormalTok{    eid = \{arXiv:2503.23595\},}
\NormalTok{    eprint = \{2503.23595\},}
\NormalTok{    journal = \{arXiv e{-}prints\},}
\NormalTok{    keywords = \{Optimization and Control, Machine Learning, Applications, 90C26, I.2.6; G.1.6\},}
\NormalTok{    month = mar,}
\NormalTok{    pages = \{arXiv:2503.23595\},}
\NormalTok{    primaryclass = \{math.OC\},}
\NormalTok{    title = \{\{Multi{-}Objective Optimization and Hyperparameter Tuning With Desirability Functions\}\},}
\NormalTok{    year = 2025,}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{\texorpdfstring{The \texttt{Python} Packages Used in This
Article}{The Python Packages Used in This Article}}\label{the-python-packages-used-in-this-article}

The following \texttt{Python} packages, classes, and functions are used
in this article:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ warnings}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.fun.mohyperlight }\ImportTok{import}\NormalTok{ MoHyperLight}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter}
\ImportTok{from}\NormalTok{ spotpython.mo.functions }\ImportTok{import}\NormalTok{ fun\_myer16a}
\ImportTok{from}\NormalTok{ spotpython.mo.plot }\ImportTok{import}\NormalTok{ plot\_mo}
\ImportTok{from}\NormalTok{ spotpython.plot.contour }\ImportTok{import}\NormalTok{ (mo\_generate\_plot\_grid, contour\_plot,}
\NormalTok{                                     contourf\_plot)}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table, print\_res\_table}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ get\_experiment\_filename}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init,}
\NormalTok{                                   design\_control\_init)}
\ImportTok{from}\NormalTok{ spotdesirability.utils.desirability }\ImportTok{import}\NormalTok{ (DOverall, DMax, DCategorical, DMin,}
\NormalTok{                                                 DTarget, DArb, DBox)}
\ImportTok{from}\NormalTok{ spotdesirability.plot.ccd }\ImportTok{import}\NormalTok{ plotCCD}
\ImportTok{from}\NormalTok{ spotdesirability.functions.rsm }\ImportTok{import}\NormalTok{ rsm\_opt, conversion\_pred, activity\_pred}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Desirability}\label{sec-desirability}

\subsection{Basic Desirability
Functions}\label{basic-desirability-functions}

The desirability function approach to simultaneously optimizing multiple
equations was originally proposed by Harington (1965). The approach
translates the functions to a common scale (\([0, 1]\)), combines them
using the geometric mean, and optimizes the overall metric. The
equations can represent model predictions or other equations. Kuhn
(2016) notes that desirability functions are popular in response surface
methodology (RSM) (Box and Wilson 1951; Myers, Montgomery, and
Anderson-Cook 2016) to simultaneously optimize a series of quadratic
models. A response surface experiment may use measurements on a set of
outcomes, where instead of optimizing each outcome separately, settings
for the predictor variables are sought to satisfy all outcomes at once.

Kuhn (2016) explains that originally, Harrington used exponential
functions to quantify desirability. In our \texttt{Python}
implementation, which is based on the \texttt{R} package
\texttt{desirablity} from Kuhn (2016), the simple discontinuous
functions of Derringer and Suich (1980) are adopted. For simultaneous
optimization of equations, individual ``desirability'' functions are
constructed for each function, and Derringer and Suich (1980) proposed
three forms of these functions corresponding to the optimization goal
type. Kuhn (2016) describes the \texttt{R} implementation as follows:

\begin{quote}
Suppose there are \(R\) equations or functions to simultaneously
optimize, denoted \(f_r(\vec{x})\) (\(r = 1 \ldots R\)). For each of the
\(R\) functions, an individual ``desirability'' function is constructed
that is high when \(f_r(\vec{x})\) is at the desirable level (such as a
maximum, minimum, or target) and low when \(f_r(\vec{x})\) is at an
undesirable value. Derringer and Suich (1980) proposed three forms of
these functions, corresponding to the type of optimization goal, namely
maximization, minimization, or target optimization. The associated
desirability functions are denoted \(d_r^{\text{max}}\),
\(d_r^{\text{min}}\), and \(d_r^{\text{target}}\).
\end{quote}

\subsubsection{Maximization}\label{maximization}

For maximization of \(f_r(\vec{x})\) (``larger-is-better''), the
following function is used:

\[
d_r^{\text{max}} =
\begin{cases}
    0 & \text{if } f_r(\vec{x}) < A \\
    \left(\frac{f_r(\vec{x}) - A}{B - A}\right)^s & \text{if } A \leq f_r(\vec{x}) \leq B \\
    1 & \text{if } f_r(\vec{x}) > B,
\end{cases}
\]

where \(A\), \(B\), and \(s\) are chosen by the investigator.

\subsubsection{Minimization}\label{minimization}

For minimization (``smaller-is-better''), the following function is
proposed:

\[
d_r^{\text{min}} =
\begin{cases}
    0 & \text{if } f_r(\vec{x}) > B \\
    \left(\frac{f_r(\vec{x}) - B}{A - B}\right)^s & \text{if } A \leq f_r(\vec{x}) \leq B \\
    1 & \text{if } f_r(\vec{x}) < A
\end{cases}
\]

\subsubsection{Target Optimization}\label{target-optimization}

In ``target-is-best'' situations, the following function is used:

\[
d_r^{\text{target}} =
\begin{cases}
    \left(\frac{f_r(\vec{x}) - A}{t_0 - A}\right)^{s_1} & \text{if } A \leq f_r(\vec{x}) \leq t_0 \\
    \left(\frac{f_r(\vec{x}) - B}{t_0 - B}\right)^{s_2} & \text{if } t_0 \leq f_r(\vec{x}) \leq B \\
    0 & \text{otherwise.}
\end{cases}
\]

Kuhn (2016) explains that these functions, which are shown in
Figure~\ref{fig-kuhn16a-1}, share the same scale and are discontinuous
at specific points \(A\), \(B\), and \(t_0\). The values of \(s\),
\(s_1\), or \(s_2\) can be chosen so that the desirability criterion is
easier or more difficult to satisfy. For example:

\begin{itemize}
\tightlist
\item
  If \(s\) is chosen to be less than 1 in \(d_r^{\text{min}}\),
  \(d_r^{\text{min}}\) is near 1 even if the model \(f_r(\vec{x})\) is
  not low.
\item
  As values of \(s\) move closer to 0, the desirability reflected by
  \(d_r^{\text{min}}\) becomes higher.
\item
  Values of \(s\) greater than 1 will make \(d_r^{\text{min}}\) harder
  to satisfy in terms of desirability.
\end{itemize}

Kuhn notes that these scaling factors are useful when one equation holds
more importance than others. He emphasizes that any function can reflect
model desirability; Del Castillo, Montgomery, and McCarville (1996)
developed alternative functions suitable for gradient-based
optimizations.

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/cell-3-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dCategorical\_obj }\OperatorTok{=}\NormalTok{ DCategorical(missing}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, values}\OperatorTok{=}\NormalTok{\{}\StringTok{"A"}\NormalTok{: }\FloatTok{0.8}\NormalTok{, }\StringTok{"B"}\NormalTok{: }\FloatTok{0.6}\NormalTok{, }\StringTok{"C"}\NormalTok{: }\FloatTok{0.4}\NormalTok{\})}
\NormalTok{dCategorical\_obj.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/cell-4-output-1.pdf}}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-1-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-1}Examples of the three primary desirability
functions. Panel (a) shows an example of a larger--is--better function,
panel (b) shows a smaller--is--better desirability function and panel
(c) shows a function where the optimal value corresponds to a target
value. Not that increasing the scale parameter makes it more difficult
to achieve higher desirability, while values smaller than 1 make it
easier to achieve good results.}

\end{figure}%

For each of these three desirability functions (and the others discussed
in Section~\ref{sec-nonstandard}), there are
\texttt{print\_class\_attributes}, \texttt{plot}, and \texttt{predict}
methods similar to the \texttt{R} implementation (Kuhn 2016). The
\texttt{print\_attributes} method prints the class attributes, the
\texttt{plot} method plots the desirability function, and the
\texttt{predict} method predicts the desirability for a given input.

\subsection{Overall Desirability}\label{overall-desirability}

Given the \(R\) desirability functions \(d_1 \ldots d_r\) are on the
{[}0,1{]} scale, they can be combined to achieve an overall desirability
function, \(D\). One method of doing this is by the geometric mean:

\[
D = \left(\prod_{r=1}^R d_r\right)^{1/R}.
\]

The geometric mean has the property that if any one model is undesirable
(\(d_r = 0\)), the overall desirability is also unacceptable
(\(D = 0\)). Once \(D\) has been defined and the prediction equations
for each of the \(R\) equations have been computed, it can be used to
optimize or rank the predictors.

\subsection{Non-Standard Features}\label{sec-nonstandard}

The \texttt{R} package \texttt{desirability} (Kuhn 2016) offers a few
non-standard features. These non-standard features are also included in
the \texttt{Python} implementation and will be discussed in the
following. First, we will consider the non-informative desirability and
missing values, followed by zero-desirability tolerances, and finally
non-standard desirability functions.

\subsubsection{Non-Informative Desirability and Missing
Values}\label{sec-missing}

According to Kuhn, if inputs to desirability functions are uncomputable,
the package estimates a non-informative value by computing
desirabilities over the possible range and taking the mean.\\
If an input to a desirability function is \texttt{NA}, by default, it is
replaced by this non-informative value. Setting \texttt{object\$missing}
to \texttt{NA} (in \texttt{R}) changes the calculation to return an
\texttt{NA} for the result, where \texttt{object} is the result of a
call to one of the desirability functions.A similar procedure is
implemented in the \texttt{Python} package. The non-informative value is
plotted as a broken line in default \texttt{plot} methods.

\subsubsection{Zero-Desirability
Tolerances}\label{zero-desirability-tolerances}

Kuhn (2016) highlights that in high-dimensional outcomes, finding
feasible solutions where every desirability value is acceptable can be
challenging. Each desirability R function has a \texttt{tol} argument,
which can be set between {[}0, 1{]} (default is \texttt{NULL}). If not
null, zero desirability values are replaced by \texttt{tol}.

\subsubsection{Non-Standard Desirability
Functions}\label{non-standard-desirability-functions}

Kuhn mentions scenarios where the three discussed desirability functions
are inadequate for user requirements.

\paragraph{Custom or Arbitary Desirability
Functions}\label{sec-arbitary}

In this case, the \texttt{dArb} function (\texttt{Arb} stands for
``Arbitary'') can be used to create a custom desirability function.
\texttt{dArb} accepts numeric vector inputs with matching desirabilities
to approximate other functional forms. For instance, a logistic function
can be used as a desirability function. The logistic function is defined
as \(d(\vec{x}) = \frac{1}{1+\exp(-\vec{x})}\). For inputs outside the
range \(\pm5\), desirability values remain near zero and one. The
desirability function is defined using 20 computation points on this
range, and these values establish the desirability function.

\phantomsection\label{kuhn16a-logistic}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define the logistic function}
\KeywordTok{def}\NormalTok{ foo(u):}
    \ControlFlowTok{return} \DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{u))}

\CommentTok{\# Generate input values}
\NormalTok{x\_input }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{20}\NormalTok{)}

\CommentTok{\# Create the DArb object}
\NormalTok{logistic\_d }\OperatorTok{=}\NormalTok{ DArb(x\_input, foo(x\_input))}
\NormalTok{logistic\_d.print\_class\_attributes()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Class: DArb
x: [-5.         -4.47368421 -3.94736842 -3.42105263 -2.89473684 -2.36842105
 -1.84210526 -1.31578947 -0.78947368 -0.26315789  0.26315789  0.78947368
  1.31578947  1.84210526  2.36842105  2.89473684  3.42105263  3.94736842
  4.47368421  5.        ]
d: [0.00669285 0.01127661 0.0189398  0.03164396 0.05241435 0.08561266
 0.1368025  0.21151967 0.31228169 0.43458759 0.56541241 0.68771831
 0.78848033 0.8631975  0.91438734 0.94758565 0.96835604 0.9810602
 0.98872339 0.99330715]
tol: None
missing: 0.5
\end{verbatim}

Inputs in-between these grid points are linearly interpolated. Using
this method, extreme values are applied outside the input range.
Figure~\ref{fig-kuhn16a-7} displays a \texttt{plot} of the
\texttt{logisticD} object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic\_d.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-7-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-7}An example of a desirability function
created using the \texttt{DArb} function. The desirability function is a
logistic curve that is defined by 20 points on the range {[}-5, 5{]}.}

\end{figure}%

\paragraph{Desirabilty Function for Box
Constraints}\label{sec-box-constraints}

Kuhn also adds that there is a desirability function for implementing
box constraints on an equation. For example, assigning zero desirability
to values beyond \(\pm 1.682\) in the design region, instead of
penalizing. Figure~\ref{fig-kuhn16a-8} demonstrates an example function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{box\_desirability }\OperatorTok{=}\NormalTok{ DBox(low}\OperatorTok{={-}}\FloatTok{1.682}\NormalTok{, high}\OperatorTok{=}\FloatTok{1.682}\NormalTok{)}
\NormalTok{box\_desirability.plot(non\_inform}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-8-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-8}An example of a box-like desirability
function that assigns zero desirability to values outside of the range
{[}-1.682, 1.682{]}.}

\end{figure}%

\paragraph{Desirability Function for Categorical
Inputs}\label{desirability-function-for-categorical-inputs}

Kuhn concludes by mentioning another non-standard application involving
categorical inputs. Desirabilities are assigned to each value. For
example:

\phantomsection\label{kuhn16a-categorical}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define desirability values for categorical inputs}
\NormalTok{values }\OperatorTok{=}\NormalTok{ \{}\StringTok{"value1"}\NormalTok{: }\FloatTok{0.1}\NormalTok{, }\StringTok{"value2"}\NormalTok{: }\FloatTok{0.9}\NormalTok{, }\StringTok{"value3"}\NormalTok{: }\FloatTok{0.2}\NormalTok{\}}

\CommentTok{\# Create a DCategorical object}
\NormalTok{grouped\_desirabilities }\OperatorTok{=}\NormalTok{ DCategorical(values)}

\CommentTok{\# Print the desirability values}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Desirability values for categories:"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ category, desirability }\KeywordTok{in}\NormalTok{ grouped\_desirabilities.values.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{category}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{desirability}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Example usage: Predict desirability for a specific category}
\NormalTok{category }\OperatorTok{=} \StringTok{"value2"}
\NormalTok{predicted\_desirability }\OperatorTok{=}\NormalTok{ grouped\_desirabilities.predict([category])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Predicted desirability for \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{category}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}: }\SpecialCharTok{\{}\NormalTok{predicted\_desirability[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Desirability values for categories:
value1: 0.1
value2: 0.9
value3: 0.2

Predicted desirability for 'value2': 0.9
\end{verbatim}

Figure~\ref{fig-kuhn16a-9} visualizes a plot of desirability profiles
for this setup.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped\_desirabilities.plot(non\_inform}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-9-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-9}Desirability function for categorical
values. The desirability values are assigned to three categories:
`value1', `value2', and `value3'.}

\end{figure}%

\section{Related Work}\label{sec-related-work}

Multiobjective approaches are established optimization tools (Emmerich
and Deutz 2018). The weighted-sum approach is a simple and widely used
method for multi-objective optimization, but probably only, because its
disadvantages are unknown. Compared to the weighted-sum approach, the
desirability-function approach is a better choice for multi-objective
optimization. The desirability function approach also allows for more
flexibility in defining the objectives and their trade-offs.

Nino et al. (2015) discuss the use of Experimental Designs and RSM to
optimize conflicting responses in the development of a 3D printer
prototype. Specifically, they focus on an interlocking device designed
to recycle polyethylene terephthalate water bottles. The optimization
involves two conflicting goals: maximizing load capacity and minimizing
mass. A Box Behnken Design (BBD) was used for the experimental setup,
and desirability functions were applied to identify the best trade-offs.

Karl et al. (2023) describe multi-objective optimization in maschine
learning. Coello et al. (2021) give an overview of Multi-Objective
Evolutionary Algorithms.

\section{An Example With Two Objectives: Chemical
Reaction}\label{sec-example-chemical-reaction}

Similar to the presentation in Kuhn (2016), we will use the example of a
chemical reaction to illustrate the desirability function approach. The
example is based on a response surface experiment described by Myers,
Montgomery, and Anderson-Cook (2016). The goal is to maximize the
percent conversion of a chemical reaction while keeping the thermal
activity within a specified range.

The central composite design (CCD) is the most popular class of designs
used for fitting second-order response surface models (Montgomery 2001).
Since the location of the optimum is unknown before the RSM starts, Box
and Hunter (1957) suggested that the design should be rotatable (it
provides equal precision of estimation in all directions or stated
differently, the variance of the predicted response is the same at all
points that are the same distance from the center of the design space).
A CCD is made rotable by using an axis distance value of
\(\alpha = (n_F)^{1/4}\), where \(n_F\) is the number of points (here
\(2^3 = 8\)) (Montgomery 2001). Figure~\ref{fig-ccd} shows the design
space for the chemical reaction example. The design space is defined by
three variables: reaction time, reaction temperature, and percent
catalyst. This rotable CCD consists of a full factorial design with
three factors, each at two levels, plus a center point and six
(\(2\times k\)) axial points. The axial points are located at a distance
of \(\pm \alpha\) from the center point in each direction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plotCCD(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{), title}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-ccd-output-1.pdf}}

}

\caption{\label{fig-ccd}Central composite design (CCD) for the chemical
reaction example.}

\end{figure}%

Montgomery (2001) note that it is not important to have exact
rotability. From a prediction variance point of view, the best choice is
to set \(\alpha = \sqrt{k}\), which results in a so-called spherical
CCD.

\subsection{The Two Objective Functions: Conversion and
Activity}\label{the-two-objective-functions-conversion-and-activity}

Myers, Montgomery, and Anderson-Cook (2016) present two equations for
the fitted quadratic response surface models.

\begin{align*}
f_{\text{con}}(x) =
&
 81.09
+
1.0284 \cdot x_1
+
4.043 \cdot x_2
+
6.2037 \cdot x_3
+
1.8366 \cdot x_1^2
+
2.9382 \cdot x_2^2 \\
&
+
5.1915 \cdot x_3^2
+
2.2150 \cdot x_1 \cdot x_2
+
11.375 \cdot x_1 \cdot x_3
+
3.875 \cdot x_2 \cdot x_3
\end{align*} and \begin{align*}
f_{\text{act}}(x) = 
 & 
 59.85
+ 3.583 \cdot x_1
+ 0.2546 \cdot x_2
+ 2.2298 \cdot x_3
+ 0.83479 \cdot x_1^2
+ 0.07484 \cdot x_2^2
\\
&
+ 0.05716 \cdot x_3^2
+ 0.3875 \cdot x_1 \cdot x_2
+ 0.375 \cdot x_1 \cdot x_3
+ 0.3125 \cdot x_2 \cdot x_3. 
\end{align*}

They are implemented as Python functions that take a vector of three
parameters (reaction time, reaction temperature, and percent catalyst)
and return the predicted values for the percent conversion and thermal
activity and available in the \texttt{spotdesirability} package.

The goal of the analysis in Myers, Montgomery, and Anderson-Cook (2016)
was to

\begin{itemize}
\tightlist
\item
  maximize conversion while
\item
  keeping the thermal activity between 55 and 60 units. An activity
  target of 57.5 was used in the analysis.
\end{itemize}

Plots of the response surface models are shown in
Figure~\ref{fig-kuhn16a-2} and Figure~\ref{fig-kuhn16a-3}, where
reaction time and percent catalyst are plotted while the reaction
temperature was varied at four different levels. Both quadratic models,
as pointed out by Kuhn, are saddle surfaces, and the stationary points
are outside of the experimental region. To determine predictor settings
for these models, a constrained optimization can be used to stay inside
the experimental region. Kuhn notes:

\begin{quote}
In practice, we would just use the \texttt{predict} method for the
linear model objects to get the prediction equation. Our results are
slightly different from those given by Myers and Montgomery because they
used prediction equations with full floating-point precision.
\end{quote}

\subsection{Contour Plot Generation}\label{sec-contour-plot-generation}

\subsubsection{Contour Plots for the Response Surface
Models}\label{contour-plots-for-the-response-surface-models}

We will generate contour plots for the percent conversion and thermal
activity models. The contour-plot generation comprehends the following
steps:

\begin{itemize}
\tightlist
\item
  generating a grid of points in the design space and evaluating the
  response surface models at these points, and
\item
  plotting the contour plots for the response surface models
\end{itemize}

We will use the function \texttt{mo\_generate\_plot\_grid} to generate
the grid and the function \texttt{mo\_contourf\_plots} for creating the
contour plots for the response surface models. Both functions are
available in the \texttt{spotpython} package.

First we define the variables, their ranges, the resolutions for the
grid, and the objective functions. The \texttt{variables} dictionary
contains the variable names as keys and their ranges as values. The
\texttt{resolutions} dictionary contains the variable names as keys and
their resolutions as values. The \texttt{functions} dictionary contains
the function names as keys and the corresponding functions as values.
Next we can generate the Pandas DataFrame \texttt{plot\_grid}. It has
the columns \texttt{time}, \texttt{temperature}, \texttt{catalyst},
\texttt{conversionPred}, and \texttt{activityPred}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{variables }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"time"}\NormalTok{: (}\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{, }\FloatTok{1.7}\NormalTok{),}
    \StringTok{"temperature"}\NormalTok{: (}\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{, }\FloatTok{1.7}\NormalTok{),}
    \StringTok{"catalyst"}\NormalTok{: (}\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{, }\FloatTok{1.7}\NormalTok{)}
\NormalTok{\}}
\NormalTok{resolutions }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"time"}\NormalTok{: }\DecValTok{50}\NormalTok{,}
    \StringTok{"temperature"}\NormalTok{: }\DecValTok{4}\NormalTok{,}
    \StringTok{"catalyst"}\NormalTok{: }\DecValTok{50}
\NormalTok{\}}
\NormalTok{functions }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"conversionPred"}\NormalTok{: conversion\_pred,}
    \StringTok{"activityPred"}\NormalTok{: activity\_pred}
\NormalTok{\}}
\NormalTok{plot\_grid }\OperatorTok{=}\NormalTok{ mo\_generate\_plot\_grid(variables, resolutions, functions)}
\end{Highlighting}
\end{Shaded}

Figure~\ref{fig-kuhn16a-2} shows the response surface for the percent
conversion model. To plot the model contours, the temperature variable
was fixed at four diverse levels. The largest effects in the fitted
model are due to the time \(\times\) catalyst interaction and the linear
and quadratic effects of catalyst. Figure~\ref{fig-kuhn16a-3} shows the
response surface for the thermal activity model. To plot the model
contours, the temperature variable was fixed at four diverse levels. The
main effects of time and catalyst have the largest effect on the fitted
model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"conversionPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,    }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-2-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-2}The response surface for the percent
conversion model. To plot the model contours, the temperature variable
was fixed at four diverse levels.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"activityPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-3-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-3}The response surface for the thermal
activity model. To plot the model contours, the temperature variable was
fixed at four diverse levels.}

\end{figure}%

\subsubsection{Defining the Desirability
Functions}\label{sec-defining-desirability}

Following the steps described in Kuhn (2016), translating the
experimental goals to desirability functions, a larger-is-better
function (\(d_r^{\text{max}}\)) is used for percent conversion with
values \(A = 80\) and \(B = 97\). A target-oriented desirability
function (\(d_r^{\text{target}}\)) was used for thermal activity with
\(t_0 = 57.5\), \(A = 55\), and \(B = 60\).

Kuhn emphasizes that to construct the overall desirability functions,
objects must be created for the individual functions. In the following,
we will use classes of the \texttt{Python} package
\texttt{spotdesirability} to create the desirability objects. The
\texttt{spotdesirability} package is part of the
\texttt{sequential\ parameter\ optimization} framework (Bartz-Beielstein
2023b). It is available on GitHub
{[}https://github.com/sequential-parameter-optimization/spotdesirability{]}
and on PyPi \url{https://pypi.org/project/spotdesirability} and can be
installed via \texttt{pip\ install\ spotdesirability}.

The desirability objects can be created as follows:

\phantomsection\label{kuhn16a-desirability-obj}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conversionD }\OperatorTok{=}\NormalTok{ DMax(}\DecValTok{80}\NormalTok{, }\DecValTok{97}\NormalTok{)}
\NormalTok{activityD }\OperatorTok{=}\NormalTok{ DTarget(}\DecValTok{55}\NormalTok{, }\FloatTok{57.5}\NormalTok{, }\DecValTok{60}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Although the original analysis in Myers, Montgomery, and Anderson-Cook
(2016) used numerous combinations of scaling parameters, following the
presentation in Kuhn (2016), we will only show analyses with the default
scaling factor values.

\begin{example}[Computing Desirability at the Center
Point]\protect\hypertarget{exm-desirability}{}\label{exm-desirability}

Using these desirability objects \texttt{conversionD}and
\texttt{activityD}, the following code segment shows how to predict the
desirability for the center point of the experimental design. The center
point is defined as {[}0, 0, 0{]}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_outcomes }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    conversion\_pred([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]),}
\NormalTok{    activity\_pred([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Predicted Outcomes:"}\NormalTok{, pred\_outcomes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Predicted Outcomes: [81.09, 59.85]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predict desirability for each outcome}
\NormalTok{conversion\_desirability }\OperatorTok{=}\NormalTok{ conversionD.predict(pred\_outcomes[}\DecValTok{0}\NormalTok{])}
\NormalTok{activity\_desirability }\OperatorTok{=}\NormalTok{ activityD.predict(pred\_outcomes[}\DecValTok{1}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Conversion Desirability:"}\NormalTok{, conversion\_desirability)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Activity Desirability:"}\NormalTok{, activity\_desirability)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Conversion Desirability: [0.06411765]
Activity Desirability: [0.06]
\end{verbatim}

Similar to the implementation in Kuhn (2016), to get the overall score
for these settings of the experimental factors, the \texttt{dOverall}
function is used to combine the objects and \texttt{predict} is used to
get the final score. The \texttt{print\_class\_attributes} method prints
the class attributes of the \texttt{DOverall} object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{overallD }\OperatorTok{=}\NormalTok{ DOverall(conversionD, activityD)}
\NormalTok{overallD.print\_class\_attributes()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Class: DOverall
d_objs: [

  Class: DMax
  low: 80
  high: 97
  scale: 1
  tol: None
  missing: 0.5

  Class: DTarget
  low: 55
  target: 57.5
  high: 60
  low_scale: 1
  high_scale: 1
  tol: None
  missing: 0.4949494949494951
]
\end{verbatim}

Note: The attribute \texttt{missing} is explained in
Section~\ref{sec-missing}.

Finally, we can print the overall desirability for the center point of
the experimental design.

\phantomsection\label{kuhn16a-desirability-predict-tree}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#] echo: true}
\NormalTok{overall\_desirability }\OperatorTok{=}\NormalTok{ overallD.predict(pred\_outcomes, }\BuiltInTok{all}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Conversion Desirability:"}\NormalTok{, overall\_desirability[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Activity Desirability:"}\NormalTok{, overall\_desirability[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Overall Desirability:"}\NormalTok{, overall\_desirability[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Conversion Desirability: [0.06411765]
Activity Desirability: [0.06]
Overall Desirability: [0.06202466]
\end{verbatim}

\end{example}

\subsubsection{Generating the Desirability
DataFrame}\label{generating-the-desirability-dataframe}

A DataFrame \texttt{d\_values\_df} is created to store the individual
desirability values for each outcome, and the overall desirability value
is added as a new column. First, we predict desirability values and
extract the individual and overall desirability values.

Note: The \texttt{all=True} argument indicates that both individual and
overall desirability values should be returned.

We add the individual and overall desirability values to the
\texttt{plot\_grid} DataFrame, that was created earlier in
Section~\ref{sec-contour-plot-generation}.

\phantomsection\label{kuhn16a-desirability-all-true}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_values }\OperatorTok{=}\NormalTok{ overallD.predict(plot\_grid.iloc[:, [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{]].values, }\BuiltInTok{all}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{individual\_desirabilities }\OperatorTok{=}\NormalTok{ d\_values[}\DecValTok{0}\NormalTok{]}
\NormalTok{overall\_desirability }\OperatorTok{=}\NormalTok{ d\_values[}\DecValTok{1}\NormalTok{]}
\NormalTok{d\_values\_df }\OperatorTok{=}\NormalTok{ pd.DataFrame(individual\_desirabilities).T  }
\NormalTok{d\_values\_df.columns }\OperatorTok{=}\NormalTok{ [}\StringTok{"D1"}\NormalTok{, }\StringTok{"D2"}\NormalTok{]}
\NormalTok{d\_values\_df[}\StringTok{"Overall"}\NormalTok{] }\OperatorTok{=}\NormalTok{ overall\_desirability}
\NormalTok{plot\_grid }\OperatorTok{=}\NormalTok{ pd.concat([plot\_grid, d\_values\_df], axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Contour Plots for the Desirability
Surfaces}\label{contour-plots-for-the-desirability-surfaces}

We will use \texttt{spotpython}'s \texttt{contourf\_plot} function to
create the contour plots for the individual desirability surfaces and
the overall desirability surface. The \texttt{plot\_grid} DataFrame
contains the predicted values for the conversion and activity models,
which are used to create the contour plots.

Figure~\ref{fig-kuhn16a-4}, Figure~\ref{fig-kuhn16a-5}, and
Figure~\ref{fig-kuhn16a-6} show contour plots of the individual
desirability function surfaces and the overall surface. These plots are
in correspondence with the figures in Kuhn (2016), but the color schemes
are different. The \texttt{plot\_grid} DataFrame contains the predicted
values for the conversion and activity models, which are used to create
the contour plots.

The individual desirability surface for the \texttt{percent\ conversion}
outcome is shown in Figure~\ref{fig-kuhn16a-4} and the individual
desirability surface for the thermal activity outcome is shown in
Figure~\ref{fig-kuhn16a-5}. Finally, the overall desirability surface is
shown in Figure~\ref{fig-kuhn16a-6}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    data}\OperatorTok{=}\NormalTok{plot\_grid,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{\textquotesingle{}time\textquotesingle{}}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{\textquotesingle{}catalyst\textquotesingle{}}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{\textquotesingle{}D1\textquotesingle{}}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{\textquotesingle{}temperature\textquotesingle{}}\NormalTok{,}
\NormalTok{    aspect}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    as\_table}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)    }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-4-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-4}The individual desirability surface for
the percent conversion outcome using \texttt{dMax(80,\ 97)}}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    data}\OperatorTok{=}\NormalTok{plot\_grid,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{\textquotesingle{}time\textquotesingle{}}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{\textquotesingle{}catalyst\textquotesingle{}}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{\textquotesingle{}D2\textquotesingle{}}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{\textquotesingle{}temperature\textquotesingle{}}\NormalTok{,}
\NormalTok{    aspect}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    as\_table}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-5-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-5}The individual desirability surface for
the thermal activity outcome using \texttt{dTarget(55,\ 57.5,\ 60)}}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    data}\OperatorTok{=}\NormalTok{plot\_grid,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{\textquotesingle{}time\textquotesingle{}}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{\textquotesingle{}catalyst\textquotesingle{}}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{\textquotesingle{}Overall\textquotesingle{}}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{\textquotesingle{}temperature\textquotesingle{}}\NormalTok{,}
\NormalTok{    aspect}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    as\_table}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-6-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-6}The overall desirability surface for the
combined outcomes of percent conversion and thermal activity}

\end{figure}%

\section{Multi-Objective Optimization and Maximizing
Desirability}\label{sec-maximizing-desirability}

Kuhn indicates that as described by Myers, Montgomery, and Anderson-Cook
(2016), desirability can be maximized within a cuboidal region defined
by the axial point values. The objective function (\texttt{rsmOpt})
utilizes a penalty approach: if a candidate point extends beyond the
cuboidal design region, desirability is set to zero. These penalties are
implemented in the \texttt{rsm\_opt} function, which is used to optimize
the desirability function. An \(\alpha\) value of 1.682
(\(\approx (2^k)^(1/4)\) with \(k=3\) in our case), see Montgomery
(2001), is used as the limit for both circular and square spaces. After
checking the bounds, predictions for all provided functions are
calculated, and the overall desirability is predicted using the
\texttt{predict} method of the \texttt{DOverall} object. The negative
desirability is returned to maximize the desirability function.

\phantomsection\label{kuhn16a-optimization-rsm}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rsm\_opt(x, d\_object, prediction\_funcs, space}\OperatorTok{=}\StringTok{"square"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{1.682}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{float}\NormalTok{:}
    \ControlFlowTok{if}\NormalTok{ space }\OperatorTok{==} \StringTok{"circular"}\NormalTok{:}
        \ControlFlowTok{if}\NormalTok{ np.sqrt(np.}\BuiltInTok{sum}\NormalTok{(np.array(x) }\OperatorTok{**} \DecValTok{2}\NormalTok{)) }\OperatorTok{\textgreater{}}\NormalTok{ alpha:}
            \ControlFlowTok{return} \FloatTok{0.0}
    \ControlFlowTok{elif}\NormalTok{ space }\OperatorTok{==} \StringTok{"square"}\NormalTok{:}
        \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{any}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(np.array(x)) }\OperatorTok{\textgreater{}}\NormalTok{ alpha):}
            \ControlFlowTok{return} \FloatTok{0.0}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"space must be \textquotesingle{}square\textquotesingle{} or \textquotesingle{}circular\textquotesingle{}"}\NormalTok{)}
\NormalTok{    predictions }\OperatorTok{=}\NormalTok{ [func(x) }\ControlFlowTok{for}\NormalTok{ func }\KeywordTok{in}\NormalTok{ prediction\_funcs]}
\NormalTok{    desirability }\OperatorTok{=}\NormalTok{ d\_object.predict(np.array([predictions]))}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{desirability}
\end{Highlighting}
\end{Shaded}

Note: Instead of using the penatlty approach, alternatively the
desirability function for box-constraints can be used, see
Section~\ref{sec-box-constraints}. Furthermore, \texttt{scipy.optimize}
provides a \texttt{bounds} argument for some optimizers to restrict the
search space.

Kuhn (2016) used \texttt{R}'s \texttt{optim} function to implement the
Nelder-Mead simplex method (Nelder and Mead 1965; Olsson and Nelson
1975). This direct search method relies on function evaluations without
using gradient information. Although this method may converge to a local
optimum, it is fast with efficient functions, allowing for multiple
feasible region restarts to find the best result. Alternatively, methods
like simulated annealing (Bohachevsky 1986), also available in
\texttt{R}'s \texttt{optim} function, might better suit global optimum
searches, though they might need parameter tuning for effective
performance. We will use the \texttt{scipy.optimize.minimize} function
to implement the Nelder-Mead simplex method in Python.

Putting the pieces together, the following code segment shows how to
create the desirability objects and use them in the optimization
process. First, a \texttt{search\_grid} is created using
\texttt{numpy}'s \texttt{meshgrid} function to generate a grid of
restarts points in the design space. For each (restart) point in the
search grid, the \texttt{rsm\_opt} function is called to calculate the
desirability for that point. The \texttt{conversion\_pred} and
\texttt{activity\_pred} functions are used as prediction functions, and
the \texttt{DOverall} object is created using the individual
desirability objects for conversion and activity. The \texttt{overallD}
(overall desirability) is passed to tne \texttt{rsm\_opt} function. The
\texttt{minimize} function from \texttt{scipy.optimize} is used to find
the optimal parameters that minimize the negative desirability.

\phantomsection\label{kuhn16a-optimization}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{time }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{temperature }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{catalyst }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{5}\NormalTok{)}

\NormalTok{search\_grid }\OperatorTok{=}\NormalTok{ pd.DataFrame(}
\NormalTok{    np.array(np.meshgrid(time, temperature, catalyst)).T.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{),}
\NormalTok{    columns}\OperatorTok{=}\NormalTok{[}\StringTok{"time"}\NormalTok{, }\StringTok{"temperature"}\NormalTok{, }\StringTok{"catalyst"}\NormalTok{]}
\NormalTok{)}

\CommentTok{\# List of prediction functions}
\NormalTok{prediction\_funcs }\OperatorTok{=}\NormalTok{ [conversion\_pred, activity\_pred]}

\CommentTok{\# Individual desirability objects}
\NormalTok{conversionD }\OperatorTok{=}\NormalTok{ DMax(}\DecValTok{80}\NormalTok{, }\DecValTok{97}\NormalTok{)}
\NormalTok{activityD }\OperatorTok{=}\NormalTok{ DTarget(}\DecValTok{55}\NormalTok{, }\FloatTok{57.5}\NormalTok{, }\DecValTok{60}\NormalTok{)}

\CommentTok{\# Desirability object (DOverall)}
\NormalTok{overallD }\OperatorTok{=}\NormalTok{ DOverall(conversionD, activityD)}

\CommentTok{\# Initialize the best result}
\NormalTok{best }\OperatorTok{=} \VariableTok{None}

\CommentTok{\# Perform optimization for each point in the search grid}
\ControlFlowTok{for}\NormalTok{ i, row }\KeywordTok{in}\NormalTok{ search\_grid.iterrows():}
\NormalTok{    initial\_guess }\OperatorTok{=}\NormalTok{ row.values  }\CommentTok{\# Initial guess for optimization}

    \CommentTok{\# Perform optimization using scipy\textquotesingle{}s minimize function}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ minimize(}
\NormalTok{        rsm\_opt,}
\NormalTok{        initial\_guess,}
\NormalTok{        args}\OperatorTok{=}\NormalTok{(overallD, prediction\_funcs, }\StringTok{"square"}\NormalTok{), }
\NormalTok{        method}\OperatorTok{=}\StringTok{"Nelder{-}Mead"}\NormalTok{,}
\NormalTok{        options}\OperatorTok{=}\NormalTok{\{}\StringTok{"maxiter"}\NormalTok{: }\DecValTok{1000}\NormalTok{, }\StringTok{"disp"}\NormalTok{: }\VariableTok{False}\NormalTok{\}}
\NormalTok{    )}

    \CommentTok{\# Update the best result if necessary}
    \CommentTok{\# Compare based on the negative desirability}
    \ControlFlowTok{if}\NormalTok{ best }\KeywordTok{is} \VariableTok{None} \KeywordTok{or}\NormalTok{ result.fun }\OperatorTok{\textless{}}\NormalTok{ best.fun:}
\NormalTok{        best }\OperatorTok{=}\NormalTok{ result}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best Parameters:"}\NormalTok{, best.x)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best Desirability:"}\NormalTok{, }\OperatorTok{{-}}\NormalTok{best.fun)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Best Parameters: [-0.51207663  1.68199987 -0.58609664]
Best Desirability: 0.9425092694688632
\end{verbatim}

Using these best parameters, the predicted values for conversion and
activity can be calculated as follows:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Conversion pred(x): }\SpecialCharTok{\{}\NormalTok{conversion\_pred(best.x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Activity pred(x): }\SpecialCharTok{\{}\NormalTok{activity\_pred(best.x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Conversion pred(x): 95.10150374903237
Activity pred(x): 57.49999992427212
\end{verbatim}

We extract the best temperature from the best parameters and remove it
from the best parameters for plotting. The \texttt{best.x} array
contains the best parameters found by the optimizer, where the second
element corresponds to the temperature variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_temperature }\OperatorTok{=}\NormalTok{ best.x[}\DecValTok{1}\NormalTok{]}
\NormalTok{best\_point }\OperatorTok{=}\NormalTok{ np.delete(best.x, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we set the values of \texttt{temperature} to the best temperature
in the \texttt{plot\_grid\_df} and recalculate the predicted values for
\texttt{conversion} and \texttt{activity} using the
\texttt{conversion\_pred} and \texttt{activity\_pred} functions. A copy
of the \texttt{plot\_grid} DataFrame is created, and the
\texttt{temperature} column is updated with the best temperature value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_grid\_best }\OperatorTok{=}\NormalTok{ plot\_grid.copy()}
\NormalTok{plot\_grid\_best[}\StringTok{"temperature"}\NormalTok{] }\OperatorTok{=}\NormalTok{ best\_temperature}
\NormalTok{plot\_grid\_best[}\StringTok{"conversionPred"}\NormalTok{] }\OperatorTok{=}\NormalTok{ conversion\_pred(plot\_grid\_best[[}\StringTok{"time"}\NormalTok{,}
     \StringTok{"temperature"}\NormalTok{, }\StringTok{"catalyst"}\NormalTok{]].values.T)}
\NormalTok{plot\_grid\_best[}\StringTok{"activityPred"}\NormalTok{] }\OperatorTok{=}\NormalTok{ activity\_pred(plot\_grid\_best[[}\StringTok{"time"}\NormalTok{,}
    \StringTok{"temperature"}\NormalTok{, }\StringTok{"catalyst"}\NormalTok{]].values.T)}
\end{Highlighting}
\end{Shaded}

Now we are ready to plot the response surfaces for the best parameters
found by the optimizer. The \texttt{contourf\_plot} function is used to
create the contour plots for the response surface models. The
\texttt{highlight\_point} argument is used to highlight the best point
found by the optimizer in the contour plots. First, the response surface
for the \texttt{percent\ conversion} model is plotted. The temperature
variable is fixed at the best value found by the optimizer, see
Figure~\ref{fig-kuhn16a-best-conversion}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid\_best,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"conversionPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,}
\NormalTok{    highlight\_point}\OperatorTok{=}\NormalTok{best\_point,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-best-conversion-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-best-conversion}The response surface for the
percent conversion model. To plot the model contours, the temperature
variable was fixed at the best value found by the optimizer.}

\end{figure}%

Second, the response surface for the \texttt{thermal\ activity} model is
plotted. The temperature variable is fixed at the best value found by
the optimizer, see Figure~\ref{fig-kuhn16a-best-activity}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid\_best,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"activityPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,}
\NormalTok{    highlight\_point}\OperatorTok{=}\NormalTok{best\_point,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-best-activity-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-best-activity}The response surface for the
thermal activity model. To plot the model contours, the temperature
variable was fixed at the best value found by the optimizer.}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Analysing the Best Values From the Nelder-Mead Optimizer}]

\begin{itemize}
\tightlist
\item
  Objective function values for the best parameters found by the
  optimizer are:

  \begin{itemize}
  \tightlist
  \item
    \texttt{conversion} = 95.1
  \item
    \texttt{activity} = 57.5
  \end{itemize}
\item
  The best value for the percent conversion should be maximized, as
  defined in \texttt{conversionD\ =\ DMax(80,\ 97)}. Here, we have
  obtained a value of 95.1, which is close to the maximum value of 97.
\item
  Since we are using the desirabilty function \texttt{DTarget}, the
  values for the \texttt{thermal\ activity} should not be maximized, but
  should be close to the target. The setting
  \texttt{activityD\ =\ DTarget(55,\ 57.5,\ 60)}, as defined in
  Section~\ref{sec-defining-desirability}, states that the best value
  for the \texttt{thermal\ activity} should be close to 57.5 as
  specified by the user (and not at its maximum). Here, we have obtained
  a value of 57.5, which is exactly the target value.
\end{itemize}

\end{tcolorbox}

An alternative approach to the optimization process is to use a circular
design region instead of a cuboidal design region can be found in the
Appendix.

\section{Surrogate-Model Based Optimization Using
Desirability}\label{sec-surrogate}

\texttt{spotpython} implements a vectorized function
\texttt{fun\_myer16a()} that computes the two objective functions for
\texttt{conversion} and \texttt{activity}. To illustrate the vectorized
evaluation, we will use two input points: the center point of the design
space and the best point found by the optimizer from
Section~\ref{sec-maximizing-desirability}. The \texttt{fun\_myer16a()}
function takes a 2D array as input, where each row corresponds to a
different set of parameters. The function returns a 2D array with the
predicted values for \texttt{conversion} and \texttt{activity}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], best.x])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun\_myer16a(X)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Objective function values:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Objective function values:
[[81.09       59.85      ]
 [95.10150375 57.49999992]]
\end{verbatim}

Next, we define the desirability objects. This step is identical to the
previous one, where we defined the desirability functions for
\texttt{conversion} and \texttt{activity}. The \texttt{DMax} function is
used for the \texttt{conversion} function, and the \texttt{DTarget}
function is used for the \texttt{activity} function. The
\texttt{DOverall} function is used to combine the two desirability
functions into an overall desirability function. The \texttt{DOverall}
function takes two arguments: the desirability object for
\texttt{conversion} and the desirability object for \texttt{activity}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotdesirability.utils.desirability }\ImportTok{import}\NormalTok{ DOverall, DMax, DTarget}
\NormalTok{conversionD }\OperatorTok{=}\NormalTok{ DMax(}\DecValTok{80}\NormalTok{, }\DecValTok{97}\NormalTok{)}
\NormalTok{activityD }\OperatorTok{=}\NormalTok{ DTarget(}\DecValTok{55}\NormalTok{, }\FloatTok{57.5}\NormalTok{, }\DecValTok{60}\NormalTok{)}
\NormalTok{overallD }\OperatorTok{=}\NormalTok{ DOverall(conversionD, activityD)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conversionD.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-desirability-plot-conversion-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-desirability-plot-conversion}The
desirability function for the conversion outcome.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{activityD.plot()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-desirability-plot-activity-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-desirability-plot-activity}The desirability
function for the activity outcome.}

\end{figure}%

Predicting the desirability for each outcome can also be vectorized. The
\texttt{predict} method of the desirability objects can take a 2D array
as input, where each row corresponds to a different set of parameters.
The method returns a 1D array with the predicted desirability values for
each set of parameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conversion\_desirability }\OperatorTok{=}\NormalTok{ conversionD.predict(y[:,}\DecValTok{0}\NormalTok{])}
\NormalTok{activity\_desirability }\OperatorTok{=}\NormalTok{ activityD.predict(y[:,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Conversion Desirability: }\SpecialCharTok{\{}\NormalTok{conversion\_desirability}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Activity Desirability: }\SpecialCharTok{\{}\NormalTok{activity\_desirability}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Conversion Desirability: [0.06411765 0.88832375]
Activity Desirability: [0.06       0.99999997]
\end{verbatim}

The \texttt{overall\_desirability} variable contains the overall
desirability values for each set of parameters. The \texttt{all=True}
argument indicates that we want to return both the individual
desirability values and the overall desirability value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{overall\_desirability }\OperatorTok{=}\NormalTok{ overallD.predict(y, }\BuiltInTok{all}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"OverallD: }\SpecialCharTok{\{}\NormalTok{overall\_desirability}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
OverallD: ([array([0.06411765, 0.88832375]), array([0.06      , 0.99999997])], array([0.06202466, 0.94250927]))
\end{verbatim}

During the surrogate-model based optimization, the argument \texttt{all}
is set to \texttt{False}, because \texttt{spotpython} does not need the
individual desirability values.

Now we have introduced all elements needed to perform surrogate-model
based optimization using desirability functions and the
\texttt{spotpython} package.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Maximization and Minimization}]

\begin{itemize}
\tightlist
\item
  Since \texttt{spotpython} uses minimization, but desirability should
  be maximized, \texttt{fun\_desirability} is defined to return
  \texttt{1\ -\ overall\_desirability}.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_desirability(X, }\OperatorTok{**}\NormalTok{kwargs):}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ fun\_myer16a(X)}
\NormalTok{    conversionD }\OperatorTok{=}\NormalTok{ DMax(}\DecValTok{80}\NormalTok{, }\DecValTok{97}\NormalTok{)}
\NormalTok{    activityD }\OperatorTok{=}\NormalTok{ DTarget(}\DecValTok{55}\NormalTok{, }\FloatTok{57.5}\NormalTok{, }\DecValTok{60}\NormalTok{)}
\NormalTok{    overallD }\OperatorTok{=}\NormalTok{ DOverall(conversionD, activityD)}
\NormalTok{    overall\_desirability }\OperatorTok{=}\NormalTok{ overallD.predict(y, }\BuiltInTok{all}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
    \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ overall\_desirability}
\end{Highlighting}
\end{Shaded}

We can test the function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], best.x])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun\_desirability(X)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Objective function values:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Objective function values:
[0.93797534 0.05749073]
\end{verbatim}

As expected, the output contains the two overall ``1 minus
desirability'' function values for the center point of the design space
and the best point found by the optimizer.

We are now ready to perform the surrogate-model based optimization using
desirability functions. The \texttt{spotpython} package provides a class
\texttt{Spot} that implements the surrogate-model based optimization
algorithm. The \texttt{Spot} class takes the objective function and the
control parameters as input. The control parameters define the search
space and other settings for the optimization process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{              lower }\OperatorTok{=}\NormalTok{ np.array( [}\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{]),}
\NormalTok{              upper }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.7}\NormalTok{, }\FloatTok{1.7}\NormalTok{, }\FloatTok{1.7}\NormalTok{]),}
\NormalTok{              var\_name }\OperatorTok{=}\NormalTok{ [}\StringTok{"time"}\NormalTok{, }\StringTok{"temperature"}\NormalTok{, }\StringTok{"catalyst"}\NormalTok{],}
\NormalTok{              fun\_evals}\OperatorTok{=} \DecValTok{50}
\NormalTok{)}
\NormalTok{surrogate\_control }\OperatorTok{=}\NormalTok{ surrogate\_control\_init(n\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{15}\NormalTok{)}
\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun\_desirability,         }
\NormalTok{         fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{         surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control,}
\NormalTok{         design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{S.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.16080910335362375 [###-------] 32.00% 
spotpython tuning: 0.16080910335362375 [###-------] 34.00% 
spotpython tuning: 0.15281120812431714 [####------] 36.00% 
spotpython tuning: 0.15281120812431714 [####------] 38.00% 
spotpython tuning: 0.11242908322548573 [####------] 40.00% 
spotpython tuning: 0.11242908322548573 [####------] 42.00% 
spotpython tuning: 0.06984744743002758 [####------] 44.00% 
spotpython tuning: 0.06984744743002758 [#####-----] 46.00% 
spotpython tuning: 0.060454619901722295 [#####-----] 48.00% 
spotpython tuning: 0.057463170449749246 [#####-----] 50.00% 
spotpython tuning: 0.05575605401032391 [#####-----] 52.00% 
spotpython tuning: 0.053811328711583006 [#####-----] 54.00% 
spotpython tuning: 0.05231885350770027 [######----] 56.00% 
spotpython tuning: 0.05231885350770027 [######----] 58.00% 
spotpython tuning: 0.0515537446788461 [######----] 60.00% 
spotpython tuning: 0.0515537446788461 [######----] 62.00% 
spotpython tuning: 0.0515537446788461 [######----] 64.00% 
spotpython tuning: 0.0515537446788461 [#######---] 66.00% 
spotpython tuning: 0.0515537446788461 [#######---] 68.00% 
spotpython tuning: 0.0515537446788461 [#######---] 70.00% 
spotpython tuning: 0.0515537446788461 [#######---] 72.00% 
spotpython tuning: 0.0515537446788461 [#######---] 74.00% 
spotpython tuning: 0.0515537446788461 [########--] 76.00% 
spotpython tuning: 0.0515537446788461 [########--] 78.00% 
spotpython tuning: 0.0515537446788461 [########--] 80.00% 
spotpython tuning: 0.0515537446788461 [########--] 82.00% 
spotpython tuning: 0.0515537446788461 [########--] 84.00% 
spotpython tuning: 0.0515537446788461 [#########-] 86.00% 
spotpython tuning: 0.0515537446788461 [#########-] 88.00% 
spotpython tuning: 0.0515537446788461 [#########-] 90.00% 
spotpython tuning: 0.0515537446788461 [#########-] 92.00% 
spotpython tuning: 0.0515537446788461 [#########-] 94.00% 
spotpython tuning: 0.0515537446788461 [##########] 96.00% 
spotpython tuning: 0.0515537446788461 [##########] 98.00% 
spotpython tuning: 0.0515537446788461 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\phantomsection\label{kuhn16a-spot}
\begin{verbatim}
<spotpython.spot.spot.Spot at 0x1676cf590>
\end{verbatim}

The progress of the optimization process can be visualized using the
\texttt{plot\_progress} method (Figure~\ref{fig-kuhn16a-spot-progress}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-spot-progress-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-spot-progress}The progress of the
surrogate-model based optimization using desirability functions. The
y-axis is on a logarithmic scale.}

\end{figure}%

We can analyze the results in detail by accessing the attributes of the
\texttt{Spot} object directly. The \texttt{min\_X} attribute contains
the best parameters found by the optimizer, and the \texttt{min\_y}
attribute contains the best desirability value. First, we take a look at
the desirability values for the best parameters found by the optimizer.
The \texttt{min\_y} attribute contains the best desirability value.
Note, we have to compute 1 minus the \texttt{min\_y} value, because the
\texttt{fun\_desirability} function returns
\texttt{1\ -\ overall\_desirability}. This results in the following best
desirability value:

\begin{verbatim}
Best Desirability: 0.9484462553211539
\end{verbatim}

We can use the \texttt{min\_X} attribute to calculate the predicted
values for \texttt{conversion} and \texttt{activity} for the best
parameters found by the optimizer. Using the \texttt{fun\_myer16a}
function, we can calculate these predicted values.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best Parameters: }\SpecialCharTok{\{}\NormalTok{S}\SpecialCharTok{.}\NormalTok{min\_X}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best Conversion: }\SpecialCharTok{\{}\NormalTok{best\_conversion}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best Activity: }\SpecialCharTok{\{}\NormalTok{best\_activity}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Best Parameters: [-0.58701584  1.7        -0.53228512]
Best Conversion: 95.29515994576552
Best Activity: 57.49954154470773
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Analysing the Best Values from the spotpython Optimizer}]

\begin{itemize}
\tightlist
\item
  Objective function values for the best parameters found by the
  optimizer are very close to the values found by the Nelder-Mead
  optimizer.
\end{itemize}

\end{tcolorbox}

Based on the information from the surrogate, which is by default a
Kriging model in \texttt{spotpython}, we can analyze the importance of
the parameters in the optimization process. The
\texttt{plot\_importance} method plots the importance of each parameter,
see Figure~\ref{fig-spot-importance}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-spot-importance-output-1.pdf}}

}

\caption{\label{fig-spot-importance}The importance of the parameters in
the optimization process}

\end{figure}%

The \texttt{plot\_important\_hyperparameter\_contour} method plots the
contour plots for the important parameters. The results are shown in
Figure~\ref{fig-spot-importance-contour}. The contour plots show the
importance of the parameters in the optimization process, which tries to
minimize the 1 minus desirability values. Regions with low values
present high desirability. Note: These surface plots illustrate how the
Kriging surrogate ``sees the world'' and decides where to sample next.
The Kriging model computes the following importance values:

\begin{verbatim}
time:  100.00000000000001
temperature:  2.8977946407851523
catalyst:  99.11238108829903
\end{verbatim}

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-spot-importance-contour-output-2.pdf}}

}

\subcaption{\label{fig-spot-importance-contour-1}The contour plots for
the important parameters}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-spot-importance-contour-output-3.pdf}}

}

\subcaption{\label{fig-spot-importance-contour-2}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-spot-importance-contour-output-4.pdf}}

}

\subcaption{\label{fig-spot-importance-contour-3}}

}

\caption{\label{fig-spot-importance-contour}}

\end{figure}%

Finally, we show a comparison with the response-surface model. Similar
to the procedure above, we generate the \texttt{plot\_grid} DataFrame
for the response surface models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_x }\OperatorTok{=}\NormalTok{ S.min\_X}
\NormalTok{best\_point }\OperatorTok{=}\NormalTok{ np.delete(best\_x, }\DecValTok{1}\NormalTok{)}
\NormalTok{best\_temperature }\OperatorTok{=}\NormalTok{ best\_x[}\DecValTok{1}\NormalTok{]}

\NormalTok{variables }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"time"}\NormalTok{: (}\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{, }\FloatTok{1.7}\NormalTok{),}
    \StringTok{"temperature"}\NormalTok{: (best\_temperature, best\_temperature),}
    \StringTok{"catalyst"}\NormalTok{: (}\OperatorTok{{-}}\FloatTok{1.7}\NormalTok{, }\FloatTok{1.7}\NormalTok{)}
\NormalTok{\}}

\NormalTok{resolutions }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"time"}\NormalTok{: }\DecValTok{50}\NormalTok{,}
    \StringTok{"temperature"}\NormalTok{: }\DecValTok{1}\NormalTok{,}
    \StringTok{"catalyst"}\NormalTok{: }\DecValTok{50}
\NormalTok{\}}

\NormalTok{functions }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"conversionPred"}\NormalTok{: conversion\_pred,}
    \StringTok{"activityPred"}\NormalTok{: activity\_pred}
\NormalTok{\}}

\NormalTok{plot\_grid }\OperatorTok{=}\NormalTok{ mo\_generate\_plot\_grid(variables, resolutions, functions)}
\end{Highlighting}
\end{Shaded}

Usintg the \texttt{plot\_grid} DataFrame, we generate contour plots
shown in Figure~\ref{fig-kuhn16a-2-surrogate} and
Figure~\ref{fig-kuhn16a-surrogate-3}. The largest effects in the fitted
model are due to the time \(\\times\) catalyst interaction and the
linear and quadratic effects of catalyst. The
Figure~\ref{fig-kuhn16a-surrogate-3} shows the response surface for the
thermal activity model. To plot the model contours, the temperature
variable was fixed at four diverse levels. The main effects of time and
catalyst have the largest effect on the fitted model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"conversionPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,}
\NormalTok{    highlight\_point}\OperatorTok{=}\NormalTok{best\_point,    }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-2-surrogate-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-2-surrogate}The response surface for the
percent conversion model. To plot the model contours, the temperature
variable was fixed at four diverse levels.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"activityPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,}
\NormalTok{    highlight\_point}\OperatorTok{=}\NormalTok{best\_point,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-surrogate-3-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-surrogate-3}The response surface for the
thermal activity model. To plot the model contours, the temperature
variable was fixed at four diverse levels.}

\end{figure}%

\section{Surrogate Model Hyperparameter
Tuning}\label{sec-hyperparameter-tuning}

This section compares three different approaches to hyperparameter
tuning using the \texttt{spotpython} package. The first approach is a
single-objective approach, where only the first objective function is
used for hyperparameter tuning. The second approach is a weighted
multi-objective approach, where a weighted mean of both objective
functions is used for hyperparameter tuning. The third approach uses a
desirability function to combine the two objective functions into a
single objective function. The desirability function is used to maximize
the desirability of the two objective functions.

The \texttt{spotpython} package provides a method for hyperparameter
tuning using a surrogate model. We will extend the single-objective
example ``Hyperparameter Tuning with spotpython and PyTorch Lightning
for the Diabetes Data Set'' from the hyperparameter tuning cookbook
(Bartz-Beielstein 2023b) in the follwoing way:

Instead of using a single-objective function, which returns the
\texttt{validation\ loss} from the neural-network training, we will use
a multi-objective function that returns two objectives:

\begin{itemize}
\tightlist
\item
  \texttt{validation\ loss} and
\item
  \texttt{number\ of\ epochs}.
\end{itemize}

Clearly, both objectives should be minimized. The
\texttt{validation\ loss} should be minimized to get the best model, and
the \texttt{number\ of\ epochs} should be minimized to reduce the
training time. However, if the number of training epochs is too small,
the model will not be trained properly. Therefore, we will adopt the
desirability function for the \texttt{number\ of\ epochs} accordingly.

\phantomsection\label{des-imports}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The following process can be used for the hyperparameter tuning of the
\texttt{Diabetes} data set using the \texttt{spotpython} package. The
\texttt{Diabetes} data set is a regression data set that contains 10
input features and a single output feature. The goal is to predict the
output feature based on the input features.

After importing the necessary libraries, the \texttt{fun\_control}
dictionary is set up via the \texttt{fun\_control\_init} function. The
\texttt{fun\_control} dictionary contains

\begin{itemize}
\tightlist
\item
  \texttt{PREFIX}: a unique identifier for the experiment
\item
  \texttt{fun\_evals}: the number of function evaluations
\item
  \texttt{max\_time}: the maximum run time in minutes
\item
  \texttt{data\_set}: the data set. Here we use the \texttt{Diabetes}
  data set that is provided by \texttt{spotpython}.
\item
  \texttt{core\_model\_name}: the class name of the neural network
  model. This neural network model is provided by \texttt{spotpython}.
\item
  \texttt{hyperdict}: the hyperparameter dictionary. This dictionary is
  used to define the hyperparameters of the neural network model. It is
  also provided by \texttt{spotpython}.
\item
  \texttt{\_L\_in}: the number of input features. Since the
  \texttt{Diabetes} data set has 10 features, \texttt{\_L\_in} is set to
  10.
\item
  \texttt{\_L\_out}: the number of output features. Since we want to
  predict a single value, \texttt{\_L\_out} is set to 1.
\end{itemize}

The \texttt{HyperLight} class is used to define the objective function
\texttt{fun}. It connects the \texttt{PyTorch} and the
\texttt{spotpython} methods and is provided by \texttt{spotpython}.
Details can be found in the hyperparameter tuning cookbook
(Bartz-Beielstein 2023b) or online
\url{https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/}.

\subsection{The Single-Objective
Approach}\label{the-single-objective-approach}

The simplest way for handling multi-objective results is to simply
ignore all but the first objective function. This is done by setting the
\texttt{fun\_mo2so} argument in the \texttt{fun\_control\_init} function
to \texttt{None}. The \texttt{fun\_mo2so} argument is used to convert
the multi-objective function to a single-objective function. If it is
set to \texttt{None}, the first objective function is used as the
single-objective function. Since the \texttt{None} is also the default,
no argument is needed for the single-objective approach.

\phantomsection\label{des_spotpython_init}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"0000\_no\_mo"}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
    \CommentTok{\# do not run, if a result file exists}
\NormalTok{    force\_run}\OperatorTok{=}\VariableTok{False}\NormalTok{,    }
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ MoHyperLight().fun}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNLinearRegressor
\end{verbatim}

The method \texttt{set\_hyperparameter} allows the user to modify
default hyperparameter settings. Here we modify some hyperparameters to
keep the model small and to decrease the tuning time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{10}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
| name           | type   | default   |   lower |   upper | transform             |
|----------------|--------|-----------|---------|---------|-----------------------|
| l1             | int    | 3         |     3   |   4     | transform_power_2_int |
| epochs         | int    | 4         |     3   |  10     | transform_power_2_int |
| batch_size     | int    | 4         |     4   |  11     | transform_power_2_int |
| act_fn         | factor | ReLU      |     0   |   5     | None                  |
| optimizer      | factor | SGD       |     0   |   2     | None                  |
| dropout_prob   | float  | 0.01      |     0   |   0.025 | None                  |
| lr_mult        | float  | 1.0       |     0.1 |  10     | None                  |
| patience       | int    | 2         |     2   |   7     | transform_power_2_int |
| batch_norm     | factor | 0         |     0   |   1     | None                  |
| initialization | factor | Default   |     0   |   4     | None                  |
\end{verbatim}

Finally, a \texttt{Spot} object is created. Calling the method
\texttt{run()} starts the hyperparameter tuning process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{S.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Result file 0000_no_mo_res.pkl exists. Loading the result.
Loaded experiment from 0000_no_mo_res.pkl
\end{verbatim}

\phantomsection\label{des_run}
\begin{verbatim}
<spotpython.spot.spot.Spot at 0x16424fc80>
\end{verbatim}

Figure~\ref{fig-plain_results} shows the hyperparameter tuning process.
The loss and epochs are plotted versus the function evaluations. The
x-axis shows the number of function evaluations, and the y-axis shows
the loss and epochs. The loss is plotted in blue, and the epochs are
plotted in red. The y-axis is set to a logarithmic scale for better
visualization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loss }\OperatorTok{=}\NormalTok{ S.y\_mo[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{epochs }\OperatorTok{=}\NormalTok{ S.y\_mo[:, }\DecValTok{1}\NormalTok{]}
\NormalTok{iterations }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(loss) }\OperatorTok{+} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Iterations (x{-}axis)}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plt.plot(iterations, loss, label}\OperatorTok{=}\StringTok{"Loss"}\NormalTok{, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{)}
\NormalTok{plt.plot(iterations, epochs, label}\OperatorTok{=}\StringTok{"Epochs"}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{, marker}\OperatorTok{=}\StringTok{"x"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Iterations"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Values"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Loss and Epochs vs. Iterations"}\NormalTok{)}
\NormalTok{plt.yscale(}\StringTok{"log"}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{, which}\OperatorTok{=}\StringTok{"both"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"{-}{-}"}\NormalTok{, linewidth}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-plain_results-output-1.pdf}}

}

\caption{\label{fig-plain_results}Results of the hyperparameter tuning
process. Loss and epochs are plotted versus the function evaluations.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ S.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 2965.705078125
l1: 3.0
epochs: 9.0
batch_size: 6.0
act_fn: 3.0
optimizer: 1.0
dropout_prob: 0.014940301372259645
lr_mult: 8.554331694855211
patience: 3.0
batch_norm: 0.0
initialization: 3.0
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Results from the Single-Objective Approach}]

\begin{itemize}
\tightlist
\item
  The single-objective approach reulsted in a validation loss of
  \texttt{2890} and \texttt{1024} (\(=2^{10}\)) epochs.
\end{itemize}

\end{tcolorbox}

\subsection{Weighted Multi-Objective
Function}\label{weighted-multi-objective-function}

The second approach is to use a weighted mean of both objective
functions. This is done by setting the \texttt{fun\_mo2so} argument in
the \texttt{fun\_control\_init} function to a custom function that
computes the weighted mean of both objective functions. The weights can
be adjusted to give more importance to one objective function over the
other. Here, we define the function \texttt{aggregate} that computes the
weighted mean of both objective functions. The first objective function
is weighted with 2 and the second objective function is weighted with
0.1.

\phantomsection\label{des-mohyperlight-0001-agg}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"0001\_aggregate"}

\CommentTok{\# Weight first objective with 2, second with 1/10}
\KeywordTok{def}\NormalTok{ aggregate(y):}
    \ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(y}\OperatorTok{*}\NormalTok{np.array([}\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{]), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
    \CommentTok{\# do not run, if a result file exists}
\NormalTok{    force\_run}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    fun\_mo2so}\OperatorTok{=}\NormalTok{aggregate,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNLinearRegressor
\end{verbatim}

The remaining code is identical to the single-objective approach. The
only difference is that the \texttt{fun\_mo2so} argument is set to the
\texttt{aggregate} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{10}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{20}\NormalTok{)}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{S.run()    }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Result file 0001_aggregate_res.pkl exists. Loading the result.
Loaded experiment from 0001_aggregate_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x165cbc800>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ S.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 5756.735546875
l1: 4.0
epochs: 7.0
batch_size: 8.0
act_fn: 2.0
optimizer: 1.0
dropout_prob: 0.015182309694564699
lr_mult: 4.398603126890015
patience: 4.0
batch_norm: 0.0
initialization: 2.0
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Results from the Weighted Multi-Objective Function Approach}]

\begin{itemize}
\tightlist
\item
  The weighted multi-objective approach reulsted in a validation loss of
  \texttt{5824} and \texttt{64} (\(=2^{6}\)) epochs.
\item
  Although the number of epochs is smaller than in the single-objective
  approach, the validation loss is larger.
\item
  This is an inherent problem of weighted multi-objective approaches,
  because the deteriination of ``good'' weights is non-trivial.
\end{itemize}

\end{tcolorbox}

\subsection{Multi-Objective Hyperparameter Tuning With
Desirability}\label{multi-objective-hyperparameter-tuning-with-desirability}

\subsubsection{Setting Up the Desirability
Function}\label{setting-up-the-desirability-function}

The third approach is to use a desirability function to combine the two
objective functions into a single objective function. The desirability
function is used to maximize the desirability of the two objective
functions. The desirability function is defined in the
\texttt{fun\_control\_init} function by setting the \texttt{fun\_mo2so}
argument to a custom function that computes the desirability of both
objective functions. The desirability function is defined in the
following code segment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"0002"}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ MoHyperLight().fun}
\end{Highlighting}
\end{Shaded}

\phantomsection\label{des-mohyperlight-0002-desirability}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ desirability(y):}
    \ImportTok{from}\NormalTok{ spotdesirability.utils.desirability }\ImportTok{import}\NormalTok{ DOverall, DMin}
\NormalTok{    lossD }\OperatorTok{=}\NormalTok{ DMin(}\DecValTok{10}\NormalTok{, }\DecValTok{6000}\NormalTok{)}
\NormalTok{    epochsD }\OperatorTok{=}\NormalTok{ DMin(}\DecValTok{32}\NormalTok{, }\DecValTok{64}\NormalTok{)}
\NormalTok{    overallD }\OperatorTok{=}\NormalTok{ DOverall(lossD, epochsD)}
\NormalTok{    overall\_desirability }\OperatorTok{=}\NormalTok{ overallD.predict(y, }\BuiltInTok{all}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
    \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ overall\_desirability}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lossD }\OperatorTok{=}\NormalTok{ DMin(}\DecValTok{10}\NormalTok{, }\DecValTok{6000}\NormalTok{)}
\NormalTok{lossD.plot(xlabel}\OperatorTok{=}\StringTok{"loss"}\NormalTok{, ylabel}\OperatorTok{=}\StringTok{"desirability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-des-mohyperlight-0002-lossd-output-1.pdf}}

}

\caption{\label{fig-des-mohyperlight-0002-lossd}The desirability
function for the loss outcome.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{epochsD }\OperatorTok{=}\NormalTok{ DMin(}\DecValTok{32}\NormalTok{, }\DecValTok{64}\NormalTok{)}
\NormalTok{epochsD.plot(xlabel}\OperatorTok{=}\StringTok{"epochs"}\NormalTok{, ylabel}\OperatorTok{=}\StringTok{"desirability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-des-mohyperlight-0002-epochsd-output-1.pdf}}

}

\caption{\label{fig-des-mohyperlight-0002-epochsd}The desirability
function for the epochs outcome.}

\end{figure}%

Note: We have chosen simple desirability functions based on
\texttt{DMin} for the \texttt{validation\ loss} and
\texttt{number\ of\ epochs}. The usage of these functions might result
in large plateaus where the optimizer does not find any improvement.
Therefore, we will explore more sophisticated desirability functions in
the future. These can easily be implemented with the approach shown in
\{Section~\ref{sec-arbitary}\}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
    \CommentTok{\# do not run, if a result file exists}
\NormalTok{    force\_run}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    fun\_mo2so}\OperatorTok{=}\NormalTok{desirability,}
\NormalTok{    device}\OperatorTok{=}\StringTok{"cpu"}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    max\_time}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{3}\NormalTok{,}\DecValTok{10}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{20}\NormalTok{)}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\NormalTok{S.run()    }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
module_name: light
submodule_name: regression
model_name: NNLinearRegressor
Result file 0002_res.pkl exists. Loading the result.
Loaded experiment from 0002_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x16404ea20>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ S.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.26662821493965283
l1: 4.0
epochs: 5.0
batch_size: 7.0
act_fn: 2.0
optimizer: 1.0
dropout_prob: 0.0
lr_mult: 4.341547337583857
patience: 6.0
batch_norm: 0.0
initialization: 2.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"S.y\_mo.shape: }\SpecialCharTok{\{}\NormalTok{S}\SpecialCharTok{.}\NormalTok{y\_mo}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"min loss: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{nanmin(S.y\_mo[:,}\DecValTok{0}\NormalTok{])}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"min epochs: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{nanmin(S.y\_mo[:,}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# print unique values of S.y\_mo[:,1]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"unique epochs values: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{unique(S.y\_mo[:,}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
S.y_mo.shape: (113, 2)
min loss: 2778.37
min epochs: 8.0
unique epochs values: [   8.   16.   32.   64.  128.  256.  512. 1024.]
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Results from the Desirability Function Approach}]

\begin{itemize}
\tightlist
\item
  The desirability multi-objective approach reulsted in a validation
  loss of \texttt{2960} and \texttt{32} (\(=2^{5}\)) epochs.
\item
  The number of epochs is much smaller than in the single-objective
  approach, and the validation loss is in a similar range.
\item
  This illustrates the applicability of desirability functions for
  multi-objective optimization.
\end{itemize}

\end{tcolorbox}

\subsubsection{Pareto Front}\label{pareto-front}

The following two figures show the Pareto front for the multi-objective
optimization problem. Figure~\ref{fig-des-mohyperlight-0002-pareto-2}
shows the Pareto front for the multi-objective optimization problem. The
y-axis uses a logarithmic scal.
Figure~\ref{fig-mohyperlight-0002-pareto-3} shows the Pareto front for
the multi-objective optimization problem. The points with loss
\textgreater{} 1e5 are removed from the plot (due to this removal, the
indices of the pointsin the plot change). The x-axis uses a
double-logarithmic scale, whereas the y-axis is set to a logarithmic
scale for better visualization.

The best point has the following values:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# generate a dataframe with S.y and S.y\_mo}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(S.y\_mo, columns}\OperatorTok{=}\NormalTok{[}\StringTok{"loss"}\NormalTok{, }\StringTok{"epochs"}\NormalTok{])}
\NormalTok{df[}\StringTok{"y"}\NormalTok{] }\OperatorTok{=}\NormalTok{ S.y}
\NormalTok{df\_min }\OperatorTok{=}\NormalTok{ df.loc[df[}\StringTok{"y"}\NormalTok{].idxmin()]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"min y: }\SpecialCharTok{\{}\NormalTok{df\_min[}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"loss: }\SpecialCharTok{\{}\NormalTok{df\_min[}\StringTok{\textquotesingle{}loss\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"epochs: }\SpecialCharTok{\{}\NormalTok{df\_min[}\StringTok{\textquotesingle{}epochs\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{best\_point }\OperatorTok{=}\NormalTok{ np.array([df\_min[}\StringTok{"loss"}\NormalTok{], df\_min[}\StringTok{"epochs"}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"best\_point: }\SpecialCharTok{\{}\NormalTok{best\_point}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.26662821493965283
loss: 2778.373291015625
epochs: 32.0
best_point: [2778.37329102   32.        ]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_orig }\OperatorTok{=}\NormalTok{ S.y\_mo}
\NormalTok{df\_z }\OperatorTok{=}\NormalTok{ pd.DataFrame(y\_orig, columns}\OperatorTok{=}\NormalTok{[}\StringTok{"loss"}\NormalTok{, }\StringTok{"epochs"}\NormalTok{])}
\NormalTok{df\_z\_sel }\OperatorTok{=}\NormalTok{ df\_z.dropna()}
\NormalTok{target\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{"loss (log{-}log)"}\NormalTok{, }\StringTok{"epochs"}\NormalTok{]}
\NormalTok{combinations}\OperatorTok{=}\NormalTok{[(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)]}
\NormalTok{plot\_mo(y\_orig}\OperatorTok{=}\NormalTok{df\_z\_sel, target\_names}\OperatorTok{=}\NormalTok{target\_names, combinations}\OperatorTok{=}\NormalTok{combinations, pareto}\OperatorTok{=}\StringTok{"min"}\NormalTok{, pareto\_front\_orig}\OperatorTok{=}\VariableTok{True}\NormalTok{, title}\OperatorTok{=}\StringTok{"Pareto front (minimization)"}\NormalTok{, pareto\_label}\OperatorTok{=}\VariableTok{True}\NormalTok{, x\_axis\_transformation}\OperatorTok{=}\StringTok{"loglog"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-des-mohyperlight-0002-pareto-2-output-1.pdf}}

}

\caption{\label{fig-des-mohyperlight-0002-pareto-2}Pareto front for the
multi-objective optimization problem.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remove loss values larger than 1e5 from the y\_orig array}
\NormalTok{y\_orig }\OperatorTok{=}\NormalTok{ S.y\_mo}
\NormalTok{df\_z }\OperatorTok{=}\NormalTok{ pd.DataFrame(y\_orig, columns}\OperatorTok{=}\NormalTok{[}\StringTok{"loss"}\NormalTok{, }\StringTok{"epochs"}\NormalTok{])}
\CommentTok{\# remove rows with loss \textgreater{} 1e5}
\NormalTok{df\_z }\OperatorTok{=}\NormalTok{ df\_z[df\_z[}\StringTok{"loss"}\NormalTok{] }\OperatorTok{\textless{}} \FloatTok{1e5}\NormalTok{]}
\NormalTok{df\_z\_sel }\OperatorTok{=}\NormalTok{ df\_z.dropna()}
\NormalTok{target\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{"loss"}\NormalTok{, }\StringTok{"epochs (log)"}\NormalTok{]}
\NormalTok{combinations}\OperatorTok{=}\NormalTok{[(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)]}
\NormalTok{plot\_mo(y\_orig}\OperatorTok{=}\NormalTok{df\_z\_sel, target\_names}\OperatorTok{=}\NormalTok{target\_names, combinations}\OperatorTok{=}\NormalTok{combinations, pareto}\OperatorTok{=}\StringTok{"min"}\NormalTok{, pareto\_front\_orig}\OperatorTok{=}\VariableTok{True}\NormalTok{, title}\OperatorTok{=}\StringTok{"Pareto front (min). Points with loss \textgreater{} 1e5 removed"}\NormalTok{, pareto\_label}\OperatorTok{=}\VariableTok{True}\NormalTok{, y\_axis\_transformation}\OperatorTok{=}\StringTok{"log"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-mohyperlight-0002-pareto-3-output-1.pdf}}

}

\caption{\label{fig-mohyperlight-0002-pareto-3}Pareto front for the
multi-objective optimization problem. Points with loss \textgreater{}
1e5 removed. The Pareto points are identical to the points in the
previous plot. Their indices changed due to the removal of points.}

\end{figure}%

\section{Conclusion}\label{sec-conclusion}

In this article, we have shown how to use the \texttt{spotdesitability}
package to perform three different multi-objective optimization tasks
using desirability functions: RSM (Myers, Montgomery, and Anderson-Cook
2016), surrogate model based optimization (Santner, Williams, and Notz
2003), and hyperparameter tuning (Bartz et al. 2022). The
\texttt{spotdesirability} package is a \texttt{Python} implementation of
the \texttt{R} \texttt{desirability} package (Kuhn 2016). We have
demonstrated how to define desirability functions for different types of
objectives, including maximization, minimization, and target objectives.

Although the desirability function approach is one of the most widely
used methods in industry for the optimization of multiple response
processes ({``{NIST/SEMATECH e-Handbook of Statistical Methods}''}
2021), it is rarely used in hyperparameter tuning. To fill this gap, we
have shown how to use the \texttt{spotdesirability} package in
combination with the \texttt{spotpython} package to perform
hyperparameter tuning using desirability functions. The
\texttt{spotpython} package provides a convenient way to perform
surrogate model based optimization, and the \texttt{spotdesirability}
package allows us to define desirability functions for different types
of objectives. First results are promising, but more research is needed
to evaluate the performance of the desirability function approach in
hyperparameter tuning.

\section{Appendix}\label{appendix}

\subsection{Alternative Optimization Approach Using a Circular Design
Region}\label{alternative-optimization-approach-using-a-circular-design-region}

Kuhn also suggests alternatively maximizing desirability such that
experimental factors are constrained within a spherical design region
with a radius equivalent to the axial point distance:

\phantomsection\label{kuhn16a-optimization-circular-0}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize the best result}
\NormalTok{best }\OperatorTok{=} \VariableTok{None}

\CommentTok{\# Perform optimization for each point in the search grid}
\ControlFlowTok{for}\NormalTok{ i, row }\KeywordTok{in}\NormalTok{ search\_grid.iterrows():}
\NormalTok{    initial\_guess }\OperatorTok{=}\NormalTok{ row.values  }\CommentTok{\# Initial guess for optimization}

    \CommentTok{\# Perform optimization using scipy\textquotesingle{}s minimize function}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ minimize(}
\NormalTok{        rsm\_opt,}
\NormalTok{        initial\_guess,}
\NormalTok{        args}\OperatorTok{=}\NormalTok{(overallD, prediction\_funcs, }\StringTok{"circular"}\NormalTok{), }
\NormalTok{        method}\OperatorTok{=}\StringTok{"Nelder{-}Mead"}\NormalTok{,}
\NormalTok{        options}\OperatorTok{=}\NormalTok{\{}\StringTok{"maxiter"}\NormalTok{: }\DecValTok{1000}\NormalTok{, }\StringTok{"disp"}\NormalTok{: }\VariableTok{False}\NormalTok{\}}
\NormalTok{    )}

    \CommentTok{\# Update the best result if necessary}
    \CommentTok{\# Compare based on the negative desirability}
    \ControlFlowTok{if}\NormalTok{ best }\KeywordTok{is} \VariableTok{None} \KeywordTok{or}\NormalTok{ result.fun }\OperatorTok{\textless{}}\NormalTok{ best.fun:  }
\NormalTok{        best }\OperatorTok{=}\NormalTok{ result}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best Parameters:"}\NormalTok{, best.x)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best Desirability:"}\NormalTok{, }\OperatorTok{{-}}\NormalTok{best.fun)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Best Parameters: [-0.50970524  1.50340746 -0.55595672]
Best Desirability: 0.8581520815997857
\end{verbatim}

Using these best parameters, the predicted values for conversion and
activity can be calculated as follows:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Conversion pred(x): }\SpecialCharTok{\{}\NormalTok{conversion\_pred(best.x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Activity pred(x): }\SpecialCharTok{\{}\NormalTok{activity\_pred(best.x)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Conversion pred(x): 92.51922540231372
Activity pred(x): 57.499999903209876
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_temperature }\OperatorTok{=}\NormalTok{ best.x[}\DecValTok{1}\NormalTok{]}
\CommentTok{\# remove the temperature variable from the best parameters}
\NormalTok{best\_point }\OperatorTok{=}\NormalTok{ np.delete(best.x, }\DecValTok{1}\NormalTok{)}
\CommentTok{\# set the values of temperature to the best temperature in the df}
\CommentTok{\# and recalculate the predicted values}
\NormalTok{plot\_grid\_best }\OperatorTok{=}\NormalTok{ plot\_grid.copy()}
\NormalTok{plot\_grid\_best[}\StringTok{"temperature"}\NormalTok{] }\OperatorTok{=}\NormalTok{ best\_temperature}
\CommentTok{\# Recalculate the predicted values for conversion and activity}
\NormalTok{plot\_grid\_best[}\StringTok{"conversionPred"}\NormalTok{] }\OperatorTok{=}\NormalTok{ conversion\_pred(plot\_grid\_best[[}\StringTok{"time"}\NormalTok{,}
     \StringTok{"temperature"}\NormalTok{, }\StringTok{"catalyst"}\NormalTok{]].values.T)}
\NormalTok{plot\_grid\_best[}\StringTok{"activityPred"}\NormalTok{] }\OperatorTok{=}\NormalTok{ activity\_pred(plot\_grid\_best[[}\StringTok{"time"}\NormalTok{,}
    \StringTok{"temperature"}\NormalTok{, }\StringTok{"catalyst"}\NormalTok{]].values.T)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid\_best,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"conversionPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,}
\NormalTok{    highlight\_point}\OperatorTok{=}\NormalTok{best\_point,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-best-conversion-circular-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-best-conversion-circular}The response
surface for the percent conversion model. To plot the model contours,
the temperature variable was fixed at the best value found by the
optimizer.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contourf\_plot(}
\NormalTok{    plot\_grid\_best,}
\NormalTok{    x\_col}\OperatorTok{=}\StringTok{"time"}\NormalTok{,}
\NormalTok{    y\_col}\OperatorTok{=}\StringTok{"catalyst"}\NormalTok{,}
\NormalTok{    z\_col}\OperatorTok{=}\StringTok{"activityPred"}\NormalTok{,}
\NormalTok{    facet\_col}\OperatorTok{=}\StringTok{"temperature"}\NormalTok{,}
\NormalTok{    highlight\_point}\OperatorTok{=}\NormalTok{best\_point,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{bart25a-desirability-latest_files/figure-pdf/fig-kuhn16a-best-activity-circular-output-1.pdf}}

}

\caption{\label{fig-kuhn16a-best-activity-circular}The response surface
for the thermal activity model. To plot the model contours, the
temperature variable was fixed at the best value found by the
optimizer.}

\end{figure}%

Kuhn (2016) comments that the process converges to relative sub-optimual
values. He suggests that using a radius of 2 achieves an overall
desirability equal to one, even if the solution slightly extrapolates
beyond the design region.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Introduction to Jupyter
Notebook}\label{introduction-to-jupyter-notebook}

Jupyter Notebook is a widely used tool in the Data Science community. It
is easy to use and the produced code can be run per cell. This has a
huge advantage, because with other tools e.g.~(pycharm, vscode, etc.)
the whole script is executed. This can be a time consuming process,
especially when working with huge data sets.

\section{Different Notebook cells}\label{different-notebook-cells}

There are different cells that the notebook is currently supporting:

\begin{itemize}
\item
  code cells
\item
  markdown cells
\item
  raw cells
\end{itemize}

As a default, every cells in jupyter is set to ``code''

\subsection{Code cells}\label{code-cells}

The code cells are used to execute the code. They are following the
logic of the choosen kernel. Therefore, it is important to keep in mind
which programming language is currently used. Otherwise one might yield
an error because of the wrong syntax.

The code cells are executed my be \textbf{â¶ Run} button (can be found in
the header of the notebook).

\subsection{Markdown cells}\label{markdown-cells}

The markdown cells are a usefull tool to comment the written code.
Especially with the help of headers can the code be brought in a more
readable format. If you are not familiar with the markdown syntax, you
can find a usefull cheat sheet here:
\href{https://www.ibm.com/docs/en/db2-event-store/2.0.0?topic=notebooks-markdown-jupyter-cheatsheet}{Markdown
Cheat Sheeet}

\subsection{Raw cells}\label{raw-cells}

The ``Raw NBConvert'' cell type can be used to render different code
formats into HTML or LaTeX by Sphinx. This information is stored in the
notebook metadata and converted appropriately.

\subsubsection{Usage}\label{usage}

To select a desired format from within Jupyter, select the cell
containing your special code and choose options from the following
dropdown menus:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select ``Raw NBConvert''
\item
  Switch the Cell Toolbar to ``Raw Cell Format'' (The cell toolbar can
  be found under View)
\item
  Chose the appropriate ``Raw NBConvert Format'' within the cell
\end{enumerate}

Data Science is fun

\section{Install Packages}\label{install-packages}

Because python is a heavily used programming language, there are many
different packags that can make your life easier. Sadly, there are only
a few standard packages that are already included in your python
enviroment. If you have the need to install a new package in your
enviroment, you can simply do that by exectuing the following code
snippet in a \textbf{code cell}

\texttt{!pip\ install\ numpy}

\begin{itemize}
\item
  The \emph{!} is used to run the cell as a shell command
\item
  \emph{pip} is package manager for python packages.
\item
  \emph{numpy} is the the package you want to install
\end{itemize}

\textbf{Hint:} It is often usefull to restart the kernel after
installing a package, otherwise loading the package could lead to an
error.

\section{Load Packages}\label{load-packages}

After successfully installing the package it is necessary to import them
before you can work with them. The import of the packages is done in the
following way:

\texttt{import\ numpy\ as\ np}

The imported packages are often abbreviated. This is because you need to
specify where the function is coming from.

The most common abbreviations for data science packages are:

\begin{longtable}[]{@{}lll@{}}
\caption{Abbreviations for data science packages}\tabularnewline
\toprule\noalign{}
Abbreviation & Package & Import \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Abbreviation & Package & Import \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
np & numpy & import numpy as np \\
pd & pandas & import pandas as pd \\
plt & matplotlib & import matplotlib.pyplot as plt \\
px & plotly & import plotly.exprss as px \\
tf & tensorflow & import tensorflow as tf \\
sns & seaborn & import seaborn as sns \\
dt & datetime & import datetime as dt \\
pkl & pickle & import pickle as pkl \\
\end{longtable}

\section{Functions in Python}\label{functions-in-python}

Because python is not using Semicolon's it is import to keep track of
indentation in your code. The indentation works as a placeholder for the
semicolons. This is especially important if your are defining loops,
functions, etc. \ldots{}

\textbf{Example:} We are defining a function that calculates the squared
sum of its input parameters

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ squared\_sum(x,y): }
\NormalTok{    z }\OperatorTok{=}\NormalTok{ x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ y}\OperatorTok{**}\DecValTok{2}
    \ControlFlowTok{return}\NormalTok{ z}
\end{Highlighting}
\end{Shaded}

If you are working with something that needs indentation, it will be
already done by the notebook.

\textbf{Hint:} Keep in mind that is good practice to use the
\emph{return} parameter. If you are not using \emph{return} and a
function has multiple paramaters that you would like to return, it will
only return the last one defined.

\section{List of Useful Jupyter Notebook
Shortcuts}\label{list-of-useful-jupyter-notebook-shortcuts}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{List of useful Jupyter Notebook Shortcuts}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Keyboard Shortcut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Menu Tools
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Keyboard Shortcut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Menu Tools
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Save notebook & Esc + s & File â Save and Checkpoint \\
Create new Cell & Esc + a (above), Esc + b (below) & Insert â Cell
above; Insert â Cell below \\
Run Cell & Ctrl + enter & Cell â Run Cell \\
Copy Cell & c & Copy Key \\
Paste Cell & v & Paste Key \\
Interrupt Kernel & Esc + i i & Kernel â Interrupt \\
Restart Kernel & Esc + 0 0 & Kernel â Restart \\
\end{longtable}

If you combine everything you can create beautiful graphics

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Generate 100 random data points along 3 dimensions}
\NormalTok{x, y, scale }\OperatorTok{=}\NormalTok{ np.random.randn(}\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}

\CommentTok{\# Map each onto a scatterplot we\textquotesingle{}ll create with Matplotlib}
\NormalTok{ax.scatter(x}\OperatorTok{=}\NormalTok{x, y}\OperatorTok{=}\NormalTok{y, c}\OperatorTok{=}\NormalTok{scale, s}\OperatorTok{=}\NormalTok{np.}\BuiltInTok{abs}\NormalTok{(scale)}\OperatorTok{*}\DecValTok{500}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{"Some random data, created with the Jupyter Notebook!"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_01_intro_to_notebooks_files/figure-pdf/cell-3-output-1.pdf}}

\chapter{Git Introduction}\label{git-introduction}

\section{Learning Objectives}\label{learning-objectives}

In this learning unit, you will learn how to set up Git as a version
control system for a project. The most important Git commands will be
explained. You will learn how to track and manage changes to your
projects with Git. Specifically:

\begin{itemize}
\tightlist
\item
  Initializing a repository: \texttt{git\ init}
\item
  Ignoring files: \texttt{.gitignore}
\item
  Adding files to the staging area: \texttt{git\ add}
\item
  Checking status changes: \texttt{git\ status}
\item
  Reviewing history: \texttt{git\ log}
\item
  Creating a new branch: \texttt{git\ branch}
\item
  Switching to the current branch: \texttt{git\ switch} and
  \texttt{git\ checkout}
\item
  Merging two branches: \texttt{git\ merge}
\item
  Resolving conflicts
\item
  Reverting changes: \texttt{git\ revert}
\item
  Uploading changes to GitLab: \texttt{git\ push}
\item
  Downloading changes from GitLab: \texttt{git\ pull}
\item
  Advanced: \texttt{git\ rebase}
\end{itemize}

\section{Basics of Git}\label{basics-of-git}

\subsection{\texorpdfstring{Initializing a Repository:
\texttt{git\ init}}{Initializing a Repository: git init}}\label{initializing-a-repository-git-init}

To set up Git as a version control system for your project, you need to
initialize a new Git repository at the top-level folder, which is the
working directory of your project. This is done using the
\texttt{git\ init} command.

All files in this folder and its subfolders will automatically become
part of the repository. Creating a Git repository is similar to adding
an all-powerful passive observer of all things to your project. Git sits
there, observes, and takes note of even the smallest changes, such as a
single character in a file within a repository with hundreds of files.
And it will tell you where these changes occurred if you forget. Once
Git is initialized, it monitors all changes made within the working
directory, and it tracks the history of events from that point forward.
For this purpose, a historical timeline is created for your project,
referred to as a ``branch,'' and the initial branch is named
\texttt{main}. So, when someone says they are on the
\texttt{main\ branch} or working on the \texttt{main\ branch}, it means
they are in the historical main timeline of the project. The Git
repository, often abbreviated as \texttt{repo}, is a virtual
representation of your project, including its history and branches, a
book, if you will, where you can look up and retrieve the entire history
of the project: you work in your working directory, and the Git
repository tracks and stores your work.

\subsection{\texorpdfstring{Ignoring Files:
\texttt{.gitignore}}{Ignoring Files: .gitignore}}\label{ignoring-files-.gitignore}

It's useful that Git watches and keeps an eye on everything in your
project. However, in most projects, there are files and folders that you
don't need or want to keep an eye on. These may include system files,
local project settings, libraries with dependencies, and so on.

You can exclude any file or folder from your Git repository by including
them in the \texttt{.gitignore} file. In the \texttt{.gitignore} file,
you create a list of file names, folder names, and other items that Git
should not track, and Git will ignore these items. Hence the name
``gitignore.'' Do you want to track a file that you previously ignored?
Simply remove the mention of the file in the gitignore file, and Git
will start tracking it again.

\subsection{\texorpdfstring{Adding Changes to the Staging Area:
\texttt{git\ add}}{Adding Changes to the Staging Area: git add}}\label{adding-changes-to-the-staging-area-git-add}

The interesting thing about Git as an all-powerful, passive observer of
all things is that it's very passive. As long as you don't tell Git what
to remember, it will passively observe the changes in the project folder
but do nothing.

When you make a change to your project that you want Git to include in
the project's history to take a snapshot of so you can refer back to it
later, your personal checkpoint, if you will, you need to first stage
the changes in the staging area. What is the staging area? The staging
area is where you collect changes to files that you want to include in
the project's history.

This is done using the \texttt{git\ add} command. You can specify which
files you want to add by naming them, or you can add all of them using
\texttt{-A}. By doing this, you're telling Git that you've made changes
and want it to remember these particular changes so you can recall them
later if needed. This is important because you can choose which changes
you want to stage, and those are the changes that will eventually be
transferred to the history.

Note: When you run \texttt{git\ add}, the changes are not transferred to
the project's history. They are only transferred to the staging area.

\begin{example}[Example of git add from the
beginning]\protect\hypertarget{exm-git-add-from-the-beginning}{}\label{exm-git-add-from-the-beginning}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new directory for your}
\CommentTok{\# repository and navigate to that directory:}

\FunctionTok{mkdir}\NormalTok{ my{-}repo}
\BuiltInTok{cd}\NormalTok{ my{-}repo}

\CommentTok{\# Initialize the repository with git init:}

\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a .gitignore file for Python code.}
\CommentTok{\# You can use a template from GitHub:}

\ExtensionTok{curl}\NormalTok{ https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore }\AttributeTok{{-}o}\NormalTok{ .gitignore}

\CommentTok{\# Add your files to the repository using git add:}

\FunctionTok{git}\NormalTok{ add .}
\end{Highlighting}
\end{Shaded}

This adds all files in the current directory to the repository, except
for the files listed in the .gitignore file.

\end{example}

\subsection{\texorpdfstring{Transferring Changes to Memory:
\texttt{git\ commit}}{Transferring Changes to Memory: git commit}}\label{transferring-changes-to-memory-git-commit}

The power of Git becomes evident when you start transferring changes to
the project history. This is done using the \texttt{git\ commit}
command. When you run \texttt{git\ commit}, you inform Git that the
changes in the staging area should be added to the history of the
project so that they can be referenced or retrieved later.

Additionally, you can add a commit message with the \texttt{-m} option
to explain what changes were made. So when you look back at the project
history, you can see that you added a new feature.

\texttt{git\ commit} creates a snapshot, an image of the current state
of your project at that specific time, and adds it to the branch you are
currently working on.

As you work on your project and transfer more snapshots, the branch
grows and forms a timeline of events. This means you can now look back
at every transfer in the branch and see what your code looked like at
that time.

You can compare any phase of your code with any other phase of your code
to find errors, restore deleted code, or do things that would otherwise
not be possible, such as resetting the project to a previous state or
creating a new timeline from any point.

So how often should you add these commits? My rule of thumb is not to
commit too often. It's better to have a Git repository with too many
commits than one with too few commits.

\begin{example}[Continuing the example from
above:]\protect\hypertarget{exm-continuing-git-add-1}{}\label{exm-continuing-git-add-1}

After adding your files with \texttt{git\ add}, you can create a commit
to save your changes. Use the \texttt{git\ commit} command with the
\texttt{-m} option to specify your commit message:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"My first commit message"}
\end{Highlighting}
\end{Shaded}

This creates a new commit with the added files and the specified commit
message.

\end{example}

\subsection{\texorpdfstring{Check the Status of Your Repository:
\texttt{git\ status}}{Check the Status of Your Repository: git status}}\label{check-the-status-of-your-repository-git-status}

If you're wondering what you've changed in your project since the last
commit snapshot, you can always check the Git status. Git will list
every modified file and the current status of each file.

This status can be either:

\begin{itemize}
\tightlist
\item
  Unchanged (\texttt{unmodified}), meaning nothing has changed since you
  last transferred it, or
\item
  It's been changed (\texttt{changed}) but not staged (\texttt{staged})
  to be transferred into the history, or
\item
  Something has been added to staging (\texttt{staged}) and is ready to
  be transferred into the history.
\end{itemize}

When you run \texttt{git\ status}, you get an overview of the current
state of your project.

\begin{example}[Continuing the example from
above:]\protect\hypertarget{exm-continuing-git-add-2}{}\label{exm-continuing-git-add-2}

The \texttt{git\ status} command displays the status of your working
directory and the staging area. It shows you which files have been
modified, which files are staged for commit, and which files are not yet
being tracked:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ status}
\end{Highlighting}
\end{Shaded}

\texttt{git\ status} is a useful tool to keep track of your changes and
ensure that you have added all the desired files for commit.

\end{example}

\subsection{\texorpdfstring{Review Your Repository's History:
\texttt{git\ log}}{Review Your Repository's History: git log}}\label{review-your-repositorys-history-git-log}

\begin{example}[Continuing the example from
above:]\protect\hypertarget{exm-continuing-git-add-3}{}\label{exm-continuing-git-add-3}

You can view the history of your commits with the \texttt{git\ log}
command. This command displays a list of all the commits in the current
branch, along with information such as the author, date, and commit
message:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ log}
\end{Highlighting}
\end{Shaded}

There are many options to customize the output of \texttt{git\ log}. For
example, you can use the \texttt{-\/-pretty} option to change the format
of the output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ log }\AttributeTok{{-}{-}pretty}\OperatorTok{=}\NormalTok{oneline}
\end{Highlighting}
\end{Shaded}

This displays each commit in a single line.

\end{example}

\section{Branches (Timelines)}\label{branches-timelines}

\subsection{\texorpdfstring{Creating an Alternative Timeline:
\texttt{git\ branch}}{Creating an Alternative Timeline: git branch}}\label{creating-an-alternative-timeline-git-branch}

In the course of developing a project, you often reach a point where you
want to add a new feature, but doing so might require changing the
existing code in a way that could be challenging to undo later.

Or maybe you just want to experiment and be able to discard your work if
the experiment fails. In such cases, Git allows you to create an
alternative timeline called a \texttt{branch} to work in.

This new \texttt{branch} has its own name and exists in parallel with
the \texttt{main\ branch} and all other branches in your project.

During development, you can switch between branches and work on
different versions of your code concurrently. This way, you can have a
stable codebase in the \texttt{main\ branch} while developing an
experimental feature in a separate \texttt{branch}. When you switch from
one \texttt{branch} to another, the code you're working on is
automatically reset to the latest commit of the branch you're currently
in.

If you're working in a team, different team members can work on their
own branches, creating an entire universe of alternative timelines for
your project. When features are completed, they can be seamlessly merged
back into the \texttt{main\ branch}.

\begin{example}[Continuing the example from
above:]\protect\hypertarget{exm-continuing-git-add-4}{}\label{exm-continuing-git-add-4}

To create a new \texttt{branch}, you can use the \texttt{git\ branch}
command with the name of the new \texttt{branch} as an argument:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ branch my{-}tests}
\end{Highlighting}
\end{Shaded}

\end{example}

\subsection{\texorpdfstring{The Pointer to the Current Branch:
\texttt{HEAD}}{The Pointer to the Current Branch: HEAD}}\label{the-pointer-to-the-current-branch-head}

How does Git know where you are on the timeline, and how can you keep
track of your position?

You're always working at the tip (\texttt{HEAD}) of the currently active
branch. The \texttt{HEAD} pointer points there quite literally. In a new
project archive with just a single \texttt{main\ branch} and only new
commits being added, \texttt{HEAD} always points to the latest commit in
the \texttt{main\ branch}. That's where you are.

However, if you're in a repository with multiple branches, meaning
multiple alternative timelines, \texttt{HEAD} will point to the latest
commit in the branch you're currently working on.

\subsection{\texorpdfstring{Switching to an Alternative Timeline:
\texttt{git\ switch}}{Switching to an Alternative Timeline: git switch}}\label{switching-to-an-alternative-timeline-git-switch}

As your project grows, and you have multiple branches, you need to be
able to switch between these branches. This is where the \texttt{switch}
command comes into play.

At any time, you can use the \texttt{git\ switch} command with the name
of the branch you want to switch to, and \texttt{HEAD} moves from your
current branch to the one you specified.

If you've made changes to your code before switching, Git will attempt
to carry those changes over to the branch you're switching to. However,
if these changes conflict with the target branch, the switch will be
canceled.

To resolve this issue without losing your changes, return to the
original branch, add and commit your recent changes, and then perform
the \texttt{switch}.

\subsection{\texorpdfstring{Switching to an Alternative Timeline and
Making Changes:
\texttt{git\ checkout}}{Switching to an Alternative Timeline and Making Changes: git checkout}}\label{switching-to-an-alternative-timeline-and-making-changes-git-checkout}

To switch between branches, you can also use the \texttt{git\ checkout}
command. It works similarly to \texttt{git\ switch} for this purpose:
you pass the name of the branch you want to switch to, and \texttt{HEAD}
moves to the beginning of that branch.

But \texttt{checkout} can do more than just switch to another timeline.
With \texttt{git\ checkout}, you can also move to any commit point in
any timeline. In other words, you can travel back in time and work on
code from the past.

To do this, use \texttt{git\ checkout} and provide the commit ID. This
is an automatically generated, random combination of letters and numbers
that identifies each commit. You can retrieve the commit ID using
\texttt{git\ log}. When you run \texttt{git\ log}, you get a list of all
the commits in your repository, starting with the most recent ones.

When you use \texttt{git\ checkout} with an older commit ID, you check
out a commit in the middle of a branch. This disrupts the timeline, as
you're actively attempting to change history. Git doesn't want you to do
that because, much like in a science fiction movie, altering the past
might also alter the future. In our case, it would break the version
control branch's coherence.

To prevent you from accidentally disrupting time and altering history,
checking out an earlier commit in any branch results in the warning
``Detached Head,'' which sounds rather ominous. The ``Detached Head''
warning is appropriate because it accurately describes what's happening.
Git literally detaches the head from the branch and sets it aside.

Now, you're working outside of time in a space unbound to any timeline,
which again sounds rather threatening but is perfectly fine in reality.

To continue working on this past code, all you need to do is reattach it
to the timeline. You can use \texttt{git\ branch} to create a new
branch, and the detached head will automatically attach to this new
branch.

Instead of breaking the history, you've now created a new alternative
timeline that starts in the past, allowing you to work safely. You can
continue working on the branch as usual.

\begin{example}[Continuing the example from
above:]\protect\hypertarget{exm-continuing-git-add-5}{}\label{exm-continuing-git-add-5}

To switch to a new branch, you can use the \texttt{git\ checkout}
command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ checkout meine{-}tests}
\end{Highlighting}
\end{Shaded}

Now you're using the new branch and can make changes independently from
the original branch.

\end{example}

\subsection{\texorpdfstring{The Difference Between \texttt{checkout} and
\texttt{switch}}{The Difference Between checkout and switch}}\label{the-difference-between-checkout-and-switch}

What is the difference between \texttt{git\ switch} and
\texttt{git\ checkout}? \texttt{git\ switch} and \texttt{git\ checkout}
are two different commands that both serve the purpose of switching
between branches. You can use both to switch between branches, but they
have an important distinction. \texttt{git\ switch} is a new command
introduced with Git 2.23. \texttt{git\ checkout} is an older command
that has existed since Git 1.6.0. So, \texttt{git\ switch} and
\texttt{git\ checkout} have different origins. \texttt{git\ switch} was
introduced to separate the purposes of \texttt{git\ checkout}.
\texttt{git\ checkout} has two different purposes: 1. It can be used to
switch between branches, and 2. It can be used to reset files to the
state of the last commit.

Here's an example: In my project, I made a change since the last commit,
but I haven't staged it yet. Then, I realized that I actually don't want
this change. I want to reset the file to the state before the last
commit. As long as I haven't committed my changes, I can do this with
\texttt{git\ checkout} by targeting the specific file. So, if that file
is named \texttt{main.js}, I can say: \texttt{git\ checkout\ main.js}.
And the file will be reset to the state of the last commit, which makes
sense. I'm checking out the file from the last commit.

But that's quite different from switching between the beginning of one
branch to another. \texttt{git\ switch} and \texttt{git\ restore} were
introduced to separate these two operations. \texttt{git\ switch} is for
switching between branches, and \texttt{git\ restore} is for resetting
the specified file to the state of the last commit. If you try to
restore a file with \texttt{git\ switch}, it simply won't work. It's not
intended for that. As I mentioned earlier, it's about separating
concerns.

\begin{example}[Difference between \texttt{git\ switch} and
\texttt{git\ checkout}]\protect\hypertarget{exm-difference-switch-checkout}{}\label{exm-difference-switch-checkout}

Here's an example demonstrating how to initialize a repository and
switch between branches:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new directory for your repository}
\CommentTok{\# and navigate to that directory:}
\FunctionTok{mkdir}\NormalTok{ my{-}repo}
\BuiltInTok{cd}\NormalTok{ my{-}repo}

\CommentTok{\# Initialize the repository with git init:}
\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a new branch with git branch:}
\FunctionTok{git}\NormalTok{ branch my{-}new{-}branch}

\CommentTok{\# Switch to the new branch using git switch:}
\FunctionTok{git}\NormalTok{ switch my{-}new{-}branch}

\CommentTok{\# Alternatively, you can also use git checkout}
\CommentTok{\# to switch to the new branch:}

\FunctionTok{git}\NormalTok{ checkout my{-}new{-}branch}
\end{Highlighting}
\end{Shaded}

Both commands lead to the same result: You are now on the new branch.

\end{example}

\section{Merging Branches and Resolving
Conflicts}\label{merging-branches-and-resolving-conflicts}

\subsection{\texorpdfstring{git \texttt{merge}: Merging Two
Timelines}{git merge: Merging Two Timelines}}\label{git-merge-merging-two-timelines}

Git allows you to split your development work into as many branches or
alternative timelines as you like, enabling you to work on many
different versions of your code simultaneously without losing or
overwriting any of your work.

This is all well and good, but at some point, you need to bring those
various versions of your code back together into one branch. That's
where \texttt{git\ merge} comes in.

Consider an example where you have two branches, a \texttt{main\ branch}
and an experimental branch called \texttt{experimental-branch}. In the
experimental branch, there is a new feature. To merge these two
branches, you set \texttt{HEAD} to the branch where you want to
incorporate the code and execute \texttt{git\ merge} followed by the
name of the branch you want to merge. \texttt{HEAD} is a special pointer
that points to the current branch. When you run \texttt{git\ merge}, it
combines the code from the branch associated with \texttt{HEAD} with the
code from the branch specified by the branch name you provide.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize the repository}
\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a new branch called "experimental{-}branch"}
\FunctionTok{git}\NormalTok{ branch experimental{-}branch}

\CommentTok{\# Switch to the "experimental{-}branch"}
\FunctionTok{git}\NormalTok{ checkout experimental{-}branch}

\CommentTok{\# Add the new feature here and}
\CommentTok{\# make a commit}
\CommentTok{\# ...}

\CommentTok{\# Switch back to the "main" branch}
\FunctionTok{git}\NormalTok{ checkout main}

\CommentTok{\# Perform the merge}
\FunctionTok{git}\NormalTok{ merge experimental{-}branch}
\end{Highlighting}
\end{Shaded}

During the merge, matching pieces of code in the branches overlap, and
any new code from the branch being merged is added to the project. So
now, the main branch also contains the code from the experimental
branch, and the events of the two separate timelines have been merged
into a single one. What's interesting is that even though the
experimental branch was merged with the main branch, the last commit of
the experimental branch remains intact, allowing you to continue working
on the experimental branch separately if you wish.

\subsection{Resolving Conflicts When
Merging}\label{resolving-conflicts-when-merging}

Merging branches where there are no code changes at the same place in
both branches is a straightforward process. It's also a rare process. In
most cases, there will be some form of conflict between the branches --
the same code or the same code area has been modified differently in the
different branches. Merging two branches with such conflicts will not
work, at least not automatically.

In this case, Git doesn't know how to merge this code. So, when such a
situation occurs, it's marked as a conflict, and the merging process is
halted. This might sound more dramatic than it is. When you get a
conflict warning, Git is saying there are two different versions here,
and Git needs to know which one you want to keep. To help you figure out
the conflict, Git combines all the code into a single file and
automatically marks the conflicting code as the current change, which is
the original code from the branch you're working on, or as the incoming
change, which is the code from the file you're trying to merge.

To resolve this conflict, you'll edit the file to literally resolve the
code conflict. This might mean accepting either the current or incoming
change and discarding the other. It could mean combining both changes or
something else entirely. It's up to you. So, you edit the code to
resolve the conflict. Once you've resolved the conflict by editing the
code, you add the new conflict-free version to the staging area with
\texttt{git\ add} and then commit the merged code with
\texttt{git\ commit}. That's how the conflict is resolved.

A merge conflict occurs when Git struggles to automatically merge
changes from two different branches. This usually happens when changes
were made to the same line in the same file in both branches. To resolve
a merge conflict, you must manually edit the affected files and choose
the desired changes. Git marks the conflict areas in the file with
special markings like
\texttt{\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}},
\texttt{=======}, and
\texttt{\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}}.
You can search for these markings and manually select the desired
changes. After resolving the conflicts, you can add the changes with
\texttt{git\ add} and create a new commit with \texttt{git\ commit} to
complete the merge.

\begin{example}[]\protect\hypertarget{exm-merge-conflict}{}\label{exm-merge-conflict}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform the merge (this will cause a conflict)}
\FunctionTok{git}\NormalTok{ merge experimenteller{-}branch}

\CommentTok{\# Open the affected file in an editor and manually resolve the conflicts}
\CommentTok{\# ...}

\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add }\OperatorTok{\textless{}}\NormalTok{filename}\OperatorTok{\textgreater{}}

\CommentTok{\# Create a new commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Resolved conflicts"}
\end{Highlighting}
\end{Shaded}

\end{example}

\subsection{\texorpdfstring{git \texttt{revert}: Undoing
Something}{git revert: Undoing Something}}\label{git-revert-undoing-something}

One of the most powerful features of any software tool is the ``Undo''
button. Make a mistake, press ``Undo,'' and it's as if it never
happened. However, that's not quite as simple when an all-powerful,
passive observer is watching and recording your project's history. How
do you undo something that you've added to the history without rewriting
the history?

The answer is that you can overwrite the history with the
\texttt{git\ reset} command, but that's quite risky and not a good
practice.

A better solution is to work with the historical timeline and simply
place an older version of your code at the top of the branch. This is
done with \texttt{git\ revert}. To make this work, you need to know the
commit ID of the commit you want to go back to.

The commit ID is a machine-generated set of random numbers and letters,
also known as a hash. To get a list of all the commits in the
repository, including the commit ID and commit message, you can run
\texttt{git\ log}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Show the list of all operations in the repository}
\FunctionTok{git}\NormalTok{ log}
\end{Highlighting}
\end{Shaded}

By the way, it's a good idea to leave clear and informative commit
messages for this reason. This way, you know what happened in your
previous commits. Once you've found the commit you want to revert to,
call that commit ID with \texttt{git\ revert}, and then the ID. This
will create a new commit at the top of the branch with the code from the
reference commit. To transfer the code to the branch, add a commit
message and save it. Now, the last commit in your branch matches the
commit you're reverting to, and your project's history remains intact.

\begin{example}[An example with
\texttt{git\ revert}]\protect\hypertarget{exm-git-revert}{}\label{exm-git-revert}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize a new repository}
\FunctionTok{git}\NormalTok{ init}

\CommentTok{\# Create a new file}
\BuiltInTok{echo} \StringTok{"Hello, World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}

\CommentTok{\# Add the file to the repository}
\FunctionTok{git}\NormalTok{ add file.txt}

\CommentTok{\# Create a new commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"First commit"}

\CommentTok{\# Modify the file}
\BuiltInTok{echo} \StringTok{"Goodbye, World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}

\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add file.txt}

\CommentTok{\# Create a new commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Second commit"}

\CommentTok{\# Use git log to find the commit ID of the second commit}
\FunctionTok{git}\NormalTok{ log}

\CommentTok{\# Use git revert to undo the changes from the second commit}
\FunctionTok{git}\NormalTok{ revert }\OperatorTok{\textless{}}\NormalTok{commit{-}id}\OperatorTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{example}

To download the \texttt{students} branch from the repository
\texttt{git@git-ce.rwth-aachen.de:spotseven-lab/numerische-mathematik-sommersemester2023.git}
to your local machine, add a file, and upload the changes, you can
follow these steps:

\begin{example}[An example with \texttt{git\ clone},
\texttt{git\ checkout}, \texttt{git\ add}, \texttt{git\ commit},
\texttt{git\ push}]\protect\hypertarget{exm-git-clone}{}\label{exm-git-clone}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clone the repository to your local machine:}
\FunctionTok{git}\NormalTok{ clone git@git{-}ce.rwth{-}aachen.de:spotseven{-}lab/numerische{-}mathematik{-}sommersemester2023.git}

\CommentTok{\# Change to the cloned repository:}
\BuiltInTok{cd}\NormalTok{ numerische{-}mathematik{-}sommersemester2023}

\CommentTok{\# Switch to the students branch:}
\FunctionTok{git}\NormalTok{ checkout students}

\CommentTok{\# Create the Test folder if it doesn\textquotesingle{}t exist:}
\FunctionTok{mkdir}\NormalTok{ Test}

\CommentTok{\# Create the Testdatei.txt file in the Test folder:}
\FunctionTok{touch}\NormalTok{ Test/Testdatei.txt}

\CommentTok{\# Add the file with git add:}
\FunctionTok{git}\NormalTok{ add Test/Testdatei.txt}

\CommentTok{\# Commit the changes with git commit:}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Added Testdatei.txt"}

\CommentTok{\# Push the changes with git push:}
\FunctionTok{git}\NormalTok{ push origin students}
\end{Highlighting}
\end{Shaded}

This will upload the changes to the server and update the students
branch in the repository.

\end{example}

\section{Downloading from GitLab}\label{downloading-from-gitlab}

To download changes from a GitLab repository to your local machine, you
can use the \texttt{git\ pull} command. This command downloads the
latest changes from the specified remote repository and merges them with
your local repository.

Here is an example:

\begin{example}[An example with
\texttt{git\ pull}]\protect\hypertarget{exm-git-pull}{}\label{exm-git-pull}

~

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Navigate to the local repository}
\CommentTok{\# linked to the GitHub repository:}
\BuiltInTok{cd}\NormalTok{ my{-}local{-}repository}

\CommentTok{\# Make sure you are in the correct branch:}
\FunctionTok{git}\NormalTok{ checkout main}

\CommentTok{\# Download the latest changes from GitHub:}
\FunctionTok{git}\NormalTok{ pull origin main}
\end{Highlighting}
\end{Shaded}

This downloads the latest changes from the main branch of the remote
repository named ``origin'' and merges them with your local repository.

\end{example}

If there are conflicts between the downloaded changes and your local
changes, you will need to resolve them manually before proceeding.

\section{Advanced}\label{advanced}

\subsection{\texorpdfstring{git \texttt{rebase}: Moving the Base of a
Branch}{git rebase: Moving the Base of a Branch}}\label{git-rebase-moving-the-base-of-a-branch}

In some cases, you may need to ``rewrite history.'' A common scenario is
that you've been working on a new feature in a feature branch, and you
realize that the work should have actually happened in the
\texttt{main\ branch}.

To resolve this issue and make it appear as if the work occurred in the
\texttt{main\ branch}, you can reset the experimental branch. ``Rebase''
literally means detaching the base of the experimental branch and moving
it to the beginning of another branch, giving the branch a new base,
thus ``rebasing.''

This operation is performed from the branch you want to ``rebase.'' You
use \texttt{git\ rebase} and specify the branch you want to use as the
new base. If there are no conflicts between the experimental branch and
the branch you want to rebase onto, this process happens automatically.

If there are conflicts, Git will guide you through the conflict
resolution process for each commit from the rebase branch.

This may sound like a lot, but there's a good reason for it. You are
literally rewriting history by transferring commits from one branch to
another. To maintain the coherence of the new version history, there
should be no conflicts within the commits. So, you need to resolve them
one by one until the history is clean. It goes without saying that this
can be a fairly labor-intensive process. Therefore, you should not use
\texttt{git\ rebase} frequently.

\begin{example}[An example with
\texttt{git\ rebase}]\protect\hypertarget{exm-rebase}{}\label{exm-rebase}

\texttt{git\ rebase} is a command used to change the base of a branch.
This means that commits from the branch are applied to a new base, which
is usually another branch. It can be used to clean up the repository
history and avoid merge conflicts.

Here is an example showing how to use \texttt{git\ rebase}:

\begin{itemize}
\item
  In this example, we initialize a new Git repository and create a new
  file. We add the file to the repository and make an initial commit.
  Then, we create a new branch called ``feature'' and switch to that
  branch. We make changes to the file in the feature branch and create a
  new commit.
\item
  Then, we switch back to the main branch and make changes to the file
  again. We add the modified file and make another commit.
\item
  To rebase the feature branch onto the main branch, we first switch to
  the feature branch and then use the \texttt{git\ rebase} command with
  the name of the main branch as an argument. This applies the commits
  from the feature branch to the main branch and changes the base of the
  feature branch.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize a new repository}
\FunctionTok{git}\NormalTok{ init}
\CommentTok{\# Create a new file}
\BuiltInTok{echo} \StringTok{"Hello World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the file to the repository}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create an initial commit}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Initial commit"}
\CommentTok{\# Create a new branch called "feature"}
\FunctionTok{git}\NormalTok{ branch feature}
\CommentTok{\# Switch to the "feature" branch}
\FunctionTok{git}\NormalTok{ checkout feature}
\CommentTok{\# Make changes to the file in the "feature" branch}
\BuiltInTok{echo} \StringTok{"Hello Feature World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create a new commit in the "feature" branch}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Feature commit"}
\CommentTok{\# Switch back to the "main" branch}
\FunctionTok{git}\NormalTok{ checkout main}
\CommentTok{\# Make changes to the file in the "main" branch}
\BuiltInTok{echo} \StringTok{"Hello Main World"} \OperatorTok{\textgreater{}}\NormalTok{ file.txt}
\CommentTok{\# Add the modified file}
\FunctionTok{git}\NormalTok{ add file.txt}
\CommentTok{\# Create a new commit in the "main" branch}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Main commit"}
\CommentTok{\# Use git rebase to rebase the "feature" branch}
\CommentTok{\# onto the "main" branch}
\FunctionTok{git}\NormalTok{ checkout feature}
\FunctionTok{git}\NormalTok{ rebase main}
\end{Highlighting}
\end{Shaded}

\end{example}

\section{Exercises}\label{exercises-10}

In order to be able to carry out this exercise, we provide you with a
functional working environment. This can be accessed
\href{https://hub.0x3e8.de/}{here}. You can log in using your GMID. If
you do not have one, you can generate one
\href{https://id.gm.fh-koeln.de/registrierung.php}{here}. Once you have
successfully logged in to the server, you must open a terminal instance.
You are now in a position to carry out the exercise.

Alternatively, you can also carry out the exercise locally on your
computer, but then you will need to install git.

\subsection{Create project folder}\label{create-project-folder}

First create the \texttt{test-repo} folder via the command line and then
navigate to this folder using the corresponding command.

\section{Initialize repo}\label{initialize-repo}

Now initialize the repository so that the future project, which will be
saved in the \texttt{test-repo} folder, and all associated files are
versioned.

\subsection{Do not upload / ignore certain file
types}\label{do-not-upload-ignore-certain-file-types}

In order to carry out this exercise, you must first download a file
which you then have git ignore. To do this, download the current
examination regulations for the Bachelor's degree program in Electrical
Engineering using the following command
\texttt{curl\ -o\ pruefungsordnung.pdf\ https://www.th-koeln.de/mam/downloads/deutsch/studium/studiengaenge/f07/ordnungen\_plaene/f07\_bpo\_ba\_ekb\_2021\_01\_04.pdf}.

The PDF file has been stored in the root directory of your repo and you
must now exclude it from being uploaded so that no changes to this file
are tracked. Please note that not only this one PDF file should be
ignored, but all PDF files in the repo.

\subsection{Create file and stage it}\label{create-file-and-stage-it}

In order to be able to commit a change later and thus make it traceable,
it must first be staged. However, as we only have a PDF file so far,
which is to be ignored by git, we cannot stage anything. Therefore, in
this task, a file \texttt{test.txt} with some string as content is to be
created and then staged.

\subsection{Create another file and check
status}\label{create-another-file-and-check-status}

To understand the status function, you should create the file
\texttt{test2.txt} and then call the status function of git.

\subsection{Commit changes}\label{commit-changes}

After the changes to the \texttt{test.txt} file have been staged and
these are now to be transferred to the project process, they must be
committed. Therefore, in this step you should perform a corresponding
commit in the current branch with the message \texttt{test-commit}.
Finally, you should also display the history of the commits.

\subsection{Create a new branch and switch to
it}\label{create-a-new-branch-and-switch-to-it}

In this task, you are to create a new branch with the name
\texttt{change-text} in which you will later make changes. You should
then switch to this branch.

\subsection{Commit changes in the new
branch}\label{commit-changes-in-the-new-branch}

To be able to merge the new branch into the main branch later, you must
first make changes to the \texttt{test.txt} file. To do this, open the
file and simply change the character string in this file before saving
the changes and closing the file. Before you now commit the file, you
should reset the file to the status of the last commit for practice
purposes and thus undo the change. After you have done this, open the
file \texttt{test.txt} again and change the character string again
before saving and closing the file. This time you should commit the file
\texttt{test.txt} and then commit it with the message
\texttt{test-commit2}.

\subsection{Merge branch into main}\label{merge-branch-into-main}

After you have committed the change to the \texttt{test.txt} file, you
should merge the \texttt{change-text} branch including the change into
the main branch so that it is also available there.

\subsection{Resolve merge conflict}\label{resolve-merge-conflict}

To simulate a merge conflict, you must first change the content of the
\texttt{test.txt} file before you commit the change. Then switch to the
branch \texttt{change-text} and change the file \texttt{test.txt} there
as well before you commit the change. Now you should try to merge the
branch \texttt{change-text} into the main branch and solve the problems
that occur in order to be able to perform the merge successfully.

\chapter{Python Introduction}\label{python-introduction}

\section{Recommendations}\label{recommendations}

\href{https://wiki.python.org/moin/BeginnersGuide}{Beginner's Guide to
Python}

\chapter{Documentation of the Sequential Parameter
Optimization}\label{documentation-of-the-sequential-parameter-optimization}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ shgo}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ direct}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

This document describes the \texttt{Spot} features. The official
\texttt{spotpython} documentation can be found here:
\url{https://sequential-parameter-optimization.github.io/spotpython/}.

\section{An Initial Example}\label{an-initial-example}

The \texttt{spotpython} package provides several classes of objective
functions. We will use an analytical objective function, i.e., a
function that can be described by a (closed) formula: \[
f(x) = x^2.
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(x)}
\NormalTok{plt.figure()}
\NormalTok{plt.plot(x,y, }\StringTok{"k"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_04_spot_doc_files/figure-pdf/cell-4-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, surrogate\_control\_init, optimizer\_control\_init}
\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                   fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                        lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                        upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{100}\NormalTok{]),}
\NormalTok{                        fun\_evals }\OperatorTok{=} \DecValTok{7}\NormalTok{,}
\NormalTok{                        fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                        noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                        tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                        var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{],}
\NormalTok{                        infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                        n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                        log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{),}
\NormalTok{                   design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(}
\NormalTok{                        init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                        repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{),}
\NormalTok{                   surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(}
\NormalTok{                        method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{,}
\NormalTok{                        min\_theta}\OperatorTok{={-}}\DecValTok{4}\NormalTok{,}
\NormalTok{                        max\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{                        n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                        model\_optimizer}\OperatorTok{=}\NormalTok{differential\_evolution,}
\NormalTok{                        model\_fun\_evals}\OperatorTok{=}\DecValTok{10000}\NormalTok{))}
\NormalTok{spot\_1.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 51.152288363788145 [#########-] 85.71% 
spotpython tuning: 21.640274267756638 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x173c8b320>
\end{verbatim}

\section{Organization}\label{organization}

\texttt{Spot} organizes the surrogate based optimization process in four
steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Selection of the objective function: \texttt{fun}.
\item
  Selection of the initial design: \texttt{design}.
\item
  Selection of the optimization algorithm: \texttt{optimizer}.
\item
  Selection of the surrogate model: \texttt{surrogate}.
\end{enumerate}

For each of these steps, the user can specify an object:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_sphere}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\NormalTok{design }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ differential\_evolution}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ differential\_evolution}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\NormalTok{surrogate }\OperatorTok{=}\NormalTok{ Kriging()}
\end{Highlighting}
\end{Shaded}

For each of these steps, the user can specify a dictionary of control
parameters.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{fun\_control}
\item
  \texttt{design\_control}
\item
  \texttt{optimizer\_control}
\item
  \texttt{surrogate\_control}
\end{enumerate}

Each of these dictionaries has an initialzaion method, e.g.,
\texttt{fun\_control\_init()}. The initialization methods set the
default values for the control parameters.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important:}]

\begin{itemize}
\tightlist
\item
  The specification of an lower bound in \texttt{fun\_control} is
  mandatory.
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init, design\_control\_init, optimizer\_control\_init, surrogate\_control\_init}
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(lower}\OperatorTok{=}\NormalTok{np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{                            upper}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]))}
\NormalTok{design\_control}\OperatorTok{=}\NormalTok{design\_control\_init()}
\NormalTok{optimizer\_control}\OperatorTok{=}\NormalTok{optimizer\_control\_init()}
\NormalTok{surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init()}
\end{Highlighting}
\end{Shaded}

\section{The Spot Object}\label{the-spot-object}

Based on the definition of the \texttt{fun}, \texttt{design},
\texttt{optimizer}, and \texttt{surrogate} objects, and their
corresponding control parameter dictionaries, \texttt{fun\_control},
\texttt{design\_control}, \texttt{optimizer\_control}, and
\texttt{surrogate\_control}, the \texttt{spot} object can be build as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                       fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                       design\_control}\OperatorTok{=}\NormalTok{design\_control,}
\NormalTok{                       optimizer\_control}\OperatorTok{=}\NormalTok{optimizer\_control,}
\NormalTok{                       surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control)}
\end{Highlighting}
\end{Shaded}

\section{Run}\label{run}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.run()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 0.0013050614212698486 [#######---] 73.33% 
spotpython tuning: 0.0003479187873901382 [########--] 80.00% 
spotpython tuning: 0.00022767416623665655 [#########-] 86.67% 
spotpython tuning: 0.00020787497784734184 [#########-] 93.33% 
spotpython tuning: 0.00020393508736265477 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\begin{verbatim}
<spotpython.spot.spot.Spot at 0x173d17860>
\end{verbatim}

\section{Print the Results}\label{print-the-results-5}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.print\_results()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
min y: 0.00020393508736265477
x0: 0.014161858193549292
x1: 0.0018376234294478113
\end{verbatim}

\begin{verbatim}
[['x0', np.float64(0.014161858193549292)],
 ['x1', np.float64(0.0018376234294478113)]]
\end{verbatim}

\section{Show the Progress}\label{show-the-progress-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_04_spot_doc_files/figure-pdf/cell-11-output-1.pdf}}

\section{Visualize the Surrogate}\label{visualize-the-surrogate}

\begin{itemize}
\tightlist
\item
  The plot method of the \texttt{kriging} surrogate is used.
\item
  Note: the plot uses the interval defined by the ranges of the natural
  variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.surrogate.plot()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_04_spot_doc_files/figure-pdf/cell-12-output-1.pdf}}

\section{Run With a Specific Start
Design}\label{run-with-a-specific-start-design}

To pass a specific start design, use the \texttt{X\_start} argument of
the \texttt{run} method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_x0 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                    fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{                        lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{10}\NormalTok{]),}
\NormalTok{                        upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{100}\NormalTok{]),}
\NormalTok{                        fun\_evals }\OperatorTok{=} \DecValTok{7}\NormalTok{,}
\NormalTok{                        fun\_repeats }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        max\_time }\OperatorTok{=}\NormalTok{ inf,}
\NormalTok{                        noise }\OperatorTok{=} \VariableTok{False}\NormalTok{,}
\NormalTok{                        tolerance\_x }\OperatorTok{=}\NormalTok{ np.sqrt(np.spacing(}\DecValTok{1}\NormalTok{)),}
\NormalTok{                        var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{],}
\NormalTok{                        infill\_criterion }\OperatorTok{=} \StringTok{"y"}\NormalTok{,}
\NormalTok{                        n\_points }\OperatorTok{=} \DecValTok{1}\NormalTok{,}
\NormalTok{                        seed}\OperatorTok{=}\DecValTok{123}\NormalTok{,}
\NormalTok{                        log\_level }\OperatorTok{=} \DecValTok{50}\NormalTok{),}
\NormalTok{                    design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(}
\NormalTok{                        init\_size}\OperatorTok{=}\DecValTok{5}\NormalTok{,}
\NormalTok{                        repeats}\OperatorTok{=}\DecValTok{1}\NormalTok{),}
\NormalTok{                    surrogate\_control}\OperatorTok{=}\NormalTok{surrogate\_control\_init(}
\NormalTok{                        method}\OperatorTok{=}\StringTok{"interpolation"}\NormalTok{,}
\NormalTok{                        min\_theta}\OperatorTok{={-}}\DecValTok{4}\NormalTok{,}
\NormalTok{                        max\_theta}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{                        n\_theta}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{                        model\_optimizer}\OperatorTok{=}\NormalTok{differential\_evolution,}
\NormalTok{                        model\_fun\_evals}\OperatorTok{=}\DecValTok{10000}\NormalTok{))}
\NormalTok{spot\_x0.run(X\_start}\OperatorTok{=}\NormalTok{np.array([}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{]))}
\NormalTok{spot\_x0.plot\_progress()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
spotpython tuning: 51.152288363788145 [#########-] 85.71% 
spotpython tuning: 21.640274267756638 [##########] 100.00% Done...

Experiment saved to 000_res.pkl
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{a_04_spot_doc_files/figure-pdf/cell-13-output-2.pdf}}

\section{Init: Build Initial Design}\label{init-build-initial-design-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.surrogate.kriging }\ImportTok{import}\NormalTok{ Kriging}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{)}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.RandomState(}\DecValTok{1}\NormalTok{)}
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\OperatorTok{{-}}\DecValTok{0}\NormalTok{])}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{])}
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_branin}

\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(sigma}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{10}\NormalTok{, lower}\OperatorTok{=}\NormalTok{lower, upper }\OperatorTok{=}\NormalTok{ upper)}
\BuiltInTok{print}\NormalTok{(X)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ fun(X, fun\_control}\OperatorTok{=}\NormalTok{fun\_control)}
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 8.97647221 13.41926847]
 [ 0.66946019  1.22344228]
 [ 5.23614115 13.78185824]
 [ 5.6149825  11.5851384 ]
 [-1.72963184  1.66516096]
 [-4.26945568  7.1325531 ]
 [ 1.26363761 10.17935555]
 [ 2.88779942  8.05508969]
 [-3.39111089  4.15213772]
 [ 7.30131231  5.22275244]]
[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975
  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]
\end{verbatim}

\section{Replicability}\label{replicability}

Seed

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{345}\NormalTok{)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{gen }\OperatorTok{=}\NormalTok{ SpaceFilling(}\DecValTok{2}\NormalTok{, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ gen.scipy\_lhd(}\DecValTok{3}\NormalTok{)}
\NormalTok{X0, X1, X2, X3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]),
 array([[0.78373509, 0.86811887],
        [0.06692621, 0.6058029 ],
        [0.41374778, 0.00525456]]),
 array([[0.121357  , 0.69043832],
        [0.41906219, 0.32838498],
        [0.86742658, 0.52910374]]),
 array([[0.77254938, 0.31539299],
        [0.59321338, 0.93854273],
        [0.27469803, 0.3959685 ]]))
\end{verbatim}

\section{Surrogates}\label{surrogates-1}

\subsection{A Simple Predictor}\label{a-simple-predictor-1}

The code below shows how to use a simple model for prediction. Assume
that only two (very costly) measurements are available:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  f(0) = 0.5
\item
  f(2) = 2.5
\end{enumerate}

We are interested in the value at \(x_0 = 1\), i.e., \(f(x_0 = 1)\), but
cannot run an additional, third experiment.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ linear\_model}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{], [}\DecValTok{2}\NormalTok{]])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{])}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ linear\_model.LinearRegression()}
\NormalTok{S\_lm }\OperatorTok{=}\NormalTok{ S\_lm.fit(X, y)}
\NormalTok{X0 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{]])}
\NormalTok{y0 }\OperatorTok{=}\NormalTok{ S\_lm.predict(X0)}
\BuiltInTok{print}\NormalTok{(y0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.5]
\end{verbatim}

Central Idea: Evaluation of the surrogate model \texttt{S\_lm} is much
cheaper (or / and much faster) than running the real-world experiment
\(f\).

\section{Tensorboard Setup}\label{tensorboard-setup}

\subsection{Tensorboard Configuration}\label{tensorboard-configuration}

The \texttt{TENSORBOARD\_CLEAN} argument can be set to \texttt{True} in
the \texttt{fun\_control} dictionary to archive the TensorBoard folder
if it already exists. This is useful if you want to start a
hyperparameter tuning process from scratch. If you want to continue a
hyperparameter tuning process, set \texttt{TENSORBOARD\_CLEAN} to
\texttt{False}. Then the TensorBoard folder will not be archived and the
old and new TensorBoard files will shown in the TensorBoard dashboard.

\subsection{Starting TensorBoard}\label{sec-tensorboard-start}

\texttt{TensorBoard} can be started as a background process with the
following command, where \texttt{./runs} is the default directory for
the TensorBoard log files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard {-}{-}logdir="./runs"}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{TENSORBOARD\_PATH}]

The TensorBoard path can be printed with the following command (after a
\texttt{fun\_control} object has been created):

\phantomsection\label{tensorboard_path}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ get\_tensorboard\_path}
\NormalTok{get\_tensorboard\_path(fun\_control)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Demo/Test: Objective Function
Fails}\label{demotest-objective-function-fails}

SPOT expects \texttt{np.nan} values from failed objective function
values. These are handled. Note: SPOT's counter considers only
successful executions of the objective function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\CommentTok{\# number of initial points:}
\NormalTok{ni }\OperatorTok{=} \DecValTok{20}
\CommentTok{\# number of points}
\NormalTok{n }\OperatorTok{=} \DecValTok{30}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical().fun\_random\_error}
\NormalTok{fun\_control}\OperatorTok{=}\NormalTok{fun\_control\_init(}
\NormalTok{    lower }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]),}
\NormalTok{    upper}\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{]),}
\NormalTok{    fun\_evals }\OperatorTok{=}\NormalTok{ n,}
\NormalTok{    show\_progress}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{design\_control}\OperatorTok{=}\NormalTok{design\_control\_init(init\_size}\OperatorTok{=}\NormalTok{ni)}

\NormalTok{spot\_1 }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,}
\NormalTok{                     fun\_control}\OperatorTok{=}\NormalTok{fun\_control,}
\NormalTok{                     design\_control}\OperatorTok{=}\NormalTok{design\_control)}

\CommentTok{\# assert value error from the run method}
\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    spot\_1.run()}
\ControlFlowTok{except} \PreprocessorTok{ValueError} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(e)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Experiment saved to 000_res.pkl
\end{verbatim}

\section{Handling Results: Printing, Saving, and
Loading}\label{handling-results-printing-saving-and-loading}

The results can be printed with the following command:

\phantomsection\label{a_04__print_results}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.print\_results(print\_screen}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The tuned hyperparameters can be obtained as a dictionary with the
following command:

\phantomsection\label{a_04__get_tuned_hyperparameters}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ get\_tuned\_hyperparameters}
\NormalTok{get\_tuned\_hyperparameters(spot\_tuner, fun\_control)}
\end{Highlighting}
\end{Shaded}

The results can be saved and reloaded with the following commands:

\phantomsection\label{a_04__save_and_load}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ save\_pickle, load\_pickle}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ get\_experiment\_name}
\NormalTok{experiment\_name }\OperatorTok{=}\NormalTok{ get\_experiment\_name(}\StringTok{"024"}\NormalTok{)}
\NormalTok{SAVE\_AND\_LOAD }\OperatorTok{=} \VariableTok{False}
\ControlFlowTok{if}\NormalTok{ SAVE\_AND\_LOAD }\OperatorTok{==} \VariableTok{True}\NormalTok{:}
\NormalTok{    save\_pickle(spot\_tuner, experiment\_name)}
\NormalTok{    spot\_tuner }\OperatorTok{=}\NormalTok{ load\_pickle(experiment\_name)}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{\texttt{spotpython} as a Hyperparameter
Tuner}{spotpython as a Hyperparameter Tuner}}\label{spotpython-as-a-hyperparameter-tuner}

\subsection{Modifying Hyperparameter
Levels}\label{sec-modifying-hyperparameter-levels}

\texttt{spotpython} distinguishes between different types of
hyperparameters. The following types are supported:

\begin{itemize}
\tightlist
\item
  \texttt{int} (integer)
\item
  \texttt{float} (floating point number)
\item
  \texttt{boolean} (boolean)
\item
  \texttt{factor} (categorical)
\end{itemize}

\subsubsection{Integer Hyperparameters}\label{integer-hyperparameters}

Integer hyperparameters can be modified with the
\texttt{set\_int\_hyperparameter\_values()}
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/hyperparameters/values/\#spotpython.hyperparameters.values.set_int_hyperparameter_values}{{[}SOURCE{]}}
function. The following code snippet shows how to modify the
\texttt{n\_estimators} hyperparameter of a random forest model:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.hyperdict.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_int\_hyperparameter\_values}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"forest.AMFRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Before modification:"}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\NormalTok{set\_int\_hyperparameter\_values(fun\_control, }\StringTok{"n\_estimators"}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"After modification:"}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Before modification:
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |       5 | transform_power_2_int |
| step            | float  |         1 |     0.1 |      10 | None                  |
| use_aggregation | factor |         1 |     0   |       1 | None                  |
Setting hyperparameter n_estimators to value [2, 5].
Variable type is int.
Core type is None.
Calling modify_hyper_parameter_bounds().
After modification:
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |       5 | transform_power_2_int |
| step            | float  |         1 |     0.1 |      10 | None                  |
| use_aggregation | factor |         1 |     0   |       1 | None                  |
\end{verbatim}

\subsubsection{Float Hyperparameters}\label{float-hyperparameters}

Float hyperparameters can be modified with the
\texttt{set\_float\_hyperparameter\_values()}
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/hyperparameters/values/\#spotpython.hyperparameters.values.set_float_hyperparameter_values}{{[}SOURCE{]}}
function. The following code snippet shows how to modify the
\texttt{step} hyperparameter of a hyperparameter of a Mondrian
Regression Tree model:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.hyperdict.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_float\_hyperparameter\_values}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"forest.AMFRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Before modification:"}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\NormalTok{set\_float\_hyperparameter\_values(fun\_control, }\StringTok{"step"}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"After modification:"}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Before modification:
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |       5 | transform_power_2_int |
| step            | float  |         1 |     0.1 |      10 | None                  |
| use_aggregation | factor |         1 |     0   |       1 | None                  |
Setting hyperparameter step to value [0.2, 5].
Variable type is float.
Core type is None.
Calling modify_hyper_parameter_bounds().
After modification:
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |       5 | transform_power_2_int |
| step            | float  |         1 |     0.2 |       5 | None                  |
| use_aggregation | factor |         1 |     0   |       1 | None                  |
\end{verbatim}

\subsubsection{Boolean Hyperparameters}\label{boolean-hyperparameters}

Boolean hyperparameters can be modified with the
\texttt{set\_boolean\_hyperparameter\_values()}
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/hyperparameters/values/\#spotpython.hyperparameters.values.set_boolean_hyperparameter_values}{{[}SOURCE{]}}
function. The following code snippet shows how to modify the
\texttt{use\_aggregation} hyperparameter of a Mondrian Regression Tree
model:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.hyperdict.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_boolean\_hyperparameter\_values}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"forest.AMFRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Before modification:"}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\NormalTok{set\_boolean\_hyperparameter\_values(fun\_control, }\StringTok{"use\_aggregation"}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"After modification:"}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Before modification:
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |       5 | transform_power_2_int |
| step            | float  |         1 |     0.1 |      10 | None                  |
| use_aggregation | factor |         1 |     0   |       1 | None                  |
Setting hyperparameter use_aggregation to value [0, 0].
Variable type is factor.
Core type is bool.
Calling modify_boolean_hyper_parameter_levels().
After modification:
| name            | type   |   default |   lower |   upper | transform             |
|-----------------|--------|-----------|---------|---------|-----------------------|
| n_estimators    | int    |         3 |     2   |       5 | transform_power_2_int |
| step            | float  |         1 |     0.1 |      10 | None                  |
| use_aggregation | factor |         1 |     0   |       0 | None                  |
\end{verbatim}

\subsubsection{Factor Hyperparameters}\label{factor-hyperparameters}

Factor hyperparameters can be modified with the
\texttt{set\_factor\_hyperparameter\_values()}
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/hyperparameters/values/\#spotpython.hyperparameters.values.set_factor_hyperparameter_values}{{[}SOURCE{]}}
function. The following code snippet shows how to modify the
\texttt{leaf\_model} hyperparameter of a Hoeffding Tree Regressor model:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotriver.hyperdict.river\_hyper\_dict }\ImportTok{import}\NormalTok{ RiverHyperDict}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ fun\_control\_init}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_factor\_hyperparameter\_values}
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_exp\_table}
\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"tree.HoeffdingTreeRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{RiverHyperDict,}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Before modification:"}\NormalTok{)}
\NormalTok{print\_exp\_table(fun\_control)}
\NormalTok{set\_factor\_hyperparameter\_values(fun\_control, }\StringTok{"leaf\_model"}\NormalTok{, [}\StringTok{\textquotesingle{}LinearRegression\textquotesingle{}}\NormalTok{,}
                                                    \StringTok{\textquotesingle{}Perceptron\textquotesingle{}}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"After modification:"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Before modification:
| name                   | type   | default          |   lower |    upper | transform              |
|------------------------|--------|------------------|---------|----------|------------------------|
| grace_period           | int    | 200              |  10     | 1000     | None                   |
| max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |
| delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |
| tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |
| leaf_prediction        | factor | mean             |   0     |    2     | None                   |
| leaf_model             | factor | LinearRegression |   0     |    2     | None                   |
| model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |
| splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |
| min_samples_split      | int    | 5                |   2     |   10     | None                   |
| binary_split           | factor | 0                |   0     |    1     | None                   |
| max_size               | float  | 500.0            | 100     | 1000     | None                   |
| memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |
| stop_mem_management    | factor | 0                |   0     |    1     | None                   |
| remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |
| merit_preprune         | factor | 1                |   0     |    1     | None                   |
After modification:
\end{verbatim}

\chapter{Datasets}\label{datasets}

\section{The Diabetes Data Set}\label{sec-a-05-diabetes-data-set}

This section describes the \texttt{Diabetes} data set. This is a PyTorch
Dataset for regression, which is derived from the \texttt{Diabetes} data
set from \texttt{scikit-learn} (sklearn). Ten baseline variables, age,
sex, body mass index, average blood pressure, and six blood serum
measurements were obtained for each of n = 442 diabetes patients, as
well as the response of interest, a quantitative measure of disease
progression one year after baseline.

\subsection{Data Exploration of the sklearn Diabetes Data
Set}\label{data-exploration-of-the-sklearn-diabetes-data-set}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_diabetes}
\ImportTok{from}\NormalTok{ spotpython.plot.xy }\ImportTok{import}\NormalTok{ plot\_y\_vs\_X}
\NormalTok{data }\OperatorTok{=}\NormalTok{ load\_diabetes()}
\NormalTok{X, y }\OperatorTok{=}\NormalTok{ data.data, data.target}
\NormalTok{plot\_y\_vs\_X(X, y, nrows}\OperatorTok{=}\DecValTok{5}\NormalTok{, ncols}\OperatorTok{=}\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{15}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_05_datasets_files/figure-pdf/load_diabetes_sklearn-output-1.pdf}}

\begin{itemize}
\item
  Each of these 10 feature variables have been mean centered and scaled
  by the standard deviation times the square root of n\_samples (i.e.,
  the sum of squares of each column totals 1).
\item
  \texttt{s3\_hdl} shows a different behavior than the other features.
  It has a negative slope. \texttt{HDL} (high-density lipoprotein)
  cholesterol, sometimes called ``good'' cholesterol, absorbs
  cholesterol in the blood and carries it back to the liver. The liver
  then flushes it from the body. High levels of HDL cholesterol can
  lower your risk for heart disease and stroke.
\end{itemize}

\subsection{Generating the PyTorch Data
Set}\label{generating-the-pytorch-data-set}

\texttt{spotpython} provides a \texttt{Diabetes} class to load the
diabetes data set. The \texttt{Diabetes} class is a subclass of
\texttt{torch.utils.data.Dataset}. It loads the diabetes data set from
\texttt{sklearn} and returns the data set as a
\texttt{torch.utils.data.Dataset} object, so that features and targets
can be accessed as \texttt{torch.tensor}s.
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/data/diabetes/}{{[}CODE
REFERENCE{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ Diabetes()}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(data\_set))}
\BuiltInTok{print}\NormalTok{(data\_set.names)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
442
['age', 'sex', 'bmi', 'bp', 's1_tc', 's2_ldl', 's3_hdl', 's4_tch', 's5_ltg', 's6_glu']
\end{verbatim}

\section{The Friedman Drift Dataset}\label{sec-a-05-friedman}

\subsection{\texorpdfstring{The Friedman Drift Dataset as Implemented in
\texttt{river}}{The Friedman Drift Dataset as Implemented in river}}\label{the-friedman-drift-dataset-as-implemented-in-river}

We will describe the Friedman synthetic dataset with concept drifts
\href{https://riverml.xyz/0.18.0/api/datasets/synth/FriedmanDrift/}{{[}SOURCE{]}},
see also Friedman (1991) and Ikonomovska, Gama, and DÅ¾eroski (2011).
Each observation is composed of ten features. Each feature value is
sampled uniformly in {[}0, 1{]}. Only the first five features are
relevant. The target is defined by different functions depending on the
type of the drift. Global Recurring Abrupt drift will be used, i.e., the
concept drift appears over the whole instance space.

The target is defined by the following function: \[
y = 10 \sin(\pi x_0 x_1) + 20 (x_2 - 0.5)^2 + 10 x_3 + 5 x_4 + \epsilon,
\] where \(\epsilon \sim \mathcal{N}(0, 1)\) is normally distributed
noise.

If the Global Recurring Abrupt drift variant of the Friedman Drift
dataset is used, the target function changes at two points in time,
namely \(p_1\) and \(p_2\). At the first point, the concept changes to:
\[
y = 10 \sin(\pi x_3 x_5) + 20 (x_1 - 0.5)^2 + 10 x_0 + 5 x_2 + \epsilon,
\] At the second point of drift the old concept reoccurs. This can be
implemented as follows, see
\url{https://riverml.xyz/latest/api/datasets/synth/FriedmanDrift/}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \FunctionTok{\_\_iter\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
\NormalTok{    rng }\OperatorTok{=}\NormalTok{ random.Random(}\VariableTok{self}\NormalTok{.seed)}

\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{while} \VariableTok{True}\NormalTok{:}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ \{i: rng.uniform(a}\OperatorTok{=}\DecValTok{0}\NormalTok{, b}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{)\}}
\NormalTok{        y }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_global\_recurring\_abrupt\_gen(x, i) }\OperatorTok{+}\NormalTok{ rng.gauss(mu}\OperatorTok{=}\DecValTok{0}\NormalTok{, sigma}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

        \ControlFlowTok{yield}\NormalTok{ x, y}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ \_global\_recurring\_abrupt\_gen(}\VariableTok{self}\NormalTok{, x, index: }\BuiltInTok{int}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ index }\OperatorTok{\textless{}} \VariableTok{self}\NormalTok{.\_change\_point1 }\KeywordTok{or}\NormalTok{ index }\OperatorTok{\textgreater{}=} \VariableTok{self}\NormalTok{.\_change\_point2:}
        \CommentTok{\# The initial concept is recurring}
        \ControlFlowTok{return}\NormalTok{ (}
            \DecValTok{10} \OperatorTok{*}\NormalTok{ math.sin(math.pi }\OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ x[}\DecValTok{1}\NormalTok{]) }\OperatorTok{+} \DecValTok{20} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{2}\NormalTok{] }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{) }\OperatorTok{**} \DecValTok{2} \OperatorTok{+} \DecValTok{10} \OperatorTok{*}\NormalTok{ x[}\DecValTok{3}\NormalTok{] }\OperatorTok{+} \DecValTok{5} \OperatorTok{*}\NormalTok{ x[}\DecValTok{4}\NormalTok{]}
\NormalTok{        )}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# Drift: the positions of the features are swapped}
        \ControlFlowTok{return}\NormalTok{ (}
            \DecValTok{10} \OperatorTok{*}\NormalTok{ math.sin(math.pi }\OperatorTok{*}\NormalTok{ x[}\DecValTok{3}\NormalTok{] }\OperatorTok{*}\NormalTok{ x[}\DecValTok{5}\NormalTok{]) }\OperatorTok{+} \DecValTok{20} \OperatorTok{*}\NormalTok{ (x[}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{) }\OperatorTok{**} \DecValTok{2} \OperatorTok{+} \DecValTok{10} \OperatorTok{*}\NormalTok{ x[}\DecValTok{0}\NormalTok{] }\OperatorTok{+} \DecValTok{5} \OperatorTok{*}\NormalTok{ x[}\DecValTok{2}\NormalTok{]}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\texttt{spotpython} requires the specification of a \texttt{train} and
\texttt{test} data set. These data sets can be generated as follows:

\phantomsection\label{a_05_friedman_data_set}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ river.datasets }\ImportTok{import}\NormalTok{ synth}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotriver.utils.data\_conversion }\ImportTok{import}\NormalTok{ convert\_to\_df}

\NormalTok{seed }\OperatorTok{=} \DecValTok{123}
\NormalTok{shuffle }\OperatorTok{=} \VariableTok{True}
\NormalTok{n\_train }\OperatorTok{=} \DecValTok{6\_000}
\NormalTok{n\_test }\OperatorTok{=} \DecValTok{4\_000}
\NormalTok{n\_samples }\OperatorTok{=}\NormalTok{ n\_train }\OperatorTok{+}\NormalTok{ n\_test}
\NormalTok{target\_column }\OperatorTok{=} \StringTok{"y"}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{(n\_train}\OperatorTok{/}\DecValTok{4}\NormalTok{, n\_train}\OperatorTok{/}\DecValTok{2}\NormalTok{),}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}

\NormalTok{train }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, n\_total}\OperatorTok{=}\NormalTok{n\_train)}
\NormalTok{train.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}


\NormalTok{dataset }\OperatorTok{=}\NormalTok{ synth.FriedmanDrift(}
\NormalTok{   drift\_type}\OperatorTok{=}\StringTok{\textquotesingle{}gra\textquotesingle{}}\NormalTok{,}
\NormalTok{   position}\OperatorTok{=}\NormalTok{(n\_test}\OperatorTok{/}\DecValTok{4}\NormalTok{, n\_test}\OperatorTok{/}\DecValTok{2}\NormalTok{),}
\NormalTok{   seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\NormalTok{test }\OperatorTok{=}\NormalTok{ convert\_to\_df(dataset, n\_total}\OperatorTok{=}\NormalTok{n\_test)}
\NormalTok{test.columns }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [target\_column]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_data\_with\_drift\_points(data, target\_column, n\_train, title}\OperatorTok{=}\StringTok{""}\NormalTok{):}
\NormalTok{    indices }\OperatorTok{=} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(data))}
\NormalTok{    y\_values }\OperatorTok{=}\NormalTok{ data[target\_column]}

\NormalTok{    plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{    plt.plot(indices, y\_values, label}\OperatorTok{=}\StringTok{"y Value"}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)}

\NormalTok{    drift\_points }\OperatorTok{=}\NormalTok{ [n\_train }\OperatorTok{/} \DecValTok{4}\NormalTok{, n\_train }\OperatorTok{/} \DecValTok{2}\NormalTok{]}
    \ControlFlowTok{for}\NormalTok{ dp }\KeywordTok{in}\NormalTok{ drift\_points:}
\NormalTok{        plt.axvline(x}\OperatorTok{=}\NormalTok{dp, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Drift Point at }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(dp)}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}

\NormalTok{    handles, labels }\OperatorTok{=}\NormalTok{ plt.gca().get\_legend\_handles\_labels()}
\NormalTok{    by\_label }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(labels, handles))}
\NormalTok{    plt.legend(by\_label.values(), by\_label.keys())}

\NormalTok{    plt.xlabel(}\StringTok{\textquotesingle{}Index\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.ylabel(}\StringTok{\textquotesingle{}Target Value (y)\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.title(title)}
\NormalTok{    plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{    plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_data\_with\_drift\_points(train, target\_column, n\_train, title}\OperatorTok{=}\StringTok{"Training Data with Drift Points"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_05_datasets_files/figure-pdf/cell-7-output-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_data\_with\_drift\_points(test, target\_column, n\_train, title}\OperatorTok{=}\StringTok{"Testing Data with Drift Points"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_05_datasets_files/figure-pdf/cell-8-output-1.pdf}}

\subsection{\texorpdfstring{The Friedman Drift Data Set from
\texttt{spotpython}}{The Friedman Drift Data Set from spotpython}}\label{the-friedman-drift-data-set-from-spotpython}

A data generator for the Friedman Drift dataset is implemented in the
\texttt{spotpython} package, see
\href{https://sequential-parameter-optimization.github.io/spotpython/reference/spotpython/data/friedman/}{friedman.py}.
The \texttt{spotpython} version is a simplified version of the
\texttt{river} implementation. The \texttt{spotPyton} version allows the
generation of constant input values for the features. This is useful for
visualizing the concept drifts. For the productive use the
\texttt{river\ version} should be used.

Plotting the first 100 samples of the Friedman Drift dataset, we can not
see the concept drifts at \(p_1\) and \(p_2\). Drift can be visualized
by plotting the target values over time for constant features, e,g, if
\(x_0\) is set to \(1\) and all other features are set to \(0\). This is
illustrated in the following plot.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.friedman }\ImportTok{import}\NormalTok{ FriedmanDriftDataset}

\KeywordTok{def}\NormalTok{ plot\_friedman\_drift\_data(n\_samples, seed, change\_point1, change\_point2, constant}\OperatorTok{=}\VariableTok{True}\NormalTok{):}
\NormalTok{    data\_generator }\OperatorTok{=}\NormalTok{ FriedmanDriftDataset(n\_samples}\OperatorTok{=}\NormalTok{n\_samples, seed}\OperatorTok{=}\NormalTok{seed, change\_point1}\OperatorTok{=}\NormalTok{change\_point1, change\_point2}\OperatorTok{=}\NormalTok{change\_point2, constant}\OperatorTok{=}\NormalTok{constant)}
\NormalTok{    data }\OperatorTok{=}\NormalTok{ [data }\ControlFlowTok{for}\NormalTok{ data }\KeywordTok{in}\NormalTok{ data\_generator]}
\NormalTok{    indices }\OperatorTok{=}\NormalTok{ [i }\ControlFlowTok{for}\NormalTok{ \_, \_, i }\KeywordTok{in}\NormalTok{ data]}
\NormalTok{    values }\OperatorTok{=}\NormalTok{ \{}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{: [] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{)\}}
\NormalTok{    values[}\StringTok{"y"}\NormalTok{] }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ x, y, \_ }\KeywordTok{in}\NormalTok{ data:}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{            values[}\SpecialStringTok{f"x}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{].append(x[i])}
\NormalTok{        values[}\StringTok{"y"}\NormalTok{].append(y)}

\NormalTok{    plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
    \ControlFlowTok{for}\NormalTok{ label, series }\KeywordTok{in}\NormalTok{ values.items():}
\NormalTok{        plt.plot(indices, series, label}\OperatorTok{=}\NormalTok{label)}
\NormalTok{    plt.xlabel(}\StringTok{\textquotesingle{}Index\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.ylabel(}\StringTok{\textquotesingle{}Value\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.axvline(x}\OperatorTok{=}\NormalTok{change\_point1, color}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Drift Point 1\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.axvline(x}\OperatorTok{=}\NormalTok{change\_point2, color}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Drift Point 2\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.legend()}
\NormalTok{    plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{    plt.show()}

\NormalTok{plot\_friedman\_drift\_data(n\_samples}\OperatorTok{=}\DecValTok{100}\NormalTok{, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{, change\_point1}\OperatorTok{=}\DecValTok{50}\NormalTok{, change\_point2}\OperatorTok{=}\DecValTok{75}\NormalTok{, constant}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{plot\_friedman\_drift\_data(n\_samples}\OperatorTok{=}\DecValTok{100}\NormalTok{, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{, change\_point1}\OperatorTok{=}\DecValTok{50}\NormalTok{, change\_point2}\OperatorTok{=}\DecValTok{75}\NormalTok{, constant}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{a_05_datasets_files/figure-pdf/cell-9-output-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{a_05_datasets_files/figure-pdf/cell-9-output-2.pdf}}

\chapter{Using Slurm}\label{using-slurm}

\section{Introduction}\label{introduction-5}

This chapter describes how to generate a \texttt{spotpython}
configuration on a local machine and run the \texttt{spotpython} code on
a remote machine using Slurm.

\section{Prepare the Slurm Scripts on the Remote
Machine}\label{prepare-the-slurm-scripts-on-the-remote-machine}

Two scripts are required to run the \texttt{spotpython} code on the
remote machine:

\begin{itemize}
\tightlist
\item
  \texttt{startSlurm.sh} and
\item
  \texttt{startPython.py}.
\end{itemize}

They should be saved in the same directory as the configuration
(\texttt{pickle}) file. These two scripts must be generated only once
and can be reused for different configurations.

The \texttt{startSlurm.sh} script is a shell script that contains the
following code:

\phantomsection\label{start}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#!/bin/bash}
 
\CommentTok{\#\#\# Vergabe von Ressourcen}
\CommentTok{\#SBATCH {-}{-}job{-}name=Test}
\CommentTok{\#SBATCH {-}{-}account=Accountname/Projektname  \# Hier den gewÃ¼nschten Account angeben}
\CommentTok{\#SBATCH {-}{-}cpus{-}per{-}task=20}
\CommentTok{\#SBATCH {-}{-}gres=gpu:1}
\CommentTok{\#SBATCH {-}{-}time=48:00:00}
\CommentTok{\#SBATCH {-}{-}error=job.\%J.err}
\CommentTok{\#SBATCH {-}{-}output=job.\%J.out}
\CommentTok{\#{-}{-}{-}{-}}
\CommentTok{\#SBATCH {-}{-}partition=gpu}

\ControlFlowTok{if}\NormalTok{ [ }\OperatorTok{{-}}\NormalTok{z }\StringTok{"$1"}\NormalTok{ ]}\OperatorTok{;}\NormalTok{ then}
\NormalTok{    echo }\StringTok{"Usage: $0 \textless{}path\_to\_spot.pkl\textgreater{}"}
\NormalTok{    exit }\DecValTok{1}
\NormalTok{fi}

\NormalTok{SPOT\_PKL}\OperatorTok{=}\NormalTok{$}\DecValTok{1}

\NormalTok{module load conda}

\CommentTok{\#\#\# change to your conda environment with spotpython installed via}
\CommentTok{\#\#\# pip install spotpython}
\NormalTok{conda activate spot312}

\NormalTok{python startPython.py }\StringTok{"$SPOT\_PKL"}

\NormalTok{exit}
\end{Highlighting}
\end{Shaded}

Save the code in a file named \texttt{startSlurm.sh} and copy the file
to the remote machine via \texttt{scp}, i.e.,

\phantomsection\label{copy-startslurm-to-remote}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scp startSlurm.sh user}\OperatorTok{@}\FloatTok{144.33.22.1}\NormalTok{:}
\end{Highlighting}
\end{Shaded}

The \texttt{startPython.py} script is a Python script that contains the
following code:

\phantomsection\label{startpython}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ argparse}
\ImportTok{import}\NormalTok{ pickle}
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_and\_run\_spot\_python\_experiment}
\ImportTok{from}\NormalTok{ spotpython.data.manydataset }\ImportTok{import}\NormalTok{ ManyToManyDataset}

\CommentTok{\# Uncomment the following if you want to use a custom model (python source code)}
\CommentTok{\# import sys}
\CommentTok{\# sys.path.insert(0, \textquotesingle{}./userModel\textquotesingle{})}
\CommentTok{\# import my\_regressor}
\CommentTok{\# import my\_hyper\_dict}


\KeywordTok{def}\NormalTok{ main(pickle\_file):}
\NormalTok{    spot\_tuner }\OperatorTok{=}\NormalTok{ load\_and\_run\_spot\_python\_experiment(filename}\OperatorTok{=}\NormalTok{pickle\_file)}

\ControlFlowTok{if} \VariableTok{\_\_name\_\_} \OperatorTok{==} \StringTok{"\_\_main\_\_"}\NormalTok{:}
\NormalTok{    parser }\OperatorTok{=}\NormalTok{ argparse.ArgumentParser(description}\OperatorTok{=}\StringTok{\textquotesingle{}Process a pickle file.\textquotesingle{}}\NormalTok{)}
\NormalTok{    parser.add\_argument(}\StringTok{\textquotesingle{}pickle\_file\textquotesingle{}}\NormalTok{, }\BuiltInTok{type}\OperatorTok{=}\BuiltInTok{str}\NormalTok{, }\BuiltInTok{help}\OperatorTok{=}\StringTok{\textquotesingle{}The path to the pickle file to be processed.\textquotesingle{}}\NormalTok{)}

\NormalTok{    args }\OperatorTok{=}\NormalTok{ parser.parse\_args()}
\NormalTok{    main(args.pickle\_file)}
\end{Highlighting}
\end{Shaded}

Save the code in a file named \texttt{startPython.py} and copy the file
to the remote machine via \texttt{scp}, i.e.,

\phantomsection\label{copy-startpython-to-remote}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scp startPython.py user}\OperatorTok{@}\FloatTok{144.33.22.1}\NormalTok{:}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Generate a \texttt{spotpython}
Configuration}{Generate a spotpython Configuration}}\label{generate-a-spotpython-configuration}

The configuration can be generated on a local machine using the
following command:

\phantomsection\label{generate-spotpython-config}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.data.diabetes }\ImportTok{import}\NormalTok{ Diabetes}
\ImportTok{from}\NormalTok{ spotpython.hyperdict.light\_hyper\_dict }\ImportTok{import}\NormalTok{ LightHyperDict}
\ImportTok{from}\NormalTok{ spotpython.fun.hyperlight }\ImportTok{import}\NormalTok{ HyperLight}
\ImportTok{from}\NormalTok{ spotpython.utils.init }\ImportTok{import}\NormalTok{ (fun\_control\_init, surrogate\_control\_init, design\_control\_init)}
\ImportTok{from}\NormalTok{ spotpython.spot }\ImportTok{import}\NormalTok{ Spot}
\ImportTok{from}\NormalTok{ spotpython.hyperparameters.values }\ImportTok{import}\NormalTok{ set\_hyperparameter, get\_tuned\_architecture}
\ImportTok{from}\NormalTok{ math }\ImportTok{import}\NormalTok{ inf}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ TensorDataset}
\CommentTok{\# generate data}
\NormalTok{num\_samples }\OperatorTok{=} \DecValTok{100\_000}
\NormalTok{input\_dim }\OperatorTok{=} \DecValTok{100}
\NormalTok{X }\OperatorTok{=}\NormalTok{ torch.randn(num\_samples, input\_dim)  }\CommentTok{\# random data for example}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ torch.randn(num\_samples, }\DecValTok{1}\NormalTok{)  }\CommentTok{\# random target for example}
\NormalTok{data\_set }\OperatorTok{=}\NormalTok{ TensorDataset(X, Y)}

\NormalTok{PREFIX}\OperatorTok{=}\StringTok{"42"}


\NormalTok{fun\_control }\OperatorTok{=}\NormalTok{ fun\_control\_init(}
\NormalTok{    accelerator}\OperatorTok{=}\StringTok{"gpu"}\NormalTok{,}
\NormalTok{    devices}\OperatorTok{=}\StringTok{"auto"}\NormalTok{,}
\NormalTok{    num\_nodes}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    num\_workers}\OperatorTok{=}\DecValTok{19}\NormalTok{,}
\NormalTok{    precision}\OperatorTok{=}\StringTok{"32"}\NormalTok{,}
\NormalTok{    strategy}\OperatorTok{=}\StringTok{"auto"}\NormalTok{,}
\NormalTok{    save\_experiment}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    PREFIX}\OperatorTok{=}\NormalTok{PREFIX,}
\NormalTok{    fun\_evals}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    max\_time}\OperatorTok{=}\NormalTok{inf,}
\NormalTok{    data\_set }\OperatorTok{=}\NormalTok{ data\_set,}
\NormalTok{    core\_model\_name}\OperatorTok{=}\StringTok{"light.regression.NNLinearRegressor"}\NormalTok{,}
\NormalTok{    hyperdict}\OperatorTok{=}\NormalTok{LightHyperDict,}
\NormalTok{    \_L\_in}\OperatorTok{=}\NormalTok{input\_dim,}
\NormalTok{    \_L\_out}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{fun }\OperatorTok{=}\NormalTok{ HyperLight().fun}

\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"optimizer"}\NormalTok{, [ }\StringTok{"Adadelta"}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, }\StringTok{"Adamax"}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"l1"}\NormalTok{, [}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"epochs"}\NormalTok{, [}\DecValTok{10}\NormalTok{,}\DecValTok{12}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"batch\_size"}\NormalTok{, [}\DecValTok{4}\NormalTok{,}\DecValTok{11}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"dropout\_prob"}\NormalTok{, [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.025}\NormalTok{])}
\NormalTok{set\_hyperparameter(fun\_control, }\StringTok{"patience"}\NormalTok{, [}\DecValTok{2}\NormalTok{,}\DecValTok{9}\NormalTok{])}

\NormalTok{design\_control }\OperatorTok{=}\NormalTok{ design\_control\_init(init\_size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{S }\OperatorTok{=}\NormalTok{ Spot(fun}\OperatorTok{=}\NormalTok{fun,fun\_control}\OperatorTok{=}\NormalTok{fun\_control, design\_control}\OperatorTok{=}\NormalTok{design\_control)}
\end{Highlighting}
\end{Shaded}

The configuration is saved as a pickle-file that contains the full
information. In our example, the filename is \texttt{42\_exp.pkl}.

\section{Copy the Configuration to the Remote
Machine}\label{copy-the-configuration-to-the-remote-machine}

You can copy the configuration to the remote machine using the
\texttt{scp} command. The following command copies the configuration to
the remote machine \texttt{144.33.22.1}:

\phantomsection\label{copy-config-to-remote}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scp }\DecValTok{42}\ErrorTok{\_exp}\NormalTok{.pkl user}\OperatorTok{@}\FloatTok{144.33.22.1}\NormalTok{:}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Run the \texttt{spotpython} Code on the Remote
Machine}{Run the spotpython Code on the Remote Machine}}\label{run-the-spotpython-code-on-the-remote-machine}

Login on the remote machine and run the following command to start the
\texttt{spotpython} code:

\phantomsection\label{run-spotpython-on-remote}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssh user}\OperatorTok{@}\FloatTok{144.33.22.1}
\CommentTok{\# change this to your conda environment!}
\NormalTok{conda activate spot312 }
\NormalTok{sbatch .}\OperatorTok{/}\NormalTok{startSlurm.sh }\DecValTok{42}\ErrorTok{\_exp}\NormalTok{.pkl}
\end{Highlighting}
\end{Shaded}

\section{Copy the Results to the Local
Machine}\label{copy-the-results-to-the-local-machine}

After the \texttt{spotpython} code has finished, you can copy the
results back to the local machine using the \texttt{scp} command. The
following command copies the results to the local machine:

\phantomsection\label{copy-results-to-local}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scp user}\OperatorTok{@}\FloatTok{144.33.22.1}\NormalTok{:}\DecValTok{42}\ErrorTok{\_res}\NormalTok{.pkl .}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Experiment and Result Files}]

\begin{itemize}
\tightlist
\item
  \texttt{spotpython} generates two files:

  \begin{itemize}
  \tightlist
  \item
    \texttt{PREFIX\_exp.pkl} (experiment file), which stores the
    information about running the experiment, and
  \item
    \texttt{PREFIX\_res.pkl} (result file), which stores the results of
    the experiment.
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\section{Analyze the Results on the Local
Machine}\label{analyze-the-results-on-the-local-machine}

The file \texttt{42\_res.pkl} contains the results of the
\texttt{spotpython} code. You can analyze the results on the local
machine using the following code. Note: \texttt{PREFIX} is the same as
in the previous steps, i.e., \texttt{"42"}.

\phantomsection\label{spotgui}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.}\BuiltInTok{file} \ImportTok{import}\NormalTok{ load\_result}
\NormalTok{spot\_tuner }\OperatorTok{=}\NormalTok{ load\_result(PREFIX)}
\end{Highlighting}
\end{Shaded}

\subsection{Visualizing the Tuning
Progress}\label{visualizing-the-tuning-progress}

Now the \texttt{spot\_tuner} object is loaded and you can analyze the
results interactively.

\phantomsection\label{analyze-results-progress-plot}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_progress(log\_y}\OperatorTok{=}\VariableTok{True}\NormalTok{, filename}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Design Table with Default and Tuned
Hyperparameters}\label{design-table-with-default-and-tuned-hyperparameters}

\phantomsection\label{analyze-results-design-table}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.utils.eda }\ImportTok{import}\NormalTok{ print\_res\_table}
\NormalTok{print\_res\_table(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\subsection{Plotting Important
Hyperparameters}\label{plotting-important-hyperparameters}

\phantomsection\label{analyze-results-plot-imp-hyperparam}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spot\_tuner.plot\_important\_hyperparameter\_contour(max\_imp}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{The Tuned Hyperparameters}\label{the-tuned-hyperparameters}

\phantomsection\label{get_tuned_hyperparameters}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_tuned\_architecture(spot\_tuner)}
\end{Highlighting}
\end{Shaded}

\chapter{Python Package Building}\label{python-package-building}

\section*{Introduction}\label{introduction-6}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

This notebook will guide you through the process of creating a Python
package.

\begin{itemize}
\tightlist
\item
  All examples can be found in the \texttt{userPackage} directory, see:
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/tree/main/userPackage}{userPackage}
\end{itemize}

\section{Create a Conda Environment}\label{create-a-conda-environment}

\begin{itemize}
\item
  \texttt{conda\ create\ -n\ userpackage\ python=3.12}
\item
  \texttt{conda\ activate\ userpackage}
\item
  Install the following packages:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python} \AttributeTok{{-}m}\NormalTok{ pip install build flake8 black mkdocs mkdocs{-}gen{-}files mkdocs{-}literate{-}nav mkdocs{-}section{-}index mkdocs{-}material mkdocs{-}exclude mkdocstrings mkdocstrings{-}python tensorflow twine jupyter matplotlib plotly pandas pytest spotpython}
\end{Highlighting}
\end{Shaded}

\section{Download the User Package}\label{download-the-user-package}

The user package can be found in the \texttt{userPackage} directory,
see:
\href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/tree/main/userPackage}{userPackage}

\section{Build the User Package}\label{build-the-user-package}

\begin{itemize}
\tightlist
\item
  cd into the \texttt{userPackage} directory and run the following
  command:

  \begin{itemize}
  \tightlist
  \item
    \texttt{./makefile.sh}
  \item
    Alternatively, you can run the following commands:

    \begin{itemize}
    \tightlist
    \item
      \texttt{rm\ -f\ dist/userpackage*;\ python\ -m\ build;\ python\ -m\ pip\ install\ dist/userpackage*.tar.gz}\strut \\
    \item
      \texttt{python\ -m\ mkdocs\ build}
    \end{itemize}
  \end{itemize}
\end{itemize}

\section{Open the Documentation of the User
Package}\label{open-the-documentation-of-the-user-package}

\begin{itemize}
\tightlist
\item
  \texttt{mkdocs\ serve} to view the documentation
\end{itemize}

\chapter{Solutions to Selected
Exercises}\label{solutions-to-selected-exercises}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-warning-color-frame, opacityback=0, toprule=.15mm, arc=.35mm, coltitle=black, opacitybacktitle=0.6, left=2mm, leftrule=.75mm, titlerule=0mm, breakable, colbacktitle=quarto-callout-warning-color!10!white, bottomrule=.15mm, rightrule=.15mm, colback=white, bottomtitle=1mm, toptitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}]

\begin{itemize}
\tightlist
\item
  Solutions are incomplete and need to be corrected!
\item
  They serve as a starting point for the final solution.
\end{itemize}

\end{tcolorbox}

\section{Data-Driven Modeling and
Optimization}\label{data-driven-modeling-and-optimization-1}

\subsection{Histograms}\label{histograms-1}

\begin{refsolution}[Density Curve]
\leavevmode

\begin{itemize}
\tightlist
\item
  We can calculate propabilities.
\item
  We only need two parameters (the mean and the sd) to form the curve
  -\textgreater{} Store data more efficently
\item
  Blanks can be filled
\end{itemize}

\label{sol-curve}

\end{refsolution}

\subsection{The Normal Distribution}\label{the-normal-distribution-3}

\begin{refsolution}[TwoSDAnswer]
95\%

\label{sol-2SD}

\end{refsolution}

\begin{refsolution}[OneSDAnswer]
68\%

\label{sol-2SD1}

\end{refsolution}

\begin{refsolution}[ThreeSDAnswer]
99,7\%

\label{sol-2SD2}

\end{refsolution}

\begin{refsolution}[DataRangeAnswer]
80 - 120

\label{sol-2SD3}

\end{refsolution}

\begin{refsolution}[PeakHeightAnswer]
low

\label{sol-2SD4}

\end{refsolution}

\subsection{The mean, the media, and the
mode}\label{the-mean-the-media-and-the-mode}

\subsection{The exponential
distribution}\label{the-exponential-distribution-1}

\subsection{Population and Estimated
Parameters}\label{population-and-estimated-parameters}

\begin{refsolution}[ProbabilityAnswer]
50\%

\label{sol-POP1}

\end{refsolution}

\subsection{Calculating the Mean, Variance and Standard
Deviation}\label{calculating-the-mean-variance-and-standard-deviation}

\begin{refsolution}[MeanDifferenceAnswer]
If we have all the data, \(\mu\) is the population mean and x-bar is the
sample mean. We don't have the full information.

\label{sol-CAL1}

\end{refsolution}

\begin{refsolution}[EstimateMeanAnswer]
Sum of the values divided by n.

\label{sol-CAL2}

\end{refsolution}

\begin{refsolution}[SigmaSquaredAnswer]
Variance

\label{sol-CAL3}

\end{refsolution}

\begin{refsolution}[EstimatedSDAnswer]
The same as the normal standard deviation, but using n-1.

\label{sol-CAL4}

\end{refsolution}

\begin{refsolution}[VarianceDifferenceAnswer]
\(n\) and \(n-1\)

\label{sol-CAL5}

\end{refsolution}

\begin{refsolution}[ModelBenefitsAnswer]
\leavevmode

\begin{itemize}
\tightlist
\item
  Approximation
\item
  Prediction
\item
  Understanding
\end{itemize}

\label{sol-MAT1}

\end{refsolution}

\begin{refsolution}[SampleDefinitionAnswer]
It's a subset of the data.

\label{sol-SAM1}

\end{refsolution}

\subsection{Hypothesis Testing and the
Null-Hypothesis}\label{hypothesis-testing-and-the-null-hypothesis-1}

\begin{refsolution}[RejectHypothesisAnswer]
It means the evidence supports the alternative hypothesis, indicating
that the null hypothesis is unlikely to be true.

\label{sol-Hyp1}

\end{refsolution}

\begin{refsolution}[NullHypothesisAnswer]
It's a statement that there is no effect or no difference, and it serves
as the default or starting assumption in hypothesis testing.

\label{sol-Hyp2}

\end{refsolution}

\begin{refsolution}[BetterDrugAnswer]
By conducting experiments and statistical tests to compare the new
drug's effectiveness against the current standard and demonstrating a
significant improvement.

\label{sol-Hyp3}

\end{refsolution}

\subsection{Alternative Hypotheses, Main
Ideas}\label{alternative-hypotheses-main-ideas-1}

\subsection{p-values: What they are and how to interpret
them}\label{p-values-what-they-are-and-how-to-interpret-them-1}

\begin{refsolution}[PValueIntroductionAnswer]
We can reject the null hypothesis. We can make a decision.

\label{sol-PVal1}

\end{refsolution}

\begin{refsolution}[PValueRangeAnswer]
It can only be between 0 and 1.

\label{sol-PVal2}

\end{refsolution}

\begin{refsolution}[PValueRangeAnswer]
It can only be between 0 and 1.

\label{sol-PVal3}

\end{refsolution}

\begin{refsolution}[TypicalPValueAnswer]
The chance that we wrongly reject the null hypothesis.

\label{sol-PVal4}

\end{refsolution}

\begin{refsolution}[FalsePositiveAnswer]
If we have a false-positive, we succeed in rejecting the null
hypothesis. But in fact/reality, this is false -\textgreater{} False
positive.

\label{sol-PVal5}

\end{refsolution}

\subsection{How to calculate
p-values}\label{how-to-calculate-p-values-1}

\begin{refsolution}[CalculatePValueAnswer]
Probability of specific result, probability of outcome with the same
probability, and probability of events with smaller probability.

\label{sol-Calc1}

\end{refsolution}

\begin{refsolution}[SDCalculationAnswer]
7 is the SD.

\label{sol-Calc2}

\end{refsolution}

\begin{refsolution}[SidedPValueAnswer]
If we are not interested in the direction of the change, we use the
two-sided. If we want to know about the direction, the one-sided.

\label{sol-Calc3}

\end{refsolution}

\begin{refsolution}[CoinTestAnswer]
TBD

\label{sol-Calc4}

\end{refsolution}

\begin{refsolution}[BorderPValueAnswer]
TBD

\label{sol-Calc5}

\end{refsolution}

\begin{refsolution}[OneSidedPValueCautionAnswer]
If you look in the wrong direction, there is no change.

\label{sol-Calc6}

\end{refsolution}

\begin{refsolution}[BinomialDistributionAnswer]
TBD

\label{sol-Calc7}

\end{refsolution}

\subsection{p-hacking: What it is and how to avoid
it}\label{p-hacking-what-it-is-and-how-to-avoid-it-1}

\begin{refsolution}[PHackingWaysAnswer]
\leavevmode

\begin{itemize}
\tightlist
\item
  Performing repeats until you find one result with a small p-value
  -\textgreater{} false positive result.
\item
  Increasing the sample size within one experiment when it is close to
  the threshold.
\end{itemize}

\label{sol-Hack1}

\end{refsolution}

\begin{refsolution}[AvoidPHackingAnswer]
Specify the number of repeats and the sample sizes at the beginning.

\label{sol-Hack2}

\end{refsolution}

\begin{refsolution}[MultipleTestingProblemAnswer]
TBD

\label{sol-Hack3}

\end{refsolution}

\subsection{Covariance}\label{covariance-3}

\begin{refsolution}[CovarianceDefinitionAnswer]
Formula

\label{sol-Cov1}

\end{refsolution}

\begin{refsolution}[CovarianceMeaningAnswer]
Large values in the first variable result in large values in the second
variable.

\label{sol-Cov2}

\end{refsolution}

\begin{refsolution}[CovarianceVarianceRelationshipAnswer]
Formula

\label{sol-Cov3}

\end{refsolution}

\begin{refsolution}[HighCovarianceAnswer]
No, size doesn't matter.

\label{sol-Cov4}

\end{refsolution}

\begin{refsolution}[ZeroCovarianceAnswer]
No relationship

\label{sol-Cov5}

\end{refsolution}

\begin{refsolution}[NegativeCovarianceAnswer]
Yes

\label{sol-Cov6}

\end{refsolution}

\begin{refsolution}[NegativeVarianceAnswer]
No

\label{sol-Cov7}

\end{refsolution}

\subsection{Pearson's Correlation}\label{pearsons-correlation-1}

\begin{refsolution}[CorrelationValueAnswer]
Recalculate

\label{sol-Corr1}

\end{refsolution}

\begin{refsolution}[CorrelationRangeAnswer]
From -1 to 1

\label{sol-Corr2}

\end{refsolution}

\begin{refsolution}[CorrelationFormulaAnswer]
Formula

\label{sol-Corr3}

\end{refsolution}

\subsection{Boxplots}\label{boxplots-1}

\begin{refsolution}[UnderstandingStatisticalPower]
It is the probability of correctly rejecting the null hypothesis.

\label{sol-StatPow1}

\end{refsolution}

\begin{refsolution}[DistributionEffectOnPower]
Power analysis is not applicable.

\label{sol-StatPow2}

\end{refsolution}

\begin{refsolution}[IncreasingPower]
By taking more samples.

\label{sol-StatPow3}

\end{refsolution}

\begin{refsolution}[PreventingPHacking]
TBD

\label{sol-StatPow4}

\end{refsolution}

\begin{refsolution}[SampleSizeAndPower]
The power will be low.

\label{sol-StatPow5}

\end{refsolution}

\subsection{Power Analysis}\label{power-analysis-1}

\begin{refsolution}[MainFactorsAffectingPower]
The overlap (distance of the two means) and sample sizes.

\label{sol-PowAn1}

\end{refsolution}

\begin{refsolution}[PowerAnalysisOutcome]
The sample size needed.

\label{sol-PowAn2}

\end{refsolution}

\begin{refsolution}[RisksInExperiments]
Few experiments lead to very low power, and many experiments might
result in p-hacking.

\label{sol-PowAn3}

\end{refsolution}

\begin{refsolution}[StepsToPerformPowerAnalysis]
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select power
\item
  Select threshold for significance (alpha)
\item
  Estimate the overlap (done by the effect size)
\end{enumerate}

\label{sol-PowAn4}

\end{refsolution}

\subsection{The Central Limit
Theorem}\label{the-central-limit-theorem-1}

\begin{refsolution}[CentralLimitTheoremAnswer]
TBD

\label{sol-CenLi1}

\end{refsolution}

\subsection{Boxplots}\label{boxplots-2}

\begin{refsolution}[MedianAnswer]
The median.

\label{sol-BoxPlo1}

\end{refsolution}

\begin{refsolution}[BoxContentAnswer]
50\% of the data.

\label{sol-BoxPlo2}

\end{refsolution}

\subsection{R-squared}\label{r-squared-1}

\begin{refsolution}[RSquaredFormulaAnswer]
TBD

\label{sol-RSqu1}

\end{refsolution}

\begin{refsolution}[NegativeRSquaredAnswer]
If you fit a line, no, but there are cases where it could be negative.
However, these are usually considered useless.

\label{sol-RSqu2}

\end{refsolution}

\begin{refsolution}[RSquaredCalculationAnswer]
TBD

\label{sol-RSqu3}

\end{refsolution}

\subsubsection{The main ideas of fitting a line to data (The main ideas
of least squares and linear
regression.)}\label{the-main-ideas-of-fitting-a-line-to-data-the-main-ideas-of-least-squares-and-linear-regression.-1}

\begin{refsolution}[LeastSquaresAnswer]
It is the calculation of the smallest sum of residuals when you fit a
model to data.

\label{sol-FitLin1}

\end{refsolution}

\subsection{Linear Regression}\label{linear-regression-3}

\subsection{Multiple Regression}\label{multiple-regression-2}

\subsection{A Gentle Introduction to Machine
Learning}\label{a-gentle-introduction-to-machine-learning-1}

\begin{refsolution}[RegressionVsClassificationAnswer]
Regression involves predicting continuous values (e.g., temperature,
size), while classification involves predicting discrete values (e.g.,
categories like cat, dog).

\label{sol-ML1}

\end{refsolution}

\subsection{Maximum Likelihood}\label{maximum-likelihood-1}

\begin{refsolution}[LikelihoodConceptAnswer]
The distribution that fits the data best.

\label{sol-MaxLike1}

\end{refsolution}

\subsection{Probability is not
Likelihood}\label{probability-is-not-likelihood}

\begin{refsolution}[ProbabilityVsLikelihoodAnswer]
Likelihood: Finding the curve that best fits the data. Probability:
Calculating the probability of an event given a specific curve.

\label{sol-Prob1}

\end{refsolution}

\subsection{Cross Validation}\label{cross-validation-3}

\begin{refsolution}[TrainVsTestDataAnswer]
Training data is used to fit the model, while testing data is used to
evaluate how well the model fits.

\label{sol-CroVal1}

\end{refsolution}

\begin{refsolution}[SingleValidationIssueAnswer]
The performance might not be representative because the data may not be
equally distributed between training and testing sets.

\label{sol-CroVal2}

\end{refsolution}

\begin{refsolution}[FoldDefinitionAnswer]
TBD

\label{sol-CroVal3}

\end{refsolution}

\begin{refsolution}[LeaveOneOutValidationAnswer]
Only one data point is used as the test set, and the rest are used as
the training set.

\label{sol-CroVal4}

\end{refsolution}

\subsection{The Confusion Matrix}\label{the-confusion-matrix-1}

\begin{refsolution}[ConfusionMatrixAnswer]
TBD

\label{sol-ConMat1}

\end{refsolution}

\subsection{Sensitivity and
Specificity}\label{sensitivity-and-specificity-1}

\begin{refsolution}[SensitivitySpecificityAnswer1]
TBD

\label{sol-SenSpe1}

\end{refsolution}

\begin{refsolution}[SensitivitySpecificityAnswer2]
TBD

\label{sol-SenSpe2}

\end{refsolution}

\subsection{Bias and Variance}\label{bias-and-variance-1}

\begin{refsolution}[BiasAndVarianceAnswer]
TBD

\label{sol-MalLea1}

\end{refsolution}

\subsection{Mutual Information}\label{mutual-information-3}

\begin{refsolution}[MutualInformationExampleAnswer]
TBD

\label{sol-MutInf1}

\end{refsolution}

\subsection{Principal Component Analysis
(PCA)}\label{principal-component-analysis-pca-1}

\begin{refsolution}[WhatIsPCAAnswer]
A dimension reduction technique that helps discover important variables.

\label{sol-PCA1}

\end{refsolution}

\begin{refsolution}[screePlotAnswer]
It shows how much variation is defined by the data.

\label{sol-PCA2}

\end{refsolution}

\begin{refsolution}[LeastSquaresInPCAAnswer]
No, in the first step it tries to maximize distances.

\label{sol-PCA3}

\end{refsolution}

\begin{refsolution}[PCAStepsAnswer]
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate mean
\item
  Shift the data to the center of the coordinate system
\item
  Fit a line by maximizing the distances
\item
  Calculate the sum of squared distances
\item
  Calculate the slope
\item
  Rotate
\end{enumerate}

\label{sol-PCA4}

\end{refsolution}

\begin{refsolution}[EigenvaluePC1Answer]
Formula (to be specified).

\label{sol-PCA5}

\end{refsolution}

\begin{refsolution}[DifferencesBetweenPointsAnswer]
No, because the first difference is measured on the PC1 scale and it is
more important.

\label{sol-PCA6}

\end{refsolution}

\begin{refsolution}[ScalingInPCAAnswer]
Scaling by dividing by the standard deviation (SD).

\label{sol-PCA7}

\end{refsolution}

\begin{refsolution}[DetermineNumberOfComponentsAnswer]
TBD

\label{sol-PCA8}

\end{refsolution}

\begin{refsolution}[LimitingNumberOfComponentsAnswer]
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The dimension of the problem
\item
  Number of samples
\end{enumerate}

\label{sol-PCA9}

\end{refsolution}

\subsection{t-SNE}\label{t-sne-1}

\begin{refsolution}[WhyUseTSNEAnswer]
For dimension reduction and picking out the relevant clusters.

\label{sol-tSNE1}

\end{refsolution}

\begin{refsolution}[MainIdeaOfTSNEAnswer]
To reduce the dimensions of the data by reconstructing the relationships
in a lower-dimensional space.

\label{sol-tSNE2}

\end{refsolution}

\begin{refsolution}[BasicConceptOfTSNEAnswer]
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, randomly arrange the points in a lower dimension
\item
  Decide whether to move points left or right, depending on distances in
  the original dimension
\item
  Finally, arrange points in the lower dimension similarly to the
  original dimension
\end{enumerate}

\label{sol-tSNE3}

\end{refsolution}

\begin{refsolution}[TSNEStepsAnswer]
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Project data to get random points
\item
  Set up a matrix of distances
\item
  Calculate the inner variances of the clusters and the Gaussian
  distribution
\item
  Do the same with the projected points
\item
  Move projected points so the second matrix gets more similar to the
  first matrix
\end{enumerate}

\label{sol-tSNE4}

\end{refsolution}

\subsection{K-means clustering}\label{k-means-clustering-1}

\begin{refsolution}[HowKMeansWorksAnswer]
\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select the number of clusters
\item
  Randomly select distinct data points as initial cluster centers
\item
  Measure the distance between each point and the cluster centers
\item
  Assign each point to the nearest cluster
\item
  Repeat the process
\end{enumerate}

\label{sol-KMeans1}

\end{refsolution}

\begin{refsolution}[QualityOfClustersAnswer]
Calculate the within-cluster variation.

\label{sol-KMeans2}

\end{refsolution}

\begin{refsolution}[IncreasingKAnswer]
If k is too high, each point would be its own cluster. If k is too low,
you cannot see the structures.

\label{sol-KMeans3}

\end{refsolution}

\subsection{DBSCAN}\label{dbscan-1}

\begin{refsolution}[CorePointInDBSCANAnswer]
A point that is close to at least k other points.

\label{sol-DBSCAN1}

\end{refsolution}

\begin{refsolution}[AddingVsExtendingAnswer]
Adding means we add a point and then stop. Extending means we add a
point and then look for other neighbors from that point.

\label{sol-DBSCAN2}

\end{refsolution}

\begin{refsolution}[OutliersInDBSCANAnswer]
Points that are not core points and do not belong to existing clusters.

\label{sol-DBSCAN3}

\end{refsolution}

\subsection{K-nearest neighbors}\label{k-nearest-neighbors}

\begin{refsolution}[AdvantagesAndDisadvantagesOfKAnswer]
\leavevmode

\begin{itemize}
\tightlist
\item
  k = 1: Noise can disturb the process because of possibly incorrect
  measurements of points.
\item
  k = 100: The majority can be wrong for some groups. It is smoother,
  but there is less chance to discover the structure of the data.
\end{itemize}

\label{sol-KNN1}

\end{refsolution}

\subsection{Naive Bayes}\label{naive-bayes-1}

\begin{refsolution}[NaiveBayesFormulaAnswer]
TBD

\label{sol-NaiveBayes1}

\end{refsolution}

\begin{refsolution}[CalculateProbabilitiesAnswer]
TBD

\label{sol-NaiveBayes2}

\end{refsolution}

\subsection{Gaussian Naive Bayes}\label{gaussian-naive-bayes-1}

\begin{refsolution}[UnderflowProblemAnswer]
Small values multiplied together can become smaller than the limits of
computer memory, resulting in zero. Using logarithms (e.g., log(1/2)
-\textgreater{} -1, log(1/4) -\textgreater{} -2) helps prevent
underflow.

\label{sol-GaussianNB1}

\end{refsolution}

\subsection{Trees}\label{trees}

\begin{refsolution}[Tree Usage]
Classication, Regression, Clustering

\label{sol-Tree1}

\end{refsolution}

\begin{refsolution}[Tree Usage]
TBD

\label{sol-DTree1}

\end{refsolution}

\begin{refsolution}[Tree Feature Importance]
The most important feature.

\label{sol-DTree2}

\end{refsolution}

\begin{refsolution}[Regression Tree Limitations]
High dimensions

\label{sol-RTree1}

\end{refsolution}

\begin{refsolution}[Regression Tree Score]
SSR + alpha * T

\label{sol-RTree2}

\end{refsolution}

\begin{refsolution}[Regression Tree Alpha Value Small]
The tree is more complex.

\label{sol-RTree3}

\end{refsolution}

\begin{refsolution}[Regression Tree Increase Alpha Value]
We get smaller trees

\label{sol-RTree4}

\end{refsolution}

\begin{refsolution}[Regression Tree Pruning]
Decreases the complexity of the tree to enhance performance and reduce
overfitting

\label{sol-RTree5}

\end{refsolution}

\section{Machine Learning and Artificial
Intelligence}\label{machine-learning-and-artificial-intelligence-1}

\subsection{Backpropagation}\label{backpropagation-1}

\begin{refsolution}[ChainRuleAndGradientDescentAnswer]
Combination of the chain rule and gradient descent.

\label{sol-BacPro1}

\end{refsolution}

\begin{refsolution}[BackpropagationNamingAnswer]
Because you start at the end and go backwards.

\label{sol-BacPro2}

\end{refsolution}

\subsection{Gradient Descent}\label{gradient-descent-4}

\begin{refsolution}[GradDescStepSize]
learning rate x slope

\label{sol-GradDesc1}

\end{refsolution}

\begin{refsolution}[GradDescIntercept]
Old intercept - step size

\label{sol-GradDesc2}

\end{refsolution}

\begin{refsolution}[GradDescIntercept]
When the step size is small or after a certain number of steps

\label{sol-GradDesc3}

\end{refsolution}

\subsection{ReLU}\label{relu-1}

\begin{refsolution}[Graph ReLU]
Graph of ReLU function: f(x) = max(0, x)

\label{sol-Relu1}

\end{refsolution}

\subsection{CNNs}\label{cnns-1}

\begin{refsolution}[CNNImageRecognitionAnswer]
\leavevmode

\begin{itemize}
\tightlist
\item
  too many features for input layer -\textgreater{} high memory
  consumption
\item
  always shift in data
\item
  it learns local informations and local correlations
\end{itemize}

\label{sol-CNN1}

\end{refsolution}

\begin{refsolution}[CNNFiltersInitializationAnswer]
The filter values in CNNs are randomly initialized and then trained and
optimized through the process of backpropagation.

\label{sol-CNN2}

\end{refsolution}

\begin{refsolution}[CNNFilterInitializationAnswer]
The filter values in CNNs are initially set by random initialization.
These filters undergo training via backpropagation, where gradients are
computed and used to adjust the filter values to optimize performance.

\label{sol-CNN3}

\end{refsolution}

\begin{refsolution}[GenNNStockPredictionAnswer]
A limitation of using classical neural networks for stock market
prediction is their reliance on fixed inputs. Stock market data is
dynamic and requires models that can adapt to changing conditions over
time.

\label{sol-CNN4}

\end{refsolution}

\subsection{RNN}\label{rnn-1}

\begin{refsolution}[RNNUnrollingAnswer]
In the unrolling process of RNNs, the network is copied and the output
from the inner loop is fed into the second layer of the copied network.

\label{sol-RNN1}

\end{refsolution}

\begin{refsolution}[RNNReliabilityAnswer]
RNNs sometimes fail to work reliably due to the vanishing gradient
problem (where gradients are less than 1) and the exploding gradient
problem (where gradients are greater than 1). Additionally, reliability
issues arise because the network and the weights are copied during the
unrolling process.

\label{sol-RNN2}

\end{refsolution}

\subsection{LSTM}\label{lstm-2}

\begin{refsolution}[LSTMSigmoidTanhAnswer]
The sigmoid activation function outputs values between 0 and 1, making
it suitable for probability determination, whereas the tanh activation
function outputs values between -1 and 1.

\label{sol-LSTM1}

\end{refsolution}

\begin{refsolution}[LSTMSigmoidTanhAnswer]
State how much of the long term memory should be used.

\label{sol-LSTM11}

\end{refsolution}

\begin{refsolution}[LSTMGatesAnswer]
An LSTM network has three types of gates: the forget gate, the input
gate, and the output gate. The forget gate decides what information to
discard from the cell state, the input gate updates the cell state with
new information, and the output gate determines what part of the cell
state should be output.

\label{sol-LSTM2}

\end{refsolution}

\begin{refsolution}[LSTMLongTermInfoAnswer]
Long-term information is used in the output gate of an LSTM network.

\label{sol-LSTM3}

\end{refsolution}

\begin{refsolution}[LSTMUpdateGatesAnswer]
In the input and forget gates.

\label{sol-LSTM4}

\end{refsolution}

\subsection{Pytorch/Lightning}\label{pytorchlightning-1}

\begin{refsolution}[PyTorchRequiresGradAnswer]
In PyTorch, \texttt{requires\_grad} indicates whether a tensor should be
trained. If set to False, the tensor will not be trained.

\label{sol-PyTorch1}

\end{refsolution}

\subsection{Embeddings}\label{embeddings-1}

\begin{refsolution}[NN STrings]
No, they process numerical values.

\label{sol-Embedding1}

\end{refsolution}

\begin{refsolution}[Embedding Definition]
Representation of a word as a vector.

\label{sol-Embedding2}

\end{refsolution}

\begin{refsolution}[Embedding Dimensions]
We can model similarities.

\label{sol-Embedding3}

\end{refsolution}

\subsection{Sequence to Sequence
Models}\label{sequence-to-sequence-models}

\begin{refsolution}[LSTM]
Because they are able to consider ``far away'' information.

\label{sol-S2S1}

\end{refsolution}

\begin{refsolution}[Teacher Forcing]
We need to force the correct words for the training.

\label{sol-S2S2}

\end{refsolution}

\begin{refsolution}[Attention]
Attention scores compute similarities for one input to the others.

\label{sol-S2S3}

\end{refsolution}

\subsection{Transformers}\label{transformers-1}

\begin{refsolution}[ChatGPT]
Decoder only.

\label{sol-Transformer1}

\end{refsolution}

\begin{refsolution}[Translation]
Encoder-Decoder structure.

\label{sol-Transformer2}

\end{refsolution}

\begin{refsolution}[Difference Encoder-Decoder and Decoder Only.]
\leavevmode

\begin{itemize}
\tightlist
\item
  Encoder-Decoder: self-attention.
\item
  Decoder only: masked self-attention.
\end{itemize}

\label{sol-Transformer3}

\end{refsolution}

\begin{refsolution}[Weights]
\leavevmode

\begin{itemize}
\tightlist
\item
  a: Randomly
\item
  b: Backpropagation
\end{itemize}

\label{sol-Transformer4}

\end{refsolution}

\begin{refsolution}[Order of Words]
Positional Encoding

\label{sol-Transformer5}

\end{refsolution}

\begin{refsolution}[Relationship Between Words]
Masked self-attention which looks at the previous tokens.

\label{sol-Transformer6}

\end{refsolution}

\begin{refsolution}[Masked Self Attention]
It works by investigating how similar each word is to itself and all of
the proceeding words in the sentence.

\label{sol-Transformer7}

\end{refsolution}

\begin{refsolution}[Softmax]
Transformation to values between 0 and 1.

\label{sol-Transformer8}

\end{refsolution}

\begin{refsolution}[Softmax Output]
We create two new numbers: Values -- like K and Q with different
weights. We scale these values by the percentage. -\textgreater{} we get
the scaled VÂ´s

\label{sol-Transformer9}

\end{refsolution}

\begin{refsolution}[VÂ´s]
Lastly, we sum these values together, which combine separate encodings
for both words relative to their similarities to ``is'', are the
masked-self-attention values for ``is''.

\label{sol-Transformer10}

\end{refsolution}

\begin{refsolution}[Residual Connections]
They are bypasses, which combine the position encoded values with
masked-self-attention values.

\label{sol-Transformer11}

\end{refsolution}

\begin{refsolution}[Generate Known Word in Sequence]
\leavevmode

\begin{itemize}
\tightlist
\item
  Training
\item
  Because it is a Decoder-Only transformer used for prediction and the
  calculations that you need.\\
\end{itemize}

\label{sol-Transformer12}

\end{refsolution}

\begin{refsolution}[Masked-Self-Attention Values and Bypass]
We use a simple neural network with two inputs and five outputs for the
vocabulary.

\label{sol-Transformer13}

\end{refsolution}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abad16a}
Abadi, Martin, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, et al. 2016. {``{TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems}.''} \emph{arXiv
e-Prints}, March, arXiv:1603.04467.

\bibitem[\citeproctext]{ref-agga07a}
Aggarwal, Charu, ed. 2007. \emph{Data Streams -- Models and Algorithms}.
Springer-Verlag.

\bibitem[\citeproctext]{ref-arlot2010}
Arlot, Sylvain, Alain Celisse, et al. 2010. {``A Survey of
Cross-Validation Procedures for Model Selection.''} \emph{Statistics
Surveys} 4: 40--79.

\bibitem[\citeproctext]{ref-bart21i}
Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf
Mersmann, eds. 2022. \emph{{Hyperparameter Tuning for Machine and Deep
Learning with R - A Practical Guide}}. Springer.

\bibitem[\citeproctext]{ref-bart23e}
Bartz-Beielstein, Thomas. 2023a. {``{PyTorch} Hyperparameter Tuning with
{SPOT}: Comparison with {Ray Tuner} and Default Hyperparameters on
{CIFAR10}.''}
\url{https://github.com/sequential-parameter-optimization/spotpython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb}.

\bibitem[\citeproctext]{ref-bart23iArXiv}
---------. 2023b. {``{Hyperparameter Tuning Cookbook: A guide for
scikit-learn, PyTorch, river, and spotpython}.''} \emph{arXiv e-Prints},
July. \url{https://doi.org/10.48550/arXiv.2307.10262}.

\bibitem[\citeproctext]{ref-bart23c5}
---------. 2024a. {``Evaluation and Performance Measurement.''} In,
edited by Eva Bartz and Thomas Bartz-Beielstein, 47--62. Singapore:
Springer Nature Singapore.

\bibitem[\citeproctext]{ref-bart23c10}
---------. 2024b. {``Hyperparameter Tuning.''} In, edited by Eva Bartz
and Thomas Bartz-Beielstein, 125--40. Singapore: Springer Nature
Singapore.

\bibitem[\citeproctext]{ref-bart23c1}
---------. 2024c. {``Introduction: From Batch to~Online Machine
Learning.''} In \emph{Online Machine Learning: A Practical Guide with
Examples in Python}, edited by Eva Bartz and Thomas Bartz-Beielstein,
1--11. Singapore: Springer Nature Singapore.
\url{https://doi.org/10.1007/978-981-99-7007-0_1}.

\bibitem[\citeproctext]{ref-Bart13j}
Bartz-Beielstein, Thomas, JÃ¼rgen Branke, JÃ¶rn Mehnen, and Olaf Mersmann.
2014. {``Evolutionary Algorithms.''} \emph{Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery} 4 (3): 178--95.

\bibitem[\citeproctext]{ref-Bart11a}
Bartz-Beielstein, Thomas, and Martina Friese. 2011. {``{Sequential
Parameter Optimization and Optimal Computational Budget Allocation for
Noisy Optimization Problems}.''} Cologne University of Applied Science,
Faculty of Computer Science; Engineering Science.

\bibitem[\citeproctext]{ref-Bart11b}
Bartz-Beielstein, Thomas, Martina Friese, Martin Zaefferer, Boris
Naujoks, Oliver Flasch, Wolfgang Konen, and Patrick Koch. 2011.
{``{Noisy optimization with sequential parameter optimization and
optimal computational budget allocation}.''} In \emph{Proceedings of the
13th Annual Conference Companion on Genetic and Evolutionary
Computation}, 119--20. New York, NY, USA: ACM.

\bibitem[\citeproctext]{ref-bart23c3}
Bartz-Beielstein, Thomas, and Lukas Hans. 2024. {``Drift Detection
and~Handling.''} In \emph{Online Machine Learning: A Practical Guide
with Examples in Python}, edited by Eva Bartz and Thomas
Bartz-Beielstein, 23--39. Singapore: Springer Nature Singapore.
\url{https://doi.org/10.1007/978-981-99-7007-0_3}.

\bibitem[\citeproctext]{ref-BLP05}
Bartz-Beielstein, Thomas, Christian Lasarczyk, and Mike Preuss. 2005.
{``{Sequential Parameter Optimization}.''} In \emph{{Proceedings 2005
Congress on Evolutionary Computation (CEC'05), Edinburgh, Scotland}},
edited by B McKay et al., 773--80. Piscataway NJ: {IEEE Press}.

\bibitem[\citeproctext]{ref-bart21ic3}
Bartz-Beielstein, Thomas, and Martin Zaefferer. 2022. {``Hyperparameter
Tuning Approaches.''} In \emph{{Hyperparameter Tuning for Machine and
Deep Learning with R - A Practical Guide}}, edited by Eva Bartz, Thomas
Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, 67--114.
Springer.

\bibitem[\citeproctext]{ref-bife10a}
Bifet, Albert. 2010. \emph{Adaptive Stream Mining: Pattern Learning and
Mining from Evolving Data Streams}. Vol. 207. Frontiers in Artificial
Intelligence and Applications. {IOS} Press.

\bibitem[\citeproctext]{ref-bife07a}
Bifet, Albert, and Ricard GavaldÃ . 2007. {``Learning from Time-Changing
Data with Adaptive Windowing.''} In \emph{Proceedings of the 2007 SIAM
International Conference on Data Mining (SDM)}, 443--48.

\bibitem[\citeproctext]{ref-bife09a}
---------. 2009. {``Adaptive Learning from Evolving Data Streams.''} In
\emph{Proceedings of the 8th International Symposium on Intelligent Data
Analysis: Advances in Intelligent Data Analysis VIII}, 249--60. IDA '09.
Berlin, Heidelberg: Springer-Verlag.

\bibitem[\citeproctext]{ref-bife10c}
Bifet, Albert, Geoff Holmes, Richard Kirkby, and Bernhard Pfahringer.
2010a. {``{MOA}: {M}assive Online Analysis.''} \emph{Journal of Machine
Learning Research} 99: 1601--4.

\bibitem[\citeproctext]{ref-bifet10a}
---------. 2010b. {``MOA: Massive Online Analysis.''} \emph{Journal of
Machine Learning Research} 11: 1601--4.

\bibitem[\citeproctext]{ref-bisc23a}
Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter,
Stefan Coors, Janek Thomas, et al. 2023. {``Hyperparameter Optimization:
Foundations, Algorithms, Best Practices, and Open Challenges.''}
\emph{WIREs Data Mining and Knowledge Discovery} 13 (2): e1484.

\bibitem[\citeproctext]{ref-Boha86a}
Bohachevsky, I O. 1986. {``{Generalized Simulated Annealing for Function
Optimization}.''} \emph{Technometrics} 28 (3): 209--17.

\bibitem[\citeproctext]{ref-box57b}
Box, G. E. P., and J. S. Hunter. 1957. {``Multi-Factor Experimental
Designs for Exploring Response Surfaces.''} \emph{The Annals of
Mathematical Statistics} 28 (1): 195--241.

\bibitem[\citeproctext]{ref-Box51a}
Box, G. E. P., and K. B. Wilson. 1951. {``{On the Experimental
Attainment of Optimum Conditions}.''} \emph{Journal of the Royal
Statistical Society. Series B (Methodological)} 13 (1): 1--45.

\bibitem[\citeproctext]{ref-Chen10a}
Chen, Chun Hung. 2010. \emph{{Stochastic simulation optimization: an
optimal computing budget allocation}}. World Scientific.

\bibitem[\citeproctext]{ref-chen18b}
Chen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David
Duvenaud. 2018. {``{Neural Ordinary Differential Equations}.''}
\emph{arXiv e-Prints}, June, arXiv:1806.07366.

\bibitem[\citeproctext]{ref-coel21a}
Coello, Carlos A. Coello, Silvia GonzÃ¡lez Brambila, JosuÃ© Figueroa
Gamboa, and Ma. Guadalupe Castillo Tapia. 2021. {``Multi-Objective
Evolutionary Algorithms: Past, Present, and Future.''} In, edited by
Panos M. Pardalos, Varvara Rasskazova, and Michael N. Vrahatis, 137--62.
Cham: Springer International Publishing.

\bibitem[\citeproctext]{ref-delc96a}
Del Castillo, E., D. C. Montgomery, and D. R. McCarville. 1996.
{``Modified Desirability Functions for Multiple Response
Optimization.''} \emph{Journal of Quality Technology} 28: 337--45.

\bibitem[\citeproctext]{ref-derr80a}
Derringer, G., and R. Suich. 1980. {``Simultaneous Optimization of
Several Response Variables.''} \emph{Journal of Quality Technology} 12:
214--19.

\bibitem[\citeproctext]{ref-devl18a}
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
{``{BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding}.''} \emph{arXiv e-Prints}, October, arXiv:1810.04805.

\bibitem[\citeproctext]{ref-domi20a}
Domingos, Pedro M., and Geoff Hulten. 2000. {``Mining High-Speed Data
Streams.''} In \emph{Proceedings of the Sixth {ACM} {SIGKDD}
International Conference on Knowledge Discovery and Data Mining, Boston,
MA, USA, August 20-23, 2000}, edited by Raghu Ramakrishnan, Salvatore J.
Stolfo, Roberto J. Bayardo, and Ismail Parsa, 71--80. {ACM}.

\bibitem[\citeproctext]{ref-deso20a}
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.
2020. {``{An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale}.''} \emph{arXiv e-Prints}, October,
arXiv:2010.11929.

\bibitem[\citeproctext]{ref-dredze2010we}
Dredze, Mark, Tim Oates, and Christine Piatko. 2010. {``We're Not in
Kansas Anymore: Detecting Domain Changes in Streams.''} In
\emph{Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing}, 585--95.

\bibitem[\citeproctext]{ref-emme18a}
Emmerich, Michael T. M., and AndrÃ©H. Deutz. 2018. {``A Tutorial on
Multiobjective Optimization: Fundamentals and Evolutionary Methods.''}
\emph{Natural Computing} 17 (3): 585--609.

\bibitem[\citeproctext]{ref-Forr08a}
Forrester, Alexander, AndrÃ¡s SÃ³bester, and Andy Keane. 2008.
\emph{{Engineering Design via Surrogate Modelling}}. Wiley.

\bibitem[\citeproctext]{ref-frie91a}
Friedman, Jerome H. 1991. {``Multivariate Adaptive Regression
Splines.''} \emph{The Annals of Statistics} 19 (1): 1--67.

\bibitem[\citeproctext]{ref-gabe05a}
Gaber, Mohamed Medhat, Arkady Zaslavsky, and Shonali Krishnaswamy. 2005.
{``Mining Data Streams: {A} Review.''} \emph{SIGMOD Rec.} 34: 18--26.

\bibitem[\citeproctext]{ref-gama14b}
Gama, JoÃ£o, Pedro Medas, Gladys Castillo, and Pedro Rodrigues. 2004.
{``Learning with Drift Detection.''} In \emph{Advances in Artificial
Intelligence -- SBIA 2004}, edited by Ana L. C. Bazzan and Sofiane
Labidi, 286--95. Berlin, Heidelberg: Springer Berlin Heidelberg.

\bibitem[\citeproctext]{ref-gama13a}
Gama, JoÃ£o, Raquel SebastiÃ£o, and Pedro Pereira Rodrigues. 2013. {``On
Evaluating Stream Learning Algorithms.''} \emph{Machine Learning} 90
(3): 317--46.

\bibitem[\citeproctext]{ref-Gram20a}
Gramacy, Robert B. 2020. \emph{Surrogates}. {CRC} press.

\bibitem[\citeproctext]{ref-hari65a}
Harington, J. 1965. {``The Desirability Function.''} \emph{Industrial
Quality Control} 21: 494--98.

\bibitem[\citeproctext]{ref-Hart95a}
Hartung, Joachim, BÃ¤rbel Elpert, and Karl-Heinz KlÃ¶sener. 1995.
\emph{{Statistik}}. Oldenbourg.

\bibitem[\citeproctext]{ref-Hast17a}
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. \emph{The
Elements of Statistical Learning}. Second. Springer.

\bibitem[\citeproctext]{ref-he15a}
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. {``Deep
Residual Learning for Image Recognition.''}

\bibitem[\citeproctext]{ref-he16a}
---------. 2016. {``{Identity Mappings in Deep Residual Networks}.''}
\emph{arXiv e-Prints}, March, arXiv:1603.05027.

\bibitem[\citeproctext]{ref-hoeg07a}
Hoeglinger, Stefan, and Russel Pears. 2007. {``Use of Hoeffding Trees in
Concept Based Data Stream Mining.''} \emph{2007 Third International
Conference on Information and Automation for Sustainability}, 57--62.

\bibitem[\citeproctext]{ref-ikon12a}
Ikonomovska, Elena. 2012. {``Algorithms for Learning Regression Trees
and Ensembles on Evolving Data Streams.''} PhD thesis, Jozef Stefan
International Postgraduate School.

\bibitem[\citeproctext]{ref-ikon11a}
Ikonomovska, Elena, JoÃ£o Gama, and SaÅ¡o DÅ¾eroski. 2011. {``Learning
Model Trees from Evolving Data Streams.''} \emph{Data Mining and
Knowledge Discovery} 23 (1): 128--68.

\bibitem[\citeproctext]{ref-jain19a}
Jain, Sarthak, and Byron C. Wallace. 2019. {``{Attention is not
Explanation}.''} \emph{arXiv e-Prints}, February, arXiv:1902.10186.

\bibitem[\citeproctext]{ref-Jame14a}
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
2014. \emph{{An Introduction to Statistical Learning with Applications
in R}}. 7th ed. Springer.

\bibitem[\citeproctext]{ref-karl22c}
Karl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan
Coors, Martin Binder, Lennart Schneider, et al. 2023. {``Multi-Objective
Hyperparameter Optimization in Machine Learning---an Overview.''}
\emph{ACM Trans. Evol. Learn. Optim.} 3 (4).

\bibitem[\citeproctext]{ref-kell04a}
Keller-McNulty, Sallie, ed. 2004. \emph{Statistical Analysis of Massive
Data Streams: {P}roceedings of a Workshop}. Washington, DC: Committee on
Applied; Theoretical Statistics, National Research Council; National
Academies Press.

\bibitem[\citeproctext]{ref-kidg22a}
Kidger, Patrick. 2022. {``{On Neural Differential Equations}.''}
\emph{arXiv e-Prints}, February, arXiv:2202.02435.

\bibitem[\citeproctext]{ref-Koha95a}
Kohavi, Ron. 1995. {``A Study of Cross-Validation and Bootstrap for
Accuracy Estimation and Model Selection.''} In \emph{Proceedings of the
14th International Joint Conference on Artificial Intelligence - Volume
2}, 1137--43. IJCAI'95. San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc.

\bibitem[\citeproctext]{ref-kuhn16a}
Kuhn, Max. 2016. {``Desirability: Function Optimization and Ranking via
Desirability Functions.''}

\bibitem[\citeproctext]{ref-Torczon00}
Lewis, R M, V Torczon, and M W Trosset. 2000. {``{Direct search methods:
Then and now}.''} \emph{Journal of Computational and Applied
Mathematics} 124 (1--2): 191--207.

\bibitem[\citeproctext]{ref-Li16a}
Li, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and
Ameet Talwalkar. 2016. {``{Hyperband: A Novel Bandit-Based Approach to
Hyperparameter Optimization}.''} \emph{arXiv e-Prints}, March,
arXiv:1603.06560.

\bibitem[\citeproctext]{ref-lipp22a}
Lippe, Phillip. 2022. {``{UvA} Deep Learning Tutorials.''}
\url{https://github.com/phlippe/uvadlc_notebooks/tree/master}.

\bibitem[\citeproctext]{ref-liu19a}
Liu, Liyuan, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu,
Jianfeng Gao, and Jiawei Han. 2019. {``{On the Variance of the Adaptive
Learning Rate and Beyond}.''} \emph{arXiv e-Prints}, August,
arXiv:1908.03265.

\bibitem[\citeproctext]{ref-mana18a}
Manapragada, Chaitanya, Geoffrey I. Webb, and Mahsa Salehi. 2018.
{``Extremely Fast Decision Tree.''} In \emph{KDD' 2018 - Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining}, edited by Chih-Jen Lin and Hui Xiong, 1953--62. United
States of America: Association for Computing Machinery (ACM).
\url{https://doi.org/10.1145/3219819.3220005}.

\bibitem[\citeproctext]{ref-masud2011classification}
Masud, Mohammad, Jing Gao, Latifur Khan, Jiawei Han, and Bhavani M
Thuraisingham. 2011. {``Classification and Novel Class Detection in
Concept-Drifting Data Streams Under Time Constraints.''} \emph{IEEE
Transactions on Knowledge and Data Engineering} 23 (6): 859--74.

\bibitem[\citeproctext]{ref-Meignan:2015vp}
Meignan, David, Sigrid Knust, Jean-Marc Frayet, Gilles Pesant, and
Nicolas Gaud. 2015. {``{A Review and Taxonomy of Interactive
Optimization Methods in Operations Research}.''} \emph{ACM Transactions
on Interactive Intelligent Systems}, September.

\bibitem[\citeproctext]{ref-Mont01a}
Montgomery, D C. 2001. \emph{{Design and Analysis of Experiments}}. 5th
ed. New York NY: Wiley.

\bibitem[\citeproctext]{ref-mont20a}
Montiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey
Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021.
{``River: Machine Learning for Streaming Data in Python.''}

\bibitem[\citeproctext]{ref-mour19a}
Mourtada, Jaouad, Stephane Gaiffas, and Erwan Scornet. 2019. {``{AMF:
Aggregated Mondrian Forests for Online Learning}.''} \emph{arXiv
e-Prints}, June, arXiv:1906.10529.
\url{https://doi.org/10.48550/arXiv.1906.10529}.

\bibitem[\citeproctext]{ref-Myers2016}
Myers, Raymond H, Douglas C Montgomery, and Christine M Anderson-Cook.
2016. \emph{Response Surface Methodology: Process and Product
Optimization Using Designed Experiments}. John Wiley \& Sons.

\bibitem[\citeproctext]{ref-neld65a}
Nelder, J. A., and R. Mead. 1965. {``{A Simplex Method for Function
Minimization}.''} \emph{The Computer Journal} 7 (4): 308--13.

\bibitem[\citeproctext]{ref-nino15a}
Nino, Esmeralda, Juan Rosas Rubio, Samuel Bonet, Nazario
Ramirez-Beltran, and Mauricio Cabrera-Rios. 2015. {``Multiple Objective
Optimization Using Desirability Functions for the Design of a 3D Printer
Prototype.''} In.

\bibitem[\citeproctext]{ref-nist25a}
{``{NIST/SEMATECH e-Handbook of Statistical Methods}.''} 2021.

\bibitem[\citeproctext]{ref-olss75a}
Olsson, Donald M, and Lloyd S Nelson. 1975. {``The Nelder-Mead Simplex
Procedure for Function Minimization.''} \emph{Technometrics} 17 (1):
45--51.

\bibitem[\citeproctext]{ref-pedr11a}
Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.
Grisel, M. Blondel, et al. 2011. {``Scikit-Learn: Machine Learning in
{P}ython.''} \emph{Journal of Machine Learning Research} 12: 2825--30.

\bibitem[\citeproctext]{ref-pont87a}
Pontryagin. 1987. \emph{Mathematical Theory of Optimal Processes}.
Routledge.

\bibitem[\citeproctext]{ref-puta21a}
Putatunda, Sayan. 2021. \emph{Practical Machine Learning for Streaming
Data with Python}. Springer.

\bibitem[\citeproctext]{ref-rumm76a}
Rummel, R. J. 1976. {``Understanding Correlation.''}
\url{https://www.hawaii.edu/powerkills/UC.HTM}.

\bibitem[\citeproctext]{ref-Sant03a}
Santner, T J, B J Williams, and W I Notz. 2003. \emph{{The Design and
Analysis of Computer Experiments}}. Berlin, Heidelberg, New York:
Springer.

\bibitem[\citeproctext]{ref-stre01a}
Street, W. Nick, and YongSeog Kim. 2001. {``A Streaming Ensemble
Algorithm (SEA) for Large-Scale Classification.''} In \emph{Proceedings
of the Seventh ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining}, 377--82. KDD '01. New York, NY, USA:
Association for Computing Machinery.

\bibitem[\citeproctext]{ref-tay20a}
Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020.
{``{Efficient Transformers: A Survey}.''} \emph{arXiv e-Prints},
September, arXiv:2009.06732.

\bibitem[\citeproctext]{ref-vasw17a}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
{``{Attention Is All You Need}.''} \emph{arXiv e-Prints}, June, 1--15.

\bibitem[\citeproctext]{ref-wang07a}
Wang, Zhiqiang. 2007. {``Two Postestimation Commands for Assessing
Confounding Effects in Epidemiological Studies.''} \emph{The Stata
Journal} 7 (2): 183--96.

\bibitem[\citeproctext]{ref-weih99a}
Weihe, Karsten, Ulrik Brandes, Annegret Liebers, Matthias MÄ±Ì
ller-Hannemann, Dorothea Wagner, and Thomas Willhalm. 1999.
{``{Empirical Design of Geometric Algorithms}.''} In \emph{SCG '99:
Proceedings of the Fifteenth Annual Symposium on Computational
Geometry}, 86--94. New York NY: Association for Computing Machinery.

\bibitem[\citeproctext]{ref-wieg19a}
Wiegreffe, Sarah, and Yuval Pinter. 2019. {``{Attention is not not
Explanation}.''} \emph{arXiv e-Prints}, August, arXiv:1908.04626.

\bibitem[\citeproctext]{ref-wiki25a}
Wikipedia contributors. 2024. {``Partial Correlation --- {Wikipedia}{,}
the Free Encyclopedia.''}
\url{https://en.wikipedia.org/w/index.php?title=Partial_correlation&oldid=1253637419}.

\end{CSLReferences}


\backmatter

\glsaddall
\printglossary[type=symbolslist,style=long]   % list of symbols
\printglossary[type=main]                     % main glossary
\printindex


\end{document}
