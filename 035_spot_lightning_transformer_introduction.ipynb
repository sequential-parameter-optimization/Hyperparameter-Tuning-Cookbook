{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e7a749b2",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6b0ba",
   "metadata": {},
   "source": [
    "# HPT PyTorch Lightning Transformer: Introduction\n",
    "\n",
    "In this tutorial, we will introduce transformer.\n",
    "\n",
    "## Decoder-Only Transformer\n",
    "\n",
    "### Embedding\n",
    "\n",
    "#### Neural Network for Embeddings\n",
    "\n",
    "Idea for word embeddings: use a relatively simple NN that has one input for every token (word, symbol) in the vocabulary. The output of the NN is a vector of a fixed size, which is the word embedding. For simplicity, we will use a 2-dimensional output vector. The weights of the NN are randomly initialized, and are learned during training.\n",
    "\n",
    "All tokens are embedded in this way. For each token there are two numerical values, the embedding vector.\n",
    "The same network is used for embedding all tokens.\n",
    "If a longer input is added, it can be embedded with the same net.\n",
    "\n",
    "#### Positional Encoding for the Embeddings\n",
    "\n",
    "Positional encoding is added to the input embeddings to give the model some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed.\n",
    "\n",
    "If a token occurs several times, it is embedded several times and receives different embedding vectors, as the position is taken into account by the positional encoding.\n",
    "\n",
    "### Masked Self-Attention\n",
    "\n",
    "How similar is each token to itself and to all preceding tokens in the input (sentence)?\n",
    "Masked self-attention is an autoregressive mechanism, which means that the attention mechanism is only allowed to look at the tokens that have already been processed.\n",
    "Calculation of the mask self-attention:\n",
    "\n",
    "1. Queries: Calculate two new values from the (two) values of the embedding vector using an NN, which are referred to as query values.\n",
    "2. Keys: Calculate two new values, called key values, from the (two) values of the embedding vector using an NN.\n",
    "3. Dot product: Calculate the dot product of the query values and the key values. This is a measure of the similarity of the query and key values.\n",
    "4. Softmax: Apply the softmax function to the outputs from the dot product. This is a measure of the attention that a token pays to other tokens.\n",
    "5. Values: Calculate two new values from the (two) values of the embedding vector using an NN, which are referred to as value values.\n",
    "6. The values are multiplied (weighted) by the values of the softmax function.\n",
    "7. The weighted values are summed. Now we have the mask-self attention value for the token.\n",
    "\n",
    "If the masked self-attention method is applied to the first token, the masked self-attention value is exactly the value of the first token, as it only takes itself into account.\n",
    "For the other tokens, the masked self-attention value is a weighted sum of the values of the previous tokens. The weighting is determined by the similarity of the query values and the key values (dot product and softmax).\n",
    "\n",
    "### Generation of outputs\n",
    "\n",
    "To calculate the output, we use a residual connector that adds the output of the neural network and the output of the masked self-attention method. We thus obtain the residual connection values. The residual connector is used to facilitate training.\n",
    "\n",
    "To generate the next token, we use another neural network that calculates the output from the (two) residual connection values.\n",
    "The input layer of the neural network has the size of the residual connection values, the output layer has the number of tokens in the vocabulary as a dimension.\n",
    "\n",
    "If we now enter the residual connection value of the first token, we receive the token (or the probabilities using Softmax) that is to come next as the output of the neural network. \n",
    "This makes sense even if we already know the second token (as with the first token): We can use it to calculate the error of the neural network and train the network.\n",
    "In addition, the decoder-transformer uses the masked self-attention method to calculate the output, i.e. the encoding and generation of new tokens is done with exactly the same elements of the network.\n",
    "\n",
    "Note: ChatGPT does not use a new neural network, but the same network that was already used to calculate the embedding. The network is therefore used for embedding, masked self-attention and calculating the output. In the last calculation, the network is inverted, i.e. it is run in the opposite direction to obtain the tokens and not the embeddings as in the original run.\n",
    "\n",
    "### End-Of-Sequence-Token\n",
    "\n",
    "The end-of-sequence token is used to signal the end of the input and also to start generating new tokens after the input.\n",
    "The EOS token recognizes all other tokens, as it comes after all tokens.\n",
    "When generating tokens, it is important to consider the relationships between the input tokens and the generation of new tokens. With a decoder-only transformer, the masked self-attention method is used to take the relationships into account.\n",
    "\n",
    "# Example: Word Predictor\n",
    "\n",
    "* The following code is based on [https://github.com/phlippe/uvadlc_notebooks/tree/master](https://github.com/phlippe/uvadlc_notebooks/tree/master) (Author: Phillip Lippe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff78743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/pvtj6mt91znd0hftcztqb0k00000gn/T/ipykernel_18368/973290807.py:14: DeprecationWarning:\n",
      "\n",
      "`set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning:\n",
      "\n",
      "Failed to load image Python extension: 'dlopen(/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <F6236B89-E4CA-3330-B665-E463D537EAF3> /Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <E854E4B4-D8A9-321E-9852-69F8F3B956BB> /Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1650x1050 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial6\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ffe866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc86daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55382048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[ 0.3367,  0.1288],\n",
      "        [ 0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863]])\n",
      "K\n",
      " tensor([[ 2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674],\n",
      "        [ 0.5349,  0.8094]])\n",
      "V\n",
      " tensor([[ 1.1103, -1.6898],\n",
      "        [-0.9890,  0.9580],\n",
      "        [ 1.3221,  0.8172]])\n",
      "Values\n",
      " tensor([[ 0.5698, -0.1520],\n",
      "        [ 0.5379, -0.0265],\n",
      "        [ 0.2246,  0.5556]])\n",
      "Attention\n",
      " tensor([[0.4028, 0.2886, 0.3086],\n",
      "        [0.3538, 0.3069, 0.3393],\n",
      "        [0.1303, 0.4630, 0.4067]])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "pl.seed_everything(42)\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
