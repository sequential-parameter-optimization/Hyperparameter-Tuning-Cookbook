---
execute:
  cache: false
  eval: true
  echo: true
  warning: false
---

<!-- exercises_rbf -->

## Exercise RBF


### Package Loading

```{python}
%matplotlib inline
import numpy as np
from numpy.matlib import eye
import scipy.linalg
from numpy import linalg as LA
from spotPython.design.spacefilling import spacefilling
from spotPython.fun.objectivefunctions import analytical
import matplotlib.pyplot as plt

```

### Define a small number

```{python}
eps = np.sqrt(np.spacing(1))
```

### The Sampling Plan (X)

* We will use 256 points.
* The first 10 points are shown below.

```{python}
gen = spacefilling(2)
rng = np.random.RandomState(1)
lower = np.array([-1,-1])
upper = np.array([2,2])
X = gen.scipy_lhd(256, lower=lower, upper = upper)
X[1:10]
```

### The Objective Function

* Here we use $\sum_{i=1}^n (x_i-1)^2$.
* `f_map()` is a helper function that maps $f$ to the entries (points) in the matrix $X$.

```{python}
def f(x):
    return np.sum((x-1.0)**2)

def f_map(x):
    return np.array(list(map(f, x)))
```

```{python}
y = f_map(X)
y[1:10]
```

* Alternatively, we can use pre-defined functions from the `pyspot` package:

```{python}
# fun = analytical(sigma=0).fun_branin
# fun = analytical(sigma=0).fun_sphere
```

```{python}
XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))
zz = np.array([f_map(np.array([xi, yi]).reshape(-1,2)) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)
```

```{python}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')
```

```{python}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 10))
```

### The Gram Matrix

```{python}
def build_Gram(X):
        """
        Construction of the Gram matrix.
        """
        n = X.shape[0]
        G = np.zeros((n, n))
        for i in range(n):
            for j in range(i, n):
                G[i, j] = np.linalg.norm(X[i] - X[j])
        G = G + G.T    
        return G
```

```{python}
G = build_Gram(X)
np.round(G,2)
```

### The Radial Basis Functions

```{python}
def basis_linear(r):
    return r*r*r
```

```{python}
def basis_gauss(r, sigma = 1e-1):
    return np.exp(-r**2/sigma)
```

+ We select the Gaussian basis function for the following examples:

```{python}
basis = basis_gauss
```

### The $\Psi$ Matrix

```{python}
def build_Phi(G, basis, eps=np.sqrt(np.spacing(1))):
    n = G.shape[0]
    Phi = np.zeros((n,n))
    for i in range(n):
        for j in range(n):
            Phi[i,j] = basis(G[i,j])
    Phi = Phi +  np.multiply(np.mat(eye(n)), eps)
    return Phi
```

```{python}
Phi = build_Phi(G, basis=basis)
Phi[0:3,0:3]
```

### Inverting $\Psi$ via Cholesky Factorization

* There a two different implementations of the Cholesky factorization oin Python:
  * `numpy`'s  `linalg.cholesky()` and
  * `scipy`'s  `linalg.cholesky()`
* We will use `numpy`'s version.

```{python}
def get_rbf_weights(Phi, y):
    """ 
    Calculating the weights of the radial basis function surrogate.
    Cholesky factorization used.
    LU decomposition otherwise (not implemented yet).
    """
    # U = scipy.linalg.cholesky(Phi, lower=True)
    U = np.linalg.cholesky(Phi)
    U = U.T
    # w = U\(U'\ModelInfo.y)
    w = np.linalg.solve(U, np.linalg.solve(U.T, y))
    return w
```

```{python}
w = get_rbf_weights(Phi, y)
w[0:3]
```

### Predictions

#### The Predictor

```{python}
def pred_rbf(x, X, basis, w):
    n = X.shape[0]
    d = np.zeros((n))
    phi = np.zeros((n))
    for i in range(n):
        d[i] = np.linalg.norm(x - X[i])
    for i in range(n):
        phi[i] = basis(d[i])
    return w @ phi    
```

#### Testing some Example Points

```{python}
x = X[0]
x
```

#### The RBF Prediction $\hat{f}$

```{python}
pred_rbf(x=x, X=X, basis=basis, w=w)
```

#### The Original (True) Value $f$

```{python}
f_map(np.array(x).reshape(1,-1))
```

#### Visualizations

```{python}
XX, YY = np.meshgrid(np.linspace(-1, 2, 128), np.linspace(-1, 2, 128))
zz = np.array([pred_rbf(x=np.array([xi, yi]), X=X, basis=basis,w=w) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(128,128)
```

```{python}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.pcolormesh(XX, YY, zz, vmin=-1, vmax=1, cmap='RdBu_r')
```

```{python}
fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')
co = ax.contourf(XX, YY, zz, levels=np.linspace(0,2, 5))
```

#### Note

The original function $f$ is cheaper than the surrogate $\hat{f}$ in this example, because we have chosen a simple analytical function as the ground truth. This is not the case in real-world settings.

### Cholesky Factorization

##### $A = U^T U$

* $U$ is an upper triangular matrix

```{python}
def cholesky_U(A):
    N = A.shape[0]
    U = np.zeros((N,N))
    for k in range(0,N):
         # compute diagonal entry
         U[k,k] = A[k,k]
         for j in range(0,k):
             U[k,k] = U[k,k] - U[j,k]*U[j,k]
         U[k,k] = np.sqrt(U[k,k])
         # compute remaining column
         for i in range(k+1,N):
             U[k,i] = A[k,i]
             for j in range(0,k):
                 U[k,i] = U[k,i] - U[j,i]*U[j,k]
             U[k,i] = U[k,i] / U[k,k]
    return U
```

#### $A = L L^T$

$L$ is a lower triangular matrix

```{python}
def cholesky_L(A):
    N = A.shape[0]
    L = np.zeros((N,N))
    for k in range(0,N):
         # compute diagonal entry
         L[k,k] = A[k,k]
         for j in range(0,k):
             L[k,k] = L[k,k] - L[k,j]*L[k,j]
         L[k,k] = np.sqrt(L[k,k])
         # compute remaining column
         for i in range(k+1,N):
             L[i,k] = A[i,k]
             for j in range(0,k):
                 L[i,k] = L[i,k] - L[i,j]*L[k,j]
             L[i,k] = L[i,k] / L[k,k]
    return L
```

#### Example

```{python}
A = np.array([[4, 2, 4, 4], [2, 10, 5, 2], [4, 5, 9, 6], [4, 2, 6, 9]])
A
```

#### Check: Is $A$ positive definite?

```{python}
assert(np.all(np.linalg.eigvals(A) > 0))
```

####  $A = U^T U$

Perform Cholesky Factorization

```{python}
U = cholesky_U(A)
U
```

Test Result

```{python}
U.T @ U
```

####  $A = L L^T$

```{python}
L = cholesky_L(A)
L
```

Test Result

```{python}
L @ L.T
```

## Exercises

### Gaussian Basis Function

* Plot the Gaussian Basis Function `basis_gauss` in the range from -2 to 2 using `matplotlib.pyplot`
  * Hint: Check the [matplotlib documentation](https://matplotlib.org/stable/tutorials/introductory/pyplot.html) for examples.
  * Generate a plot with several `sigma` values, e.g., 0.1, 1.0, and 10.
* What is the meaning of the `sigma` parameter: Can you explain its influence / effect on the model quality?
  * Is the `sigma` value important?

### Linear Basis Function

* Select the linear basis function?
* What errors occur?
* Do you have any ideas how to fix this error?


