{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Hyperparameter Tuning with PyTorch Lightning: ResNets {#sec-light-resnets-601}\n",
        "\n",
        "\n",
        "Neural ODEs are related to Residual Neural Networks (ResNets).\n",
        "We consider ResNets in @sec-resnets.\n",
        "\n",
        "\n",
        "## Residual Neural Networks {#sec-resnets}\n",
        "\n",
        "@he15a introduced Residual Neural Networks (ResNets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "#| label: import-libraries\n",
        "import os\n",
        "import numpy as np \n",
        "import random\n",
        "from PIL import Image\n",
        "from types import SimpleNamespace\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual Connections\n",
        "\n",
        "Residual connections are a key component of ResNets. They are used to stabilize the training of very deep networks. The idea is to learn a residual mapping instead of the full mapping. The residual mapping is defined as:\n",
        "\n",
        "::: {#def-residual-connection}\n",
        "\n",
        "### Residual Connection\n",
        "\n",
        "Let $F$ denote a non-linear mapping (usually a sequence of NN modules likes convolutions, activation functions, and normalizations).\n",
        "\n",
        "Instead of modeling\n",
        "$$\n",
        "x_{l+1}=F(x_{l}),\n",
        "$$\n",
        "residual connections model \n",
        "$$\n",
        "x_{l+1}=x_{l}+F(x_{l}).\n",
        "$$ {#eq-residual-connection}\n",
        "\n",
        "This is illustrated in @fig-block.\n",
        "\n",
        "![Residual Connection. Figure credit @he15a](./figures_static/block.png){width=70% #fig-block}\n",
        "\n",
        " \n",
        " Applying backpropagation to the residual mapping results in the following gradient calculation:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial x_{l+1}}{\\partial x_{l}} = \\mathbf{I} + \\frac{\\partial F(x_{l})}{\\partial x_{l}},\n",
        "$$ {#eq-residual-grad}\n",
        "\n",
        "where $\\mathbf{I}$ is the identity matrix. The identity matrix is added to the gradient, which helps to stabilize the training of very deep networks. The identity matrix ensures that the gradient is not too small, which can happen if the gradient of $F$ is close to zero. This is especially important for very deep networks, where the gradient can vanish quickly.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "The bias towards the identity matrix guarantees a stable gradient propagation being less effected by $F$ itself.\n",
        "\n",
        "There have been many variants of ResNet proposed, which mostly concern the function $F$, or operations applied on the sum. @fig-resnet-block shows two different ResNet blocks:\n",
        "\n",
        "* the original ResNet block, which applies a non-linear activation function, usually ReLU, after the skip connection. and \n",
        "* the pre-activation ResNet block, which applies the non-linearity at the beginning of $F$.\n",
        "\n",
        "\n",
        "![ResNet Block. Left: original Residual block in @he15a. Right: pre-activation block. BN  describes batch-normalization. Figure credit @he16a](./figures_static/resnet_block.svg){width=50% #fig-resnet-block}\n",
        "\n",
        "For very deep network the pre-activation ResNet has shown to perform better as the gradient flow is guaranteed to have the identity matrix as shown in @eq-residual-grad, and is not harmed by any non-linear activation applied to it. \n",
        "\n",
        "\n",
        "### Implementation of the Original ResNet Block\n",
        "\n",
        "One special case we have to handle is when we want to reduce the image dimensions in terms of width and height. The basic ResNet block requires $F(x_{l})$ to be of the same shape as $x_{l}$. Thus, we need to change the dimensionality of $x_{l}$ as well before adding to $F(x_{l})$. The original implementation used an identity mapping with stride 2 and padded additional feature dimensions with 0. However, the more common implementation is to use a 1x1 convolution with stride 2 as it allows us to change the feature dimensionality while being efficient in parameter and computation cost. The code for the ResNet block is relatively simple, and shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Number of input features\n",
        "            act_fn - Activation class constructor (e.g. nn.ReLU)\n",
        "            subsample - If True, we need to apply a transformation inside the block to change the feature dimensionality\n",
        "            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if not subsample:\n",
        "            c_out = c_in\n",
        "\n",
        "        # Network representing F\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(c_in, c_out, bias=False),  # Linear layer for feature transformation\n",
        "            nn.BatchNorm1d(c_out),               # Batch normalization for stable learning\n",
        "            act_fn(),                            # Activation function\n",
        "            nn.Linear(c_out, c_out, bias=False), # Second linear layer\n",
        "            nn.BatchNorm1d(c_out)                # Batch normalization\n",
        "        )\n",
        "        \n",
        "        # If subsampling, adjust the input feature dimensionality using a linear layer\n",
        "        self.downsample = nn.Linear(c_in, c_out) if subsample else None\n",
        "        self.act_fn = act_fn()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)  # Apply the main network\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)  # Adjust dimensionality if necessary\n",
        "        out = z + x  # Residual connection\n",
        "        out = self.act_fn(out)  # Apply activation function\n",
        "        return out\n",
        "\n",
        "class ResNetRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, block, num_blocks=1, hidden_dim=64, act_fn=nn.ReLU):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)  # Input layer transformation\n",
        "        self.blocks = nn.ModuleList([block(hidden_dim, act_fn) for _ in range(num_blocks)])  # List of ResNet blocks\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)  # Output layer for regression\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)  # Apply input layer\n",
        "        for block in self.blocks:\n",
        "            x = block(x)  # Apply each block\n",
        "        x = self.output_layer(x)  # Get final output\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "input_dim = 10\n",
        "output_dim = 1\n",
        "hidden_dim = 64\n",
        "model = ResNetRegression(input_dim, output_dim, ResNetBlock, num_blocks=2, hidden_dim=hidden_dim, act_fn=nn.ReLU)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "# Create a sample input tensor with a batch size of 2\n",
        "from torchviz import make_dot\n",
        "sample_input = torch.randn(2, input_dim)\n",
        "\n",
        "# Generate the visualization\n",
        "output = model(sample_input)\n",
        "dot = make_dot(output, params=dict(model.named_parameters()))\n",
        "\n",
        "# Save and render the visualization\n",
        "dot.format = 'png'\n",
        "dot.render('./figures_static/resnet_regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![ResNet Regression](./figures_static/resnet_regression.png){width=100%}\n",
        "\n",
        "\n",
        "\n",
        "### Implementation of the Pre-Activation ResNet Block\n",
        "\n",
        "\n",
        "The second block we implement is the pre-activation ResNet block. For this, we have to change the order of layer in `self.net`, and do not apply an activation function on the output. Additionally, the downsampling operation has to apply a non-linearity as well as the input, $x_l$, has not been processed by a non-linearity yet. Hence, the block looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PreActResNetBlock(nn.Module):\n",
        "    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n",
        "        super().__init__()\n",
        "        if not subsample:\n",
        "            c_out = c_in\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(c_in),  # Replacing BatchNorm1d with LayerNorm\n",
        "            act_fn(),\n",
        "            nn.Linear(c_in, c_out, bias=False),\n",
        "            nn.LayerNorm(c_out),\n",
        "            act_fn(),\n",
        "            nn.Linear(c_out, c_out, bias=False)\n",
        "        )\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.LayerNorm(c_in),\n",
        "            act_fn(),\n",
        "            nn.Linear(c_in, c_out, bias=False)\n",
        "        ) if subsample else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        out = z + x\n",
        "        return out\n",
        "\n",
        "class PreActResNetRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, block, num_blocks=1, hidden_dim=64, act_fn=nn.ReLU):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.blocks = nn.ModuleList([block(hidden_dim, act_fn) for _ in range(num_blocks)])\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "input_dim = 10\n",
        "output_dim = 1\n",
        "hidden_dim = 64\n",
        "model = PreActResNetRegression(input_dim, output_dim, PreActResNetBlock, num_blocks=2, hidden_dim=hidden_dim, act_fn=nn.ReLU)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from torchviz import make_dot\n",
        "# Create a sample input tensor\n",
        "sample_input = torch.randn(1, input_dim)\n",
        "\n",
        "# Generate the visualization\n",
        "output = model(sample_input)\n",
        "dot = make_dot(output, params=dict(model.named_parameters()))\n",
        "\n",
        "# Save and render the visualization\n",
        "dot.format = 'png'\n",
        "dot.render('./figures_static/preact_resnet_regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Pre-Activation ResNet Regression](./figures_static/preact_resnet_regression.png){width=100%}\n",
        "\n",
        "### The Overall ResNet Architecture\n",
        "\n",
        "The overall ResNet architecture for regression consists of stacking multiple ResNet blocks, of which some are downsampling the input. When discussing ResNet blocks within the entire network, they are usually grouped by output shape. If we describe the ResNet as having `[3,3,3]` blocks, it means there are three groups of ResNet blocks, each containing three blocks, with downsampling occurring in the first block of the second and third groups. The final layer produces continuous outputs suitable for regression tasks.\n",
        "\n",
        "![ResNet Notation. Figure credit @lipp22a](./figures_static/resnet_notation.svg){width=100%}\n",
        "\n",
        "The `output_dim` parameter is used to determine the number of outputs for regression. This is set to 1 for a single regression target by default, but can be adjusted for multiple targets. Note, a final layer without a softmax or similar classification layer has to be added for regression tasks.\n",
        "A similar notation is used by many other implementations such as in the [torchvision library](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet18) from PyTorch.\n",
        "\n",
        "\n",
        "::: {#exm-example-resnet}\n",
        "\n",
        "### Example ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_regression\n",
        "from types import SimpleNamespace\n",
        "\n",
        "def get_resnet_blocks_by_name():\n",
        "    return {\"ResNetBlock\": ResNetBlock}\n",
        "\n",
        "def get_act_fn_by_name():\n",
        "    return {\"relu\": nn.ReLU}\n",
        "\n",
        "# Define a simple ResNetBlock for fully connected layers\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n",
        "        super().__init__()\n",
        "        if not subsample:\n",
        "            c_out = c_in\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(c_in, c_out, bias=False),\n",
        "            nn.BatchNorm1d(c_out),\n",
        "            act_fn(),\n",
        "            nn.Linear(c_out, c_out, bias=False),\n",
        "            nn.BatchNorm1d(c_out)\n",
        "        )\n",
        "        \n",
        "        self.downsample = nn.Linear(c_in, c_out) if subsample else None\n",
        "        self.act_fn = act_fn()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        out = z + x\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "# Generate a simple random dataset for regression\n",
        "num_samples = 100\n",
        "num_features = 20  # Number of features, typical in a regression dataset\n",
        "X, y = make_regression(n_samples=num_samples, n_features=num_features, noise=0.1)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Add a dimension for compatibility\n",
        "\n",
        "# Define the ResNet model for regression\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_blocks=[3, 3, 3], c_hidden=[64, 64, 64], act_fn_name=\"relu\", block_name=\"ResNetBlock\", **kwargs):\n",
        "        super().__init__()\n",
        "        resnet_blocks_by_name = get_resnet_blocks_by_name()\n",
        "        act_fn_by_name = get_act_fn_by_name()\n",
        "        assert block_name in resnet_blocks_by_name\n",
        "        self.hparams = SimpleNamespace(output_dim=output_dim, \n",
        "                                       c_hidden=c_hidden, \n",
        "                                       num_blocks=num_blocks, \n",
        "                                       act_fn_name=act_fn_name,\n",
        "                                       act_fn=act_fn_by_name[act_fn_name],\n",
        "                                       block_class=resnet_blocks_by_name[block_name])\n",
        "        self._create_network(input_dim)\n",
        "        self._init_params()\n",
        "\n",
        "    def _create_network(self, input_dim):\n",
        "        c_hidden = self.hparams.c_hidden\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, c_hidden[0]),\n",
        "            self.hparams.act_fn()\n",
        "        )\n",
        "\n",
        "        blocks = []\n",
        "        for block_idx, block_count in enumerate(self.hparams.num_blocks):\n",
        "            for bc in range(block_count):\n",
        "                subsample = (bc == 0 and block_idx > 0)\n",
        "                blocks.append(\n",
        "                    self.hparams.block_class(c_in=c_hidden[block_idx if not subsample else block_idx-1],\n",
        "                                             act_fn=self.hparams.act_fn,\n",
        "                                             subsample=subsample,\n",
        "                                             c_out=c_hidden[block_idx])\n",
        "                )\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.output_net = nn.Linear(c_hidden[-1], self.hparams.output_dim)\n",
        "\n",
        "    def _init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_net(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.output_net(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = ResNet(input_dim=num_features, output_dim=1)\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Example training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    output = model(X_tensor)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = criterion(output, y_tensor)\n",
        "    \n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}