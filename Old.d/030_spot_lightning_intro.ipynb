{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Lightning with Sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "CHECKPOINT_PATH = \"runs\"\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from types import SimpleNamespace\n",
    "# import transforms\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/ConvNets\")\n",
    "\n",
    "\n",
    "# Function for setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data mean [0.49139968 0.48215841 0.44653091]\n",
      "Data std [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n",
    "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0, 1, 2))\n",
    "DATA_STD = (train_dataset.data / 255.0).std(axis=(0, 1, 2))\n",
    "print(\"Data mean\", DATA_MEANS)\n",
    "print(\"Data std\", DATA_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEANS, DATA_STD)])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(DATA_MEANS, DATA_STD),\n",
    "    ]\n",
    ")\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "L.seed_everything(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "L.seed_everything(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Data Handling with Sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import lightning as L\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "dataset = PKLDataset(directory=\"./userData/\",\n",
    "                     filename=\"data_sensitive.pkl\",\n",
    "                     target_column='N',\n",
    "                     feature_type=torch.float32,\n",
    "                     target_type=torch.float32,\n",
    "                     rmNA=True)\n",
    "fun_control = fun_control_init()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167 500 714\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(42)\n",
    "traindata_set, test_set = torch.utils.data.random_split(dataset, [0.7, 0.3])\n",
    "L.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(traindata_set, [0.7, 0.3])\n",
    "print(len(train_set), len(val_set), len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and std of the dataset\n",
    "DATA_MEANS = traindata_set.dataset.data.mean(axis=0)\n",
    "DATA_STD = traindata_set.dataset.data.std(axis=0)\n",
    "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEANS, DATA_STD)])\n",
    "# TODO: normalize the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 2\n",
      "Inputs Shape: torch.Size([2, 133])\n",
      "Targets Shape: torch.Size([2])\n",
      "---------------\n",
      "Inputs: tensor([[2.5100e+02, 1.3770e+03, 3.0000e+00, 2.0000e+00, 1.0000e+00, 6.0000e+00,\n",
      "         8.4600e+02, 1.1140e+03, 9.0000e+00, 1.2080e+03, 4.0000e+00, 6.0000e+00,\n",
      "         1.8330e+03, 1.7910e+03, 1.5590e+03, 2.3000e+01, 3.9000e+01, 5.0000e+01,\n",
      "         3.3000e+01, 0.0000e+00, 0.0000e+00, 3.4000e+01, 2.9000e+01, 0.0000e+00,\n",
      "         8.2700e+02, 9.1000e+02, 1.2100e+03, 1.2090e+03, 1.2080e+03, 1.7200e+03,\n",
      "         1.3600e+03, 1.2000e+01, 8.4600e+02, 1.0800e+03, 7.2100e+02, 6.8400e+02,\n",
      "         6.8200e+02, 6.7900e+02, 5.4700e+02, 6.8800e+02, 6.4400e+02, 1.5200e+02,\n",
      "         5.3800e+02, 6.4900e+02, 3.6200e+02, 3.8000e+01, 5.4300e+02, 6.0200e+02,\n",
      "         5.7800e+02, 4.6200e+02, 7.6000e+01, 6.2600e+02, 1.1800e+02, 3.1400e+02,\n",
      "         5.4100e+02, 6.2100e+02, 6.8400e+02, 6.8800e+02, 3.6200e+02, 6.8400e+02,\n",
      "         6.8400e+02, 5.9000e+02, 5.9000e+02, 6.7600e+02, 4.0500e+02, 4.3700e+02,\n",
      "         6.4900e+02, 6.4900e+02, 6.2500e+02, 5.7400e+02, 6.2200e+02, 6.3000e+02,\n",
      "         3.7000e+02, 7.2100e+02, 6.8700e+02, 7.2100e+02, 6.8600e+02, 6.7600e+02,\n",
      "         6.8000e+02, 6.8000e+02, 3.8000e+01, 5.4300e+02, 0.0000e+00, 7.4500e+02,\n",
      "         7.5800e+02, 4.6500e+02, 4.8900e+02, 1.8000e+02, 5.6000e+02, 2.5800e+02,\n",
      "         6.8400e+02, 1.7300e+02, 6.7000e+02, 6.2600e+02, 6.0200e+02, 5.7700e+02,\n",
      "         1.9300e+02, 6.5500e+02, 7.5700e+02, 2.6500e+02, 6.7700e+02, 6.8400e+02,\n",
      "         3.2500e+02, 6.5500e+02, 1.0100e+02, 2.0500e+02, 2.7200e+02, 1.7800e+02,\n",
      "         1.8500e+02, 1.8200e+02, 6.3500e+02, 4.8000e+02, 6.0500e+02, 6.8200e+02,\n",
      "         7.4200e+02, 6.8300e+02, 7.2500e+02, 6.7600e+02, 6.7800e+02, 6.7800e+02,\n",
      "         2.0500e+02, 2.0500e+02, 5.0100e+02, 7.2800e+02, 6.2200e+02, 6.7700e+02,\n",
      "         6.8200e+02, 3.2600e+02, 1.3800e+02, 6.0200e+02, 5.6700e+02, 2.0000e+01,\n",
      "         2.0000e+01],\n",
      "        [1.6560e+03, 9.1700e+02, 2.6000e+01, 0.0000e+00, 2.0000e+00, 0.0000e+00,\n",
      "         7.6500e+02, 8.4700e+02, 2.2000e+01, 2.3100e+03, 3.0000e+00, 1.9000e+01,\n",
      "         2.5600e+02, 9.0800e+02, 1.7690e+03, 6.4000e+01, 6.5000e+01, 6.2000e+01,\n",
      "         7.3000e+01, 7.2000e+01, 0.0000e+00, 5.0000e+00, 5.0000e+00, 2.0000e+00,\n",
      "         7.9000e+01, 7.1000e+01, 2.3110e+03, 2.3100e+03, 2.3100e+03, 1.5250e+03,\n",
      "         1.9650e+03, 3.0000e+00, 7.6500e+02, 8.2500e+02, 4.4900e+02, 4.6100e+02,\n",
      "         4.7000e+02, 4.7200e+02, 4.6700e+02, 4.2600e+02, 6.6300e+02, 2.1600e+02,\n",
      "         5.0800e+02, 5.0800e+02, 7.4700e+02, 2.5200e+02, 5.1300e+02, 5.5400e+02,\n",
      "         6.1600e+02, 4.2800e+02, 1.3500e+02, 4.7900e+02, 3.2700e+02, 3.9000e+02,\n",
      "         1.0100e+02, 1.5800e+02, 4.7400e+02, 4.6600e+02, 7.4700e+02, 5.1200e+02,\n",
      "         5.1200e+02, 4.8900e+02, 4.8900e+02, 4.6200e+02, 2.5400e+02, 2.6500e+02,\n",
      "         5.0800e+02, 5.0800e+02, 4.8400e+02, 4.8800e+02, 4.5700e+02, 4.5600e+02,\n",
      "         4.9200e+02, 4.4900e+02, 4.6000e+02, 4.4900e+02, 4.6000e+02, 4.6200e+02,\n",
      "         4.6100e+02, 4.6100e+02, 2.5200e+02, 5.1300e+02, 0.0000e+00, 5.1800e+02,\n",
      "         4.4100e+02, 2.9200e+02, 4.6000e+02, 1.8800e+02, 5.3800e+02, 2.9300e+02,\n",
      "         5.1200e+02, 4.1200e+02, 6.2700e+02, 4.8400e+02, 5.5400e+02, 6.1600e+02,\n",
      "         2.0200e+02, 6.0800e+02, 4.0500e+02, 1.6700e+02, 4.7400e+02, 4.4400e+02,\n",
      "         4.2300e+02, 6.7200e+02, 2.4300e+02, 1.1700e+02, 5.1900e+02, 1.7100e+02,\n",
      "         1.7100e+02, 1.7100e+02, 5.2200e+02, 5.2600e+02, 5.3100e+02, 4.7000e+02,\n",
      "         4.5300e+02, 4.6900e+02, 4.4700e+02, 4.6800e+02, 4.6700e+02, 4.6800e+02,\n",
      "         1.7400e+02, 1.7400e+02, 4.3800e+02, 4.2800e+02, 5.0400e+02, 4.7400e+02,\n",
      "         4.4500e+02, 4.2700e+02, 2.6000e+02, 5.5400e+02, 5.1900e+02, 4.3000e+01,\n",
      "         4.3000e+01]])\n",
      "Targets: tensor([1667., 1467.])\n"
     ]
    }
   ],
   "source": [
    "# Set batch size for DataLoader\n",
    "batch_size = 2\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLightRegression(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        l1: int,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        initialization: str,\n",
    "        act_fn: nn.Module,\n",
    "        optimizer: str,\n",
    "        dropout_prob: float,\n",
    "        lr_mult: float,\n",
    "        patience: int,\n",
    "        _L_in: int,\n",
    "        _L_out: int,\n",
    "    ):\n",
    "        \"\"\"NetLightRegression.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model/CNN to run. Used for creating the model (see function below)\n",
    "            model_hparams: Hyperparameters for the model, as dictionary.\n",
    "            optimizer_name: Name of the optimizer to use. Currently supported: Adam, SGD\n",
    "            optimizer_hparams: Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        # super().__init__()\n",
    "        # # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        # self.save_hyperparameters()\n",
    "        # # Create model\n",
    "        # self.model = create_model(model_name, model_hparams)\n",
    "        # # Create loss module\n",
    "        # self.loss_module = nn.CrossEntropyLoss()\n",
    "        # # Example input for visualizing the graph in Tensorboard\n",
    "        # self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n",
    "\n",
    "        super().__init__()\n",
    "        self._L_in = _L_in\n",
    "        self._L_out = _L_out\n",
    "        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n",
    "        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n",
    "        # set dummy input array for Tensorboard Graphs\n",
    "        # set log_graph=True in Trainer to see the graph (in traintest.py)\n",
    "        self.example_input_array = torch.zeros((batch_size, self._L_in))\n",
    "        if self.hparams.l1 < 4:\n",
    "            raise ValueError(\"l1 must be at least 4\")\n",
    "\n",
    "        hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n",
    "\n",
    "        # Create the network based on the specified hidden sizes\n",
    "        layers = []\n",
    "        layer_sizes = [self._L_in] + hidden_sizes\n",
    "        layer_size_last = layer_sizes[0]\n",
    "        for layer_size in layer_sizes[1:]:\n",
    "            layers += [\n",
    "                nn.Linear(layer_size_last, layer_size),\n",
    "                self.hparams.act_fn,\n",
    "                nn.Dropout(self.hparams.dropout_prob),\n",
    "            ]\n",
    "            layer_size_last = layer_size\n",
    "        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n",
    "        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    # def forward(self, imgs):\n",
    "    #     return self.model(imgs)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     if self.hparams.optimizer_name == \"Adam\":\n",
    "    #         optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "    #     elif self.hparams.optimizer_name == \"SGD\":\n",
    "    #         optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "    #     else:\n",
    "    #         assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n",
    "    # # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
    "    #     scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "    #     return [optimizer], [scheduler]\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer = optimizer_handler(\n",
    "            optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    # def training_step(self, batch, batch_idx):\n",
    "    #     # \"batch\" is the output of the training data loader.\n",
    "    #     imgs, labels = batch\n",
    "    #     preds = self.model(imgs)\n",
    "    #     loss = self.loss_module(preds, labels)\n",
    "    #     acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "    #     # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
    "    #     self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "    #     self.log(\"train_loss\", loss)\n",
    "    #     return loss  # Return tensor to call \".backward\" on\n",
    "    def training_step(self, batch: tuple) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y = y.view(len(y), 1)\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        # mae_loss = F.l1_loss(y_hat, y)\n",
    "        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     imgs, labels = batch\n",
    "    #     preds = self.model(imgs).argmax(dim=-1)\n",
    "    #     acc = (labels == preds).float().mean()\n",
    "    #     # By default logs it per epoch (weighted average over batches)\n",
    "    #     self.log(\"val_acc\", acc)\n",
    "    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y = y.view(len(y), 1)\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        # mae_loss = F.l1_loss(y_hat, y)\n",
    "        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
    "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
    "        return val_loss\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     imgs, labels = batch\n",
    "    #     preds = self.model(imgs).argmax(dim=-1)\n",
    "    #     acc = (labels == preds).float().mean()\n",
    "    #     # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
    "    #     self.log(\"test_acc\", acc)\n",
    "    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = y.view(len(y), 1)\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        # mae_loss = F.l1_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n",
    "        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.utils.eda import generate_config_id\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelSummary\n",
    "from spotpython.torch.initialization import kaiming_init, xavier_init\n",
    "import os\n",
    "# def train_model(model_name, save_name=None, **kwargs):\n",
    "    # \"\"\"Train model.\n",
    "\n",
    "    # Args:\n",
    "    #     model_name: Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "    #     save_name (optional): If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    # \"\"\"\n",
    "    # if save_name is None:\n",
    "    #     save_name = model_name\n",
    "def train_model(config: dict, fun_control: dict) -> float:    \n",
    "    _L_in = fun_control[\"_L_in\"]\n",
    "    _L_out = fun_control[\"_L_out\"]\n",
    "    if fun_control[\"enable_progress_bar\"] is None:\n",
    "        enable_progress_bar = False\n",
    "    else:\n",
    "        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n",
    "    # config id is unique. Since the model is not loaded from a checkpoint,\n",
    "    # the config id is generated here with a timestamp.\n",
    "    config_id = generate_config_id(config, timestamp=True)\n",
    "    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n",
    "    initialization = config[\"initialization\"]\n",
    "    if initialization == \"Xavier\":\n",
    "        xavier_init(model)\n",
    "    elif initialization == \"Kaiming\":\n",
    "        kaiming_init(model)\n",
    "    else:\n",
    "        pass\n",
    "    dm = LightDataModule(\n",
    "        dataset=fun_control[\"data_set\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=fun_control[\"num_workers\"],\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        test_seed=fun_control[\"test_seed\"],\n",
    "    )\n",
    "    dm.setup()\n",
    "    print(f\"train_model(): Test set size: {len(dm.data_test)}\")\n",
    "    print(f\"train_model(): Train set size: {len(dm.data_train)}\")\n",
    "    print(f\"train_model(): Batch size: {config['batch_size']}\")\n",
    "\n",
    "    # # Create a PyTorch Lightning trainer with the generation callback\n",
    "    # trainer = L.Trainer(\n",
    "    #     default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),  # Where to save models\n",
    "    #     # We run on a single GPU (if possible)\n",
    "    #     accelerator=\"auto\",\n",
    "    #     devices=1,\n",
    "    #     # How many epochs to train for if no patience is set\n",
    "    #     max_epochs=180,\n",
    "    #     callbacks=[\n",
    "    #         ModelCheckpoint(\n",
    "    #             save_weights_only=True, mode=\"max\", monitor=\"val_acc\"\n",
    "    #         ),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "    #         LearningRateMonitor(\"epoch\"),\n",
    "    #     ],  # Log learning rate every epoch\n",
    "    # )  # In case your notebook crashes due to the progress bar, consider increasing the refresh rate\n",
    "    # trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    # trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "    trainer = L.Trainer(\n",
    "        # Where to save models\n",
    "        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n",
    "        max_epochs=model.hparams.epochs,\n",
    "        accelerator=fun_control[\"accelerator\"],\n",
    "        devices=fun_control[\"devices\"],\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=True\n",
    "        ),\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n",
    "            ModelSummary(max_depth=-1)\n",
    "        ],\n",
    "        enable_progress_bar=enable_progress_bar,\n",
    "    )\n",
    "\n",
    "\n",
    "    # # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    # pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    # if os.path.isfile(pretrained_filename):\n",
    "    #     print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "    #     # Automatically loads the model with the saved hyperparameters\n",
    "    #     model = CIFARModule.load_from_checkpoint(pretrained_filename)\n",
    "    # else:\n",
    "    #     L.seed_everything(42)  # To be reproducible\n",
    "    #     model = CIFARModule(model_name=model_name, **kwargs)\n",
    "    #     trainer.fit(model, train_loader, val_loader)\n",
    "    #     model = CIFARModule.load_from_checkpoint(\n",
    "    #         trainer.checkpoint_callback.best_model_path\n",
    "    #     )  # Load best checkpoint after training\n",
    "    \n",
    "    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n",
    "    trainer.fit(model=model, datamodule=dm)\n",
    "    # Test best model on validation and test set\n",
    "    # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n",
    "    result = trainer.validate(model=model, datamodule=dm)\n",
    "\n",
    "    # # Test best model on validation and test set\n",
    "    # val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    # test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    # result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    # return model, result\n",
    "    # unlist the result (from a list of one dict)\n",
    "    result = result[0]\n",
    "    print(f\"train_model result: {result}\")\n",
    "    return result[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from math import inf\n",
    "MAX_TIME = 10\n",
    "FUN_EVALS = inf\n",
    "FUN_REPEATS = 2\n",
    "OCBA_DELTA = 1\n",
    "REPEATS = 2\n",
    "INIT_SIZE = 10\n",
    "WORKERS = 0\n",
    "PREFIX=\"032\"\n",
    "DEVICE = getDevice()\n",
    "DEVICES = 1\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2024_01_12_11_07_29\n",
      "Created spot_tensorboard_path: runs/spot_logs/032_p040025_2024-01-12_11-07-29 for SummaryWriter()\n"
     ]
    }
   ],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=133,\n",
    "    _L_out=1,\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    fun_repeats=FUN_REPEATS,\n",
    "    log_level=10,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    ocba_delta = OCBA_DELTA,\n",
    "    show_progress=True,\n",
    "    test_size=TEST_SIZE,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    verbosity=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381\n"
     ]
    }
   ],
   "source": [
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(directory=\"./userData/\",\n",
    "                     filename=\"data_sensitive.pkl\",\n",
    "                     target_column='N',\n",
    "                     feature_type=torch.float32,\n",
    "                     target_type=torch.float32,\n",
    "                     rmNA=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "import sys\n",
    "sys.path.insert(0, './userModel')\n",
    "import netlightregression\n",
    "import light_hyper_dict\n",
    "add_core_model_to_fun_control(fun_control=fun_control,\n",
    "                              core_model=netlightregression.NetLightRegression,\n",
    "                              hyper_dict=light_hyper_dict.LightHyperDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name           | type   | default   |   lower |   upper | transform             |\n",
      "|----------------|--------|-----------|---------|---------|-----------------------|\n",
      "| l1             | int    | 3         |     3   |    8    | transform_power_2_int |\n",
      "| epochs         | int    | 4         |     4   |    9    | transform_power_2_int |\n",
      "| batch_size     | int    | 4         |     1   |    4    | transform_power_2_int |\n",
      "| act_fn         | factor | ReLU      |     0   |    5    | None                  |\n",
      "| optimizer      | factor | SGD       |     0   |   11    | None                  |\n",
      "| dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |\n",
      "| lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |\n",
      "| patience       | int    | 2         |     2   |    6    | transform_power_2_int |\n",
      "| initialization | factor | Default   |     0   |    2    | None                  |\n"
     ]
    }
   ],
   "source": [
    "from spotpython.utils.eda import gen_design_table\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Default Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "# import logging\n",
    "# import numpy as np\n",
    "# import pprint\n",
    "# from numpy.random import default_rng\n",
    "# # from spotpython.light.trainmodel import train_model\n",
    "# from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name, get_one_config_from_X\n",
    "\n",
    "# X = get_default_hyperparameters_as_array(fun_control)\n",
    "# var_dict = assign_values(X, get_var_name(fun_control))\n",
    "# for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "#     print(config)\n",
    "#     train_model(config, fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Tuned Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[\n",
    "            8.,\n",
    "            8.,\n",
    "            6.,\n",
    "            2,\n",
    "            2.,\n",
    "            0.05448797,\n",
    "            1.51706109, \n",
    "            4.,\n",
    "            0\n",
    "            ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': 256,\n",
       " 'epochs': 256,\n",
       " 'batch_size': 64,\n",
       " 'act_fn': ReLU(),\n",
       " 'optimizer': 'Adam',\n",
       " 'dropout_prob': 0.05448797,\n",
       " 'lr_mult': 1.51706109,\n",
       " 'patience': 16,\n",
       " 'initialization': 'Default'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spotpython.hyperparameters.values import get_one_config_from_X\n",
    "\n",
    "cfg = get_one_config_from_X(X, fun_control)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model(): Test set size: 715\n",
      "train_model(): Train set size: 1167\n",
      "train_model(): Batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name      | Type       | Params | In sizes  | Out sizes\n",
      "------------------------------------------------------------------\n",
      "0  | layers    | Sequential | 92.0 K | [64, 133] | [64, 1]  \n",
      "1  | layers.0  | Linear     | 34.3 K | [64, 133] | [64, 256]\n",
      "2  | layers.1  | ReLU       | 0      | [64, 256] | [64, 256]\n",
      "3  | layers.2  | Dropout    | 0      | [64, 256] | [64, 256]\n",
      "4  | layers.3  | Linear     | 32.9 K | [64, 256] | [64, 128]\n",
      "5  | layers.5  | Dropout    | 0      | [64, 128] | [64, 128]\n",
      "6  | layers.6  | Linear     | 16.5 K | [64, 128] | [64, 128]\n",
      "7  | layers.8  | Dropout    | 0      | [64, 128] | [64, 128]\n",
      "8  | layers.9  | Linear     | 8.3 K  | [64, 128] | [64, 64] \n",
      "9  | layers.11 | Dropout    | 0      | [64, 64]  | [64, 64] \n",
      "10 | layers.12 | Linear     | 65     | [64, 64]  | [64, 1]  \n",
      "------------------------------------------------------------------\n",
      "92.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "92.0 K    Total params\n",
      "0.368     Total estimated model params size (MB)\n",
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightDataModule: train_dataloader(). Training set size: 1167\n",
      "LightDataModule: train_dataloader(). batch_size: 64\n",
      "LightDataModule: train_dataloader(). num_workers: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         hp_metric         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2880.50732421875      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2880.50732421875      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        hp_metric        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2880.50732421875     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2880.50732421875     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model result: {'val_loss': 2880.50732421875, 'hp_metric': 2880.50732421875}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2880.50732421875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42)\n",
    "train_model(config=cfg, fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
